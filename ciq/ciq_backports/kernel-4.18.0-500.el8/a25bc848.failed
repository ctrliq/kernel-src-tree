KVM: arm64: Fix buffer overflow in kvm_arm_set_fw_reg()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-500.el8
commit-author Dan Carpenter <dan.carpenter@linaro.org>
commit a25bc8486f9c01c1af6b6c5657234b2eee2c39d6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-500.el8/a25bc848.failed

The KVM_REG_SIZE() comes from the ioctl and it can be a power of two
between 0-32768 but if it is more than sizeof(long) this will corrupt
memory.

Fixes: 99adb567632b ("KVM: arm/arm64: Add save/restore support for firmware workaround state")
	Signed-off-by: Dan Carpenter <dan.carpenter@linaro.org>
	Reviewed-by: Steven Price <steven.price@arm.com>
	Reviewed-by: Eric Auger <eric.auger@redhat.com>
	Reviewed-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/4efbab8c-640f-43b2-8ac6-6d68e08280fe@kili.mountain
	Signed-off-by: Oliver Upton <oliver.upton@linux.dev>
(cherry picked from commit a25bc8486f9c01c1af6b6c5657234b2eee2c39d6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kvm/hypercalls.c
diff --cc arch/arm64/kvm/hypercalls.c
index c0c0ef61fae4,c4b4678bc4a4..000000000000
--- a/arch/arm64/kvm/hypercalls.c
+++ b/arch/arm64/kvm/hypercalls.c
@@@ -78,6 -223,261 +78,263 @@@ int kvm_hvc_call_handler(struct kvm_vcp
  		return kvm_psci_call(vcpu);
  	}
  
 -out:
 -	smccc_set_retval(vcpu, val[0], val[1], val[2], val[3]);
 +	smccc_set_retval(vcpu, val, 0, 0, 0);
  	return 1;
  }
++<<<<<<< HEAD
++=======
+ 
+ static const u64 kvm_arm_fw_reg_ids[] = {
+ 	KVM_REG_ARM_PSCI_VERSION,
+ 	KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1,
+ 	KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2,
+ 	KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3,
+ 	KVM_REG_ARM_STD_BMAP,
+ 	KVM_REG_ARM_STD_HYP_BMAP,
+ 	KVM_REG_ARM_VENDOR_HYP_BMAP,
+ };
+ 
+ void kvm_arm_init_hypercalls(struct kvm *kvm)
+ {
+ 	struct kvm_smccc_features *smccc_feat = &kvm->arch.smccc_feat;
+ 
+ 	smccc_feat->std_bmap = KVM_ARM_SMCCC_STD_FEATURES;
+ 	smccc_feat->std_hyp_bmap = KVM_ARM_SMCCC_STD_HYP_FEATURES;
+ 	smccc_feat->vendor_hyp_bmap = KVM_ARM_SMCCC_VENDOR_HYP_FEATURES;
+ }
+ 
+ int kvm_arm_get_fw_num_regs(struct kvm_vcpu *vcpu)
+ {
+ 	return ARRAY_SIZE(kvm_arm_fw_reg_ids);
+ }
+ 
+ int kvm_arm_copy_fw_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(kvm_arm_fw_reg_ids); i++) {
+ 		if (put_user(kvm_arm_fw_reg_ids[i], uindices++))
+ 			return -EFAULT;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ #define KVM_REG_FEATURE_LEVEL_MASK	GENMASK(3, 0)
+ 
+ /*
+  * Convert the workaround level into an easy-to-compare number, where higher
+  * values mean better protection.
+  */
+ static int get_kernel_wa_level(u64 regid)
+ {
+ 	switch (regid) {
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1:
+ 		switch (arm64_get_spectre_v2_state()) {
+ 		case SPECTRE_VULNERABLE:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1_NOT_AVAIL;
+ 		case SPECTRE_MITIGATED:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1_AVAIL;
+ 		case SPECTRE_UNAFFECTED:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1_NOT_REQUIRED;
+ 		}
+ 		return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1_NOT_AVAIL;
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2:
+ 		switch (arm64_get_spectre_v4_state()) {
+ 		case SPECTRE_MITIGATED:
+ 			/*
+ 			 * As for the hypercall discovery, we pretend we
+ 			 * don't have any FW mitigation if SSBS is there at
+ 			 * all times.
+ 			 */
+ 			if (cpus_have_final_cap(ARM64_SSBS))
+ 				return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_NOT_AVAIL;
+ 			fallthrough;
+ 		case SPECTRE_UNAFFECTED:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_NOT_REQUIRED;
+ 		case SPECTRE_VULNERABLE:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_NOT_AVAIL;
+ 		}
+ 		break;
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3:
+ 		switch (arm64_get_spectre_bhb_state()) {
+ 		case SPECTRE_VULNERABLE:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3_NOT_AVAIL;
+ 		case SPECTRE_MITIGATED:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3_AVAIL;
+ 		case SPECTRE_UNAFFECTED:
+ 			return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3_NOT_REQUIRED;
+ 		}
+ 		return KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3_NOT_AVAIL;
+ 	}
+ 
+ 	return -EINVAL;
+ }
+ 
+ int kvm_arm_get_fw_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
+ {
+ 	struct kvm_smccc_features *smccc_feat = &vcpu->kvm->arch.smccc_feat;
+ 	void __user *uaddr = (void __user *)(long)reg->addr;
+ 	u64 val;
+ 
+ 	switch (reg->id) {
+ 	case KVM_REG_ARM_PSCI_VERSION:
+ 		val = kvm_psci_version(vcpu);
+ 		break;
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1:
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2:
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3:
+ 		val = get_kernel_wa_level(reg->id) & KVM_REG_FEATURE_LEVEL_MASK;
+ 		break;
+ 	case KVM_REG_ARM_STD_BMAP:
+ 		val = READ_ONCE(smccc_feat->std_bmap);
+ 		break;
+ 	case KVM_REG_ARM_STD_HYP_BMAP:
+ 		val = READ_ONCE(smccc_feat->std_hyp_bmap);
+ 		break;
+ 	case KVM_REG_ARM_VENDOR_HYP_BMAP:
+ 		val = READ_ONCE(smccc_feat->vendor_hyp_bmap);
+ 		break;
+ 	default:
+ 		return -ENOENT;
+ 	}
+ 
+ 	if (copy_to_user(uaddr, &val, KVM_REG_SIZE(reg->id)))
+ 		return -EFAULT;
+ 
+ 	return 0;
+ }
+ 
+ static int kvm_arm_set_fw_reg_bmap(struct kvm_vcpu *vcpu, u64 reg_id, u64 val)
+ {
+ 	int ret = 0;
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct kvm_smccc_features *smccc_feat = &kvm->arch.smccc_feat;
+ 	unsigned long *fw_reg_bmap, fw_reg_features;
+ 
+ 	switch (reg_id) {
+ 	case KVM_REG_ARM_STD_BMAP:
+ 		fw_reg_bmap = &smccc_feat->std_bmap;
+ 		fw_reg_features = KVM_ARM_SMCCC_STD_FEATURES;
+ 		break;
+ 	case KVM_REG_ARM_STD_HYP_BMAP:
+ 		fw_reg_bmap = &smccc_feat->std_hyp_bmap;
+ 		fw_reg_features = KVM_ARM_SMCCC_STD_HYP_FEATURES;
+ 		break;
+ 	case KVM_REG_ARM_VENDOR_HYP_BMAP:
+ 		fw_reg_bmap = &smccc_feat->vendor_hyp_bmap;
+ 		fw_reg_features = KVM_ARM_SMCCC_VENDOR_HYP_FEATURES;
+ 		break;
+ 	default:
+ 		return -ENOENT;
+ 	}
+ 
+ 	/* Check for unsupported bit */
+ 	if (val & ~fw_reg_features)
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	if (test_bit(KVM_ARCH_FLAG_HAS_RAN_ONCE, &kvm->arch.flags) &&
+ 	    val != *fw_reg_bmap) {
+ 		ret = -EBUSY;
+ 		goto out;
+ 	}
+ 
+ 	WRITE_ONCE(*fw_reg_bmap, val);
+ out:
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
+ int kvm_arm_set_fw_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
+ {
+ 	void __user *uaddr = (void __user *)(long)reg->addr;
+ 	u64 val;
+ 	int wa_level;
+ 
+ 	if (KVM_REG_SIZE(reg->id) != sizeof(val))
+ 		return -ENOENT;
+ 	if (copy_from_user(&val, uaddr, KVM_REG_SIZE(reg->id)))
+ 		return -EFAULT;
+ 
+ 	switch (reg->id) {
+ 	case KVM_REG_ARM_PSCI_VERSION:
+ 	{
+ 		bool wants_02;
+ 
+ 		wants_02 = test_bit(KVM_ARM_VCPU_PSCI_0_2, vcpu->arch.features);
+ 
+ 		switch (val) {
+ 		case KVM_ARM_PSCI_0_1:
+ 			if (wants_02)
+ 				return -EINVAL;
+ 			vcpu->kvm->arch.psci_version = val;
+ 			return 0;
+ 		case KVM_ARM_PSCI_0_2:
+ 		case KVM_ARM_PSCI_1_0:
+ 		case KVM_ARM_PSCI_1_1:
+ 			if (!wants_02)
+ 				return -EINVAL;
+ 			vcpu->kvm->arch.psci_version = val;
+ 			return 0;
+ 		}
+ 		break;
+ 	}
+ 
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_1:
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_3:
+ 		if (val & ~KVM_REG_FEATURE_LEVEL_MASK)
+ 			return -EINVAL;
+ 
+ 		if (get_kernel_wa_level(reg->id) < val)
+ 			return -EINVAL;
+ 
+ 		return 0;
+ 
+ 	case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2:
+ 		if (val & ~(KVM_REG_FEATURE_LEVEL_MASK |
+ 			    KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_ENABLED))
+ 			return -EINVAL;
+ 
+ 		/* The enabled bit must not be set unless the level is AVAIL. */
+ 		if ((val & KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_ENABLED) &&
+ 		    (val & KVM_REG_FEATURE_LEVEL_MASK) != KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_AVAIL)
+ 			return -EINVAL;
+ 
+ 		/*
+ 		 * Map all the possible incoming states to the only two we
+ 		 * really want to deal with.
+ 		 */
+ 		switch (val & KVM_REG_FEATURE_LEVEL_MASK) {
+ 		case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_NOT_AVAIL:
+ 		case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_UNKNOWN:
+ 			wa_level = KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_NOT_AVAIL;
+ 			break;
+ 		case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_AVAIL:
+ 		case KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_NOT_REQUIRED:
+ 			wa_level = KVM_REG_ARM_SMCCC_ARCH_WORKAROUND_2_NOT_REQUIRED;
+ 			break;
+ 		default:
+ 			return -EINVAL;
+ 		}
+ 
+ 		/*
+ 		 * We can deal with NOT_AVAIL on NOT_REQUIRED, but not the
+ 		 * other way around.
+ 		 */
+ 		if (get_kernel_wa_level(reg->id) < wa_level)
+ 			return -EINVAL;
+ 
+ 		return 0;
+ 	case KVM_REG_ARM_STD_BMAP:
+ 	case KVM_REG_ARM_STD_HYP_BMAP:
+ 	case KVM_REG_ARM_VENDOR_HYP_BMAP:
+ 		return kvm_arm_set_fw_reg_bmap(vcpu, reg->id, val);
+ 	default:
+ 		return -ENOENT;
+ 	}
+ 
+ 	return -EINVAL;
+ }
++>>>>>>> a25bc8486f9c (KVM: arm64: Fix buffer overflow in kvm_arm_set_fw_reg())
* Unmerged path arch/arm64/kvm/hypercalls.c
