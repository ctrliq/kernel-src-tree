huge tmpfs: shmem_is_huge(vma, inode, index)

jira LE-4623
Rebuild_History Non-Buildable kernel-4.18.0-553.81.1.el8_10
commit-author Hugh Dickins <hughd@google.com>
commit 5e6e5a12a44ca5ff2b130d8d39aaf9b8c026de94
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.81.1.el8_10/5e6e5a12.failed

Extend shmem_huge_enabled(vma) to shmem_is_huge(vma, inode, index), so
that a consistent set of checks can be applied, even when the inode is
accessed through read/write syscalls (with NULL vma) instead of mmaps (the
index argument is seldom of interest, but required by mount option
"huge=within_size").  Clean up and rearrange the checks a little.

This then replaces the checks which shmem_fault() and shmem_getpage_gfp()
were making, and eliminates the SGP_HUGE and SGP_NOHUGE modes.

Replace a couple of 0s by explicit SHMEM_HUGE_NEVERs; and replace the
obscure !shmem_mapping() symlink check by explicit S_ISLNK() - nothing
else needs that symlink check, so leave it there in shmem_getpage_gfp().

Link: https://lkml.kernel.org/r/23a77889-2ddc-b030-75cd-44ca27fd4d1@google.com
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Reviewed-by: Yang Shi <shy828301@gmail.com>
	Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Miaohe Lin <linmiaohe@huawei.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: Shakeel Butt <shakeelb@google.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5e6e5a12a44ca5ff2b130d8d39aaf9b8c026de94)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/shmem.c
diff --cc mm/shmem.c
index ef7127a77071,69c9788a0094..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -413,9 -471,40 +413,44 @@@ static bool shmem_confirm_swap(struct a
  #ifdef CONFIG_TRANSPARENT_HUGEPAGE
  /* ifdef here to avoid bloating shmem.o when not necessary */
  
- static int shmem_huge __read_mostly;
+ static int shmem_huge __read_mostly = SHMEM_HUGE_NEVER;
  
++<<<<<<< HEAD
 +#if defined(CONFIG_SYSFS) || defined(CONFIG_TMPFS)
++=======
+ bool shmem_is_huge(struct vm_area_struct *vma,
+ 		   struct inode *inode, pgoff_t index)
+ {
+ 	loff_t i_size;
+ 
+ 	if (shmem_huge == SHMEM_HUGE_DENY)
+ 		return false;
+ 	if (vma && ((vma->vm_flags & VM_NOHUGEPAGE) ||
+ 	    test_bit(MMF_DISABLE_THP, &vma->vm_mm->flags)))
+ 		return false;
+ 	if (shmem_huge == SHMEM_HUGE_FORCE)
+ 		return true;
+ 
+ 	switch (SHMEM_SB(inode->i_sb)->huge) {
+ 	case SHMEM_HUGE_ALWAYS:
+ 		return true;
+ 	case SHMEM_HUGE_WITHIN_SIZE:
+ 		index = round_up(index, HPAGE_PMD_NR);
+ 		i_size = round_up(i_size_read(inode), PAGE_SIZE);
+ 		if (i_size >= HPAGE_PMD_SIZE && (i_size >> PAGE_SHIFT) >= index)
+ 			return true;
+ 		fallthrough;
+ 	case SHMEM_HUGE_ADVISE:
+ 		if (vma && (vma->vm_flags & VM_HUGEPAGE))
+ 			return true;
+ 		fallthrough;
+ 	default:
+ 		return false;
+ 	}
+ }
+ 
+ #if defined(CONFIG_SYSFS)
++>>>>>>> 5e6e5a12a44c (huge tmpfs: shmem_is_huge(vma, inode, index))
  static int shmem_parse_huge(const char *str)
  {
  	if (!strcmp(str, "never"))
@@@ -1802,36 -1885,12 +1840,17 @@@ repeat
  		return 0;
  	}
  
++<<<<<<< HEAD
 +	/* shmem_symlink() */
 +	if (mapping->a_ops != &shmem_aops)
++=======
+ 	/* Never use a huge page for shmem_symlink() */
+ 	if (S_ISLNK(inode->i_mode))
++>>>>>>> 5e6e5a12a44c (huge tmpfs: shmem_is_huge(vma, inode, index))
  		goto alloc_nohuge;
- 	if (shmem_huge == SHMEM_HUGE_DENY || sgp_huge == SGP_NOHUGE)
+ 	if (!shmem_is_huge(vma, inode, index))
  		goto alloc_nohuge;
- 	if (shmem_huge == SHMEM_HUGE_FORCE)
- 		goto alloc_huge;
- 	switch (sbinfo->huge) {
- 	case SHMEM_HUGE_NEVER:
- 		goto alloc_nohuge;
- 	case SHMEM_HUGE_WITHIN_SIZE: {
- 		loff_t i_size;
- 		pgoff_t off;
  
- 		off = round_up(index, HPAGE_PMD_NR);
- 		i_size = round_up(i_size_read(inode), PAGE_SIZE);
- 		if (i_size >= HPAGE_PMD_SIZE &&
- 		    i_size >> PAGE_SHIFT >= off)
- 			goto alloc_huge;
- 
- 		fallthrough;
- 	}
- 	case SHMEM_HUGE_ADVISE:
- 		if (sgp_huge == SGP_HUGE)
- 			goto alloc_huge;
- 		/* TODO: implement fadvise() hints */
- 		goto alloc_nohuge;
- 	}
- 
- alloc_huge:
  	huge_gfp = vma_thp_gfp_mask(vma);
  	huge_gfp = limit_gfp_mask(huge_gfp, gfp);
  	page = shmem_alloc_and_acct_page(huge_gfp, inode, index, true);
diff --git a/include/linux/shmem_fs.h b/include/linux/shmem_fs.h
index 03958535e7ef..ad1322e8f024 100644
--- a/include/linux/shmem_fs.h
+++ b/include/linux/shmem_fs.h
@@ -79,7 +79,12 @@ extern void shmem_truncate_range(struct inode *inode, loff_t start, loff_t end);
 extern int shmem_unuse(unsigned int type, bool frontswap,
 		       unsigned long *fs_pages_to_unuse);
 
-extern bool shmem_huge_enabled(struct vm_area_struct *vma);
+extern bool shmem_is_huge(struct vm_area_struct *vma,
+			  struct inode *inode, pgoff_t index);
+static inline bool shmem_huge_enabled(struct vm_area_struct *vma)
+{
+	return shmem_is_huge(vma, file_inode(vma->vm_file), vma->vm_pgoff);
+}
 extern unsigned long shmem_swap_usage(struct vm_area_struct *vma);
 extern unsigned long shmem_partial_swap_usage(struct address_space *mapping,
 						pgoff_t start, pgoff_t end);
@@ -89,8 +94,6 @@ enum sgp_type {
 	SGP_READ,	/* don't exceed i_size, don't allocate page */
 	SGP_NOALLOC,	/* similar, but fail on hole or use fallocated page */
 	SGP_CACHE,	/* don't exceed i_size, may allocate page */
-	SGP_NOHUGE,	/* like SGP_CACHE, but no huge pages */
-	SGP_HUGE,	/* like SGP_CACHE, huge pages preferred */
 	SGP_WRITE,	/* may exceed i_size, may allocate !Uptodate page */
 	SGP_FALLOC,	/* like SGP_WRITE, but make existing page Uptodate */
 };
* Unmerged path mm/shmem.c
