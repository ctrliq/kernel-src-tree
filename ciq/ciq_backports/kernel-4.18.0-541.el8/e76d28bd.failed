cgroup/rstat: Reduce cpu_lock hold time in cgroup_rstat_flush_locked()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-541.el8
commit-author Waiman Long <longman@redhat.com>
commit e76d28bdf9ba5388b8c4835a5199dc427b603188
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-541.el8/e76d28bd.failed

When cgroup_rstat_updated() isn't being called concurrently with
cgroup_rstat_flush_locked(), its run time is pretty short. When
both are called concurrently, the cgroup_rstat_updated() run time
can spike to a pretty high value due to high cpu_lock hold time in
cgroup_rstat_flush_locked(). This can be problematic if the task calling
cgroup_rstat_updated() is a realtime task running on an isolated CPU
with a strict latency requirement. The cgroup_rstat_updated() call can
happen when there is a page fault even though the task is running in
user space most of the time.

The percpu cpu_lock is used to protect the update tree -
updated_next and updated_children. This protection is only needed when
cgroup_rstat_cpu_pop_updated() is being called. The subsequent flushing
operation which can take a much longer time does not need that protection
as it is already protected by cgroup_rstat_lock.

To reduce the cpu_lock hold time, we need to perform all the
cgroup_rstat_cpu_pop_updated() calls up front with the lock
released afterward before doing any flushing. This patch adds a new
cgroup_rstat_updated_list() function to return a singly linked list of
cgroups to be flushed.

Some instrumentation code are added to measure the cpu_lock hold time
right after lock acquisition to after releasing the lock. Parallel
kernel build on a 2-socket x86-64 server is used as the benchmarking
tool for measuring the lock hold time.

The maximum cpu_lock hold time before and after the patch are 100us and
29us respectively. So the worst case time is reduced to about 30% of
the original. However, there may be some OS or hardware noises like NMI
or SMI in the test system that can worsen the worst case value. Those
noises are usually tuned out in a real production environment to get
a better result.

OTOH, the lock hold time frequency distribution should give a better
idea of the performance benefit of the patch.  Below were the frequency
distribution before and after the patch:

     Hold time        Before patch       After patch
     ---------        ------------       -----------
       0-01 us           804,139         13,738,708
      01-05 us         9,772,767          1,177,194
      05-10 us         4,595,028              4,984
      10-15 us           303,481              3,562
      15-20 us            78,971              1,314
      20-25 us            24,583                 18
      25-30 us             6,908                 12
      30-40 us             8,015
      40-50 us             2,192
      50-60 us               316
      60-70 us                43
      70-80 us                 7
      80-90 us                 2
        >90 us                 3

	Signed-off-by: Waiman Long <longman@redhat.com>
	Reviewed-by: Yosry Ahmed <yosryahmed@google.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit e76d28bdf9ba5388b8c4835a5199dc427b603188)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cgroup/rstat.c
diff --cc kernel/cgroup/rstat.c
index a4fd6b3c4097,1f300bf4dc40..000000000000
--- a/kernel/cgroup/rstat.c
+++ b/kernel/cgroup/rstat.c
@@@ -140,8 -145,59 +140,62 @@@ static struct cgroup *cgroup_rstat_cpu_
  	return pos;
  }
  
++<<<<<<< HEAD
++=======
+ /* Return a list of updated cgroups to be flushed */
+ static struct cgroup *cgroup_rstat_updated_list(struct cgroup *root, int cpu)
+ {
+ 	raw_spinlock_t *cpu_lock = per_cpu_ptr(&cgroup_rstat_cpu_lock, cpu);
+ 	struct cgroup *head, *tail, *next;
+ 	unsigned long flags;
+ 
+ 	/*
+ 	 * The _irqsave() is needed because cgroup_rstat_lock is
+ 	 * spinlock_t which is a sleeping lock on PREEMPT_RT. Acquiring
+ 	 * this lock with the _irq() suffix only disables interrupts on
+ 	 * a non-PREEMPT_RT kernel. The raw_spinlock_t below disables
+ 	 * interrupts on both configurations. The _irqsave() ensures
+ 	 * that interrupts are always disabled and later restored.
+ 	 */
+ 	raw_spin_lock_irqsave(cpu_lock, flags);
+ 	head = tail = cgroup_rstat_cpu_pop_updated(NULL, root, cpu);
+ 	while (tail) {
+ 		next = cgroup_rstat_cpu_pop_updated(tail, root, cpu);
+ 		tail->rstat_flush_next = next;
+ 		tail = next;
+ 	}
+ 	raw_spin_unlock_irqrestore(cpu_lock, flags);
+ 	return head;
+ }
+ 
+ /*
+  * A hook for bpf stat collectors to attach to and flush their stats.
+  * Together with providing bpf kfuncs for cgroup_rstat_updated() and
+  * cgroup_rstat_flush(), this enables a complete workflow where bpf progs that
+  * collect cgroup stats can integrate with rstat for efficient flushing.
+  *
+  * A static noinline declaration here could cause the compiler to optimize away
+  * the function. A global noinline declaration will keep the definition, but may
+  * optimize away the callsite. Therefore, __weak is needed to ensure that the
+  * call is still emitted, by telling the compiler that we don't know what the
+  * function might eventually be.
+  *
+  * __diag_* below are needed to dismiss the missing prototype warning.
+  */
+ __diag_push();
+ __diag_ignore_all("-Wmissing-prototypes",
+ 		  "kfuncs which will be used in BPF programs");
+ 
+ __weak noinline void bpf_rstat_flush(struct cgroup *cgrp,
+ 				     struct cgroup *parent, int cpu)
+ {
+ }
+ 
+ __diag_pop();
+ 
++>>>>>>> e76d28bdf9ba (cgroup/rstat: Reduce cpu_lock hold time in cgroup_rstat_flush_locked())
  /* see cgroup_rstat_flush() */
 -static void cgroup_rstat_flush_locked(struct cgroup *cgrp)
 +static void cgroup_rstat_flush_locked(struct cgroup *cgrp, bool may_sleep)
  	__releases(&cgroup_rstat_lock) __acquires(&cgroup_rstat_lock)
  {
  	int cpu;
@@@ -174,11 -219,9 +216,10 @@@
  				css->ss->css_rstat_flush(css, cpu);
  			rcu_read_unlock();
  		}
- 		raw_spin_unlock_irqrestore(cpu_lock, flags);
  
 -		/* play nice and yield if necessary */
 -		if (need_resched() || spin_needbreak(&cgroup_rstat_lock)) {
 +		/* if @may_sleep, play nice and yield if necessary */
 +		if (may_sleep && (need_resched() ||
 +				  spin_needbreak(&cgroup_rstat_lock))) {
  			spin_unlock_irq(&cgroup_rstat_lock);
  			if (!cond_resched())
  				cpu_relax();
diff --git a/include/linux/cgroup-defs.h b/include/linux/cgroup-defs.h
index dcd51ba95429..00a331bb880e 100644
--- a/include/linux/cgroup-defs.h
+++ b/include/linux/cgroup-defs.h
@@ -468,6 +468,13 @@ struct cgroup {
 	struct cgroup_rstat_cpu __percpu *rstat_cpu;
 	struct list_head rstat_css_list;
 
+	/*
+	 * A singly-linked list of cgroup structures to be rstat flushed.
+	 * This is a scratch field to be used exclusively by
+	 * cgroup_rstat_flush_locked() and protected by cgroup_rstat_lock.
+	 */
+	struct cgroup	*rstat_flush_next;
+
 	/* cgroup basic resource statistics */
 	struct cgroup_base_stat RH_KABI_RENAME(pending_bstat, last_bstat);
 	struct cgroup_base_stat bstat;
* Unmerged path kernel/cgroup/rstat.c
