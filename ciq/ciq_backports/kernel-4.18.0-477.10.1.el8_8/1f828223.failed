memcg: flush lruvec stats in the refault

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-477.10.1.el8_8
commit-author Shakeel Butt <shakeelb@google.com>
commit 1f828223b7991a228bc2aef837b78737946d44b2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-477.10.1.el8_8/1f828223.failed

Prior to the commit 7e1c0d6f5820 ("memcg: switch lruvec stats to rstat")
and the commit aa48e47e3906 ("memcg: infrastructure to flush memcg
stats"), each lruvec memcg stats can be off by (nr_cgroups * nr_cpus *
32) at worst and for unbounded amount of time.  The commit aa48e47e3906
moved the lruvec stats to rstat infrastructure and the commit
7e1c0d6f5820 bounded the error for all the lruvec stats to (nr_cpus *
32) at worst for at most 2 seconds.  More specifically it decoupled the
number of stats and the number of cgroups from the error rate.

However this reduction in error comes with the cost of triggering the
slowpath of stats update more frequently.  Previously in the slowpath
the kernel adds the stats up the memcg tree.  After aa48e47e3906, the
kernel triggers the asyn lruvec stats flush through queue_work().  This
causes regression reports from 0day kernel bot [1] as well as from
phoronix test suite [2].

We tried two options to fix the regression:

 1) Increase the threshold to trigger the slowpath in lruvec stats
    update codepath from 32 to 512.

 2) Remove the slowpath from lruvec stats update codepath and instead
    flush the stats in the page refault codepath. The assumption is that
    the kernel timely flush the stats, so, the update tree would be
    small in the refault codepath to not cause the preformance impact.

Following are the results of will-it-scale/page_fault[1|2|3] benchmark
on four settings i.e.  (1) 5.15-rc1 as baseline (2) 5.15-rc1 with
aa48e47e3906 and 7e1c0d6f5820 reverted (3) 5.15-rc1 with option-1
(4) 5.15-rc1 with option-2.

  test       (1)      (2)               (3)               (4)
  pg_f1   368563   406277 (10.23%)   399693  (8.44%)   416398 (12.97%)
  pg_f2   338399   372133  (9.96%)   369180  (9.09%)   381024 (12.59%)
  pg_f3   500853   575399 (14.88%)   570388 (13.88%)   576083 (15.02%)

From the above result, it seems like the option-2 not only solves the
regression but also improves the performance for at least these
benchmarks.

Feng Tang (intel) ran the aim7 benchmark with these two options and
confirms that option-1 reduces the regression but option-2 removes the
regression.

Michael Larabel (phoronix) ran multiple benchmarks with these options
and reported the results at [3] and it shows for most benchmarks
option-2 removes the regression introduced by the commit aa48e47e3906
("memcg: infrastructure to flush memcg stats").

Based on the experiment results, this patch proposed the option-2 as the
solution to resolve the regression.

Link: https://lore.kernel.org/all/20210726022421.GB21872@xsang-OptiPlex-9020 [1]
Link: https://www.phoronix.com/scan.php?page=article&item=linux515-compile-regress [2]
Link: https://openbenchmarking.org/result/2109226-DEBU-LINUX5104 [3]
Fixes: aa48e47e3906 ("memcg: infrastructure to flush memcg stats")
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Tested-by: Michael Larabel <Michael@phoronix.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Roman Gushchin <guro@fb.com>
	Cc: Feng Tang <feng.tang@intel.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Hillf Danton <hdanton@sina.com>,
	Cc: Michal Koutn√Ω <mkoutny@suse.com>
	Cc: Andrew Morton <akpm@linux-foundation.org>,
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1f828223b7991a228bc2aef837b78737946d44b2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index 6bc1413fddb1,6da5020a8656..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -104,6 -103,11 +104,14 @@@ static bool do_memsw_account(void
  	return !cgroup_subsys_on_dfl(memory_cgrp_subsys) && !cgroup_memory_noswap;
  }
  
++<<<<<<< HEAD
++=======
+ /* memcg and lruvec stats flushing */
+ static void flush_memcg_stats_dwork(struct work_struct *w);
+ static DECLARE_DEFERRABLE_WORK(stats_flush_dwork, flush_memcg_stats_dwork);
+ static DEFINE_SPINLOCK(stats_flush_lock);
+ 
++>>>>>>> 1f828223b799 (memcg: flush lruvec stats in the refault)
  #define THRESHOLDS_EVENTS_TARGET 128
  #define SOFTLIMIT_EVENTS_TARGET 1024
  
@@@ -857,20 -677,8 +865,25 @@@ void __mod_memcg_lruvec_state(struct lr
  	/* Update memcg */
  	__mod_memcg_state(memcg, idx, val);
  
++<<<<<<< HEAD
 +	if (vmstat_item_in_bytes(idx))
 +		threshold <<= PAGE_SHIFT;
 +
 +	x = val + __this_cpu_read(pn->lruvec_stat_cpu->count[idx]);
 +	if (unlikely(abs(x) > threshold)) {
 +		pg_data_t *pgdat = lruvec_pgdat(lruvec);
 +		struct mem_cgroup_per_node *pi;
 +
 +		for (pi = pn; pi; pi = parent_nodeinfo(pi, pgdat->node_id))
 +			atomic_long_add(x, &pi->lruvec_stat[idx]);
 +		x = 0;
 +	}
 +	__this_cpu_write(pn->lruvec_stat_cpu->count[idx], x);
 +	memcg_stats_unlock();
++=======
+ 	/* Update lruvec */
+ 	__this_cpu_add(pn->lruvec_stats_percpu->state[idx], val);
++>>>>>>> 1f828223b799 (memcg: flush lruvec stats in the refault)
  }
  
  /**
@@@ -5589,6 -5341,21 +5602,24 @@@ static void mem_cgroup_css_reset(struc
  	memcg_wb_domain_size_changed(memcg);
  }
  
++<<<<<<< HEAD
++=======
+ void mem_cgroup_flush_stats(void)
+ {
+ 	if (!spin_trylock(&stats_flush_lock))
+ 		return;
+ 
+ 	cgroup_rstat_flush_irqsafe(root_mem_cgroup->css.cgroup);
+ 	spin_unlock(&stats_flush_lock);
+ }
+ 
+ static void flush_memcg_stats_dwork(struct work_struct *w)
+ {
+ 	mem_cgroup_flush_stats();
+ 	queue_delayed_work(system_unbound_wq, &stats_flush_dwork, 2UL*HZ);
+ }
+ 
++>>>>>>> 1f828223b799 (memcg: flush lruvec stats in the refault)
  static void mem_cgroup_css_rstat_flush(struct cgroup_subsys_state *css, int cpu)
  {
  	struct mem_cgroup *memcg = mem_cgroup_from_css(css);
* Unmerged path mm/memcontrol.c
diff --git a/mm/workingset.c b/mm/workingset.c
index 2ad3de738d84..d0e5d886eeae 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -350,6 +350,7 @@ void workingset_refault(struct page *page, void *shadow)
 
 	inc_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file);
 
+	mem_cgroup_flush_stats();
 	/*
 	 * Compare the distance to the existing workingset size. We
 	 * don't activate pages that couldn't stay resident even if
