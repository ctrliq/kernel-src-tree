arm64: perf: Remove PMU locking

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-477.10.1.el8_8
commit-author Julien Thierry <julien.thierry@arm.com>
commit 2a0e2a02e4b719174547d6f04c27410c6fe456f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-477.10.1.el8_8/2a0e2a02.failed

The PMU is disabled and enabled, and the counters are programmed from
contexts where interrupts or preemption is disabled.

The functions to toggle the PMU and to program the PMU counters access the
registers directly and don't access data modified by the interrupt handler.
That, and the fact that they're always called from non-preemptible
contexts, means that we don't need to disable interrupts or use a spinlock.

[Alexandru E.: Explained why locking is not needed, removed WARN_ONs]

	Signed-off-by: Julien Thierry <julien.thierry@arm.com>
	Signed-off-by: Alexandru Elisei <alexandru.elisei@arm.com>
	Tested-by: Sumit Garg <sumit.garg@linaro.org> (Developerbox)
	Cc: Will Deacon <will.deacon@arm.com>
	Cc: Mark Rutland <mark.rutland@arm.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
	Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
	Cc: Jiri Olsa <jolsa@redhat.com>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
Link: https://lore.kernel.org/r/20200924110706.254996-4-alexandru.elisei@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 2a0e2a02e4b719174547d6f04c27410c6fe456f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/perf_event.c
diff --cc arch/arm64/kernel/perf_event.c
index 7ad9c0a6c4f6,077df5aea19a..000000000000
--- a/arch/arm64/kernel/perf_event.c
+++ b/arch/arm64/kernel/perf_event.c
@@@ -680,34 -689,8 +680,30 @@@ static inline u32 armv8pmu_getreset_fla
  	return value;
  }
  
 +static void armv8pmu_disable_user_access(void)
 +{
 +	write_sysreg(0, pmuserenr_el0);
 +}
 +
 +static void armv8pmu_enable_user_access(struct arm_pmu *cpu_pmu)
 +{
 +	int i;
 +	struct pmu_hw_events *cpuc = this_cpu_ptr(cpu_pmu->hw_events);
 +
 +	/* Clear any unused counters to avoid leaking their contents */
 +	for_each_clear_bit(i, cpuc->used_mask, cpu_pmu->num_events) {
 +		if (i == ARMV8_IDX_CYCLE_COUNTER)
 +			write_sysreg(0, pmccntr_el0);
 +		else
 +			armv8pmu_write_evcntr(i, 0);
 +	}
 +
 +	write_sysreg(0, pmuserenr_el0);
 +	write_sysreg(ARMV8_PMU_USERENR_ER | ARMV8_PMU_USERENR_CR, pmuserenr_el0);
 +}
 +
  static void armv8pmu_enable_event(struct perf_event *event)
  {
- 	unsigned long flags;
- 	struct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);
- 	struct pmu_hw_events *events = this_cpu_ptr(cpu_pmu->hw_events);
- 
  	/*
  	 * Enable counter and interrupt, and set the counter to count
  	 * the event that we're interested in.
@@@ -763,22 -732,8 +745,24 @@@ static void armv8pmu_disable_event(stru
  
  static void armv8pmu_start(struct arm_pmu *cpu_pmu)
  {
++<<<<<<< HEAD
 +	unsigned long flags;
 +	struct pmu_hw_events *events = this_cpu_ptr(cpu_pmu->hw_events);
 +
 +	raw_spin_lock_irqsave(&events->pmu_lock, flags);
 +
 +	struct perf_event_context *task_ctx =
 +		this_cpu_ptr(cpu_pmu->pmu.pmu_cpu_context)->task_ctx;
 +
 +	if (sysctl_perf_user_access && task_ctx && task_ctx->nr_user)
 +		armv8pmu_enable_user_access(cpu_pmu);
 +	else
 +		armv8pmu_disable_user_access();
 +
++=======
++>>>>>>> 2a0e2a02e4b7 (arm64: perf: Remove PMU locking)
  	/* Enable all counters */
  	armv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMU_PMCR_E);
- 	raw_spin_unlock_irqrestore(&events->pmu_lock, flags);
  }
  
  static void armv8pmu_stop(struct arm_pmu *cpu_pmu)
* Unmerged path arch/arm64/kernel/perf_event.c
