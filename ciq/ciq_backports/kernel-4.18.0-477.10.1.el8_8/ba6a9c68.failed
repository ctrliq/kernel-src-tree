RDMA/mlx5: Simplify get_umr_update_access_mask()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-477.10.1.el8_8
commit-author Aharon Landau <aharonl@nvidia.com>
commit ba6a9c6899b2dce3c615ce9fac6bead976e43a48
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-477.10.1.el8_8/ba6a9c68.failed

Instead of getting the update access capabilities each call to
get_umr_update_access_mask(), pass struct mlx5_ib_dev and get the
capabilities inside the function.

Link: https://lore.kernel.org/r/f22b8a84ef32e29ada26691f06b57e2ed5943b76.1649747695.git.leonro@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Michael Guralnik <michaelgur@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit ba6a9c6899b2dce3c615ce9fac6bead976e43a48)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/umr.c
diff --cc drivers/infiniband/hw/mlx5/umr.c
index 46eaf919eb49,8131501dc052..000000000000
--- a/drivers/infiniband/hw/mlx5/umr.c
+++ b/drivers/infiniband/hw/mlx5/umr.c
@@@ -4,6 -4,130 +4,133 @@@
  #include "mlx5_ib.h"
  #include "umr.h"
  
++<<<<<<< HEAD
++=======
+ static __be64 get_umr_enable_mr_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_KEY |
+ 		 MLX5_MKEY_MASK_FREE;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_disable_mr_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_FREE;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_update_translation_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_LEN |
+ 		 MLX5_MKEY_MASK_PAGE_SIZE |
+ 		 MLX5_MKEY_MASK_START_ADDR;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_update_access_mask(struct mlx5_ib_dev *dev)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_LR |
+ 		 MLX5_MKEY_MASK_LW |
+ 		 MLX5_MKEY_MASK_RR |
+ 		 MLX5_MKEY_MASK_RW;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, atomic))
+ 		result |= MLX5_MKEY_MASK_A;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))
+ 		result |= MLX5_MKEY_MASK_RELAXED_ORDERING_WRITE;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))
+ 		result |= MLX5_MKEY_MASK_RELAXED_ORDERING_READ;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_update_pd_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_PD;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static int umr_check_mkey_mask(struct mlx5_ib_dev *dev, u64 mask)
+ {
+ 	if (mask & MLX5_MKEY_MASK_PAGE_SIZE &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))
+ 		return -EPERM;
+ 
+ 	if (mask & MLX5_MKEY_MASK_A &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
+ 		return -EPERM;
+ 
+ 	if (mask & MLX5_MKEY_MASK_RELAXED_ORDERING_WRITE &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))
+ 		return -EPERM;
+ 
+ 	if (mask & MLX5_MKEY_MASK_RELAXED_ORDERING_READ &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))
+ 		return -EPERM;
+ 
+ 	return 0;
+ }
+ 
+ int mlx5r_umr_set_umr_ctrl_seg(struct mlx5_ib_dev *dev,
+ 			       struct mlx5_wqe_umr_ctrl_seg *umr,
+ 			       const struct ib_send_wr *wr)
+ {
+ 	const struct mlx5_umr_wr *umrwr = umr_wr(wr);
+ 
+ 	memset(umr, 0, sizeof(*umr));
+ 
+ 	if (!umrwr->ignore_free_state) {
+ 		if (wr->send_flags & MLX5_IB_SEND_UMR_FAIL_IF_FREE)
+ 			 /* fail if free */
+ 			umr->flags = MLX5_UMR_CHECK_FREE;
+ 		else
+ 			/* fail if not free */
+ 			umr->flags = MLX5_UMR_CHECK_NOT_FREE;
+ 	}
+ 
+ 	umr->xlt_octowords =
+ 		cpu_to_be16(mlx5r_umr_get_xlt_octo(umrwr->xlt_size));
+ 	if (wr->send_flags & MLX5_IB_SEND_UMR_UPDATE_XLT) {
+ 		u64 offset = mlx5r_umr_get_xlt_octo(umrwr->offset);
+ 
+ 		umr->xlt_offset = cpu_to_be16(offset & 0xffff);
+ 		umr->xlt_offset_47_16 = cpu_to_be32(offset >> 16);
+ 		umr->flags |= MLX5_UMR_TRANSLATION_OFFSET_EN;
+ 	}
+ 	if (wr->send_flags & MLX5_IB_SEND_UMR_UPDATE_TRANSLATION)
+ 		umr->mkey_mask |= get_umr_update_translation_mask();
+ 	if (wr->send_flags & MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS) {
+ 		umr->mkey_mask |= get_umr_update_access_mask(dev);
+ 		umr->mkey_mask |= get_umr_update_pd_mask();
+ 	}
+ 	if (wr->send_flags & MLX5_IB_SEND_UMR_ENABLE_MR)
+ 		umr->mkey_mask |= get_umr_enable_mr_mask();
+ 	if (wr->send_flags & MLX5_IB_SEND_UMR_DISABLE_MR)
+ 		umr->mkey_mask |= get_umr_disable_mr_mask();
+ 
+ 	if (!wr->num_sge)
+ 		umr->flags |= MLX5_UMR_INLINE;
+ 
+ 	return umr_check_mkey_mask(dev, be64_to_cpu(umr->mkey_mask));
+ }
+ 
++>>>>>>> ba6a9c6899b2 (RDMA/mlx5: Simplify get_umr_update_access_mask())
  enum {
  	MAX_UMR_WR = 128,
  };
* Unmerged path drivers/infiniband/hw/mlx5/umr.c
