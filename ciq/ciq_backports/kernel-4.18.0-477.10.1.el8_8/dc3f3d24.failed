x86/mm: Validate memory when changing the C-bit

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-477.10.1.el8_8
commit-author Brijesh Singh <brijesh.singh@amd.com>
commit dc3f3d2474b80eaee8be89f4c5eb344f10648f42
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-477.10.1.el8_8/dc3f3d24.failed

Add the needed functionality to change pages state from shared
to private and vice-versa using the Page State Change VMGEXIT as
documented in the GHCB spec.

	Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lore.kernel.org/r/20220307213356.2797205-22-brijesh.singh@amd.com
(cherry picked from commit dc3f3d2474b80eaee8be89f4c5eb344f10648f42)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/sev.h
#	arch/x86/include/uapi/asm/svm.h
#	arch/x86/kernel/sev.c
#	arch/x86/mm/mem_encrypt_amd.c
diff --cc arch/x86/include/asm/sev.h
index 45ba3d868cd3,feeb93e6ec97..000000000000
--- a/arch/x86/include/asm/sev.h
+++ b/arch/x86/include/asm/sev.h
@@@ -107,6 -122,14 +107,17 @@@ static inline int pvalidate(unsigned lo
  
  	return rc;
  }
++<<<<<<< HEAD
++=======
+ void setup_ghcb(void);
+ void __init early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr,
+ 					 unsigned int npages);
+ void __init early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr,
+ 					unsigned int npages);
+ void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op);
+ void snp_set_memory_shared(unsigned long vaddr, unsigned int npages);
+ void snp_set_memory_private(unsigned long vaddr, unsigned int npages);
++>>>>>>> dc3f3d2474b8 (x86/mm: Validate memory when changing the C-bit)
  #else
  static inline void sev_es_ist_enter(struct pt_regs *regs) { }
  static inline void sev_es_ist_exit(void) { }
@@@ -114,6 -137,15 +125,18 @@@ static inline int sev_es_setup_ap_jump_
  static inline void sev_es_nmi_complete(void) { }
  static inline int sev_es_efi_map_ghcbs(pgd_t *pgd) { return 0; }
  static inline int pvalidate(unsigned long vaddr, bool rmp_psize, bool validate) { return 0; }
++<<<<<<< HEAD
++=======
+ static inline int rmpadjust(unsigned long vaddr, bool rmp_psize, unsigned long attrs) { return 0; }
+ static inline void setup_ghcb(void) { }
+ static inline void __init
+ early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr, unsigned int npages) { }
+ static inline void __init
+ early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr, unsigned int npages) { }
+ static inline void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op) { }
+ static inline void snp_set_memory_shared(unsigned long vaddr, unsigned int npages) { }
+ static inline void snp_set_memory_private(unsigned long vaddr, unsigned int npages) { }
++>>>>>>> dc3f3d2474b8 (x86/mm: Validate memory when changing the C-bit)
  #endif
  
  #endif
diff --cc arch/x86/include/uapi/asm/svm.h
index efa969325ede,64404b47b773..000000000000
--- a/arch/x86/include/uapi/asm/svm.h
+++ b/arch/x86/include/uapi/asm/svm.h
@@@ -108,6 -108,8 +108,11 @@@
  #define SVM_VMGEXIT_AP_JUMP_TABLE		0x80000005
  #define SVM_VMGEXIT_SET_AP_JUMP_TABLE		0
  #define SVM_VMGEXIT_GET_AP_JUMP_TABLE		1
++<<<<<<< HEAD
++=======
+ #define SVM_VMGEXIT_PSC				0x80000010
+ #define SVM_VMGEXIT_HV_FEATURES			0x8000fffd
++>>>>>>> dc3f3d2474b8 (x86/mm: Validate memory when changing the C-bit)
  #define SVM_VMGEXIT_UNSUPPORTED_EVENT		0x8000ffff
  
  /* Exit code reserved for hypervisor/software use */
@@@ -218,6 -220,8 +223,11 @@@
  	{ SVM_VMGEXIT_NMI_COMPLETE,	"vmgexit_nmi_complete" }, \
  	{ SVM_VMGEXIT_AP_HLT_LOOP,	"vmgexit_ap_hlt_loop" }, \
  	{ SVM_VMGEXIT_AP_JUMP_TABLE,	"vmgexit_ap_jump_table" }, \
++<<<<<<< HEAD
++=======
+ 	{ SVM_VMGEXIT_PSC,		"vmgexit_page_state_change" }, \
+ 	{ SVM_VMGEXIT_HV_FEATURES,	"vmgexit_hypervisor_feature" }, \
++>>>>>>> dc3f3d2474b8 (x86/mm: Validate memory when changing the C-bit)
  	{ SVM_EXIT_ERR,         "invalid_guest_state" }
  
  
diff --cc arch/x86/kernel/sev.c
index 9f3a4a57b1e6,bf4b57835694..000000000000
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@@ -539,6 -556,273 +539,276 @@@ static u64 get_jump_table_addr(void
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static void pvalidate_pages(unsigned long vaddr, unsigned int npages, bool validate)
+ {
+ 	unsigned long vaddr_end;
+ 	int rc;
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		rc = pvalidate(vaddr, RMP_PG_SIZE_4K, validate);
+ 		if (WARN(rc, "Failed to validate address 0x%lx ret %d", vaddr, rc))
+ 			sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PVALIDATE);
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 	}
+ }
+ 
+ static void __init early_set_pages_state(unsigned long paddr, unsigned int npages, enum psc_op op)
+ {
+ 	unsigned long paddr_end;
+ 	u64 val;
+ 
+ 	paddr = paddr & PAGE_MASK;
+ 	paddr_end = paddr + (npages << PAGE_SHIFT);
+ 
+ 	while (paddr < paddr_end) {
+ 		/*
+ 		 * Use the MSR protocol because this function can be called before
+ 		 * the GHCB is established.
+ 		 */
+ 		sev_es_wr_ghcb_msr(GHCB_MSR_PSC_REQ_GFN(paddr >> PAGE_SHIFT, op));
+ 		VMGEXIT();
+ 
+ 		val = sev_es_rd_ghcb_msr();
+ 
+ 		if (WARN(GHCB_RESP_CODE(val) != GHCB_MSR_PSC_RESP,
+ 			 "Wrong PSC response code: 0x%x\n",
+ 			 (unsigned int)GHCB_RESP_CODE(val)))
+ 			goto e_term;
+ 
+ 		if (WARN(GHCB_MSR_PSC_RESP_VAL(val),
+ 			 "Failed to change page state to '%s' paddr 0x%lx error 0x%llx\n",
+ 			 op == SNP_PAGE_STATE_PRIVATE ? "private" : "shared",
+ 			 paddr, GHCB_MSR_PSC_RESP_VAL(val)))
+ 			goto e_term;
+ 
+ 		paddr = paddr + PAGE_SIZE;
+ 	}
+ 
+ 	return;
+ 
+ e_term:
+ 	sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ void __init early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr,
+ 					 unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	 /*
+ 	  * Ask the hypervisor to mark the memory pages as private in the RMP
+ 	  * table.
+ 	  */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	/* Validate the memory pages after they've been added in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
+ void __init early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr,
+ 					unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	/* Invalidate the memory pages before they are marked shared in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	 /* Ask hypervisor to mark the memory pages shared in the RMP table. */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op)
+ {
+ 	unsigned long vaddr, npages;
+ 
+ 	vaddr = (unsigned long)__va(paddr);
+ 	npages = PAGE_ALIGN(sz) >> PAGE_SHIFT;
+ 
+ 	if (op == SNP_PAGE_STATE_PRIVATE)
+ 		early_snp_set_memory_private(vaddr, paddr, npages);
+ 	else if (op == SNP_PAGE_STATE_SHARED)
+ 		early_snp_set_memory_shared(vaddr, paddr, npages);
+ 	else
+ 		WARN(1, "invalid memory op %d\n", op);
+ }
+ 
+ static int vmgexit_psc(struct snp_psc_desc *desc)
+ {
+ 	int cur_entry, end_entry, ret = 0;
+ 	struct snp_psc_desc *data;
+ 	struct ghcb_state state;
+ 	struct es_em_ctxt ctxt;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 
+ 	/*
+ 	 * __sev_get_ghcb() needs to run with IRQs disabled because it is using
+ 	 * a per-CPU GHCB.
+ 	 */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 	if (!ghcb) {
+ 		ret = 1;
+ 		goto out_unlock;
+ 	}
+ 
+ 	/* Copy the input desc into GHCB shared buffer */
+ 	data = (struct snp_psc_desc *)ghcb->shared_buffer;
+ 	memcpy(ghcb->shared_buffer, desc, min_t(int, GHCB_SHARED_BUF_SIZE, sizeof(*desc)));
+ 
+ 	/*
+ 	 * As per the GHCB specification, the hypervisor can resume the guest
+ 	 * before processing all the entries. Check whether all the entries
+ 	 * are processed. If not, then keep retrying. Note, the hypervisor
+ 	 * will update the data memory directly to indicate the status, so
+ 	 * reference the data->hdr everywhere.
+ 	 *
+ 	 * The strategy here is to wait for the hypervisor to change the page
+ 	 * state in the RMP table before guest accesses the memory pages. If the
+ 	 * page state change was not successful, then later memory access will
+ 	 * result in a crash.
+ 	 */
+ 	cur_entry = data->hdr.cur_entry;
+ 	end_entry = data->hdr.end_entry;
+ 
+ 	while (data->hdr.cur_entry <= data->hdr.end_entry) {
+ 		ghcb_set_sw_scratch(ghcb, (u64)__pa(data));
+ 
+ 		/* This will advance the shared buffer data points to. */
+ 		ret = sev_es_ghcb_hv_call(ghcb, true, &ctxt, SVM_VMGEXIT_PSC, 0, 0);
+ 
+ 		/*
+ 		 * Page State Change VMGEXIT can pass error code through
+ 		 * exit_info_2.
+ 		 */
+ 		if (WARN(ret || ghcb->save.sw_exit_info_2,
+ 			 "SNP: PSC failed ret=%d exit_info_2=%llx\n",
+ 			 ret, ghcb->save.sw_exit_info_2)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/* Verify that reserved bit is not set */
+ 		if (WARN(data->hdr.reserved, "Reserved bit is set in the PSC header\n")) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/*
+ 		 * Sanity check that entry processing is not going backwards.
+ 		 * This will happen only if hypervisor is tricking us.
+ 		 */
+ 		if (WARN(data->hdr.end_entry > end_entry || cur_entry > data->hdr.cur_entry,
+ "SNP: PSC processing going backward, end_entry %d (got %d) cur_entry %d (got %d)\n",
+ 			 end_entry, data->hdr.end_entry, cur_entry, data->hdr.cur_entry)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 	}
+ 
+ out:
+ 	__sev_put_ghcb(&state);
+ 
+ out_unlock:
+ 	local_irq_restore(flags);
+ 
+ 	return ret;
+ }
+ 
+ static void __set_pages_state(struct snp_psc_desc *data, unsigned long vaddr,
+ 			      unsigned long vaddr_end, int op)
+ {
+ 	struct psc_hdr *hdr;
+ 	struct psc_entry *e;
+ 	unsigned long pfn;
+ 	int i;
+ 
+ 	hdr = &data->hdr;
+ 	e = data->entries;
+ 
+ 	memset(data, 0, sizeof(*data));
+ 	i = 0;
+ 
+ 	while (vaddr < vaddr_end) {
+ 		if (is_vmalloc_addr((void *)vaddr))
+ 			pfn = vmalloc_to_pfn((void *)vaddr);
+ 		else
+ 			pfn = __pa(vaddr) >> PAGE_SHIFT;
+ 
+ 		e->gfn = pfn;
+ 		e->operation = op;
+ 		hdr->end_entry = i;
+ 
+ 		/*
+ 		 * Current SNP implementation doesn't keep track of the RMP page
+ 		 * size so use 4K for simplicity.
+ 		 */
+ 		e->pagesize = RMP_PG_SIZE_4K;
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 		e++;
+ 		i++;
+ 	}
+ 
+ 	if (vmgexit_psc(data))
+ 		sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ static void set_pages_state(unsigned long vaddr, unsigned int npages, int op)
+ {
+ 	unsigned long vaddr_end, next_vaddr;
+ 	struct snp_psc_desc *desc;
+ 
+ 	desc = kmalloc(sizeof(*desc), GFP_KERNEL_ACCOUNT);
+ 	if (!desc)
+ 		panic("SNP: failed to allocate memory for PSC descriptor\n");
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		/* Calculate the last vaddr that fits in one struct snp_psc_desc. */
+ 		next_vaddr = min_t(unsigned long, vaddr_end,
+ 				   (VMGEXIT_PSC_MAX_ENTRY * PAGE_SIZE) + vaddr);
+ 
+ 		__set_pages_state(desc, vaddr, next_vaddr, op);
+ 
+ 		vaddr = next_vaddr;
+ 	}
+ 
+ 	kfree(desc);
+ }
+ 
+ void snp_set_memory_shared(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void snp_set_memory_private(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
++>>>>>>> dc3f3d2474b8 (x86/mm: Validate memory when changing the C-bit)
  int sev_es_setup_ap_jump_table(struct real_mode_header *rmh)
  {
  	u16 startup_cs, startup_ip;
diff --cc arch/x86/mm/mem_encrypt_amd.c
index 48e92fcc23e3,d3c88d9ef8d6..000000000000
--- a/arch/x86/mm/mem_encrypt_amd.c
+++ b/arch/x86/mm/mem_encrypt_amd.c
@@@ -286,6 -314,32 +286,35 @@@ void notify_range_enc_status_changed(un
  #endif
  }
  
++<<<<<<< HEAD
++=======
+ static void amd_enc_status_change_prepare(unsigned long vaddr, int npages, bool enc)
+ {
+ 	/*
+ 	 * To maintain the security guarantees of SEV-SNP guests, make sure
+ 	 * to invalidate the memory before encryption attribute is cleared.
+ 	 */
+ 	if (cc_platform_has(CC_ATTR_GUEST_SEV_SNP) && !enc)
+ 		snp_set_memory_shared(vaddr, npages);
+ }
+ 
+ /* Return true unconditionally: return value doesn't matter for the SEV side */
+ static bool amd_enc_status_change_finish(unsigned long vaddr, int npages, bool enc)
+ {
+ 	/*
+ 	 * After memory is mapped encrypted in the page table, validate it
+ 	 * so that it is consistent with the page table updates.
+ 	 */
+ 	if (cc_platform_has(CC_ATTR_GUEST_SEV_SNP) && enc)
+ 		snp_set_memory_private(vaddr, npages);
+ 
+ 	if (!cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT))
+ 		enc_dec_hypercall(vaddr, npages, enc);
+ 
+ 	return true;
+ }
+ 
++>>>>>>> dc3f3d2474b8 (x86/mm: Validate memory when changing the C-bit)
  static void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)
  {
  	pgprot_t old_prot, new_prot;
diff --git a/arch/x86/include/asm/sev-common.h b/arch/x86/include/asm/sev-common.h
index 94f0ea574049..d76f4b3c2656 100644
--- a/arch/x86/include/asm/sev-common.h
+++ b/arch/x86/include/asm/sev-common.h
@@ -61,6 +61,28 @@
 #define GHCB_MSR_HV_FT_REQ		0x080
 #define GHCB_MSR_HV_FT_RESP		0x081
 
+/* SNP Page State Change NAE event */
+#define VMGEXIT_PSC_MAX_ENTRY		253
+
+struct psc_hdr {
+	u16 cur_entry;
+	u16 end_entry;
+	u32 reserved;
+} __packed;
+
+struct psc_entry {
+	u64	cur_page	: 12,
+		gfn		: 40,
+		operation	: 4,
+		pagesize	: 1,
+		reserved	: 7;
+} __packed;
+
+struct snp_psc_desc {
+	struct psc_hdr hdr;
+	struct psc_entry entries[VMGEXIT_PSC_MAX_ENTRY];
+} __packed;
+
 #define GHCB_MSR_TERM_REQ		0x100
 #define GHCB_MSR_TERM_REASON_SET_POS	12
 #define GHCB_MSR_TERM_REASON_SET_MASK	0xf
* Unmerged path arch/x86/include/asm/sev.h
* Unmerged path arch/x86/include/uapi/asm/svm.h
* Unmerged path arch/x86/kernel/sev.c
* Unmerged path arch/x86/mm/mem_encrypt_amd.c
