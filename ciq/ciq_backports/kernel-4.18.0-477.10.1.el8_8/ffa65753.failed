mm/migrate.c: rework migration_entry_wait() to not take a pageref

jira LE-1907
cve CVE-2022-3623
Rebuild_History Non-Buildable kernel-4.18.0-477.10.1.el8_8
commit-author Alistair Popple <apopple@nvidia.com>
commit ffa65753c43142f3b803486442813744da71cff2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-477.10.1.el8_8/ffa65753.failed

This fixes the FIXME in migrate_vma_check_page().

Before migrating a page migration code will take a reference and check
there are no unexpected page references, failing the migration if there
are.  When a thread faults on a migration entry it will take a temporary
reference to the page to wait for the page to become unlocked signifying
the migration entry has been removed.

This reference is dropped just prior to waiting on the page lock,
however the extra reference can cause migration failures so it is
desirable to avoid taking it.

As migration code already has a reference to the migrating page an extra
reference to wait on PG_locked is unnecessary so long as the reference
can't be dropped whilst setting up the wait.

When faulting on a migration entry the ptl is taken to check the
migration entry.  Removing a migration entry also requires the ptl, and
migration code won't drop its page reference until after the migration
entry has been removed.  Therefore retaining the ptl of a migration
entry is sufficient to ensure the page has a reference.  Reworking
migration_entry_wait() to hold the ptl until the wait setup is complete
means the extra page reference is no longer needed.

[apopple@nvidia.com: v5]
  Link: https://lkml.kernel.org/r/20211213033848.1973946-1-apopple@nvidia.com

Link: https://lkml.kernel.org/r/20211118020754.954425-1-apopple@nvidia.com
	Signed-off-by: Alistair Popple <apopple@nvidia.com>
	Acked-by: David Hildenbrand <david@redhat.com>
	Cc: David Howells <dhowells@redhat.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Jason Gunthorpe <jgg@nvidia.com>
	Cc: Jerome Glisse <jglisse@redhat.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
	Cc: Ralph Campbell <rcampbell@nvidia.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ffa65753c43142f3b803486442813744da71cff2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/migrate.h
#	mm/filemap.c
#	mm/migrate.c
diff --cc include/linux/migrate.h
index 1b7f50abcd9c,db96e10eb8da..000000000000
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@@ -54,6 -40,14 +54,17 @@@ extern int migrate_huge_page_move_mappi
  				  struct page *newpage, struct page *page);
  extern int migrate_page_move_mapping(struct address_space *mapping,
  		struct page *newpage, struct page *page, int extra_count);
++<<<<<<< HEAD
++=======
+ void migration_entry_wait_on_locked(swp_entry_t entry, pte_t *ptep,
+ 				spinlock_t *ptl);
+ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);
+ void folio_migrate_copy(struct folio *newfolio, struct folio *folio);
+ int folio_migrate_mapping(struct address_space *mapping,
+ 		struct folio *newfolio, struct folio *folio, int extra_count);
+ 
+ extern bool numa_demotion_enabled;
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  #else
  
  static inline void putback_movable_pages(struct list_head *l) {}
diff --cc mm/filemap.c
index 931209c1c503,60866ae711e2..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -39,9 -38,13 +40,15 @@@
  #include <linux/cleancache.h>
  #include <linux/shmem_fs.h>
  #include <linux/rmap.h>
 -#include <linux/delayacct.h>
 +#include RH_KABI_HIDE_INCLUDE(<linux/delayacct.h>)
  #include <linux/psi.h>
 -#include <linux/ramfs.h>
  #include <linux/page_idle.h>
++<<<<<<< HEAD
++=======
+ #include <linux/migrate.h>
+ #include <asm/pgalloc.h>
+ #include <asm/tlbflush.h>
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  #include "internal.h"
  
  #define CREATE_TRACE_POINTS
@@@ -1267,37 -1390,123 +1274,130 @@@ static inline int wait_on_page_bit_comm
  	return wait->flags & WQ_FLAG_WOKEN ? 0 : -EINTR;
  }
  
++<<<<<<< HEAD
 +void wait_on_page_bit(struct page *page, int bit_nr)
++=======
+ #ifdef CONFIG_MIGRATION
+ /**
+  * migration_entry_wait_on_locked - Wait for a migration entry to be removed
+  * @entry: migration swap entry.
+  * @ptep: mapped pte pointer. Will return with the ptep unmapped. Only required
+  *        for pte entries, pass NULL for pmd entries.
+  * @ptl: already locked ptl. This function will drop the lock.
+  *
+  * Wait for a migration entry referencing the given page to be removed. This is
+  * equivalent to put_and_wait_on_page_locked(page, TASK_UNINTERRUPTIBLE) except
+  * this can be called without taking a reference on the page. Instead this
+  * should be called while holding the ptl for the migration entry referencing
+  * the page.
+  *
+  * Returns after unmapping and unlocking the pte/ptl with pte_unmap_unlock().
+  *
+  * This follows the same logic as folio_wait_bit_common() so see the comments
+  * there.
+  */
+ void migration_entry_wait_on_locked(swp_entry_t entry, pte_t *ptep,
+ 				spinlock_t *ptl)
+ {
+ 	struct wait_page_queue wait_page;
+ 	wait_queue_entry_t *wait = &wait_page.wait;
+ 	bool thrashing = false;
+ 	bool delayacct = false;
+ 	unsigned long pflags;
+ 	wait_queue_head_t *q;
+ 	struct folio *folio = page_folio(pfn_swap_entry_to_page(entry));
+ 
+ 	q = folio_waitqueue(folio);
+ 	if (!folio_test_uptodate(folio) && folio_test_workingset(folio)) {
+ 		if (!folio_test_swapbacked(folio)) {
+ 			delayacct_thrashing_start();
+ 			delayacct = true;
+ 		}
+ 		psi_memstall_enter(&pflags);
+ 		thrashing = true;
+ 	}
+ 
+ 	init_wait(wait);
+ 	wait->func = wake_page_function;
+ 	wait_page.folio = folio;
+ 	wait_page.bit_nr = PG_locked;
+ 	wait->flags = 0;
+ 
+ 	spin_lock_irq(&q->lock);
+ 	folio_set_waiters(folio);
+ 	if (!folio_trylock_flag(folio, PG_locked, wait))
+ 		__add_wait_queue_entry_tail(q, wait);
+ 	spin_unlock_irq(&q->lock);
+ 
+ 	/*
+ 	 * If a migration entry exists for the page the migration path must hold
+ 	 * a valid reference to the page, and it must take the ptl to remove the
+ 	 * migration entry. So the page is valid until the ptl is dropped.
+ 	 */
+ 	if (ptep)
+ 		pte_unmap_unlock(ptep, ptl);
+ 	else
+ 		spin_unlock(ptl);
+ 
+ 	for (;;) {
+ 		unsigned int flags;
+ 
+ 		set_current_state(TASK_UNINTERRUPTIBLE);
+ 
+ 		/* Loop until we've been woken or interrupted */
+ 		flags = smp_load_acquire(&wait->flags);
+ 		if (!(flags & WQ_FLAG_WOKEN)) {
+ 			if (signal_pending_state(TASK_UNINTERRUPTIBLE, current))
+ 				break;
+ 
+ 			io_schedule();
+ 			continue;
+ 		}
+ 		break;
+ 	}
+ 
+ 	finish_wait(q, wait);
+ 
+ 	if (thrashing) {
+ 		if (delayacct)
+ 			delayacct_thrashing_end();
+ 		psi_memstall_leave(&pflags);
+ 	}
+ }
+ #endif
+ 
+ void folio_wait_bit(struct folio *folio, int bit_nr)
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  {
 -	folio_wait_bit_common(folio, bit_nr, TASK_UNINTERRUPTIBLE, SHARED);
 +	wait_queue_head_t *q = page_waitqueue(page);
 +	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, SHARED);
  }
 -EXPORT_SYMBOL(folio_wait_bit);
 +EXPORT_SYMBOL(wait_on_page_bit);
  
 -int folio_wait_bit_killable(struct folio *folio, int bit_nr)
 +int wait_on_page_bit_killable(struct page *page, int bit_nr)
  {
 -	return folio_wait_bit_common(folio, bit_nr, TASK_KILLABLE, SHARED);
 +	wait_queue_head_t *q = page_waitqueue(page);
 +	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, SHARED);
  }
 -EXPORT_SYMBOL(folio_wait_bit_killable);
 +EXPORT_SYMBOL(wait_on_page_bit_killable);
  
  /**
 - * folio_put_wait_locked - Drop a reference and wait for it to be unlocked
 - * @folio: The folio to wait for.
 - * @state: The sleep state (TASK_KILLABLE, TASK_UNINTERRUPTIBLE, etc).
 + * put_and_wait_on_page_locked - Drop a reference and wait for it to be unlocked
 + * @page: The page to wait for.
   *
 - * The caller should hold a reference on @folio.  They expect the page to
 + * The caller should hold a reference on @page.  They expect the page to
   * become unlocked relatively soon, but do not wish to hold up migration
 - * (for example) by holding the reference while waiting for the folio to
 + * (for example) by holding the reference while waiting for the page to
   * come unlocked.  After this function returns, the caller should not
 - * dereference @folio.
 - *
 - * Return: 0 if the folio was unlocked or -EINTR if interrupted by a signal.
 + * dereference @page.
   */
 -int folio_put_wait_locked(struct folio *folio, int state)
 +void put_and_wait_on_page_locked(struct page *page)
  {
 -	return folio_wait_bit_common(folio, PG_locked, state, DROP);
 +	wait_queue_head_t *q;
 +
 +	page = compound_head(page);
 +	q = page_waitqueue(page);
 +	wait_on_page_bit_common(q, page, PG_locked, TASK_UNINTERRUPTIBLE, DROP);
  }
  
  /**
diff --cc mm/migrate.c
index 2523c4bf4b93,c7da064b4781..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -314,7 -291,6 +314,10 @@@ void __migration_entry_wait(struct mm_s
  {
  	pte_t pte;
  	swp_entry_t entry;
++<<<<<<< HEAD
 +	struct page *page;
++=======
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  
  	spin_lock(ptl);
  	pte = *ptep;
@@@ -325,18 -301,7 +328,22 @@@
  	if (!is_migration_entry(entry))
  		goto out;
  
++<<<<<<< HEAD
 +	page = pfn_swap_entry_to_page(entry);
 +	page = compound_head(page);
 +
 +	/*
 +	 * Once page cache replacement of page migration started, page_count
 +	 * is zero; but we must not call put_and_wait_on_page_locked() without
 +	 * a ref. Use get_page_unless_zero(), and just fault again if it fails.
 +	 */
 +	if (!get_page_unless_zero(page))
 +		goto out;
 +	pte_unmap_unlock(ptep, ptl);
 +	put_and_wait_on_page_locked(page);
++=======
+ 	migration_entry_wait_on_locked(entry, ptep, ptl);
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  	return;
  out:
  	pte_unmap_unlock(ptep, ptl);
@@@ -361,16 -326,11 +368,23 @@@ void migration_entry_wait_huge(struct v
  void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)
  {
  	spinlock_t *ptl;
++<<<<<<< HEAD
 +	struct page *page;
++=======
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  
  	ptl = pmd_lock(mm, pmd);
  	if (!is_pmd_migration_entry(*pmd))
  		goto unlock;
++<<<<<<< HEAD
 +	page = pfn_swap_entry_to_page(pmd_to_swp_entry(*pmd));
 +	if (!get_page_unless_zero(page))
 +		goto unlock;
 +	spin_unlock(ptl);
 +	put_and_wait_on_page_locked(page);
++=======
+ 	migration_entry_wait_on_locked(pmd_to_swp_entry(*pmd), NULL, ptl);
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  	return;
  unlock:
  	spin_unlock(ptl);
@@@ -2508,22 -2415,8 +2522,27 @@@ static bool migrate_vma_check_page(stru
  		return false;
  
  	/* Page from ZONE_DEVICE have one extra reference */
++<<<<<<< HEAD
 +	if (is_zone_device_page(page)) {
 +		/*
 +		 * Private page can never be pin as they have no valid pte and
 +		 * GUP will fail for those. Yet if there is a pending migration
 +		 * a thread might try to wait on the pte migration entry and
 +		 * will bump the page reference count. Sadly there is no way to
 +		 * differentiate a regular pin from migration wait. Hence to
 +		 * avoid 2 racing thread trying to migrate back to CPU to enter
 +		 * infinite loop (one stoping migration because the other is
 +		 * waiting on pte migration entry). We always return true here.
 +		 *
 +		 * FIXME proper solution is to rework migration_entry_wait() so
 +		 * it does not need to take a reference on page.
 +		 */
 +		return is_device_private_page(page);
 +	}
++=======
+ 	if (is_zone_device_page(page))
+ 		extra++;
++>>>>>>> ffa65753c431 (mm/migrate.c: rework migration_entry_wait() to not take a pageref)
  
  	/* For file back page */
  	if (page_mapping(page))
* Unmerged path include/linux/migrate.h
* Unmerged path mm/filemap.c
* Unmerged path mm/migrate.c
