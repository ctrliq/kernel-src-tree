RDMA/cma: Introduce and use cma_ib_acquire_dev()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Parav Pandit <parav@mellanox.com>
commit 41ab1cb7d1cd5d53d68bcf5fb3fddad77af15545
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/41ab1cb7.failed

When RDMA CM connect request arrives for IB transport, it already contains
device, port, netdevice (optional).

Instead of traversing all the cma devices, use the cma device already
found by the cma_find_listener() for which a listener id is provided.

iWarp devices doesn't need to derive RoCE GIDs, therefore drop RoCE
specific checks from cma_acquire_dev() and rename it to
cma_iw_acquire_dev().

	Signed-off-by: Parav Pandit <parav@mellanox.com>
	Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit 41ab1cb7d1cd5d53d68bcf5fb3fddad77af15545)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/cma.c
diff --cc drivers/infiniband/core/cma.c
index 9ce634301eda,897aac68158b..000000000000
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@@ -690,8 -691,57 +690,62 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int cma_acquire_dev(struct rdma_id_private *id_priv,
 +			   struct rdma_id_private *listen_id_priv)
++=======
+ /**
+  * cma_ib_acquire_dev - Acquire cma device, port and SGID attribute
+  * @id_priv:		cm id to bind to cma device
+  * @listen_id_priv:	listener cm id to match against
+  * @req:		Pointer to req structure containaining incoming
+  *			request information
+  * cma_ib_acquire_dev() acquires cma device, port and SGID attribute when
+  * rdma device matches for listen_id and incoming request. It also verifies
+  * that a GID table entry is present for the source address.
+  * Returns 0 on success, or returns error code otherwise.
+  */
+ static int cma_ib_acquire_dev(struct rdma_id_private *id_priv,
+ 			      const struct rdma_id_private *listen_id_priv,
+ 			      struct cma_req_info *req)
+ {
+ 	struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
+ 	const struct ib_gid_attr *sgid_attr;
+ 	enum ib_gid_type gid_type;
+ 	union ib_gid gid;
+ 
+ 	if (dev_addr->dev_type != ARPHRD_INFINIBAND &&
+ 	    id_priv->id.ps == RDMA_PS_IPOIB)
+ 		return -EINVAL;
+ 
+ 	if (rdma_protocol_roce(req->device, req->port))
+ 		rdma_ip2gid((struct sockaddr *)&id_priv->id.route.addr.src_addr,
+ 			    &gid);
+ 	else
+ 		memcpy(&gid, dev_addr->src_dev_addr +
+ 		       rdma_addr_gid_offset(dev_addr), sizeof(gid));
+ 
+ 	gid_type = listen_id_priv->cma_dev->default_gid_type[req->port - 1];
+ 	sgid_attr = cma_validate_port(req->device, req->port,
+ 				      gid_type, &gid, id_priv);
+ 	if (IS_ERR(sgid_attr))
+ 		return PTR_ERR(sgid_attr);
+ 
+ 	id_priv->id.port_num = req->port;
+ 	cma_bind_sgid_attr(id_priv, sgid_attr);
+ 	/* Need to acquire lock to protect against reader
+ 	 * of cma_dev->id_list such as cma_netdev_callback() and
+ 	 * cma_process_remove().
+ 	 */
+ 	mutex_lock(&lock);
+ 	cma_attach_to_dev(id_priv, listen_id_priv->cma_dev);
+ 	mutex_unlock(&lock);
+ 	return 0;
+ }
+ 
+ static int cma_iw_acquire_dev(struct rdma_id_private *id_priv,
+ 			      const struct rdma_id_private *listen_id_priv)
++>>>>>>> 41ab1cb7d1cd (RDMA/cma: Introduce and use cma_ib_acquire_dev())
  {
  	struct rdma_dev_addr *dev_addr = &id_priv->id.route.addr.dev_addr;
  	const struct ib_gid_attr *sgid_attr;
@@@ -1573,11 -1623,12 +1622,18 @@@ static struct rdma_id_private *cma_find
  	return ERR_PTR(-EINVAL);
  }
  
++<<<<<<< HEAD
 +static struct rdma_id_private *cma_id_from_event(struct ib_cm_id *cm_id,
 +						 struct ib_cm_event *ib_event,
 +						 struct net_device **net_dev)
++=======
+ static struct rdma_id_private *
+ cma_ib_id_from_event(struct ib_cm_id *cm_id,
+ 		     const struct ib_cm_event *ib_event,
+ 		     struct cma_req_info *req,
+ 		     struct net_device **net_dev)
++>>>>>>> 41ab1cb7d1cd (RDMA/cma: Introduce and use cma_ib_acquire_dev())
  {
- 	struct cma_req_info req;
  	struct rdma_bind_list *bind_list;
  	struct rdma_id_private *id_priv;
  	int err;
@@@ -2041,15 -2102,17 +2097,24 @@@ static int cma_check_req_qp_type(struc
  		(!id->qp_type));
  }
  
 -static int cma_ib_req_handler(struct ib_cm_id *cm_id,
 -			      const struct ib_cm_event *ib_event)
 +static int cma_req_handler(struct ib_cm_id *cm_id, struct ib_cm_event *ib_event)
  {
  	struct rdma_id_private *listen_id, *conn_id = NULL;
++<<<<<<< HEAD
 +	struct rdma_cm_event event;
++=======
+ 	struct rdma_cm_event event = {};
+ 	struct cma_req_info req = {};
++>>>>>>> 41ab1cb7d1cd (RDMA/cma: Introduce and use cma_ib_acquire_dev())
  	struct net_device *net_dev;
  	u8 offset;
  	int ret;
  
++<<<<<<< HEAD
 +	listen_id = cma_id_from_event(cm_id, ib_event, &net_dev);
++=======
+ 	listen_id = cma_ib_id_from_event(cm_id, ib_event, &req, &net_dev);
++>>>>>>> 41ab1cb7d1cd (RDMA/cma: Introduce and use cma_ib_acquire_dev())
  	if (IS_ERR(listen_id))
  		return PTR_ERR(listen_id);
  
* Unmerged path drivers/infiniband/core/cma.c
