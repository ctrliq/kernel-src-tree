scsi: mpt3sas: Fix sparse warnings

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [scsi] mpt3sas: Fix sparse warnings (Tomas Henzl) [1513855]
Rebuild_FUZZ: 90.32%
commit-author Suganath Prabu Subramani <suganath-prabu.subramani@broadcom.com>
commit 494f401bcd07d6b39f49f114e4eaa788842f16fe
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/494f401b.failed

1) Used variable __le64/__le32 whichever required in building NVME
PRP, and passed to LE Controller.

2) Remove unused functions, And Declared functions as static which are
used only in mpt3sas_scsih.c.

	Signed-off-by: Chaitra P B <chaitra.basappa@broadcom.com>
	Signed-off-by: Suganath Prabu S <suganath-prabu.subramani@broadcom.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit 494f401bcd07d6b39f49f114e4eaa788842f16fe)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/mpt3sas/mpt3sas_base.c
diff --cc drivers/scsi/mpt3sas/mpt3sas_base.c
index 028ee6d296e5,3061c1724eaf..000000000000
--- a/drivers/scsi/mpt3sas/mpt3sas_base.c
+++ b/drivers/scsi/mpt3sas/mpt3sas_base.c
@@@ -1367,6 -1375,436 +1367,439 @@@ _base_build_sg(struct MPT3SAS_ADAPTER *
  /* IEEE format sgls */
  
  /**
++<<<<<<< HEAD
++=======
+  * _base_build_nvme_prp - This function is called for NVMe end devices to build
+  * a native SGL (NVMe PRP). The native SGL is built starting in the first PRP
+  * entry of the NVMe message (PRP1).  If the data buffer is small enough to be
+  * described entirely using PRP1, then PRP2 is not used.  If needed, PRP2 is
+  * used to describe a larger data buffer.  If the data buffer is too large to
+  * describe using the two PRP entriess inside the NVMe message, then PRP1
+  * describes the first data memory segment, and PRP2 contains a pointer to a PRP
+  * list located elsewhere in memory to describe the remaining data memory
+  * segments.  The PRP list will be contiguous.
+ 
+  * The native SGL for NVMe devices is a Physical Region Page (PRP).  A PRP
+  * consists of a list of PRP entries to describe a number of noncontigous
+  * physical memory segments as a single memory buffer, just as a SGL does.  Note
+  * however, that this function is only used by the IOCTL call, so the memory
+  * given will be guaranteed to be contiguous.  There is no need to translate
+  * non-contiguous SGL into a PRP in this case.  All PRPs will describe
+  * contiguous space that is one page size each.
+  *
+  * Each NVMe message contains two PRP entries.  The first (PRP1) either contains
+  * a PRP list pointer or a PRP element, depending upon the command.  PRP2
+  * contains the second PRP element if the memory being described fits within 2
+  * PRP entries, or a PRP list pointer if the PRP spans more than two entries.
+  *
+  * A PRP list pointer contains the address of a PRP list, structured as a linear
+  * array of PRP entries.  Each PRP entry in this list describes a segment of
+  * physical memory.
+  *
+  * Each 64-bit PRP entry comprises an address and an offset field.  The address
+  * always points at the beginning of a 4KB physical memory page, and the offset
+  * describes where within that 4KB page the memory segment begins.  Only the
+  * first element in a PRP list may contain a non-zero offest, implying that all
+  * memory segments following the first begin at the start of a 4KB page.
+  *
+  * Each PRP element normally describes 4KB of physical memory, with exceptions
+  * for the first and last elements in the list.  If the memory being described
+  * by the list begins at a non-zero offset within the first 4KB page, then the
+  * first PRP element will contain a non-zero offset indicating where the region
+  * begins within the 4KB page.  The last memory segment may end before the end
+  * of the 4KB segment, depending upon the overall size of the memory being
+  * described by the PRP list.
+  *
+  * Since PRP entries lack any indication of size, the overall data buffer length
+  * is used to determine where the end of the data memory buffer is located, and
+  * how many PRP entries are required to describe it.
+  *
+  * @ioc: per adapter object
+  * @smid: system request message index for getting asscociated SGL
+  * @nvme_encap_request: the NVMe request msg frame pointer
+  * @data_out_dma: physical address for WRITES
+  * @data_out_sz: data xfer size for WRITES
+  * @data_in_dma: physical address for READS
+  * @data_in_sz: data xfer size for READS
+  *
+  * Returns nothing.
+  */
+ static void
+ _base_build_nvme_prp(struct MPT3SAS_ADAPTER *ioc, u16 smid,
+ 	Mpi26NVMeEncapsulatedRequest_t *nvme_encap_request,
+ 	dma_addr_t data_out_dma, size_t data_out_sz, dma_addr_t data_in_dma,
+ 	size_t data_in_sz)
+ {
+ 	int		prp_size = NVME_PRP_SIZE;
+ 	__le64		*prp_entry, *prp1_entry, *prp2_entry, *prp_entry_phys;
+ 	__le64		*prp_page, *prp_page_phys;
+ 	u32		offset, entry_len;
+ 	u32		page_mask_result, page_mask;
+ 	dma_addr_t	paddr;
+ 	size_t		length;
+ 
+ 	/*
+ 	 * Not all commands require a data transfer. If no data, just return
+ 	 * without constructing any PRP.
+ 	 */
+ 	if (!data_in_sz && !data_out_sz)
+ 		return;
+ 	/*
+ 	 * Set pointers to PRP1 and PRP2, which are in the NVMe command.
+ 	 * PRP1 is located at a 24 byte offset from the start of the NVMe
+ 	 * command.  Then set the current PRP entry pointer to PRP1.
+ 	 */
+ 	prp1_entry = (__le64 *)(nvme_encap_request->NVMe_Command +
+ 	    NVME_CMD_PRP1_OFFSET);
+ 	prp2_entry = (__le64 *)(nvme_encap_request->NVMe_Command +
+ 	    NVME_CMD_PRP2_OFFSET);
+ 	prp_entry = prp1_entry;
+ 	/*
+ 	 * For the PRP entries, use the specially allocated buffer of
+ 	 * contiguous memory.
+ 	 */
+ 	prp_page = (__le64 *)mpt3sas_base_get_pcie_sgl(ioc, smid);
+ 	prp_page_phys = (__le64 *)mpt3sas_base_get_pcie_sgl_dma(ioc, smid);
+ 
+ 	/*
+ 	 * Check if we are within 1 entry of a page boundary we don't
+ 	 * want our first entry to be a PRP List entry.
+ 	 */
+ 	page_mask = ioc->page_size - 1;
+ 	page_mask_result = (uintptr_t)((u8 *)prp_page + prp_size) & page_mask;
+ 	if (!page_mask_result) {
+ 		/* Bump up to next page boundary. */
+ 		prp_page = (__le64 *)((u8 *)prp_page + prp_size);
+ 		prp_page_phys = (__le64 *)((u8 *)prp_page_phys + prp_size);
+ 	}
+ 
+ 	/*
+ 	 * Set PRP physical pointer, which initially points to the current PRP
+ 	 * DMA memory page.
+ 	 */
+ 	prp_entry_phys = prp_page_phys;
+ 
+ 	/* Get physical address and length of the data buffer. */
+ 	if (data_in_sz) {
+ 		paddr = data_in_dma;
+ 		length = data_in_sz;
+ 	} else {
+ 		paddr = data_out_dma;
+ 		length = data_out_sz;
+ 	}
+ 
+ 	/* Loop while the length is not zero. */
+ 	while (length) {
+ 		/*
+ 		 * Check if we need to put a list pointer here if we are at
+ 		 * page boundary - prp_size (8 bytes).
+ 		 */
+ 		page_mask_result =
+ 		    (uintptr_t)((u8 *)prp_entry_phys + prp_size) & page_mask;
+ 		if (!page_mask_result) {
+ 			/*
+ 			 * This is the last entry in a PRP List, so we need to
+ 			 * put a PRP list pointer here.  What this does is:
+ 			 *   - bump the current memory pointer to the next
+ 			 *     address, which will be the next full page.
+ 			 *   - set the PRP Entry to point to that page.  This
+ 			 *     is now the PRP List pointer.
+ 			 *   - bump the PRP Entry pointer the start of the
+ 			 *     next page.  Since all of this PRP memory is
+ 			 *     contiguous, no need to get a new page - it's
+ 			 *     just the next address.
+ 			 */
+ 			prp_entry_phys++;
+ 			*prp_entry = cpu_to_le64((uintptr_t)prp_entry_phys);
+ 			prp_entry++;
+ 		}
+ 
+ 		/* Need to handle if entry will be part of a page. */
+ 		offset = (u32)paddr & page_mask;
+ 		entry_len = ioc->page_size - offset;
+ 
+ 		if (prp_entry == prp1_entry) {
+ 			/*
+ 			 * Must fill in the first PRP pointer (PRP1) before
+ 			 * moving on.
+ 			 */
+ 			*prp1_entry = cpu_to_le64((u64)paddr);
+ 
+ 			/*
+ 			 * Now point to the second PRP entry within the
+ 			 * command (PRP2).
+ 			 */
+ 			prp_entry = prp2_entry;
+ 		} else if (prp_entry == prp2_entry) {
+ 			/*
+ 			 * Should the PRP2 entry be a PRP List pointer or just
+ 			 * a regular PRP pointer?  If there is more than one
+ 			 * more page of data, must use a PRP List pointer.
+ 			 */
+ 			if (length > ioc->page_size) {
+ 				/*
+ 				 * PRP2 will contain a PRP List pointer because
+ 				 * more PRP's are needed with this command. The
+ 				 * list will start at the beginning of the
+ 				 * contiguous buffer.
+ 				 */
+ 				*prp2_entry =
+ 				    cpu_to_le64((uintptr_t)prp_entry_phys);
+ 
+ 				/*
+ 				 * The next PRP Entry will be the start of the
+ 				 * first PRP List.
+ 				 */
+ 				prp_entry = prp_page;
+ 			} else {
+ 				/*
+ 				 * After this, the PRP Entries are complete.
+ 				 * This command uses 2 PRP's and no PRP list.
+ 				 */
+ 				*prp2_entry = cpu_to_le64((u64)paddr);
+ 			}
+ 		} else {
+ 			/*
+ 			 * Put entry in list and bump the addresses.
+ 			 *
+ 			 * After PRP1 and PRP2 are filled in, this will fill in
+ 			 * all remaining PRP entries in a PRP List, one per
+ 			 * each time through the loop.
+ 			 */
+ 			*prp_entry = cpu_to_le64((u64)paddr);
+ 			prp_entry++;
+ 			prp_entry_phys++;
+ 		}
+ 
+ 		/*
+ 		 * Bump the phys address of the command's data buffer by the
+ 		 * entry_len.
+ 		 */
+ 		paddr += entry_len;
+ 
+ 		/* Decrement length accounting for last partial page. */
+ 		if (entry_len > length)
+ 			length = 0;
+ 		else
+ 			length -= entry_len;
+ 	}
+ }
+ 
+ /**
+  * base_make_prp_nvme -
+  * Prepare PRPs(Physical Region Page)- SGLs specific to NVMe drives only
+  *
+  * @ioc:		per adapter object
+  * @scmd:		SCSI command from the mid-layer
+  * @mpi_request:	mpi request
+  * @smid:		msg Index
+  * @sge_count:		scatter gather element count.
+  *
+  * Returns:		true: PRPs are built
+  *			false: IEEE SGLs needs to be built
+  */
+ static void
+ base_make_prp_nvme(struct MPT3SAS_ADAPTER *ioc,
+ 		struct scsi_cmnd *scmd,
+ 		Mpi25SCSIIORequest_t *mpi_request,
+ 		u16 smid, int sge_count)
+ {
+ 	int sge_len, offset, num_prp_in_chain = 0;
+ 	Mpi25IeeeSgeChain64_t *main_chain_element, *ptr_first_sgl;
+ 	__le64 *curr_buff;
+ 	dma_addr_t msg_phys;
+ 	u64 sge_addr;
+ 	u32 page_mask, page_mask_result;
+ 	struct scatterlist *sg_scmd;
+ 	u32 first_prp_len;
+ 	int data_len = scsi_bufflen(scmd);
+ 	u32 nvme_pg_size;
+ 
+ 	nvme_pg_size = max_t(u32, ioc->page_size, NVME_PRP_PAGE_SIZE);
+ 	/*
+ 	 * Nvme has a very convoluted prp format.  One prp is required
+ 	 * for each page or partial page. Driver need to split up OS sg_list
+ 	 * entries if it is longer than one page or cross a page
+ 	 * boundary.  Driver also have to insert a PRP list pointer entry as
+ 	 * the last entry in each physical page of the PRP list.
+ 	 *
+ 	 * NOTE: The first PRP "entry" is actually placed in the first
+ 	 * SGL entry in the main message as IEEE 64 format.  The 2nd
+ 	 * entry in the main message is the chain element, and the rest
+ 	 * of the PRP entries are built in the contiguous pcie buffer.
+ 	 */
+ 	page_mask = nvme_pg_size - 1;
+ 
+ 	/*
+ 	 * Native SGL is needed.
+ 	 * Put a chain element in main message frame that points to the first
+ 	 * chain buffer.
+ 	 *
+ 	 * NOTE:  The ChainOffset field must be 0 when using a chain pointer to
+ 	 *        a native SGL.
+ 	 */
+ 
+ 	/* Set main message chain element pointer */
+ 	main_chain_element = (pMpi25IeeeSgeChain64_t)&mpi_request->SGL;
+ 	/*
+ 	 * For NVMe the chain element needs to be the 2nd SG entry in the main
+ 	 * message.
+ 	 */
+ 	main_chain_element = (Mpi25IeeeSgeChain64_t *)
+ 		((u8 *)main_chain_element + sizeof(MPI25_IEEE_SGE_CHAIN64));
+ 
+ 	/*
+ 	 * For the PRP entries, use the specially allocated buffer of
+ 	 * contiguous memory.  Normal chain buffers can't be used
+ 	 * because each chain buffer would need to be the size of an OS
+ 	 * page (4k).
+ 	 */
+ 	curr_buff = mpt3sas_base_get_pcie_sgl(ioc, smid);
+ 	msg_phys = (dma_addr_t)mpt3sas_base_get_pcie_sgl_dma(ioc, smid);
+ 
+ 	main_chain_element->Address = cpu_to_le64(msg_phys);
+ 	main_chain_element->NextChainOffset = 0;
+ 	main_chain_element->Flags = MPI2_IEEE_SGE_FLAGS_CHAIN_ELEMENT |
+ 			MPI2_IEEE_SGE_FLAGS_SYSTEM_ADDR |
+ 			MPI26_IEEE_SGE_FLAGS_NSF_NVME_PRP;
+ 
+ 	/* Build first prp, sge need not to be page aligned*/
+ 	ptr_first_sgl = (pMpi25IeeeSgeChain64_t)&mpi_request->SGL;
+ 	sg_scmd = scsi_sglist(scmd);
+ 	sge_addr = sg_dma_address(sg_scmd);
+ 	sge_len = sg_dma_len(sg_scmd);
+ 
+ 	offset = (u32)(sge_addr & page_mask);
+ 	first_prp_len = nvme_pg_size - offset;
+ 
+ 	ptr_first_sgl->Address = cpu_to_le64(sge_addr);
+ 	ptr_first_sgl->Length = cpu_to_le32(first_prp_len);
+ 
+ 	data_len -= first_prp_len;
+ 
+ 	if (sge_len > first_prp_len) {
+ 		sge_addr += first_prp_len;
+ 		sge_len -= first_prp_len;
+ 	} else if (data_len && (sge_len == first_prp_len)) {
+ 		sg_scmd = sg_next(sg_scmd);
+ 		sge_addr = sg_dma_address(sg_scmd);
+ 		sge_len = sg_dma_len(sg_scmd);
+ 	}
+ 
+ 	for (;;) {
+ 		offset = (u32)(sge_addr & page_mask);
+ 
+ 		/* Put PRP pointer due to page boundary*/
+ 		page_mask_result = (uintptr_t)(curr_buff + 1) & page_mask;
+ 		if (unlikely(!page_mask_result)) {
+ 			scmd_printk(KERN_NOTICE,
+ 				scmd, "page boundary curr_buff: 0x%p\n",
+ 				curr_buff);
+ 			msg_phys += 8;
+ 			*curr_buff = cpu_to_le64(msg_phys);
+ 			curr_buff++;
+ 			num_prp_in_chain++;
+ 		}
+ 
+ 		*curr_buff = cpu_to_le64(sge_addr);
+ 		curr_buff++;
+ 		msg_phys += 8;
+ 		num_prp_in_chain++;
+ 
+ 		sge_addr += nvme_pg_size;
+ 		sge_len -= nvme_pg_size;
+ 		data_len -= nvme_pg_size;
+ 
+ 		if (data_len <= 0)
+ 			break;
+ 
+ 		if (sge_len > 0)
+ 			continue;
+ 
+ 		sg_scmd = sg_next(sg_scmd);
+ 		sge_addr = sg_dma_address(sg_scmd);
+ 		sge_len = sg_dma_len(sg_scmd);
+ 	}
+ 
+ 	main_chain_element->Length =
+ 		cpu_to_le32(num_prp_in_chain * sizeof(u64));
+ 	return;
+ }
+ 
+ static bool
+ base_is_prp_possible(struct MPT3SAS_ADAPTER *ioc,
+ 	struct _pcie_device *pcie_device, struct scsi_cmnd *scmd, int sge_count)
+ {
+ 	u32 data_length = 0;
+ 	struct scatterlist *sg_scmd;
+ 	bool build_prp = true;
+ 
+ 	data_length = scsi_bufflen(scmd);
+ 	sg_scmd = scsi_sglist(scmd);
+ 
+ 	/* If Datalenth is <= 16K and number of SGEâ€™s entries are <= 2
+ 	 * we built IEEE SGL
+ 	 */
+ 	if ((data_length <= NVME_PRP_PAGE_SIZE*4) && (sge_count <= 2))
+ 		build_prp = false;
+ 
+ 	return build_prp;
+ }
+ 
+ /**
+  * _base_check_pcie_native_sgl - This function is called for PCIe end devices to
+  * determine if the driver needs to build a native SGL.  If so, that native
+  * SGL is built in the special contiguous buffers allocated especially for
+  * PCIe SGL creation.  If the driver will not build a native SGL, return
+  * TRUE and a normal IEEE SGL will be built.  Currently this routine
+  * supports NVMe.
+  * @ioc: per adapter object
+  * @mpi_request: mf request pointer
+  * @smid: system request message index
+  * @scmd: scsi command
+  * @pcie_device: points to the PCIe device's info
+  *
+  * Returns 0 if native SGL was built, 1 if no SGL was built
+  */
+ static int
+ _base_check_pcie_native_sgl(struct MPT3SAS_ADAPTER *ioc,
+ 	Mpi25SCSIIORequest_t *mpi_request, u16 smid, struct scsi_cmnd *scmd,
+ 	struct _pcie_device *pcie_device)
+ {
+ 	struct scatterlist *sg_scmd;
+ 	int sges_left;
+ 
+ 	/* Get the SG list pointer and info. */
+ 	sg_scmd = scsi_sglist(scmd);
+ 	sges_left = scsi_dma_map(scmd);
+ 	if (sges_left < 0) {
+ 		sdev_printk(KERN_ERR, scmd->device,
+ 			"scsi_dma_map failed: request for %d bytes!\n",
+ 			scsi_bufflen(scmd));
+ 		return 1;
+ 	}
+ 
+ 	/* Check if we need to build a native SG list. */
+ 	if (base_is_prp_possible(ioc, pcie_device,
+ 				scmd, sges_left) == 0) {
+ 		/* We built a native SG list, just return. */
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * Build native NVMe PRP.
+ 	 */
+ 	base_make_prp_nvme(ioc, scmd, mpi_request,
+ 			smid, sges_left);
+ 
+ 	return 0;
+ out:
+ 	scsi_dma_unmap(scmd);
+ 	return 1;
+ }
+ 
+ /**
++>>>>>>> 494f401bcd07 (scsi: mpt3sas: Fix sparse warnings)
   * _base_add_sg_single_ieee - add sg element for IEEE format
   * @paddr: virtual address for SGE
   * @flags: SGE flags
* Unmerged path drivers/scsi/mpt3sas/mpt3sas_base.c
diff --git a/drivers/scsi/mpt3sas/mpt3sas_scsih.c b/drivers/scsi/mpt3sas/mpt3sas_scsih.c
index df8b52858086..bac7063a9e93 100644
--- a/drivers/scsi/mpt3sas/mpt3sas_scsih.c
+++ b/drivers/scsi/mpt3sas/mpt3sas_scsih.c
@@ -596,7 +596,7 @@ __mpt3sas_get_pdev_from_target(struct MPT3SAS_ADAPTER *ioc,
  *
  * This searches for pcie_device from target, then return pcie_device object.
  */
-struct _pcie_device *
+static struct _pcie_device *
 mpt3sas_get_pdev_from_target(struct MPT3SAS_ADAPTER *ioc,
 	struct MPT3SAS_TARGET *tgt_priv)
 {
@@ -939,7 +939,7 @@ _scsih_sas_device_init_add(struct MPT3SAS_ADAPTER *ioc,
 }
 
 
-struct _pcie_device *
+static struct _pcie_device *
 __mpt3sas_get_pdev_by_wwid(struct MPT3SAS_ADAPTER *ioc, u64 wwid)
 {
 	struct _pcie_device *pcie_device;
@@ -972,7 +972,7 @@ found_device:
  *
  * This searches for pcie_device based on wwid, then return pcie_device object.
  */
-struct _pcie_device *
+static struct _pcie_device *
 mpt3sas_get_pdev_by_wwid(struct MPT3SAS_ADAPTER *ioc, u64 wwid)
 {
 	struct _pcie_device *pcie_device;
@@ -986,7 +986,7 @@ mpt3sas_get_pdev_by_wwid(struct MPT3SAS_ADAPTER *ioc, u64 wwid)
 }
 
 
-struct _pcie_device *
+static struct _pcie_device *
 __mpt3sas_get_pdev_by_idchannel(struct MPT3SAS_ADAPTER *ioc, int id,
 	int channel)
 {
@@ -1009,34 +1009,7 @@ found_device:
 	return pcie_device;
 }
 
-
-/**
- * mpt3sas_get_pdev_by_idchannel - pcie device search
- * @ioc: per adapter object
- * @id: Target ID
- * @channel: Channel ID
- *
- * Context: This function will acquire ioc->pcie_device_lock and will release
- * before returning the pcie_device object.
- *
- * This searches for pcie_device based on id and channel, then return
- * pcie_device object.
- */
-struct _pcie_device *
-mpt3sas_get_pdev_by_idchannel(struct MPT3SAS_ADAPTER *ioc, int id, int channel)
-{
-	struct _pcie_device *pcie_device;
-	unsigned long flags;
-
-	spin_lock_irqsave(&ioc->pcie_device_lock, flags);
-	pcie_device = __mpt3sas_get_pdev_by_idchannel(ioc, id, channel);
-	spin_unlock_irqrestore(&ioc->pcie_device_lock, flags);
-
-	return pcie_device;
-}
-
-
-struct _pcie_device *
+static struct _pcie_device *
 __mpt3sas_get_pdev_by_handle(struct MPT3SAS_ADAPTER *ioc, u16 handle)
 {
 	struct _pcie_device *pcie_device;
