x86/bpf: Clean up non-standard comments, to make the code more readable

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [x86] bpf: Clean up non-standard comments, to make the code more readable (Jiri Olsa) [1640092]
Rebuild_FUZZ: 97.10%
commit-author Ingo Molnar <mingo@kernel.org>
commit 5f26c50143f58f256535bee8d93a105f36d4d2da
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/5f26c501.failed

So by chance I looked into x86 assembly in arch/x86/net/bpf_jit_comp.c and
noticed the weird and inconsistent comment style it mistakenly learned from
the networking code:

 /* Multi-line comment ...
  * ... looks like this.
  */

Fix this to use the standard comment style specified in Documentation/CodingStyle
and used in arch/x86/ as well:

 /*
  * Multi-line comment ...
  * ... looks like this.
  */

Also, to quote Linus's ... more explicit views about this:

  http://article.gmane.org/gmane.linux.kernel.cryptoapi/21066

  > But no, the networking code picked *none* of the above sane formats.
  > Instead, it picked these two models that are just half-arsed
  > shit-for-brains:
  >
  >  (no)
  >      /* This is disgusting drug-induced
  >        * crap, and should die
  >        */
  >
  >   (no-no-no)
  >       /* This is also very nasty
  >        * and visually unbalanced */
  >
  > Please. The networking code actually has the *worst* possible comment
  > style. You can literally find that (no-no-no) style, which is just
  > really horribly disgusting and worse than the otherwise fairly similar
  > (d) in pretty much every way.

Also improve the comments and some other details while at it:

 - Don't mix same-line and previous-line comment style on otherwise
   identical code patterns within the same function,

 - capitalize 'BPF' and x86 register names consistently,

 - capitalize sentences consistently,

 - instead of 'x64' use 'x86-64': x64 is a Microsoft specific term,

 - use more consistent punctuation,

 - use standard coding style in macros as well,

 - fix typos and a few other minor details.

Consistent coding style is not optional, at least in arch/x86/.

No change in functionality.

( In case this commit causes conflicts with pending development code
  I'll be glad to help resolve any conflicts! )

	Acked-by: Thomas Gleixner <tglx@linutronix.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: David S. Miller <davem@davemloft.net>
	Cc: Eric Dumazet <edumazet@google.com>
	Cc: Daniel Borkmann <daniel@iogearbox.net>
	Cc: Alexei Starovoitov <ast@fb.com>
	Cc: Hideaki YOSHIFUJI <yoshfuji@linux-ipv6.org>
	Cc: netdev@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit 5f26c50143f58f256535bee8d93a105f36d4d2da)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
diff --cc arch/x86/net/bpf_jit_comp.c
index 9f43ac6fc966,ac4df93d4105..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -1,6 -1,8 +1,7 @@@
- /* bpf_jit_comp.c : BPF JIT compiler
+ /*
+  * bpf_jit_comp.c: BPF JIT compiler
   *
   * Copyright (C) 2011-2013 Eric Dumazet (eric.dumazet@gmail.com)
 - * Internal BPF Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com
   *
   * This program is free software; you can redistribute it and/or
   * modify it under the terms of the GNU General Public License
@@@ -12,30 -12,21 +13,30 @@@
  #include <linux/netdevice.h>
  #include <linux/filter.h>
  #include <linux/if_vlan.h>
 -#include <linux/bpf.h>
  
 -#include <asm/set_memory.h>
 -#include <asm/nospec-branch.h>
 +/*
 + * Conventions :
 + *  EAX : BPF A accumulator
 + *  EBX : BPF X accumulator
 + *  RDI : pointer to skb   (first argument given to JIT function)
 + *  RBP : frame pointer (even if CONFIG_FRAME_POINTER=n)
 + *  ECX,EDX,ESI : scratch registers
 + *  r9d : skb->len - skb->data_len (headlen)
 + *  r8  : skb->data
 + * -8(RBP) : saved RBX value
 + * -16(RBP)..-80(RBP) : BPF_MEMWORDS values
 + */
  
  /*
-  * assembly code in arch/x86/net/bpf_jit.S
+  * Assembly code in arch/x86/net/bpf_jit.S
   */
 -extern u8 sk_load_word[], sk_load_half[], sk_load_byte[];
 +extern u8 sk_load_word[], sk_load_half[], sk_load_byte[], sk_load_byte_msh[];
  extern u8 sk_load_word_positive_offset[], sk_load_half_positive_offset[];
 -extern u8 sk_load_byte_positive_offset[];
 +extern u8 sk_load_byte_positive_offset[], sk_load_byte_msh_positive_offset[];
  extern u8 sk_load_word_negative_offset[], sk_load_half_negative_offset[];
 -extern u8 sk_load_byte_negative_offset[];
 +extern u8 sk_load_byte_negative_offset[], sk_load_byte_msh_negative_offset[];
  
 -static u8 *emit_code(u8 *ptr, u32 bytes, unsigned int len)
 +static inline u8 *emit_code(u8 *ptr, u32 bytes, unsigned int len)
  {
  	if (len == 1)
  		*ptr = bytes;
@@@ -54,32 -46,54 +55,73 @@@
  #define EMIT2(b1, b2)		EMIT((b1) + ((b2) << 8), 2)
  #define EMIT3(b1, b2, b3)	EMIT((b1) + ((b2) << 8) + ((b3) << 16), 3)
  #define EMIT4(b1, b2, b3, b4)   EMIT((b1) + ((b2) << 8) + ((b3) << 16) + ((b4) << 24), 4)
++<<<<<<< HEAD
 +#define EMIT1_off32(b1, off)	do { EMIT1(b1); EMIT(off, 4);} while (0)
++=======
+ 
+ #define EMIT1_off32(b1, off) \
+ 	do { EMIT1(b1); EMIT(off, 4); } while (0)
+ #define EMIT2_off32(b1, b2, off) \
+ 	do { EMIT2(b1, b2); EMIT(off, 4); } while (0)
+ #define EMIT3_off32(b1, b2, b3, off) \
+ 	do { EMIT3(b1, b2, b3); EMIT(off, 4); } while (0)
+ #define EMIT4_off32(b1, b2, b3, b4, off) \
+ 	do { EMIT4(b1, b2, b3, b4); EMIT(off, 4); } while (0)
++>>>>>>> 5f26c50143f5 (x86/bpf: Clean up non-standard comments, to make the code more readable)
 +
 +#define CLEAR_A() EMIT2(0x31, 0xc0) /* xor %eax,%eax */
 +#define CLEAR_X() EMIT2(0x31, 0xdb) /* xor %ebx,%ebx */
  
 -static bool is_imm8(int value)
 +static inline bool is_imm8(int value)
  {
  	return value <= 127 && value >= -128;
  }
  
 -static bool is_simm32(s64 value)
 +static inline bool is_near(int offset)
  {
 -	return value == (s64)(s32)value;
 +	return offset <= 127 && offset >= -128;
  }
  
++<<<<<<< HEAD
 +#define EMIT_JMP(offset)						\
 +do {									\
 +	if (offset) {							\
 +		if (is_near(offset))					\
 +			EMIT2(0xeb, offset); /* jmp .+off8 */		\
 +		else							\
 +			EMIT1_off32(0xe9, offset); /* jmp .+off32 */	\
 +	}								\
 +} while (0)
++=======
+ static bool is_uimm32(u64 value)
+ {
+ 	return value == (u64)(u32)value;
+ }
+ 
+ /* mov dst, src */
+ #define EMIT_mov(DST, SRC)								 \
+ 	do {										 \
+ 		if (DST != SRC)								 \
+ 			EMIT3(add_2mod(0x48, DST, SRC), 0x89, add_2reg(0xC0, DST, SRC)); \
+ 	} while (0)
+ 
+ static int bpf_size_to_x86_bytes(int bpf_size)
+ {
+ 	if (bpf_size == BPF_W)
+ 		return 4;
+ 	else if (bpf_size == BPF_H)
+ 		return 2;
+ 	else if (bpf_size == BPF_B)
+ 		return 1;
+ 	else if (bpf_size == BPF_DW)
+ 		return 4; /* imm32 */
+ 	else
+ 		return 0;
+ }
++>>>>>>> 5f26c50143f5 (x86/bpf: Clean up non-standard comments, to make the code more readable)
  
- /* list of x86 cond jumps opcodes (. + s8)
+ /*
+  * List of x86 cond jumps opcodes (. + s8)
   * Add 0x10 (and an extra 0x0f) to generate far jumps (. + s32)
   */
  #define X86_JB  0x72
@@@ -123,607 -110,1168 +165,1727 @@@ static inline void bpf_flush_icache(voi
  #define CHOOSE_LOAD_FUNC(K, func) \
  	((int)K < 0 ? ((int)K >= SKF_LL_OFF ? func##_negative_offset : func) : func##_positive_offset)
  
++<<<<<<< HEAD
 +/* Helper to find the offset of pkt_type in sk_buff
 + * We want to make sure its still a 3bit field starting at a byte boundary.
 + */
 +#define PKT_TYPE_MAX 7
 +static int pkt_type_offset(void)
++=======
+ /* Pick a register outside of BPF range for JIT internal work */
+ #define AUX_REG (MAX_BPF_JIT_REG + 1)
+ 
+ /*
+  * The following table maps BPF registers to x86-64 registers.
+  *
+  * x86-64 register R12 is unused, since if used as base address
+  * register in load/store instructions, it always needs an
+  * extra byte of encoding and is callee saved.
+  *
+  * R9  caches skb->len - skb->data_len
+  * R10 caches skb->data, and used for blinding (if enabled)
+  */
+ static const int reg2hex[] = {
+ 	[BPF_REG_0] = 0,  /* RAX */
+ 	[BPF_REG_1] = 7,  /* RDI */
+ 	[BPF_REG_2] = 6,  /* RSI */
+ 	[BPF_REG_3] = 2,  /* RDX */
+ 	[BPF_REG_4] = 1,  /* RCX */
+ 	[BPF_REG_5] = 0,  /* R8  */
+ 	[BPF_REG_6] = 3,  /* RBX callee saved */
+ 	[BPF_REG_7] = 5,  /* R13 callee saved */
+ 	[BPF_REG_8] = 6,  /* R14 callee saved */
+ 	[BPF_REG_9] = 7,  /* R15 callee saved */
+ 	[BPF_REG_FP] = 5, /* RBP readonly */
+ 	[BPF_REG_AX] = 2, /* R10 temp register */
+ 	[AUX_REG] = 3,    /* R11 temp register */
+ };
+ 
+ /*
+  * is_ereg() == true if BPF register 'reg' maps to x86-64 r8..r15
+  * which need extra byte of encoding.
+  * rax,rcx,...,rbp have simpler encoding
+  */
+ static bool is_ereg(u32 reg)
++>>>>>>> 5f26c50143f5 (x86/bpf: Clean up non-standard comments, to make the code more readable)
  {
 -	return (1 << reg) & (BIT(BPF_REG_5) |
 -			     BIT(AUX_REG) |
 -			     BIT(BPF_REG_7) |
 -			     BIT(BPF_REG_8) |
 -			     BIT(BPF_REG_9) |
 -			     BIT(BPF_REG_AX));
 -}
 -
 +	struct sk_buff skb_probe = {
 +		.pkt_type = ~0,
 +	};
 +	char *ct = (char *)&skb_probe;
 +	unsigned int off;
 +
++<<<<<<< HEAD
 +	for (off = 0; off < sizeof(struct sk_buff); off++) {
 +		if (ct[off] == PKT_TYPE_MAX)
 +			return off;
++=======
+ static bool is_axreg(u32 reg)
+ {
+ 	return reg == BPF_REG_0;
+ }
+ 
+ /* Add modifiers if 'reg' maps to x86-64 registers R8..R15 */
+ static u8 add_1mod(u8 byte, u32 reg)
+ {
+ 	if (is_ereg(reg))
+ 		byte |= 1;
+ 	return byte;
+ }
+ 
+ static u8 add_2mod(u8 byte, u32 r1, u32 r2)
+ {
+ 	if (is_ereg(r1))
+ 		byte |= 1;
+ 	if (is_ereg(r2))
+ 		byte |= 4;
+ 	return byte;
+ }
+ 
+ /* Encode 'dst_reg' register into x86-64 opcode 'byte' */
+ static u8 add_1reg(u8 byte, u32 dst_reg)
+ {
+ 	return byte + reg2hex[dst_reg];
+ }
+ 
+ /* Encode 'dst_reg' and 'src_reg' registers into x86-64 opcode 'byte' */
+ static u8 add_2reg(u8 byte, u32 dst_reg, u32 src_reg)
+ {
+ 	return byte + reg2hex[dst_reg] + (reg2hex[src_reg] << 3);
+ }
+ 
+ static void jit_fill_hole(void *area, unsigned int size)
+ {
+ 	/* Fill whole space with INT3 instructions */
+ 	memset(area, 0xcc, size);
+ }
+ 
+ struct jit_context {
+ 	int cleanup_addr; /* Epilogue code offset */
+ 	bool seen_ld_abs;
+ 	bool seen_ax_reg;
+ };
+ 
+ /* Maximum number of bytes emitted while JITing one eBPF insn */
+ #define BPF_MAX_INSN_SIZE	128
+ #define BPF_INSN_SAFETY		64
+ 
+ #define AUX_STACK_SPACE \
+ 	(32 /* Space for RBX, R13, R14, R15 */ + \
+ 	  8 /* Space for skb_copy_bits() buffer */)
+ 
+ #define PROLOGUE_SIZE 37
+ 
+ /*
+  * Emit x86-64 prologue code for BPF program and check its size.
+  * bpf_tail_call helper will skip it while jumping into another program
+  */
+ static void emit_prologue(u8 **pprog, u32 stack_depth, bool ebpf_from_cbpf)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	/* push rbp */
+ 	EMIT1(0x55);
+ 
+ 	/* mov rbp,rsp */
+ 	EMIT3(0x48, 0x89, 0xE5);
+ 
+ 	/* sub rsp, rounded_stack_depth + AUX_STACK_SPACE */
+ 	EMIT3_off32(0x48, 0x81, 0xEC,
+ 		    round_up(stack_depth, 8) + AUX_STACK_SPACE);
+ 
+ 	/* sub rbp, AUX_STACK_SPACE */
+ 	EMIT4(0x48, 0x83, 0xED, AUX_STACK_SPACE);
+ 
+ 	/* All classic BPF filters use R6(rbx) save it */
+ 
+ 	/* mov qword ptr [rbp+0],rbx */
+ 	EMIT4(0x48, 0x89, 0x5D, 0);
+ 
+ 	/*
+ 	 * bpf_convert_filter() maps classic BPF register X to R7 and uses R8
+ 	 * as temporary, so all tcpdump filters need to spill/fill R7(R13) and
+ 	 * R8(R14). R9(R15) spill could be made conditional, but there is only
+ 	 * one 'bpf_error' return path out of helper functions inside bpf_jit.S
+ 	 * The overhead of extra spill is negligible for any filter other
+ 	 * than synthetic ones. Therefore not worth adding complexity.
+ 	 */
+ 
+ 	/* mov qword ptr [rbp+8],r13 */
+ 	EMIT4(0x4C, 0x89, 0x6D, 8);
+ 	/* mov qword ptr [rbp+16],r14 */
+ 	EMIT4(0x4C, 0x89, 0x75, 16);
+ 	/* mov qword ptr [rbp+24],r15 */
+ 	EMIT4(0x4C, 0x89, 0x7D, 24);
+ 
+ 	if (!ebpf_from_cbpf) {
+ 		/*
+ 		 * Clear the tail call counter (tail_call_cnt): for eBPF tail
+ 		 * calls we need to reset the counter to 0. It's done in two
+ 		 * instructions, resetting RAX register to 0, and moving it
+ 		 * to the counter location.
+ 		 */
+ 
+ 		/* xor eax, eax */
+ 		EMIT2(0x31, 0xc0);
+ 		/* mov qword ptr [rbp+32], rax */
+ 		EMIT4(0x48, 0x89, 0x45, 32);
+ 
+ 		BUILD_BUG_ON(cnt != PROLOGUE_SIZE);
++>>>>>>> 5f26c50143f5 (x86/bpf: Clean up non-standard comments, to make the code more readable)
  	}
 -
 -	*pprog = prog;
 +	pr_err_once("Please fix pkt_type_offset(), as pkt_type couldn't be found\n");
 +	return -1;
  }
  
++<<<<<<< HEAD
 +void bpf_jit_compile(struct sk_filter *fp)
 +{
 +	u8 temp[64];
 +	u8 *prog;
 +	unsigned int proglen, oldproglen = 0;
 +	int ilen, i;
 +	int t_offset, f_offset;
 +	u8 t_op, f_op, seen = 0, pass;
++=======
+ /*
+  * Generate the following code:
+  *
+  * ... bpf_tail_call(void *ctx, struct bpf_array *array, u64 index) ...
+  *   if (index >= array->map.max_entries)
+  *     goto out;
+  *   if (++tail_call_cnt > MAX_TAIL_CALL_CNT)
+  *     goto out;
+  *   prog = array->ptrs[index];
+  *   if (prog == NULL)
+  *     goto out;
+  *   goto *(prog->bpf_func + prologue_size);
+  * out:
+  */
+ static void emit_bpf_tail_call(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int label1, label2, label3;
+ 	int cnt = 0;
+ 
+ 	/*
+ 	 * rdi - pointer to ctx
+ 	 * rsi - pointer to bpf_array
+ 	 * rdx - index in bpf_array
+ 	 */
+ 
+ 	/*
+ 	 * if (index >= array->map.max_entries)
+ 	 *	goto out;
+ 	 */
+ 	EMIT2(0x89, 0xD2);                        /* mov edx, edx */
+ 	EMIT3(0x39, 0x56,                         /* cmp dword ptr [rsi + 16], edx */
+ 	      offsetof(struct bpf_array, map.max_entries));
+ #define OFFSET1 (41 + RETPOLINE_RAX_BPF_JIT_SIZE) /* Number of bytes to jump */
+ 	EMIT2(X86_JBE, OFFSET1);                  /* jbe out */
+ 	label1 = cnt;
+ 
+ 	/*
+ 	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)
+ 	 *	goto out;
+ 	 */
+ 	EMIT2_off32(0x8B, 0x85, 36);              /* mov eax, dword ptr [rbp + 36] */
+ 	EMIT3(0x83, 0xF8, MAX_TAIL_CALL_CNT);     /* cmp eax, MAX_TAIL_CALL_CNT */
+ #define OFFSET2 (30 + RETPOLINE_RAX_BPF_JIT_SIZE)
+ 	EMIT2(X86_JA, OFFSET2);                   /* ja out */
+ 	label2 = cnt;
+ 	EMIT3(0x83, 0xC0, 0x01);                  /* add eax, 1 */
+ 	EMIT2_off32(0x89, 0x85, 36);              /* mov dword ptr [rbp + 36], eax */
+ 
+ 	/* prog = array->ptrs[index]; */
+ 	EMIT4_off32(0x48, 0x8B, 0x84, 0xD6,       /* mov rax, [rsi + rdx * 8 + offsetof(...)] */
+ 		    offsetof(struct bpf_array, ptrs));
+ 
+ 	/*
+ 	 * if (prog == NULL)
+ 	 *	goto out;
+ 	 */
+ 	EMIT3(0x48, 0x85, 0xC0);		  /* test rax,rax */
+ #define OFFSET3 (8 + RETPOLINE_RAX_BPF_JIT_SIZE)
+ 	EMIT2(X86_JE, OFFSET3);                   /* je out */
+ 	label3 = cnt;
+ 
+ 	/* goto *(prog->bpf_func + prologue_size); */
+ 	EMIT4(0x48, 0x8B, 0x40,                   /* mov rax, qword ptr [rax + 32] */
+ 	      offsetof(struct bpf_prog, bpf_func));
+ 	EMIT4(0x48, 0x83, 0xC0, PROLOGUE_SIZE);   /* add rax, prologue_size */
+ 
+ 	/*
+ 	 * Wow we're ready to jump into next BPF program
+ 	 * rdi == ctx (1st arg)
+ 	 * rax == prog->bpf_func + prologue_size
+ 	 */
+ 	RETPOLINE_RAX_BPF_JIT();
+ 
+ 	/* out: */
+ 	BUILD_BUG_ON(cnt - label1 != OFFSET1);
+ 	BUILD_BUG_ON(cnt - label2 != OFFSET2);
+ 	BUILD_BUG_ON(cnt - label3 != OFFSET3);
+ 	*pprog = prog;
+ }
+ 
+ 
+ static void emit_load_skb_data_hlen(u8 **pprog)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	/*
+ 	 * r9d = skb->len - skb->data_len (headlen)
+ 	 * r10 = skb->data
+ 	 */
+ 	/* mov %r9d, off32(%rdi) */
+ 	EMIT3_off32(0x44, 0x8b, 0x8f, offsetof(struct sk_buff, len));
+ 
+ 	/* sub %r9d, off32(%rdi) */
+ 	EMIT3_off32(0x44, 0x2b, 0x8f, offsetof(struct sk_buff, data_len));
+ 
+ 	/* mov %r10, off32(%rdi) */
+ 	EMIT3_off32(0x4c, 0x8b, 0x97, offsetof(struct sk_buff, data));
+ 	*pprog = prog;
+ }
+ 
+ static void emit_mov_imm32(u8 **pprog, bool sign_propagate,
+ 			   u32 dst_reg, const u32 imm32)
+ {
+ 	u8 *prog = *pprog;
+ 	u8 b1, b2, b3;
+ 	int cnt = 0;
+ 
+ 	/*
+ 	 * Optimization: if imm32 is positive, use 'mov %eax, imm32'
+ 	 * (which zero-extends imm32) to save 2 bytes.
+ 	 */
+ 	if (sign_propagate && (s32)imm32 < 0) {
+ 		/* 'mov %rax, imm32' sign extends imm32 */
+ 		b1 = add_1mod(0x48, dst_reg);
+ 		b2 = 0xC7;
+ 		b3 = 0xC0;
+ 		EMIT3_off32(b1, b2, add_1reg(b3, dst_reg), imm32);
+ 		goto done;
+ 	}
+ 
+ 	/*
+ 	 * Optimization: if imm32 is zero, use 'xor %eax, %eax'
+ 	 * to save 3 bytes.
+ 	 */
+ 	if (imm32 == 0) {
+ 		if (is_ereg(dst_reg))
+ 			EMIT1(add_2mod(0x40, dst_reg, dst_reg));
+ 		b2 = 0x31; /* xor */
+ 		b3 = 0xC0;
+ 		EMIT2(b2, add_2reg(b3, dst_reg, dst_reg));
+ 		goto done;
+ 	}
+ 
+ 	/* mov %eax, imm32 */
+ 	if (is_ereg(dst_reg))
+ 		EMIT1(add_1mod(0x40, dst_reg));
+ 	EMIT1_off32(add_1reg(0xB8, dst_reg), imm32);
+ done:
+ 	*pprog = prog;
+ }
+ 
+ static void emit_mov_imm64(u8 **pprog, u32 dst_reg,
+ 			   const u32 imm32_hi, const u32 imm32_lo)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	if (is_uimm32(((u64)imm32_hi << 32) | (u32)imm32_lo)) {
+ 		/*
+ 		 * For emitting plain u32, where sign bit must not be
+ 		 * propagated LLVM tends to load imm64 over mov32
+ 		 * directly, so save couple of bytes by just doing
+ 		 * 'mov %eax, imm32' instead.
+ 		 */
+ 		emit_mov_imm32(&prog, false, dst_reg, imm32_lo);
+ 	} else {
+ 		/* movabsq %rax, imm64 */
+ 		EMIT2(add_1mod(0x48, dst_reg), add_1reg(0xB8, dst_reg));
+ 		EMIT(imm32_lo, 4);
+ 		EMIT(imm32_hi, 4);
+ 	}
+ 
+ 	*pprog = prog;
+ }
+ 
+ static void emit_mov_reg(u8 **pprog, bool is64, u32 dst_reg, u32 src_reg)
+ {
+ 	u8 *prog = *pprog;
+ 	int cnt = 0;
+ 
+ 	if (is64) {
+ 		/* mov dst, src */
+ 		EMIT_mov(dst_reg, src_reg);
+ 	} else {
+ 		/* mov32 dst, src */
+ 		if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 			EMIT1(add_2mod(0x40, dst_reg, src_reg));
+ 		EMIT2(0x89, add_2reg(0xC0, dst_reg, src_reg));
+ 	}
+ 
+ 	*pprog = prog;
+ }
+ 
+ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,
+ 		  int oldproglen, struct jit_context *ctx)
+ {
+ 	struct bpf_insn *insn = bpf_prog->insnsi;
+ 	int insn_cnt = bpf_prog->len;
+ 	bool seen_ld_abs = ctx->seen_ld_abs | (oldproglen == 0);
+ 	bool seen_ax_reg = ctx->seen_ax_reg | (oldproglen == 0);
+ 	bool seen_exit = false;
+ 	u8 temp[BPF_MAX_INSN_SIZE + BPF_INSN_SAFETY];
+ 	int i, cnt = 0;
+ 	int proglen = 0;
+ 	u8 *prog = temp;
+ 
+ 	emit_prologue(&prog, bpf_prog->aux->stack_depth,
+ 		      bpf_prog_was_classic(bpf_prog));
+ 
+ 	if (seen_ld_abs)
+ 		emit_load_skb_data_hlen(&prog);
+ 
+ 	for (i = 0; i < insn_cnt; i++, insn++) {
+ 		const s32 imm32 = insn->imm;
+ 		u32 dst_reg = insn->dst_reg;
+ 		u32 src_reg = insn->src_reg;
+ 		u8 b2 = 0, b3 = 0;
+ 		s64 jmp_offset;
+ 		u8 jmp_cond;
+ 		bool reload_skb_data;
+ 		int ilen;
+ 		u8 *func;
+ 
+ 		if (dst_reg == BPF_REG_AX || src_reg == BPF_REG_AX)
+ 			ctx->seen_ax_reg = seen_ax_reg = true;
+ 
+ 		switch (insn->code) {
+ 			/* ALU */
+ 		case BPF_ALU | BPF_ADD | BPF_X:
+ 		case BPF_ALU | BPF_SUB | BPF_X:
+ 		case BPF_ALU | BPF_AND | BPF_X:
+ 		case BPF_ALU | BPF_OR | BPF_X:
+ 		case BPF_ALU | BPF_XOR | BPF_X:
+ 		case BPF_ALU64 | BPF_ADD | BPF_X:
+ 		case BPF_ALU64 | BPF_SUB | BPF_X:
+ 		case BPF_ALU64 | BPF_AND | BPF_X:
+ 		case BPF_ALU64 | BPF_OR | BPF_X:
+ 		case BPF_ALU64 | BPF_XOR | BPF_X:
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_ADD: b2 = 0x01; break;
+ 			case BPF_SUB: b2 = 0x29; break;
+ 			case BPF_AND: b2 = 0x21; break;
+ 			case BPF_OR: b2 = 0x09; break;
+ 			case BPF_XOR: b2 = 0x31; break;
+ 			}
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_2mod(0x48, dst_reg, src_reg));
+ 			else if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT1(add_2mod(0x40, dst_reg, src_reg));
+ 			EMIT2(b2, add_2reg(0xC0, dst_reg, src_reg));
+ 			break;
+ 
+ 		case BPF_ALU64 | BPF_MOV | BPF_X:
+ 		case BPF_ALU | BPF_MOV | BPF_X:
+ 			emit_mov_reg(&prog,
+ 				     BPF_CLASS(insn->code) == BPF_ALU64,
+ 				     dst_reg, src_reg);
+ 			break;
+ 
+ 			/* neg dst */
+ 		case BPF_ALU | BPF_NEG:
+ 		case BPF_ALU64 | BPF_NEG:
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 			EMIT2(0xF7, add_1reg(0xD8, dst_reg));
+ 			break;
+ 
+ 		case BPF_ALU | BPF_ADD | BPF_K:
+ 		case BPF_ALU | BPF_SUB | BPF_K:
+ 		case BPF_ALU | BPF_AND | BPF_K:
+ 		case BPF_ALU | BPF_OR | BPF_K:
+ 		case BPF_ALU | BPF_XOR | BPF_K:
+ 		case BPF_ALU64 | BPF_ADD | BPF_K:
+ 		case BPF_ALU64 | BPF_SUB | BPF_K:
+ 		case BPF_ALU64 | BPF_AND | BPF_K:
+ 		case BPF_ALU64 | BPF_OR | BPF_K:
+ 		case BPF_ALU64 | BPF_XOR | BPF_K:
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 
+ 			/*
+ 			 * b3 holds 'normal' opcode, b2 short form only valid
+ 			 * in case dst is eax/rax.
+ 			 */
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_ADD:
+ 				b3 = 0xC0;
+ 				b2 = 0x05;
+ 				break;
+ 			case BPF_SUB:
+ 				b3 = 0xE8;
+ 				b2 = 0x2D;
+ 				break;
+ 			case BPF_AND:
+ 				b3 = 0xE0;
+ 				b2 = 0x25;
+ 				break;
+ 			case BPF_OR:
+ 				b3 = 0xC8;
+ 				b2 = 0x0D;
+ 				break;
+ 			case BPF_XOR:
+ 				b3 = 0xF0;
+ 				b2 = 0x35;
+ 				break;
+ 			}
+ 
+ 			if (is_imm8(imm32))
+ 				EMIT3(0x83, add_1reg(b3, dst_reg), imm32);
+ 			else if (is_axreg(dst_reg))
+ 				EMIT1_off32(b2, imm32);
+ 			else
+ 				EMIT2_off32(0x81, add_1reg(b3, dst_reg), imm32);
+ 			break;
+ 
+ 		case BPF_ALU64 | BPF_MOV | BPF_K:
+ 		case BPF_ALU | BPF_MOV | BPF_K:
+ 			emit_mov_imm32(&prog, BPF_CLASS(insn->code) == BPF_ALU64,
+ 				       dst_reg, imm32);
+ 			break;
+ 
+ 		case BPF_LD | BPF_IMM | BPF_DW:
+ 			emit_mov_imm64(&prog, dst_reg, insn[1].imm, insn[0].imm);
+ 			insn++;
+ 			i++;
+ 			break;
+ 
+ 			/* dst %= src, dst /= src, dst %= imm32, dst /= imm32 */
+ 		case BPF_ALU | BPF_MOD | BPF_X:
+ 		case BPF_ALU | BPF_DIV | BPF_X:
+ 		case BPF_ALU | BPF_MOD | BPF_K:
+ 		case BPF_ALU | BPF_DIV | BPF_K:
+ 		case BPF_ALU64 | BPF_MOD | BPF_X:
+ 		case BPF_ALU64 | BPF_DIV | BPF_X:
+ 		case BPF_ALU64 | BPF_MOD | BPF_K:
+ 		case BPF_ALU64 | BPF_DIV | BPF_K:
+ 			EMIT1(0x50); /* push rax */
+ 			EMIT1(0x52); /* push rdx */
+ 
+ 			if (BPF_SRC(insn->code) == BPF_X)
+ 				/* mov r11, src_reg */
+ 				EMIT_mov(AUX_REG, src_reg);
+ 			else
+ 				/* mov r11, imm32 */
+ 				EMIT3_off32(0x49, 0xC7, 0xC3, imm32);
+ 
+ 			/* mov rax, dst_reg */
+ 			EMIT_mov(BPF_REG_0, dst_reg);
+ 
+ 			/*
+ 			 * xor edx, edx
+ 			 * equivalent to 'xor rdx, rdx', but one byte less
+ 			 */
+ 			EMIT2(0x31, 0xd2);
+ 
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				/* div r11 */
+ 				EMIT3(0x49, 0xF7, 0xF3);
+ 			else
+ 				/* div r11d */
+ 				EMIT3(0x41, 0xF7, 0xF3);
+ 
+ 			if (BPF_OP(insn->code) == BPF_MOD)
+ 				/* mov r11, rdx */
+ 				EMIT3(0x49, 0x89, 0xD3);
+ 			else
+ 				/* mov r11, rax */
+ 				EMIT3(0x49, 0x89, 0xC3);
+ 
+ 			EMIT1(0x5A); /* pop rdx */
+ 			EMIT1(0x58); /* pop rax */
+ 
+ 			/* mov dst_reg, r11 */
+ 			EMIT_mov(dst_reg, AUX_REG);
+ 			break;
+ 
+ 		case BPF_ALU | BPF_MUL | BPF_K:
+ 		case BPF_ALU | BPF_MUL | BPF_X:
+ 		case BPF_ALU64 | BPF_MUL | BPF_K:
+ 		case BPF_ALU64 | BPF_MUL | BPF_X:
+ 		{
+ 			bool is64 = BPF_CLASS(insn->code) == BPF_ALU64;
+ 
+ 			if (dst_reg != BPF_REG_0)
+ 				EMIT1(0x50); /* push rax */
+ 			if (dst_reg != BPF_REG_3)
+ 				EMIT1(0x52); /* push rdx */
+ 
+ 			/* mov r11, dst_reg */
+ 			EMIT_mov(AUX_REG, dst_reg);
+ 
+ 			if (BPF_SRC(insn->code) == BPF_X)
+ 				emit_mov_reg(&prog, is64, BPF_REG_0, src_reg);
+ 			else
+ 				emit_mov_imm32(&prog, is64, BPF_REG_0, imm32);
+ 
+ 			if (is64)
+ 				EMIT1(add_1mod(0x48, AUX_REG));
+ 			else if (is_ereg(AUX_REG))
+ 				EMIT1(add_1mod(0x40, AUX_REG));
+ 			/* mul(q) r11 */
+ 			EMIT2(0xF7, add_1reg(0xE0, AUX_REG));
+ 
+ 			if (dst_reg != BPF_REG_3)
+ 				EMIT1(0x5A); /* pop rdx */
+ 			if (dst_reg != BPF_REG_0) {
+ 				/* mov dst_reg, rax */
+ 				EMIT_mov(dst_reg, BPF_REG_0);
+ 				EMIT1(0x58); /* pop rax */
+ 			}
+ 			break;
+ 		}
+ 			/* Shifts */
+ 		case BPF_ALU | BPF_LSH | BPF_K:
+ 		case BPF_ALU | BPF_RSH | BPF_K:
+ 		case BPF_ALU | BPF_ARSH | BPF_K:
+ 		case BPF_ALU64 | BPF_LSH | BPF_K:
+ 		case BPF_ALU64 | BPF_RSH | BPF_K:
+ 		case BPF_ALU64 | BPF_ARSH | BPF_K:
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_LSH: b3 = 0xE0; break;
+ 			case BPF_RSH: b3 = 0xE8; break;
+ 			case BPF_ARSH: b3 = 0xF8; break;
+ 			}
+ 
+ 			if (imm32 == 1)
+ 				EMIT2(0xD1, add_1reg(b3, dst_reg));
+ 			else
+ 				EMIT3(0xC1, add_1reg(b3, dst_reg), imm32);
+ 			break;
+ 
+ 		case BPF_ALU | BPF_LSH | BPF_X:
+ 		case BPF_ALU | BPF_RSH | BPF_X:
+ 		case BPF_ALU | BPF_ARSH | BPF_X:
+ 		case BPF_ALU64 | BPF_LSH | BPF_X:
+ 		case BPF_ALU64 | BPF_RSH | BPF_X:
+ 		case BPF_ALU64 | BPF_ARSH | BPF_X:
+ 
+ 			/* Check for bad case when dst_reg == rcx */
+ 			if (dst_reg == BPF_REG_4) {
+ 				/* mov r11, dst_reg */
+ 				EMIT_mov(AUX_REG, dst_reg);
+ 				dst_reg = AUX_REG;
+ 			}
+ 
+ 			if (src_reg != BPF_REG_4) { /* common case */
+ 				EMIT1(0x51); /* push rcx */
+ 
+ 				/* mov rcx, src_reg */
+ 				EMIT_mov(BPF_REG_4, src_reg);
+ 			}
+ 
+ 			/* shl %rax, %cl | shr %rax, %cl | sar %rax, %cl */
+ 			if (BPF_CLASS(insn->code) == BPF_ALU64)
+ 				EMIT1(add_1mod(0x48, dst_reg));
+ 			else if (is_ereg(dst_reg))
+ 				EMIT1(add_1mod(0x40, dst_reg));
+ 
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_LSH: b3 = 0xE0; break;
+ 			case BPF_RSH: b3 = 0xE8; break;
+ 			case BPF_ARSH: b3 = 0xF8; break;
+ 			}
+ 			EMIT2(0xD3, add_1reg(b3, dst_reg));
+ 
+ 			if (src_reg != BPF_REG_4)
+ 				EMIT1(0x59); /* pop rcx */
+ 
+ 			if (insn->dst_reg == BPF_REG_4)
+ 				/* mov dst_reg, r11 */
+ 				EMIT_mov(insn->dst_reg, AUX_REG);
+ 			break;
+ 
+ 		case BPF_ALU | BPF_END | BPF_FROM_BE:
+ 			switch (imm32) {
+ 			case 16:
+ 				/* Emit 'ror %ax, 8' to swap lower 2 bytes */
+ 				EMIT1(0x66);
+ 				if (is_ereg(dst_reg))
+ 					EMIT1(0x41);
+ 				EMIT3(0xC1, add_1reg(0xC8, dst_reg), 8);
+ 
+ 				/* Emit 'movzwl eax, ax' */
+ 				if (is_ereg(dst_reg))
+ 					EMIT3(0x45, 0x0F, 0xB7);
+ 				else
+ 					EMIT2(0x0F, 0xB7);
+ 				EMIT1(add_2reg(0xC0, dst_reg, dst_reg));
+ 				break;
+ 			case 32:
+ 				/* Emit 'bswap eax' to swap lower 4 bytes */
+ 				if (is_ereg(dst_reg))
+ 					EMIT2(0x41, 0x0F);
+ 				else
+ 					EMIT1(0x0F);
+ 				EMIT1(add_1reg(0xC8, dst_reg));
+ 				break;
+ 			case 64:
+ 				/* Emit 'bswap rax' to swap 8 bytes */
+ 				EMIT3(add_1mod(0x48, dst_reg), 0x0F,
+ 				      add_1reg(0xC8, dst_reg));
+ 				break;
+ 			}
+ 			break;
+ 
+ 		case BPF_ALU | BPF_END | BPF_FROM_LE:
+ 			switch (imm32) {
+ 			case 16:
+ 				/*
+ 				 * Emit 'movzwl eax, ax' to zero extend 16-bit
+ 				 * into 64 bit
+ 				 */
+ 				if (is_ereg(dst_reg))
+ 					EMIT3(0x45, 0x0F, 0xB7);
+ 				else
+ 					EMIT2(0x0F, 0xB7);
+ 				EMIT1(add_2reg(0xC0, dst_reg, dst_reg));
+ 				break;
+ 			case 32:
+ 				/* Emit 'mov eax, eax' to clear upper 32-bits */
+ 				if (is_ereg(dst_reg))
+ 					EMIT1(0x45);
+ 				EMIT2(0x89, add_2reg(0xC0, dst_reg, dst_reg));
+ 				break;
+ 			case 64:
+ 				/* nop */
+ 				break;
+ 			}
+ 			break;
+ 
+ 			/* ST: *(u8*)(dst_reg + off) = imm */
+ 		case BPF_ST | BPF_MEM | BPF_B:
+ 			if (is_ereg(dst_reg))
+ 				EMIT2(0x41, 0xC6);
+ 			else
+ 				EMIT1(0xC6);
+ 			goto st;
+ 		case BPF_ST | BPF_MEM | BPF_H:
+ 			if (is_ereg(dst_reg))
+ 				EMIT3(0x66, 0x41, 0xC7);
+ 			else
+ 				EMIT2(0x66, 0xC7);
+ 			goto st;
+ 		case BPF_ST | BPF_MEM | BPF_W:
+ 			if (is_ereg(dst_reg))
+ 				EMIT2(0x41, 0xC7);
+ 			else
+ 				EMIT1(0xC7);
+ 			goto st;
+ 		case BPF_ST | BPF_MEM | BPF_DW:
+ 			EMIT2(add_1mod(0x48, dst_reg), 0xC7);
+ 
+ st:			if (is_imm8(insn->off))
+ 				EMIT2(add_1reg(0x40, dst_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_1reg(0x80, dst_reg), insn->off);
+ 
+ 			EMIT(imm32, bpf_size_to_x86_bytes(BPF_SIZE(insn->code)));
+ 			break;
+ 
+ 			/* STX: *(u8*)(dst_reg + off) = src_reg */
+ 		case BPF_STX | BPF_MEM | BPF_B:
+ 			/* Emit 'mov byte ptr [rax + off], al' */
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg) ||
+ 			    /* We have to add extra byte for x86 SIL, DIL regs */
+ 			    src_reg == BPF_REG_1 || src_reg == BPF_REG_2)
+ 				EMIT2(add_2mod(0x40, dst_reg, src_reg), 0x88);
+ 			else
+ 				EMIT1(0x88);
+ 			goto stx;
+ 		case BPF_STX | BPF_MEM | BPF_H:
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT3(0x66, add_2mod(0x40, dst_reg, src_reg), 0x89);
+ 			else
+ 				EMIT2(0x66, 0x89);
+ 			goto stx;
+ 		case BPF_STX | BPF_MEM | BPF_W:
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT2(add_2mod(0x40, dst_reg, src_reg), 0x89);
+ 			else
+ 				EMIT1(0x89);
+ 			goto stx;
+ 		case BPF_STX | BPF_MEM | BPF_DW:
+ 			EMIT2(add_2mod(0x48, dst_reg, src_reg), 0x89);
+ stx:			if (is_imm8(insn->off))
+ 				EMIT2(add_2reg(0x40, dst_reg, src_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_2reg(0x80, dst_reg, src_reg),
+ 					    insn->off);
+ 			break;
+ 
+ 			/* LDX: dst_reg = *(u8*)(src_reg + off) */
+ 		case BPF_LDX | BPF_MEM | BPF_B:
+ 			/* Emit 'movzx rax, byte ptr [rax + off]' */
+ 			EMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB6);
+ 			goto ldx;
+ 		case BPF_LDX | BPF_MEM | BPF_H:
+ 			/* Emit 'movzx rax, word ptr [rax + off]' */
+ 			EMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB7);
+ 			goto ldx;
+ 		case BPF_LDX | BPF_MEM | BPF_W:
+ 			/* Emit 'mov eax, dword ptr [rax+0x14]' */
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT2(add_2mod(0x40, src_reg, dst_reg), 0x8B);
+ 			else
+ 				EMIT1(0x8B);
+ 			goto ldx;
+ 		case BPF_LDX | BPF_MEM | BPF_DW:
+ 			/* Emit 'mov rax, qword ptr [rax+0x14]' */
+ 			EMIT2(add_2mod(0x48, src_reg, dst_reg), 0x8B);
+ ldx:			/*
+ 			 * If insn->off == 0 we can save one extra byte, but
+ 			 * special case of x86 R13 which always needs an offset
+ 			 * is not worth the hassle
+ 			 */
+ 			if (is_imm8(insn->off))
+ 				EMIT2(add_2reg(0x40, src_reg, dst_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_2reg(0x80, src_reg, dst_reg),
+ 					    insn->off);
+ 			break;
+ 
+ 			/* STX XADD: lock *(u32*)(dst_reg + off) += src_reg */
+ 		case BPF_STX | BPF_XADD | BPF_W:
+ 			/* Emit 'lock add dword ptr [rax + off], eax' */
+ 			if (is_ereg(dst_reg) || is_ereg(src_reg))
+ 				EMIT3(0xF0, add_2mod(0x40, dst_reg, src_reg), 0x01);
+ 			else
+ 				EMIT2(0xF0, 0x01);
+ 			goto xadd;
+ 		case BPF_STX | BPF_XADD | BPF_DW:
+ 			EMIT3(0xF0, add_2mod(0x48, dst_reg, src_reg), 0x01);
+ xadd:			if (is_imm8(insn->off))
+ 				EMIT2(add_2reg(0x40, dst_reg, src_reg), insn->off);
+ 			else
+ 				EMIT1_off32(add_2reg(0x80, dst_reg, src_reg),
+ 					    insn->off);
+ 			break;
+ 
+ 			/* call */
+ 		case BPF_JMP | BPF_CALL:
+ 			func = (u8 *) __bpf_call_base + imm32;
+ 			jmp_offset = func - (image + addrs[i]);
+ 			if (seen_ld_abs) {
+ 				reload_skb_data = bpf_helper_changes_pkt_data(func);
+ 				if (reload_skb_data) {
+ 					EMIT1(0x57); /* push %rdi */
+ 					jmp_offset += 22; /* pop, mov, sub, mov */
+ 				} else {
+ 					EMIT2(0x41, 0x52); /* push %r10 */
+ 					EMIT2(0x41, 0x51); /* push %r9 */
+ 					/*
+ 					 * We need to adjust jmp offset, since
+ 					 * pop %r9, pop %r10 take 4 bytes after call insn
+ 					 */
+ 					jmp_offset += 4;
+ 				}
+ 			}
+ 			if (!imm32 || !is_simm32(jmp_offset)) {
+ 				pr_err("unsupported BPF func %d addr %p image %p\n",
+ 				       imm32, func, image);
+ 				return -EINVAL;
+ 			}
+ 			EMIT1_off32(0xE8, jmp_offset);
+ 			if (seen_ld_abs) {
+ 				if (reload_skb_data) {
+ 					EMIT1(0x5F); /* pop %rdi */
+ 					emit_load_skb_data_hlen(&prog);
+ 				} else {
+ 					EMIT2(0x41, 0x59); /* pop %r9 */
+ 					EMIT2(0x41, 0x5A); /* pop %r10 */
+ 				}
+ 			}
+ 			break;
+ 
+ 		case BPF_JMP | BPF_TAIL_CALL:
+ 			emit_bpf_tail_call(&prog);
+ 			break;
+ 
+ 			/* cond jump */
+ 		case BPF_JMP | BPF_JEQ | BPF_X:
+ 		case BPF_JMP | BPF_JNE | BPF_X:
+ 		case BPF_JMP | BPF_JGT | BPF_X:
+ 		case BPF_JMP | BPF_JLT | BPF_X:
+ 		case BPF_JMP | BPF_JGE | BPF_X:
+ 		case BPF_JMP | BPF_JLE | BPF_X:
+ 		case BPF_JMP | BPF_JSGT | BPF_X:
+ 		case BPF_JMP | BPF_JSLT | BPF_X:
+ 		case BPF_JMP | BPF_JSGE | BPF_X:
+ 		case BPF_JMP | BPF_JSLE | BPF_X:
+ 			/* cmp dst_reg, src_reg */
+ 			EMIT3(add_2mod(0x48, dst_reg, src_reg), 0x39,
+ 			      add_2reg(0xC0, dst_reg, src_reg));
+ 			goto emit_cond_jmp;
+ 
+ 		case BPF_JMP | BPF_JSET | BPF_X:
+ 			/* test dst_reg, src_reg */
+ 			EMIT3(add_2mod(0x48, dst_reg, src_reg), 0x85,
+ 			      add_2reg(0xC0, dst_reg, src_reg));
+ 			goto emit_cond_jmp;
+ 
+ 		case BPF_JMP | BPF_JSET | BPF_K:
+ 			/* test dst_reg, imm32 */
+ 			EMIT1(add_1mod(0x48, dst_reg));
+ 			EMIT2_off32(0xF7, add_1reg(0xC0, dst_reg), imm32);
+ 			goto emit_cond_jmp;
+ 
+ 		case BPF_JMP | BPF_JEQ | BPF_K:
+ 		case BPF_JMP | BPF_JNE | BPF_K:
+ 		case BPF_JMP | BPF_JGT | BPF_K:
+ 		case BPF_JMP | BPF_JLT | BPF_K:
+ 		case BPF_JMP | BPF_JGE | BPF_K:
+ 		case BPF_JMP | BPF_JLE | BPF_K:
+ 		case BPF_JMP | BPF_JSGT | BPF_K:
+ 		case BPF_JMP | BPF_JSLT | BPF_K:
+ 		case BPF_JMP | BPF_JSGE | BPF_K:
+ 		case BPF_JMP | BPF_JSLE | BPF_K:
+ 			/* cmp dst_reg, imm8/32 */
+ 			EMIT1(add_1mod(0x48, dst_reg));
+ 
+ 			if (is_imm8(imm32))
+ 				EMIT3(0x83, add_1reg(0xF8, dst_reg), imm32);
+ 			else
+ 				EMIT2_off32(0x81, add_1reg(0xF8, dst_reg), imm32);
+ 
+ emit_cond_jmp:		/* Convert BPF opcode to x86 */
+ 			switch (BPF_OP(insn->code)) {
+ 			case BPF_JEQ:
+ 				jmp_cond = X86_JE;
+ 				break;
+ 			case BPF_JSET:
+ 			case BPF_JNE:
+ 				jmp_cond = X86_JNE;
+ 				break;
+ 			case BPF_JGT:
+ 				/* GT is unsigned '>', JA in x86 */
+ 				jmp_cond = X86_JA;
+ 				break;
+ 			case BPF_JLT:
+ 				/* LT is unsigned '<', JB in x86 */
+ 				jmp_cond = X86_JB;
+ 				break;
+ 			case BPF_JGE:
+ 				/* GE is unsigned '>=', JAE in x86 */
+ 				jmp_cond = X86_JAE;
+ 				break;
+ 			case BPF_JLE:
+ 				/* LE is unsigned '<=', JBE in x86 */
+ 				jmp_cond = X86_JBE;
+ 				break;
+ 			case BPF_JSGT:
+ 				/* Signed '>', GT in x86 */
+ 				jmp_cond = X86_JG;
+ 				break;
+ 			case BPF_JSLT:
+ 				/* Signed '<', LT in x86 */
+ 				jmp_cond = X86_JL;
+ 				break;
+ 			case BPF_JSGE:
+ 				/* Signed '>=', GE in x86 */
+ 				jmp_cond = X86_JGE;
+ 				break;
+ 			case BPF_JSLE:
+ 				/* Signed '<=', LE in x86 */
+ 				jmp_cond = X86_JLE;
+ 				break;
+ 			default: /* to silence GCC warning */
+ 				return -EFAULT;
+ 			}
+ 			jmp_offset = addrs[i + insn->off] - addrs[i];
+ 			if (is_imm8(jmp_offset)) {
+ 				EMIT2(jmp_cond, jmp_offset);
+ 			} else if (is_simm32(jmp_offset)) {
+ 				EMIT2_off32(0x0F, jmp_cond + 0x10, jmp_offset);
+ 			} else {
+ 				pr_err("cond_jmp gen bug %llx\n", jmp_offset);
+ 				return -EFAULT;
+ 			}
+ 
+ 			break;
+ 
+ 		case BPF_JMP | BPF_JA:
+ 			jmp_offset = addrs[i + insn->off] - addrs[i];
+ 			if (!jmp_offset)
+ 				/* Optimize out nop jumps */
+ 				break;
+ emit_jmp:
+ 			if (is_imm8(jmp_offset)) {
+ 				EMIT2(0xEB, jmp_offset);
+ 			} else if (is_simm32(jmp_offset)) {
+ 				EMIT1_off32(0xE9, jmp_offset);
+ 			} else {
+ 				pr_err("jmp gen bug %llx\n", jmp_offset);
+ 				return -EFAULT;
+ 			}
+ 			break;
+ 
+ 		case BPF_LD | BPF_IND | BPF_W:
+ 			func = sk_load_word;
+ 			goto common_load;
+ 		case BPF_LD | BPF_ABS | BPF_W:
+ 			func = CHOOSE_LOAD_FUNC(imm32, sk_load_word);
+ common_load:
+ 			ctx->seen_ld_abs = seen_ld_abs = true;
+ 			jmp_offset = func - (image + addrs[i]);
+ 			if (!func || !is_simm32(jmp_offset)) {
+ 				pr_err("unsupported BPF func %d addr %p image %p\n",
+ 				       imm32, func, image);
+ 				return -EINVAL;
+ 			}
+ 			if (BPF_MODE(insn->code) == BPF_ABS) {
+ 				/* mov %esi, imm32 */
+ 				EMIT1_off32(0xBE, imm32);
+ 			} else {
+ 				/* mov %rsi, src_reg */
+ 				EMIT_mov(BPF_REG_2, src_reg);
+ 				if (imm32) {
+ 					if (is_imm8(imm32))
+ 						/* add %esi, imm8 */
+ 						EMIT3(0x83, 0xC6, imm32);
+ 					else
+ 						/* add %esi, imm32 */
+ 						EMIT2_off32(0x81, 0xC6, imm32);
+ 				}
+ 			}
+ 			/*
+ 			 * skb pointer is in R6 (%rbx), it will be copied into
+ 			 * %rdi if skb_copy_bits() call is necessary.
+ 			 * sk_load_* helpers also use %r10 and %r9d.
+ 			 * See bpf_jit.S
+ 			 */
+ 			if (seen_ax_reg)
+ 				/* r10 = skb->data, mov %r10, off32(%rbx) */
+ 				EMIT3_off32(0x4c, 0x8b, 0x93,
+ 					    offsetof(struct sk_buff, data));
+ 			EMIT1_off32(0xE8, jmp_offset); /* call */
+ 			break;
+ 
+ 		case BPF_LD | BPF_IND | BPF_H:
+ 			func = sk_load_half;
+ 			goto common_load;
+ 		case BPF_LD | BPF_ABS | BPF_H:
+ 			func = CHOOSE_LOAD_FUNC(imm32, sk_load_half);
+ 			goto common_load;
+ 		case BPF_LD | BPF_IND | BPF_B:
+ 			func = sk_load_byte;
+ 			goto common_load;
+ 		case BPF_LD | BPF_ABS | BPF_B:
+ 			func = CHOOSE_LOAD_FUNC(imm32, sk_load_byte);
+ 			goto common_load;
+ 
+ 		case BPF_JMP | BPF_EXIT:
+ 			if (seen_exit) {
+ 				jmp_offset = ctx->cleanup_addr - addrs[i];
+ 				goto emit_jmp;
+ 			}
+ 			seen_exit = true;
+ 			/* Update cleanup_addr */
+ 			ctx->cleanup_addr = proglen;
+ 			/* mov rbx, qword ptr [rbp+0] */
+ 			EMIT4(0x48, 0x8B, 0x5D, 0);
+ 			/* mov r13, qword ptr [rbp+8] */
+ 			EMIT4(0x4C, 0x8B, 0x6D, 8);
+ 			/* mov r14, qword ptr [rbp+16] */
+ 			EMIT4(0x4C, 0x8B, 0x75, 16);
+ 			/* mov r15, qword ptr [rbp+24] */
+ 			EMIT4(0x4C, 0x8B, 0x7D, 24);
+ 
+ 			/* add rbp, AUX_STACK_SPACE */
+ 			EMIT4(0x48, 0x83, 0xC5, AUX_STACK_SPACE);
+ 			EMIT1(0xC9); /* leave */
+ 			EMIT1(0xC3); /* ret */
+ 			break;
+ 
+ 		default:
+ 			/*
+ 			 * By design x86-64 JIT should support all BPF instructions.
+ 			 * This error will be seen if new instruction was added
+ 			 * to the interpreter, but not to the JIT, or if there is
+ 			 * junk in bpf_prog.
+ 			 */
+ 			pr_err("bpf_jit: unknown opcode %02x\n", insn->code);
+ 			return -EINVAL;
+ 		}
+ 
+ 		ilen = prog - temp;
+ 		if (ilen > BPF_MAX_INSN_SIZE) {
+ 			pr_err("bpf_jit: fatal insn size error\n");
+ 			return -EFAULT;
+ 		}
+ 
+ 		if (image) {
+ 			if (unlikely(proglen + ilen > oldproglen)) {
+ 				pr_err("bpf_jit: fatal error\n");
+ 				return -EFAULT;
+ 			}
+ 			memcpy(image + proglen, temp, ilen);
+ 		}
+ 		proglen += ilen;
+ 		addrs[i] = proglen;
+ 		prog = temp;
+ 	}
+ 	return proglen;
+ }
+ 
+ struct x64_jit_data {
+ 	struct bpf_binary_header *header;
+ 	int *addrs;
+ 	u8 *image;
+ 	int proglen;
+ 	struct jit_context ctx;
+ };
+ 
+ struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog)
+ {
+ 	struct bpf_binary_header *header = NULL;
+ 	struct bpf_prog *tmp, *orig_prog = prog;
+ 	struct x64_jit_data *jit_data;
+ 	int proglen, oldproglen = 0;
+ 	struct jit_context ctx = {};
+ 	bool tmp_blinded = false;
+ 	bool extra_pass = false;
++>>>>>>> 5f26c50143f5 (x86/bpf: Clean up non-standard comments, to make the code more readable)
  	u8 *image = NULL;
 -	int *addrs;
 -	int pass;
 -	int i;
 -
 -	if (!prog->jit_requested)
 -		return orig_prog;
 -
 +	u8 *func;
 +	int pc_ret0 = -1; /* bpf index of first RET #0 instruction (if any) */
 +	unsigned int cleanup_addr; /* epilogue code offset */
 +	unsigned int *addrs;
 +	const struct sock_filter *filter = fp->insns;
 +	int flen = fp->len;
 +
 +	if (!bpf_jit_enable)
 +		return;
 +
++<<<<<<< HEAD
 +	addrs = kmalloc(flen * sizeof(*addrs), GFP_KERNEL);
 +	if (addrs == NULL)
 +		return;
++=======
+ 	tmp = bpf_jit_blind_constants(prog);
+ 	/*
+ 	 * If blinding was requested and we failed during blinding,
+ 	 * we must fall back to the interpreter.
+ 	 */
+ 	if (IS_ERR(tmp))
+ 		return orig_prog;
+ 	if (tmp != prog) {
+ 		tmp_blinded = true;
+ 		prog = tmp;
+ 	}
+ 
+ 	jit_data = prog->aux->jit_data;
+ 	if (!jit_data) {
+ 		jit_data = kzalloc(sizeof(*jit_data), GFP_KERNEL);
+ 		if (!jit_data) {
+ 			prog = orig_prog;
+ 			goto out;
+ 		}
+ 		prog->aux->jit_data = jit_data;
+ 	}
+ 	addrs = jit_data->addrs;
+ 	if (addrs) {
+ 		ctx = jit_data->ctx;
+ 		oldproglen = jit_data->proglen;
+ 		image = jit_data->image;
+ 		header = jit_data->header;
+ 		extra_pass = true;
+ 		goto skip_init_addrs;
+ 	}
+ 	addrs = kmalloc(prog->len * sizeof(*addrs), GFP_KERNEL);
+ 	if (!addrs) {
+ 		prog = orig_prog;
+ 		goto out_addrs;
+ 	}
++>>>>>>> 5f26c50143f5 (x86/bpf: Clean up non-standard comments, to make the code more readable)
  
- 	/* Before first pass, make a rough estimation of addrs[]
- 	 * each bpf instruction is translated to less than 64 bytes
+ 	/*
+ 	 * Before first pass, make a rough estimation of addrs[]
+ 	 * each BPF instruction is translated to less than 64 bytes
  	 */
 -	for (proglen = 0, i = 0; i < prog->len; i++) {
 +	for (proglen = 0, i = 0; i < flen; i++) {
  		proglen += 64;
  		addrs[i] = proglen;
  	}
 -	ctx.cleanup_addr = proglen;
 -skip_init_addrs:
 +	cleanup_addr = proglen; /* epilogue address */
  
- 	/* JITed image shrinks with every pass and the loop iterates
- 	 * until the image stops shrinking. Very large bpf programs
+ 	/*
+ 	 * JITed image shrinks with every pass and the loop iterates
+ 	 * until the image stops shrinking. Very large BPF programs
  	 * may converge on the last pass. In such case do one more
- 	 * pass to emit the final image
+ 	 * pass to emit the final image.
  	 */
 -	for (pass = 0; pass < 20 || image; pass++) {
 -		proglen = do_jit(prog, addrs, image, oldproglen, &ctx);
 -		if (proglen <= 0) {
 -			image = NULL;
 -			if (header)
 -				bpf_jit_binary_free(header);
 -			prog = orig_prog;
 -			goto out_addrs;
 +	for (pass = 0; pass < 10 || image; pass++) {
 +		u8 seen_or_pass0 = (pass == 0) ? (SEEN_XREG | SEEN_DATAREF | SEEN_MEM) : seen;
 +		/* no prologue/epilogue for trivial filters (RET something) */
 +		proglen = 0;
 +		prog = temp;
 +
 +		if (seen_or_pass0) {
 +			EMIT4(0x55, 0x48, 0x89, 0xe5); /* push %rbp; mov %rsp,%rbp */
 +			EMIT4(0x48, 0x83, 0xec, 96);	/* subq  $96,%rsp	*/
 +			/* note : must save %rbx in case bpf_error is hit */
 +			if (seen_or_pass0 & (SEEN_XREG | SEEN_DATAREF))
 +				EMIT4(0x48, 0x89, 0x5d, 0xf8); /* mov %rbx, -8(%rbp) */
 +			if (seen_or_pass0 & SEEN_XREG)
 +				CLEAR_X(); /* make sure we dont leek kernel memory */
 +
 +			/*
 +			 * If this filter needs to access skb data,
 +			 * loads r9 and r8 with :
 +			 *  r9 = skb->len - skb->data_len
 +			 *  r8 = skb->data
 +			 */
 +			if (seen_or_pass0 & SEEN_DATAREF) {
 +				if (offsetof(struct sk_buff, len) <= 127)
 +					/* mov    off8(%rdi),%r9d */
 +					EMIT4(0x44, 0x8b, 0x4f, offsetof(struct sk_buff, len));
 +				else {
 +					/* mov    off32(%rdi),%r9d */
 +					EMIT3(0x44, 0x8b, 0x8f);
 +					EMIT(offsetof(struct sk_buff, len), 4);
 +				}
 +				if (is_imm8(offsetof(struct sk_buff, data_len)))
 +					/* sub    off8(%rdi),%r9d */
 +					EMIT4(0x44, 0x2b, 0x4f, offsetof(struct sk_buff, data_len));
 +				else {
 +					EMIT3(0x44, 0x2b, 0x8f);
 +					EMIT(offsetof(struct sk_buff, data_len), 4);
 +				}
 +
 +				if (is_imm8(offsetof(struct sk_buff, data)))
 +					/* mov off8(%rdi),%r8 */
 +					EMIT4(0x4c, 0x8b, 0x47, offsetof(struct sk_buff, data));
 +				else {
 +					/* mov off32(%rdi),%r8 */
 +					EMIT3(0x4c, 0x8b, 0x87);
 +					EMIT(offsetof(struct sk_buff, data), 4);
 +				}
 +			}
  		}
 -		if (image) {
 -			if (proglen != oldproglen) {
 -				pr_err("bpf_jit: proglen=%d != oldproglen=%d\n",
 -				       proglen, oldproglen);
 -				prog = orig_prog;
 -				goto out_addrs;
 +
 +		switch (filter[0].code) {
 +		case BPF_S_RET_K:
 +		case BPF_S_LD_W_LEN:
 +		case BPF_S_ANC_PROTOCOL:
 +		case BPF_S_ANC_IFINDEX:
 +		case BPF_S_ANC_MARK:
 +		case BPF_S_ANC_RXHASH:
 +		case BPF_S_ANC_CPU:
 +		case BPF_S_ANC_VLAN_TAG:
 +		case BPF_S_ANC_VLAN_TAG_PRESENT:
 +		case BPF_S_ANC_QUEUE:
 +		case BPF_S_ANC_PKTTYPE:
 +		case BPF_S_LD_W_ABS:
 +		case BPF_S_LD_H_ABS:
 +		case BPF_S_LD_B_ABS:
 +			/* first instruction sets A register (or is RET 'constant') */
 +			break;
 +		default:
 +			/* make sure we dont leak kernel information to user */
 +			CLEAR_A(); /* A = 0 */
 +		}
 +
 +		for (i = 0; i < flen; i++) {
 +			unsigned int K = filter[i].k;
 +
 +			switch (filter[i].code) {
 +			case BPF_S_ALU_ADD_X: /* A += X; */
 +				seen |= SEEN_XREG;
 +				EMIT2(0x01, 0xd8);		/* add %ebx,%eax */
 +				break;
 +			case BPF_S_ALU_ADD_K: /* A += K; */
 +				if (!K)
 +					break;
 +				if (is_imm8(K))
 +					EMIT3(0x83, 0xc0, K);	/* add imm8,%eax */
 +				else
 +					EMIT1_off32(0x05, K);	/* add imm32,%eax */
 +				break;
 +			case BPF_S_ALU_SUB_X: /* A -= X; */
 +				seen |= SEEN_XREG;
 +				EMIT2(0x29, 0xd8);		/* sub    %ebx,%eax */
 +				break;
 +			case BPF_S_ALU_SUB_K: /* A -= K */
 +				if (!K)
 +					break;
 +				if (is_imm8(K))
 +					EMIT3(0x83, 0xe8, K); /* sub imm8,%eax */
 +				else
 +					EMIT1_off32(0x2d, K); /* sub imm32,%eax */
 +				break;
 +			case BPF_S_ALU_MUL_X: /* A *= X; */
 +				seen |= SEEN_XREG;
 +				EMIT3(0x0f, 0xaf, 0xc3);	/* imul %ebx,%eax */
 +				break;
 +			case BPF_S_ALU_MUL_K: /* A *= K */
 +				if (is_imm8(K))
 +					EMIT3(0x6b, 0xc0, K); /* imul imm8,%eax,%eax */
 +				else {
 +					EMIT2(0x69, 0xc0);		/* imul imm32,%eax */
 +					EMIT(K, 4);
 +				}
 +				break;
 +			case BPF_S_ALU_DIV_X: /* A /= X; */
 +				seen |= SEEN_XREG;
 +				EMIT2(0x85, 0xdb);	/* test %ebx,%ebx */
 +				if (pc_ret0 > 0) {
 +					/* addrs[pc_ret0 - 1] is start address of target
 +					 * (addrs[i] - 4) is the address following this jmp
 +					 * ("xor %edx,%edx; div %ebx" being 4 bytes long)
 +					 */
 +					EMIT_COND_JMP(X86_JE, addrs[pc_ret0 - 1] -
 +								(addrs[i] - 4));
 +				} else {
 +					EMIT_COND_JMP(X86_JNE, 2 + 5);
 +					CLEAR_A();
 +					EMIT1_off32(0xe9, cleanup_addr - (addrs[i] - 4)); /* jmp .+off32 */
 +				}
 +				EMIT4(0x31, 0xd2, 0xf7, 0xf3); /* xor %edx,%edx; div %ebx */
 +				break;
 +			case BPF_S_ALU_MOD_X: /* A %= X; */
 +				seen |= SEEN_XREG;
 +				EMIT2(0x85, 0xdb);	/* test %ebx,%ebx */
 +				if (pc_ret0 > 0) {
 +					/* addrs[pc_ret0 - 1] is start address of target
 +					 * (addrs[i] - 6) is the address following this jmp
 +					 * ("xor %edx,%edx; div %ebx;mov %edx,%eax" being 6 bytes long)
 +					 */
 +					EMIT_COND_JMP(X86_JE, addrs[pc_ret0 - 1] -
 +								(addrs[i] - 6));
 +				} else {
 +					EMIT_COND_JMP(X86_JNE, 2 + 5);
 +					CLEAR_A();
 +					EMIT1_off32(0xe9, cleanup_addr - (addrs[i] - 6)); /* jmp .+off32 */
 +				}
 +				EMIT2(0x31, 0xd2);	/* xor %edx,%edx */
 +				EMIT2(0xf7, 0xf3);	/* div %ebx */
 +				EMIT2(0x89, 0xd0);	/* mov %edx,%eax */
 +				break;
 +			case BPF_S_ALU_MOD_K: /* A %= K; */
 +				if (K == 1) {
 +					CLEAR_A();
 +					break;
 +				}
 +				EMIT2(0x31, 0xd2);	/* xor %edx,%edx */
 +				EMIT1(0xb9);EMIT(K, 4);	/* mov imm32,%ecx */
 +				EMIT2(0xf7, 0xf1);	/* div %ecx */
 +				EMIT2(0x89, 0xd0);	/* mov %edx,%eax */
 +				break;
 +			case BPF_S_ALU_DIV_K: /* A /= K */
 +				if (K == 1)
 +					break;
 +				EMIT2(0x31, 0xd2);	/* xor %edx,%edx */
 +				EMIT1(0xb9);EMIT(K, 4);	/* mov imm32,%ecx */
 +				EMIT2(0xf7, 0xf1);	/* div %ecx */
 +				break;
 +			case BPF_S_ALU_AND_X:
 +				seen |= SEEN_XREG;
 +				EMIT2(0x21, 0xd8);		/* and %ebx,%eax */
 +				break;
 +			case BPF_S_ALU_AND_K:
 +				if (K >= 0xFFFFFF00) {
 +					EMIT2(0x24, K & 0xFF); /* and imm8,%al */
 +				} else if (K >= 0xFFFF0000) {
 +					EMIT2(0x66, 0x25);	/* and imm16,%ax */
 +					EMIT(K, 2);
 +				} else {
 +					EMIT1_off32(0x25, K);	/* and imm32,%eax */
 +				}
 +				break;
 +			case BPF_S_ALU_OR_X:
 +				seen |= SEEN_XREG;
 +				EMIT2(0x09, 0xd8);		/* or %ebx,%eax */
 +				break;
 +			case BPF_S_ALU_OR_K:
 +				if (is_imm8(K))
 +					EMIT3(0x83, 0xc8, K); /* or imm8,%eax */
 +				else
 +					EMIT1_off32(0x0d, K);	/* or imm32,%eax */
 +				break;
 +			case BPF_S_ANC_ALU_XOR_X: /* A ^= X; */
 +			case BPF_S_ALU_XOR_X:
 +				seen |= SEEN_XREG;
 +				EMIT2(0x31, 0xd8);		/* xor %ebx,%eax */
 +				break;
 +			case BPF_S_ALU_XOR_K: /* A ^= K; */
 +				if (K == 0)
 +					break;
 +				if (is_imm8(K))
 +					EMIT3(0x83, 0xf0, K);	/* xor imm8,%eax */
 +				else
 +					EMIT1_off32(0x35, K);	/* xor imm32,%eax */
 +				break;
 +			case BPF_S_ALU_LSH_X: /* A <<= X; */
 +				seen |= SEEN_XREG;
 +				EMIT4(0x89, 0xd9, 0xd3, 0xe0);	/* mov %ebx,%ecx; shl %cl,%eax */
 +				break;
 +			case BPF_S_ALU_LSH_K:
 +				if (K == 0)
 +					break;
 +				else if (K == 1)
 +					EMIT2(0xd1, 0xe0); /* shl %eax */
 +				else
 +					EMIT3(0xc1, 0xe0, K);
 +				break;
 +			case BPF_S_ALU_RSH_X: /* A >>= X; */
 +				seen |= SEEN_XREG;
 +				EMIT4(0x89, 0xd9, 0xd3, 0xe8);	/* mov %ebx,%ecx; shr %cl,%eax */
 +				break;
 +			case BPF_S_ALU_RSH_K: /* A >>= K; */
 +				if (K == 0)
 +					break;
 +				else if (K == 1)
 +					EMIT2(0xd1, 0xe8); /* shr %eax */
 +				else
 +					EMIT3(0xc1, 0xe8, K);
 +				break;
 +			case BPF_S_ALU_NEG:
 +				EMIT2(0xf7, 0xd8);		/* neg %eax */
 +				break;
 +			case BPF_S_RET_K:
 +				if (!K) {
 +					if (pc_ret0 == -1)
 +						pc_ret0 = i;
 +					CLEAR_A();
 +				} else {
 +					EMIT1_off32(0xb8, K);	/* mov $imm32,%eax */
 +				}
 +				/* fallinto */
 +			case BPF_S_RET_A:
 +				if (seen_or_pass0) {
 +					if (i != flen - 1) {
 +						EMIT_JMP(cleanup_addr - addrs[i]);
 +						break;
 +					}
 +					if (seen_or_pass0 & SEEN_XREG)
 +						EMIT4(0x48, 0x8b, 0x5d, 0xf8);  /* mov  -8(%rbp),%rbx */
 +					EMIT1(0xc9);		/* leaveq */
 +				}
 +				EMIT1(0xc3);		/* ret */
 +				break;
 +			case BPF_S_MISC_TAX: /* X = A */
 +				seen |= SEEN_XREG;
 +				EMIT2(0x89, 0xc3);	/* mov    %eax,%ebx */
 +				break;
 +			case BPF_S_MISC_TXA: /* A = X */
 +				seen |= SEEN_XREG;
 +				EMIT2(0x89, 0xd8);	/* mov    %ebx,%eax */
 +				break;
 +			case BPF_S_LD_IMM: /* A = K */
 +				if (!K)
 +					CLEAR_A();
 +				else
 +					EMIT1_off32(0xb8, K); /* mov $imm32,%eax */
 +				break;
 +			case BPF_S_LDX_IMM: /* X = K */
 +				seen |= SEEN_XREG;
 +				if (!K)
 +					CLEAR_X();
 +				else
 +					EMIT1_off32(0xbb, K); /* mov $imm32,%ebx */
 +				break;
 +			case BPF_S_LD_MEM: /* A = mem[K] : mov off8(%rbp),%eax */
 +				seen |= SEEN_MEM;
 +				EMIT3(0x8b, 0x45, 0xf0 - K*4);
 +				break;
 +			case BPF_S_LDX_MEM: /* X = mem[K] : mov off8(%rbp),%ebx */
 +				seen |= SEEN_XREG | SEEN_MEM;
 +				EMIT3(0x8b, 0x5d, 0xf0 - K*4);
 +				break;
 +			case BPF_S_ST: /* mem[K] = A : mov %eax,off8(%rbp) */
 +				seen |= SEEN_MEM;
 +				EMIT3(0x89, 0x45, 0xf0 - K*4);
 +				break;
 +			case BPF_S_STX: /* mem[K] = X : mov %ebx,off8(%rbp) */
 +				seen |= SEEN_XREG | SEEN_MEM;
 +				EMIT3(0x89, 0x5d, 0xf0 - K*4);
 +				break;
 +			case BPF_S_LD_W_LEN: /*	A = skb->len; */
 +				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);
 +				if (is_imm8(offsetof(struct sk_buff, len)))
 +					/* mov    off8(%rdi),%eax */
 +					EMIT3(0x8b, 0x47, offsetof(struct sk_buff, len));
 +				else {
 +					EMIT2(0x8b, 0x87);
 +					EMIT(offsetof(struct sk_buff, len), 4);
 +				}
 +				break;
 +			case BPF_S_LDX_W_LEN: /* X = skb->len; */
 +				seen |= SEEN_XREG;
 +				if (is_imm8(offsetof(struct sk_buff, len)))
 +					/* mov off8(%rdi),%ebx */
 +					EMIT3(0x8b, 0x5f, offsetof(struct sk_buff, len));
 +				else {
 +					EMIT2(0x8b, 0x9f);
 +					EMIT(offsetof(struct sk_buff, len), 4);
 +				}
 +				break;
 +			case BPF_S_ANC_PROTOCOL: /* A = ntohs(skb->protocol); */
 +				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
 +				if (is_imm8(offsetof(struct sk_buff, protocol))) {
 +					/* movzwl off8(%rdi),%eax */
 +					EMIT4(0x0f, 0xb7, 0x47, offsetof(struct sk_buff, protocol));
 +				} else {
 +					EMIT3(0x0f, 0xb7, 0x87); /* movzwl off32(%rdi),%eax */
 +					EMIT(offsetof(struct sk_buff, protocol), 4);
 +				}
 +				EMIT2(0x86, 0xc4); /* ntohs() : xchg   %al,%ah */
 +				break;
 +			case BPF_S_ANC_IFINDEX:
 +				if (is_imm8(offsetof(struct sk_buff, dev))) {
 +					/* movq off8(%rdi),%rax */
 +					EMIT4(0x48, 0x8b, 0x47, offsetof(struct sk_buff, dev));
 +				} else {
 +					EMIT3(0x48, 0x8b, 0x87); /* movq off32(%rdi),%rax */
 +					EMIT(offsetof(struct sk_buff, dev), 4);
 +				}
 +				EMIT3(0x48, 0x85, 0xc0);	/* test %rax,%rax */
 +				EMIT_COND_JMP(X86_JE, cleanup_addr - (addrs[i] - 6));
 +				BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
 +				EMIT2(0x8b, 0x80);	/* mov off32(%rax),%eax */
 +				EMIT(offsetof(struct net_device, ifindex), 4);
 +				break;
 +			case BPF_S_ANC_MARK:
 +				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
 +				if (is_imm8(offsetof(struct sk_buff, mark))) {
 +					/* mov off8(%rdi),%eax */
 +					EMIT3(0x8b, 0x47, offsetof(struct sk_buff, mark));
 +				} else {
 +					EMIT2(0x8b, 0x87);
 +					EMIT(offsetof(struct sk_buff, mark), 4);
 +				}
 +				break;
 +			case BPF_S_ANC_RXHASH:
 +				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
 +				if (is_imm8(offsetof(struct sk_buff, hash))) {
 +					/* mov off8(%rdi),%eax */
 +					EMIT3(0x8b, 0x47, offsetof(struct sk_buff, hash));
 +				} else {
 +					EMIT2(0x8b, 0x87);
 +					EMIT(offsetof(struct sk_buff, hash), 4);
 +				}
 +				break;
 +			case BPF_S_ANC_QUEUE:
 +				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
 +				if (is_imm8(offsetof(struct sk_buff, queue_mapping))) {
 +					/* movzwl off8(%rdi),%eax */
 +					EMIT4(0x0f, 0xb7, 0x47, offsetof(struct sk_buff, queue_mapping));
 +				} else {
 +					EMIT3(0x0f, 0xb7, 0x87); /* movzwl off32(%rdi),%eax */
 +					EMIT(offsetof(struct sk_buff, queue_mapping), 4);
 +				}
 +				break;
 +			case BPF_S_ANC_CPU:
 +#ifdef CONFIG_SMP
 +				EMIT4(0x65, 0x8b, 0x04, 0x25); /* mov %gs:off32,%eax */
 +				EMIT((u32)(unsigned long)&cpu_number, 4); /* A = smp_processor_id(); */
 +#else
 +				CLEAR_A();
 +#endif
 +				break;
 +			case BPF_S_ANC_VLAN_TAG:
 +			case BPF_S_ANC_VLAN_TAG_PRESENT:
 +				BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
 +				if (is_imm8(offsetof(struct sk_buff, vlan_tci))) {
 +					/* movzwl off8(%rdi),%eax */
 +					EMIT4(0x0f, 0xb7, 0x47, offsetof(struct sk_buff, vlan_tci));
 +				} else {
 +					EMIT3(0x0f, 0xb7, 0x87); /* movzwl off32(%rdi),%eax */
 +					EMIT(offsetof(struct sk_buff, vlan_tci), 4);
 +				}
 +				BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
 +				if (filter[i].code == BPF_S_ANC_VLAN_TAG) {
 +					EMIT3(0x80, 0xe4, 0xef); /* and    $0xef,%ah */
 +				} else {
 +					EMIT3(0xc1, 0xe8, 0x0c); /* shr    $0xc,%eax */
 +					EMIT3(0x83, 0xe0, 0x01); /* and    $0x1,%eax */
 +				}
 +				break;
 +			case BPF_S_ANC_PKTTYPE:
 +			{
 +				int off = pkt_type_offset();
 +
 +				if (off < 0)
 +					goto out;
 +				if (is_imm8(off)) {
 +					/* movzbl off8(%rdi),%eax */
 +					EMIT4(0x0f, 0xb6, 0x47, off);
 +				} else {
 +					/* movbl off32(%rdi),%eax */
 +					EMIT3(0x0f, 0xb6, 0x87);
 +					EMIT(off, 4);
 +				}
 +				EMIT3(0x83, 0xe0, PKT_TYPE_MAX); /* and    $0x7,%eax */
 +				break;
  			}
 +			case BPF_S_LD_W_ABS:
 +				func = CHOOSE_LOAD_FUNC(K, sk_load_word);
 +common_load:			seen |= SEEN_DATAREF;
 +				t_offset = func - (image + addrs[i]);
 +				EMIT1_off32(0xbe, K); /* mov imm32,%esi */
 +				EMIT1_off32(0xe8, t_offset); /* call */
 +				break;
 +			case BPF_S_LD_H_ABS:
 +				func = CHOOSE_LOAD_FUNC(K, sk_load_half);
 +				goto common_load;
 +			case BPF_S_LD_B_ABS:
 +				func = CHOOSE_LOAD_FUNC(K, sk_load_byte);
 +				goto common_load;
 +			case BPF_S_LDX_B_MSH:
 +				func = CHOOSE_LOAD_FUNC(K, sk_load_byte_msh);
 +				seen |= SEEN_DATAREF | SEEN_XREG;
 +				t_offset = func - (image + addrs[i]);
 +				EMIT1_off32(0xbe, K);	/* mov imm32,%esi */
 +				EMIT1_off32(0xe8, t_offset); /* call sk_load_byte_msh */
 +				break;
 +			case BPF_S_LD_W_IND:
 +				func = sk_load_word;
 +common_load_ind:		seen |= SEEN_DATAREF | SEEN_XREG;
 +				t_offset = func - (image + addrs[i]);
 +				if (K) {
 +					if (is_imm8(K)) {
 +						EMIT3(0x8d, 0x73, K); /* lea imm8(%rbx), %esi */
 +					} else {
 +						EMIT2(0x8d, 0xb3); /* lea imm32(%rbx),%esi */
 +						EMIT(K, 4);
 +					}
 +				} else {
 +					EMIT2(0x89,0xde); /* mov %ebx,%esi */
 +				}
 +				EMIT1_off32(0xe8, t_offset);	/* call sk_load_xxx_ind */
 +				break;
 +			case BPF_S_LD_H_IND:
 +				func = sk_load_half;
 +				goto common_load_ind;
 +			case BPF_S_LD_B_IND:
 +				func = sk_load_byte;
 +				goto common_load_ind;
 +			case BPF_S_JMP_JA:
 +				t_offset = addrs[i + K] - addrs[i];
 +				EMIT_JMP(t_offset);
 +				break;
 +			COND_SEL(BPF_S_JMP_JGT_K, X86_JA, X86_JBE);
 +			COND_SEL(BPF_S_JMP_JGE_K, X86_JAE, X86_JB);
 +			COND_SEL(BPF_S_JMP_JEQ_K, X86_JE, X86_JNE);
 +			COND_SEL(BPF_S_JMP_JSET_K,X86_JNE, X86_JE);
 +			COND_SEL(BPF_S_JMP_JGT_X, X86_JA, X86_JBE);
 +			COND_SEL(BPF_S_JMP_JGE_X, X86_JAE, X86_JB);
 +			COND_SEL(BPF_S_JMP_JEQ_X, X86_JE, X86_JNE);
 +			COND_SEL(BPF_S_JMP_JSET_X,X86_JNE, X86_JE);
 +
 +cond_branch:			f_offset = addrs[i + filter[i].jf] - addrs[i];
 +				t_offset = addrs[i + filter[i].jt] - addrs[i];
 +
 +				/* same targets, can avoid doing the test :) */
 +				if (filter[i].jt == filter[i].jf) {
 +					EMIT_JMP(t_offset);
 +					break;
 +				}
 +
 +				switch (filter[i].code) {
 +				case BPF_S_JMP_JGT_X:
 +				case BPF_S_JMP_JGE_X:
 +				case BPF_S_JMP_JEQ_X:
 +					seen |= SEEN_XREG;
 +					EMIT2(0x39, 0xd8); /* cmp %ebx,%eax */
 +					break;
 +				case BPF_S_JMP_JSET_X:
 +					seen |= SEEN_XREG;
 +					EMIT2(0x85, 0xd8); /* test %ebx,%eax */
 +					break;
 +				case BPF_S_JMP_JEQ_K:
 +					if (K == 0) {
 +						EMIT2(0x85, 0xc0); /* test   %eax,%eax */
 +						break;
 +					}
 +				case BPF_S_JMP_JGT_K:
 +				case BPF_S_JMP_JGE_K:
 +					if (K <= 127)
 +						EMIT3(0x83, 0xf8, K); /* cmp imm8,%eax */
 +					else
 +						EMIT1_off32(0x3d, K); /* cmp imm32,%eax */
 +					break;
 +				case BPF_S_JMP_JSET_K:
 +					if (K <= 0xFF)
 +						EMIT2(0xa8, K); /* test imm8,%al */
 +					else if (!(K & 0xFFFF00FF))
 +						EMIT3(0xf6, 0xc4, K >> 8); /* test imm8,%ah */
 +					else if (K <= 0xFFFF) {
 +						EMIT2(0x66, 0xa9); /* test imm16,%ax */
 +						EMIT(K, 2);
 +					} else {
 +						EMIT1_off32(0xa9, K); /* test imm32,%eax */
 +					}
 +					break;
 +				}
 +				if (filter[i].jt != 0) {
 +					if (filter[i].jf && f_offset)
 +						t_offset += is_near(f_offset) ? 2 : 5;
 +					EMIT_COND_JMP(t_op, t_offset);
 +					if (filter[i].jf)
 +						EMIT_JMP(f_offset);
 +					break;
 +				}
 +				EMIT_COND_JMP(f_op, f_offset);
 +				break;
 +			default:
 +				/* hmm, too complex filter, give up with jit compiler */
 +				goto out;
 +			}
 +			ilen = prog - temp;
 +			if (image) {
 +				if (unlikely(proglen + ilen > oldproglen)) {
 +					pr_err("bpb_jit_compile fatal error\n");
 +					kfree(addrs);
 +					module_free(NULL, image);
 +					return;
 +				}
 +				memcpy(image + proglen, temp, ilen);
 +			}
 +			proglen += ilen;
 +			addrs[i] = proglen;
 +			prog = temp;
 +		}
 +		/* last bpf instruction is always a RET :
 +		 * use it to give the cleanup instruction(s) addr
 +		 */
 +		cleanup_addr = proglen - 1; /* ret */
 +		if (seen_or_pass0)
 +			cleanup_addr -= 1; /* leaveq */
 +		if (seen_or_pass0 & SEEN_XREG)
 +			cleanup_addr -= 4; /* mov  -8(%rbp),%rbx */
 +
 +		if (image) {
 +			if (proglen != oldproglen)
 +				pr_err("bpb_jit_compile proglen=%u != oldproglen=%u\n", proglen, oldproglen);
  			break;
  		}
  		if (proglen == oldproglen) {
* Unmerged path arch/x86/net/bpf_jit_comp.c
