scsi: mpt3sas: fix dma_addr_t casts

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
Rebuild_CHGLOG: - [scsi] mpt3sas: fix dma_addr_t casts (Tomas Henzl) [1513855]
Rebuild_FUZZ: 90.62%
commit-author Arnd Bergmann <arnd@arndb.de>
commit d8335ae2b453f9efb2e88b71ae3cde0c010a5ac0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/d8335ae2.failed

The newly added base_make_prp_nvme function triggers a build warning on
some 32-bit configurations:

drivers/scsi/mpt3sas/mpt3sas_base.c: In function 'base_make_prp_nvme':
drivers/scsi/mpt3sas/mpt3sas_base.c:1664:13: error: cast from pointer to integer of different size [-Werror=pointer-to-int-cast]
  msg_phys = (dma_addr_t)mpt3sas_base_get_pcie_sgl_dma(ioc, smid);

After taking a closer look, I found that the problem is that the new
code mixes up pointers and dma_addr_t values unnecessarily.

This changes it to use the correct types consistently, which lets us get
rid of a lot of type casts in the process. I'm also renaming some
variables to avoid confusion between physical and dma address spaces
that are often distinct.

Fixes: 016d5c35e278 ("scsi: mpt3sas: SGL to PRP Translation for I/Os to NVMe devices")
	Signed-off-by: Arnd Bergmann <arnd@arndb.de>
	Acked-by: Sathya Prakash Veerichetty <sathya.prakash@broadcom.com>
	Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
(cherry picked from commit d8335ae2b453f9efb2e88b71ae3cde0c010a5ac0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/scsi/mpt3sas/mpt3sas_base.c
#	drivers/scsi/mpt3sas/mpt3sas_base.h
diff --cc drivers/scsi/mpt3sas/mpt3sas_base.c
index 028ee6d296e5,a29534c1824e..000000000000
--- a/drivers/scsi/mpt3sas/mpt3sas_base.c
+++ b/drivers/scsi/mpt3sas/mpt3sas_base.c
@@@ -1367,6 -1375,433 +1367,436 @@@ _base_build_sg(struct MPT3SAS_ADAPTER *
  /* IEEE format sgls */
  
  /**
++<<<<<<< HEAD
++=======
+  * _base_build_nvme_prp - This function is called for NVMe end devices to build
+  * a native SGL (NVMe PRP). The native SGL is built starting in the first PRP
+  * entry of the NVMe message (PRP1).  If the data buffer is small enough to be
+  * described entirely using PRP1, then PRP2 is not used.  If needed, PRP2 is
+  * used to describe a larger data buffer.  If the data buffer is too large to
+  * describe using the two PRP entriess inside the NVMe message, then PRP1
+  * describes the first data memory segment, and PRP2 contains a pointer to a PRP
+  * list located elsewhere in memory to describe the remaining data memory
+  * segments.  The PRP list will be contiguous.
+ 
+  * The native SGL for NVMe devices is a Physical Region Page (PRP).  A PRP
+  * consists of a list of PRP entries to describe a number of noncontigous
+  * physical memory segments as a single memory buffer, just as a SGL does.  Note
+  * however, that this function is only used by the IOCTL call, so the memory
+  * given will be guaranteed to be contiguous.  There is no need to translate
+  * non-contiguous SGL into a PRP in this case.  All PRPs will describe
+  * contiguous space that is one page size each.
+  *
+  * Each NVMe message contains two PRP entries.  The first (PRP1) either contains
+  * a PRP list pointer or a PRP element, depending upon the command.  PRP2
+  * contains the second PRP element if the memory being described fits within 2
+  * PRP entries, or a PRP list pointer if the PRP spans more than two entries.
+  *
+  * A PRP list pointer contains the address of a PRP list, structured as a linear
+  * array of PRP entries.  Each PRP entry in this list describes a segment of
+  * physical memory.
+  *
+  * Each 64-bit PRP entry comprises an address and an offset field.  The address
+  * always points at the beginning of a 4KB physical memory page, and the offset
+  * describes where within that 4KB page the memory segment begins.  Only the
+  * first element in a PRP list may contain a non-zero offest, implying that all
+  * memory segments following the first begin at the start of a 4KB page.
+  *
+  * Each PRP element normally describes 4KB of physical memory, with exceptions
+  * for the first and last elements in the list.  If the memory being described
+  * by the list begins at a non-zero offset within the first 4KB page, then the
+  * first PRP element will contain a non-zero offset indicating where the region
+  * begins within the 4KB page.  The last memory segment may end before the end
+  * of the 4KB segment, depending upon the overall size of the memory being
+  * described by the PRP list.
+  *
+  * Since PRP entries lack any indication of size, the overall data buffer length
+  * is used to determine where the end of the data memory buffer is located, and
+  * how many PRP entries are required to describe it.
+  *
+  * @ioc: per adapter object
+  * @smid: system request message index for getting asscociated SGL
+  * @nvme_encap_request: the NVMe request msg frame pointer
+  * @data_out_dma: physical address for WRITES
+  * @data_out_sz: data xfer size for WRITES
+  * @data_in_dma: physical address for READS
+  * @data_in_sz: data xfer size for READS
+  *
+  * Returns nothing.
+  */
+ static void
+ _base_build_nvme_prp(struct MPT3SAS_ADAPTER *ioc, u16 smid,
+ 	Mpi26NVMeEncapsulatedRequest_t *nvme_encap_request,
+ 	dma_addr_t data_out_dma, size_t data_out_sz, dma_addr_t data_in_dma,
+ 	size_t data_in_sz)
+ {
+ 	int		prp_size = NVME_PRP_SIZE;
+ 	__le64		*prp_entry, *prp1_entry, *prp2_entry;
+ 	__le64		*prp_page;
+ 	dma_addr_t	prp_entry_dma, prp_page_dma, dma_addr;
+ 	u32		offset, entry_len;
+ 	u32		page_mask_result, page_mask;
+ 	size_t		length;
+ 
+ 	/*
+ 	 * Not all commands require a data transfer. If no data, just return
+ 	 * without constructing any PRP.
+ 	 */
+ 	if (!data_in_sz && !data_out_sz)
+ 		return;
+ 	/*
+ 	 * Set pointers to PRP1 and PRP2, which are in the NVMe command.
+ 	 * PRP1 is located at a 24 byte offset from the start of the NVMe
+ 	 * command.  Then set the current PRP entry pointer to PRP1.
+ 	 */
+ 	prp1_entry = (__le64 *)(nvme_encap_request->NVMe_Command +
+ 	    NVME_CMD_PRP1_OFFSET);
+ 	prp2_entry = (__le64 *)(nvme_encap_request->NVMe_Command +
+ 	    NVME_CMD_PRP2_OFFSET);
+ 	prp_entry = prp1_entry;
+ 	/*
+ 	 * For the PRP entries, use the specially allocated buffer of
+ 	 * contiguous memory.
+ 	 */
+ 	prp_page = (__le64 *)mpt3sas_base_get_pcie_sgl(ioc, smid);
+ 	prp_page_dma = mpt3sas_base_get_pcie_sgl_dma(ioc, smid);
+ 
+ 	/*
+ 	 * Check if we are within 1 entry of a page boundary we don't
+ 	 * want our first entry to be a PRP List entry.
+ 	 */
+ 	page_mask = ioc->page_size - 1;
+ 	page_mask_result = (uintptr_t)((u8 *)prp_page + prp_size) & page_mask;
+ 	if (!page_mask_result) {
+ 		/* Bump up to next page boundary. */
+ 		prp_page = (__le64 *)((u8 *)prp_page + prp_size);
+ 		prp_page_dma = prp_page_dma + prp_size;
+ 	}
+ 
+ 	/*
+ 	 * Set PRP physical pointer, which initially points to the current PRP
+ 	 * DMA memory page.
+ 	 */
+ 	prp_entry_dma = prp_page_dma;
+ 
+ 	/* Get physical address and length of the data buffer. */
+ 	if (data_in_sz) {
+ 		dma_addr = data_in_dma;
+ 		length = data_in_sz;
+ 	} else {
+ 		dma_addr = data_out_dma;
+ 		length = data_out_sz;
+ 	}
+ 
+ 	/* Loop while the length is not zero. */
+ 	while (length) {
+ 		/*
+ 		 * Check if we need to put a list pointer here if we are at
+ 		 * page boundary - prp_size (8 bytes).
+ 		 */
+ 		page_mask_result = (prp_entry_dma + prp_size) & page_mask;
+ 		if (!page_mask_result) {
+ 			/*
+ 			 * This is the last entry in a PRP List, so we need to
+ 			 * put a PRP list pointer here.  What this does is:
+ 			 *   - bump the current memory pointer to the next
+ 			 *     address, which will be the next full page.
+ 			 *   - set the PRP Entry to point to that page.  This
+ 			 *     is now the PRP List pointer.
+ 			 *   - bump the PRP Entry pointer the start of the
+ 			 *     next page.  Since all of this PRP memory is
+ 			 *     contiguous, no need to get a new page - it's
+ 			 *     just the next address.
+ 			 */
+ 			prp_entry_dma++;
+ 			*prp_entry = cpu_to_le64(prp_entry_dma);
+ 			prp_entry++;
+ 		}
+ 
+ 		/* Need to handle if entry will be part of a page. */
+ 		offset = dma_addr & page_mask;
+ 		entry_len = ioc->page_size - offset;
+ 
+ 		if (prp_entry == prp1_entry) {
+ 			/*
+ 			 * Must fill in the first PRP pointer (PRP1) before
+ 			 * moving on.
+ 			 */
+ 			*prp1_entry = cpu_to_le64(dma_addr);
+ 
+ 			/*
+ 			 * Now point to the second PRP entry within the
+ 			 * command (PRP2).
+ 			 */
+ 			prp_entry = prp2_entry;
+ 		} else if (prp_entry == prp2_entry) {
+ 			/*
+ 			 * Should the PRP2 entry be a PRP List pointer or just
+ 			 * a regular PRP pointer?  If there is more than one
+ 			 * more page of data, must use a PRP List pointer.
+ 			 */
+ 			if (length > ioc->page_size) {
+ 				/*
+ 				 * PRP2 will contain a PRP List pointer because
+ 				 * more PRP's are needed with this command. The
+ 				 * list will start at the beginning of the
+ 				 * contiguous buffer.
+ 				 */
+ 				*prp2_entry = cpu_to_le64(prp_entry_dma);
+ 
+ 				/*
+ 				 * The next PRP Entry will be the start of the
+ 				 * first PRP List.
+ 				 */
+ 				prp_entry = prp_page;
+ 			} else {
+ 				/*
+ 				 * After this, the PRP Entries are complete.
+ 				 * This command uses 2 PRP's and no PRP list.
+ 				 */
+ 				*prp2_entry = cpu_to_le64(dma_addr);
+ 			}
+ 		} else {
+ 			/*
+ 			 * Put entry in list and bump the addresses.
+ 			 *
+ 			 * After PRP1 and PRP2 are filled in, this will fill in
+ 			 * all remaining PRP entries in a PRP List, one per
+ 			 * each time through the loop.
+ 			 */
+ 			*prp_entry = cpu_to_le64(dma_addr);
+ 			prp_entry++;
+ 			prp_entry_dma++;
+ 		}
+ 
+ 		/*
+ 		 * Bump the phys address of the command's data buffer by the
+ 		 * entry_len.
+ 		 */
+ 		dma_addr += entry_len;
+ 
+ 		/* Decrement length accounting for last partial page. */
+ 		if (entry_len > length)
+ 			length = 0;
+ 		else
+ 			length -= entry_len;
+ 	}
+ }
+ 
+ /**
+  * base_make_prp_nvme -
+  * Prepare PRPs(Physical Region Page)- SGLs specific to NVMe drives only
+  *
+  * @ioc:		per adapter object
+  * @scmd:		SCSI command from the mid-layer
+  * @mpi_request:	mpi request
+  * @smid:		msg Index
+  * @sge_count:		scatter gather element count.
+  *
+  * Returns:		true: PRPs are built
+  *			false: IEEE SGLs needs to be built
+  */
+ static void
+ base_make_prp_nvme(struct MPT3SAS_ADAPTER *ioc,
+ 		struct scsi_cmnd *scmd,
+ 		Mpi25SCSIIORequest_t *mpi_request,
+ 		u16 smid, int sge_count)
+ {
+ 	int sge_len, num_prp_in_chain = 0;
+ 	Mpi25IeeeSgeChain64_t *main_chain_element, *ptr_first_sgl;
+ 	__le64 *curr_buff;
+ 	dma_addr_t msg_dma, sge_addr, offset;
+ 	u32 page_mask, page_mask_result;
+ 	struct scatterlist *sg_scmd;
+ 	u32 first_prp_len;
+ 	int data_len = scsi_bufflen(scmd);
+ 	u32 nvme_pg_size;
+ 
+ 	nvme_pg_size = max_t(u32, ioc->page_size, NVME_PRP_PAGE_SIZE);
+ 	/*
+ 	 * Nvme has a very convoluted prp format.  One prp is required
+ 	 * for each page or partial page. Driver need to split up OS sg_list
+ 	 * entries if it is longer than one page or cross a page
+ 	 * boundary.  Driver also have to insert a PRP list pointer entry as
+ 	 * the last entry in each physical page of the PRP list.
+ 	 *
+ 	 * NOTE: The first PRP "entry" is actually placed in the first
+ 	 * SGL entry in the main message as IEEE 64 format.  The 2nd
+ 	 * entry in the main message is the chain element, and the rest
+ 	 * of the PRP entries are built in the contiguous pcie buffer.
+ 	 */
+ 	page_mask = nvme_pg_size - 1;
+ 
+ 	/*
+ 	 * Native SGL is needed.
+ 	 * Put a chain element in main message frame that points to the first
+ 	 * chain buffer.
+ 	 *
+ 	 * NOTE:  The ChainOffset field must be 0 when using a chain pointer to
+ 	 *        a native SGL.
+ 	 */
+ 
+ 	/* Set main message chain element pointer */
+ 	main_chain_element = (pMpi25IeeeSgeChain64_t)&mpi_request->SGL;
+ 	/*
+ 	 * For NVMe the chain element needs to be the 2nd SG entry in the main
+ 	 * message.
+ 	 */
+ 	main_chain_element = (Mpi25IeeeSgeChain64_t *)
+ 		((u8 *)main_chain_element + sizeof(MPI25_IEEE_SGE_CHAIN64));
+ 
+ 	/*
+ 	 * For the PRP entries, use the specially allocated buffer of
+ 	 * contiguous memory.  Normal chain buffers can't be used
+ 	 * because each chain buffer would need to be the size of an OS
+ 	 * page (4k).
+ 	 */
+ 	curr_buff = mpt3sas_base_get_pcie_sgl(ioc, smid);
+ 	msg_dma = mpt3sas_base_get_pcie_sgl_dma(ioc, smid);
+ 
+ 	main_chain_element->Address = cpu_to_le64(msg_dma);
+ 	main_chain_element->NextChainOffset = 0;
+ 	main_chain_element->Flags = MPI2_IEEE_SGE_FLAGS_CHAIN_ELEMENT |
+ 			MPI2_IEEE_SGE_FLAGS_SYSTEM_ADDR |
+ 			MPI26_IEEE_SGE_FLAGS_NSF_NVME_PRP;
+ 
+ 	/* Build first prp, sge need not to be page aligned*/
+ 	ptr_first_sgl = (pMpi25IeeeSgeChain64_t)&mpi_request->SGL;
+ 	sg_scmd = scsi_sglist(scmd);
+ 	sge_addr = sg_dma_address(sg_scmd);
+ 	sge_len = sg_dma_len(sg_scmd);
+ 
+ 	offset = sge_addr & page_mask;
+ 	first_prp_len = nvme_pg_size - offset;
+ 
+ 	ptr_first_sgl->Address = cpu_to_le64(sge_addr);
+ 	ptr_first_sgl->Length = cpu_to_le32(first_prp_len);
+ 
+ 	data_len -= first_prp_len;
+ 
+ 	if (sge_len > first_prp_len) {
+ 		sge_addr += first_prp_len;
+ 		sge_len -= first_prp_len;
+ 	} else if (data_len && (sge_len == first_prp_len)) {
+ 		sg_scmd = sg_next(sg_scmd);
+ 		sge_addr = sg_dma_address(sg_scmd);
+ 		sge_len = sg_dma_len(sg_scmd);
+ 	}
+ 
+ 	for (;;) {
+ 		offset = sge_addr & page_mask;
+ 
+ 		/* Put PRP pointer due to page boundary*/
+ 		page_mask_result = (uintptr_t)(curr_buff + 1) & page_mask;
+ 		if (unlikely(!page_mask_result)) {
+ 			scmd_printk(KERN_NOTICE,
+ 				scmd, "page boundary curr_buff: 0x%p\n",
+ 				curr_buff);
+ 			msg_dma += 8;
+ 			*curr_buff = cpu_to_le64(msg_dma);
+ 			curr_buff++;
+ 			num_prp_in_chain++;
+ 		}
+ 
+ 		*curr_buff = cpu_to_le64(sge_addr);
+ 		curr_buff++;
+ 		msg_dma += 8;
+ 		num_prp_in_chain++;
+ 
+ 		sge_addr += nvme_pg_size;
+ 		sge_len -= nvme_pg_size;
+ 		data_len -= nvme_pg_size;
+ 
+ 		if (data_len <= 0)
+ 			break;
+ 
+ 		if (sge_len > 0)
+ 			continue;
+ 
+ 		sg_scmd = sg_next(sg_scmd);
+ 		sge_addr = sg_dma_address(sg_scmd);
+ 		sge_len = sg_dma_len(sg_scmd);
+ 	}
+ 
+ 	main_chain_element->Length =
+ 		cpu_to_le32(num_prp_in_chain * sizeof(u64));
+ 	return;
+ }
+ 
+ static bool
+ base_is_prp_possible(struct MPT3SAS_ADAPTER *ioc,
+ 	struct _pcie_device *pcie_device, struct scsi_cmnd *scmd, int sge_count)
+ {
+ 	u32 data_length = 0;
+ 	struct scatterlist *sg_scmd;
+ 	bool build_prp = true;
+ 
+ 	data_length = scsi_bufflen(scmd);
+ 	sg_scmd = scsi_sglist(scmd);
+ 
+ 	/* If Datalenth is <= 16K and number of SGEâ€™s entries are <= 2
+ 	 * we built IEEE SGL
+ 	 */
+ 	if ((data_length <= NVME_PRP_PAGE_SIZE*4) && (sge_count <= 2))
+ 		build_prp = false;
+ 
+ 	return build_prp;
+ }
+ 
+ /**
+  * _base_check_pcie_native_sgl - This function is called for PCIe end devices to
+  * determine if the driver needs to build a native SGL.  If so, that native
+  * SGL is built in the special contiguous buffers allocated especially for
+  * PCIe SGL creation.  If the driver will not build a native SGL, return
+  * TRUE and a normal IEEE SGL will be built.  Currently this routine
+  * supports NVMe.
+  * @ioc: per adapter object
+  * @mpi_request: mf request pointer
+  * @smid: system request message index
+  * @scmd: scsi command
+  * @pcie_device: points to the PCIe device's info
+  *
+  * Returns 0 if native SGL was built, 1 if no SGL was built
+  */
+ static int
+ _base_check_pcie_native_sgl(struct MPT3SAS_ADAPTER *ioc,
+ 	Mpi25SCSIIORequest_t *mpi_request, u16 smid, struct scsi_cmnd *scmd,
+ 	struct _pcie_device *pcie_device)
+ {
+ 	struct scatterlist *sg_scmd;
+ 	int sges_left;
+ 
+ 	/* Get the SG list pointer and info. */
+ 	sg_scmd = scsi_sglist(scmd);
+ 	sges_left = scsi_dma_map(scmd);
+ 	if (sges_left < 0) {
+ 		sdev_printk(KERN_ERR, scmd->device,
+ 			"scsi_dma_map failed: request for %d bytes!\n",
+ 			scsi_bufflen(scmd));
+ 		return 1;
+ 	}
+ 
+ 	/* Check if we need to build a native SG list. */
+ 	if (base_is_prp_possible(ioc, pcie_device,
+ 				scmd, sges_left) == 0) {
+ 		/* We built a native SG list, just return. */
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * Build native NVMe PRP.
+ 	 */
+ 	base_make_prp_nvme(ioc, scmd, mpi_request,
+ 			smid, sges_left);
+ 
+ 	return 0;
+ out:
+ 	scsi_dma_unmap(scmd);
+ 	return 1;
+ }
+ 
+ /**
++>>>>>>> d8335ae2b453 (scsi: mpt3sas: fix dma_addr_t casts)
   * _base_add_sg_single_ieee - add sg element for IEEE format
   * @paddr: virtual address for SGE
   * @flags: SGE flags
@@@ -2290,6 -2733,32 +2720,35 @@@ mpt3sas_base_get_sense_buffer_dma(struc
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * mpt3sas_base_get_pcie_sgl - obtain a PCIe SGL virt addr
+  * @ioc: per adapter object
+  * @smid: system request message index
+  *
+  * Returns virt pointer to a PCIe SGL.
+  */
+ void *
+ mpt3sas_base_get_pcie_sgl(struct MPT3SAS_ADAPTER *ioc, u16 smid)
+ {
+ 	return (void *)(ioc->scsi_lookup[smid - 1].pcie_sg_list.pcie_sgl);
+ }
+ 
+ /**
+  * mpt3sas_base_get_pcie_sgl_dma - obtain a PCIe SGL dma addr
+  * @ioc: per adapter object
+  * @smid: system request message index
+  *
+  * Returns phys pointer to the address of the PCIe buffer.
+  */
+ dma_addr_t
+ mpt3sas_base_get_pcie_sgl_dma(struct MPT3SAS_ADAPTER *ioc, u16 smid)
+ {
+ 	return ioc->scsi_lookup[smid - 1].pcie_sg_list.pcie_sgl_dma;
+ }
+ 
+ /**
++>>>>>>> d8335ae2b453 (scsi: mpt3sas: fix dma_addr_t casts)
   * mpt3sas_base_get_reply_virt_addr - obtain reply frames virt address
   * @ioc: per adapter object
   * @phys_addr: lower 32 physical addr of the reply
diff --cc drivers/scsi/mpt3sas/mpt3sas_base.h
index 59e1477b2da9,60f42ca3954f..000000000000
--- a/drivers/scsi/mpt3sas/mpt3sas_base.h
+++ b/drivers/scsi/mpt3sas/mpt3sas_base.h
@@@ -1350,15 -1394,10 +1350,20 @@@ void *mpt3sas_base_get_msg_frame(struc
  void *mpt3sas_base_get_sense_buffer(struct MPT3SAS_ADAPTER *ioc, u16 smid);
  __le32 mpt3sas_base_get_sense_buffer_dma(struct MPT3SAS_ADAPTER *ioc,
  	u16 smid);
++<<<<<<< HEAD
 +
++=======
+ void *mpt3sas_base_get_pcie_sgl(struct MPT3SAS_ADAPTER *ioc, u16 smid);
+ dma_addr_t mpt3sas_base_get_pcie_sgl_dma(struct MPT3SAS_ADAPTER *ioc, u16 smid);
++>>>>>>> d8335ae2b453 (scsi: mpt3sas: fix dma_addr_t casts)
  void mpt3sas_base_sync_reply_irqs(struct MPT3SAS_ADAPTER *ioc);
  
 +void mpt3sas_base_put_smid_fast_path(struct MPT3SAS_ADAPTER *ioc, u16 smid,
 +	u16 handle);
 +void mpt3sas_base_put_smid_hi_priority(struct MPT3SAS_ADAPTER *ioc, u16 smid,
 +	u16 msix_task);
 +void mpt3sas_base_put_smid_nvme_encap(struct MPT3SAS_ADAPTER *ioc, u16 smid);
 +void mpt3sas_base_put_smid_default(struct MPT3SAS_ADAPTER *ioc, u16 smid);
  /* hi-priority queue */
  u16 mpt3sas_base_get_smid_hpr(struct MPT3SAS_ADAPTER *ioc, u8 cb_idx);
  u16 mpt3sas_base_get_smid_scsiio(struct MPT3SAS_ADAPTER *ioc, u8 cb_idx,
* Unmerged path drivers/scsi/mpt3sas/mpt3sas_base.c
* Unmerged path drivers/scsi/mpt3sas/mpt3sas_base.h
