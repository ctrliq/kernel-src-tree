bpf, s390: fix jit branch offset related to ldimm64

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-1062.el7
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit b0a0c2566f28e71e5e32121992ac8060cec75510
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1062.el7/b0a0c256.failed

While testing some other work that required JIT modifications, I
run into test_bpf causing a hang when JIT enabled on s390. The
problematic test case was the one from ddc665a4bb4b (bpf, arm64:
fix jit branch offset related to ldimm64), and turns out that we
do have a similar issue on s390 as well. In bpf_jit_prog() we
update next instruction address after returning from bpf_jit_insn()
with an insn_count. bpf_jit_insn() returns either -1 in case of
error (e.g. unsupported insn), 1 or 2. The latter is only the
case for ldimm64 due to spanning 2 insns, however, next address
is only set to i + 1 not taking actual insn_count into account,
thus fix is to use insn_count instead of 1. bpf_jit_enable in
mode 2 provides also disasm on s390:

Before fix:

  000003ff800349b6: a7f40003   brc     15,3ff800349bc                 ; target
  000003ff800349ba: 0000               unknown
  000003ff800349bc: e3b0f0700024       stg     %r11,112(%r15)
  000003ff800349c2: e3e0f0880024       stg     %r14,136(%r15)
  000003ff800349c8: 0db0               basr    %r11,%r0
  000003ff800349ca: c0ef00000000       llilf   %r14,0
  000003ff800349d0: e320b0360004       lg      %r2,54(%r11)
  000003ff800349d6: e330b03e0004       lg      %r3,62(%r11)
  000003ff800349dc: ec23ffeda065       clgrj   %r2,%r3,10,3ff800349b6 ; jmp
  000003ff800349e2: e3e0b0460004       lg      %r14,70(%r11)
  000003ff800349e8: e3e0b04e0004       lg      %r14,78(%r11)
  000003ff800349ee: b904002e   lgr     %r2,%r14
  000003ff800349f2: e3b0f0700004       lg      %r11,112(%r15)
  000003ff800349f8: e3e0f0880004       lg      %r14,136(%r15)
  000003ff800349fe: 07fe               bcr     15,%r14

After fix:

  000003ff80ef3db4: a7f40003   brc     15,3ff80ef3dba
  000003ff80ef3db8: 0000               unknown
  000003ff80ef3dba: e3b0f0700024       stg     %r11,112(%r15)
  000003ff80ef3dc0: e3e0f0880024       stg     %r14,136(%r15)
  000003ff80ef3dc6: 0db0               basr    %r11,%r0
  000003ff80ef3dc8: c0ef00000000       llilf   %r14,0
  000003ff80ef3dce: e320b0360004       lg      %r2,54(%r11)
  000003ff80ef3dd4: e330b03e0004       lg      %r3,62(%r11)
  000003ff80ef3dda: ec230006a065       clgrj   %r2,%r3,10,3ff80ef3de6 ; jmp
  000003ff80ef3de0: e3e0b0460004       lg      %r14,70(%r11)
  000003ff80ef3de6: e3e0b04e0004       lg      %r14,78(%r11)          ; target
  000003ff80ef3dec: b904002e   lgr     %r2,%r14
  000003ff80ef3df0: e3b0f0700004       lg      %r11,112(%r15)
  000003ff80ef3df6: e3e0f0880004       lg      %r14,136(%r15)
  000003ff80ef3dfc: 07fe               bcr     15,%r14

test_bpf.ko suite runs fine after the fix.

Fixes: 054623105728 ("s390/bpf: Add s390x eBPF JIT compiler backend")
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Tested-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit b0a0c2566f28e71e5e32121992ac8060cec75510)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/net/bpf_jit_comp.c
diff --cc arch/s390/net/bpf_jit_comp.c
index 15be4f92e665,1803797fc885..000000000000
--- a/arch/s390/net/bpf_jit_comp.c
+++ b/arch/s390/net/bpf_jit_comp.c
@@@ -698,187 -1067,275 +698,252 @@@ call_fn:	/* lg %r1,<d(function)>(%r13) 
  		/* j <exit> */
  		EMIT4_PCREL(0xa7f40000, jit->exit_ip - jit->prg);
  		break;
 -	/*
 -	 * Branch relative (number of skipped instructions) to offset on
 -	 * condition.
 -	 *
 -	 * Condition code to mask mapping:
 -	 *
 -	 * CC | Description	   | Mask
 -	 * ------------------------------
 -	 * 0  | Operands equal	   |	8
 -	 * 1  | First operand low  |	4
 -	 * 2  | First operand high |	2
 -	 * 3  | Unused		   |	1
 -	 *
 -	 * For s390x relative branches: ip = ip + off_bytes
 -	 * For BPF relative branches:	insn = insn + off_insns + 1
 -	 *
 -	 * For example for s390x with offset 0 we jump to the branch
 -	 * instruction itself (loop) and for BPF with offset 0 we
 -	 * branch to the instruction behind the branch.
 -	 */
 -	case BPF_JMP | BPF_JA: /* if (true) */
 -		mask = 0xf000; /* j */
 -		goto branch_oc;
 -	case BPF_JMP | BPF_JSGT | BPF_K: /* ((s64) dst > (s64) imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JSGE | BPF_K: /* ((s64) dst >= (s64) imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ks;
 -	case BPF_JMP | BPF_JGT | BPF_K: /* (dst_reg > imm) */
 -		mask = 0x2000; /* jh */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JGE | BPF_K: /* (dst_reg >= imm) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JNE | BPF_K: /* (dst_reg != imm) */
 -		mask = 0x7000; /* jne */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JEQ | BPF_K: /* (dst_reg == imm) */
 -		mask = 0x8000; /* je */
 -		goto branch_ku;
 -	case BPF_JMP | BPF_JSET | BPF_K: /* (dst_reg & imm) */
 -		mask = 0x7000; /* jnz */
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* ngr %w1,%dst */
 -		EMIT4(0xb9800000, REG_W1, dst_reg);
 -		goto branch_oc;
 -
 -	case BPF_JMP | BPF_JSGT | BPF_X: /* ((s64) dst > (s64) src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JSGE | BPF_X: /* ((s64) dst >= (s64) src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xs;
 -	case BPF_JMP | BPF_JGT | BPF_X: /* (dst > src) */
 -		mask = 0x2000; /* jh */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JGE | BPF_X: /* (dst >= src) */
 -		mask = 0xa000; /* jhe */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JNE | BPF_X: /* (dst != src) */
 -		mask = 0x7000; /* jne */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JEQ | BPF_X: /* (dst == src) */
 -		mask = 0x8000; /* je */
 -		goto branch_xu;
 -	case BPF_JMP | BPF_JSET | BPF_X: /* (dst & src) */
 -		mask = 0x7000; /* jnz */
 -		/* ngrk %w1,%dst,%src */
 -		EMIT4_RRF(0xb9e40000, REG_W1, dst_reg, src_reg);
 -		goto branch_oc;
 -branch_ks:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* cgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_ku:
 -		/* lgfi %w1,imm (load sign extend imm) */
 -		EMIT6_IMM(0xc0010000, REG_W1, imm);
 -		/* clgrj %dst,%w1,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, REG_W1, i, off, mask);
 -		break;
 -branch_xs:
 -		/* cgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0064, dst_reg, src_reg, i, off, mask);
 -		break;
 -branch_xu:
 -		/* clgrj %dst,%src,mask,off */
 -		EMIT6_PCREL(0xec000000, 0x0065, dst_reg, src_reg, i, off, mask);
 -		break;
 -branch_oc:
 -		/* brc mask,jmp_off (branch instruction needs 4 bytes) */
 -		jmp_off = addrs[i + off + 1] - (addrs[i + 1] - 4);
 -		EMIT4_PCREL(0xa7040000 | mask << 8, jmp_off);
 +	case BPF_S_ST: /* mem[K] = A */
 +		jit->seen |= SEEN_MEM;
 +		/* st %r5,<K>(%r15) */
 +		EMIT4_DISP(0x5050f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_STX: /* mem[K] = X : mov %ebx,off8(%rbp) */
 +		jit->seen |= SEEN_XREG | SEEN_MEM;
 +		/* st %r12,<K>(%r15) */
 +		EMIT4_DISP(0x50c0f000,
 +			   (jit->seen & SEEN_DATAREF) ? 160 + K*4 : K*4);
 +		break;
 +	case BPF_S_ANC_PROTOCOL: /* A = ntohs(skb->protocol); */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(protocol)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, protocol));
 +		break;
 +	case BPF_S_ANC_IFINDEX:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->ifindex */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* l %r5,<d(ifindex)>(%r1) */
 +		EMIT4_DISP(0x58501000, offsetof(struct net_device, ifindex));
 +		break;
 +	case BPF_S_ANC_MARK: /* A = skb->mark */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);
 +		/* l %r5,<d(mark)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, mark));
 +		break;
 +	case BPF_S_ANC_QUEUE: /* A = skb->queue_mapping */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(queue_mapping)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, queue_mapping));
 +		break;
 +	case BPF_S_ANC_HATYPE:	/* if (!skb->dev) return 0;
 +				 * A = skb->dev->type */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);
 +		jit->seen |= SEEN_RET0;
 +		/* lg %r1,<d(dev)>(%r2) */
 +		EMIT6_DISP(0xe3102000, 0x0004, offsetof(struct sk_buff, dev));
 +		/* ltgr %r1,%r1 */
 +		EMIT4(0xb9020011);
 +		/* jz <ret0> */
 +		EMIT4_PCREL(0xa7840000, jit->ret0_ip - jit->prg);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(type)>(%r1) */
 +		EMIT4_DISP(0xbf531000, offsetof(struct net_device, type));
 +		break;
 +	case BPF_S_ANC_RXHASH: /* A = skb->hash */
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);
 +		/* l %r5,<d(hash)>(%r2) */
 +		EMIT4_DISP(0x58502000, offsetof(struct sk_buff, hash));
 +		break;
 +	case BPF_S_ANC_VLAN_TAG:
 +	case BPF_S_ANC_VLAN_TAG_PRESENT:
 +		BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);
 +		BUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +		/* icm	%r5,3,<d(vlan_tci)>(%r2) */
 +		EMIT4_DISP(0xbf532000, offsetof(struct sk_buff, vlan_tci));
 +		if (filter->code == BPF_S_ANC_VLAN_TAG) {
 +			/* nill %r5,0xefff */
 +			EMIT4_IMM(0xa5570000, ~VLAN_TAG_PRESENT);
 +		} else {
 +			/* nill %r5,0x1000 */
 +			EMIT4_IMM(0xa5570000, VLAN_TAG_PRESENT);
 +			/* srl %r5,12 */
 +			EMIT4_DISP(0x88500000, 12);
 +		}
  		break;
 -	/*
 -	 * BPF_LD
 -	 */
 -	case BPF_LD | BPF_ABS | BPF_B: /* b0 = *(u8 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_B: /* b0 = *(u8 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_byte_pos);
 -		else
 -			func_addr = __pa(sk_load_byte);
 -		goto call_fn;
 -	case BPF_LD | BPF_ABS | BPF_H: /* b0 = *(u16 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_H: /* b0 = *(u16 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_half_pos);
 -		else
 -			func_addr = __pa(sk_load_half);
 -		goto call_fn;
 -	case BPF_LD | BPF_ABS | BPF_W: /* b0 = *(u32 *) (skb->data+imm) */
 -	case BPF_LD | BPF_IND | BPF_W: /* b0 = *(u32 *) (skb->data+imm+src) */
 -		if ((BPF_MODE(insn->code) == BPF_ABS) && (imm >= 0))
 -			func_addr = __pa(sk_load_word_pos);
 -		else
 -			func_addr = __pa(sk_load_word);
 -		goto call_fn;
 -call_fn:
 -		jit->seen |= SEEN_SKB | SEEN_RET0 | SEEN_FUNC;
 -		REG_SET_SEEN(REG_14); /* Return address of possible func call */
 -
 -		/*
 -		 * Implicit input:
 -		 *  BPF_REG_6	 (R7) : skb pointer
 -		 *  REG_SKB_DATA (R12): skb data pointer (if no BPF_REG_AX)
 -		 *
 -		 * Calculated input:
 -		 *  BPF_REG_2	 (R3) : offset of byte(s) to fetch in skb
 -		 *  BPF_REG_5	 (R6) : return address
 -		 *
 -		 * Output:
 -		 *  BPF_REG_0	 (R14): data read from skb
 -		 *
 -		 * Scratch registers (BPF_REG_1-5)
 -		 */
 -
 -		/* Call function: llilf %w1,func_addr  */
 -		EMIT6_IMM(0xc00f0000, REG_W1, func_addr);
 -
 -		/* Offset: lgfi %b2,imm */
 -		EMIT6_IMM(0xc0010000, BPF_REG_2, imm);
 -		if (BPF_MODE(insn->code) == BPF_IND)
 -			/* agfr %b2,%src (%src is s32 here) */
 -			EMIT4(0xb9180000, BPF_REG_2, src_reg);
 -
 -		/* Reload REG_SKB_DATA if BPF_REG_AX is used */
 -		if (jit->seen & SEEN_REG_AX)
 -			/* lg %skb_data,data_off(%b6) */
 -			EMIT6_DISP_LH(0xe3000000, 0x0004, REG_SKB_DATA, REG_0,
 -				      BPF_REG_6, offsetof(struct sk_buff, data));
 -		/* basr %b5,%w1 (%b5 is call saved) */
 -		EMIT2(0x0d00, BPF_REG_5, REG_W1);
 -
 -		/*
 -		 * Note: For fast access we jump directly after the
 -		 * jnz instruction from bpf_jit.S
 -		 */
 -		/* jnz <ret0> */
 -		EMIT4_PCREL(0xa7740000, jit->ret0_ip - jit->prg);
 +	case BPF_S_ANC_CPU: /* A = smp_processor_id() */
 +#ifdef CONFIG_SMP
 +		/* l %r5,<d(cpu_nr)> */
 +		EMIT4_DISP(0x58500000, offsetof(struct _lowcore, cpu_nr));
 +#else
 +		/* lhi %r5,0 */
 +		EMIT4(0xa7580000);
 +#endif
  		break;
  	default: /* too complex, give up */
++<<<<<<< HEAD
++=======
+ 		pr_err("Unknown opcode %02x\n", insn->code);
+ 		return -1;
+ 	}
+ 	return insn_count;
+ }
+ 
+ /*
+  * Compile eBPF program into s390x code
+  */
+ static int bpf_jit_prog(struct bpf_jit *jit, struct bpf_prog *fp)
+ {
+ 	int i, insn_count;
+ 
+ 	jit->lit = jit->lit_start;
+ 	jit->prg = 0;
+ 
+ 	bpf_jit_prologue(jit);
+ 	for (i = 0; i < fp->len; i += insn_count) {
+ 		insn_count = bpf_jit_insn(jit, fp, i);
+ 		if (insn_count < 0)
+ 			return -1;
+ 		/* Next instruction address */
+ 		jit->addrs[i + insn_count] = jit->prg;
+ 	}
+ 	bpf_jit_epilogue(jit);
+ 
+ 	jit->lit_start = jit->prg;
+ 	jit->size = jit->lit;
+ 	jit->size_prg = jit->prg;
+ 	return 0;
+ }
+ 
+ /*
+  * Compile eBPF program "fp"
+  */
+ struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *fp)
+ {
+ 	struct bpf_prog *tmp, *orig_fp = fp;
+ 	struct bpf_binary_header *header;
+ 	bool tmp_blinded = false;
+ 	struct bpf_jit jit;
+ 	int pass;
+ 
+ 	if (!bpf_jit_enable)
+ 		return orig_fp;
+ 
+ 	tmp = bpf_jit_blind_constants(fp);
+ 	/*
+ 	 * If blinding was requested and we failed during blinding,
+ 	 * we must fall back to the interpreter.
+ 	 */
+ 	if (IS_ERR(tmp))
+ 		return orig_fp;
+ 	if (tmp != fp) {
+ 		tmp_blinded = true;
+ 		fp = tmp;
+ 	}
+ 
+ 	memset(&jit, 0, sizeof(jit));
+ 	jit.addrs = kcalloc(fp->len + 1, sizeof(*jit.addrs), GFP_KERNEL);
+ 	if (jit.addrs == NULL) {
+ 		fp = orig_fp;
++>>>>>>> b0a0c2566f28 (bpf, s390: fix jit branch offset related to ldimm64)
  		goto out;
  	}
 -	/*
 -	 * Three initial passes:
 -	 *   - 1/2: Determine clobbered registers
 -	 *   - 3:   Calculate program size and addrs arrray
 -	 */
 -	for (pass = 1; pass <= 3; pass++) {
 -		if (bpf_jit_prog(&jit, fp)) {
 -			fp = orig_fp;
 -			goto free_addrs;
 +	addrs[i] = jit->prg - jit->start;
 +	return 0;
 +out:
 +	return -1;
 +}
 +
 +void bpf_jit_compile(struct sk_filter *fp)
 +{
 +	unsigned long size, prg_len, lit_len;
 +	struct bpf_jit jit, cjit;
 +	unsigned int *addrs;
 +	int pass, i;
 +
 +	if (!bpf_jit_enable)
 +		return;
 +	addrs = kcalloc(fp->len, sizeof(*addrs), GFP_KERNEL);
 +	if (addrs == NULL)
 +		return;
 +	memset(&jit, 0, sizeof(cjit));
 +	memset(&cjit, 0, sizeof(cjit));
 +
 +	for (pass = 0; pass < 10; pass++) {
 +		jit.prg = jit.start;
 +		jit.lit = jit.mid;
 +
 +		bpf_jit_prologue(&jit);
 +		bpf_jit_noleaks(&jit, fp->insns);
 +		for (i = 0; i < fp->len; i++) {
 +			if (bpf_jit_insn(&jit, fp->insns + i, addrs, i,
 +					 i == fp->len - 1))
 +				goto out;
  		}
 -	}
 -	/*
 -	 * Final pass: Allocate and generate program
 -	 */
 -	if (jit.size >= BPF_SIZE_MAX) {
 -		fp = orig_fp;
 -		goto free_addrs;
 -	}
 -	header = bpf_jit_binary_alloc(jit.size, &jit.prg_buf, 2, jit_fill_hole);
 -	if (!header) {
 -		fp = orig_fp;
 -		goto free_addrs;
 -	}
 -	if (bpf_jit_prog(&jit, fp)) {
 -		fp = orig_fp;
 -		goto free_addrs;
 +		bpf_jit_epilogue(&jit);
 +		if (jit.start) {
 +			WARN_ON(jit.prg > cjit.prg || jit.lit > cjit.lit);
 +			if (memcmp(&jit, &cjit, sizeof(jit)) == 0)
 +				break;
 +		} else if (jit.prg == cjit.prg && jit.lit == cjit.lit) {
 +			prg_len = jit.prg - jit.start;
 +			lit_len = jit.lit - jit.mid;
 +			size = max_t(unsigned long, prg_len + lit_len,
 +				     sizeof(struct work_struct));
 +			if (size >= BPF_SIZE_MAX)
 +				goto out;
 +			jit.start = module_alloc(size);
 +			if (!jit.start)
 +				goto out;
 +			jit.prg = jit.mid = jit.start + prg_len;
 +			jit.lit = jit.end = jit.start + prg_len + lit_len;
 +			jit.base_ip += (unsigned long) jit.start;
 +			jit.exit_ip += (unsigned long) jit.start;
 +			jit.ret0_ip += (unsigned long) jit.start;
 +		}
 +		cjit = jit;
  	}
  	if (bpf_jit_enable > 1) {
 -		bpf_jit_dump(fp->len, jit.size, pass, jit.prg_buf);
 -		print_fn_code(jit.prg_buf, jit.size_prg);
 +		pr_err("flen=%d proglen=%lu pass=%d image=%p\n",
 +		       fp->len, jit.end - jit.start, pass, jit.start);
 +		if (jit.start) {
 +			printk(KERN_ERR "JIT code:\n");
 +			print_fn_code(jit.start, jit.mid - jit.start);
 +			print_hex_dump(KERN_ERR, "JIT literals:\n",
 +				       DUMP_PREFIX_ADDRESS, 16, 1,
 +				       jit.mid, jit.end - jit.mid, false);
 +		}
  	}
 -	bpf_jit_binary_lock_ro(header);
 -	fp->bpf_func = (void *) jit.prg_buf;
 -	fp->jited = 1;
 -	fp->jited_len = jit.size;
 -free_addrs:
 -	kfree(jit.addrs);
 +	if (jit.start)
 +		fp->bpf_func = (void *) jit.start;
  out:
 -	if (tmp_blinded)
 -		bpf_jit_prog_release_other(fp, fp == orig_fp ?
 -					   tmp : orig_fp);
 -	return fp;
 +	kfree(addrs);
 +}
 +
 +static void jit_free_defer(struct work_struct *arg)
 +{
 +	module_free(NULL, arg);
 +}
 +
 +/* run from softirq, we must use a work_struct to call
 + * module_free() from process context
 + */
 +void bpf_jit_free(struct sk_filter *fp)
 +{
 +	struct work_struct *work;
 +
 +	if (fp->bpf_func == sk_run_filter)
 +		return;
 +	work = (struct work_struct *)fp->bpf_func;
 +	INIT_WORK(work, jit_free_defer);
 +	schedule_work(work);
  }
* Unmerged path arch/s390/net/bpf_jit_comp.c
