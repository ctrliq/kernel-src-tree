locking: Introduce __cleanup() based infrastructure

jira LE-3587
Rebuild_History Non-Buildable kernel-4.18.0-553.62.1.el8_10
commit-author Peter Zijlstra <peterz@infradead.org>
commit 54da6a0924311c7cf5015533991e44fb8eb12773
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.62.1.el8_10/54da6a09.failed

Use __attribute__((__cleanup__(func))) to build:

 - simple auto-release pointers using __free()

 - 'classes' with constructor and destructor semantics for
   scope-based resource management.

 - lock guards based on the above classes.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20230612093537.614161713%40infradead.org
(cherry picked from commit 54da6a0924311c7cf5015533991e44fb8eb12773)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/compiler-clang.h
#	include/linux/device.h
#	include/linux/file.h
#	include/linux/mutex.h
#	include/linux/preempt.h
#	include/linux/sched/task.h
#	include/linux/spinlock.h
diff --cc include/linux/compiler-clang.h
index c8556b0be4fd,9b673fefcef8..000000000000
--- a/include/linux/compiler-clang.h
+++ b/include/linux/compiler-clang.h
@@@ -5,7 -5,14 +5,18 @@@
  
  /* Compiler specific definitions for Clang compiler */
  
++<<<<<<< HEAD
 +#define uninitialized_var(x) x = *(&(x))
++=======
+ /*
+  * Clang prior to 17 is being silly and considers many __cleanup() variables
+  * as unused (because they are, their sole purpose is to go out of scope).
+  *
+  * https://reviews.llvm.org/D152180
+  */
+ #undef __cleanup
+ #define __cleanup(func) __maybe_unused __attribute__((__cleanup__(func)))
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  
  /* same as gcc, this was present in clang-2.6 so we can assume it works
   * with any version that can compile the kernel
diff --cc include/linux/device.h
index 9ead0cc21aca,2b64268d776c..000000000000
--- a/include/linux/device.h
+++ b/include/linux/device.h
@@@ -26,7 -27,10 +26,14 @@@
  #include <linux/uidgid.h>
  #include <linux/gfp.h>
  #include <linux/overflow.h>
++<<<<<<< HEAD
 +#include <linux/rh_kabi.h>
++=======
+ #include <linux/device/bus.h>
+ #include <linux/device/class.h>
+ #include <linux/device/driver.h>
+ #include <linux/cleanup.h>
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  #include <asm/device.h>
  
  struct device;
@@@ -1668,25 -895,29 +1675,51 @@@ static inline bool dev_removable_is_val
  /*
   * High level routines for use by the bus drivers
   */
++<<<<<<< HEAD
 +extern int __must_check device_register(struct device *dev);
 +extern void device_unregister(struct device *dev);
 +extern void device_initialize(struct device *dev);
 +extern int __must_check device_add(struct device *dev);
 +extern void device_del(struct device *dev);
 +extern int device_for_each_child(struct device *dev, void *data,
 +		     int (*fn)(struct device *dev, void *data));
 +extern int device_for_each_child_reverse(struct device *dev, void *data,
 +		     int (*fn)(struct device *dev, void *data));
 +extern struct device *device_find_child(struct device *dev, void *data,
 +				int (*match)(struct device *dev, void *data));
 +extern struct device *device_find_child_by_name(struct device *parent,
 +						const char *name);
 +extern struct device *device_find_any_child(struct device *parent);
 +extern int device_rename(struct device *dev, const char *new_name);
 +extern int device_move(struct device *dev, struct device *new_parent,
 +		       enum dpm_order dpm_order);
 +extern int device_change_owner(struct device *dev, kuid_t kuid, kgid_t kgid);
 +extern int device_is_dependent(struct device *dev, void *target);
++=======
+ int __must_check device_register(struct device *dev);
+ void device_unregister(struct device *dev);
+ void device_initialize(struct device *dev);
+ int __must_check device_add(struct device *dev);
+ void device_del(struct device *dev);
+ 
+ DEFINE_FREE(device_del, struct device *, if (_T) device_del(_T))
+ 
+ int device_for_each_child(struct device *dev, void *data,
+ 			  int (*fn)(struct device *dev, void *data));
+ int device_for_each_child_reverse(struct device *dev, void *data,
+ 				  int (*fn)(struct device *dev, void *data));
+ struct device *device_find_child(struct device *dev, void *data,
+ 				 int (*match)(struct device *dev, void *data));
+ struct device *device_find_child_by_name(struct device *parent,
+ 					 const char *name);
+ struct device *device_find_any_child(struct device *parent);
+ 
+ int device_rename(struct device *dev, const char *new_name);
+ int device_move(struct device *dev, struct device *new_parent,
+ 		enum dpm_order dpm_order);
+ int device_change_owner(struct device *dev, kuid_t kuid, kgid_t kgid);
+ int device_is_dependent(struct device *dev, void *target);
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  
  static inline bool device_supports_offline(struct device *dev)
  {
@@@ -1797,18 -1068,17 +1830,27 @@@ extern int (*platform_notify_remove)(st
   * get_device - atomically increment the reference count for the device.
   *
   */
++<<<<<<< HEAD
 +extern struct device *get_device(struct device *dev);
 +extern void put_device(struct device *dev);
 +extern bool kill_device(struct device *dev);
++=======
+ struct device *get_device(struct device *dev);
+ void put_device(struct device *dev);
+ 
+ DEFINE_FREE(put_device, struct device *, if (_T) put_device(_T))
+ 
+ bool kill_device(struct device *dev);
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  
  #ifdef CONFIG_DEVTMPFS
 -int devtmpfs_mount(void);
 +extern int devtmpfs_create_node(struct device *dev);
 +extern int devtmpfs_delete_node(struct device *dev);
 +extern int devtmpfs_mount(const char *mntdir);
  #else
 -static inline int devtmpfs_mount(void) { return 0; }
 +static inline int devtmpfs_create_node(struct device *dev) { return 0; }
 +static inline int devtmpfs_delete_node(struct device *dev) { return 0; }
 +static inline int devtmpfs_mount(const char *mountpoint) { return 0; }
  #endif
  
  /* drivers/base/power/shutdown.c */
diff --cc include/linux/file.h
index e6f5d05a242c,6e9099d29343..000000000000
--- a/include/linux/file.h
+++ b/include/linux/file.h
@@@ -9,6 -9,8 +9,11 @@@
  #include <linux/compiler.h>
  #include <linux/types.h>
  #include <linux/posix_types.h>
++<<<<<<< HEAD
++=======
+ #include <linux/errno.h>
+ #include <linux/cleanup.h>
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  
  struct file;
  
@@@ -89,8 -91,25 +96,11 @@@ extern int __get_unused_fd_flags(unsign
  extern int get_unused_fd_flags(unsigned flags);
  extern void put_unused_fd(unsigned int fd);
  
+ DEFINE_CLASS(get_unused_fd, int, if (_T >= 0) put_unused_fd(_T),
+ 	     get_unused_fd_flags(flags), unsigned flags)
+ 
  extern void fd_install(unsigned int fd, struct file *file);
  
 -extern int __receive_fd(struct file *file, int __user *ufd,
 -			unsigned int o_flags);
 -
 -extern int receive_fd(struct file *file, unsigned int o_flags);
 -
 -static inline int receive_fd_user(struct file *file, int __user *ufd,
 -				  unsigned int o_flags)
 -{
 -	if (ufd == NULL)
 -		return -EFAULT;
 -	return __receive_fd(file, ufd, o_flags);
 -}
 -int receive_fd_replace(int new_fd, struct file *file, unsigned int o_flags);
 -
  extern void flush_delayed_fput(void);
  extern void __fput_sync(struct file *);
  
diff --cc include/linux/mutex.h
index 997f1bb49f2b,a33aa9eb9fc3..000000000000
--- a/include/linux/mutex.h
+++ b/include/linux/mutex.h
@@@ -220,29 -220,7 +221,34 @@@ extern void mutex_unlock(struct mutex *
  
  extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
  
++<<<<<<< HEAD
 +/*
 + * These values are chosen such that FAIL and SUCCESS match the
 + * values of the regular mutex_trylock().
 + */
 +enum mutex_trylock_recursive_enum {
 +	MUTEX_TRYLOCK_FAILED    = 0,
 +	MUTEX_TRYLOCK_SUCCESS   = 1,
 +	MUTEX_TRYLOCK_RECURSIVE,
 +};
 +
 +/**
 + * mutex_trylock_recursive - trylock variant that allows recursive locking
 + * @lock: mutex to be locked
 + *
 + * This function should not be used, _ever_. It is purely for hysterical GEM
 + * raisins, and once those are gone this will be removed.
 + *
 + * Returns:
 + *  - MUTEX_TRYLOCK_FAILED    - trylock failed,
 + *  - MUTEX_TRYLOCK_SUCCESS   - lock acquired,
 + *  - MUTEX_TRYLOCK_RECURSIVE - we already owned the lock.
 + */
 +extern /* __deprecated */ __must_check enum mutex_trylock_recursive_enum
 +mutex_trylock_recursive(struct mutex *lock);
++=======
+ DEFINE_GUARD(mutex, struct mutex *, mutex_lock(_T), mutex_unlock(_T))
+ DEFINE_FREE(mutex, struct mutex *, if (_T) mutex_unlock(_T))
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  
  #endif /* __LINUX_MUTEX_H */
diff --cc include/linux/preempt.h
index 59a402e0c116,1424670df161..000000000000
--- a/include/linux/preempt.h
+++ b/include/linux/preempt.h
@@@ -393,4 -422,50 +394,53 @@@ static inline void migrate_enable(void
  
  #endif /* CONFIG_SMP */
  
++<<<<<<< HEAD
++=======
+ /**
+  * preempt_disable_nested - Disable preemption inside a normally preempt disabled section
+  *
+  * Use for code which requires preemption protection inside a critical
+  * section which has preemption disabled implicitly on non-PREEMPT_RT
+  * enabled kernels, by e.g.:
+  *  - holding a spinlock/rwlock
+  *  - soft interrupt context
+  *  - regular interrupt handlers
+  *
+  * On PREEMPT_RT enabled kernels spinlock/rwlock held sections, soft
+  * interrupt context and regular interrupt handlers are preemptible and
+  * only prevent migration. preempt_disable_nested() ensures that preemption
+  * is disabled for cases which require CPU local serialization even on
+  * PREEMPT_RT. For non-PREEMPT_RT kernels this is a NOP.
+  *
+  * The use cases are code sequences which are not serialized by a
+  * particular lock instance, e.g.:
+  *  - seqcount write side critical sections where the seqcount is not
+  *    associated to a particular lock and therefore the automatic
+  *    protection mechanism does not work. This prevents a live lock
+  *    against a preempting high priority reader.
+  *  - RMW per CPU variable updates like vmstat.
+  */
+ /* Macro to avoid header recursion hell vs. lockdep */
+ #define preempt_disable_nested()				\
+ do {								\
+ 	if (IS_ENABLED(CONFIG_PREEMPT_RT))			\
+ 		preempt_disable();				\
+ 	else							\
+ 		lockdep_assert_preemption_disabled();		\
+ } while (0)
+ 
+ /**
+  * preempt_enable_nested - Undo the effect of preempt_disable_nested()
+  */
+ static __always_inline void preempt_enable_nested(void)
+ {
+ 	if (IS_ENABLED(CONFIG_PREEMPT_RT))
+ 		preempt_enable();
+ }
+ 
+ DEFINE_LOCK_GUARD_0(preempt, preempt_disable(), preempt_enable())
+ DEFINE_LOCK_GUARD_0(preempt_notrace, preempt_disable_notrace(), preempt_enable_notrace())
+ DEFINE_LOCK_GUARD_0(migrate, migrate_disable(), migrate_enable())
+ 
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  #endif /* __LINUX_PREEMPT_H */
diff --cc include/linux/sched/task.h
index 8935299dd9db,dd35ce28bb90..000000000000
--- a/include/linux/sched/task.h
+++ b/include/linux/sched/task.h
@@@ -101,32 -121,15 +101,40 @@@ extern void __put_task_struct_rcu_cb(st
  
  static inline void put_task_struct(struct task_struct *t)
  {
 -	if (refcount_dec_and_test(&t->usage))
 -		__put_task_struct(t);
 -}
 -
 +	if (!refcount_dec_and_test(&t->usage))
 +		return;
 +
++<<<<<<< HEAD
 +	/*
 +	 * under PREEMPT_RT, we can't call put_task_struct
 +	 * in atomic context because it will indirectly
 +	 * acquire sleeping locks.
 +	 *
 +	 * call_rcu() will schedule delayed_put_task_struct_rcu()
 +	 * to be called in process context.
 +	 *
 +	 * __put_task_struct() is called when
 +	 * refcount_dec_and_test(&t->usage) succeeds.
 +	 *
 +	 * This means that it can't "conflict" with
 +	 * put_task_struct_rcu_user() which abuses ->rcu the same
 +	 * way; rcu_users has a reference so task->usage can't be
 +	 * zero after rcu_users 1 -> 0 transition.
 +	 *
 +	 * delayed_free_task() also uses ->rcu, but it is only called
 +	 * when it fails to fork a process. Therefore, there is no
 +	 * way it can conflict with put_task_struct().
 +	 */
 +	if (IS_ENABLED(CONFIG_PREEMPT_RT) && !preemptible())
 +		call_rcu(&t->rcu, __put_task_struct_rcu_cb);
 +	else
++=======
+ DEFINE_FREE(put_task, struct task_struct *, if (_T) put_task_struct(_T))
+ 
+ static inline void put_task_struct_many(struct task_struct *t, int nr)
+ {
+ 	if (refcount_sub_and_test(nr, &t->usage))
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  		__put_task_struct(t);
  }
  
diff --cc include/linux/spinlock.h
index f7eafe0a7b5b,31d3d747a9db..000000000000
--- a/include/linux/spinlock.h
+++ b/include/linux/spinlock.h
@@@ -493,27 -503,35 +494,61 @@@ int __alloc_bucket_spinlocks(spinlock_
  
  void free_bucket_spinlocks(spinlock_t *locks);
  
++<<<<<<< HEAD
 +/*
 + * RHEL8 qrwlock macros
 + */
 +#ifndef qrwlock_t
 +#define qrwlock_t	rwlock_t
 +#endif
 +
 +#ifndef qread_lock
 +#define qread_lock(l)	read_lock(l)
 +#endif
 +
 +#ifndef qread_unlock
 +#define qread_unlock(l)	read_unlock(l)
 +#endif
 +
 +#ifndef qwrite_lock_irq
 +#define qwrite_lock_irq(l)	write_lock_irq(l)
 +#endif
 +
 +#ifndef qwrite_unlock_irq
 +#define qwrite_unlock_irq(l)	write_unlock_irq(l)
 +#endif
 +
++=======
+ DEFINE_LOCK_GUARD_1(raw_spinlock, raw_spinlock_t,
+ 		    raw_spin_lock(_T->lock),
+ 		    raw_spin_unlock(_T->lock))
+ 
+ DEFINE_LOCK_GUARD_1(raw_spinlock_nested, raw_spinlock_t,
+ 		    raw_spin_lock_nested(_T->lock, SINGLE_DEPTH_NESTING),
+ 		    raw_spin_unlock(_T->lock))
+ 
+ DEFINE_LOCK_GUARD_1(raw_spinlock_irq, raw_spinlock_t,
+ 		    raw_spin_lock_irq(_T->lock),
+ 		    raw_spin_unlock_irq(_T->lock))
+ 
+ DEFINE_LOCK_GUARD_1(raw_spinlock_irqsave, raw_spinlock_t,
+ 		    raw_spin_lock_irqsave(_T->lock, _T->flags),
+ 		    raw_spin_unlock_irqrestore(_T->lock, _T->flags),
+ 		    unsigned long flags)
+ 
+ DEFINE_LOCK_GUARD_1(spinlock, spinlock_t,
+ 		    spin_lock(_T->lock),
+ 		    spin_unlock(_T->lock))
+ 
+ DEFINE_LOCK_GUARD_1(spinlock_irq, spinlock_t,
+ 		    spin_lock_irq(_T->lock),
+ 		    spin_unlock_irq(_T->lock))
+ 
+ DEFINE_LOCK_GUARD_1(spinlock_irqsave, spinlock_t,
+ 		    spin_lock_irqsave(_T->lock, _T->flags),
+ 		    spin_unlock_irqrestore(_T->lock, _T->flags),
+ 		    unsigned long flags)
+ 
+ #undef __LINUX_INSIDE_SPINLOCK_H
++>>>>>>> 54da6a092431 (locking: Introduce __cleanup() based infrastructure)
  #endif /* __LINUX_SPINLOCK_H */
diff --git a/include/linux/cleanup.h b/include/linux/cleanup.h
new file mode 100644
index 000000000000..53f1a7a932b0
--- /dev/null
+++ b/include/linux/cleanup.h
@@ -0,0 +1,171 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __LINUX_GUARDS_H
+#define __LINUX_GUARDS_H
+
+#include <linux/compiler.h>
+
+/*
+ * DEFINE_FREE(name, type, free):
+ *	simple helper macro that defines the required wrapper for a __free()
+ *	based cleanup function. @free is an expression using '_T' to access
+ *	the variable.
+ *
+ * __free(name):
+ *	variable attribute to add a scoped based cleanup to the variable.
+ *
+ * no_free_ptr(var):
+ *	like a non-atomic xchg(var, NULL), such that the cleanup function will
+ *	be inhibited -- provided it sanely deals with a NULL value.
+ *
+ * return_ptr(p):
+ *	returns p while inhibiting the __free().
+ *
+ * Ex.
+ *
+ * DEFINE_FREE(kfree, void *, if (_T) kfree(_T))
+ *
+ *	struct obj *p __free(kfree) = kmalloc(...);
+ *	if (!p)
+ *		return NULL;
+ *
+ *	if (!init_obj(p))
+ *		return NULL;
+ *
+ *	return_ptr(p);
+ */
+
+#define DEFINE_FREE(_name, _type, _free) \
+	static inline void __free_##_name(void *p) { _type _T = *(_type *)p; _free; }
+
+#define __free(_name)	__cleanup(__free_##_name)
+
+#define no_free_ptr(p) \
+	({ __auto_type __ptr = (p); (p) = NULL; __ptr; })
+
+#define return_ptr(p)	return no_free_ptr(p)
+
+
+/*
+ * DEFINE_CLASS(name, type, exit, init, init_args...):
+ *	helper to define the destructor and constructor for a type.
+ *	@exit is an expression using '_T' -- similar to FREE above.
+ *	@init is an expression in @init_args resulting in @type
+ *
+ * EXTEND_CLASS(name, ext, init, init_args...):
+ *	extends class @name to @name@ext with the new constructor
+ *
+ * CLASS(name, var)(args...):
+ *	declare the variable @var as an instance of the named class
+ *
+ * Ex.
+ *
+ * DEFINE_CLASS(fdget, struct fd, fdput(_T), fdget(fd), int fd)
+ *
+ *	CLASS(fdget, f)(fd);
+ *	if (!f.file)
+ *		return -EBADF;
+ *
+ *	// use 'f' without concern
+ */
+
+#define DEFINE_CLASS(_name, _type, _exit, _init, _init_args...)		\
+typedef _type class_##_name##_t;					\
+static inline void class_##_name##_destructor(_type *p)			\
+{ _type _T = *p; _exit; }						\
+static inline _type class_##_name##_constructor(_init_args)		\
+{ _type t = _init; return t; }
+
+#define EXTEND_CLASS(_name, ext, _init, _init_args...)			\
+typedef class_##_name##_t class_##_name##ext##_t;			\
+static inline void class_##_name##ext##_destructor(class_##_name##_t *p)\
+{ class_##_name##_destructor(p); }					\
+static inline class_##_name##_t class_##_name##ext##_constructor(_init_args) \
+{ class_##_name##_t t = _init; return t; }
+
+#define CLASS(_name, var)						\
+	class_##_name##_t var __cleanup(class_##_name##_destructor) =	\
+		class_##_name##_constructor
+
+
+/*
+ * DEFINE_GUARD(name, type, lock, unlock):
+ *	trivial wrapper around DEFINE_CLASS() above specifically
+ *	for locks.
+ *
+ * guard(name):
+ *	an anonymous instance of the (guard) class
+ *
+ * scoped_guard (name, args...) { }:
+ *	similar to CLASS(name, scope)(args), except the variable (with the
+ *	explicit name 'scope') is declard in a for-loop such that its scope is
+ *	bound to the next (compound) statement.
+ *
+ */
+
+#define DEFINE_GUARD(_name, _type, _lock, _unlock) \
+	DEFINE_CLASS(_name, _type, _unlock, ({ _lock; _T; }), _type _T)
+
+#define guard(_name) \
+	CLASS(_name, __UNIQUE_ID(guard))
+
+#define scoped_guard(_name, args...)					\
+	for (CLASS(_name, scope)(args),					\
+	     *done = NULL; !done; done = (void *)1)
+
+/*
+ * Additional helper macros for generating lock guards with types, either for
+ * locks that don't have a native type (eg. RCU, preempt) or those that need a
+ * 'fat' pointer (eg. spin_lock_irqsave).
+ *
+ * DEFINE_LOCK_GUARD_0(name, lock, unlock, ...)
+ * DEFINE_LOCK_GUARD_1(name, type, lock, unlock, ...)
+ *
+ * will result in the following type:
+ *
+ *   typedef struct {
+ *	type *lock;		// 'type := void' for the _0 variant
+ *	__VA_ARGS__;
+ *   } class_##name##_t;
+ *
+ * As above, both _lock and _unlock are statements, except this time '_T' will
+ * be a pointer to the above struct.
+ */
+
+#define __DEFINE_UNLOCK_GUARD(_name, _type, _unlock, ...)		\
+typedef struct {							\
+	_type *lock;							\
+	__VA_ARGS__;							\
+} class_##_name##_t;							\
+									\
+static inline void class_##_name##_destructor(class_##_name##_t *_T)	\
+{									\
+	if (_T->lock) { _unlock; }					\
+}
+
+
+#define __DEFINE_LOCK_GUARD_1(_name, _type, _lock)			\
+static inline class_##_name##_t class_##_name##_constructor(_type *l)	\
+{									\
+	class_##_name##_t _t = { .lock = l }, *_T = &_t;		\
+	_lock;								\
+	return _t;							\
+}
+
+#define __DEFINE_LOCK_GUARD_0(_name, _lock)				\
+static inline class_##_name##_t class_##_name##_constructor(void)	\
+{									\
+	class_##_name##_t _t = { .lock = (void*)1 },			\
+			 *_T __maybe_unused = &_t;			\
+	_lock;								\
+	return _t;							\
+}
+
+#define DEFINE_LOCK_GUARD_1(_name, _type, _lock, _unlock, ...)		\
+__DEFINE_UNLOCK_GUARD(_name, _type, _unlock, __VA_ARGS__)		\
+__DEFINE_LOCK_GUARD_1(_name, _type, _lock)
+
+#define DEFINE_LOCK_GUARD_0(_name, _lock, _unlock, ...)			\
+__DEFINE_UNLOCK_GUARD(_name, void, _unlock, __VA_ARGS__)		\
+__DEFINE_LOCK_GUARD_0(_name, _lock)
+
+#endif /* __LINUX_GUARDS_H */
* Unmerged path include/linux/compiler-clang.h
diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
index 1ac7907e076a..976856d472bd 100644
--- a/include/linux/compiler_attributes.h
+++ b/include/linux/compiler_attributes.h
@@ -91,6 +91,12 @@
  */
 #define __cold                          __attribute__((__cold__))
 
+/*
+ *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-cleanup-variable-attribute
+ * clang: https://clang.llvm.org/docs/AttributeReference.html#cleanup
+ */
+#define __cleanup(func)			__attribute__((__cleanup__(func)))
+
 /*
  * Note the long name.
  *
* Unmerged path include/linux/device.h
* Unmerged path include/linux/file.h
diff --git a/include/linux/irqflags.h b/include/linux/irqflags.h
index 600c10da321a..449c8b4712e1 100644
--- a/include/linux/irqflags.h
+++ b/include/linux/irqflags.h
@@ -13,6 +13,7 @@
 #define _LINUX_TRACE_IRQFLAGS_H
 
 #include <linux/typecheck.h>
+#include <linux/cleanup.h>
 #include <asm/irqflags.h>
 #include <asm/percpu.h>
 
@@ -260,4 +261,10 @@ extern void warn_bogus_irq_restore(void);
 
 #define irqs_disabled_flags(flags) raw_irqs_disabled_flags(flags)
 
+DEFINE_LOCK_GUARD_0(irq, local_irq_disable(), local_irq_enable())
+DEFINE_LOCK_GUARD_0(irqsave,
+		    local_irq_save(_T->flags),
+		    local_irq_restore(_T->flags),
+		    unsigned long flags)
+
 #endif
* Unmerged path include/linux/mutex.h
diff --git a/include/linux/percpu.h b/include/linux/percpu.h
index 9909dc0e273a..dace727a7041 100644
--- a/include/linux/percpu.h
+++ b/include/linux/percpu.h
@@ -9,6 +9,7 @@
 #include <linux/printk.h>
 #include <linux/pfn.h>
 #include <linux/init.h>
+#include <linux/cleanup.h>
 
 #include <asm/percpu.h>
 
@@ -134,6 +135,9 @@ extern void __init setup_per_cpu_areas(void);
 extern void __percpu *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp);
 extern void __percpu *__alloc_percpu(size_t size, size_t align);
 extern void free_percpu(void __percpu *__pdata);
+
+DEFINE_FREE(free_percpu, void __percpu *, free_percpu(_T))
+
 extern phys_addr_t per_cpu_ptr_to_phys(void *addr);
 
 #define alloc_percpu_gfp(type, gfp)					\
* Unmerged path include/linux/preempt.h
diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h
index d506f440fa05..4810da22e4e8 100644
--- a/include/linux/rcupdate.h
+++ b/include/linux/rcupdate.h
@@ -40,6 +40,7 @@
 #include <linux/preempt.h>
 #include <linux/bottom_half.h>
 #include <linux/lockdep.h>
+#include <linux/cleanup.h>
 #include <asm/processor.h>
 #include <linux/cpumask.h>
 
@@ -1034,4 +1035,6 @@ rcu_head_after_call_rcu(struct rcu_head *rhp, rcu_callback_t f)
 extern int rcu_expedited;
 extern int rcu_normal;
 
+DEFINE_LOCK_GUARD_0(rcu, rcu_read_lock(), rcu_read_unlock())
+
 #endif /* __LINUX_RCUPDATE_H */
diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
index 897f14f3ab0d..a5caf0e86805 100644
--- a/include/linux/rwsem.h
+++ b/include/linux/rwsem.h
@@ -16,6 +16,7 @@
 #include <linux/spinlock.h>
 #include <linux/atomic.h>
 #include <linux/err.h>
+#include <linux/cleanup.h>
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 # define __RWSEM_DEP_MAP_INIT(lockname)			\
@@ -195,6 +196,13 @@ extern void up_read(struct rw_semaphore *sem);
  */
 extern void up_write(struct rw_semaphore *sem);
 
+DEFINE_GUARD(rwsem_read, struct rw_semaphore *, down_read(_T), up_read(_T))
+DEFINE_GUARD(rwsem_write, struct rw_semaphore *, down_write(_T), up_write(_T))
+
+DEFINE_FREE(up_read, struct rw_semaphore *, if (_T) up_read(_T))
+DEFINE_FREE(up_write, struct rw_semaphore *, if (_T) up_write(_T))
+
+
 /*
  * downgrade write lock to read lock
  */
* Unmerged path include/linux/sched/task.h
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 7cec7ba2bcd8..9928bbf62d22 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -17,6 +17,7 @@
 #include <linux/types.h>
 #include <linux/workqueue.h>
 #include <linux/percpu-refcount.h>
+#include <linux/cleanup.h>
 
 
 /*
@@ -191,6 +192,8 @@ void kfree(const void *);
 void kfree_sensitive(const void *);
 size_t __ksize(const void *);
 
+DEFINE_FREE(kfree, void *, if (_T) kfree(_T))
+
 /**
  * ksize - Report actual allocation size of associated object
  *
* Unmerged path include/linux/spinlock.h
diff --git a/include/linux/srcu.h b/include/linux/srcu.h
index d23004d74a3d..b8a77a5bc9ec 100644
--- a/include/linux/srcu.h
+++ b/include/linux/srcu.h
@@ -223,4 +223,9 @@ static inline void smp_mb__after_srcu_read_unlock(void)
 	/* __srcu_read_unlock has smp_mb() internally so nothing to do here. */
 }
 
+DEFINE_LOCK_GUARD_1(srcu, struct srcu_struct,
+		    _T->idx = srcu_read_lock(_T->lock),
+		    srcu_read_unlock(_T->lock, _T->idx),
+		    int idx)
+
 #endif
diff --git a/scripts/checkpatch.pl b/scripts/checkpatch.pl
index 0a986e812fb1..b3f0dfc7803f 100755
--- a/scripts/checkpatch.pl
+++ b/scripts/checkpatch.pl
@@ -4135,7 +4135,7 @@ sub process {
 				if|for|while|switch|return|case|
 				volatile|__volatile__|
 				__attribute__|format|__extension__|
-				asm|__asm__)$/x)
+				asm|__asm__|scoped_guard)$/x)
 			{
 			# cpp #define statements have non-optional spaces, ie
 			# if there is a space between the name and the open
