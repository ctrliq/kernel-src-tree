x86/retpoline: Document some thunk handling aspects

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Borislav Petkov (AMD) <bp@alien8.de>
commit 9d9c22cc444af01ce254872b729af26864c43a3a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/9d9c22cc.failed

After a lot of experimenting (see thread Link points to) document for
now the issues and requirements for future improvements to the thunk
handling and potential issuing of a diagnostic when the default thunk
hasn't been patched out.

This documentation is only temporary and that close before the merge
window it is only a placeholder for those future improvements.

	Suggested-by: Ingo Molnar <mingo@kernel.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/20231010171020.462211-1-david.kaplan@amd.com
(cherry picked from commit 9d9c22cc444af01ce254872b729af26864c43a3a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/lib/retpoline.S
diff --cc arch/x86/lib/retpoline.S
index 72a7235229fd,a48077c5ca61..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -24,36 -49,188 +24,141 @@@ SYM_FUNC_END(__x86_indirect_thunk_\reg
   * only see one instance of "__x86_indirect_thunk_\reg" rather
   * than one per register with the correct names. So we do it
   * the simple and nasty way...
 - *
 - * Worse, you can only have a single EXPORT_SYMBOL per line,
 - * and CPP can't insert newlines, so we have to repeat everything
 - * at least twice.
   */
 -
 -#define __EXPORT_THUNK(sym)	_ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_thunk_array)
 -
 -#define GEN(reg) THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -.macro CALL_THUNK reg
 -	.align RETPOLINE_THUNK_SIZE
 -
 -SYM_INNER_LABEL(__x86_indirect_call_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_UNDEFINED
 -	ANNOTATE_NOENDBR
 -
 -	CALL_DEPTH_ACCOUNT
 -	POLINE \reg
 -	ANNOTATE_UNRET_SAFE
 -	ret
 -	int3
 -.endm
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_call_thunk_array)
 -
 -#define GEN(reg) CALL_THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_call_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_call_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -.macro JUMP_THUNK reg
 -	.align RETPOLINE_THUNK_SIZE
 -
 -SYM_INNER_LABEL(__x86_indirect_jump_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_UNDEFINED
 -	ANNOTATE_NOENDBR
 -	POLINE \reg
 -	ANNOTATE_UNRET_SAFE
 -	ret
 -	int3
 -.endm
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_jump_thunk_array)
 -
 -#define GEN(reg) JUMP_THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_jump_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_jump_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 +#define __EXPORT_THUNK(sym) _ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 +#define EXPORT_THUNK(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 +#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)
 +
 +GENERATE_THUNK(_ASM_AX)
 +GENERATE_THUNK(_ASM_BX)
 +GENERATE_THUNK(_ASM_CX)
 +GENERATE_THUNK(_ASM_DX)
 +GENERATE_THUNK(_ASM_SI)
 +GENERATE_THUNK(_ASM_DI)
 +GENERATE_THUNK(_ASM_BP)
 +#ifdef CONFIG_64BIT
 +GENERATE_THUNK(r8)
 +GENERATE_THUNK(r9)
 +GENERATE_THUNK(r10)
 +GENERATE_THUNK(r11)
 +GENERATE_THUNK(r12)
 +GENERATE_THUNK(r13)
 +GENERATE_THUNK(r14)
 +GENERATE_THUNK(r15)
  #endif
  
 +/*
 + * This function name is magical and is used by -mfunction-return=thunk-extern
 + * for the compiler to generate JMPs to it.
 + */
  #ifdef CONFIG_RETHUNK
  
++<<<<<<< HEAD
 +	.section .text.__x86.return_thunk
++=======
+ /*
+  * Be careful here: that label cannot really be removed because in
+  * some configurations and toolchains, the JMP __x86_return_thunk the
+  * compiler issues is either a short one or the compiler doesn't use
+  * relocations for same-section JMPs and that breaks the returns
+  * detection logic in apply_returns() and in objtool.
+  */
+ 	.section .text..__x86.return_thunk
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 
+ /*
+  * srso_alias_untrain_ret() and srso_alias_safe_ret() are placed at
+  * special addresses:
+  *
+  * - srso_alias_untrain_ret() is 2M aligned
+  * - srso_alias_safe_ret() is also in the same 2M page but bits 2, 8, 14
+  * and 20 in its virtual address are set (while those bits in the
+  * srso_alias_untrain_ret() function are cleared).
+  *
+  * This guarantees that those two addresses will alias in the branch
+  * target buffer of Zen3/4 generations, leading to any potential
+  * poisoned entries at that BTB slot to get evicted.
+  *
+  * As a result, srso_alias_safe_ret() becomes a safe return.
+  */
+ 	.pushsection .text..__x86.rethunk_untrain
+ SYM_CODE_START_NOALIGN(srso_alias_untrain_ret)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ASM_NOP2
+ 	lfence
+ 	jmp srso_alias_return_thunk
+ SYM_FUNC_END(srso_alias_untrain_ret)
+ 	.popsection
+ 
+ 	.pushsection .text..__x86.rethunk_safe
+ SYM_CODE_START_NOALIGN(srso_alias_safe_ret)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_alias_safe_ret)
+ 
+ SYM_CODE_START_NOALIGN(srso_alias_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_alias_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_alias_return_thunk)
+ 	.popsection
+ 
+ /*
+  * SRSO untraining sequence for Zen1/2, similar to retbleed_untrain_ret()
+  * above. On kernel entry, srso_untrain_ret() is executed which is a
+  *
+  * movabs $0xccccc30824648d48,%rax
+  *
+  * and when the return thunk executes the inner label srso_safe_ret()
+  * later, it is a stack manipulation and a RET which is mispredicted and
+  * thus a "safe" one to use.
+  */
+ 	.align 64
+ 	.skip 64 - (srso_safe_ret - srso_untrain_ret), 0xcc
+ SYM_CODE_START_LOCAL_NOALIGN(srso_untrain_ret)
+ 	ANNOTATE_NOENDBR
+ 	.byte 0x48, 0xb8
+ 
+ /*
+  * This forces the function return instruction to speculate into a trap
+  * (UD2 in srso_return_thunk() below).  This RET will then mispredict
+  * and execution will continue at the return site read from the top of
+  * the stack.
+  */
+ SYM_INNER_LABEL(srso_safe_ret, SYM_L_GLOBAL)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	ret
+ 	int3
+ 	int3
+ 	/* end of movabs */
+ 	lfence
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_safe_ret)
+ SYM_FUNC_END(srso_untrain_ret)
+ 
+ SYM_CODE_START(srso_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_return_thunk)
+ 
+ #define JMP_SRSO_UNTRAIN_RET "jmp srso_untrain_ret"
+ #define JMP_SRSO_ALIAS_UNTRAIN_RET "jmp srso_alias_untrain_ret"
+ #else /* !CONFIG_CPU_SRSO */
+ #define JMP_SRSO_UNTRAIN_RET "ud2"
+ #define JMP_SRSO_ALIAS_UNTRAIN_RET "ud2"
+ #endif /* CONFIG_CPU_SRSO */
+ 
+ #ifdef CONFIG_CPU_UNRET_ENTRY
++>>>>>>> 9d9c22cc444a (x86/retpoline: Document some thunk handling aspects)
  
  /*
   * Some generic notes on the untraining sequences:
@@@ -130,11 -307,83 +235,87 @@@ SYM_CODE_END(__x86_return_thunk
  	 * Jump back and execute the RET in the middle of the TEST instruction.
  	 * INT3 is for SLS protection.
  	 */
 -	jmp retbleed_return_thunk
 +	jmp __x86_return_thunk
  	int3
 -SYM_FUNC_END(retbleed_untrain_ret)
 +SYM_FUNC_END(zen_untrain_ret)
 +__EXPORT_THUNK(zen_untrain_ret)
  
++<<<<<<< HEAD
++=======
+ #define JMP_RETBLEED_UNTRAIN_RET "jmp retbleed_untrain_ret"
+ #else /* !CONFIG_CPU_UNRET_ENTRY */
+ #define JMP_RETBLEED_UNTRAIN_RET "ud2"
+ #endif /* CONFIG_CPU_UNRET_ENTRY */
+ 
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_SRSO)
+ 
+ SYM_FUNC_START(entry_untrain_ret)
+ 	ALTERNATIVE_2 JMP_RETBLEED_UNTRAIN_RET,				\
+ 		      JMP_SRSO_UNTRAIN_RET, X86_FEATURE_SRSO,		\
+ 		      JMP_SRSO_ALIAS_UNTRAIN_RET, X86_FEATURE_SRSO_ALIAS
+ SYM_FUNC_END(entry_untrain_ret)
+ __EXPORT_THUNK(entry_untrain_ret)
+ 
+ #endif /* CONFIG_CPU_UNRET_ENTRY || CONFIG_CPU_SRSO */
+ 
+ #ifdef CONFIG_CALL_DEPTH_TRACKING
+ 
+ 	.align 64
+ SYM_FUNC_START(call_depth_return_thunk)
+ 	ANNOTATE_NOENDBR
+ 	/*
+ 	 * Keep the hotpath in a 16byte I-fetch for the non-debug
+ 	 * case.
+ 	 */
+ 	CALL_THUNKS_DEBUG_INC_RETS
+ 	shlq	$5, PER_CPU_VAR(pcpu_hot + X86_call_depth)
+ 	jz	1f
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ 1:
+ 	CALL_THUNKS_DEBUG_INC_STUFFS
+ 	.rept	16
+ 	ANNOTATE_INTRA_FUNCTION_CALL
+ 	call	2f
+ 	int3
+ 2:
+ 	.endr
+ 	add	$(8*16), %rsp
+ 
+ 	CREDIT_CALL_DEPTH
+ 
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(call_depth_return_thunk)
+ 
+ #endif /* CONFIG_CALL_DEPTH_TRACKING */
+ 
+ /*
+  * This function name is magical and is used by -mfunction-return=thunk-extern
+  * for the compiler to generate JMPs to it.
+  *
+  * This code is only used during kernel boot or module init.  All
+  * 'JMP __x86_return_thunk' sites are changed to something else by
+  * apply_returns().
+  *
+  * This should be converted eventually to call a warning function which
+  * should scream loudly when the default return thunk is called after
+  * alternatives have been applied.
+  *
+  * That warning function cannot BUG() because the bug splat cannot be
+  * displayed in all possible configurations, leading to users not really
+  * knowing why the machine froze.
+  */
+ SYM_CODE_START(__x86_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
++>>>>>>> 9d9c22cc444a (x86/retpoline: Document some thunk handling aspects)
  EXPORT_SYMBOL(__x86_return_thunk)
  
  #endif /* CONFIG_RETHUNK */
* Unmerged path arch/x86/lib/retpoline.S
