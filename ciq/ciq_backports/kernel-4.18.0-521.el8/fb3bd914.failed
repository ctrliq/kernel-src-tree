x86/srso: Add a Speculative RAS Overflow mitigation

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Borislav Petkov (AMD) <bp@alien8.de>
commit fb3bd914b3ec28f5fb697ac55c4846ac2d542855
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/fb3bd914.failed

Add a mitigation for the speculative return address stack overflow
vulnerability found on AMD processors.

The mitigation works by ensuring all RET instructions speculate to
a controlled location, similar to how speculation is controlled in the
retpoline sequence.  To accomplish this, the __x86_return_thunk forces
the CPU to mispredict every function return using a 'safe return'
sequence.

To ensure the safety of this mitigation, the kernel must ensure that the
safe return sequence is itself free from attacker interference.  In Zen3
and Zen4, this is accomplished by creating a BTB alias between the
untraining function srso_untrain_ret_alias() and the safe return
function srso_safe_ret_alias() which results in evicting a potentially
poisoned BTB entry and using that safe one for all function returns.

In older Zen1 and Zen2, this is accomplished using a reinterpretation
technique similar to Retbleed one: srso_untrain_ret() and
srso_safe_ret().

	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
(cherry picked from commit fb3bd914b3ec28f5fb697ac55c4846ac2d542855)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/admin-guide/hw-vuln/index.rst
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/alternative.c
#	arch/x86/kernel/cpu/amd.c
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
#	arch/x86/kernel/vmlinux.lds.S
#	arch/x86/lib/retpoline.S
#	drivers/base/cpu.c
diff --cc Documentation/admin-guide/hw-vuln/index.rst
index 245468b0f2be,ff4d3fa2a75c..000000000000
--- a/Documentation/admin-guide/hw-vuln/index.rst
+++ b/Documentation/admin-guide/hw-vuln/index.rst
@@@ -15,5 -15,8 +15,10 @@@ are configurable at compile, boot or ru
     tsx_async_abort
     multihit.rst
     special-register-buffer-data-sampling.rst
 -   core-scheduling.rst
 -   l1d_flush.rst
     processor_mmio_stale_data.rst
++<<<<<<< HEAD
 +   gather_data_sampling.rst
++=======
+    cross-thread-rsb.rst
+    srso
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
diff --cc arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70,43fe1c747085..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -120,6 -207,31 +120,34 @@@
  #define ANNOTATE_UNRET_SAFE ANNOTATE_RETPOLINE_SAFE
  
  /*
++<<<<<<< HEAD
++=======
+  * Abuse ANNOTATE_RETPOLINE_SAFE on a NOP to indicate UNRET_END, should
+  * eventually turn into it's own annotation.
+  */
+ .macro VALIDATE_UNRET_END
+ #if defined(CONFIG_NOINSTR_VALIDATION) && \
+ 	(defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_SRSO))
+ 	ANNOTATE_RETPOLINE_SAFE
+ 	nop
+ #endif
+ .endm
+ 
+ /*
+  * Equivalent to -mindirect-branch-cs-prefix; emit the 5 byte jmp/call
+  * to the retpoline thunk with a CS prefix when the register requires
+  * a RAX prefix byte to encode. Also see apply_retpolines().
+  */
+ .macro __CS_PREFIX reg:req
+ 	.irp rs,r8,r9,r10,r11,r12,r13,r14,r15
+ 	.ifc \reg,\rs
+ 	.byte 0x2e
+ 	.endif
+ 	.endr
+ .endm
+ 
+ /*
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
   * JMP_NOSPEC and CALL_NOSPEC macros can be used instead of a simple
   * indirect jmp/call which may be susceptible to the Spectre variant 2
   * attack.
@@@ -189,10 -289,42 +217,49 @@@
   * where we have a stack but before any RET instruction.
   */
  .macro UNTRAIN_RET
++<<<<<<< HEAD
 +#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY)
 +	ALTERNATIVE_2 "",						\
 +	              CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
 +		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB
++=======
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \
+ 	defined(CONFIG_CALL_DEPTH_TRACKING)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		      __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 	ALTERNATIVE_2 "", "call srso_untrain_ret", X86_FEATURE_SRSO, \
+ 			  "call srso_untrain_ret_alias", X86_FEATURE_SRSO_ALIAS
+ #endif
+ .endm
+ 
+ .macro UNTRAIN_RET_FROM_CALL
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \
+ 	defined(CONFIG_CALL_DEPTH_TRACKING)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		      __stringify(RESET_CALL_DEPTH_FROM_CALL), X86_FEATURE_CALL_DEPTH
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 	ALTERNATIVE_2 "", "call srso_untrain_ret", X86_FEATURE_SRSO, \
+ 			  "call srso_untrain_ret_alias", X86_FEATURE_SRSO_ALIAS
+ #endif
+ .endm
+ 
+ 
+ .macro CALL_DEPTH_ACCOUNT
+ #ifdef CONFIG_CALL_DEPTH_TRACKING
+ 	ALTERNATIVE "",							\
+ 		    __stringify(ASM_INCREMENT_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  #endif
  .endm
  
@@@ -201,14 -333,69 +268,16 @@@
  #define ANNOTATE_RETPOLINE_SAFE					\
  	"999:\n\t"						\
  	".pushsection .discard.retpoline_safe\n\t"		\
 -	".long 999b - .\n\t"					\
 +	_ASM_PTR " 999b\n\t"					\
  	".popsection\n\t"
  
 -typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
 -extern retpoline_thunk_t __x86_indirect_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];
 -
  extern void __x86_return_thunk(void);
  extern void zen_untrain_ret(void);
+ extern void srso_untrain_ret(void);
+ extern void srso_untrain_ret_alias(void);
  extern void entry_ibpb(void);
  
 -#ifdef CONFIG_CALL_THUNKS
 -extern void (*x86_return_thunk)(void);
 -#else
 -#define x86_return_thunk	(&__x86_return_thunk)
 -#endif
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -extern void __x86_return_skl(void);
 -
 -static inline void x86_set_skl_return_thunk(void)
 -{
 -	x86_return_thunk = &__x86_return_skl;
 -}
 -
 -#define CALL_DEPTH_ACCOUNT					\
 -	ALTERNATIVE("",						\
 -		    __stringify(INCREMENT_CALL_DEPTH),		\
 -		    X86_FEATURE_CALL_DEPTH)
 -
 -#ifdef CONFIG_CALL_THUNKS_DEBUG
 -DECLARE_PER_CPU(u64, __x86_call_count);
 -DECLARE_PER_CPU(u64, __x86_ret_count);
 -DECLARE_PER_CPU(u64, __x86_stuffs_count);
 -DECLARE_PER_CPU(u64, __x86_ctxsw_count);
 -#endif
 -#else
 -static inline void x86_set_skl_return_thunk(void) {}
 -
 -#define CALL_DEPTH_ACCOUNT ""
 -
 -#endif
 -
  #ifdef CONFIG_RETPOLINE
 -
 -#define GEN(reg) \
 -	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_call_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_jump_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
  #ifdef CONFIG_X86_64
  
  /*
diff --cc arch/x86/kernel/alternative.c
index 76e6ae92f7f2,920a8ca7a8f8..000000000000
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@@ -461,51 -706,555 +461,64 @@@ static int patch_return(void *addr, str
  {
  	int i = 0;
  
++<<<<<<< HEAD
 +	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
 +		return -1;
 +
 +	bytes[i++] = RET_INSN_OPCODE;
++=======
+ 	/* Patch the custom return thunks... */
+ 	if (cpu_feature_enabled(X86_FEATURE_RETHUNK) ||
 -	    cpu_feature_enabled(X86_FEATURE_SRSO) ||
 -	    cpu_feature_enabled(X86_FEATURE_SRSO_ALIAS)) {
 -		i = JMP32_INSN_SIZE;
 -		__text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);
 -	} else {
 -		/* ... or patch them out if not needed. */
 -		bytes[i++] = RET_INSN_OPCODE;
 -	}
 -
 -	for (; i < insn->length;)
 -		bytes[i++] = INT3_INSN_OPCODE;
 -	return i;
 -}
 -
 -void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	/*
 -	 * Do not patch out the default return thunks if those needed are the
 -	 * ones generated by the compiler.
 -	 */
 -	if (cpu_feature_enabled(X86_FEATURE_RETHUNK) &&
 -	    (x86_return_thunk == __x86_return_thunk))
 -		return;
 -
 -	for (s = start; s < end; s++) {
 -		void *dest = NULL, *addr = (void *)s + *s;
 -		struct insn insn;
 -		int len, ret;
 -		u8 bytes[16];
 -		u8 op;
 -
 -		ret = insn_decode_kernel(&insn, addr);
 -		if (WARN_ON_ONCE(ret < 0))
 -			continue;
 -
 -		op = insn.opcode.bytes[0];
 -		if (op == JMP32_INSN_OPCODE)
 -			dest = addr + insn.length + insn.immediate.value;
 -
 -		if (__static_call_fixup(addr, op, dest) ||
 -		    WARN_ONCE(dest != &__x86_return_thunk,
 -			      "missing return thunk: %pS-%pS: %*ph",
 -			      addr, dest, 5, addr))
 -			continue;
 -
 -		DPRINTK(RET, "return thunk at: %pS (%px) len: %d to: %pS",
 -			addr, addr, insn.length,
 -			addr + insn.length + insn.immediate.value);
 -
 -		len = patch_return(addr, &insn, bytes);
 -		if (len == insn.length) {
 -			DUMP_BYTES(RET, ((u8*)addr),  len, "%px: orig: ", addr);
 -			DUMP_BYTES(RET, ((u8*)bytes), len, "%px: repl: ", addr);
 -			text_poke_early(addr, bytes, len);
 -		}
 -	}
 -}
 -#else
 -void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 -#endif /* CONFIG_RETHUNK */
 -
 -#else /* !CONFIG_RETPOLINE || !CONFIG_OBJTOOL */
 -
 -void __init_or_module noinline apply_retpolines(s32 *start, s32 *end) { }
 -void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 -
 -#endif /* CONFIG_RETPOLINE && CONFIG_OBJTOOL */
 -
 -#ifdef CONFIG_X86_KERNEL_IBT
 -
 -static void poison_cfi(void *addr);
 -
 -static void __init_or_module poison_endbr(void *addr, bool warn)
 -{
 -	u32 endbr, poison = gen_endbr_poison();
 -
 -	if (WARN_ON_ONCE(get_kernel_nofault(endbr, addr)))
 -		return;
 -
 -	if (!is_endbr(endbr)) {
 -		WARN_ON_ONCE(warn);
 -		return;
 -	}
 -
 -	DPRINTK(ENDBR, "ENDBR at: %pS (%px)", addr, addr);
 -
 -	/*
 -	 * When we have IBT, the lack of ENDBR will trigger #CP
 -	 */
 -	DUMP_BYTES(ENDBR, ((u8*)addr), 4, "%px: orig: ", addr);
 -	DUMP_BYTES(ENDBR, ((u8*)&poison), 4, "%px: repl: ", addr);
 -	text_poke_early(addr, &poison, 4);
 -}
 -
 -/*
 - * Generated by: objtool --ibt
 - *
 - * Seal the functions for indirect calls by clobbering the ENDBR instructions
 - * and the kCFI hash value.
 - */
 -void __init_or_module noinline apply_seal_endbr(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -
 -		poison_endbr(addr, true);
 -		if (IS_ENABLED(CONFIG_FINEIBT))
 -			poison_cfi(addr - 16);
 -	}
 -}
 -
 -#else
 -
 -void __init_or_module apply_seal_endbr(s32 *start, s32 *end) { }
 -
 -#endif /* CONFIG_X86_KERNEL_IBT */
 -
 -#ifdef CONFIG_FINEIBT
 -
 -enum cfi_mode {
 -	CFI_DEFAULT,
 -	CFI_OFF,
 -	CFI_KCFI,
 -	CFI_FINEIBT,
 -};
 -
 -static enum cfi_mode cfi_mode __ro_after_init = CFI_DEFAULT;
 -static bool cfi_rand __ro_after_init = true;
 -static u32  cfi_seed __ro_after_init;
 -
 -/*
 - * Re-hash the CFI hash with a boot-time seed while making sure the result is
 - * not a valid ENDBR instruction.
 - */
 -static u32 cfi_rehash(u32 hash)
 -{
 -	hash ^= cfi_seed;
 -	while (unlikely(is_endbr(hash) || is_endbr(-hash))) {
 -		bool lsb = hash & 1;
 -		hash >>= 1;
 -		if (lsb)
 -			hash ^= 0x80200003;
 -	}
 -	return hash;
 -}
 -
 -static __init int cfi_parse_cmdline(char *str)
 -{
 -	if (!str)
 -		return -EINVAL;
 -
 -	while (str) {
 -		char *next = strchr(str, ',');
 -		if (next) {
 -			*next = 0;
 -			next++;
 -		}
 -
 -		if (!strcmp(str, "auto")) {
 -			cfi_mode = CFI_DEFAULT;
 -		} else if (!strcmp(str, "off")) {
 -			cfi_mode = CFI_OFF;
 -			cfi_rand = false;
 -		} else if (!strcmp(str, "kcfi")) {
 -			cfi_mode = CFI_KCFI;
 -		} else if (!strcmp(str, "fineibt")) {
 -			cfi_mode = CFI_FINEIBT;
 -		} else if (!strcmp(str, "norand")) {
 -			cfi_rand = false;
 -		} else {
 -			pr_err("Ignoring unknown cfi option (%s).", str);
 -		}
 -
 -		str = next;
 -	}
 -
 -	return 0;
 -}
 -early_param("cfi", cfi_parse_cmdline);
 -
 -/*
 - * kCFI						FineIBT
 - *
 - * __cfi_\func:					__cfi_\func:
 - *	movl   $0x12345678,%eax		// 5	     endbr64			// 4
 - *	nop					     subl   $0x12345678,%r10d   // 7
 - *	nop					     jz     1f			// 2
 - *	nop					     ud2			// 2
 - *	nop					1:   nop			// 1
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *
 - *
 - * caller:					caller:
 - *	movl	$(-0x12345678),%r10d	 // 6	     movl   $0x12345678,%r10d	// 6
 - *	addl	$-15(%r11),%r10d	 // 4	     sub    $16,%r11		// 4
 - *	je	1f			 // 2	     nop4			// 4
 - *	ud2				 // 2
 - * 1:	call	__x86_indirect_thunk_r11 // 5	     call   *%r11; nop2;	// 5
 - *
 - */
 -
 -asm(	".pushsection .rodata			\n"
 -	"fineibt_preamble_start:		\n"
 -	"	endbr64				\n"
 -	"	subl	$0x12345678, %r10d	\n"
 -	"	je	fineibt_preamble_end	\n"
 -	"	ud2				\n"
 -	"	nop				\n"
 -	"fineibt_preamble_end:			\n"
 -	".popsection\n"
 -);
 -
 -extern u8 fineibt_preamble_start[];
 -extern u8 fineibt_preamble_end[];
 -
 -#define fineibt_preamble_size (fineibt_preamble_end - fineibt_preamble_start)
 -#define fineibt_preamble_hash 7
 -
 -asm(	".pushsection .rodata			\n"
 -	"fineibt_caller_start:			\n"
 -	"	movl	$0x12345678, %r10d	\n"
 -	"	sub	$16, %r11		\n"
 -	ASM_NOP4
 -	"fineibt_caller_end:			\n"
 -	".popsection				\n"
 -);
 -
 -extern u8 fineibt_caller_start[];
 -extern u8 fineibt_caller_end[];
 -
 -#define fineibt_caller_size (fineibt_caller_end - fineibt_caller_start)
 -#define fineibt_caller_hash 2
 -
 -#define fineibt_caller_jmp (fineibt_caller_size - 2)
 -
 -static u32 decode_preamble_hash(void *addr)
 -{
 -	u8 *p = addr;
 -
 -	/* b8 78 56 34 12          mov    $0x12345678,%eax */
 -	if (p[0] == 0xb8)
 -		return *(u32 *)(addr + 1);
 -
 -	return 0; /* invalid hash value */
 -}
 -
 -static u32 decode_caller_hash(void *addr)
 -{
 -	u8 *p = addr;
 -
 -	/* 41 ba 78 56 34 12       mov    $0x12345678,%r10d */
 -	if (p[0] == 0x41 && p[1] == 0xba)
 -		return -*(u32 *)(addr + 2);
 -
 -	/* e8 0c 78 56 34 12	   jmp.d8  +12 */
 -	if (p[0] == JMP8_INSN_OPCODE && p[1] == fineibt_caller_jmp)
 -		return -*(u32 *)(addr + 2);
 -
 -	return 0; /* invalid hash value */
 -}
 -
 -/* .retpoline_sites */
 -static int cfi_disable_callers(s32 *start, s32 *end)
 -{
 -	/*
 -	 * Disable kCFI by patching in a JMP.d8, this leaves the hash immediate
 -	 * in tact for later usage. Also see decode_caller_hash() and
 -	 * cfi_rewrite_callers().
 -	 */
 -	const u8 jmp[] = { JMP8_INSN_OPCODE, fineibt_caller_jmp };
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (!hash) /* nocfi callers */
 -			continue;
 -
 -		text_poke_early(addr, jmp, 2);
 -	}
 -
 -	return 0;
 -}
 -
 -static int cfi_enable_callers(s32 *start, s32 *end)
 -{
 -	/*
 -	 * Re-enable kCFI, undo what cfi_disable_callers() did.
 -	 */
 -	const u8 mov[] = { 0x41, 0xba };
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (!hash) /* nocfi callers */
 -			continue;
 -
 -		text_poke_early(addr, mov, 2);
 -	}
 -
 -	return 0;
 -}
 -
 -/* .cfi_sites */
 -static int cfi_rand_preamble(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		hash = decode_preamble_hash(addr);
 -		if (WARN(!hash, "no CFI hash found at: %pS %px %*ph\n",
 -			 addr, addr, 5, addr))
 -			return -EINVAL;
 -
 -		hash = cfi_rehash(hash);
 -		text_poke_early(addr + 1, &hash, 4);
 -	}
 -
 -	return 0;
 -}
 -
 -static int cfi_rewrite_preamble(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		hash = decode_preamble_hash(addr);
 -		if (WARN(!hash, "no CFI hash found at: %pS %px %*ph\n",
 -			 addr, addr, 5, addr))
 -			return -EINVAL;
 -
 -		text_poke_early(addr, fineibt_preamble_start, fineibt_preamble_size);
 -		WARN_ON(*(u32 *)(addr + fineibt_preamble_hash) != 0x12345678);
 -		text_poke_early(addr + fineibt_preamble_hash, &hash, 4);
 -	}
 -
 -	return 0;
 -}
 -
 -static void cfi_rewrite_endbr(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -
 -		poison_endbr(addr+16, false);
 -	}
 -}
 -
 -/* .retpoline_sites */
 -static int cfi_rand_callers(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (hash) {
 -			hash = -cfi_rehash(hash);
 -			text_poke_early(addr + 2, &hash, 4);
 -		}
 -	}
 -
 -	return 0;
 -}
 -
 -static int cfi_rewrite_callers(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (hash) {
 -			text_poke_early(addr, fineibt_caller_start, fineibt_caller_size);
 -			WARN_ON(*(u32 *)(addr + fineibt_caller_hash) != 0x12345678);
 -			text_poke_early(addr + fineibt_caller_hash, &hash, 4);
 -		}
 -		/* rely on apply_retpolines() */
 -	}
 -
 -	return 0;
 -}
 -
 -static void __apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,
 -			    s32 *start_cfi, s32 *end_cfi, bool builtin)
 -{
 -	int ret;
 -
 -	if (WARN_ONCE(fineibt_preamble_size != 16,
 -		      "FineIBT preamble wrong size: %ld", fineibt_preamble_size))
 -		return;
 -
 -	if (cfi_mode == CFI_DEFAULT) {
 -		cfi_mode = CFI_KCFI;
 -		if (HAS_KERNEL_IBT && cpu_feature_enabled(X86_FEATURE_IBT))
 -			cfi_mode = CFI_FINEIBT;
 -	}
 -
 -	/*
 -	 * Rewrite the callers to not use the __cfi_ stubs, such that we might
 -	 * rewrite them. This disables all CFI. If this succeeds but any of the
 -	 * later stages fails, we're without CFI.
 -	 */
 -	ret = cfi_disable_callers(start_retpoline, end_retpoline);
 -	if (ret)
 -		goto err;
 -
 -	if (cfi_rand) {
 -		if (builtin)
 -			cfi_seed = get_random_u32();
 -
 -		ret = cfi_rand_preamble(start_cfi, end_cfi);
 -		if (ret)
 -			goto err;
 -
 -		ret = cfi_rand_callers(start_retpoline, end_retpoline);
 -		if (ret)
 -			goto err;
 -	}
 -
 -	switch (cfi_mode) {
 -	case CFI_OFF:
 -		if (builtin)
 -			pr_info("Disabling CFI\n");
 -		return;
 -
 -	case CFI_KCFI:
 -		ret = cfi_enable_callers(start_retpoline, end_retpoline);
 -		if (ret)
 -			goto err;
 -
 -		if (builtin)
 -			pr_info("Using kCFI\n");
 -		return;
 -
 -	case CFI_FINEIBT:
 -		/* place the FineIBT preamble at func()-16 */
 -		ret = cfi_rewrite_preamble(start_cfi, end_cfi);
 -		if (ret)
 -			goto err;
 -
 -		/* rewrite the callers to target func()-16 */
 -		ret = cfi_rewrite_callers(start_retpoline, end_retpoline);
 -		if (ret)
 -			goto err;
 -
 -		/* now that nobody targets func()+0, remove ENDBR there */
 -		cfi_rewrite_endbr(start_cfi, end_cfi);
 -
 -		if (builtin)
 -			pr_info("Using FineIBT CFI\n");
 -		return;
 -
 -	default:
 -		break;
++	    cpu_feature_enabled(X86_FEATURE_SRSO) ||
++	    cpu_feature_enabled(X86_FEATURE_SRSO_ALIAS)) {
++		i = JMP32_INSN_SIZE;
++		__text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);
++	} else {
++		/* ... or patch them out if not needed. */
++		bytes[i++] = RET_INSN_OPCODE;
+ 	}
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  
 -err:
 -	pr_err("Something went horribly wrong trying to rewrite the CFI implementation.\n");
 -}
 +	for (; i < insn->length;)
 +		bytes[i++] = INT3_INSN_OPCODE;
  
 -static inline void poison_hash(void *addr)
 -{
 -	*(u32 *)addr = 0;
 +	return i;
  }
  
 -static void poison_cfi(void *addr)
 +void __init_or_module noinline apply_returns(s32 *start, s32 *end)
  {
 -	switch (cfi_mode) {
 -	case CFI_FINEIBT:
 -		/*
 -		 * __cfi_\func:
 -		 *	osp nopl (%rax)
 -		 *	subl	$0, %r10d
 -		 *	jz	1f
 -		 *	ud2
 -		 * 1:	nop
 -		 */
 -		poison_endbr(addr, false);
 -		poison_hash(addr + fineibt_preamble_hash);
 -		break;
 -
 -	case CFI_KCFI:
 -		/*
 -		 * __cfi_\func:
 -		 *	movl	$0, %eax
 -		 *	.skip	11, 0x90
 -		 */
 -		poison_hash(addr + 1);
 -		break;
 -
 -	default:
 -		break;
 -	}
 -}
 +	s32 *s;
  
 -#else
 +	for (s = start; s < end; s++) {
 +		void *addr = (void *)s + *s;
 +		struct insn insn;
 +		int len, ret;
 +		u8 bytes[16];
 +		u8 op1;
  
 -static void __apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,
 -			    s32 *start_cfi, s32 *end_cfi, bool builtin)
 -{
 -}
 +		ret = insn_decode_kernel(&insn, addr);
 +		if (WARN_ON_ONCE(ret < 0))
 +			continue;
  
 -#ifdef CONFIG_X86_KERNEL_IBT
 -static void poison_cfi(void *addr) { }
 -#endif
 +		op1 = insn.opcode.bytes[0];
 +		if (WARN_ON_ONCE(op1 != JMP32_INSN_OPCODE))
 +			continue;
  
 -#endif
 +		DPRINTK("return thunk at: %pS (%px) len: %d to: %pS",
 +			addr, addr, insn.length,
 +			addr + insn.length + insn.immediate.value);
  
 -void apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,
 -		   s32 *start_cfi, s32 *end_cfi)
 -{
 -	return __apply_fineibt(start_retpoline, end_retpoline,
 -			       start_cfi, end_cfi,
 -			       /* .builtin = */ false);
 +		len = patch_return(addr, &insn, bytes);
 +		if (len == insn.length) {
 +			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
 +			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
 +			text_poke_early(addr, bytes, len);
 +		}
 +	}
  }
 +#else
 +void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 +#endif /* CONFIG_RETHUNK */
  
  #ifdef CONFIG_SMP
  static void alternatives_smp_lock(const s32 *start, const s32 *end,
diff --cc arch/x86/kernel/cpu/amd.c
index 92e42262f300,169cb255c483..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -1318,14 -1236,16 +1318,29 @@@ u32 amd_get_highest_perf(void
  }
  EXPORT_SYMBOL_GPL(amd_get_highest_perf);
  
++<<<<<<< HEAD
 +static void zenbleed_check_cpu(void *unused)
 +{
 +	struct cpuinfo_x86 *c = &cpu_data(smp_processor_id());
 +
 +	zenbleed_check(c);
 +}
 +
 +void amd_check_microcode(void)
 +{
 +	on_each_cpu(zenbleed_check_cpu, NULL, 1);
++=======
+ bool cpu_has_ibpb_brtype_microcode(void)
+ {
+ 	u8 fam = boot_cpu_data.x86;
+ 
+ 	if (fam == 0x17) {
+ 		/* Zen1/2 IBPB flushes branch type predictions too. */
+ 		return boot_cpu_has(X86_FEATURE_AMD_IBPB);
+ 	} else if (fam == 0x19) {
+ 		return false;
+ 	}
+ 
+ 	return false;
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  }
diff --cc arch/x86/kernel/cpu/bugs.c
index adea0002a1a8,31cef61da03a..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -48,7 -46,8 +48,12 @@@ static void __init md_clear_select_miti
  static void __init taa_select_mitigation(void);
  static void __init mmio_select_mitigation(void);
  static void __init srbds_select_mitigation(void);
++<<<<<<< HEAD
 +static void __init gds_select_mitigation(void);
++=======
+ static void __init l1d_flush_select_mitigation(void);
+ static void __init srso_select_mitigation(void);
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  
  /* The base value of the SPEC_CTRL MSR without task-specific bits set */
  u64 x86_spec_ctrl_base;
@@@ -161,40 -160,8 +166,45 @@@ void __init check_bugs(void
  	l1tf_select_mitigation();
  	md_clear_select_mitigation();
  	srbds_select_mitigation();
++<<<<<<< HEAD
 +	gds_select_mitigation();
 +
 +	arch_smt_update();
 +
 +#ifdef CONFIG_X86_32
 +	/*
 +	 * Check whether we are able to run this kernel safely on SMP.
 +	 *
 +	 * - i386 is no longer supported.
 +	 * - In order to run on anything without a TSC, we need to be
 +	 *   compiled for a i486.
 +	 */
 +	if (boot_cpu_data.x86 < 4)
 +		panic("Kernel requires i486+ for 'invlpg' and other features");
 +
 +	init_utsname()->machine[1] =
 +		'0' + (boot_cpu_data.x86 > 6 ? 6 : boot_cpu_data.x86);
 +	alternative_instructions();
 +
 +	fpu__init_check_bugs();
 +#else /* CONFIG_X86_64 */
 +	alternative_instructions();
 +
 +	/*
 +	 * Make sure the first 2MB area is not mapped by huge pages
 +	 * There are typically fixed size MTRRs in there and overlapping
 +	 * MTRRs into large pages causes slow downs.
 +	 *
 +	 * Right now we don't do that with gbpages because there seems
 +	 * very little benefit for that case.
 +	 */
 +	if (!direct_gbpages)
 +		set_memory_4k((unsigned long)__va(0), 1);
 +#endif
++=======
+ 	l1d_flush_select_mitigation();
+ 	srso_select_mitigation();
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  }
  
  /*
@@@ -2506,9 -2473,11 +2605,17 @@@ static ssize_t retbleed_show_state(cha
  	return sysfs_emit(buf, "%s\n", retbleed_strings[retbleed_mitigation]);
  }
  
++<<<<<<< HEAD
 +static ssize_t gds_show_state(char *buf)
 +{
 +	return sysfs_emit(buf, "%s\n", gds_strings[gds_mitigation]);
++=======
+ static ssize_t srso_show_state(char *buf)
+ {
+ 	return sysfs_emit(buf, "%s%s\n",
+ 			  srso_strings[srso_mitigation],
+ 			  (cpu_has_ibpb_brtype_microcode() ? "" : ", no microcode"));
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  }
  
  static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
@@@ -2560,8 -2529,8 +2667,13 @@@
  	case X86_BUG_RETBLEED:
  		return retbleed_show_state(buf);
  
++<<<<<<< HEAD
 +	case X86_BUG_GDS:
 +		return gds_show_state(buf);
++=======
+ 	case X86_BUG_SRSO:
+ 		return srso_show_state(buf);
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  
  	default:
  		break;
@@@ -2628,8 -2597,8 +2740,14 @@@ ssize_t cpu_show_retbleed(struct devic
  	return cpu_show_common(dev, attr, buf, X86_BUG_RETBLEED);
  }
  
++<<<<<<< HEAD
 +ssize_t cpu_show_gds(struct device *dev, struct device_attribute *attr, char *buf)
 +{
 +	return cpu_show_common(dev, attr, buf, X86_BUG_GDS);
++=======
+ ssize_t cpu_show_spec_rstack_overflow(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return cpu_show_common(dev, attr, buf, X86_BUG_SRSO);
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  }
  #endif
diff --cc arch/x86/kernel/cpu/common.c
index 6335deb6c15c,d4d823eae0fc..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -1147,10 -1248,12 +1147,17 @@@ static const __initconst struct x86_cpu
  #define MMIO_SBDS	BIT(2)
  /* CPU is affected by RETbleed, speculating where you would not expect it */
  #define RETBLEED	BIT(3)
++<<<<<<< HEAD
 +/* CPU is affected by GDS */
 +#define GDS		BIT(5)
++=======
+ /* CPU is affected by SMT (cross-thread) return predictions */
+ #define SMT_RSB		BIT(4)
+ /* CPU is affected by SRSO */
+ #define SRSO		BIT(5)
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  
 -static const struct x86_cpu_id cpu_vuln_blacklist[] __initconst = {
 +static const struct x86_cpu_id_v2 cpu_vuln_blacklist[] __initconst = {
  	VULNBL_INTEL_STEPPINGS(IVYBRIDGE,	X86_STEPPING_ANY,		SRBDS),
  	VULNBL_INTEL_STEPPINGS(HASWELL,		X86_STEPPING_ANY,		SRBDS),
  	VULNBL_INTEL_STEPPINGS(HASWELL_L,	X86_STEPPING_ANY,		SRBDS),
@@@ -1182,8 -1283,9 +1189,14 @@@
  
  	VULNBL_AMD(0x15, RETBLEED),
  	VULNBL_AMD(0x16, RETBLEED),
++<<<<<<< HEAD
 +	VULNBL_AMD(0x17, RETBLEED),
 +	VULNBL_HYGON(0x18, RETBLEED),
++=======
+ 	VULNBL_AMD(0x17, RETBLEED | SMT_RSB | SRSO),
+ 	VULNBL_HYGON(0x18, RETBLEED | SMT_RSB),
+ 	VULNBL_AMD(0x19, SRSO),
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  	{}
  };
  
@@@ -1303,16 -1406,12 +1316,19 @@@ static void __init cpu_set_bug_bits(str
  			setup_force_cpu_bug(X86_BUG_RETBLEED);
  	}
  
 -	if (cpu_matches(cpu_vuln_blacklist, SMT_RSB))
 -		setup_force_cpu_bug(X86_BUG_SMT_RSB);
 +	/*
 +	 * Check if CPU is vulnerable to GDS. If running in a virtual machine on
 +	 * an affected processor, the VMM may have disabled the use of GATHER by
 +	 * disabling AVX2. The only way to do this in HW is to clear XCR0[2],
 +	 * which means that AVX will be disabled.
 +	 */
 +	if (cpu_matches(cpu_vuln_blacklist, GDS) && !(ia32_cap & ARCH_CAP_GDS_NO) &&
 +	    boot_cpu_has(X86_FEATURE_AVX))
 +		setup_force_cpu_bug(X86_BUG_GDS);
  
+ 	if (cpu_matches(cpu_vuln_blacklist, SRSO))
+ 		setup_force_cpu_bug(X86_BUG_SRSO);
+ 
  	if (cpu_matches(cpu_vuln_whitelist, NO_MELTDOWN))
  		return;
  
diff --cc arch/x86/kernel/vmlinux.lds.S
index 6df2973aa8ce,e76813230192..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -122,36 -129,44 +122,57 @@@ SECTION
  		HEAD_TEXT
  		TEXT_TEXT
  		SCHED_TEXT
 +		CPUIDLE_TEXT
  		LOCK_TEXT
  		KPROBES_TEXT
 +		ALIGN_ENTRY_TEXT_BEGIN
 +		ENTRY_TEXT
 +		IRQENTRY_TEXT
 +		ALIGN_ENTRY_TEXT_END
  		SOFTIRQENTRY_TEXT
 +		*(.fixup)
 +		*(.gnu.warning)
 +
  #ifdef CONFIG_RETPOLINE
  		__indirect_thunk_start = .;
- 		*(.text.__x86.*)
+ 		*(.text.__x86.indirect_thunk)
+ 		*(.text.__x86.return_thunk)
  		__indirect_thunk_end = .;
  #endif
 -		STATIC_CALL_TEXT
  
++<<<<<<< HEAD
 +		/* End of text section */
 +		_etext = .;
 +	} :text = 0x9090
++=======
+ 		ALIGN_ENTRY_TEXT_BEGIN
+ #ifdef CONFIG_CPU_SRSO
+ 		*(.text.__x86.rethunk_untrain)
+ #endif
+ 
+ 		ENTRY_TEXT
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 		/*
+ 		 * See the comment above srso_untrain_ret_alias()'s
+ 		 * definition.
+ 		 */
+ 		. = srso_untrain_ret_alias | (1 << 2) | (1 << 8) | (1 << 14) | (1 << 20);
+ 		*(.text.__x86.rethunk_safe)
+ #endif
+ 		ALIGN_ENTRY_TEXT_END
+ 		*(.gnu.warning)
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  
 -	} :text =0xcccc
 +	NOTES :text :note
  
 -	/* End of text section, which should occupy whole number of pages */
 -	_etext = .;
 -	. = ALIGN(PAGE_SIZE);
 +	EXCEPTION_TABLE(16) :text = 0x9090
  
 -	X86_ALIGN_RODATA_BEGIN
 +	/* .text should occupy whole number of pages */
 +	. = ALIGN(PAGE_SIZE);
 +	X64_ALIGN_RODATA_BEGIN
  	RO_DATA(PAGE_SIZE)
 -	X86_ALIGN_RODATA_END
 +	X64_ALIGN_RODATA_END
  
  	/* Data */
  	.data : AT(ADDR(.data) - LOAD_OFFSET) {
@@@ -441,12 -522,19 +462,28 @@@ INIT_PER_CPU(irq_stack_backing_store)
             "fixed_percpu_data is not at start of per-cpu area");
  #endif
  
++<<<<<<< HEAD
 +#endif /* CONFIG_X86_32 */
 +
 +#ifdef CONFIG_KEXEC_CORE
 +#include <asm/kexec.h>
 +
 +. = ASSERT(kexec_control_code_size <= KEXEC_CONTROL_CODE_MAX_SIZE,
 +           "kexec control code size is too big");
++=======
+ #ifdef CONFIG_RETHUNK
+ . = ASSERT((__ret & 0x3f) == 0, "__ret not cacheline-aligned");
+ . = ASSERT((srso_safe_ret & 0x3f) == 0, "srso_safe_ret not cacheline-aligned");
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ /*
+  * GNU ld cannot do XOR so do: (A | B) - (A & B) in order to compute the XOR
+  * of the two function addresses:
+  */
+ . = ASSERT(((srso_untrain_ret_alias | srso_safe_ret_alias) -
+ 		(srso_untrain_ret_alias & srso_safe_ret_alias)) == ((1 << 2) | (1 << 8) | (1 << 14) | (1 << 20)),
+ 		"SRSO function pair won't alias");
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  #endif
  
 -#endif /* CONFIG_X86_64 */
diff --cc arch/x86/lib/retpoline.S
index ce2b806fe45d,845cfb0d748f..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -5,17 -5,42 +5,41 @@@
  #include <asm/dwarf2.h>
  #include <asm/cpufeatures.h>
  #include <asm/alternative.h>
 -#include <asm/asm-offsets.h>
  #include <asm/export.h>
  #include <asm/nospec-branch.h>
++<<<<<<< HEAD
++=======
+ #include <asm/unwind_hints.h>
+ #include <asm/percpu.h>
+ #include <asm/frame.h>
+ #include <asm/nops.h>
+ 
+ 	.section .text.__x86.indirect_thunk
+ 
+ 
+ .macro POLINE reg
+ 	ANNOTATE_INTRA_FUNCTION_CALL
+ 	call    .Ldo_rop_\@
+ 	int3
+ .Ldo_rop_\@:
+ 	mov     %\reg, (%_ASM_SP)
+ 	UNWIND_HINT_FUNC
+ .endm
+ 
+ .macro RETPOLINE reg
+ 	POLINE \reg
+ 	RET
+ .endm
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  
  .macro THUNK reg
 +	.section .text.__x86.indirect_thunk
  
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_INNER_LABEL(__x86_indirect_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_UNDEFINED
 -	ANNOTATE_NOENDBR
 -
 -	ALTERNATIVE_2 __stringify(RETPOLINE \reg), \
 -		      __stringify(lfence; ANNOTATE_RETPOLINE_SAFE; jmp *%\reg; int3), X86_FEATURE_RETPOLINE_LFENCE, \
 -		      __stringify(ANNOTATE_RETPOLINE_SAFE; jmp *%\reg), ALT_NOT(X86_FEATURE_RETPOLINE)
 -
 +SYM_FUNC_START(__x86_indirect_thunk_\reg)
 +	CFI_STARTPROC
 +	JMP_NOSPEC %\reg
 +	CFI_ENDPROC
 +SYM_FUNC_END(__x86_indirect_thunk_\reg)
  .endm
  
  /*
@@@ -65,7 -183,7 +128,11 @@@ SYM_FUNC_END(srso_safe_ret_alias
   *    from re-poisioning the BTB prediction.
   */
  	.align 64
++<<<<<<< HEAD
 +	.skip 63, 0xcc
++=======
+ 	.skip 64 - (__ret - zen_untrain_ret), 0xcc
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  SYM_START(zen_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
  	ANNOTATE_NOENDBR
  	/*
diff --cc drivers/base/cpu.c
index 87400da7f43b,f111586d1cce..000000000000
--- a/drivers/base/cpu.c
+++ b/drivers/base/cpu.c
@@@ -575,8 -577,8 +575,13 @@@ ssize_t __weak cpu_show_retbleed(struc
  	return sysfs_emit(buf, "Not affected\n");
  }
  
++<<<<<<< HEAD
 +ssize_t __weak cpu_show_gds(struct device *dev,
 +			    struct device_attribute *attr, char *buf)
++=======
+ ssize_t __weak cpu_show_spec_rstack_overflow(struct device *dev,
+ 					     struct device_attribute *attr, char *buf)
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  {
  	return sysfs_emit(buf, "Not affected\n");
  }
@@@ -592,7 -594,7 +597,11 @@@ static DEVICE_ATTR(itlb_multihit, 0444
  static DEVICE_ATTR(srbds, 0444, cpu_show_srbds, NULL);
  static DEVICE_ATTR(mmio_stale_data, 0444, cpu_show_mmio_stale_data, NULL);
  static DEVICE_ATTR(retbleed, 0444, cpu_show_retbleed, NULL);
++<<<<<<< HEAD
 +static DEVICE_ATTR(gather_data_sampling, 0444, cpu_show_gds, NULL);
++=======
+ static DEVICE_ATTR(spec_rstack_overflow, 0444, cpu_show_spec_rstack_overflow, NULL);
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  
  static struct attribute *cpu_root_vulnerabilities_attrs[] = {
  	&dev_attr_meltdown.attr,
@@@ -606,7 -608,7 +615,11 @@@
  	&dev_attr_srbds.attr,
  	&dev_attr_mmio_stale_data.attr,
  	&dev_attr_retbleed.attr,
++<<<<<<< HEAD
 +	&dev_attr_gather_data_sampling.attr,
++=======
+ 	&dev_attr_spec_rstack_overflow.attr,
++>>>>>>> fb3bd914b3ec (x86/srso: Add a Speculative RAS Overflow mitigation)
  	NULL
  };
  
* Unmerged path Documentation/admin-guide/hw-vuln/index.rst
diff --git a/Documentation/admin-guide/hw-vuln/srso.rst b/Documentation/admin-guide/hw-vuln/srso.rst
new file mode 100644
index 000000000000..32eb5e6db272
--- /dev/null
+++ b/Documentation/admin-guide/hw-vuln/srso.rst
@@ -0,0 +1,133 @@
+.. SPDX-License-Identifier: GPL-2.0
+
+Speculative Return Stack Overflow (SRSO)
+========================================
+
+This is a mitigation for the speculative return stack overflow (SRSO)
+vulnerability found on AMD processors. The mechanism is by now the well
+known scenario of poisoning CPU functional units - the Branch Target
+Buffer (BTB) and Return Address Predictor (RAP) in this case - and then
+tricking the elevated privilege domain (the kernel) into leaking
+sensitive data.
+
+AMD CPUs predict RET instructions using a Return Address Predictor (aka
+Return Address Stack/Return Stack Buffer). In some cases, a non-architectural
+CALL instruction (i.e., an instruction predicted to be a CALL but is
+not actually a CALL) can create an entry in the RAP which may be used
+to predict the target of a subsequent RET instruction.
+
+The specific circumstances that lead to this varies by microarchitecture
+but the concern is that an attacker can mis-train the CPU BTB to predict
+non-architectural CALL instructions in kernel space and use this to
+control the speculative target of a subsequent kernel RET, potentially
+leading to information disclosure via a speculative side-channel.
+
+The issue is tracked under CVE-2023-20569.
+
+Affected processors
+-------------------
+
+AMD Zen, generations 1-4. That is, all families 0x17 and 0x19. Older
+processors have not been investigated.
+
+System information and options
+------------------------------
+
+First of all, it is required that the latest microcode be loaded for
+mitigations to be effective.
+
+The sysfs file showing SRSO mitigation status is:
+
+  /sys/devices/system/cpu/vulnerabilities/spec_rstack_overflow
+
+The possible values in this file are:
+
+ - 'Not affected'               The processor is not vulnerable
+
+ - 'Vulnerable: no microcode'   The processor is vulnerable, no
+                                microcode extending IBPB functionality
+                                to address the vulnerability has been
+                                applied.
+
+ - 'Mitigation: microcode'      Extended IBPB functionality microcode
+                                patch has been applied. It does not
+                                address User->Kernel and Guest->Host
+                                transitions protection but it does
+                                address User->User and VM->VM attack
+                                vectors.
+
+                                (spec_rstack_overflow=microcode)
+
+ - 'Mitigation: safe RET'       Software-only mitigation. It complements
+                                the extended IBPB microcode patch
+                                functionality by addressing User->Kernel 
+                                and Guest->Host transitions protection.
+
+                                Selected by default or by
+                                spec_rstack_overflow=safe-ret
+
+ - 'Mitigation: IBPB'           Similar protection as "safe RET" above
+                                but employs an IBPB barrier on privilege
+                                domain crossings (User->Kernel,
+                                Guest->Host).
+
+                                (spec_rstack_overflow=ibpb)
+
+ - 'Mitigation: IBPB on VMEXIT' Mitigation addressing the cloud provider
+                                scenario - the Guest->Host transitions
+                                only.
+
+                                (spec_rstack_overflow=ibpb-vmexit)
+
+In order to exploit vulnerability, an attacker needs to:
+
+ - gain local access on the machine
+
+ - break kASLR
+
+ - find gadgets in the running kernel in order to use them in the exploit
+
+ - potentially create and pin an additional workload on the sibling
+   thread, depending on the microarchitecture (not necessary on fam 0x19)
+
+ - run the exploit
+
+Considering the performance implications of each mitigation type, the
+default one is 'Mitigation: safe RET' which should take care of most
+attack vectors, including the local User->Kernel one.
+
+As always, the user is advised to keep her/his system up-to-date by
+applying software updates regularly.
+
+The default setting will be reevaluated when needed and especially when
+new attack vectors appear.
+
+As one can surmise, 'Mitigation: safe RET' does come at the cost of some
+performance depending on the workload. If one trusts her/his userspace
+and does not want to suffer the performance impact, one can always
+disable the mitigation with spec_rstack_overflow=off.
+
+Similarly, 'Mitigation: IBPB' is another full mitigation type employing
+an indrect branch prediction barrier after having applied the required
+microcode patch for one's system. This mitigation comes also at
+a performance cost.
+
+Mitigation: safe RET
+--------------------
+
+The mitigation works by ensuring all RET instructions speculate to
+a controlled location, similar to how speculation is controlled in the
+retpoline sequence.  To accomplish this, the __x86_return_thunk forces
+the CPU to mispredict every function return using a 'safe return'
+sequence.
+
+To ensure the safety of this mitigation, the kernel must ensure that the
+safe return sequence is itself free from attacker interference.  In Zen3
+and Zen4, this is accomplished by creating a BTB alias between the
+untraining function srso_untrain_ret_alias() and the safe return
+function srso_safe_ret_alias() which results in evicting a potentially
+poisoned BTB entry and using that safe one for all function returns.
+
+In older Zen1 and Zen2, this is accomplished using a reinterpretation
+technique similar to Retbleed one: srso_untrain_ret() and
+srso_safe_ret().
diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index e4fe25255b8a..6e23890d9af0 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -5080,6 +5080,17 @@
 			Not specifying this option is equivalent to
 			spectre_v2_user=auto.
 
+	spec_rstack_overflow=
+			[X86] Control RAS overflow mitigation on AMD Zen CPUs
+
+			off		- Disable mitigation
+			microcode	- Enable microcode mitigation only
+			safe-ret	- Enable sw-only safe RET mitigation (default)
+			ibpb		- Enable mitigation by issuing IBPB on
+					  kernel entry
+			ibpb-vmexit	- Issue IBPB only on VMEXIT
+					  (cloud-specific mitigation)
+
 	spec_store_bypass_disable=
 			[HW] Control Speculative Store Bypass (SSB) Disable mitigation
 			(Speculative Store Bypass vulnerability)
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index ed4bea645c8d..733f556dfb2d 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -2570,6 +2570,13 @@ config CPU_IBRS_ENTRY
 	  This mitigates both spectre_v2 and retbleed at great cost to
 	  performance.
 
+config CPU_SRSO
+	bool "Mitigate speculative RAS overflow on AMD"
+	depends on CPU_SUP_AMD && X86_64 && RETHUNK
+	default y
+	help
+	  Enable the SRSO mitigation needed on AMD Zen1-4 machines.
+
 config SLS
 	bool "Mitigate Straight-Line-Speculation"
 	depends on CC_HAS_SLS && X86_64
diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h
index b52ff6ff9447..b82be6365a48 100644
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@ -306,6 +306,9 @@
 #define X86_FEATURE_SMBA		(11*32+21) /* "" Slow Memory Bandwidth Allocation */
 #define X86_FEATURE_BMEC		(11*32+22) /* "" Bandwidth Monitoring Event Configuration */
 
+#define X86_FEATURE_SRSO		(11*32+24) /* "" AMD BTB untrain RETs */
+#define X86_FEATURE_SRSO_ALIAS		(11*32+25) /* "" AMD BTB untrain RETs through aliasing */
+
 /* Intel-defined CPU features, CPUID level 0x00000007:1 (EAX), word 12 */
 #define X86_FEATURE_AVX_VNNI		(12*32+ 4) /* AVX VNNI instructions */
 #define X86_FEATURE_AVX512_BF16		(12*32+ 5) /* AVX512 BFLOAT16 instructions */
@@ -474,4 +477,6 @@
 #define X86_BUG_EIBRS_PBRSB		X86_BUG(28) /* EIBRS is vulnerable to Post Barrier RSB Predictions */
 #define X86_BUG_GDS			X86_BUG(30) /* CPU is affected by Gather Data Sampling */
 
+/* BUG word 2 */
+#define X86_BUG_SRSO			X86_BUG(1*32 + 0) /* AMD SRSO bug */
 #endif /* _ASM_X86_CPUFEATURES_H */
* Unmerged path arch/x86/include/asm/nospec-branch.h
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 820104bdee1f..a5ef5b0d5c56 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -883,9 +883,11 @@ extern u16 get_llc_id(unsigned int cpu);
 #ifdef CONFIG_CPU_SUP_AMD
 extern u32 amd_get_nodes_per_socket(void);
 extern u32 amd_get_highest_perf(void);
+extern bool cpu_has_ibpb_brtype_microcode(void);
 #else
 static inline u32 amd_get_nodes_per_socket(void)	{ return 0; }
 static inline u32 amd_get_highest_perf(void)		{ return 0; }
+static inline bool cpu_has_ibpb_brtype_microcode(void)	{ return false; }
 #endif
 
 #define for_each_possible_hypervisor_cpuid_base(function) \
* Unmerged path arch/x86/kernel/alternative.c
* Unmerged path arch/x86/kernel/cpu/amd.c
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
* Unmerged path arch/x86/kernel/vmlinux.lds.S
* Unmerged path arch/x86/lib/retpoline.S
* Unmerged path drivers/base/cpu.c
diff --git a/include/linux/cpu.h b/include/linux/cpu.h
index df6af88049d0..f7ef75d5bcdd 100644
--- a/include/linux/cpu.h
+++ b/include/linux/cpu.h
@@ -70,6 +70,8 @@ extern ssize_t cpu_show_mmio_stale_data(struct device *dev,
 					char *buf);
 extern ssize_t cpu_show_retbleed(struct device *dev,
 				 struct device_attribute *attr, char *buf);
+extern ssize_t cpu_show_spec_rstack_overflow(struct device *dev,
+					     struct device_attribute *attr, char *buf);
 
 extern __printf(4, 5)
 struct device *cpu_device_create(struct device *parent, void *drvdata,
diff --git a/tools/objtool/arch/x86/decode.c b/tools/objtool/arch/x86/decode.c
index 603f71ad0722..8622117bd2a4 100644
--- a/tools/objtool/arch/x86/decode.c
+++ b/tools/objtool/arch/x86/decode.c
@@ -598,5 +598,8 @@ void arch_initial_func_cfi_state(struct cfi_state *state)
 
 bool arch_is_rethunk(struct symbol *sym)
 {
-	return !strcmp(sym->name, "__x86_return_thunk");
+	return !strcmp(sym->name, "__x86_return_thunk") ||
+	       !strcmp(sym->name, "srso_untrain_ret") ||
+	       !strcmp(sym->name, "srso_safe_ret") ||
+	       !strcmp(sym->name, "__ret");
 }
