x86/rethunk: Use SYM_CODE_START[_LOCAL]_NOALIGN macros

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Josh Poimboeuf <jpoimboe@kernel.org>
commit 0a3c49178c3c3e6f29280567ccb549826dd3a3f1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/0a3c4917.failed

Macros already exist for unaligned code block symbols.  Use them.

	Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
	Acked-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/26d461bd509cc840af24c94586561c06d39812b2.1693889988.git.jpoimboe@kernel.org
(cherry picked from commit 0a3c49178c3c3e6f29280567ccb549826dd3a3f1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/lib/retpoline.S
diff --cc arch/x86/lib/retpoline.S
index 72a7235229fd,415521dbe15e..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -24,36 -49,181 +24,134 @@@ SYM_FUNC_END(__x86_indirect_thunk_\reg
   * only see one instance of "__x86_indirect_thunk_\reg" rather
   * than one per register with the correct names. So we do it
   * the simple and nasty way...
 - *
 - * Worse, you can only have a single EXPORT_SYMBOL per line,
 - * and CPP can't insert newlines, so we have to repeat everything
 - * at least twice.
   */
 -
 -#define __EXPORT_THUNK(sym)	_ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_thunk_array)
 -
 -#define GEN(reg) THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -.macro CALL_THUNK reg
 -	.align RETPOLINE_THUNK_SIZE
 -
 -SYM_INNER_LABEL(__x86_indirect_call_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_UNDEFINED
 -	ANNOTATE_NOENDBR
 -
 -	CALL_DEPTH_ACCOUNT
 -	POLINE \reg
 -	ANNOTATE_UNRET_SAFE
 -	ret
 -	int3
 -.endm
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_call_thunk_array)
 -
 -#define GEN(reg) CALL_THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_call_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_call_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -.macro JUMP_THUNK reg
 -	.align RETPOLINE_THUNK_SIZE
 -
 -SYM_INNER_LABEL(__x86_indirect_jump_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_UNDEFINED
 -	ANNOTATE_NOENDBR
 -	POLINE \reg
 -	ANNOTATE_UNRET_SAFE
 -	ret
 -	int3
 -.endm
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_jump_thunk_array)
 -
 -#define GEN(reg) JUMP_THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_jump_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_jump_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 +#define __EXPORT_THUNK(sym) _ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 +#define EXPORT_THUNK(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 +#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)
 +
 +GENERATE_THUNK(_ASM_AX)
 +GENERATE_THUNK(_ASM_BX)
 +GENERATE_THUNK(_ASM_CX)
 +GENERATE_THUNK(_ASM_DX)
 +GENERATE_THUNK(_ASM_SI)
 +GENERATE_THUNK(_ASM_DI)
 +GENERATE_THUNK(_ASM_BP)
 +#ifdef CONFIG_64BIT
 +GENERATE_THUNK(r8)
 +GENERATE_THUNK(r9)
 +GENERATE_THUNK(r10)
 +GENERATE_THUNK(r11)
 +GENERATE_THUNK(r12)
 +GENERATE_THUNK(r13)
 +GENERATE_THUNK(r14)
 +GENERATE_THUNK(r15)
  #endif
  
 +/*
 + * This function name is magical and is used by -mfunction-return=thunk-extern
 + * for the compiler to generate JMPs to it.
 + */
  #ifdef CONFIG_RETHUNK
  
++<<<<<<< HEAD
 +	.section .text.__x86.return_thunk
++=======
+ 	.section .text..__x86.return_thunk
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 
+ /*
+  * srso_alias_untrain_ret() and srso_alias_safe_ret() are placed at
+  * special addresses:
+  *
+  * - srso_alias_untrain_ret() is 2M aligned
+  * - srso_alias_safe_ret() is also in the same 2M page but bits 2, 8, 14
+  * and 20 in its virtual address are set (while those bits in the
+  * srso_alias_untrain_ret() function are cleared).
+  *
+  * This guarantees that those two addresses will alias in the branch
+  * target buffer of Zen3/4 generations, leading to any potential
+  * poisoned entries at that BTB slot to get evicted.
+  *
+  * As a result, srso_alias_safe_ret() becomes a safe return.
+  */
+ 	.pushsection .text..__x86.rethunk_untrain
+ SYM_CODE_START_NOALIGN(srso_alias_untrain_ret)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ASM_NOP2
+ 	lfence
+ 	jmp srso_alias_return_thunk
+ SYM_FUNC_END(srso_alias_untrain_ret)
+ 	.popsection
+ 
+ 	.pushsection .text..__x86.rethunk_safe
+ SYM_CODE_START_NOALIGN(srso_alias_safe_ret)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_alias_safe_ret)
+ 
+ SYM_CODE_START_NOALIGN(srso_alias_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_alias_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_alias_return_thunk)
+ 	.popsection
+ 
+ /*
+  * SRSO untraining sequence for Zen1/2, similar to retbleed_untrain_ret()
+  * above. On kernel entry, srso_untrain_ret() is executed which is a
+  *
+  * movabs $0xccccc30824648d48,%rax
+  *
+  * and when the return thunk executes the inner label srso_safe_ret()
+  * later, it is a stack manipulation and a RET which is mispredicted and
+  * thus a "safe" one to use.
+  */
+ 	.align 64
+ 	.skip 64 - (srso_safe_ret - srso_untrain_ret), 0xcc
+ SYM_CODE_START_LOCAL_NOALIGN(srso_untrain_ret)
+ 	ANNOTATE_NOENDBR
+ 	.byte 0x48, 0xb8
+ 
+ /*
+  * This forces the function return instruction to speculate into a trap
+  * (UD2 in srso_return_thunk() below).  This RET will then mispredict
+  * and execution will continue at the return site read from the top of
+  * the stack.
+  */
+ SYM_INNER_LABEL(srso_safe_ret, SYM_L_GLOBAL)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	ret
+ 	int3
+ 	int3
+ 	/* end of movabs */
+ 	lfence
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_safe_ret)
+ SYM_FUNC_END(srso_untrain_ret)
+ 
+ SYM_CODE_START(srso_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_return_thunk)
+ 
+ #define JMP_SRSO_UNTRAIN_RET "jmp srso_untrain_ret"
+ #define JMP_SRSO_ALIAS_UNTRAIN_RET "jmp srso_alias_untrain_ret"
+ #else /* !CONFIG_CPU_SRSO */
+ #define JMP_SRSO_UNTRAIN_RET "ud2"
+ #define JMP_SRSO_ALIAS_UNTRAIN_RET "ud2"
+ #endif /* CONFIG_CPU_SRSO */
+ 
+ #ifdef CONFIG_CPU_UNRET_ENTRY
++>>>>>>> 0a3c49178c3c (x86/rethunk: Use SYM_CODE_START[_LOCAL]_NOALIGN macros)
  
  /*
   * Some generic notes on the untraining sequences:
@@@ -84,11 -254,11 +182,16 @@@
   *    from re-poisioning the BTB prediction.
   */
  	.align 64
++<<<<<<< HEAD
 +	.skip 63, 0xcc
 +SYM_START(zen_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
++=======
+ 	.skip 64 - (retbleed_return_thunk - retbleed_untrain_ret), 0xcc
+ SYM_CODE_START_LOCAL_NOALIGN(retbleed_untrain_ret)
++>>>>>>> 0a3c49178c3c (x86/rethunk: Use SYM_CODE_START[_LOCAL]_NOALIGN macros)
  	ANNOTATE_NOENDBR
  	/*
 -	 * As executed from retbleed_untrain_ret, this is:
 +	 * As executed from zen_untrain_ret, this is:
  	 *
  	 *   TEST $0xcc, %bl
  	 *   LFENCE
* Unmerged path arch/x86/lib/retpoline.S
