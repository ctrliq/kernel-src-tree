x86/srso: Add IBPB

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Borislav Petkov (AMD) <bp@alien8.de>
commit 233d6f68b98d480a7c42ebe78c38f79d44741ca9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/233d6f68.failed

Add the option to mitigate using IBPB on a kernel entry. Pull in the
Retbleed alternative so that the IBPB call from there can be used. Also,
if Retbleed mitigation is done using IBPB, the same mitigation can and
must be used here.

	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
(cherry picked from commit 233d6f68b98d480a7c42ebe78c38f79d44741ca9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
diff --cc arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70,3faf044569a5..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -189,10 -289,42 +189,49 @@@
   * where we have a stack but before any RET instruction.
   */
  .macro UNTRAIN_RET
++<<<<<<< HEAD
 +#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY)
 +	ALTERNATIVE_2 "",						\
 +	              CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
 +		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB
++=======
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \
+ 	defined(CONFIG_CALL_DEPTH_TRACKING) || defined(CONFIG_CPU_SRSO)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		      __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 	ALTERNATIVE_2 "", "call srso_untrain_ret", X86_FEATURE_SRSO, \
+ 			  "call srso_untrain_ret_alias", X86_FEATURE_SRSO_ALIAS
+ #endif
+ .endm
+ 
+ .macro UNTRAIN_RET_FROM_CALL
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \
+ 	defined(CONFIG_CALL_DEPTH_TRACKING)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		      __stringify(RESET_CALL_DEPTH_FROM_CALL), X86_FEATURE_CALL_DEPTH
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 	ALTERNATIVE_2 "", "call srso_untrain_ret", X86_FEATURE_SRSO, \
+ 			  "call srso_untrain_ret_alias", X86_FEATURE_SRSO_ALIAS
+ #endif
+ .endm
+ 
+ 
+ .macro CALL_DEPTH_ACCOUNT
+ #ifdef CONFIG_CALL_DEPTH_TRACKING
+ 	ALTERNATIVE "",							\
+ 		    __stringify(ASM_INCREMENT_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
++>>>>>>> 233d6f68b98d (x86/srso: Add IBPB)
  #endif
  .endm
  
diff --cc arch/x86/kernel/cpu/bugs.c
index adea0002a1a8,f3cc432ed818..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -2309,6 -2191,141 +2309,144 @@@ static int __init l1tf_cmdline(char *st
  early_param("l1tf", l1tf_cmdline);
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Return Stack Overflow: " fmt
+ 
+ enum srso_mitigation {
+ 	SRSO_MITIGATION_NONE,
+ 	SRSO_MITIGATION_MICROCODE,
+ 	SRSO_MITIGATION_SAFE_RET,
+ 	SRSO_MITIGATION_IBPB,
+ };
+ 
+ enum srso_mitigation_cmd {
+ 	SRSO_CMD_OFF,
+ 	SRSO_CMD_MICROCODE,
+ 	SRSO_CMD_SAFE_RET,
+ 	SRSO_CMD_IBPB,
+ };
+ 
+ static const char * const srso_strings[] = {
+ 	[SRSO_MITIGATION_NONE]           = "Vulnerable",
+ 	[SRSO_MITIGATION_MICROCODE]      = "Mitigation: microcode",
+ 	[SRSO_MITIGATION_SAFE_RET]	 = "Mitigation: safe RET",
+ 	[SRSO_MITIGATION_IBPB]		 = "Mitigation: IBPB",
+ };
+ 
+ static enum srso_mitigation srso_mitigation __ro_after_init = SRSO_MITIGATION_NONE;
+ static enum srso_mitigation_cmd srso_cmd __ro_after_init = SRSO_CMD_SAFE_RET;
+ 
+ static int __init srso_parse_cmdline(char *str)
+ {
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	if (!strcmp(str, "off"))
+ 		srso_cmd = SRSO_CMD_OFF;
+ 	else if (!strcmp(str, "microcode"))
+ 		srso_cmd = SRSO_CMD_MICROCODE;
+ 	else if (!strcmp(str, "safe-ret"))
+ 		srso_cmd = SRSO_CMD_SAFE_RET;
+ 	else if (!strcmp(str, "ibpb"))
+ 		srso_cmd = SRSO_CMD_IBPB;
+ 	else
+ 		pr_err("Ignoring unknown SRSO option (%s).", str);
+ 
+ 	return 0;
+ }
+ early_param("spec_rstack_overflow", srso_parse_cmdline);
+ 
+ #define SRSO_NOTICE "WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options."
+ 
+ static void __init srso_select_mitigation(void)
+ {
+ 	bool has_microcode;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_SRSO) || cpu_mitigations_off())
+ 		goto pred_cmd;
+ 
+ 	/*
+ 	 * The first check is for the kernel running as a guest in order
+ 	 * for guests to verify whether IBPB is a viable mitigation.
+ 	 */
+ 	has_microcode = boot_cpu_has(X86_FEATURE_IBPB_BRTYPE) || cpu_has_ibpb_brtype_microcode();
+ 	if (!has_microcode) {
+ 		pr_warn("IBPB-extending microcode not applied!\n");
+ 		pr_warn(SRSO_NOTICE);
+ 	} else {
+ 		/*
+ 		 * Enable the synthetic (even if in a real CPUID leaf)
+ 		 * flags for guests.
+ 		 */
+ 		setup_force_cpu_cap(X86_FEATURE_IBPB_BRTYPE);
+ 		setup_force_cpu_cap(X86_FEATURE_SBPB);
+ 
+ 		/*
+ 		 * Zen1/2 with SMT off aren't vulnerable after the right
+ 		 * IBPB microcode has been applied.
+ 		 */
+ 		if ((boot_cpu_data.x86 < 0x19) &&
+ 		    (cpu_smt_control == CPU_SMT_DISABLED))
+ 			setup_force_cpu_cap(X86_FEATURE_SRSO_NO);
+ 	}
+ 
+ 	if (retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {
+ 		if (has_microcode) {
+ 			pr_err("Retbleed IBPB mitigation enabled, using same for SRSO\n");
+ 			srso_mitigation = SRSO_MITIGATION_IBPB;
+ 			goto pred_cmd;
+ 		}
+ 	}
+ 
+ 	switch (srso_cmd) {
+ 	case SRSO_CMD_OFF:
+ 		return;
+ 
+ 	case SRSO_CMD_MICROCODE:
+ 		if (has_microcode) {
+ 			srso_mitigation = SRSO_MITIGATION_MICROCODE;
+ 			pr_warn(SRSO_NOTICE);
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_SAFE_RET:
+ 		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
+ 			if (boot_cpu_data.x86 == 0x19)
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO_ALIAS);
+ 			else
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO);
+ 			srso_mitigation = SRSO_MITIGATION_SAFE_RET;
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
+ 			goto pred_cmd;
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_IBPB:
+ 		if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY)) {
+ 			if (has_microcode) {
+ 				setup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);
+ 				srso_mitigation = SRSO_MITIGATION_IBPB;
+ 			}
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_IBPB_ENTRY.\n");
+ 			goto pred_cmd;
+ 		}
+ 	default:
+ 		break;
+ 	}
+ 
+ 	pr_info("%s%s\n", srso_strings[srso_mitigation], (has_microcode ? "" : ", no microcode"));
+ 
+ pred_cmd:
+ 	if (boot_cpu_has(X86_FEATURE_SRSO_NO) ||
+ 	    srso_cmd == SRSO_CMD_OFF)
+ 		x86_pred_cmd = PRED_CMD_SBPB;
+ }
+ 
+ #undef pr_fmt
++>>>>>>> 233d6f68b98d (x86/srso: Add IBPB)
  #define pr_fmt(fmt) fmt
  
  #ifdef CONFIG_SYSFS
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
