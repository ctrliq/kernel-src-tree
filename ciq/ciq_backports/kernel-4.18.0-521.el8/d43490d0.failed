x86/cpu: Clean up SRSO return thunk mess

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit d43490d0ab824023e11d0b57d0aeec17a6e0ca13
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/d43490d0.failed

Use the existing configurable return thunk. There is absolute no
justification for having created this __x86_return_thunk alternative.

To clarify, the whole thing looks like:

Zen3/4 does:

  srso_alias_untrain_ret:
	  nop2
	  lfence
	  jmp srso_alias_return_thunk
	  int3

  srso_alias_safe_ret: // aliasses srso_alias_untrain_ret just so
	  add $8, %rsp
	  ret
	  int3

  srso_alias_return_thunk:
	  call srso_alias_safe_ret
	  ud2

While Zen1/2 does:

  srso_untrain_ret:
	  movabs $foo, %rax
	  lfence
	  call srso_safe_ret           (jmp srso_return_thunk ?)
	  int3

  srso_safe_ret: // embedded in movabs instruction
	  add $8,%rsp
          ret
          int3

  srso_return_thunk:
	  call srso_safe_ret
	  ud2

While retbleed does:

  zen_untrain_ret:
	  test $0xcc, %bl
	  lfence
	  jmp zen_return_thunk
          int3

  zen_return_thunk: // embedded in the test instruction
	  ret
          int3

Where Zen1/2 flush the BTB entry using the instruction decoder trick
(test,movabs) Zen3/4 use BTB aliasing. SRSO adds a return sequence
(srso_safe_ret()) which forces the function return instruction to
speculate into a trap (UD2).  This RET will then mispredict and
execution will continue at the return site read from the top of the
stack.

Pick one of three options at boot (evey function can only ever return
once).

  [ bp: Fixup commit message uarch details and add them in a comment in
    the code too. Add a comment about the srso_select_mitigation()
    dependency on retbleed_select_mitigation(). Add moar ifdeffery for
    32-bit builds. Add a dummy srso_untrain_ret_alias() definition for
    32-bit alternatives needing the symbol. ]

Fixes: fb3bd914b3ec ("x86/srso: Add a Speculative RAS Overflow mitigation")
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/20230814121148.842775684@infradead.org
(cherry picked from commit d43490d0ab824023e11d0b57d0aeec17a6e0ca13)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/vmlinux.lds.S
#	arch/x86/lib/retpoline.S
#	tools/objtool/arch/x86/decode.c
diff --cc arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70,5ed78ad9f274..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -201,14 -333,75 +201,29 @@@
  #define ANNOTATE_RETPOLINE_SAFE					\
  	"999:\n\t"						\
  	".pushsection .discard.retpoline_safe\n\t"		\
 -	".long 999b - .\n\t"					\
 +	_ASM_PTR " 999b\n\t"					\
  	".popsection\n\t"
  
 -typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
 -extern retpoline_thunk_t __x86_indirect_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];
 -
 -#ifdef CONFIG_RETHUNK
  extern void __x86_return_thunk(void);
++<<<<<<< HEAD
 +extern void zen_untrain_ret(void);
++=======
+ #else
+ static inline void __x86_return_thunk(void) {}
+ #endif
+ 
+ extern void zen_return_thunk(void);
+ extern void srso_return_thunk(void);
+ extern void srso_alias_return_thunk(void);
+ 
+ extern void zen_untrain_ret(void);
+ extern void srso_untrain_ret(void);
+ extern void srso_untrain_ret_alias(void);
+ 
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  extern void entry_ibpb(void);
  
 -extern void (*x86_return_thunk)(void);
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -extern void __x86_return_skl(void);
 -
 -static inline void x86_set_skl_return_thunk(void)
 -{
 -	x86_return_thunk = &__x86_return_skl;
 -}
 -
 -#define CALL_DEPTH_ACCOUNT					\
 -	ALTERNATIVE("",						\
 -		    __stringify(INCREMENT_CALL_DEPTH),		\
 -		    X86_FEATURE_CALL_DEPTH)
 -
 -#ifdef CONFIG_CALL_THUNKS_DEBUG
 -DECLARE_PER_CPU(u64, __x86_call_count);
 -DECLARE_PER_CPU(u64, __x86_ret_count);
 -DECLARE_PER_CPU(u64, __x86_stuffs_count);
 -DECLARE_PER_CPU(u64, __x86_ctxsw_count);
 -#endif
 -#else
 -static inline void x86_set_skl_return_thunk(void) {}
 -
 -#define CALL_DEPTH_ACCOUNT ""
 -
 -#endif
 -
  #ifdef CONFIG_RETPOLINE
 -
 -#define GEN(reg) \
 -	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_call_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_jump_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
  #ifdef CONFIG_X86_64
  
  /*
diff --cc arch/x86/kernel/cpu/bugs.c
index adea0002a1a8,56cf250c13f6..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -161,40 -166,14 +161,50 @@@ void __init check_bugs(void
  	l1tf_select_mitigation();
  	md_clear_select_mitigation();
  	srbds_select_mitigation();
++<<<<<<< HEAD
++=======
+ 	l1d_flush_select_mitigation();
+ 
+ 	/*
+ 	 * srso_select_mitigation() depends and must run after
+ 	 * retbleed_select_mitigation().
+ 	 */
+ 	srso_select_mitigation();
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  	gds_select_mitigation();
 +
 +	arch_smt_update();
 +
 +#ifdef CONFIG_X86_32
 +	/*
 +	 * Check whether we are able to run this kernel safely on SMP.
 +	 *
 +	 * - i386 is no longer supported.
 +	 * - In order to run on anything without a TSC, we need to be
 +	 *   compiled for a i486.
 +	 */
 +	if (boot_cpu_data.x86 < 4)
 +		panic("Kernel requires i486+ for 'invlpg' and other features");
 +
 +	init_utsname()->machine[1] =
 +		'0' + (boot_cpu_data.x86 > 6 ? 6 : boot_cpu_data.x86);
 +	alternative_instructions();
 +
 +	fpu__init_check_bugs();
 +#else /* CONFIG_X86_64 */
 +	alternative_instructions();
 +
 +	/*
 +	 * Make sure the first 2MB area is not mapped by huge pages
 +	 * There are typically fixed size MTRRs in there and overlapping
 +	 * MTRRs into large pages causes slow downs.
 +	 *
 +	 * Right now we don't do that with gbpages because there seems
 +	 * very little benefit for that case.
 +	 */
 +	if (!direct_gbpages)
 +		set_memory_4k((unsigned long)__va(0), 1);
 +#endif
  }
  
  /*
@@@ -2309,6 -2348,170 +2322,173 @@@ static int __init l1tf_cmdline(char *st
  early_param("l1tf", l1tf_cmdline);
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Return Stack Overflow: " fmt
+ 
+ enum srso_mitigation {
+ 	SRSO_MITIGATION_NONE,
+ 	SRSO_MITIGATION_MICROCODE,
+ 	SRSO_MITIGATION_SAFE_RET,
+ 	SRSO_MITIGATION_IBPB,
+ 	SRSO_MITIGATION_IBPB_ON_VMEXIT,
+ };
+ 
+ enum srso_mitigation_cmd {
+ 	SRSO_CMD_OFF,
+ 	SRSO_CMD_MICROCODE,
+ 	SRSO_CMD_SAFE_RET,
+ 	SRSO_CMD_IBPB,
+ 	SRSO_CMD_IBPB_ON_VMEXIT,
+ };
+ 
+ static const char * const srso_strings[] = {
+ 	[SRSO_MITIGATION_NONE]           = "Vulnerable",
+ 	[SRSO_MITIGATION_MICROCODE]      = "Mitigation: microcode",
+ 	[SRSO_MITIGATION_SAFE_RET]	 = "Mitigation: safe RET",
+ 	[SRSO_MITIGATION_IBPB]		 = "Mitigation: IBPB",
+ 	[SRSO_MITIGATION_IBPB_ON_VMEXIT] = "Mitigation: IBPB on VMEXIT only"
+ };
+ 
+ static enum srso_mitigation srso_mitigation __ro_after_init = SRSO_MITIGATION_NONE;
+ static enum srso_mitigation_cmd srso_cmd __ro_after_init = SRSO_CMD_SAFE_RET;
+ 
+ static int __init srso_parse_cmdline(char *str)
+ {
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	if (!strcmp(str, "off"))
+ 		srso_cmd = SRSO_CMD_OFF;
+ 	else if (!strcmp(str, "microcode"))
+ 		srso_cmd = SRSO_CMD_MICROCODE;
+ 	else if (!strcmp(str, "safe-ret"))
+ 		srso_cmd = SRSO_CMD_SAFE_RET;
+ 	else if (!strcmp(str, "ibpb"))
+ 		srso_cmd = SRSO_CMD_IBPB;
+ 	else if (!strcmp(str, "ibpb-vmexit"))
+ 		srso_cmd = SRSO_CMD_IBPB_ON_VMEXIT;
+ 	else
+ 		pr_err("Ignoring unknown SRSO option (%s).", str);
+ 
+ 	return 0;
+ }
+ early_param("spec_rstack_overflow", srso_parse_cmdline);
+ 
+ #define SRSO_NOTICE "WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options."
+ 
+ static void __init srso_select_mitigation(void)
+ {
+ 	bool has_microcode;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_SRSO) || cpu_mitigations_off())
+ 		goto pred_cmd;
+ 
+ 	/*
+ 	 * The first check is for the kernel running as a guest in order
+ 	 * for guests to verify whether IBPB is a viable mitigation.
+ 	 */
+ 	has_microcode = boot_cpu_has(X86_FEATURE_IBPB_BRTYPE) || cpu_has_ibpb_brtype_microcode();
+ 	if (!has_microcode) {
+ 		pr_warn("IBPB-extending microcode not applied!\n");
+ 		pr_warn(SRSO_NOTICE);
+ 	} else {
+ 		/*
+ 		 * Enable the synthetic (even if in a real CPUID leaf)
+ 		 * flags for guests.
+ 		 */
+ 		setup_force_cpu_cap(X86_FEATURE_IBPB_BRTYPE);
+ 
+ 		/*
+ 		 * Zen1/2 with SMT off aren't vulnerable after the right
+ 		 * IBPB microcode has been applied.
+ 		 */
+ 		if ((boot_cpu_data.x86 < 0x19) &&
+ 		    (!cpu_smt_possible() || (cpu_smt_control == CPU_SMT_DISABLED))) {
+ 			setup_force_cpu_cap(X86_FEATURE_SRSO_NO);
+ 			return;
+ 		}
+ 	}
+ 
+ 	if (retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {
+ 		if (has_microcode) {
+ 			pr_err("Retbleed IBPB mitigation enabled, using same for SRSO\n");
+ 			srso_mitigation = SRSO_MITIGATION_IBPB;
+ 			goto pred_cmd;
+ 		}
+ 	}
+ 
+ 	switch (srso_cmd) {
+ 	case SRSO_CMD_OFF:
+ 		return;
+ 
+ 	case SRSO_CMD_MICROCODE:
+ 		if (has_microcode) {
+ 			srso_mitigation = SRSO_MITIGATION_MICROCODE;
+ 			pr_warn(SRSO_NOTICE);
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_SAFE_RET:
+ 		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
+ 			/*
+ 			 * Enable the return thunk for generated code
+ 			 * like ftrace, static_call, etc.
+ 			 */
+ 			setup_force_cpu_cap(X86_FEATURE_RETHUNK);
+ 
+ 			if (boot_cpu_data.x86 == 0x19) {
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO_ALIAS);
+ 				x86_return_thunk = srso_alias_return_thunk;
+ 			} else {
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO);
+ 				x86_return_thunk = srso_return_thunk;
+ 			}
+ 			srso_mitigation = SRSO_MITIGATION_SAFE_RET;
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
+ 			goto pred_cmd;
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_IBPB:
+ 		if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY)) {
+ 			if (has_microcode) {
+ 				setup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);
+ 				srso_mitigation = SRSO_MITIGATION_IBPB;
+ 			}
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_IBPB_ENTRY.\n");
+ 			goto pred_cmd;
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_IBPB_ON_VMEXIT:
+ 		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
+ 			if (!boot_cpu_has(X86_FEATURE_ENTRY_IBPB) && has_microcode) {
+ 				setup_force_cpu_cap(X86_FEATURE_IBPB_ON_VMEXIT);
+ 				srso_mitigation = SRSO_MITIGATION_IBPB_ON_VMEXIT;
+ 			}
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
+ 			goto pred_cmd;
+                 }
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ 
+ 	pr_info("%s%s\n", srso_strings[srso_mitigation], (has_microcode ? "" : ", no microcode"));
+ 
+ pred_cmd:
+ 	if ((boot_cpu_has(X86_FEATURE_SRSO_NO) || srso_cmd == SRSO_CMD_OFF) &&
+ 	     boot_cpu_has(X86_FEATURE_SBPB))
+ 		x86_pred_cmd = PRED_CMD_SBPB;
+ }
+ 
+ #undef pr_fmt
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  #define pr_fmt(fmt) fmt
  
  #ifdef CONFIG_SYSFS
diff --cc arch/x86/kernel/vmlinux.lds.S
index 6df2973aa8ce,d3b02d67321b..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -441,12 -520,25 +441,18 @@@ INIT_PER_CPU(irq_stack_backing_store)
             "fixed_percpu_data is not at start of per-cpu area");
  #endif
  
++<<<<<<< HEAD
 +#endif /* CONFIG_X86_32 */
 +
 +#ifdef CONFIG_KEXEC_CORE
 +#include <asm/kexec.h>
 +
 +. = ASSERT(kexec_control_code_size <= KEXEC_CONTROL_CODE_MAX_SIZE,
 +           "kexec control code size is too big");
++=======
+ #ifdef CONFIG_RETHUNK
+ . = ASSERT((zen_return_thunk & 0x3f) == 0, "zen_return_thunk not cacheline-aligned");
+ . = ASSERT((srso_safe_ret & 0x3f) == 0, "srso_safe_ret not cacheline-aligned");
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  #endif
  
 -#ifdef CONFIG_CPU_SRSO
 -/*
 - * GNU ld cannot do XOR until 2.41.
 - * https://sourceware.org/git/?p=binutils-gdb.git;a=commit;h=f6f78318fca803c4907fb8d7f6ded8295f1947b1
 - *
 - * LLVM lld cannot do XOR until lld-17.
 - * https://github.com/llvm/llvm-project/commit/fae96104d4378166cbe5c875ef8ed808a356f3fb
 - *
 - * Instead do: (A | B) - (A & B) in order to compute the XOR
 - * of the two function addresses:
 - */
 -. = ASSERT(((ABSOLUTE(srso_untrain_ret_alias) | srso_safe_ret_alias) -
 -		(ABSOLUTE(srso_untrain_ret_alias) & srso_safe_ret_alias)) == ((1 << 2) | (1 << 8) | (1 << 14) | (1 << 20)),
 -		"SRSO function pair won't alias");
 -#endif
 -
 -#endif /* CONFIG_X86_64 */
diff --cc arch/x86/lib/retpoline.S
index ce2b806fe45d,7df8582fb64e..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -53,7 -132,59 +53,63 @@@ GENERATE_THUNK(r15
   */
  #ifdef CONFIG_RETHUNK
  
++<<<<<<< HEAD
 +	.section .text.__x86.return_thunk
++=======
+ /*
+  * srso_untrain_ret_alias() and srso_safe_ret_alias() are placed at
+  * special addresses:
+  *
+  * - srso_untrain_ret_alias() is 2M aligned
+  * - srso_safe_ret_alias() is also in the same 2M page but bits 2, 8, 14
+  * and 20 in its virtual address are set (while those bits in the
+  * srso_untrain_ret_alias() function are cleared).
+  *
+  * This guarantees that those two addresses will alias in the branch
+  * target buffer of Zen3/4 generations, leading to any potential
+  * poisoned entries at that BTB slot to get evicted.
+  *
+  * As a result, srso_safe_ret_alias() becomes a safe return.
+  */
+ #ifdef CONFIG_CPU_SRSO
+ 	.section .text..__x86.rethunk_untrain
+ 
+ SYM_START(srso_untrain_ret_alias, SYM_L_GLOBAL, SYM_A_NONE)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ASM_NOP2
+ 	lfence
+ 	jmp srso_alias_return_thunk
+ SYM_FUNC_END(srso_untrain_ret_alias)
+ __EXPORT_THUNK(srso_untrain_ret_alias)
+ 
+ 	.section .text..__x86.rethunk_safe
+ #else
+ /* dummy definition for alternatives */
+ SYM_START(srso_untrain_ret_alias, SYM_L_GLOBAL, SYM_A_NONE)
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_untrain_ret_alias)
+ #endif
+ 
+ SYM_START(srso_safe_ret_alias, SYM_L_GLOBAL, SYM_A_NONE)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_safe_ret_alias)
+ 
+ 	.section .text..__x86.return_thunk
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
+ 
+ SYM_CODE_START(srso_alias_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_safe_ret_alias
+ 	ud2
+ SYM_CODE_END(srso_alias_return_thunk)
  
  /*
   * Safety details here pertain to the AMD Zen{1,2} microarchitecture:
@@@ -65,7 -196,7 +121,11 @@@
   *    from re-poisioning the BTB prediction.
   */
  	.align 64
++<<<<<<< HEAD
 +	.skip 63, 0xcc
++=======
+ 	.skip 64 - (zen_return_thunk - zen_untrain_ret), 0xcc
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  SYM_START(zen_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
  	ANNOTATE_NOENDBR
  	/*
@@@ -94,13 -225,13 +154,20 @@@
  	 * With SMT enabled and STIBP active, a sibling thread cannot poison
  	 * RET's prediction to a type of its choice, but can evict the
  	 * prediction due to competitive sharing. If the prediction is
- 	 * evicted, __x86_return_thunk will suffer Straight Line Speculation
+ 	 * evicted, zen_return_thunk will suffer Straight Line Speculation
  	 * which will be contained safely by the INT3.
  	 */
++<<<<<<< HEAD
 +SYM_INNER_LABEL(__x86_return_thunk, SYM_L_GLOBAL)
 +	ret
 +	int3
 +SYM_CODE_END(__x86_return_thunk)
++=======
+ SYM_INNER_LABEL(zen_return_thunk, SYM_L_GLOBAL)
+ 	ret
+ 	int3
+ SYM_CODE_END(zen_return_thunk)
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  
  	/*
  	 * Ensure the TEST decoding / BTB invalidation is complete.
@@@ -111,11 -242,60 +178,67 @@@
  	 * Jump back and execute the RET in the middle of the TEST instruction.
  	 * INT3 is for SLS protection.
  	 */
++<<<<<<< HEAD
 +	jmp __x86_return_thunk
++=======
+ 	jmp zen_return_thunk
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  	int3
  SYM_FUNC_END(zen_untrain_ret)
  __EXPORT_THUNK(zen_untrain_ret)
  
++<<<<<<< HEAD
++=======
+ /*
+  * SRSO untraining sequence for Zen1/2, similar to zen_untrain_ret()
+  * above. On kernel entry, srso_untrain_ret() is executed which is a
+  *
+  * movabs $0xccccc30824648d48,%rax
+  *
+  * and when the return thunk executes the inner label srso_safe_ret()
+  * later, it is a stack manipulation and a RET which is mispredicted and
+  * thus a "safe" one to use.
+  */
+ 	.align 64
+ 	.skip 64 - (srso_safe_ret - srso_untrain_ret), 0xcc
+ SYM_START(srso_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	ANNOTATE_NOENDBR
+ 	.byte 0x48, 0xb8
+ 
+ /*
+  * This forces the function return instruction to speculate into a trap
+  * (UD2 in srso_return_thunk() below).  This RET will then mispredict
+  * and execution will continue at the return site read from the top of
+  * the stack.
+  */
+ SYM_INNER_LABEL(srso_safe_ret, SYM_L_GLOBAL)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	ret
+ 	int3
+ 	int3
+ 	/* end of movabs */
+ 	lfence
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_safe_ret)
+ SYM_FUNC_END(srso_untrain_ret)
+ __EXPORT_THUNK(srso_untrain_ret)
+ 
+ SYM_CODE_START(srso_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_return_thunk)
+ 
+ SYM_CODE_START(__x86_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
  EXPORT_SYMBOL(__x86_return_thunk)
  
  #endif /* CONFIG_RETHUNK */
diff --cc tools/objtool/arch/x86/decode.c
index 603f71ad0722,c55f3bb59b31..000000000000
--- a/tools/objtool/arch/x86/decode.c
+++ b/tools/objtool/arch/x86/decode.c
@@@ -600,3 -826,9 +600,12 @@@ bool arch_is_rethunk(struct symbol *sym
  {
  	return !strcmp(sym->name, "__x86_return_thunk");
  }
++<<<<<<< HEAD
++=======
+ 
+ bool arch_is_embedded_insn(struct symbol *sym)
+ {
+ 	return !strcmp(sym->name, "zen_return_thunk") ||
+ 	       !strcmp(sym->name, "srso_safe_ret");
+ }
++>>>>>>> d43490d0ab82 (x86/cpu: Clean up SRSO return thunk mess)
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/vmlinux.lds.S
* Unmerged path arch/x86/lib/retpoline.S
* Unmerged path tools/objtool/arch/x86/decode.c
