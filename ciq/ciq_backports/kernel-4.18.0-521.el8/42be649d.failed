x86/cpu: Rename srso_(.*)_alias to srso_alias_\1

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 42be649dd1f2eee6b1fb185f1a231b9494cf095f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/42be649d.failed

For a more consistent namespace.

  [ bp: Fixup names in the doc too. ]

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/20230814121148.976236447@infradead.org
(cherry picked from commit 42be649dd1f2eee6b1fb185f1a231b9494cf095f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/admin-guide/hw-vuln/srso.rst
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/vmlinux.lds.S
#	arch/x86/lib/retpoline.S
diff --cc arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70,f7c337515d1e..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -189,10 -289,42 +189,49 @@@
   * where we have a stack but before any RET instruction.
   */
  .macro UNTRAIN_RET
++<<<<<<< HEAD
 +#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY)
 +	ALTERNATIVE_2 "",						\
 +	              CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
 +		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB
++=======
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \
+ 	defined(CONFIG_CALL_DEPTH_TRACKING) || defined(CONFIG_CPU_SRSO)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		      __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 	ALTERNATIVE_2 "", "call srso_untrain_ret", X86_FEATURE_SRSO, \
+ 			  "call srso_alias_untrain_ret", X86_FEATURE_SRSO_ALIAS
+ #endif
+ .endm
+ 
+ .macro UNTRAIN_RET_FROM_CALL
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \
+ 	defined(CONFIG_CALL_DEPTH_TRACKING)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		      __stringify(RESET_CALL_DEPTH_FROM_CALL), X86_FEATURE_CALL_DEPTH
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 	ALTERNATIVE_2 "", "call srso_untrain_ret", X86_FEATURE_SRSO, \
+ 			  "call srso_alias_untrain_ret", X86_FEATURE_SRSO_ALIAS
+ #endif
+ .endm
+ 
+ 
+ .macro CALL_DEPTH_ACCOUNT
+ #ifdef CONFIG_CALL_DEPTH_TRACKING
+ 	ALTERNATIVE "",							\
+ 		    __stringify(ASM_INCREMENT_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
++>>>>>>> 42be649dd1f2 (x86/cpu: Rename srso_(.*)_alias to srso_alias_\1)
  #endif
  .endm
  
@@@ -201,14 -333,75 +240,29 @@@
  #define ANNOTATE_RETPOLINE_SAFE					\
  	"999:\n\t"						\
  	".pushsection .discard.retpoline_safe\n\t"		\
 -	".long 999b - .\n\t"					\
 +	_ASM_PTR " 999b\n\t"					\
  	".popsection\n\t"
  
 -typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
 -extern retpoline_thunk_t __x86_indirect_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];
 -
 -#ifdef CONFIG_RETHUNK
  extern void __x86_return_thunk(void);
++<<<<<<< HEAD
 +extern void zen_untrain_ret(void);
++=======
+ #else
+ static inline void __x86_return_thunk(void) {}
+ #endif
+ 
+ extern void retbleed_return_thunk(void);
+ extern void srso_return_thunk(void);
+ extern void srso_alias_return_thunk(void);
+ 
+ extern void retbleed_untrain_ret(void);
+ extern void srso_untrain_ret(void);
+ extern void srso_alias_untrain_ret(void);
+ 
++>>>>>>> 42be649dd1f2 (x86/cpu: Rename srso_(.*)_alias to srso_alias_\1)
  extern void entry_ibpb(void);
  
 -extern void (*x86_return_thunk)(void);
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -extern void __x86_return_skl(void);
 -
 -static inline void x86_set_skl_return_thunk(void)
 -{
 -	x86_return_thunk = &__x86_return_skl;
 -}
 -
 -#define CALL_DEPTH_ACCOUNT					\
 -	ALTERNATIVE("",						\
 -		    __stringify(INCREMENT_CALL_DEPTH),		\
 -		    X86_FEATURE_CALL_DEPTH)
 -
 -#ifdef CONFIG_CALL_THUNKS_DEBUG
 -DECLARE_PER_CPU(u64, __x86_call_count);
 -DECLARE_PER_CPU(u64, __x86_ret_count);
 -DECLARE_PER_CPU(u64, __x86_stuffs_count);
 -DECLARE_PER_CPU(u64, __x86_ctxsw_count);
 -#endif
 -#else
 -static inline void x86_set_skl_return_thunk(void) {}
 -
 -#define CALL_DEPTH_ACCOUNT ""
 -
 -#endif
 -
  #ifdef CONFIG_RETPOLINE
 -
 -#define GEN(reg) \
 -	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_call_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_jump_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
  #ifdef CONFIG_X86_64
  
  /*
diff --cc arch/x86/kernel/vmlinux.lds.S
index 6df2973aa8ce,83d41c2601d7..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -122,36 -129,42 +122,48 @@@ SECTION
  		HEAD_TEXT
  		TEXT_TEXT
  		SCHED_TEXT
 +		CPUIDLE_TEXT
  		LOCK_TEXT
  		KPROBES_TEXT
 -		SOFTIRQENTRY_TEXT
 -#ifdef CONFIG_RETPOLINE
 -		*(.text..__x86.indirect_thunk)
 -		*(.text..__x86.return_thunk)
 -#endif
 -		STATIC_CALL_TEXT
 -
  		ALIGN_ENTRY_TEXT_BEGIN
 -#ifdef CONFIG_CPU_SRSO
 -		*(.text..__x86.rethunk_untrain)
 -#endif
 -
  		ENTRY_TEXT
++<<<<<<< HEAD
 +		IRQENTRY_TEXT
++=======
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 		/*
+ 		 * See the comment above srso_alias_untrain_ret()'s
+ 		 * definition.
+ 		 */
+ 		. = srso_alias_untrain_ret | (1 << 2) | (1 << 8) | (1 << 14) | (1 << 20);
+ 		*(.text..__x86.rethunk_safe)
+ #endif
++>>>>>>> 42be649dd1f2 (x86/cpu: Rename srso_(.*)_alias to srso_alias_\1)
  		ALIGN_ENTRY_TEXT_END
 +		SOFTIRQENTRY_TEXT
 +		*(.fixup)
  		*(.gnu.warning)
  
 -	} :text =0xcccc
 +#ifdef CONFIG_RETPOLINE
 +		__indirect_thunk_start = .;
 +		*(.text.__x86.*)
 +		__indirect_thunk_end = .;
 +#endif
  
 -	/* End of text section, which should occupy whole number of pages */
 -	_etext = .;
 -	. = ALIGN(PAGE_SIZE);
 +		/* End of text section */
 +		_etext = .;
 +	} :text = 0x9090
 +
 +	NOTES :text :note
  
 -	X86_ALIGN_RODATA_BEGIN
 +	EXCEPTION_TABLE(16) :text = 0x9090
 +
 +	/* .text should occupy whole number of pages */
 +	. = ALIGN(PAGE_SIZE);
 +	X64_ALIGN_RODATA_BEGIN
  	RO_DATA(PAGE_SIZE)
 -	X86_ALIGN_RODATA_END
 +	X64_ALIGN_RODATA_END
  
  	/* Data */
  	.data : AT(ADDR(.data) - LOAD_OFFSET) {
@@@ -441,12 -520,25 +453,32 @@@ INIT_PER_CPU(irq_stack_backing_store)
             "fixed_percpu_data is not at start of per-cpu area");
  #endif
  
 -#ifdef CONFIG_RETHUNK
 -. = ASSERT((retbleed_return_thunk & 0x3f) == 0, "retbleed_return_thunk not cacheline-aligned");
 -. = ASSERT((srso_safe_ret & 0x3f) == 0, "srso_safe_ret not cacheline-aligned");
 +#endif /* CONFIG_X86_32 */
 +
 +#ifdef CONFIG_KEXEC_CORE
 +#include <asm/kexec.h>
 +
 +. = ASSERT(kexec_control_code_size <= KEXEC_CONTROL_CODE_MAX_SIZE,
 +           "kexec control code size is too big");
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_CPU_SRSO
+ /*
+  * GNU ld cannot do XOR until 2.41.
+  * https://sourceware.org/git/?p=binutils-gdb.git;a=commit;h=f6f78318fca803c4907fb8d7f6ded8295f1947b1
+  *
+  * LLVM lld cannot do XOR until lld-17.
+  * https://github.com/llvm/llvm-project/commit/fae96104d4378166cbe5c875ef8ed808a356f3fb
+  *
+  * Instead do: (A | B) - (A & B) in order to compute the XOR
+  * of the two function addresses:
+  */
+ . = ASSERT(((ABSOLUTE(srso_alias_untrain_ret) | srso_alias_safe_ret) -
+ 		(ABSOLUTE(srso_alias_untrain_ret) & srso_alias_safe_ret)) == ((1 << 2) | (1 << 8) | (1 << 14) | (1 << 20)),
+ 		"SRSO function pair won't alias");
+ #endif
+ 
+ #endif /* CONFIG_X86_64 */
++>>>>>>> 42be649dd1f2 (x86/cpu: Rename srso_(.*)_alias to srso_alias_\1)
diff --cc arch/x86/lib/retpoline.S
index ce2b806fe45d,d37e5ab0d5ee..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -53,7 -132,59 +53,63 @@@ GENERATE_THUNK(r15
   */
  #ifdef CONFIG_RETHUNK
  
++<<<<<<< HEAD
 +	.section .text.__x86.return_thunk
++=======
+ /*
+  * srso_alias_untrain_ret() and srso_alias_safe_ret() are placed at
+  * special addresses:
+  *
+  * - srso_alias_untrain_ret() is 2M aligned
+  * - srso_alias_safe_ret() is also in the same 2M page but bits 2, 8, 14
+  * and 20 in its virtual address are set (while those bits in the
+  * srso_alias_untrain_ret() function are cleared).
+  *
+  * This guarantees that those two addresses will alias in the branch
+  * target buffer of Zen3/4 generations, leading to any potential
+  * poisoned entries at that BTB slot to get evicted.
+  *
+  * As a result, srso_alias_safe_ret() becomes a safe return.
+  */
+ #ifdef CONFIG_CPU_SRSO
+ 	.section .text..__x86.rethunk_untrain
+ 
+ SYM_START(srso_alias_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ASM_NOP2
+ 	lfence
+ 	jmp srso_alias_return_thunk
+ SYM_FUNC_END(srso_alias_untrain_ret)
+ __EXPORT_THUNK(srso_alias_untrain_ret)
+ 
+ 	.section .text..__x86.rethunk_safe
+ #else
+ /* dummy definition for alternatives */
+ SYM_START(srso_alias_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_alias_untrain_ret)
+ #endif
+ 
+ SYM_START(srso_alias_safe_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_alias_safe_ret)
+ 
+ 	.section .text..__x86.return_thunk
+ 
+ SYM_CODE_START(srso_alias_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_alias_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_alias_return_thunk)
++>>>>>>> 42be649dd1f2 (x86/cpu: Rename srso_(.*)_alias to srso_alias_\1)
  
  /*
   * Safety details here pertain to the AMD Zen{1,2} microarchitecture:
* Unmerged path Documentation/admin-guide/hw-vuln/srso.rst
* Unmerged path Documentation/admin-guide/hw-vuln/srso.rst
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/vmlinux.lds.S
* Unmerged path arch/x86/lib/retpoline.S
