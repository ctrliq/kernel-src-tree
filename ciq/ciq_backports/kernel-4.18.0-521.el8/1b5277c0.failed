x86/srso: Add SRSO_NO support

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Borislav Petkov (AMD) <bp@alien8.de>
commit 1b5277c0ea0b247393a9c426769fde18cff5e2f6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/1b5277c0.failed

Add support for the CPUID flag which denotes that the CPU is not
affected by SRSO.

	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
(cherry picked from commit 1b5277c0ea0b247393a9c426769fde18cff5e2f6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/kernel/cpu/amd.c
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/cpu/common.c
#	arch/x86/kvm/cpuid.c
diff --cc arch/x86/include/asm/cpufeatures.h
index b52ff6ff9447,93070aabbb2f..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -426,11 -445,9 +426,17 @@@
  #define X86_FEATURE_AUTOIBRS		(20*32+ 8) /* "" Automatic IBRS */
  #define X86_FEATURE_NO_SMM_CTL_MSR	(20*32+ 9) /* "" SMM_CTL MSR is not present */
  
++<<<<<<< HEAD
 +/* Word 21: reserved for future extension */
 +
 +/* RHEL specific auxillary flags, word 22 */
 +#define X86_FEATURE_IBRS_EXIT_SET	(22*32+ 0) /* ""  Set IBRS on kernel exit */
 +#define X86_FEATURE_IBRS_EXIT_SKIP	(22*32+ 1) /* ""  Skip SPEC_CTRL MSR write on exit */
++=======
+ #define X86_FEATURE_SBPB		(20*32+27) /* "" Selective Branch Prediction Barrier */
+ #define X86_FEATURE_IBPB_BRTYPE		(20*32+28) /* "" MSR_PRED_CMD[IBPB] flushes all branch type predictions */
+ #define X86_FEATURE_SRSO_NO		(20*32+29) /* "" CPU is not affected by SRSO */
++>>>>>>> 1b5277c0ea0b (x86/srso: Add SRSO_NO support)
  
  /*
   * BUG word(s)
diff --cc arch/x86/kernel/cpu/amd.c
index 92e42262f300,834f310b2f1a..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -1318,14 -1236,16 +1318,25 @@@ u32 amd_get_highest_perf(void
  }
  EXPORT_SYMBOL_GPL(amd_get_highest_perf);
  
 -bool cpu_has_ibpb_brtype_microcode(void)
 +static void zenbleed_check_cpu(void *unused)
  {
 -	u8 fam = boot_cpu_data.x86;
 +	struct cpuinfo_x86 *c = &cpu_data(smp_processor_id());
  
++<<<<<<< HEAD
 +	zenbleed_check(c);
 +}
 +
 +void amd_check_microcode(void)
 +{
 +	on_each_cpu(zenbleed_check_cpu, NULL, 1);
++=======
+ 	/* Zen1/2 IBPB flushes branch type predictions too. */
+ 	if (fam == 0x17)
+ 		return boot_cpu_has(X86_FEATURE_AMD_IBPB);
+ 	/* Poke the MSR bit on Zen3/4 to check its presence. */
+ 	else if (fam == 0x19)
+ 		return !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB);
+ 	else
+ 		return false;
++>>>>>>> 1b5277c0ea0b (x86/srso: Add SRSO_NO support)
  }
diff --cc arch/x86/kernel/cpu/bugs.c
index adea0002a1a8,439ecad62317..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -2309,6 -2191,118 +2312,121 @@@ static int __init l1tf_cmdline(char *st
  early_param("l1tf", l1tf_cmdline);
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Return Stack Overflow: " fmt
+ 
+ enum srso_mitigation {
+ 	SRSO_MITIGATION_NONE,
+ 	SRSO_MITIGATION_MICROCODE,
+ 	SRSO_MITIGATION_SAFE_RET,
+ };
+ 
+ enum srso_mitigation_cmd {
+ 	SRSO_CMD_OFF,
+ 	SRSO_CMD_MICROCODE,
+ 	SRSO_CMD_SAFE_RET,
+ };
+ 
+ static const char * const srso_strings[] = {
+ 	[SRSO_MITIGATION_NONE]           = "Vulnerable",
+ 	[SRSO_MITIGATION_MICROCODE]      = "Mitigation: microcode",
+ 	[SRSO_MITIGATION_SAFE_RET]	 = "Mitigation: safe RET",
+ };
+ 
+ static enum srso_mitigation srso_mitigation __ro_after_init = SRSO_MITIGATION_NONE;
+ static enum srso_mitigation_cmd srso_cmd __ro_after_init = SRSO_CMD_SAFE_RET;
+ 
+ static int __init srso_parse_cmdline(char *str)
+ {
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	if (!strcmp(str, "off"))
+ 		srso_cmd = SRSO_CMD_OFF;
+ 	else if (!strcmp(str, "microcode"))
+ 		srso_cmd = SRSO_CMD_MICROCODE;
+ 	else if (!strcmp(str, "safe-ret"))
+ 		srso_cmd = SRSO_CMD_SAFE_RET;
+ 	else
+ 		pr_err("Ignoring unknown SRSO option (%s).", str);
+ 
+ 	return 0;
+ }
+ early_param("spec_rstack_overflow", srso_parse_cmdline);
+ 
+ #define SRSO_NOTICE "WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options."
+ 
+ static void __init srso_select_mitigation(void)
+ {
+ 	bool has_microcode;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_SRSO) || cpu_mitigations_off())
+ 		goto pred_cmd;
+ 
+ 	/*
+ 	 * The first check is for the kernel running as a guest in order
+ 	 * for guests to verify whether IBPB is a viable mitigation.
+ 	 */
+ 	has_microcode = boot_cpu_has(X86_FEATURE_IBPB_BRTYPE) || cpu_has_ibpb_brtype_microcode();
+ 	if (!has_microcode) {
+ 		pr_warn("IBPB-extending microcode not applied!\n");
+ 		pr_warn(SRSO_NOTICE);
+ 	} else {
+ 		/*
+ 		 * Enable the synthetic (even if in a real CPUID leaf)
+ 		 * flags for guests.
+ 		 */
+ 		setup_force_cpu_cap(X86_FEATURE_IBPB_BRTYPE);
+ 		setup_force_cpu_cap(X86_FEATURE_SBPB);
+ 
+ 		/*
+ 		 * Zen1/2 with SMT off aren't vulnerable after the right
+ 		 * IBPB microcode has been applied.
+ 		 */
+ 		if ((boot_cpu_data.x86 < 0x19) &&
+ 		    (cpu_smt_control == CPU_SMT_DISABLED))
+ 			setup_force_cpu_cap(X86_FEATURE_SRSO_NO);
+ 	}
+ 
+ 	switch (srso_cmd) {
+ 	case SRSO_CMD_OFF:
+ 		return;
+ 
+ 	case SRSO_CMD_MICROCODE:
+ 		if (has_microcode) {
+ 			srso_mitigation = SRSO_MITIGATION_MICROCODE;
+ 			pr_warn(SRSO_NOTICE);
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_SAFE_RET:
+ 		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
+ 			if (boot_cpu_data.x86 == 0x19)
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO_ALIAS);
+ 			else
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO);
+ 			srso_mitigation = SRSO_MITIGATION_SAFE_RET;
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
+ 			goto pred_cmd;
+ 		}
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ 
+ 	pr_info("%s%s\n", srso_strings[srso_mitigation], (has_microcode ? "" : ", no microcode"));
+ 
+ pred_cmd:
+ 	if (boot_cpu_has(X86_FEATURE_SRSO_NO) ||
+ 	    srso_cmd == SRSO_CMD_OFF)
+ 		x86_pred_cmd = PRED_CMD_SBPB;
+ }
+ 
+ #undef pr_fmt
++>>>>>>> 1b5277c0ea0b (x86/srso: Add SRSO_NO support)
  #define pr_fmt(fmt) fmt
  
  #ifdef CONFIG_SYSFS
diff --cc arch/x86/kernel/cpu/common.c
index 6335deb6c15c,5576cdac3b4a..000000000000
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@@ -1303,15 -1406,13 +1303,25 @@@ static void __init cpu_set_bug_bits(str
  			setup_force_cpu_bug(X86_BUG_RETBLEED);
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Check if CPU is vulnerable to GDS. If running in a virtual machine on
 +	 * an affected processor, the VMM may have disabled the use of GATHER by
 +	 * disabling AVX2. The only way to do this in HW is to clear XCR0[2],
 +	 * which means that AVX will be disabled.
 +	 */
 +	if (cpu_matches(cpu_vuln_blacklist, GDS) && !(ia32_cap & ARCH_CAP_GDS_NO) &&
 +	    boot_cpu_has(X86_FEATURE_AVX))
 +		setup_force_cpu_bug(X86_BUG_GDS);
++=======
+ 	if (cpu_matches(cpu_vuln_blacklist, SMT_RSB))
+ 		setup_force_cpu_bug(X86_BUG_SMT_RSB);
+ 
+ 	if (!cpu_has(c, X86_FEATURE_SRSO_NO)) {
+ 		if (cpu_matches(cpu_vuln_blacklist, SRSO))
+ 			setup_force_cpu_bug(X86_BUG_SRSO);
+ 	}
++>>>>>>> 1b5277c0ea0b (x86/srso: Add SRSO_NO support)
  
  	if (cpu_matches(cpu_vuln_whitelist, NO_MELTDOWN))
  		return;
diff --cc arch/x86/kvm/cpuid.c
index eec55b2b9e94,d3432687c9e6..000000000000
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@@ -687,6 -729,13 +687,16 @@@ void kvm_set_cpu_caps(void
  		F(NULL_SEL_CLR_BASE) | F(AUTOIBRS) | 0 /* PrefetchCtlMsr */
  	);
  
++<<<<<<< HEAD
++=======
+ 	if (cpu_feature_enabled(X86_FEATURE_SRSO_NO))
+ 		kvm_cpu_cap_set(X86_FEATURE_SRSO_NO);
+ 
+ 	kvm_cpu_cap_init_kvm_defined(CPUID_8000_0022_EAX,
+ 		F(PERFMON_V2)
+ 	);
+ 
++>>>>>>> 1b5277c0ea0b (x86/srso: Add SRSO_NO support)
  	/*
  	 * Synthesize "LFENCE is serializing" into the AMD-defined entry in
  	 * KVM's supported CPUID if the feature is reported as supported by the
* Unmerged path arch/x86/include/asm/cpufeatures.h
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 6699caababf4..483952c58ced 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -53,6 +53,7 @@
 
 #define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */
 #define PRED_CMD_IBPB			BIT(0)	   /* Indirect Branch Prediction Barrier */
+#define PRED_CMD_SBPB			BIT(7)	   /* Selective Branch Prediction Barrier */
 
 #define MSR_PPIN_CTL			0x0000004e
 #define MSR_PPIN			0x0000004f
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70..b91ea7ff5474 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -308,11 +308,11 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 		: "memory");
 }
 
+extern u64 x86_pred_cmd;
+
 static inline void indirect_branch_prediction_barrier(void)
 {
-	u64 val = PRED_CMD_IBPB;
-
-	alternative_msr_write(MSR_IA32_PRED_CMD, val, X86_FEATURE_USE_IBPB);
+	alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
 }
 
 /* The Intel SPEC CTRL MSR base value cache */
* Unmerged path arch/x86/kernel/cpu/amd.c
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/cpu/common.c
* Unmerged path arch/x86/kvm/cpuid.c
