x86/cpu: Rename original retbleed methods

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit d025b7bac07a6e90b6b98b487f88854ad9247c39
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/d025b7ba.failed

Rename the original retbleed return thunk and untrain_ret to
retbleed_return_thunk() and retbleed_untrain_ret().

No functional changes.

	Suggested-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/20230814121148.909378169@infradead.org
(cherry picked from commit d025b7bac07a6e90b6b98b487f88854ad9247c39)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/vmlinux.lds.S
#	arch/x86/lib/retpoline.S
#	tools/objtool/arch/x86/decode.c
diff --cc arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70,8a0d4c5f4cb7..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -201,14 -333,75 +201,29 @@@
  #define ANNOTATE_RETPOLINE_SAFE					\
  	"999:\n\t"						\
  	".pushsection .discard.retpoline_safe\n\t"		\
 -	".long 999b - .\n\t"					\
 +	_ASM_PTR " 999b\n\t"					\
  	".popsection\n\t"
  
 -typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
 -extern retpoline_thunk_t __x86_indirect_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];
 -
 -#ifdef CONFIG_RETHUNK
  extern void __x86_return_thunk(void);
++<<<<<<< HEAD
 +extern void zen_untrain_ret(void);
++=======
+ #else
+ static inline void __x86_return_thunk(void) {}
+ #endif
+ 
+ extern void retbleed_return_thunk(void);
+ extern void srso_return_thunk(void);
+ extern void srso_alias_return_thunk(void);
+ 
+ extern void retbleed_untrain_ret(void);
+ extern void srso_untrain_ret(void);
+ extern void srso_untrain_ret_alias(void);
+ 
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  extern void entry_ibpb(void);
  
 -extern void (*x86_return_thunk)(void);
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -extern void __x86_return_skl(void);
 -
 -static inline void x86_set_skl_return_thunk(void)
 -{
 -	x86_return_thunk = &__x86_return_skl;
 -}
 -
 -#define CALL_DEPTH_ACCOUNT					\
 -	ALTERNATIVE("",						\
 -		    __stringify(INCREMENT_CALL_DEPTH),		\
 -		    X86_FEATURE_CALL_DEPTH)
 -
 -#ifdef CONFIG_CALL_THUNKS_DEBUG
 -DECLARE_PER_CPU(u64, __x86_call_count);
 -DECLARE_PER_CPU(u64, __x86_ret_count);
 -DECLARE_PER_CPU(u64, __x86_stuffs_count);
 -DECLARE_PER_CPU(u64, __x86_ctxsw_count);
 -#endif
 -#else
 -static inline void x86_set_skl_return_thunk(void) {}
 -
 -#define CALL_DEPTH_ACCOUNT ""
 -
 -#endif
 -
  #ifdef CONFIG_RETPOLINE
 -
 -#define GEN(reg) \
 -	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_call_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_jump_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
  #ifdef CONFIG_X86_64
  
  /*
diff --cc arch/x86/kernel/cpu/bugs.c
index adea0002a1a8,bbbbda981bab..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -1020,6 -1042,9 +1020,12 @@@ do_cmd_auto
  		setup_force_cpu_cap(X86_FEATURE_RETHUNK);
  		setup_force_cpu_cap(X86_FEATURE_UNRET);
  
++<<<<<<< HEAD
++=======
+ 		if (IS_ENABLED(CONFIG_RETHUNK))
+ 			x86_return_thunk = retbleed_return_thunk;
+ 
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  		if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&
  		    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)
  			pr_err(RETBLEED_UNTRAIN_MSG);
diff --cc arch/x86/kernel/vmlinux.lds.S
index 6df2973aa8ce,7c0e2b4fc344..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -441,12 -520,25 +441,18 @@@ INIT_PER_CPU(irq_stack_backing_store)
             "fixed_percpu_data is not at start of per-cpu area");
  #endif
  
++<<<<<<< HEAD
 +#endif /* CONFIG_X86_32 */
 +
 +#ifdef CONFIG_KEXEC_CORE
 +#include <asm/kexec.h>
 +
 +. = ASSERT(kexec_control_code_size <= KEXEC_CONTROL_CODE_MAX_SIZE,
 +           "kexec control code size is too big");
++=======
+ #ifdef CONFIG_RETHUNK
+ . = ASSERT((retbleed_return_thunk & 0x3f) == 0, "retbleed_return_thunk not cacheline-aligned");
+ . = ASSERT((srso_safe_ret & 0x3f) == 0, "srso_safe_ret not cacheline-aligned");
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  #endif
  
 -#ifdef CONFIG_CPU_SRSO
 -/*
 - * GNU ld cannot do XOR until 2.41.
 - * https://sourceware.org/git/?p=binutils-gdb.git;a=commit;h=f6f78318fca803c4907fb8d7f6ded8295f1947b1
 - *
 - * LLVM lld cannot do XOR until lld-17.
 - * https://github.com/llvm/llvm-project/commit/fae96104d4378166cbe5c875ef8ed808a356f3fb
 - *
 - * Instead do: (A | B) - (A & B) in order to compute the XOR
 - * of the two function addresses:
 - */
 -. = ASSERT(((ABSOLUTE(srso_untrain_ret_alias) | srso_safe_ret_alias) -
 -		(ABSOLUTE(srso_untrain_ret_alias) & srso_safe_ret_alias)) == ((1 << 2) | (1 << 8) | (1 << 14) | (1 << 20)),
 -		"SRSO function pair won't alias");
 -#endif
 -
 -#endif /* CONFIG_X86_64 */
diff --cc arch/x86/lib/retpoline.S
index ce2b806fe45d,adabd07eab99..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -57,32 -188,32 +57,53 @@@ GENERATE_THUNK(r15
  
  /*
   * Safety details here pertain to the AMD Zen{1,2} microarchitecture:
++<<<<<<< HEAD
 + * 1) The RET at __x86_return_thunk must be on a 64 byte boundary, for
++=======
+  * 1) The RET at retbleed_return_thunk must be on a 64 byte boundary, for
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
   *    alignment within the BTB.
-  * 2) The instruction at zen_untrain_ret must contain, and not
+  * 2) The instruction at retbleed_untrain_ret must contain, and not
   *    end with, the 0xc3 byte of the RET.
   * 3) STIBP must be enabled, or SMT disabled, to prevent the sibling thread
   *    from re-poisioning the BTB prediction.
   */
  	.align 64
++<<<<<<< HEAD
 +	.skip 63, 0xcc
 +SYM_START(zen_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
++=======
+ 	.skip 64 - (retbleed_return_thunk - retbleed_untrain_ret), 0xcc
+ SYM_START(retbleed_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  	ANNOTATE_NOENDBR
  	/*
- 	 * As executed from zen_untrain_ret, this is:
+ 	 * As executed from retbleed_untrain_ret, this is:
  	 *
  	 *   TEST $0xcc, %bl
  	 *   LFENCE
++<<<<<<< HEAD
 +	 *   JMP __x86_return_thunk
 +	 *
 +	 * Executing the TEST instruction has a side effect of evicting any BTB
 +	 * prediction (potentially attacker controlled) attached to the RET, as
 +	 * __x86_return_thunk + 1 isn't an instruction boundary at the moment.
++=======
+ 	 *   JMP retbleed_return_thunk
+ 	 *
+ 	 * Executing the TEST instruction has a side effect of evicting any BTB
+ 	 * prediction (potentially attacker controlled) attached to the RET, as
+ 	 * retbleed_return_thunk + 1 isn't an instruction boundary at the moment.
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  	 */
  	.byte	0xf6
  
  	/*
++<<<<<<< HEAD
 +	 * As executed from __x86_return_thunk, this is a plain RET.
++=======
+ 	 * As executed from retbleed_return_thunk, this is a plain RET.
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  	 *
  	 * As part of the TEST above, RET is the ModRM byte, and INT3 the imm8.
  	 *
@@@ -94,13 -225,13 +115,23 @@@
  	 * With SMT enabled and STIBP active, a sibling thread cannot poison
  	 * RET's prediction to a type of its choice, but can evict the
  	 * prediction due to competitive sharing. If the prediction is
++<<<<<<< HEAD
 +	 * evicted, __x86_return_thunk will suffer Straight Line Speculation
 +	 * which will be contained safely by the INT3.
 +	 */
 +SYM_INNER_LABEL(__x86_return_thunk, SYM_L_GLOBAL)
 +	ret
 +	int3
 +SYM_CODE_END(__x86_return_thunk)
++=======
+ 	 * evicted, retbleed_return_thunk will suffer Straight Line Speculation
+ 	 * which will be contained safely by the INT3.
+ 	 */
+ SYM_INNER_LABEL(retbleed_return_thunk, SYM_L_GLOBAL)
+ 	ret
+ 	int3
+ SYM_CODE_END(retbleed_return_thunk)
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  
  	/*
  	 * Ensure the TEST decoding / BTB invalidation is complete.
@@@ -111,11 -242,60 +142,67 @@@
  	 * Jump back and execute the RET in the middle of the TEST instruction.
  	 * INT3 is for SLS protection.
  	 */
++<<<<<<< HEAD
 +	jmp __x86_return_thunk
++=======
+ 	jmp retbleed_return_thunk
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  	int3
- SYM_FUNC_END(zen_untrain_ret)
- __EXPORT_THUNK(zen_untrain_ret)
+ SYM_FUNC_END(retbleed_untrain_ret)
+ __EXPORT_THUNK(retbleed_untrain_ret)
  
++<<<<<<< HEAD
++=======
+ /*
+  * SRSO untraining sequence for Zen1/2, similar to retbleed_untrain_ret()
+  * above. On kernel entry, srso_untrain_ret() is executed which is a
+  *
+  * movabs $0xccccc30824648d48,%rax
+  *
+  * and when the return thunk executes the inner label srso_safe_ret()
+  * later, it is a stack manipulation and a RET which is mispredicted and
+  * thus a "safe" one to use.
+  */
+ 	.align 64
+ 	.skip 64 - (srso_safe_ret - srso_untrain_ret), 0xcc
+ SYM_START(srso_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	ANNOTATE_NOENDBR
+ 	.byte 0x48, 0xb8
+ 
+ /*
+  * This forces the function return instruction to speculate into a trap
+  * (UD2 in srso_return_thunk() below).  This RET will then mispredict
+  * and execution will continue at the return site read from the top of
+  * the stack.
+  */
+ SYM_INNER_LABEL(srso_safe_ret, SYM_L_GLOBAL)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	ret
+ 	int3
+ 	int3
+ 	/* end of movabs */
+ 	lfence
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_safe_ret)
+ SYM_FUNC_END(srso_untrain_ret)
+ __EXPORT_THUNK(srso_untrain_ret)
+ 
+ SYM_CODE_START(srso_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_return_thunk)
+ 
+ SYM_CODE_START(__x86_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
  EXPORT_SYMBOL(__x86_return_thunk)
  
  #endif /* CONFIG_RETHUNK */
diff --cc tools/objtool/arch/x86/decode.c
index 603f71ad0722,c0f25d00181e..000000000000
--- a/tools/objtool/arch/x86/decode.c
+++ b/tools/objtool/arch/x86/decode.c
@@@ -600,3 -826,9 +600,12 @@@ bool arch_is_rethunk(struct symbol *sym
  {
  	return !strcmp(sym->name, "__x86_return_thunk");
  }
++<<<<<<< HEAD
++=======
+ 
+ bool arch_is_embedded_insn(struct symbol *sym)
+ {
+ 	return !strcmp(sym->name, "retbleed_return_thunk") ||
+ 	       !strcmp(sym->name, "srso_safe_ret");
+ }
++>>>>>>> d025b7bac07a (x86/cpu: Rename original retbleed methods)
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/vmlinux.lds.S
* Unmerged path arch/x86/lib/retpoline.S
* Unmerged path tools/objtool/arch/x86/decode.c
diff --git a/tools/objtool/check.c b/tools/objtool/check.c
index 4b7db795960d..f9e9460605b9 100644
--- a/tools/objtool/check.c
+++ b/tools/objtool/check.c
@@ -690,7 +690,7 @@ static int add_jump_destinations(struct objtool_file *file)
 			struct symbol *sym = find_symbol_by_offset(dest_sec, dest_off);
 
 			/*
-			 * This is a special case for zen_untrain_ret().
+			 * This is a special case for retbleed_untrain_ret().
 			 * It jumps to __x86_return_thunk(), but objtool
 			 * can't find the thunk's starting RET
 			 * instruction, because the RET is also in the
