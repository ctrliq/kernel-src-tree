x86/srso: Disentangle rethunk-dependent options

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Josh Poimboeuf <jpoimboe@kernel.org>
commit 34a3cae7474c6e6f4a85aad4a7b8191b8b35cdcd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/34a3cae7.failed

CONFIG_RETHUNK, CONFIG_CPU_UNRET_ENTRY and CONFIG_CPU_SRSO are all
tangled up.  De-spaghettify the code a bit.

Some of the rethunk-related code has been shuffled around within the
'.text..__x86.return_thunk' section, but otherwise there are no
functional changes.  srso_alias_untrain_ret() and srso_alias_safe_ret()
((which are very address-sensitive) haven't moved.

	Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
	Acked-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/2845084ed303d8384905db3b87b77693945302b4.1693889988.git.jpoimboe@kernel.org
(cherry picked from commit 34a3cae7474c6e6f4a85aad4a7b8191b8b35cdcd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/vmlinux.lds.S
#	arch/x86/lib/retpoline.S
diff --cc arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70,51e3f1a287d2..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -189,10 -289,40 +189,47 @@@
   * where we have a stack but before any RET instruction.
   */
  .macro UNTRAIN_RET
++<<<<<<< HEAD
 +#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY)
 +	ALTERNATIVE_2 "",						\
 +	              CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
 +		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB
++=======
+ #if defined(CONFIG_RETHUNK) || defined(CONFIG_CPU_IBPB_ENTRY)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		     __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
+ #endif
+ .endm
+ 
+ .macro UNTRAIN_RET_VM
+ #if defined(CONFIG_RETHUNK) || defined(CONFIG_CPU_IBPB_ENTRY)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_IBPB_ON_VMEXIT,	\
+ 		      __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
+ #endif
+ .endm
+ 
+ .macro UNTRAIN_RET_FROM_CALL
+ #if defined(CONFIG_RETHUNK) || defined(CONFIG_CPU_IBPB_ENTRY)
+ 	VALIDATE_UNRET_END
+ 	ALTERNATIVE_3 "",						\
+ 		      CALL_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB,	\
+ 		      __stringify(RESET_CALL_DEPTH_FROM_CALL), X86_FEATURE_CALL_DEPTH
+ #endif
+ .endm
+ 
+ 
+ .macro CALL_DEPTH_ACCOUNT
+ #ifdef CONFIG_CALL_DEPTH_TRACKING
+ 	ALTERNATIVE "",							\
+ 		    __stringify(ASM_INCREMENT_CALL_DEPTH), X86_FEATURE_CALL_DEPTH
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
  #endif
  .endm
  
@@@ -201,14 -331,86 +238,40 @@@
  #define ANNOTATE_RETPOLINE_SAFE					\
  	"999:\n\t"						\
  	".pushsection .discard.retpoline_safe\n\t"		\
 -	".long 999b - .\n\t"					\
 +	_ASM_PTR " 999b\n\t"					\
  	".popsection\n\t"
  
 -typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
 -extern retpoline_thunk_t __x86_indirect_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];
 -
 -#ifdef CONFIG_RETHUNK
  extern void __x86_return_thunk(void);
++<<<<<<< HEAD
 +extern void zen_untrain_ret(void);
++=======
+ #else
+ static inline void __x86_return_thunk(void) {}
+ #endif
+ 
+ #ifdef CONFIG_CPU_UNRET_ENTRY
+ extern void retbleed_return_thunk(void);
+ #else
+ static inline void retbleed_return_thunk(void) {}
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ extern void srso_return_thunk(void);
+ extern void srso_alias_return_thunk(void);
+ #else
+ static inline void srso_return_thunk(void) {}
+ static inline void srso_alias_return_thunk(void) {}
+ #endif
+ 
+ extern void retbleed_return_thunk(void);
+ extern void srso_return_thunk(void);
+ extern void srso_alias_return_thunk(void);
+ 
+ extern void entry_untrain_ret(void);
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
  extern void entry_ibpb(void);
  
 -extern void (*x86_return_thunk)(void);
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -extern void __x86_return_skl(void);
 -
 -static inline void x86_set_skl_return_thunk(void)
 -{
 -	x86_return_thunk = &__x86_return_skl;
 -}
 -
 -#define CALL_DEPTH_ACCOUNT					\
 -	ALTERNATIVE("",						\
 -		    __stringify(INCREMENT_CALL_DEPTH),		\
 -		    X86_FEATURE_CALL_DEPTH)
 -
 -#ifdef CONFIG_CALL_THUNKS_DEBUG
 -DECLARE_PER_CPU(u64, __x86_call_count);
 -DECLARE_PER_CPU(u64, __x86_ret_count);
 -DECLARE_PER_CPU(u64, __x86_stuffs_count);
 -DECLARE_PER_CPU(u64, __x86_ctxsw_count);
 -#endif
 -#else
 -static inline void x86_set_skl_return_thunk(void) {}
 -
 -#define CALL_DEPTH_ACCOUNT ""
 -
 -#endif
 -
  #ifdef CONFIG_RETPOLINE
 -
 -#define GEN(reg) \
 -	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_call_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_jump_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
  #ifdef CONFIG_X86_64
  
  /*
diff --cc arch/x86/kernel/cpu/bugs.c
index adea0002a1a8,9731e814dcb6..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -58,8 -58,13 +58,13 @@@ EXPORT_SYMBOL_GPL(x86_spec_ctrl_base)
  DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
  EXPORT_SYMBOL_GPL(x86_spec_ctrl_current);
  
 -u64 x86_pred_cmd __ro_after_init = PRED_CMD_IBPB;
 -EXPORT_SYMBOL_GPL(x86_pred_cmd);
 -
  static DEFINE_MUTEX(spec_ctrl_mutex);
  
++<<<<<<< HEAD
++=======
+ void (*x86_return_thunk)(void) __ro_after_init = __x86_return_thunk;
+ 
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
  /* Update SPEC_CTRL MSR and its cached copy unconditionally */
  static void update_spec_ctrl(u64 val)
  {
@@@ -1020,6 -1041,8 +1025,11 @@@ do_cmd_auto
  		setup_force_cpu_cap(X86_FEATURE_RETHUNK);
  		setup_force_cpu_cap(X86_FEATURE_UNRET);
  
++<<<<<<< HEAD
++=======
+ 		x86_return_thunk = retbleed_return_thunk;
+ 
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
  		if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&
  		    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)
  			pr_err(RETBLEED_UNTRAIN_MSG);
diff --cc arch/x86/kernel/vmlinux.lds.S
index 6df2973aa8ce,54a5596adaa6..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -122,36 -129,39 +122,40 @@@ SECTION
  		HEAD_TEXT
  		TEXT_TEXT
  		SCHED_TEXT
 +		CPUIDLE_TEXT
  		LOCK_TEXT
  		KPROBES_TEXT
 -		SOFTIRQENTRY_TEXT
 -#ifdef CONFIG_RETPOLINE
 -		*(.text..__x86.indirect_thunk)
 -		*(.text..__x86.return_thunk)
 -#endif
 -		STATIC_CALL_TEXT
 -
  		ALIGN_ENTRY_TEXT_BEGIN
++<<<<<<< HEAD
++=======
+ 		*(.text..__x86.rethunk_untrain)
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
  		ENTRY_TEXT
 -
 -#ifdef CONFIG_CPU_SRSO
 -		/*
 -		 * See the comment above srso_alias_untrain_ret()'s
 -		 * definition.
 -		 */
 -		. = srso_alias_untrain_ret | (1 << 2) | (1 << 8) | (1 << 14) | (1 << 20);
 -		*(.text..__x86.rethunk_safe)
 -#endif
 +		IRQENTRY_TEXT
  		ALIGN_ENTRY_TEXT_END
 +		SOFTIRQENTRY_TEXT
 +		*(.fixup)
  		*(.gnu.warning)
  
 -	} :text = 0xcccccccc
 +#ifdef CONFIG_RETPOLINE
 +		__indirect_thunk_start = .;
 +		*(.text.__x86.*)
 +		__indirect_thunk_end = .;
 +#endif
  
 -	/* End of text section, which should occupy whole number of pages */
 -	_etext = .;
 -	. = ALIGN(PAGE_SIZE);
 +		/* End of text section */
 +		_etext = .;
 +	} :text = 0x9090
  
 -	X86_ALIGN_RODATA_BEGIN
 +	NOTES :text :note
 +
 +	EXCEPTION_TABLE(16) :text = 0x9090
 +
 +	/* .text should occupy whole number of pages */
 +	. = ALIGN(PAGE_SIZE);
 +	X64_ALIGN_RODATA_BEGIN
  	RO_DATA(PAGE_SIZE)
 -	X86_ALIGN_RODATA_END
 +	X64_ALIGN_RODATA_END
  
  	/* Data */
  	.data : AT(ADDR(.data) - LOAD_OFFSET) {
@@@ -441,12 -517,25 +445,37 @@@ INIT_PER_CPU(irq_stack_backing_store)
             "fixed_percpu_data is not at start of per-cpu area");
  #endif
  
++<<<<<<< HEAD
 +#endif /* CONFIG_X86_32 */
 +
 +#ifdef CONFIG_KEXEC_CORE
 +#include <asm/kexec.h>
 +
 +. = ASSERT(kexec_control_code_size <= KEXEC_CONTROL_CODE_MAX_SIZE,
 +           "kexec control code size is too big");
 +#endif
 +
++=======
+ #ifdef CONFIG_CPU_UNRET_ENTRY
+ . = ASSERT((retbleed_return_thunk & 0x3f) == 0, "retbleed_return_thunk not cacheline-aligned");
+ #endif
+ 
+ #ifdef CONFIG_CPU_SRSO
+ . = ASSERT((srso_safe_ret & 0x3f) == 0, "srso_safe_ret not cacheline-aligned");
+ /*
+  * GNU ld cannot do XOR until 2.41.
+  * https://sourceware.org/git/?p=binutils-gdb.git;a=commit;h=f6f78318fca803c4907fb8d7f6ded8295f1947b1
+  *
+  * LLVM lld cannot do XOR until lld-17.
+  * https://github.com/llvm/llvm-project/commit/fae96104d4378166cbe5c875ef8ed808a356f3fb
+  *
+  * Instead do: (A | B) - (A & B) in order to compute the XOR
+  * of the two function addresses:
+  */
+ . = ASSERT(((ABSOLUTE(srso_alias_untrain_ret) | srso_alias_safe_ret) -
+ 		(ABSOLUTE(srso_alias_untrain_ret) & srso_alias_safe_ret)) == ((1 << 2) | (1 << 8) | (1 << 14) | (1 << 20)),
+ 		"SRSO function pair won't alias");
+ #endif
+ 
+ #endif /* CONFIG_X86_64 */
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
diff --cc arch/x86/lib/retpoline.S
index 72a7235229fd,8ba79d2b8997..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -24,36 -49,181 +24,136 @@@ SYM_FUNC_END(__x86_indirect_thunk_\reg
   * only see one instance of "__x86_indirect_thunk_\reg" rather
   * than one per register with the correct names. So we do it
   * the simple and nasty way...
 - *
 - * Worse, you can only have a single EXPORT_SYMBOL per line,
 - * and CPP can't insert newlines, so we have to repeat everything
 - * at least twice.
   */
 -
 -#define __EXPORT_THUNK(sym)	_ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_thunk_array)
 -
 -#define GEN(reg) THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -.macro CALL_THUNK reg
 -	.align RETPOLINE_THUNK_SIZE
 -
 -SYM_INNER_LABEL(__x86_indirect_call_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_UNDEFINED
 -	ANNOTATE_NOENDBR
 -
 -	CALL_DEPTH_ACCOUNT
 -	POLINE \reg
 -	ANNOTATE_UNRET_SAFE
 -	ret
 -	int3
 -.endm
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_call_thunk_array)
 -
 -#define GEN(reg) CALL_THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_call_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_call_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -.macro JUMP_THUNK reg
 -	.align RETPOLINE_THUNK_SIZE
 -
 -SYM_INNER_LABEL(__x86_indirect_jump_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_UNDEFINED
 -	ANNOTATE_NOENDBR
 -	POLINE \reg
 -	ANNOTATE_UNRET_SAFE
 -	ret
 -	int3
 -.endm
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_START(__x86_indirect_jump_thunk_array)
 -
 -#define GEN(reg) JUMP_THUNK reg
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_CODE_END(__x86_indirect_jump_thunk_array)
 -
 -#define GEN(reg) __EXPORT_THUNK(__x86_indirect_jump_thunk_ ## reg)
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 +#define __EXPORT_THUNK(sym) _ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 +#define EXPORT_THUNK(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 +#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)
 +
 +GENERATE_THUNK(_ASM_AX)
 +GENERATE_THUNK(_ASM_BX)
 +GENERATE_THUNK(_ASM_CX)
 +GENERATE_THUNK(_ASM_DX)
 +GENERATE_THUNK(_ASM_SI)
 +GENERATE_THUNK(_ASM_DI)
 +GENERATE_THUNK(_ASM_BP)
 +#ifdef CONFIG_64BIT
 +GENERATE_THUNK(r8)
 +GENERATE_THUNK(r9)
 +GENERATE_THUNK(r10)
 +GENERATE_THUNK(r11)
 +GENERATE_THUNK(r12)
 +GENERATE_THUNK(r13)
 +GENERATE_THUNK(r14)
 +GENERATE_THUNK(r15)
  #endif
  
++<<<<<<< HEAD
 +/*
 + * This function name is magical and is used by -mfunction-return=thunk-extern
 + * for the compiler to generate JMPs to it.
 + */
 +#ifdef CONFIG_RETHUNK
 +
 +	.section .text.__x86.return_thunk
++=======
+ #ifdef CONFIG_RETHUNK
+ 
+ 	.section .text..__x86.return_thunk
+ 
+ #ifdef CONFIG_CPU_SRSO
+ 
+ /*
+  * srso_alias_untrain_ret() and srso_alias_safe_ret() are placed at
+  * special addresses:
+  *
+  * - srso_alias_untrain_ret() is 2M aligned
+  * - srso_alias_safe_ret() is also in the same 2M page but bits 2, 8, 14
+  * and 20 in its virtual address are set (while those bits in the
+  * srso_alias_untrain_ret() function are cleared).
+  *
+  * This guarantees that those two addresses will alias in the branch
+  * target buffer of Zen3/4 generations, leading to any potential
+  * poisoned entries at that BTB slot to get evicted.
+  *
+  * As a result, srso_alias_safe_ret() becomes a safe return.
+  */
+ 	.pushsection .text..__x86.rethunk_untrain
+ SYM_START(srso_alias_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ASM_NOP2
+ 	lfence
+ 	jmp srso_alias_return_thunk
+ SYM_FUNC_END(srso_alias_untrain_ret)
+ 	.popsection
+ 
+ 	.pushsection .text..__x86.rethunk_safe
+ SYM_START(srso_alias_safe_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_alias_safe_ret)
+ 
+ SYM_CODE_START_NOALIGN(srso_alias_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_alias_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_alias_return_thunk)
+ 	.popsection
+ 
+ /*
+  * SRSO untraining sequence for Zen1/2, similar to retbleed_untrain_ret()
+  * above. On kernel entry, srso_untrain_ret() is executed which is a
+  *
+  * movabs $0xccccc30824648d48,%rax
+  *
+  * and when the return thunk executes the inner label srso_safe_ret()
+  * later, it is a stack manipulation and a RET which is mispredicted and
+  * thus a "safe" one to use.
+  */
+ 	.align 64
+ 	.skip 64 - (srso_safe_ret - srso_untrain_ret), 0xcc
+ SYM_START(srso_untrain_ret, SYM_L_LOCAL, SYM_A_NONE)
+ 	ANNOTATE_NOENDBR
+ 	.byte 0x48, 0xb8
+ 
+ /*
+  * This forces the function return instruction to speculate into a trap
+  * (UD2 in srso_return_thunk() below).  This RET will then mispredict
+  * and execution will continue at the return site read from the top of
+  * the stack.
+  */
+ SYM_INNER_LABEL(srso_safe_ret, SYM_L_GLOBAL)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	ret
+ 	int3
+ 	int3
+ 	/* end of movabs */
+ 	lfence
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_safe_ret)
+ SYM_FUNC_END(srso_untrain_ret)
+ 
+ SYM_CODE_START(srso_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_return_thunk)
+ 
+ #define JMP_SRSO_UNTRAIN_RET "jmp srso_untrain_ret"
+ #define JMP_SRSO_ALIAS_UNTRAIN_RET "jmp srso_alias_untrain_ret"
+ #else /* !CONFIG_CPU_SRSO */
+ #define JMP_SRSO_UNTRAIN_RET "ud2"
+ #define JMP_SRSO_ALIAS_UNTRAIN_RET "ud2"
+ #endif /* CONFIG_CPU_SRSO */
+ 
+ #ifdef CONFIG_CPU_UNRET_ENTRY
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
  
  /*
   * Some generic notes on the untraining sequences:
@@@ -130,11 -300,75 +230,79 @@@ SYM_CODE_END(__x86_return_thunk
  	 * Jump back and execute the RET in the middle of the TEST instruction.
  	 * INT3 is for SLS protection.
  	 */
 -	jmp retbleed_return_thunk
 +	jmp __x86_return_thunk
  	int3
 -SYM_FUNC_END(retbleed_untrain_ret)
 +SYM_FUNC_END(zen_untrain_ret)
 +__EXPORT_THUNK(zen_untrain_ret)
  
++<<<<<<< HEAD
++=======
+ #define JMP_RETBLEED_UNTRAIN_RET "jmp retbleed_untrain_ret"
+ #else /* !CONFIG_CPU_UNRET_ENTRY */
+ #define JMP_RETBLEED_UNTRAIN_RET "ud2"
+ #endif /* CONFIG_CPU_UNRET_ENTRY */
+ 
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_SRSO)
+ 
+ SYM_FUNC_START(entry_untrain_ret)
+ 	ALTERNATIVE_2 JMP_RETBLEED_UNTRAIN_RET,				\
+ 		      JMP_SRSO_UNTRAIN_RET, X86_FEATURE_SRSO,		\
+ 		      JMP_SRSO_ALIAS_UNTRAIN_RET, X86_FEATURE_SRSO_ALIAS
+ SYM_FUNC_END(entry_untrain_ret)
+ __EXPORT_THUNK(entry_untrain_ret)
+ 
+ #endif /* CONFIG_CPU_UNRET_ENTRY || CONFIG_CPU_SRSO */
+ 
+ #ifdef CONFIG_CALL_DEPTH_TRACKING
+ 
+ 	.align 64
+ SYM_FUNC_START(__x86_return_skl)
+ 	ANNOTATE_NOENDBR
+ 	/*
+ 	 * Keep the hotpath in a 16byte I-fetch for the non-debug
+ 	 * case.
+ 	 */
+ 	CALL_THUNKS_DEBUG_INC_RETS
+ 	shlq	$5, PER_CPU_VAR(pcpu_hot + X86_call_depth)
+ 	jz	1f
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ 1:
+ 	CALL_THUNKS_DEBUG_INC_STUFFS
+ 	.rept	16
+ 	ANNOTATE_INTRA_FUNCTION_CALL
+ 	call	2f
+ 	int3
+ 2:
+ 	.endr
+ 	add	$(8*16), %rsp
+ 
+ 	CREDIT_CALL_DEPTH
+ 
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(__x86_return_skl)
+ 
+ #endif /* CONFIG_CALL_DEPTH_TRACKING */
+ 
+ /*
+  * This function name is magical and is used by -mfunction-return=thunk-extern
+  * for the compiler to generate JMPs to it.
+  *
+  * This code is only used during kernel boot or module init.  All
+  * 'JMP __x86_return_thunk' sites are changed to something else by
+  * apply_returns().
+  */
+ SYM_CODE_START(__x86_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
++>>>>>>> 34a3cae7474c (x86/srso: Disentangle rethunk-dependent options)
  EXPORT_SYMBOL(__x86_return_thunk)
  
  #endif /* CONFIG_RETHUNK */
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/vmlinux.lds.S
* Unmerged path arch/x86/lib/retpoline.S
