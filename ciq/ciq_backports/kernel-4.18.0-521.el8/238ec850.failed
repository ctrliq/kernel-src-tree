x86/srso: Fix return thunks in generated code

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Josh Poimboeuf <jpoimboe@kernel.org>
commit 238ec850b95a02dcdff3edc86781aa913549282f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/238ec850.failed

Set X86_FEATURE_RETHUNK when enabling the SRSO mitigation so that
generated code (e.g., ftrace, static call, eBPF) generates "jmp
__x86_return_thunk" instead of RET.

  [ bp: Add a comment. ]

Fixes: fb3bd914b3ec ("x86/srso: Add a Speculative RAS Overflow mitigation")
	Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
(cherry picked from commit 238ec850b95a02dcdff3edc86781aa913549282f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/alternative.c
#	arch/x86/kernel/cpu/bugs.c
diff --cc arch/x86/kernel/alternative.c
index 76e6ae92f7f2,2dcf3a06af09..000000000000
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@@ -461,51 -706,553 +461,62 @@@ static int patch_return(void *addr, str
  {
  	int i = 0;
  
 -	/* Patch the custom return thunks... */
 -	if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
 -		i = JMP32_INSN_SIZE;
 -		__text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);
 -	} else {
 -		/* ... or patch them out if not needed. */
 -		bytes[i++] = RET_INSN_OPCODE;
 -	}
 -
 -	for (; i < insn->length;)
 -		bytes[i++] = INT3_INSN_OPCODE;
 -	return i;
 -}
 -
 -void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	/*
 -	 * Do not patch out the default return thunks if those needed are the
 -	 * ones generated by the compiler.
 -	 */
 -	if (cpu_feature_enabled(X86_FEATURE_RETHUNK) &&
 -	    (x86_return_thunk == __x86_return_thunk))
 -		return;
 -
 -	for (s = start; s < end; s++) {
 -		void *dest = NULL, *addr = (void *)s + *s;
 -		struct insn insn;
 -		int len, ret;
 -		u8 bytes[16];
 -		u8 op;
 -
 -		ret = insn_decode_kernel(&insn, addr);
 -		if (WARN_ON_ONCE(ret < 0))
 -			continue;
 -
 -		op = insn.opcode.bytes[0];
 -		if (op == JMP32_INSN_OPCODE)
 -			dest = addr + insn.length + insn.immediate.value;
 -
 -		if (__static_call_fixup(addr, op, dest) ||
 -		    WARN_ONCE(dest != &__x86_return_thunk,
 -			      "missing return thunk: %pS-%pS: %*ph",
 -			      addr, dest, 5, addr))
 -			continue;
 -
 -		DPRINTK(RET, "return thunk at: %pS (%px) len: %d to: %pS",
 -			addr, addr, insn.length,
 -			addr + insn.length + insn.immediate.value);
 -
 -		len = patch_return(addr, &insn, bytes);
 -		if (len == insn.length) {
 -			DUMP_BYTES(RET, ((u8*)addr),  len, "%px: orig: ", addr);
 -			DUMP_BYTES(RET, ((u8*)bytes), len, "%px: repl: ", addr);
 -			text_poke_early(addr, bytes, len);
 -		}
 -	}
 -}
 -#else
 -void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 -#endif /* CONFIG_RETHUNK */
 -
 -#else /* !CONFIG_RETPOLINE || !CONFIG_OBJTOOL */
 -
 -void __init_or_module noinline apply_retpolines(s32 *start, s32 *end) { }
 -void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 -
 -#endif /* CONFIG_RETPOLINE && CONFIG_OBJTOOL */
 -
 -#ifdef CONFIG_X86_KERNEL_IBT
 -
 -static void poison_cfi(void *addr);
 -
 -static void __init_or_module poison_endbr(void *addr, bool warn)
 -{
 -	u32 endbr, poison = gen_endbr_poison();
 -
 -	if (WARN_ON_ONCE(get_kernel_nofault(endbr, addr)))
 -		return;
 -
 -	if (!is_endbr(endbr)) {
 -		WARN_ON_ONCE(warn);
 -		return;
 -	}
 -
 -	DPRINTK(ENDBR, "ENDBR at: %pS (%px)", addr, addr);
 -
 -	/*
 -	 * When we have IBT, the lack of ENDBR will trigger #CP
 -	 */
 -	DUMP_BYTES(ENDBR, ((u8*)addr), 4, "%px: orig: ", addr);
 -	DUMP_BYTES(ENDBR, ((u8*)&poison), 4, "%px: repl: ", addr);
 -	text_poke_early(addr, &poison, 4);
 -}
 -
 -/*
 - * Generated by: objtool --ibt
 - *
 - * Seal the functions for indirect calls by clobbering the ENDBR instructions
 - * and the kCFI hash value.
 - */
 -void __init_or_module noinline apply_seal_endbr(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -
 -		poison_endbr(addr, true);
 -		if (IS_ENABLED(CONFIG_FINEIBT))
 -			poison_cfi(addr - 16);
 -	}
 -}
 -
 -#else
 -
 -void __init_or_module apply_seal_endbr(s32 *start, s32 *end) { }
 -
 -#endif /* CONFIG_X86_KERNEL_IBT */
 -
 -#ifdef CONFIG_FINEIBT
 -
 -enum cfi_mode {
 -	CFI_DEFAULT,
 -	CFI_OFF,
 -	CFI_KCFI,
 -	CFI_FINEIBT,
 -};
 -
 -static enum cfi_mode cfi_mode __ro_after_init = CFI_DEFAULT;
 -static bool cfi_rand __ro_after_init = true;
 -static u32  cfi_seed __ro_after_init;
 -
 -/*
 - * Re-hash the CFI hash with a boot-time seed while making sure the result is
 - * not a valid ENDBR instruction.
 - */
 -static u32 cfi_rehash(u32 hash)
 -{
 -	hash ^= cfi_seed;
 -	while (unlikely(is_endbr(hash) || is_endbr(-hash))) {
 -		bool lsb = hash & 1;
 -		hash >>= 1;
 -		if (lsb)
 -			hash ^= 0x80200003;
 -	}
 -	return hash;
 -}
 -
 -static __init int cfi_parse_cmdline(char *str)
 -{
 -	if (!str)
 -		return -EINVAL;
 -
 -	while (str) {
 -		char *next = strchr(str, ',');
 -		if (next) {
 -			*next = 0;
 -			next++;
 -		}
 -
 -		if (!strcmp(str, "auto")) {
 -			cfi_mode = CFI_DEFAULT;
 -		} else if (!strcmp(str, "off")) {
 -			cfi_mode = CFI_OFF;
 -			cfi_rand = false;
 -		} else if (!strcmp(str, "kcfi")) {
 -			cfi_mode = CFI_KCFI;
 -		} else if (!strcmp(str, "fineibt")) {
 -			cfi_mode = CFI_FINEIBT;
 -		} else if (!strcmp(str, "norand")) {
 -			cfi_rand = false;
 -		} else {
 -			pr_err("Ignoring unknown cfi option (%s).", str);
 -		}
 -
 -		str = next;
 -	}
 -
 -	return 0;
 -}
 -early_param("cfi", cfi_parse_cmdline);
 -
 -/*
 - * kCFI						FineIBT
 - *
 - * __cfi_\func:					__cfi_\func:
 - *	movl   $0x12345678,%eax		// 5	     endbr64			// 4
 - *	nop					     subl   $0x12345678,%r10d   // 7
 - *	nop					     jz     1f			// 2
 - *	nop					     ud2			// 2
 - *	nop					1:   nop			// 1
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *	nop
 - *
 - *
 - * caller:					caller:
 - *	movl	$(-0x12345678),%r10d	 // 6	     movl   $0x12345678,%r10d	// 6
 - *	addl	$-15(%r11),%r10d	 // 4	     sub    $16,%r11		// 4
 - *	je	1f			 // 2	     nop4			// 4
 - *	ud2				 // 2
 - * 1:	call	__x86_indirect_thunk_r11 // 5	     call   *%r11; nop2;	// 5
 - *
 - */
 -
 -asm(	".pushsection .rodata			\n"
 -	"fineibt_preamble_start:		\n"
 -	"	endbr64				\n"
 -	"	subl	$0x12345678, %r10d	\n"
 -	"	je	fineibt_preamble_end	\n"
 -	"	ud2				\n"
 -	"	nop				\n"
 -	"fineibt_preamble_end:			\n"
 -	".popsection\n"
 -);
 -
 -extern u8 fineibt_preamble_start[];
 -extern u8 fineibt_preamble_end[];
 -
 -#define fineibt_preamble_size (fineibt_preamble_end - fineibt_preamble_start)
 -#define fineibt_preamble_hash 7
 -
 -asm(	".pushsection .rodata			\n"
 -	"fineibt_caller_start:			\n"
 -	"	movl	$0x12345678, %r10d	\n"
 -	"	sub	$16, %r11		\n"
 -	ASM_NOP4
 -	"fineibt_caller_end:			\n"
 -	".popsection				\n"
 -);
 -
 -extern u8 fineibt_caller_start[];
 -extern u8 fineibt_caller_end[];
 -
 -#define fineibt_caller_size (fineibt_caller_end - fineibt_caller_start)
 -#define fineibt_caller_hash 2
 -
 -#define fineibt_caller_jmp (fineibt_caller_size - 2)
 -
 -static u32 decode_preamble_hash(void *addr)
 -{
 -	u8 *p = addr;
 -
 -	/* b8 78 56 34 12          mov    $0x12345678,%eax */
 -	if (p[0] == 0xb8)
 -		return *(u32 *)(addr + 1);
 -
 -	return 0; /* invalid hash value */
 -}
 -
 -static u32 decode_caller_hash(void *addr)
 -{
 -	u8 *p = addr;
 -
 -	/* 41 ba 78 56 34 12       mov    $0x12345678,%r10d */
 -	if (p[0] == 0x41 && p[1] == 0xba)
 -		return -*(u32 *)(addr + 2);
 -
 -	/* e8 0c 78 56 34 12	   jmp.d8  +12 */
 -	if (p[0] == JMP8_INSN_OPCODE && p[1] == fineibt_caller_jmp)
 -		return -*(u32 *)(addr + 2);
 -
 -	return 0; /* invalid hash value */
 -}
 -
 -/* .retpoline_sites */
 -static int cfi_disable_callers(s32 *start, s32 *end)
 -{
 -	/*
 -	 * Disable kCFI by patching in a JMP.d8, this leaves the hash immediate
 -	 * in tact for later usage. Also see decode_caller_hash() and
 -	 * cfi_rewrite_callers().
 -	 */
 -	const u8 jmp[] = { JMP8_INSN_OPCODE, fineibt_caller_jmp };
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (!hash) /* nocfi callers */
 -			continue;
 -
 -		text_poke_early(addr, jmp, 2);
 -	}
 -
 -	return 0;
 -}
 -
 -static int cfi_enable_callers(s32 *start, s32 *end)
 -{
 -	/*
 -	 * Re-enable kCFI, undo what cfi_disable_callers() did.
 -	 */
 -	const u8 mov[] = { 0x41, 0xba };
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (!hash) /* nocfi callers */
 -			continue;
 -
 -		text_poke_early(addr, mov, 2);
 -	}
 -
 -	return 0;
 -}
 -
 -/* .cfi_sites */
 -static int cfi_rand_preamble(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		hash = decode_preamble_hash(addr);
 -		if (WARN(!hash, "no CFI hash found at: %pS %px %*ph\n",
 -			 addr, addr, 5, addr))
 -			return -EINVAL;
 -
 -		hash = cfi_rehash(hash);
 -		text_poke_early(addr + 1, &hash, 4);
 -	}
 -
 -	return 0;
 -}
 -
 -static int cfi_rewrite_preamble(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		hash = decode_preamble_hash(addr);
 -		if (WARN(!hash, "no CFI hash found at: %pS %px %*ph\n",
 -			 addr, addr, 5, addr))
 -			return -EINVAL;
 -
 -		text_poke_early(addr, fineibt_preamble_start, fineibt_preamble_size);
 -		WARN_ON(*(u32 *)(addr + fineibt_preamble_hash) != 0x12345678);
 -		text_poke_early(addr + fineibt_preamble_hash, &hash, 4);
 -	}
 -
 -	return 0;
 -}
 -
 -static void cfi_rewrite_endbr(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -
 -		poison_endbr(addr+16, false);
 -	}
 -}
 -
 -/* .retpoline_sites */
 -static int cfi_rand_callers(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (hash) {
 -			hash = -cfi_rehash(hash);
 -			text_poke_early(addr + 2, &hash, 4);
 -		}
 -	}
 -
 -	return 0;
 -}
 -
 -static int cfi_rewrite_callers(s32 *start, s32 *end)
 -{
 -	s32 *s;
 -
 -	for (s = start; s < end; s++) {
 -		void *addr = (void *)s + *s;
 -		u32 hash;
 -
 -		addr -= fineibt_caller_size;
 -		hash = decode_caller_hash(addr);
 -		if (hash) {
 -			text_poke_early(addr, fineibt_caller_start, fineibt_caller_size);
 -			WARN_ON(*(u32 *)(addr + fineibt_caller_hash) != 0x12345678);
 -			text_poke_early(addr + fineibt_caller_hash, &hash, 4);
 -		}
 -		/* rely on apply_retpolines() */
 -	}
 -
 -	return 0;
 -}
 -
 -static void __apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,
 -			    s32 *start_cfi, s32 *end_cfi, bool builtin)
 -{
 -	int ret;
 -
 -	if (WARN_ONCE(fineibt_preamble_size != 16,
 -		      "FineIBT preamble wrong size: %ld", fineibt_preamble_size))
 -		return;
 -
 -	if (cfi_mode == CFI_DEFAULT) {
 -		cfi_mode = CFI_KCFI;
 -		if (HAS_KERNEL_IBT && cpu_feature_enabled(X86_FEATURE_IBT))
 -			cfi_mode = CFI_FINEIBT;
 -	}
 -
 -	/*
 -	 * Rewrite the callers to not use the __cfi_ stubs, such that we might
 -	 * rewrite them. This disables all CFI. If this succeeds but any of the
 -	 * later stages fails, we're without CFI.
 -	 */
 -	ret = cfi_disable_callers(start_retpoline, end_retpoline);
 -	if (ret)
 -		goto err;
 -
 -	if (cfi_rand) {
 -		if (builtin)
 -			cfi_seed = get_random_u32();
 -
 -		ret = cfi_rand_preamble(start_cfi, end_cfi);
 -		if (ret)
 -			goto err;
 -
 -		ret = cfi_rand_callers(start_retpoline, end_retpoline);
 -		if (ret)
 -			goto err;
 -	}
 -
 -	switch (cfi_mode) {
 -	case CFI_OFF:
 -		if (builtin)
 -			pr_info("Disabling CFI\n");
 -		return;
 -
 -	case CFI_KCFI:
 -		ret = cfi_enable_callers(start_retpoline, end_retpoline);
 -		if (ret)
 -			goto err;
 -
 -		if (builtin)
 -			pr_info("Using kCFI\n");
 -		return;
 -
 -	case CFI_FINEIBT:
 -		/* place the FineIBT preamble at func()-16 */
 -		ret = cfi_rewrite_preamble(start_cfi, end_cfi);
 -		if (ret)
 -			goto err;
 -
 -		/* rewrite the callers to target func()-16 */
 -		ret = cfi_rewrite_callers(start_retpoline, end_retpoline);
 -		if (ret)
 -			goto err;
 -
 -		/* now that nobody targets func()+0, remove ENDBR there */
 -		cfi_rewrite_endbr(start_cfi, end_cfi);
 -
 -		if (builtin)
 -			pr_info("Using FineIBT CFI\n");
 -		return;
++<<<<<<< HEAD
 +	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
 +		return -1;
  
 -	default:
 -		break;
 +	bytes[i++] = RET_INSN_OPCODE;
++=======
++	/* Patch the custom return thunks... */
++	if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
++		i = JMP32_INSN_SIZE;
++		__text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);
++	} else {
++		/* ... or patch them out if not needed. */
++		bytes[i++] = RET_INSN_OPCODE;
+ 	}
++>>>>>>> 238ec850b95a (x86/srso: Fix return thunks in generated code)
  
 -err:
 -	pr_err("Something went horribly wrong trying to rewrite the CFI implementation.\n");
 -}
 +	for (; i < insn->length;)
 +		bytes[i++] = INT3_INSN_OPCODE;
  
 -static inline void poison_hash(void *addr)
 -{
 -	*(u32 *)addr = 0;
 +	return i;
  }
  
 -static void poison_cfi(void *addr)
 +void __init_or_module noinline apply_returns(s32 *start, s32 *end)
  {
 -	switch (cfi_mode) {
 -	case CFI_FINEIBT:
 -		/*
 -		 * __cfi_\func:
 -		 *	osp nopl (%rax)
 -		 *	subl	$0, %r10d
 -		 *	jz	1f
 -		 *	ud2
 -		 * 1:	nop
 -		 */
 -		poison_endbr(addr, false);
 -		poison_hash(addr + fineibt_preamble_hash);
 -		break;
 -
 -	case CFI_KCFI:
 -		/*
 -		 * __cfi_\func:
 -		 *	movl	$0, %eax
 -		 *	.skip	11, 0x90
 -		 */
 -		poison_hash(addr + 1);
 -		break;
 -
 -	default:
 -		break;
 -	}
 -}
 +	s32 *s;
  
 -#else
 +	for (s = start; s < end; s++) {
 +		void *addr = (void *)s + *s;
 +		struct insn insn;
 +		int len, ret;
 +		u8 bytes[16];
 +		u8 op1;
  
 -static void __apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,
 -			    s32 *start_cfi, s32 *end_cfi, bool builtin)
 -{
 -}
 +		ret = insn_decode_kernel(&insn, addr);
 +		if (WARN_ON_ONCE(ret < 0))
 +			continue;
  
 -#ifdef CONFIG_X86_KERNEL_IBT
 -static void poison_cfi(void *addr) { }
 -#endif
 +		op1 = insn.opcode.bytes[0];
 +		if (WARN_ON_ONCE(op1 != JMP32_INSN_OPCODE))
 +			continue;
  
 -#endif
 +		DPRINTK("return thunk at: %pS (%px) len: %d to: %pS",
 +			addr, addr, insn.length,
 +			addr + insn.length + insn.immediate.value);
  
 -void apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,
 -		   s32 *start_cfi, s32 *end_cfi)
 -{
 -	return __apply_fineibt(start_retpoline, end_retpoline,
 -			       start_cfi, end_cfi,
 -			       /* .builtin = */ false);
 +		len = patch_return(addr, &insn, bytes);
 +		if (len == insn.length) {
 +			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
 +			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
 +			text_poke_early(addr, bytes, len);
 +		}
 +	}
  }
 +#else
 +void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 +#endif /* CONFIG_RETHUNK */
  
  #ifdef CONFIG_SMP
  static void alternatives_smp_lock(const s32 *start, const s32 *end,
diff --cc arch/x86/kernel/cpu/bugs.c
index adea0002a1a8,7314a6bdc862..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -2309,6 -2191,166 +2309,169 @@@ static int __init l1tf_cmdline(char *st
  early_param("l1tf", l1tf_cmdline);
  
  #undef pr_fmt
++<<<<<<< HEAD
++=======
+ #define pr_fmt(fmt)	"Speculative Return Stack Overflow: " fmt
+ 
+ enum srso_mitigation {
+ 	SRSO_MITIGATION_NONE,
+ 	SRSO_MITIGATION_MICROCODE,
+ 	SRSO_MITIGATION_SAFE_RET,
+ 	SRSO_MITIGATION_IBPB,
+ 	SRSO_MITIGATION_IBPB_ON_VMEXIT,
+ };
+ 
+ enum srso_mitigation_cmd {
+ 	SRSO_CMD_OFF,
+ 	SRSO_CMD_MICROCODE,
+ 	SRSO_CMD_SAFE_RET,
+ 	SRSO_CMD_IBPB,
+ 	SRSO_CMD_IBPB_ON_VMEXIT,
+ };
+ 
+ static const char * const srso_strings[] = {
+ 	[SRSO_MITIGATION_NONE]           = "Vulnerable",
+ 	[SRSO_MITIGATION_MICROCODE]      = "Mitigation: microcode",
+ 	[SRSO_MITIGATION_SAFE_RET]	 = "Mitigation: safe RET",
+ 	[SRSO_MITIGATION_IBPB]		 = "Mitigation: IBPB",
+ 	[SRSO_MITIGATION_IBPB_ON_VMEXIT] = "Mitigation: IBPB on VMEXIT only"
+ };
+ 
+ static enum srso_mitigation srso_mitigation __ro_after_init = SRSO_MITIGATION_NONE;
+ static enum srso_mitigation_cmd srso_cmd __ro_after_init = SRSO_CMD_SAFE_RET;
+ 
+ static int __init srso_parse_cmdline(char *str)
+ {
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	if (!strcmp(str, "off"))
+ 		srso_cmd = SRSO_CMD_OFF;
+ 	else if (!strcmp(str, "microcode"))
+ 		srso_cmd = SRSO_CMD_MICROCODE;
+ 	else if (!strcmp(str, "safe-ret"))
+ 		srso_cmd = SRSO_CMD_SAFE_RET;
+ 	else if (!strcmp(str, "ibpb"))
+ 		srso_cmd = SRSO_CMD_IBPB;
+ 	else if (!strcmp(str, "ibpb-vmexit"))
+ 		srso_cmd = SRSO_CMD_IBPB_ON_VMEXIT;
+ 	else
+ 		pr_err("Ignoring unknown SRSO option (%s).", str);
+ 
+ 	return 0;
+ }
+ early_param("spec_rstack_overflow", srso_parse_cmdline);
+ 
+ #define SRSO_NOTICE "WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options."
+ 
+ static void __init srso_select_mitigation(void)
+ {
+ 	bool has_microcode;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_SRSO) || cpu_mitigations_off())
+ 		goto pred_cmd;
+ 
+ 	/*
+ 	 * The first check is for the kernel running as a guest in order
+ 	 * for guests to verify whether IBPB is a viable mitigation.
+ 	 */
+ 	has_microcode = boot_cpu_has(X86_FEATURE_IBPB_BRTYPE) || cpu_has_ibpb_brtype_microcode();
+ 	if (!has_microcode) {
+ 		pr_warn("IBPB-extending microcode not applied!\n");
+ 		pr_warn(SRSO_NOTICE);
+ 	} else {
+ 		/*
+ 		 * Enable the synthetic (even if in a real CPUID leaf)
+ 		 * flags for guests.
+ 		 */
+ 		setup_force_cpu_cap(X86_FEATURE_IBPB_BRTYPE);
+ 		setup_force_cpu_cap(X86_FEATURE_SBPB);
+ 
+ 		/*
+ 		 * Zen1/2 with SMT off aren't vulnerable after the right
+ 		 * IBPB microcode has been applied.
+ 		 */
+ 		if ((boot_cpu_data.x86 < 0x19) &&
+ 		    (cpu_smt_control == CPU_SMT_DISABLED))
+ 			setup_force_cpu_cap(X86_FEATURE_SRSO_NO);
+ 	}
+ 
+ 	if (retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {
+ 		if (has_microcode) {
+ 			pr_err("Retbleed IBPB mitigation enabled, using same for SRSO\n");
+ 			srso_mitigation = SRSO_MITIGATION_IBPB;
+ 			goto pred_cmd;
+ 		}
+ 	}
+ 
+ 	switch (srso_cmd) {
+ 	case SRSO_CMD_OFF:
+ 		return;
+ 
+ 	case SRSO_CMD_MICROCODE:
+ 		if (has_microcode) {
+ 			srso_mitigation = SRSO_MITIGATION_MICROCODE;
+ 			pr_warn(SRSO_NOTICE);
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_SAFE_RET:
+ 		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
+ 			/*
+ 			 * Enable the return thunk for generated code
+ 			 * like ftrace, static_call, etc.
+ 			 */
+ 			setup_force_cpu_cap(X86_FEATURE_RETHUNK);
+ 
+ 			if (boot_cpu_data.x86 == 0x19)
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO_ALIAS);
+ 			else
+ 				setup_force_cpu_cap(X86_FEATURE_SRSO);
+ 			srso_mitigation = SRSO_MITIGATION_SAFE_RET;
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
+ 			goto pred_cmd;
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_IBPB:
+ 		if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY)) {
+ 			if (has_microcode) {
+ 				setup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);
+ 				srso_mitigation = SRSO_MITIGATION_IBPB;
+ 			}
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_IBPB_ENTRY.\n");
+ 			goto pred_cmd;
+ 		}
+ 		break;
+ 
+ 	case SRSO_CMD_IBPB_ON_VMEXIT:
+ 		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
+ 			if (!boot_cpu_has(X86_FEATURE_ENTRY_IBPB) && has_microcode) {
+ 				setup_force_cpu_cap(X86_FEATURE_IBPB_ON_VMEXIT);
+ 				srso_mitigation = SRSO_MITIGATION_IBPB_ON_VMEXIT;
+ 			}
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
+ 			goto pred_cmd;
+                 }
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ 
+ 	pr_info("%s%s\n", srso_strings[srso_mitigation], (has_microcode ? "" : ", no microcode"));
+ 
+ pred_cmd:
+ 	if (boot_cpu_has(X86_FEATURE_SRSO_NO) ||
+ 	    srso_cmd == SRSO_CMD_OFF)
+ 		x86_pred_cmd = PRED_CMD_SBPB;
+ }
+ 
+ #undef pr_fmt
++>>>>>>> 238ec850b95a (x86/srso: Fix return thunks in generated code)
  #define pr_fmt(fmt) fmt
  
  #ifdef CONFIG_SYSFS
* Unmerged path arch/x86/kernel/alternative.c
* Unmerged path arch/x86/kernel/cpu/bugs.c
