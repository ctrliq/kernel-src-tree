x86/srso: Unexport untraining functions

jira LE-1907
cve CVE-2023-20569
Rebuild_History Non-Buildable kernel-4.18.0-521.el8
commit-author Josh Poimboeuf <jpoimboe@kernel.org>
commit eb54be26b0d25222809b16f335fe13756ff4a206
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-521.el8/eb54be26.failed

These functions aren't called outside of retpoline.S.

	Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
	Acked-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/1ae080f95ce7266c82cba6d2adde82349b832654.1693889988.git.jpoimboe@kernel.org
(cherry picked from commit eb54be26b0d25222809b16f335fe13756ff4a206)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/lib/retpoline.S
diff --cc arch/x86/include/asm/nospec-branch.h
index 53e56fc9cf70,6c14fd1f5912..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -201,14 -334,72 +201,26 @@@
  #define ANNOTATE_RETPOLINE_SAFE					\
  	"999:\n\t"						\
  	".pushsection .discard.retpoline_safe\n\t"		\
 -	".long 999b - .\n\t"					\
 +	_ASM_PTR " 999b\n\t"					\
  	".popsection\n\t"
  
 -typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
 -extern retpoline_thunk_t __x86_indirect_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
 -extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];
 -
 -#ifdef CONFIG_RETHUNK
  extern void __x86_return_thunk(void);
++<<<<<<< HEAD
 +extern void zen_untrain_ret(void);
++=======
+ #else
+ static inline void __x86_return_thunk(void) {}
+ #endif
+ 
+ extern void retbleed_return_thunk(void);
+ extern void srso_return_thunk(void);
+ extern void srso_alias_return_thunk(void);
+ 
+ extern void entry_untrain_ret(void);
++>>>>>>> eb54be26b0d2 (x86/srso: Unexport untraining functions)
  extern void entry_ibpb(void);
  
 -extern void (*x86_return_thunk)(void);
 -
 -#ifdef CONFIG_CALL_DEPTH_TRACKING
 -extern void __x86_return_skl(void);
 -
 -static inline void x86_set_skl_return_thunk(void)
 -{
 -	x86_return_thunk = &__x86_return_skl;
 -}
 -
 -#define CALL_DEPTH_ACCOUNT					\
 -	ALTERNATIVE("",						\
 -		    __stringify(INCREMENT_CALL_DEPTH),		\
 -		    X86_FEATURE_CALL_DEPTH)
 -
 -#ifdef CONFIG_CALL_THUNKS_DEBUG
 -DECLARE_PER_CPU(u64, __x86_call_count);
 -DECLARE_PER_CPU(u64, __x86_ret_count);
 -DECLARE_PER_CPU(u64, __x86_stuffs_count);
 -DECLARE_PER_CPU(u64, __x86_ctxsw_count);
 -#endif
 -#else
 -static inline void x86_set_skl_return_thunk(void) {}
 -
 -#define CALL_DEPTH_ACCOUNT ""
 -
 -#endif
 -
  #ifdef CONFIG_RETPOLINE
 -
 -#define GEN(reg) \
 -	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_call_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
 -#define GEN(reg)						\
 -	extern retpoline_thunk_t __x86_indirect_jump_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
  #ifdef CONFIG_X86_64
  
  /*
diff --cc arch/x86/lib/retpoline.S
index 72a7235229fd,a40ba18610d8..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -53,8 -132,58 +53,45 @@@ GENERATE_THUNK(r15
   */
  #ifdef CONFIG_RETHUNK
  
 -/*
 - * srso_alias_untrain_ret() and srso_alias_safe_ret() are placed at
 - * special addresses:
 - *
 - * - srso_alias_untrain_ret() is 2M aligned
 - * - srso_alias_safe_ret() is also in the same 2M page but bits 2, 8, 14
 - * and 20 in its virtual address are set (while those bits in the
 - * srso_alias_untrain_ret() function are cleared).
 - *
 - * This guarantees that those two addresses will alias in the branch
 - * target buffer of Zen3/4 generations, leading to any potential
 - * poisoned entries at that BTB slot to get evicted.
 - *
 - * As a result, srso_alias_safe_ret() becomes a safe return.
 - */
 -#ifdef CONFIG_CPU_SRSO
 -	.section .text..__x86.rethunk_untrain
 +	.section .text.__x86.return_thunk
  
++<<<<<<< HEAD
++=======
+ SYM_START(srso_alias_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ASM_NOP2
+ 	lfence
+ 	jmp srso_alias_return_thunk
+ SYM_FUNC_END(srso_alias_untrain_ret)
+ 
+ 	.section .text..__x86.rethunk_safe
+ #else
+ /* dummy definition for alternatives */
+ SYM_START(srso_alias_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_alias_untrain_ret)
+ #endif
+ 
+ SYM_START(srso_alias_safe_ret, SYM_L_GLOBAL, SYM_A_NONE)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_FUNC_END(srso_alias_safe_ret)
+ 
+ SYM_CODE_START_NOALIGN(srso_alias_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_alias_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_alias_return_thunk)
+ 
+ 	.section .text..__x86.return_thunk
++>>>>>>> eb54be26b0d2 (x86/srso: Unexport untraining functions)
  /*
   * Some generic notes on the untraining sequences:
   *
@@@ -84,11 -213,11 +121,16 @@@
   *    from re-poisioning the BTB prediction.
   */
  	.align 64
++<<<<<<< HEAD
 +	.skip 63, 0xcc
 +SYM_START(zen_untrain_ret, SYM_L_GLOBAL, SYM_A_NONE)
++=======
+ 	.skip 64 - (retbleed_return_thunk - retbleed_untrain_ret), 0xcc
+ SYM_START(retbleed_untrain_ret, SYM_L_LOCAL, SYM_A_NONE)
++>>>>>>> eb54be26b0d2 (x86/srso: Unexport untraining functions)
  	ANNOTATE_NOENDBR
  	/*
 -	 * As executed from retbleed_untrain_ret, this is:
 +	 * As executed from zen_untrain_ret, this is:
  	 *
  	 *   TEST $0xcc, %bl
  	 *   LFENCE
@@@ -130,11 -259,65 +172,71 @@@ SYM_CODE_END(__x86_return_thunk
  	 * Jump back and execute the RET in the middle of the TEST instruction.
  	 * INT3 is for SLS protection.
  	 */
 -	jmp retbleed_return_thunk
 +	jmp __x86_return_thunk
  	int3
++<<<<<<< HEAD
 +SYM_FUNC_END(zen_untrain_ret)
 +__EXPORT_THUNK(zen_untrain_ret)
 +
++=======
+ SYM_FUNC_END(retbleed_untrain_ret)
+ 
+ /*
+  * SRSO untraining sequence for Zen1/2, similar to retbleed_untrain_ret()
+  * above. On kernel entry, srso_untrain_ret() is executed which is a
+  *
+  * movabs $0xccccc30824648d48,%rax
+  *
+  * and when the return thunk executes the inner label srso_safe_ret()
+  * later, it is a stack manipulation and a RET which is mispredicted and
+  * thus a "safe" one to use.
+  */
+ 	.align 64
+ 	.skip 64 - (srso_safe_ret - srso_untrain_ret), 0xcc
+ SYM_START(srso_untrain_ret, SYM_L_LOCAL, SYM_A_NONE)
+ 	ANNOTATE_NOENDBR
+ 	.byte 0x48, 0xb8
+ 
+ /*
+  * This forces the function return instruction to speculate into a trap
+  * (UD2 in srso_return_thunk() below).  This RET will then mispredict
+  * and execution will continue at the return site read from the top of
+  * the stack.
+  */
+ SYM_INNER_LABEL(srso_safe_ret, SYM_L_GLOBAL)
+ 	lea 8(%_ASM_SP), %_ASM_SP
+ 	ret
+ 	int3
+ 	int3
+ 	/* end of movabs */
+ 	lfence
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_safe_ret)
+ SYM_FUNC_END(srso_untrain_ret)
+ 
+ SYM_CODE_START(srso_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	call srso_safe_ret
+ 	ud2
+ SYM_CODE_END(srso_return_thunk)
+ 
+ SYM_FUNC_START(entry_untrain_ret)
+ 	ALTERNATIVE_2 "jmp retbleed_untrain_ret", \
+ 		      "jmp srso_untrain_ret", X86_FEATURE_SRSO, \
+ 		      "jmp srso_alias_untrain_ret", X86_FEATURE_SRSO_ALIAS
+ SYM_FUNC_END(entry_untrain_ret)
+ __EXPORT_THUNK(entry_untrain_ret)
+ 
+ SYM_CODE_START(__x86_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
++>>>>>>> eb54be26b0d2 (x86/srso: Unexport untraining functions)
  EXPORT_SYMBOL(__x86_return_thunk)
  
  #endif /* CONFIG_RETHUNK */
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/lib/retpoline.S
