block: remove some blk_mq_hw_ctx debugfs entries

jira LE-4559
Rebuild_History Non-Buildable kernel-4.18.0-553.80.1.el8_10
commit-author Jens Axboe <axboe@kernel.dk>
commit afd7de03c5268f74202c1dd4780a8532a11f4c6b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.80.1.el8_10/afd7de03.failed

Just like the blk_mq_ctx counterparts, we've got a bunch of counters
in here that are only for debugfs and are of questionnable value. They
are:

- dispatched, index of how many requests were dispatched in one go

- poll_{considered,invoked,success}, which track poll sucess rates. We're
  confident in the iopoll implementation at this point, don't bother
  tracking these.

As a bonus, this shrinks each hardware queue from 576 bytes to 512 bytes,
dropping a whole cacheline.

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit afd7de03c5268f74202c1dd4780a8532a11f4c6b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	include/linux/blk-mq.h
diff --cc block/blk-mq.c
index 97348b138e4c,bd241fd7ee49..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1164,17 -1300,9 +1163,9 @@@ struct request *blk_mq_dequeue_from_ctx
  	return data.rq;
  }
  
- static inline unsigned int queued_to_index(unsigned int queued)
- {
- 	if (!queued)
- 		return 0;
- 
- 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
- }
- 
  static bool __blk_mq_get_driver_tag(struct request *rq)
  {
 -	struct sbitmap_queue *bt = &rq->mq_hctx->tags->bitmap_tags;
 +	struct sbitmap_queue *bt = rq->mq_hctx->tags->bitmap_tags;
  	unsigned int tag_offset = rq->mq_hctx->tags->nr_reserved_tags;
  	int tag;
  
@@@ -4050,79 -4174,24 +4039,82 @@@ static bool blk_mq_poll_hybrid_sleep(st
  	return true;
  }
  
 -static int blk_mq_poll_classic(struct request_queue *q, blk_qc_t cookie,
 -		unsigned int flags)
 +static bool blk_mq_poll_hybrid(struct request_queue *q,
 +			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
  {
 -	struct blk_mq_hw_ctx *hctx = blk_qc_to_hctx(q, cookie);
 -	long state = get_current_state();
 -	int ret;
 +	struct request *rq;
 +
 +	if (q->poll_nsec == BLK_MQ_POLL_CLASSIC)
 +		return false;
 +
 +	if (!blk_qc_t_is_internal(cookie))
 +		rq = blk_mq_tag_to_rq(hctx->tags, blk_qc_t_to_tag(cookie));
 +	else {
 +		rq = blk_mq_tag_to_rq(hctx->sched_tags, blk_qc_t_to_tag(cookie));
 +		/*
 +		 * With scheduling, if the request has completed, we'll
 +		 * get a NULL return here, as we clear the sched tag when
 +		 * that happens. The request still remains valid, like always,
 +		 * so we should be safe with just the NULL check.
 +		 */
 +		if (!rq)
 +			return false;
 +	}
 +
 +	return blk_mq_poll_hybrid_sleep(q, rq);
 +}
 +
 +/**
 + * blk_poll - poll for IO completions
 + * @q:  the queue
 + * @cookie: cookie passed back at IO submission time
 + * @spin: whether to spin for completions
 + *
 + * Description:
 + *    Poll for completions on the passed in queue. Returns number of
 + *    completed entries found. If @spin is true, then blk_poll will continue
 + *    looping until at least one completion is found, unless the task is
 + *    otherwise marked running (or we need to reschedule).
 + */
 +int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 +{
 +	struct blk_mq_hw_ctx *hctx;
 +	unsigned int state;
 +
 +	if (!blk_qc_t_valid(cookie) ||
 +	    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
 +		return 0;
 +
 +	if (current->plug)
 +		blk_flush_plug_list(current->plug, false);
 +
 +	hctx = q->queue_hw_ctx[blk_qc_t_to_queue_num(cookie)];
 +
 +	/*
 +	 * If we sleep, have the caller restart the poll loop to reset
 +	 * the state. Like for the other success return cases, the
 +	 * caller is responsible for checking if the IO completed. If
 +	 * the IO isn't complete, we'll get called again and will go
 +	 * straight to the busy poll loop. If specified not to spin,
 +	 * we also should not sleep.
 +	 */
 +	if (spin && blk_mq_poll_hybrid(q, hctx, cookie))
 +		return 1;
 +
++<<<<<<< HEAD
 +	hctx->poll_considered++;
  
 +	state = get_current_state();
  	do {
 +		int ret;
 +
 +		hctx->poll_invoked++;
 +
++=======
++	do {
++>>>>>>> afd7de03c526 (block: remove some blk_mq_hw_ctx debugfs entries)
  		ret = q->mq_ops->poll(hctx);
  		if (ret > 0) {
- 			hctx->poll_success++;
  			__set_current_state(TASK_RUNNING);
  			return ret;
  		}
diff --cc include/linux/blk-mq.h
index 40aee1dbb6d3,9fb8618fb957..000000000000
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@@ -15,60 -231,142 +15,66 @@@ struct blk_flush_queue
   */
  struct blk_mq_hw_ctx {
  	struct {
 -		/** @lock: Protects the dispatch list. */
  		spinlock_t		lock;
 -		/**
 -		 * @dispatch: Used for requests that are ready to be
 -		 * dispatched to the hardware but for some reason (e.g. lack of
 -		 * resources) could not be sent to the hardware. As soon as the
 -		 * driver can send new requests, requests at this list will
 -		 * be sent first for a fairer dispatch.
 -		 */
  		struct list_head	dispatch;
 -		 /**
 -		  * @state: BLK_MQ_S_* flags. Defines the state of the hw
 -		  * queue (active, scheduled to restart, stopped).
 -		  */
 -		unsigned long		state;
 +		unsigned long		state;		/* BLK_MQ_S_* flags */
  	} ____cacheline_aligned_in_smp;
  
 -	/**
 -	 * @run_work: Used for scheduling a hardware queue run at a later time.
 -	 */
  	struct delayed_work	run_work;
 -	/** @cpumask: Map of available CPUs where this hctx can run. */
  	cpumask_var_t		cpumask;
 -	/**
 -	 * @next_cpu: Used by blk_mq_hctx_next_cpu() for round-robin CPU
 -	 * selection from @cpumask.
 -	 */
  	int			next_cpu;
 -	/**
 -	 * @next_cpu_batch: Counter of how many works left in the batch before
 -	 * changing to the next CPU.
 -	 */
  	int			next_cpu_batch;
  
 -	/** @flags: BLK_MQ_F_* flags. Defines the behaviour of the queue. */
 -	unsigned long		flags;
 +	unsigned long		flags;		/* BLK_MQ_F_* flags */
  
 -	/**
 -	 * @sched_data: Pointer owned by the IO scheduler attached to a request
 -	 * queue. It's up to the IO scheduler how to use this pointer.
 -	 */
  	void			*sched_data;
 -	/**
 -	 * @queue: Pointer to the request queue that owns this hardware context.
 -	 */
  	struct request_queue	*queue;
 -	/** @fq: Queue of requests that need to perform a flush operation. */
  	struct blk_flush_queue	*fq;
  
 -	/**
 -	 * @driver_data: Pointer to data owned by the block driver that created
 -	 * this hctx
 -	 */
  	void			*driver_data;
  
 -	/**
 -	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
 -	 * pending request in that software queue.
 -	 */
  	struct sbitmap		ctx_map;
  
 -	/**
 -	 * @dispatch_from: Software queue to be used when no scheduler was
 -	 * selected.
 -	 */
  	struct blk_mq_ctx	*dispatch_from;
 -	/**
 -	 * @dispatch_busy: Number used by blk_mq_update_dispatch_busy() to
 -	 * decide if the hw_queue is busy using Exponential Weighted Moving
 -	 * Average algorithm.
 -	 */
  	unsigned int		dispatch_busy;
  
 -	/** @type: HCTX_TYPE_* flags. Type of hardware queue. */
  	unsigned short		type;
 -	/** @nr_ctx: Number of software queues. */
  	unsigned short		nr_ctx;
 -	/** @ctxs: Array of software queues. */
  	struct blk_mq_ctx	**ctxs;
  
 -	/** @dispatch_wait_lock: Lock for dispatch_wait queue. */
  	spinlock_t		dispatch_wait_lock;
 -	/**
 -	 * @dispatch_wait: Waitqueue to put requests when there is no tag
 -	 * available at the moment, to wait for another try in the future.
 -	 */
  	wait_queue_entry_t	dispatch_wait;
 -
 -	/**
 -	 * @wait_index: Index of next available dispatch_wait queue to insert
 -	 * requests.
 -	 */
  	atomic_t		wait_index;
  
 -	/**
 -	 * @tags: Tags owned by the block driver. A tag at this set is only
 -	 * assigned when a request is dispatched from a hardware queue.
 -	 */
  	struct blk_mq_tags	*tags;
 -	/**
 -	 * @sched_tags: Tags owned by I/O scheduler. If there is an I/O
 -	 * scheduler associated with a request queue, a tag is assigned when
 -	 * that request is allocated. Else, this member is not used.
 -	 */
  	struct blk_mq_tags	*sched_tags;
  
 -	/** @queued: Number of queued requests. */
  	unsigned long		queued;
 -	/** @run: Number of dispatched requests. */
  	unsigned long		run;
++<<<<<<< HEAD
 +#define BLK_MQ_MAX_DISPATCH_ORDER	7
 +	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
++=======
++>>>>>>> afd7de03c526 (block: remove some blk_mq_hw_ctx debugfs entries)
  
 -	/** @numa_node: NUMA node the storage adapter has been connected to. */
  	unsigned int		numa_node;
 -	/** @queue_num: Index of this hardware queue. */
  	unsigned int		queue_num;
  
 -	/**
 -	 * @nr_active: Number of active requests. Only used when a tag set is
 -	 * shared across request queues.
 -	 */
  	atomic_t		nr_active;
 +	RH_KABI_DEPRECATE(unsigned int,	nr_expired)
  
 -	/** @cpuhp_online: List to store request if CPU is going to die */
 -	struct hlist_node	cpuhp_online;
 -	/** @cpuhp_dead: List to store request if some CPU die. */
  	struct hlist_node	cpuhp_dead;
 -	/** @kobj: Kernel object for sysfs. */
  	struct kobject		kobj;
  
++<<<<<<< HEAD
 +	unsigned long		poll_considered;
 +	unsigned long		poll_invoked;
 +	unsigned long		poll_success;
 +
++=======
++>>>>>>> afd7de03c526 (block: remove some blk_mq_hw_ctx debugfs entries)
  #ifdef CONFIG_BLK_DEBUG_FS
 -	/**
 -	 * @debugfs_dir: debugfs directory for this hardware queue. Named
 -	 * as cpu<cpu_number>.
 -	 */
  	struct dentry		*debugfs_dir;
 -	/** @sched_debugfs_dir:	debugfs directory for the scheduler. */
  	struct dentry		*sched_debugfs_dir;
  #endif
  
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index 249cccf5675b..2ca326c94483 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -540,70 +540,6 @@ static int hctx_sched_tags_bitmap_show(void *data, struct seq_file *m)
 	return res;
 }
 
-static int hctx_io_poll_show(void *data, struct seq_file *m)
-{
-	struct blk_mq_hw_ctx *hctx = data;
-
-	seq_printf(m, "considered=%lu\n", hctx->poll_considered);
-	seq_printf(m, "invoked=%lu\n", hctx->poll_invoked);
-	seq_printf(m, "success=%lu\n", hctx->poll_success);
-	return 0;
-}
-
-static ssize_t hctx_io_poll_write(void *data, const char __user *buf,
-				  size_t count, loff_t *ppos)
-{
-	struct blk_mq_hw_ctx *hctx = data;
-
-	hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
-	return count;
-}
-
-static int hctx_dispatched_show(void *data, struct seq_file *m)
-{
-	struct blk_mq_hw_ctx *hctx = data;
-	int i;
-
-	seq_printf(m, "%8u\t%lu\n", 0U, hctx->dispatched[0]);
-
-	for (i = 1; i < BLK_MQ_MAX_DISPATCH_ORDER - 1; i++) {
-		unsigned int d = 1U << (i - 1);
-
-		seq_printf(m, "%8u\t%lu\n", d, hctx->dispatched[i]);
-	}
-
-	seq_printf(m, "%8u+\t%lu\n", 1U << (i - 1), hctx->dispatched[i]);
-	return 0;
-}
-
-static ssize_t hctx_dispatched_write(void *data, const char __user *buf,
-				     size_t count, loff_t *ppos)
-{
-	struct blk_mq_hw_ctx *hctx = data;
-	int i;
-
-	for (i = 0; i < BLK_MQ_MAX_DISPATCH_ORDER; i++)
-		hctx->dispatched[i] = 0;
-	return count;
-}
-
-static int hctx_queued_show(void *data, struct seq_file *m)
-{
-	struct blk_mq_hw_ctx *hctx = data;
-
-	seq_printf(m, "%lu\n", hctx->queued);
-	return 0;
-}
-
-static ssize_t hctx_queued_write(void *data, const char __user *buf,
-				 size_t count, loff_t *ppos)
-{
-	struct blk_mq_hw_ctx *hctx = data;
-
-	hctx->queued = 0;
-	return count;
-}
-
 static int hctx_run_show(void *data, struct seq_file *m)
 {
 	struct blk_mq_hw_ctx *hctx = data;
@@ -800,9 +736,6 @@ static const struct blk_mq_debugfs_attr blk_mq_debugfs_hctx_attrs[] = {
 	{"tags_bitmap", 0400, hctx_tags_bitmap_show},
 	{"sched_tags", 0400, hctx_sched_tags_show},
 	{"sched_tags_bitmap", 0400, hctx_sched_tags_bitmap_show},
-	{"io_poll", 0600, hctx_io_poll_show, hctx_io_poll_write},
-	{"dispatched", 0600, hctx_dispatched_show, hctx_dispatched_write},
-	{"queued", 0600, hctx_queued_show, hctx_queued_write},
 	{"run", 0600, hctx_run_show, hctx_run_write},
 	{"active", 0400, hctx_active_show},
 	{"dispatch_busy", 0400, hctx_dispatch_busy_show},
* Unmerged path block/blk-mq.c
* Unmerged path include/linux/blk-mq.h
