sbitmap: Try each queue to wake up at least one waiter

jira LE-4066
Rebuild_History Non-Buildable kernel-4.18.0-553.72.1.el8_10
commit-author Gabriel Krisman Bertazi <krisman@suse.de>
commit 26edb30dd1c0c9be11fa676b4f330ada7b794ba6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.72.1.el8_10/26edb30d.failed

Jan reported the new algorithm as merged might be problematic if the
queue being awaken becomes empty between the waitqueue_active inside
sbq_wake_ptr check and the wake up.  If that happens, wake_up_nr will
not wake up any waiter and we loose too many wake ups.  In order to
guarantee progress, we need to wake up at least one waiter here, if
there are any.  This now requires trying to wake up from every queue.

Instead of walking through all the queues with sbq_wake_ptr, this call
moves the wake up inside that function.  In a previous version of the
patch, I found that updating wake_index several times when walking
through queues had a measurable overhead.  This ensures we only update
it once, at the end.

Fixes: 4f8126bb2308 ("sbitmap: Use single per-bitmap counting to wake up queued tags")
	Reported-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Gabriel Krisman Bertazi <krisman@suse.de>
	Reviewed-by: Jan Kara <jack@suse.cz>
Link: https://lore.kernel.org/r/20221115224553.23594-4-krisman@suse.de
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 26edb30dd1c0c9be11fa676b4f330ada7b794ba6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/sbitmap.c
diff --cc lib/sbitmap.c
index 0956b5ca3935,586deb333237..000000000000
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@@ -539,95 -571,45 +539,133 @@@ static void __sbitmap_queue_wake_up(str
  	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
  		struct sbq_wait_state *ws = &sbq->ws[wake_index];
  
++<<<<<<< HEAD
 +		if (waitqueue_active(&ws->wait) && atomic_read(&ws->wait_cnt)) {
 +			if (wake_index != atomic_read(&sbq->wake_index))
 +				atomic_set(&sbq->wake_index, wake_index);
 +			return ws;
 +		}
 +
 +		wake_index = sbq_index_inc(wake_index);
++=======
+ 		/*
+ 		 * Advance the index before checking the current queue.
+ 		 * It improves fairness, by ensuring the queue doesn't
+ 		 * need to be fully emptied before trying to wake up
+ 		 * from the next one.
+ 		 */
+ 		wake_index = sbq_index_inc(wake_index);
+ 
+ 		/*
+ 		 * It is sufficient to wake up at least one waiter to
+ 		 * guarantee forward progress.
+ 		 */
+ 		if (waitqueue_active(&ws->wait) &&
+ 		    wake_up_nr(&ws->wait, nr))
+ 			break;
++>>>>>>> 26edb30dd1c0 (sbitmap: Try each queue to wake up at least one waiter)
  	}
  
- 	return NULL;
+ 	if (wake_index != atomic_read(&sbq->wake_index))
+ 		atomic_set(&sbq->wake_index, wake_index);
  }
  
 +static bool __sbq_wake_up(struct sbitmap_queue *sbq, int *nr)
 +{
 +	struct sbq_wait_state *ws;
 +	unsigned int wake_batch;
 +	int wait_cnt, cur, sub;
 +	bool ret;
 +
 +	if (*nr <= 0)
 +		return false;
 +
 +	ws = sbq_wake_ptr(sbq);
 +	if (!ws)
 +		return false;
 +
 +	cur = atomic_read(&ws->wait_cnt);
 +	do {
 +		/*
 +		 * For concurrent callers of this, callers should call this
 +		 * function again to wakeup a new batch on a different 'ws'.
 +		 */
 +		if (cur == 0)
 +			return true;
 +		sub = min(*nr, cur);
 +		wait_cnt = cur - sub;
 +	} while (!atomic_try_cmpxchg(&ws->wait_cnt, &cur, wait_cnt));
 +
 +	/*
 +	 * If we decremented queue without waiters, retry to avoid lost
 +	 * wakeups.
 +	 */
 +	if (wait_cnt > 0)
 +		return !waitqueue_active(&ws->wait);
 +
 +	*nr -= sub;
 +
 +	/*
 +	 * When wait_cnt == 0, we have to be particularly careful as we are
 +	 * responsible to reset wait_cnt regardless whether we've actually
 +	 * woken up anybody. But in case we didn't wakeup anybody, we still
 +	 * need to retry.
 +	 */
 +	ret = !waitqueue_active(&ws->wait);
 +	wake_batch = READ_ONCE(sbq->wake_batch);
 +
 +	/*
 +	 * Wake up first in case that concurrent callers decrease wait_cnt
 +	 * while waitqueue is empty.
 +	 */
 +	wake_up_nr(&ws->wait, wake_batch);
 +
 +	/*
 +	 * Pairs with the memory barrier in sbitmap_queue_resize() to
 +	 * ensure that we see the batch size update before the wait
 +	 * count is reset.
 +	 *
 +	 * Also pairs with the implicit barrier between decrementing wait_cnt
 +	 * and checking for waitqueue_active() to make sure waitqueue_active()
 +	 * sees result of the wakeup if atomic_dec_return() has seen the result
 +	 * of atomic_set().
 +	 */
 +	smp_mb__before_atomic();
 +
 +	/*
 +	 * Increase wake_index before updating wait_cnt, otherwise concurrent
 +	 * callers can see valid wait_cnt in old waitqueue, which can cause
 +	 * invalid wakeup on the old waitqueue.
 +	 */
 +	sbq_index_atomic_inc(&sbq->wake_index);
 +	atomic_set(&ws->wait_cnt, wake_batch);
 +
 +	return ret || *nr;
 +}
 +
  void sbitmap_queue_wake_up(struct sbitmap_queue *sbq, int nr)
  {
++<<<<<<< HEAD
 +	while (__sbq_wake_up(sbq, &nr))
 +		;
++=======
+ 	unsigned int wake_batch = READ_ONCE(sbq->wake_batch);
+ 	unsigned int wakeups;
+ 
+ 	if (!atomic_read(&sbq->ws_active))
+ 		return;
+ 
+ 	atomic_add(nr, &sbq->completion_cnt);
+ 	wakeups = atomic_read(&sbq->wakeup_cnt);
+ 
+ 	do {
+ 		if (atomic_read(&sbq->completion_cnt) - wakeups < wake_batch)
+ 			return;
+ 	} while (!atomic_try_cmpxchg(&sbq->wakeup_cnt,
+ 				     &wakeups, wakeups + wake_batch));
+ 
+ 	__sbitmap_queue_wake_up(sbq, wake_batch);
++>>>>>>> 26edb30dd1c0 (sbitmap: Try each queue to wake up at least one waiter)
  }
  EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
  
* Unmerged path lib/sbitmap.c
