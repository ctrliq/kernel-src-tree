sbitmap: Use single per-bitmap counting to wake up queued tags

jira LE-4066
Rebuild_History Non-Buildable kernel-4.18.0-553.72.1.el8_10
commit-author Gabriel Krisman Bertazi <krisman@suse.de>
commit 4f8126bb2308066b877859e4b5923ffb54143630
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.72.1.el8_10/4f8126bb.failed

sbitmap suffers from code complexity, as demonstrated by recent fixes,
and eventual lost wake ups on nested I/O completion.  The later happens,
from what I understand, due to the non-atomic nature of the updates to
wait_cnt, which needs to be subtracted and eventually reset when equal
to zero.  This two step process can eventually miss an update when a
nested completion happens to interrupt the CPU in between the wait_cnt
updates.  This is very hard to fix, as shown by the recent changes to
this code.

The code complexity arises mostly from the corner cases to avoid missed
wakes in this scenario.  In addition, the handling of wake_batch
recalculation plus the synchronization with sbq_queue_wake_up is
non-trivial.

This patchset implements the idea originally proposed by Jan [1], which
removes the need for the two-step updates of wait_cnt.  This is done by
tracking the number of completions and wakeups in always increasing,
per-bitmap counters.  Instead of having to reset the wait_cnt when it
reaches zero, we simply keep counting, and attempt to wake up N threads
in a single wait queue whenever there is enough space for a batch.
Waking up less than batch_wake shouldn't be a problem, because we
haven't changed the conditions for wake up, and the existing batch
calculation guarantees at least enough remaining completions to wake up
a batch for each queue at any time.

Performance-wise, one should expect very similar performance to the
original algorithm for the case where there is no queueing.  In both the
old algorithm and this implementation, the first thing is to check
ws_active, which bails out if there is no queueing to be managed. In the
new code, we took care to avoid accounting completions and wakeups when
there is no queueing, to not pay the cost of atomic operations
unnecessarily, since it doesn't skew the numbers.

For more interesting cases, where there is queueing, we need to take
into account the cross-communication of the atomic operations.  I've
been benchmarking by running parallel fio jobs against a single hctx
nullb in different hardware queue depth scenarios, and verifying both
IOPS and queueing.

Each experiment was repeated 5 times on a 20-CPU box, with 20 parallel
jobs. fio was issuing fixed-size randwrites with qd=64 against nullb,
varying only the hardware queue length per test.

queue size 2                 4                 8                 16                 32                 64
6.1-rc2    1681.1K (1.6K)    2633.0K (12.7K)   6940.8K (16.3K)   8172.3K (617.5K)   8391.7K (367.1K)   8606.1K (351.2K)
patched    1721.8K (15.1K)   3016.7K (3.8K)    7543.0K (89.4K)   8132.5K (303.4K)   8324.2K (230.6K)   8401.8K (284.7K)

The following is a similar experiment, ran against a nullb with a single
bitmap shared by 20 hctx spread across 2 NUMA nodes. This has 40
parallel fio jobs operating on the same device

queue size 2 	             4                 8              	16             	    32		       64
6.1-rc2	   1081.0K (2.3K)    957.2K (1.5K)     1699.1K (5.7K) 	6178.2K (124.6K)    12227.9K (37.7K)   13286.6K (92.9K)
patched	   1081.8K (2.8K)    1316.5K (5.4K)    2364.4K (1.8K) 	6151.4K  (20.0K)    11893.6K (17.5K)   12385.6K (18.4K)

It has also survived blktests and a 12h-stress run against nullb. I also
ran the code against nvme and a scsi SSD, and I didn't observe
performance regression in those. If there are other tests you think I
should run, please let me know and I will follow up with results.

[1] https://lore.kernel.org/all/aef9de29-e9f5-259a-f8be-12d1b734e72@google.com/

	Cc: Hugh Dickins <hughd@google.com>
	Cc: Keith Busch <kbusch@kernel.org>
	Cc: Liu Song <liusong@linux.alibaba.com>
	Suggested-by: Jan Kara <jack@suse.cz>
	Signed-off-by: Gabriel Krisman Bertazi <krisman@suse.de>
Link: https://lore.kernel.org/r/20221105231055.25953-1-krisman@suse.de
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 4f8126bb2308066b877859e4b5923ffb54143630)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/sbitmap.c
diff --cc lib/sbitmap.c
index 0956b5ca3935,eca462cba398..000000000000
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@@ -482,22 -453,29 +482,48 @@@ EXPORT_SYMBOL_GPL(sbitmap_queue_init_no
  static void sbitmap_queue_update_wake_batch(struct sbitmap_queue *sbq,
  					    unsigned int depth)
  {
++<<<<<<< HEAD
 +	unsigned int wake_batch = sbq_calc_wake_batch(sbq, depth);
 +	int i;
 +
 +	if (sbq->wake_batch != wake_batch) {
 +		WRITE_ONCE(sbq->wake_batch, wake_batch);
 +		/*
 +		 * Pairs with the memory barrier in sbitmap_queue_wake_up()
 +		 * to ensure that the batch size is updated before the wait
 +		 * counts.
 +		 */
 +		smp_mb();
 +		for (i = 0; i < SBQ_WAIT_QUEUES; i++)
 +			atomic_set(&sbq->ws[i].wait_cnt, 1);
 +	}
 +}
 +
++=======
+ 	unsigned int wake_batch;
+ 
+ 	wake_batch = sbq_calc_wake_batch(sbq, depth);
+ 	if (sbq->wake_batch != wake_batch)
+ 		WRITE_ONCE(sbq->wake_batch, wake_batch);
+ }
+ 
+ void sbitmap_queue_recalculate_wake_batch(struct sbitmap_queue *sbq,
+ 					    unsigned int users)
+ {
+ 	unsigned int wake_batch;
+ 	unsigned int min_batch;
+ 	unsigned int depth = (sbq->sb.depth + users - 1) / users;
+ 
+ 	min_batch = sbq->sb.depth >= (4 * SBQ_WAIT_QUEUES) ? 4 : 1;
+ 
+ 	wake_batch = clamp_val(depth / SBQ_WAIT_QUEUES,
+ 			min_batch, SBQ_WAKE_BATCH);
+ 
+ 	WRITE_ONCE(sbq->wake_batch, wake_batch);
+ }
+ EXPORT_SYMBOL_GPL(sbitmap_queue_recalculate_wake_batch);
+ 
++>>>>>>> 4f8126bb2308 (sbitmap: Use single per-bitmap counting to wake up queued tags)
  void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
  {
  	sbitmap_queue_update_wake_batch(sbq, depth);
diff --git a/include/linux/sbitmap.h b/include/linux/sbitmap.h
index b4856b4b2b45..e73bb4730adb 100644
--- a/include/linux/sbitmap.h
+++ b/include/linux/sbitmap.h
@@ -93,11 +93,6 @@ struct sbitmap {
  * struct sbq_wait_state - Wait queue in a &struct sbitmap_queue.
  */
 struct sbq_wait_state {
-	/**
-	 * @wait_cnt: Number of frees remaining before we wake up.
-	 */
-	atomic_t wait_cnt;
-
 	/**
 	 * @wait: Wait queue.
 	 */
@@ -158,6 +153,17 @@ struct sbitmap_queue {
 	 * sbitmap_queue_get_shallow() or __sbitmap_queue_get_shallow().
 	 */
 	unsigned int min_shallow_depth;
+
+	/**
+	 * @completion_cnt: Number of bits cleared passed to the
+	 * wakeup function.
+	 */
+	atomic_t completion_cnt;
+
+	/**
+	 * @wakeup_cnt: Number of thread wake ups issued.
+	 */
+	atomic_t wakeup_cnt;
 };
 
 /**
* Unmerged path lib/sbitmap.c
