mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-477.27.1.el8_8
commit-author Qi Zheng <zhengqi.arch@bytedance.com>
commit 36ef159f4408b08eae7f2af6d62bedd3f4343758
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-477.27.1.el8_8/36ef159f.failed

Since commit 4064b9827063 ("mm: allow VM_FAULT_RETRY for multiple
times") allowed VM_FAULT_RETRY for multiple times, the
FAULT_FLAG_ALLOW_RETRY bit of fault_flag will not be changed in the page
fault path, so the following check is no longer needed:

	flags & FAULT_FLAG_ALLOW_RETRY

So just remove it.

[akpm@linux-foundation.org: coding style fixes]

Link: https://lkml.kernel.org/r/20211110123358.36511-1-zhengqi.arch@bytedance.com
	Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: Kirill Shutemov <kirill@shutemov.name>
	Cc: Peter Xu <peterx@redhat.com>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Cc: Chengming Zhou <zhouchengming@bytedance.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 36ef159f4408b08eae7f2af6d62bedd3f4343758)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/alpha/mm/fault.c
#	arch/arc/mm/fault.c
#	arch/arm/mm/fault.c
#	arch/hexagon/mm/vm_fault.c
#	arch/ia64/mm/fault.c
#	arch/m68k/mm/fault.c
#	arch/microblaze/mm/fault.c
#	arch/mips/mm/fault.c
#	arch/nds32/mm/fault.c
#	arch/nios2/mm/fault.c
#	arch/openrisc/mm/fault.c
#	arch/parisc/mm/fault.c
#	arch/riscv/mm/fault.c
#	arch/sh/mm/fault.c
#	arch/sparc/mm/fault_32.c
#	arch/sparc/mm/fault_64.c
#	arch/um/kernel/trap.c
#	arch/xtensa/mm/fault.c
diff --cc arch/alpha/mm/fault.c
index d73dc473fbb9,6c0a277388dd..000000000000
--- a/arch/alpha/mm/fault.c
+++ b/arch/alpha/mm/fault.c
@@@ -163,24 -165,18 +163,33 @@@ retry
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR)
 +			current->maj_flt++;
 +		else
 +			current->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +
 +			 /* No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
  
- 			goto retry;
- 		}
+ 		/* No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  
  	return;
  
diff --cc arch/arc/mm/fault.c
index db6913094be3,dad27e4d69ff..000000000000
--- a/arch/arc/mm/fault.c
+++ b/arch/arc/mm/fault.c
@@@ -178,53 -146,29 +178,61 @@@ good_area
  		return;
  	}
  
 +	if (fault & VM_FAULT_OOM)
 +		goto out_of_memory;
 +	else if (fault & VM_FAULT_SIGSEGV)
 +		goto bad_area;
 +	else if (fault & VM_FAULT_SIGBUS)
 +		goto do_sigbus;
 +
 +	/* no man's land */
 +	BUG();
 +
  	/*
 -	 * Fault retry nuances, mmap_lock already relinquished by core mm
 +	 * Something tried to access memory that isn't in our memory map..
 +	 * Fix it, but check if it's kernel or user first..
  	 */
++<<<<<<< HEAD
++=======
+ 	if (unlikely(fault & VM_FAULT_RETRY)) {
+ 		flags |= FAULT_FLAG_TRIED;
+ 		goto retry;
+ 	}
+ 
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  bad_area:
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
 +
 +bad_area_nosemaphore:
 +	/* User mode accesses just cause a SIGSEGV */
 +	if (user_mode(regs)) {
 +		tsk->thread.fault_address = address;
 +		info.si_signo = SIGSEGV;
 +		info.si_errno = 0;
 +		/* info.si_code has been set above */
 +		info.si_addr = (void __user *)address;
 +		force_sig_info(SIGSEGV, &info, tsk);
 +		return;
 +	}
  
 -	/*
 -	 * Major/minor page fault accounting
 -	 * (in case of retry we only land here once)
 +no_context:
 +	/* Are we prepared to handle this kernel fault?
 +	 *
 +	 * (The kernel has valid exception-points in the source
 +	 *  when it accesses user-memory. When it fails in one
 +	 *  of those points, we find it in a table and do a jump
 +	 *  to some fixup code that loads an appropriate error
 +	 *  code)
  	 */
 -	if (likely(!(fault & VM_FAULT_ERROR)))
 -		/* Normal return path: fault Handled Gracefully */
 +	if (fixup_exception(regs))
  		return;
  
 -	if (!user_mode(regs))
 -		goto no_context;
 +	die("Oops", regs, address);
 +
 +out_of_memory:
 +	up_read(&mm->mmap_sem);
  
 -	if (fault & VM_FAULT_OOM) {
 +	if (user_mode(regs)) {
  		pagefault_out_of_memory();
  		return;
  	}
diff --cc arch/arm/mm/fault.c
index f4ea4c62c613,c7326a521a69..000000000000
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@@ -318,27 -312,8 +318,31 @@@ retry
  		return 0;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Major/minor page fault accounting is only done on the
 +	 * initial attempt. If we go through a retry, it is extremely
 +	 * likely that the page will be found in page cache at that point.
 +	 */
 +
 +	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);
 +	if (!(fault & VM_FAULT_ERROR) && flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR) {
 +			tsk->maj_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 +					regs, addr);
 +		} else {
 +			tsk->min_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 +					regs, addr);
 +		}
++=======
+ 	if (!(fault & VM_FAULT_ERROR)) {
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  		if (fault & VM_FAULT_RETRY) {
 +			/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk
 +			* of starvation. */
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
  			flags |= FAULT_FLAG_TRIED;
  			goto retry;
  		}
diff --cc arch/hexagon/mm/vm_fault.c
index eb263e61daf4,4fac4b9eb316..000000000000
--- a/arch/hexagon/mm/vm_fault.c
+++ b/arch/hexagon/mm/vm_fault.c
@@@ -109,19 -98,12 +109,25 @@@ good_area
  
  	/* The most common case -- we are done. */
  	if (likely(!(fault & VM_FAULT_ERROR))) {
++<<<<<<< HEAD
 +		if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +			if (fault & VM_FAULT_MAJOR)
 +				current->maj_flt++;
 +			else
 +				current->min_flt++;
 +			if (fault & VM_FAULT_RETRY) {
 +				flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +				flags |= FAULT_FLAG_TRIED;
 +				goto retry;
 +			}
++=======
+ 		if (fault & VM_FAULT_RETRY) {
+ 			flags |= FAULT_FLAG_TRIED;
+ 			goto retry;
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  		}
  
 -		mmap_read_unlock(mm);
 +		up_read(&mm->mmap_sem);
  		return;
  	}
  
diff --cc arch/ia64/mm/fault.c
index a9d55ad8d67b,32417f49ad2f..000000000000
--- a/arch/ia64/mm/fault.c
+++ b/arch/ia64/mm/fault.c
@@@ -183,25 -156,18 +183,34 @@@ retry
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR)
 +			current->maj_flt++;
 +		else
 +			current->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			 /* No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
  
- 			goto retry;
- 		}
+ 		/* No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  	return;
  
    check_expansion:
diff --cc arch/m68k/mm/fault.c
index 9b6163c05a75,53cfb9bc1066..000000000000
--- a/arch/m68k/mm/fault.c
+++ b/arch/m68k/mm/fault.c
@@@ -151,33 -153,19 +151,43 @@@ good_area
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Major/minor page fault accounting is only done on the
 +	 * initial attempt. If we go through a retry, it is extremely
 +	 * likely that the page will be found in page cache at that point.
 +	 */
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR)
 +			current->maj_flt++;
 +		else
 +			current->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk
 +			 * of starvation. */
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/*
 +			 * No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
  
- 			goto retry;
- 		}
+ 		/*
+ 		 * No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  	return 0;
  
  /*
diff --cc arch/microblaze/mm/fault.c
index 202ad6a494f5,a9626e6a68af..000000000000
--- a/arch/microblaze/mm/fault.c
+++ b/arch/microblaze/mm/fault.c
@@@ -230,26 -232,19 +230,36 @@@ good_area
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (unlikely(fault & VM_FAULT_MAJOR))
 +			current->maj_flt++;
 +		else
 +			current->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/*
 +			 * No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
  
- 			goto retry;
- 		}
+ 		/*
+ 		 * No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  
  	/*
  	 * keep track of tlb+htab misses that are good addrs but
diff --cc arch/mips/mm/fault.c
index c6c649690877,44f98100e84e..000000000000
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@@ -167,28 -171,17 +167,39 @@@ good_area
  			goto do_sigbus;
  		BUG();
  	}
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR) {
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 +						  regs, address);
 +			tsk->maj_flt++;
 +		} else {
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 +						  regs, address);
 +			tsk->min_flt++;
 +		}
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/*
 +			 * No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
  
- 			goto retry;
- 		}
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		/*
+ 		 * No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
+ 
+ 		goto retry;
  	}
  
  	mmap_read_unlock(mm);
diff --cc arch/nds32/mm/fault.c
index b740534b152c,636977a1c8b9..000000000000
--- a/arch/nds32/mm/fault.c
+++ b/arch/nds32/mm/fault.c
@@@ -226,29 -230,17 +226,40 @@@ good_area
  			goto bad_area;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Major/minor page fault accounting is only done on the initial
 +	 * attempt. If we go through a retry, it is extremely likely that the
 +	 * page will be found in page cache at that point.
 +	 */
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR)
 +			tsk->maj_flt++;
 +		else
 +			tsk->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/* No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
 +			goto retry;
 +		}
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
+ 
+ 		/* No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
+ 		goto retry;
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  	return;
  
  	/*
diff --cc arch/nios2/mm/fault.c
index 24fd84cf6006,a32f14cd72f2..000000000000
--- a/arch/nios2/mm/fault.c
+++ b/arch/nios2/mm/fault.c
@@@ -147,33 -149,19 +147,43 @@@ good_area
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * Major/minor page fault accounting is only done on the
 +	 * initial attempt. If we go through a retry, it is extremely
 +	 * likely that the page will be found in page cache at that point.
 +	 */
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR)
 +			current->maj_flt++;
 +		else
 +			current->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk
 +			 * of starvation. */
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/*
 +			 * No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
- 
- 			goto retry;
- 		}
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
+ 
+ 		/*
+ 		 * No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  	return;
  
  /*
diff --cc arch/openrisc/mm/fault.c
index dc4dbafc1d83,80bb66ad42f6..000000000000
--- a/arch/openrisc/mm/fault.c
+++ b/arch/openrisc/mm/fault.c
@@@ -178,26 -177,19 +178,36 @@@ good_area
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		/*RGD modeled on Cris */
 +		if (fault & VM_FAULT_MAJOR)
 +			tsk->maj_flt++;
 +		else
 +			tsk->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			 /* No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
+ 	/*RGD modeled on Cris */
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
+ 
+ 		/* No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  
- 			goto retry;
- 		}
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  	return;
  
  	/*
diff --cc arch/parisc/mm/fault.c
index c8e8b7c05558,360b627645cc..000000000000
--- a/arch/parisc/mm/fault.c
+++ b/arch/parisc/mm/fault.c
@@@ -321,24 -324,16 +321,35 @@@ good_area
  			goto bad_area;
  		BUG();
  	}
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR)
 +			current->maj_flt++;
 +		else
 +			current->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +
 +			/*
 +			 * No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
 +
 +			goto retry;
 +		}
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		/*
+ 		 * No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
+ 		flags |= FAULT_FLAG_TRIED;
+ 		goto retry;
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  	}
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  	return;
  
  check_expansion:
diff --cc arch/riscv/mm/fault.c
index 88401d5125bc,cae4b6363607..000000000000
--- a/arch/riscv/mm/fault.c
+++ b/arch/riscv/mm/fault.c
@@@ -124,163 -324,30 +124,179 @@@ good_area
  
  	/*
  	 * If we need to retry but a fatal signal is pending, handle the
 -	 * signal first. We do not need to release the mmap_lock because it
 +	 * signal first. We do not need to release the mmap_sem because it
  	 * would already be released in __lock_page_or_retry in mm/filemap.c.
  	 */
 -	if (fault_signal_pending(fault, regs))
 +	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(tsk))
  		return;
  
++<<<<<<< HEAD
++=======
+ 	if (unlikely(fault & VM_FAULT_RETRY)) {
+ 		flags |= FAULT_FLAG_TRIED;
+ 
+ 		/*
+ 		 * No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
+ 		goto retry;
+ 	}
+ 
+ 	mmap_read_unlock(mm);
+ 
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  	if (unlikely(fault & VM_FAULT_ERROR)) {
 -		tsk->thread.bad_cause = cause;
 -		mm_fault_error(regs, addr, fault);
 +		if (fault & VM_FAULT_OOM)
 +			goto out_of_memory;
 +		else if (fault & VM_FAULT_SIGBUS)
 +			goto do_sigbus;
 +		BUG();
 +	}
 +
 +	/*
 +	 * Major/minor page fault accounting is only done on the
 +	 * initial attempt. If we go through a retry, it is extremely
 +	 * likely that the page will be found in page cache at that point.
 +	 */
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR) {
 +			tsk->maj_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,
 +				      1, regs, addr);
 +		} else {
 +			tsk->min_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,
 +				      1, regs, addr);
 +		}
 +		if (fault & VM_FAULT_RETRY) {
 +			/*
 +			 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk
 +			 * of starvation.
 +			 */
 +			flags &= ~(FAULT_FLAG_ALLOW_RETRY);
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/*
 +			 * No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
 +			goto retry;
 +		}
 +	}
 +
 +	up_read(&mm->mmap_sem);
 +	return;
 +
 +	/*
 +	 * Something tried to access memory that isn't in our memory map.
 +	 * Fix it, but check if it's kernel or user first.
 +	 */
 +bad_area:
 +	up_read(&mm->mmap_sem);
 +	/* User mode accesses just cause a SIGSEGV */
 +	if (user_mode(regs)) {
 +		do_trap(regs, SIGSEGV, code, addr, tsk);
  		return;
  	}
 +
 +no_context:
 +	/* Are we prepared to handle this kernel fault? */
 +	if (fixup_exception(regs))
 +		return;
 +
 +	/*
 +	 * Oops. The kernel tried to access some bad page. We'll have to
 +	 * terminate things with extreme prejudice.
 +	 */
 +	bust_spinlocks(1);
 +	pr_alert("Unable to handle kernel %s at virtual address " REG_FMT "\n",
 +		(addr < PAGE_SIZE) ? "NULL pointer dereference" :
 +		"paging request", addr);
 +	die(regs, "Oops");
 +	do_exit(SIGKILL);
 +
 +	/*
 +	 * We ran out of memory, call the OOM killer, and return the userspace
 +	 * (which will retry the fault, or kill us if we got oom-killed).
 +	 */
 +out_of_memory:
 +	up_read(&mm->mmap_sem);
 +	if (!user_mode(regs))
 +		goto no_context;
 +	pagefault_out_of_memory();
  	return;
 +
 +do_sigbus:
 +	up_read(&mm->mmap_sem);
 +	/* Kernel mode? Handle exceptions or die */
 +	if (!user_mode(regs))
 +		goto no_context;
 +	do_trap(regs, SIGBUS, BUS_ADRERR, addr, tsk);
 +	return;
 +
 +vmalloc_fault:
 +	{
 +		pgd_t *pgd, *pgd_k;
 +		pud_t *pud, *pud_k;
 +		p4d_t *p4d, *p4d_k;
 +		pmd_t *pmd, *pmd_k;
 +		pte_t *pte_k;
 +		int index;
 +
 +		if (user_mode(regs))
 +			goto bad_area;
 +
 +		/*
 +		 * Synchronize this task's top level page-table
 +		 * with the 'reference' page table.
 +		 *
 +		 * Do _not_ use "tsk->active_mm->pgd" here.
 +		 * We might be inside an interrupt in the middle
 +		 * of a task switch.
 +		 *
 +		 * Note: Use the old spbtr name instead of using the current
 +		 * satp name to support binutils 2.29 which doesn't know about
 +		 * the privileged ISA 1.10 yet.
 +		 */
 +		index = pgd_index(addr);
 +		pgd = (pgd_t *)pfn_to_virt(csr_read(sptbr)) + index;
 +		pgd_k = init_mm.pgd + index;
 +
 +		if (!pgd_present(*pgd_k))
 +			goto no_context;
 +		set_pgd(pgd, *pgd_k);
 +
 +		p4d = p4d_offset(pgd, addr);
 +		p4d_k = p4d_offset(pgd_k, addr);
 +		if (!p4d_present(*p4d_k))
 +			goto no_context;
 +
 +		pud = pud_offset(p4d, addr);
 +		pud_k = pud_offset(p4d_k, addr);
 +		if (!pud_present(*pud_k))
 +			goto no_context;
 +
 +		/*
 +		 * Since the vmalloc area is global, it is unnecessary
 +		 * to copy individual PTEs
 +		 */
 +		pmd = pmd_offset(pud, addr);
 +		pmd_k = pmd_offset(pud_k, addr);
 +		if (!pmd_present(*pmd_k))
 +			goto no_context;
 +		set_pmd(pmd, *pmd_k);
 +
 +		/*
 +		 * Make sure the actual PTE exists as well to
 +		 * catch kernel vmalloc-area accesses to non-mapped
 +		 * addresses. If we don't do this, this will just
 +		 * silently loop forever.
 +		 */
 +		pte_k = pte_offset_kernel(pmd_k, addr);
 +		if (!pte_present(*pte_k))
 +			goto no_context;
 +		return;
 +	}
  }
 -NOKPROBE_SYMBOL(do_page_fault);
diff --cc arch/sh/mm/fault.c
index a6204b0b1a3e,e175667b1363..000000000000
--- a/arch/sh/mm/fault.c
+++ b/arch/sh/mm/fault.c
@@@ -487,28 -485,16 +487,40 @@@ good_area
  		if (mm_fault_error(regs, error_code, address, fault))
  			return;
  
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR) {
 +			tsk->maj_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
 +				      regs, address);
 +		} else {
 +			tsk->min_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
 +				      regs, address);
 +		}
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/*
 +			 * No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
 +			goto retry;
 +		}
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
+ 
+ 		/*
+ 		 * No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
+ 		goto retry;
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  }
diff --cc arch/sparc/mm/fault_32.c
index b0440b0edd97,ad569d9bd124..000000000000
--- a/arch/sparc/mm/fault_32.c
+++ b/arch/sparc/mm/fault_32.c
@@@ -250,30 -200,18 +250,39 @@@ good_area
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR) {
 +			current->maj_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,
 +				      1, regs, address);
 +		} else {
 +			current->min_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,
 +				      1, regs, address);
 +		}
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/* No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
  
- 			goto retry;
- 		}
+ 		/* No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  	return;
  
  	/*
diff --cc arch/sparc/mm/fault_64.c
index 8f8a604c1300,253e07043298..000000000000
--- a/arch/sparc/mm/fault_64.c
+++ b/arch/sparc/mm/fault_64.c
@@@ -448,29 -437,17 +448,38 @@@ good_area
  		BUG();
  	}
  
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR) {
 +			current->maj_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,
 +				      1, regs, address);
 +		} else {
 +			current->min_flt++;
 +			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,
 +				      1, regs, address);
 +		}
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			/* No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
  
- 			goto retry;
- 		}
+ 		/* No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		goto retry;
  	}
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
  
  	mm_rss = get_mm_rss(mm);
  #if defined(CONFIG_TRANSPARENT_HUGEPAGE)
diff --cc arch/um/kernel/trap.c
index eb03c74541ae,193503484af5..000000000000
--- a/arch/um/kernel/trap.c
+++ b/arch/um/kernel/trap.c
@@@ -90,18 -87,13 +90,22 @@@ good_area
  			}
  			BUG();
  		}
++<<<<<<< HEAD
 +		if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +			if (fault & VM_FAULT_RETRY) {
 +				flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +				flags |= FAULT_FLAG_TRIED;
++=======
+ 		if (fault & VM_FAULT_RETRY) {
+ 			flags |= FAULT_FLAG_TRIED;
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
  
- 				goto retry;
- 			}
+ 			goto retry;
  		}
  
 -		pmd = pmd_off(mm, address);
 +		pgd = pgd_offset(mm, address);
 +		pud = pud_offset(pgd, address);
 +		pmd = pmd_offset(pud, address);
  		pte = pte_offset_kernel(pmd, address);
  	} while (!pte_present(*pte));
  	err = 0;
diff --cc arch/xtensa/mm/fault.c
index 2ab0e0dcd166,06d0973a0d74..000000000000
--- a/arch/xtensa/mm/fault.c
+++ b/arch/xtensa/mm/fault.c
@@@ -122,31 -127,19 +122,41 @@@ good_area
  			goto do_sigbus;
  		BUG();
  	}
++<<<<<<< HEAD
 +	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 +		if (fault & VM_FAULT_MAJOR)
 +			current->maj_flt++;
 +		else
 +			current->min_flt++;
 +		if (fault & VM_FAULT_RETRY) {
 +			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 +			flags |= FAULT_FLAG_TRIED;
 +
 +			 /* No need to up_read(&mm->mmap_sem) as we would
 +			 * have already released it in __lock_page_or_retry
 +			 * in mm/filemap.c.
 +			 */
++=======
  
- 			goto retry;
- 		}
+ 	if (fault & VM_FAULT_RETRY) {
+ 		flags |= FAULT_FLAG_TRIED;
++>>>>>>> 36ef159f4408 (mm: remove redundant check about FAULT_FLAG_ALLOW_RETRY bit)
+ 
+ 		/* No need to mmap_read_unlock(mm) as we would
+ 		 * have already released it in __lock_page_or_retry
+ 		 * in mm/filemap.c.
+ 		 */
+ 
+ 		goto retry;
  	}
  
 -	mmap_read_unlock(mm);
 +	up_read(&mm->mmap_sem);
 +	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 +	if (flags & VM_FAULT_MAJOR)
 +		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);
 +	else
 +		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);
 +
  	return;
  
  	/* Something tried to access memory that isn't in our memory map..
* Unmerged path arch/alpha/mm/fault.c
* Unmerged path arch/arc/mm/fault.c
* Unmerged path arch/arm/mm/fault.c
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 1ba01ace6ef9..28a5720d67cc 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -554,10 +554,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	}
 
 	if (fault & VM_FAULT_RETRY) {
-		if (mm_flags & FAULT_FLAG_ALLOW_RETRY) {
-			mm_flags |= FAULT_FLAG_TRIED;
-			goto retry;
-		}
+		mm_flags |= FAULT_FLAG_TRIED;
+		goto retry;
 	}
 	mmap_read_unlock(mm);
 
* Unmerged path arch/hexagon/mm/vm_fault.c
* Unmerged path arch/ia64/mm/fault.c
* Unmerged path arch/m68k/mm/fault.c
* Unmerged path arch/microblaze/mm/fault.c
* Unmerged path arch/mips/mm/fault.c
* Unmerged path arch/nds32/mm/fault.c
* Unmerged path arch/nios2/mm/fault.c
* Unmerged path arch/openrisc/mm/fault.c
* Unmerged path arch/parisc/mm/fault.c
diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
index 288e47193ab3..e56b584b8422 100644
--- a/arch/powerpc/mm/fault.c
+++ b/arch/powerpc/mm/fault.c
@@ -629,10 +629,8 @@ static int __do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * case.
 	 */
 	if (unlikely(fault & VM_FAULT_RETRY)) {
-		if (flags & FAULT_FLAG_ALLOW_RETRY) {
-			flags |= FAULT_FLAG_TRIED;
-			goto retry;
-		}
+		flags |= FAULT_FLAG_TRIED;
+		goto retry;
 	}
 
 	mmap_read_unlock(current->mm);
* Unmerged path arch/riscv/mm/fault.c
diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c
index 35778ce3ae24..e7a917df62dd 100644
--- a/arch/s390/mm/fault.c
+++ b/arch/s390/mm/fault.c
@@ -525,21 +525,21 @@ static inline vm_fault_t do_exception(struct pt_regs *regs, int access)
 	if (unlikely(fault & VM_FAULT_ERROR))
 		goto out_up;
 
-	if (flags & FAULT_FLAG_ALLOW_RETRY) {
-		if (fault & VM_FAULT_RETRY) {
-			if (IS_ENABLED(CONFIG_PGSTE) && gmap &&
-			    (flags & FAULT_FLAG_RETRY_NOWAIT)) {
-				/* FAULT_FLAG_RETRY_NOWAIT has been set,
-				 * mmap_lock has not been released */
-				current->thread.gmap_pfault = 1;
-				fault = VM_FAULT_PFAULT;
-				goto out_up;
-			}
-			flags &= ~FAULT_FLAG_RETRY_NOWAIT;
-			flags |= FAULT_FLAG_TRIED;
-			mmap_read_lock(mm);
-			goto retry;
+	if (fault & VM_FAULT_RETRY) {
+		if (IS_ENABLED(CONFIG_PGSTE) && gmap &&
+			(flags & FAULT_FLAG_RETRY_NOWAIT)) {
+			/*
+			 * FAULT_FLAG_RETRY_NOWAIT has been set, mmap_lock has
+			 * not been released
+			 */
+			current->thread.gmap_pfault = 1;
+			fault = VM_FAULT_PFAULT;
+			goto out_up;
 		}
+		flags &= ~FAULT_FLAG_RETRY_NOWAIT;
+		flags |= FAULT_FLAG_TRIED;
+		mmap_read_lock(mm);
+		goto retry;
 	}
 	if (IS_ENABLED(CONFIG_PGSTE) && gmap) {
 		address =  __gmap_link(gmap, current->thread.gmap_addr,
* Unmerged path arch/sh/mm/fault.c
* Unmerged path arch/sparc/mm/fault_32.c
* Unmerged path arch/sparc/mm/fault_64.c
* Unmerged path arch/um/kernel/trap.c
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 322898ed670a..59145bcfaa4a 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1449,8 +1449,7 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * and if there is a fatal signal pending there is no guarantee
 	 * that we made any progress. Handle this case first.
 	 */
-	if (unlikely((fault & VM_FAULT_RETRY) &&
-		     (flags & FAULT_FLAG_ALLOW_RETRY))) {
+	if (unlikely(fault & VM_FAULT_RETRY)) {
 		flags |= FAULT_FLAG_TRIED;
 		goto retry;
 	}
* Unmerged path arch/xtensa/mm/fault.c
