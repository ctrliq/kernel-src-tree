mm: memory-tiering: fix PGPROMOTE_CANDIDATE counting

jira KERNEL-452
Rebuild_History Non-Buildable kernel-6.12.0-124.27.1.el10_1
commit-author Ruan Shiyang <ruansy.fnst@fujitsu.com>
commit 337135e6124b6d37d7ef1cd5a6c0b9681938c5ee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-6.12.0-124.27.1.el10_1/337135e6.failed

Goto-san reported confusing pgpromote statistics where the
pgpromote_success count significantly exceeded pgpromote_candidate.

On a system with three nodes (nodes 0-1: DRAM 4GB, node 2: NVDIMM 4GB):
 # Enable demotion only
 echo 1 > /sys/kernel/mm/numa/demotion_enabled
 numactl -m 0-1 memhog -r200 3500M >/dev/null &
 pid=$!
 sleep 2
 numactl memhog -r100 2500M >/dev/null &
 sleep 10
 kill -9 $pid # terminate the 1st memhog
 # Enable promotion
 echo 2 > /proc/sys/kernel/numa_balancing

After a few seconds, we observeed `pgpromote_candidate < pgpromote_success`
$ grep -e pgpromote /proc/vmstat
pgpromote_success 2579
pgpromote_candidate 0

In this scenario, after terminating the first memhog, the conditions for
pgdat_free_space_enough() are quickly met, and triggers promotion. 
However, these migrated pages are only counted for in PGPROMOTE_SUCCESS,
not in PGPROMOTE_CANDIDATE.

To solve these confusing statistics, introduce PGPROMOTE_CANDIDATE_NRL to
count the missed promotion pages.  And also, not counting these pages into
PGPROMOTE_CANDIDATE is to avoid changing the existing algorithm or
performance of the promotion rate limit.

Link: https://lkml.kernel.org/r/20250901090122.124262-1-ruansy.fnst@fujitsu.com
Link: https://lkml.kernel.org/r/20250729035101.1601407-1-ruansy.fnst@fujitsu.com
Fixes: c6833e10008f ("memory tiering: rate limit NUMA migration throughput")
Co-developed-by: Li Zhijian <lizhijian@fujitsu.com>
	Signed-off-by: Li Zhijian <lizhijian@fujitsu.com>
	Signed-off-by: Ruan Shiyang <ruansy.fnst@fujitsu.com>
	Reported-by: Yasunori Gotou (Fujitsu) <y-goto@fujitsu.com>
	Suggested-by: Huang Ying <ying.huang@linux.alibaba.com>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Huang Ying <ying.huang@linux.alibaba.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Juri Lelli <juri.lelli@redhat.com>
	Cc: Vincent Guittot <vincent.guittot@linaro.org>
	Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Ben Segall <bsegall@google.com>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Valentin Schneider <vschneid@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 337135e6124b6d37d7ef1cd5a6c0b9681938c5ee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmstat.c
diff --cc mm/vmstat.c
index 37d90ac659a8,e74f0b2a1021..000000000000
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@@ -1189,175 -1196,194 +1189,181 @@@ int fragmentation_index(struct zone *zo
  
  const char * const vmstat_text[] = {
  	/* enum zone_stat_item counters */
 -#define I(x) (x)
 -	[I(NR_FREE_PAGES)]			= "nr_free_pages",
 -	[I(NR_FREE_PAGES_BLOCKS)]		= "nr_free_pages_blocks",
 -	[I(NR_ZONE_INACTIVE_ANON)]		= "nr_zone_inactive_anon",
 -	[I(NR_ZONE_ACTIVE_ANON)]		= "nr_zone_active_anon",
 -	[I(NR_ZONE_INACTIVE_FILE)]		= "nr_zone_inactive_file",
 -	[I(NR_ZONE_ACTIVE_FILE)]		= "nr_zone_active_file",
 -	[I(NR_ZONE_UNEVICTABLE)]		= "nr_zone_unevictable",
 -	[I(NR_ZONE_WRITE_PENDING)]		= "nr_zone_write_pending",
 -	[I(NR_MLOCK)]				= "nr_mlock",
 +	"nr_free_pages",
 +	"nr_zone_inactive_anon",
 +	"nr_zone_active_anon",
 +	"nr_zone_inactive_file",
 +	"nr_zone_active_file",
 +	"nr_zone_unevictable",
 +	"nr_zone_write_pending",
 +	"nr_mlock",
 +	"nr_bounce",
  #if IS_ENABLED(CONFIG_ZSMALLOC)
 -	[I(NR_ZSPAGES)]				= "nr_zspages",
 +	"nr_zspages",
  #endif
 -	[I(NR_FREE_CMA_PAGES)]			= "nr_free_cma",
 +	"nr_free_cma",
  #ifdef CONFIG_UNACCEPTED_MEMORY
 -	[I(NR_UNACCEPTED)]			= "nr_unaccepted",
 +	"nr_unaccepted",
  #endif
 -#undef I
  
  	/* enum numa_stat_item counters */
 -#define I(x) (NR_VM_ZONE_STAT_ITEMS + x)
  #ifdef CONFIG_NUMA
 -	[I(NUMA_HIT)]				= "numa_hit",
 -	[I(NUMA_MISS)]				= "numa_miss",
 -	[I(NUMA_FOREIGN)]			= "numa_foreign",
 -	[I(NUMA_INTERLEAVE_HIT)]		= "numa_interleave",
 -	[I(NUMA_LOCAL)]				= "numa_local",
 -	[I(NUMA_OTHER)]				= "numa_other",
 +	"numa_hit",
 +	"numa_miss",
 +	"numa_foreign",
 +	"numa_interleave",
 +	"numa_local",
 +	"numa_other",
  #endif
 -#undef I
  
  	/* enum node_stat_item counters */
 -#define I(x) (NR_VM_ZONE_STAT_ITEMS + NR_VM_NUMA_EVENT_ITEMS + x)
 -	[I(NR_INACTIVE_ANON)]			= "nr_inactive_anon",
 -	[I(NR_ACTIVE_ANON)]			= "nr_active_anon",
 -	[I(NR_INACTIVE_FILE)]			= "nr_inactive_file",
 -	[I(NR_ACTIVE_FILE)]			= "nr_active_file",
 -	[I(NR_UNEVICTABLE)]			= "nr_unevictable",
 -	[I(NR_SLAB_RECLAIMABLE_B)]		= "nr_slab_reclaimable",
 -	[I(NR_SLAB_UNRECLAIMABLE_B)]		= "nr_slab_unreclaimable",
 -	[I(NR_ISOLATED_ANON)]			= "nr_isolated_anon",
 -	[I(NR_ISOLATED_FILE)]			= "nr_isolated_file",
 -	[I(WORKINGSET_NODES)]			= "workingset_nodes",
 -	[I(WORKINGSET_REFAULT_ANON)]		= "workingset_refault_anon",
 -	[I(WORKINGSET_REFAULT_FILE)]		= "workingset_refault_file",
 -	[I(WORKINGSET_ACTIVATE_ANON)]		= "workingset_activate_anon",
 -	[I(WORKINGSET_ACTIVATE_FILE)]		= "workingset_activate_file",
 -	[I(WORKINGSET_RESTORE_ANON)]		= "workingset_restore_anon",
 -	[I(WORKINGSET_RESTORE_FILE)]		= "workingset_restore_file",
 -	[I(WORKINGSET_NODERECLAIM)]		= "workingset_nodereclaim",
 -	[I(NR_ANON_MAPPED)]			= "nr_anon_pages",
 -	[I(NR_FILE_MAPPED)]			= "nr_mapped",
 -	[I(NR_FILE_PAGES)]			= "nr_file_pages",
 -	[I(NR_FILE_DIRTY)]			= "nr_dirty",
 -	[I(NR_WRITEBACK)]			= "nr_writeback",
 -	[I(NR_SHMEM)]				= "nr_shmem",
 -	[I(NR_SHMEM_THPS)]			= "nr_shmem_hugepages",
 -	[I(NR_SHMEM_PMDMAPPED)]			= "nr_shmem_pmdmapped",
 -	[I(NR_FILE_THPS)]			= "nr_file_hugepages",
 -	[I(NR_FILE_PMDMAPPED)]			= "nr_file_pmdmapped",
 -	[I(NR_ANON_THPS)]			= "nr_anon_transparent_hugepages",
 -	[I(NR_VMSCAN_WRITE)]			= "nr_vmscan_write",
 -	[I(NR_VMSCAN_IMMEDIATE)]		= "nr_vmscan_immediate_reclaim",
 -	[I(NR_DIRTIED)]				= "nr_dirtied",
 -	[I(NR_WRITTEN)]				= "nr_written",
 -	[I(NR_THROTTLED_WRITTEN)]		= "nr_throttled_written",
 -	[I(NR_KERNEL_MISC_RECLAIMABLE)]		= "nr_kernel_misc_reclaimable",
 -	[I(NR_FOLL_PIN_ACQUIRED)]		= "nr_foll_pin_acquired",
 -	[I(NR_FOLL_PIN_RELEASED)]		= "nr_foll_pin_released",
 -	[I(NR_KERNEL_STACK_KB)]			= "nr_kernel_stack",
 +	"nr_inactive_anon",
 +	"nr_active_anon",
 +	"nr_inactive_file",
 +	"nr_active_file",
 +	"nr_unevictable",
 +	"nr_slab_reclaimable",
 +	"nr_slab_unreclaimable",
 +	"nr_isolated_anon",
 +	"nr_isolated_file",
 +	"workingset_nodes",
 +	"workingset_refault_anon",
 +	"workingset_refault_file",
 +	"workingset_activate_anon",
 +	"workingset_activate_file",
 +	"workingset_restore_anon",
 +	"workingset_restore_file",
 +	"workingset_nodereclaim",
 +	"nr_anon_pages",
 +	"nr_mapped",
 +	"nr_file_pages",
 +	"nr_dirty",
 +	"nr_writeback",
 +	"nr_writeback_temp",
 +	"nr_shmem",
 +	"nr_shmem_hugepages",
 +	"nr_shmem_pmdmapped",
 +	"nr_file_hugepages",
 +	"nr_file_pmdmapped",
 +	"nr_anon_transparent_hugepages",
 +	"nr_vmscan_write",
 +	"nr_vmscan_immediate_reclaim",
 +	"nr_dirtied",
 +	"nr_written",
 +	"nr_throttled_written",
 +	"nr_kernel_misc_reclaimable",
 +	"nr_foll_pin_acquired",
 +	"nr_foll_pin_released",
 +	"nr_kernel_stack",
  #if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)
 -	[I(NR_KERNEL_SCS_KB)]			= "nr_shadow_call_stack",
 +	"nr_shadow_call_stack",
  #endif
 -	[I(NR_PAGETABLE)]			= "nr_page_table_pages",
 -	[I(NR_SECONDARY_PAGETABLE)]		= "nr_sec_page_table_pages",
 +	"nr_page_table_pages",
 +	"nr_sec_page_table_pages",
  #ifdef CONFIG_IOMMU_SUPPORT
 -	[I(NR_IOMMU_PAGES)]			= "nr_iommu_pages",
 +	"nr_iommu_pages",
  #endif
  #ifdef CONFIG_SWAP
 -	[I(NR_SWAPCACHE)]			= "nr_swapcached",
 +	"nr_swapcached",
  #endif
  #ifdef CONFIG_NUMA_BALANCING
++<<<<<<< HEAD
 +	"pgpromote_success",
 +	"pgpromote_candidate",
++=======
+ 	[I(PGPROMOTE_SUCCESS)]			= "pgpromote_success",
+ 	[I(PGPROMOTE_CANDIDATE)]		= "pgpromote_candidate",
+ 	[I(PGPROMOTE_CANDIDATE_NRL)]		= "pgpromote_candidate_nrl",
++>>>>>>> 337135e6124b (mm: memory-tiering: fix PGPROMOTE_CANDIDATE counting)
  #endif
 -	[I(PGDEMOTE_KSWAPD)]			= "pgdemote_kswapd",
 -	[I(PGDEMOTE_DIRECT)]			= "pgdemote_direct",
 -	[I(PGDEMOTE_KHUGEPAGED)]		= "pgdemote_khugepaged",
 -	[I(PGDEMOTE_PROACTIVE)]			= "pgdemote_proactive",
 +	"pgdemote_kswapd",
 +	"pgdemote_direct",
 +	"pgdemote_khugepaged",
  #ifdef CONFIG_HUGETLB_PAGE
 -	[I(NR_HUGETLB)]				= "nr_hugetlb",
 +	"nr_hugetlb",
  #endif
 -	[I(NR_BALLOON_PAGES)]			= "nr_balloon_pages",
 -#undef I
 -
 +	"nr_balloon_pages",
  	/* system-wide enum vm_stat_item counters */
 -#define I(x) (NR_VM_ZONE_STAT_ITEMS + NR_VM_NUMA_EVENT_ITEMS + \
 -	     NR_VM_NODE_STAT_ITEMS + x)
 -	[I(NR_DIRTY_THRESHOLD)]			= "nr_dirty_threshold",
 -	[I(NR_DIRTY_BG_THRESHOLD)]		= "nr_dirty_background_threshold",
 -	[I(NR_MEMMAP_PAGES)]			= "nr_memmap_pages",
 -	[I(NR_MEMMAP_BOOT_PAGES)]		= "nr_memmap_boot_pages",
 -#undef I
 -
 -#if defined(CONFIG_VM_EVENT_COUNTERS)
 +	"nr_dirty_threshold",
 +	"nr_dirty_background_threshold",
 +	"nr_memmap_pages",
 +	"nr_memmap_boot_pages",
 +
 +#if defined(CONFIG_VM_EVENT_COUNTERS) || defined(CONFIG_MEMCG)
  	/* enum vm_event_item counters */
 -#define I(x) (NR_VM_ZONE_STAT_ITEMS + NR_VM_NUMA_EVENT_ITEMS + \
 -	     NR_VM_NODE_STAT_ITEMS + NR_VM_STAT_ITEMS + x)
 -
 -	[I(PGPGIN)]				= "pgpgin",
 -	[I(PGPGOUT)]				= "pgpgout",
 -	[I(PSWPIN)]				= "pswpin",
 -	[I(PSWPOUT)]				= "pswpout",
 -
 -#define OFF (NR_VM_ZONE_STAT_ITEMS + NR_VM_NUMA_EVENT_ITEMS + \
 -	     NR_VM_NODE_STAT_ITEMS + NR_VM_STAT_ITEMS)
 -	TEXTS_FOR_ZONES(OFF+PGALLOC, "pgalloc")
 -	TEXTS_FOR_ZONES(OFF+ALLOCSTALL, "allocstall")
 -	TEXTS_FOR_ZONES(OFF+PGSCAN_SKIP, "pgskip")
 -#undef OFF
 -
 -	[I(PGFREE)]				= "pgfree",
 -	[I(PGACTIVATE)]				= "pgactivate",
 -	[I(PGDEACTIVATE)]			= "pgdeactivate",
 -	[I(PGLAZYFREE)]				= "pglazyfree",
 -
 -	[I(PGFAULT)]				= "pgfault",
 -	[I(PGMAJFAULT)]				= "pgmajfault",
 -	[I(PGLAZYFREED)]			= "pglazyfreed",
 -
 -	[I(PGREFILL)]				= "pgrefill",
 -	[I(PGREUSE)]				= "pgreuse",
 -	[I(PGSTEAL_KSWAPD)]			= "pgsteal_kswapd",
 -	[I(PGSTEAL_DIRECT)]			= "pgsteal_direct",
 -	[I(PGSTEAL_KHUGEPAGED)]			= "pgsteal_khugepaged",
 -	[I(PGSTEAL_PROACTIVE)]			= "pgsteal_proactive",
 -	[I(PGSCAN_KSWAPD)]			= "pgscan_kswapd",
 -	[I(PGSCAN_DIRECT)]			= "pgscan_direct",
 -	[I(PGSCAN_KHUGEPAGED)]			= "pgscan_khugepaged",
 -	[I(PGSCAN_PROACTIVE)]			= "pgscan_proactive",
 -	[I(PGSCAN_DIRECT_THROTTLE)]		= "pgscan_direct_throttle",
 -	[I(PGSCAN_ANON)]			= "pgscan_anon",
 -	[I(PGSCAN_FILE)]			= "pgscan_file",
 -	[I(PGSTEAL_ANON)]			= "pgsteal_anon",
 -	[I(PGSTEAL_FILE)]			= "pgsteal_file",
 +	"pgpgin",
 +	"pgpgout",
 +	"pswpin",
 +	"pswpout",
 +
 +	TEXTS_FOR_ZONES("pgalloc")
 +	TEXTS_FOR_ZONES("allocstall")
 +	TEXTS_FOR_ZONES("pgskip")
 +
 +	"pgfree",
 +	"pgactivate",
 +	"pgdeactivate",
 +	"pglazyfree",
 +
 +	"pgfault",
 +	"pgmajfault",
 +	"pglazyfreed",
 +
 +	"pgrefill",
 +	"pgreuse",
 +	"pgsteal_kswapd",
 +	"pgsteal_direct",
 +	"pgsteal_khugepaged",
 +	"pgscan_kswapd",
 +	"pgscan_direct",
 +	"pgscan_khugepaged",
 +	"pgscan_direct_throttle",
 +	"pgscan_anon",
 +	"pgscan_file",
 +	"pgsteal_anon",
 +	"pgsteal_file",
  
  #ifdef CONFIG_NUMA
 -	[I(PGSCAN_ZONE_RECLAIM_SUCCESS)]	= "zone_reclaim_success",
 -	[I(PGSCAN_ZONE_RECLAIM_FAILED)]		= "zone_reclaim_failed",
 +	"zone_reclaim_success",
 +	"zone_reclaim_failed",
  #endif
 -	[I(PGINODESTEAL)]			= "pginodesteal",
 -	[I(SLABS_SCANNED)]			= "slabs_scanned",
 -	[I(KSWAPD_INODESTEAL)]			= "kswapd_inodesteal",
 -	[I(KSWAPD_LOW_WMARK_HIT_QUICKLY)]	= "kswapd_low_wmark_hit_quickly",
 -	[I(KSWAPD_HIGH_WMARK_HIT_QUICKLY)]	= "kswapd_high_wmark_hit_quickly",
 -	[I(PAGEOUTRUN)]				= "pageoutrun",
 +	"pginodesteal",
 +	"slabs_scanned",
 +	"kswapd_inodesteal",
 +	"kswapd_low_wmark_hit_quickly",
 +	"kswapd_high_wmark_hit_quickly",
 +	"pageoutrun",
  
 -	[I(PGROTATED)]				= "pgrotated",
 +	"pgrotated",
  
 -	[I(DROP_PAGECACHE)]			= "drop_pagecache",
 -	[I(DROP_SLAB)]				= "drop_slab",
 -	[I(OOM_KILL)]				= "oom_kill",
 +	"drop_pagecache",
 +	"drop_slab",
 +	"oom_kill",
  
  #ifdef CONFIG_NUMA_BALANCING
 -	[I(NUMA_PTE_UPDATES)]			= "numa_pte_updates",
 -	[I(NUMA_HUGE_PTE_UPDATES)]		= "numa_huge_pte_updates",
 -	[I(NUMA_HINT_FAULTS)]			= "numa_hint_faults",
 -	[I(NUMA_HINT_FAULTS_LOCAL)]		= "numa_hint_faults_local",
 -	[I(NUMA_PAGE_MIGRATE)]			= "numa_pages_migrated",
 +	"numa_pte_updates",
 +	"numa_huge_pte_updates",
 +	"numa_hint_faults",
 +	"numa_hint_faults_local",
 +	"numa_pages_migrated",
  #endif
  #ifdef CONFIG_MIGRATION
 -	[I(PGMIGRATE_SUCCESS)]			= "pgmigrate_success",
 -	[I(PGMIGRATE_FAIL)]			= "pgmigrate_fail",
 -	[I(THP_MIGRATION_SUCCESS)]		= "thp_migration_success",
 -	[I(THP_MIGRATION_FAIL)]			= "thp_migration_fail",
 -	[I(THP_MIGRATION_SPLIT)]		= "thp_migration_split",
 +	"pgmigrate_success",
 +	"pgmigrate_fail",
 +	"thp_migration_success",
 +	"thp_migration_fail",
 +	"thp_migration_split",
  #endif
  #ifdef CONFIG_COMPACTION
 -	[I(COMPACTMIGRATE_SCANNED)]		= "compact_migrate_scanned",
 -	[I(COMPACTFREE_SCANNED)]		= "compact_free_scanned",
 -	[I(COMPACTISOLATED)]			= "compact_isolated",
 -	[I(COMPACTSTALL)]			= "compact_stall",
 -	[I(COMPACTFAIL)]			= "compact_fail",
 -	[I(COMPACTSUCCESS)]			= "compact_success",
 -	[I(KCOMPACTD_WAKE)]			= "compact_daemon_wake",
 -	[I(KCOMPACTD_MIGRATE_SCANNED)]		= "compact_daemon_migrate_scanned",
 -	[I(KCOMPACTD_FREE_SCANNED)]		= "compact_daemon_free_scanned",
 +	"compact_migrate_scanned",
 +	"compact_free_scanned",
 +	"compact_isolated",
 +	"compact_stall",
 +	"compact_fail",
 +	"compact_success",
 +	"compact_daemon_wake",
 +	"compact_daemon_migrate_scanned",
 +	"compact_daemon_free_scanned",
  #endif
  
  #ifdef CONFIG_HUGETLB_PAGE
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index da6e4edaaf01..c6274d4beaf5 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -217,7 +217,21 @@ enum node_stat_item {
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 	PGPROMOTE_SUCCESS,	/* promote successfully */
-	PGPROMOTE_CANDIDATE,	/* candidate pages to promote */
+	/**
+	 * Candidate pages for promotion based on hint fault latency.  This
+	 * counter is used to control the promotion rate and adjust the hot
+	 * threshold.
+	 */
+	PGPROMOTE_CANDIDATE,
+	/**
+	 * Not rate-limited (NRL) candidate pages for those can be promoted
+	 * without considering hot threshold because of enough free pages in
+	 * fast-tier node.  These promotions bypass the regular hotness checks
+	 * and do NOT influence the promotion rate-limiter or
+	 * threshold-adjustment logic.
+	 * This is for statistics/monitoring purposes.
+	 */
+	PGPROMOTE_CANDIDATE_NRL,
 #endif
 	/* PGDEMOTE_*: pages demoted */
 	PGDEMOTE_KSWAPD,
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index bb175b70a974..e06a0d6c6fa9 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1936,11 +1936,13 @@ bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,
 		struct pglist_data *pgdat;
 		unsigned long rate_limit;
 		unsigned int latency, th, def_th;
+		long nr = folio_nr_pages(folio);
 
 		pgdat = NODE_DATA(dst_nid);
 		if (pgdat_free_space_enough(pgdat)) {
 			/* workload changed, reset hot threshold */
 			pgdat->nbp_threshold = 0;
+			mod_node_page_state(pgdat, PGPROMOTE_CANDIDATE_NRL, nr);
 			return true;
 		}
 
@@ -1954,8 +1956,7 @@ bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,
 		if (latency >= th)
 			return false;
 
-		return !numa_promotion_rate_limit(pgdat, rate_limit,
-						  folio_nr_pages(folio));
+		return !numa_promotion_rate_limit(pgdat, rate_limit, nr);
 	}
 
 	this_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);
* Unmerged path mm/vmstat.c
