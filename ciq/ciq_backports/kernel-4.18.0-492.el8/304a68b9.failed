xfs: use iomap_valid method to detect stale cached iomaps

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-492.el8
commit-author Dave Chinner <dchinner@redhat.com>
commit 304a68b9c63bbfc1f6e159d68e8892fc54a06067
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-492.el8/304a68b9.failed

Now that iomap supports a mechanism to validate cached iomaps for
buffered write operations, hook it up to the XFS buffered write ops
so that we can avoid data corruptions that result from stale cached
iomaps. See:

https://lore.kernel.org/linux-xfs/20220817093627.GZ3600936@dread.disaster.area/

or the ->iomap_valid() introduction commit for exact details of the
corruption vector.

The validity cookie we store in the iomap is based on the type of
iomap we return. It is expected that the iomap->flags we set in
xfs_bmbt_to_iomap() is not perturbed by the iomap core and are
returned to us in the iomap passed via the .iomap_valid() callback.
This ensures that the validity cookie is always checking the correct
inode fork sequence numbers to detect potential changes that affect
the extent cached by the iomap.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Darrick J. Wong <djwong@kernel.org>
(cherry picked from commit 304a68b9c63bbfc1f6e159d68e8892fc54a06067)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/libxfs/xfs_bmap.c
#	fs/xfs/xfs_aops.c
#	fs/xfs/xfs_iomap.c
#	fs/xfs/xfs_iomap.h
#	fs/xfs/xfs_pnfs.c
diff --cc fs/xfs/libxfs/xfs_bmap.c
index 3d8a1fa87ea2,56b9b7db38bb..000000000000
--- a/fs/xfs/libxfs/xfs_bmap.c
+++ b/fs/xfs/libxfs/xfs_bmap.c
@@@ -4602,7 -4551,8 +4602,12 @@@ xfs_bmapi_convert_delalloc
  	 * the extent.  Just return the real extent at this offset.
  	 */
  	if (!isnullstartblock(bma.got.br_startblock)) {
++<<<<<<< HEAD
 +		xfs_bmbt_to_iomap(ip, iomap, &bma.got, flags);
++=======
+ 		xfs_bmbt_to_iomap(ip, iomap, &bma.got, 0, flags,
+ 				xfs_iomap_inode_sequence(ip, flags));
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  		*seq = READ_ONCE(ifp->if_seq);
  		goto out_trans_cancel;
  	}
@@@ -4649,7 -4600,8 +4654,12 @@@
  	XFS_STATS_INC(mp, xs_xstrat_quick);
  
  	ASSERT(!isnullstartblock(bma.got.br_startblock));
++<<<<<<< HEAD
 +	xfs_bmbt_to_iomap(ip, iomap, &bma.got, flags);
++=======
+ 	xfs_bmbt_to_iomap(ip, iomap, &bma.got, 0, flags,
+ 				xfs_iomap_inode_sequence(ip, flags));
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  	*seq = READ_ONCE(ifp->if_seq);
  
  	if (whichfork == XFS_COW_FORK)
diff --cc fs/xfs/xfs_aops.c
index 60870f8fd954,a22d90af40c8..000000000000
--- a/fs/xfs/xfs_aops.c
+++ b/fs/xfs/xfs_aops.c
@@@ -359,7 -372,7 +359,11 @@@ retry
  	    isnullstartblock(imap.br_startblock))
  		goto allocate_blocks;
  
++<<<<<<< HEAD
 +	xfs_bmbt_to_iomap(ip, &wpc->iomap, &imap, 0);
++=======
+ 	xfs_bmbt_to_iomap(ip, &wpc->iomap, &imap, 0, 0, XFS_WPC(wpc)->data_seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  	trace_xfs_map_blocks_found(ip, offset, count, whichfork, &imap);
  	return 0;
  allocate_blocks:
diff --cc fs/xfs/xfs_iomap.c
index d8cd2583dedb,26ca3cc1a048..000000000000
--- a/fs/xfs/xfs_iomap.c
+++ b/fs/xfs/xfs_iomap.c
@@@ -54,7 -84,9 +85,13 @@@ xfs_bmbt_to_iomap
  	struct xfs_inode	*ip,
  	struct iomap		*iomap,
  	struct xfs_bmbt_irec	*imap,
++<<<<<<< HEAD
 +	u16			flags)
++=======
+ 	unsigned int		mapping_flags,
+ 	u16			iomap_flags,
+ 	u64			sequence_cookie)
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  {
  	struct xfs_mount	*mp = ip->i_mount;
  	struct xfs_buftarg	*target = xfs_inode_buftarg(ip);
@@@ -188,7 -229,9 +228,13 @@@ xfs_iomap_write_direct
  	struct xfs_inode	*ip,
  	xfs_fileoff_t		offset_fsb,
  	xfs_fileoff_t		count_fsb,
++<<<<<<< HEAD
 +	struct xfs_bmbt_irec	*imap)
++=======
+ 	unsigned int		flags,
+ 	struct xfs_bmbt_irec	*imap,
+ 	u64			*seq)
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  {
  	struct xfs_mount	*mp = ip->i_mount;
  	struct xfs_trans	*tp;
@@@ -730,7 -779,8 +777,12 @@@ xfs_direct_write_iomap_begin
  	int			nimaps = 1, error = 0;
  	bool			shared = false;
  	u16			iomap_flags = 0;
++<<<<<<< HEAD
 +	unsigned		lockmode;
++=======
+ 	unsigned int		lockmode = XFS_ILOCK_SHARED;
+ 	u64			seq;
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  	ASSERT(flags & (IOMAP_WRITE | IOMAP_ZERO));
  
@@@ -798,9 -849,10 +850,14 @@@
  			goto out_unlock;
  	}
  
+ 	seq = xfs_iomap_inode_sequence(ip, iomap_flags);
  	xfs_iunlock(ip, lockmode);
  	trace_xfs_iomap_found(ip, offset, length, XFS_DATA_FORK, &imap);
++<<<<<<< HEAD
 +	return xfs_bmbt_to_iomap(ip, iomap, &imap, iomap_flags);
++=======
+ 	return xfs_bmbt_to_iomap(ip, iomap, &imap, flags, iomap_flags, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  allocate_blocks:
  	error = -EAGAIN;
@@@ -826,23 -878,26 +883,42 @@@
  	xfs_iunlock(ip, lockmode);
  
  	error = xfs_iomap_write_direct(ip, offset_fsb, end_fsb - offset_fsb,
++<<<<<<< HEAD
 +			&imap);
++=======
+ 			flags, &imap, &seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  	if (error)
  		return error;
  
  	trace_xfs_iomap_alloc(ip, offset, length, XFS_DATA_FORK, &imap);
++<<<<<<< HEAD
 +	return xfs_bmbt_to_iomap(ip, iomap, &imap, iomap_flags | IOMAP_F_NEW);
++=======
+ 	return xfs_bmbt_to_iomap(ip, iomap, &imap, flags,
+ 				 iomap_flags | IOMAP_F_NEW, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  out_found_cow:
- 	xfs_iunlock(ip, lockmode);
  	length = XFS_FSB_TO_B(mp, cmap.br_startoff + cmap.br_blockcount);
  	trace_xfs_iomap_found(ip, offset, length - offset, XFS_COW_FORK, &cmap);
  	if (imap.br_startblock != HOLESTARTBLOCK) {
++<<<<<<< HEAD
 +		error = xfs_bmbt_to_iomap(ip, srcmap, &imap, 0);
++=======
+ 		seq = xfs_iomap_inode_sequence(ip, 0);
+ 		error = xfs_bmbt_to_iomap(ip, srcmap, &imap, flags, 0, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  		if (error)
- 			return error;
+ 			goto out_unlock;
  	}
++<<<<<<< HEAD
 +	return xfs_bmbt_to_iomap(ip, iomap, &cmap, IOMAP_F_SHARED);
++=======
+ 	seq = xfs_iomap_inode_sequence(ip, IOMAP_F_SHARED);
+ 	xfs_iunlock(ip, lockmode);
+ 	return xfs_bmbt_to_iomap(ip, iomap, &cmap, flags, IOMAP_F_SHARED, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  out_unlock:
  	if (lockmode)
@@@ -873,8 -955,10 +949,13 @@@ xfs_buffered_write_iomap_begin
  	bool			eof = false, cow_eof = false, shared = false;
  	int			allocfork = XFS_DATA_FORK;
  	int			error = 0;
++<<<<<<< HEAD
++=======
+ 	unsigned int		lockmode = XFS_ILOCK_EXCL;
+ 	u64			seq;
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
 -	if (xfs_is_shutdown(mp))
 +	if (XFS_FORCED_SHUTDOWN(mp))
  		return -EIO;
  
  	/* we can't use delayed allocations when using extent size hints */
@@@ -1050,25 -1136,31 +1131,50 @@@ retry
  	 * Flag newly allocated delalloc blocks with IOMAP_F_NEW so we punch
  	 * them out if the write happens to fail.
  	 */
+ 	seq = xfs_iomap_inode_sequence(ip, IOMAP_F_NEW);
  	xfs_iunlock(ip, XFS_ILOCK_EXCL);
  	trace_xfs_iomap_alloc(ip, offset, count, allocfork, &imap);
++<<<<<<< HEAD
 +	return xfs_bmbt_to_iomap(ip, iomap, &imap, IOMAP_F_NEW);
++=======
+ 	return xfs_bmbt_to_iomap(ip, iomap, &imap, flags, IOMAP_F_NEW, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  found_imap:
+ 	seq = xfs_iomap_inode_sequence(ip, 0);
  	xfs_iunlock(ip, XFS_ILOCK_EXCL);
++<<<<<<< HEAD
 +	return xfs_bmbt_to_iomap(ip, iomap, &imap, 0);
++=======
+ 	return xfs_bmbt_to_iomap(ip, iomap, &imap, flags, 0, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  found_cow:
- 	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	seq = xfs_iomap_inode_sequence(ip, 0);
  	if (imap.br_startoff <= offset_fsb) {
++<<<<<<< HEAD
 +		error = xfs_bmbt_to_iomap(ip, srcmap, &imap, 0);
 +		if (error)
 +			return error;
 +	} else {
 +		xfs_trim_extent(&cmap, offset_fsb,
 +				imap.br_startoff - offset_fsb);
 +	}
 +	return xfs_bmbt_to_iomap(ip, iomap, &cmap, IOMAP_F_SHARED);
++=======
+ 		error = xfs_bmbt_to_iomap(ip, srcmap, &imap, flags, 0, seq);
+ 		if (error)
+ 			goto out_unlock;
+ 		seq = xfs_iomap_inode_sequence(ip, IOMAP_F_SHARED);
+ 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 		return xfs_bmbt_to_iomap(ip, iomap, &cmap, flags,
+ 					 IOMAP_F_SHARED, seq);
+ 	}
+ 
+ 	xfs_trim_extent(&cmap, offset_fsb, imap.br_startoff - offset_fsb);
+ 	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+ 	return xfs_bmbt_to_iomap(ip, iomap, &cmap, flags, 0, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  out_unlock:
  	xfs_iunlock(ip, XFS_ILOCK_EXCL);
@@@ -1158,7 -1239,8 +1264,12 @@@ xfs_read_iomap_begin
  	xfs_fileoff_t		end_fsb = xfs_iomap_end_fsb(mp, offset, length);
  	int			nimaps = 1, error = 0;
  	bool			shared = false;
++<<<<<<< HEAD
 +	unsigned		lockmode;
++=======
+ 	unsigned int		lockmode = XFS_ILOCK_SHARED;
+ 	u64			seq;
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  	ASSERT(!(flags & (IOMAP_WRITE | IOMAP_ZERO)));
  
@@@ -1177,7 -1260,8 +1289,12 @@@
  	if (error)
  		return error;
  	trace_xfs_iomap_found(ip, offset, length, XFS_DATA_FORK, &imap);
++<<<<<<< HEAD
 +	return xfs_bmbt_to_iomap(ip, iomap, &imap, shared ? IOMAP_F_SHARED : 0);
++=======
+ 	return xfs_bmbt_to_iomap(ip, iomap, &imap, flags,
+ 				 shared ? IOMAP_F_SHARED : 0, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  }
  
  const struct iomap_ops xfs_read_iomap_ops = {
@@@ -1202,8 -1286,9 +1319,9 @@@ xfs_seek_iomap_begin
  	struct xfs_bmbt_irec	imap, cmap;
  	int			error = 0;
  	unsigned		lockmode;
+ 	u64			seq;
  
 -	if (xfs_is_shutdown(mp))
 +	if (XFS_FORCED_SHUTDOWN(mp))
  		return -EIO;
  
  	lockmode = xfs_ilock_data_map_shared(ip);
@@@ -1236,7 -1321,9 +1354,13 @@@
  		if (data_fsb < cow_fsb + cmap.br_blockcount)
  			end_fsb = min(end_fsb, data_fsb);
  		xfs_trim_extent(&cmap, offset_fsb, end_fsb);
++<<<<<<< HEAD
 +		error = xfs_bmbt_to_iomap(ip, iomap, &cmap, IOMAP_F_SHARED);
++=======
+ 		seq = xfs_iomap_inode_sequence(ip, IOMAP_F_SHARED);
+ 		error = xfs_bmbt_to_iomap(ip, iomap, &cmap, flags,
+ 				IOMAP_F_SHARED, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  		/*
  		 * This is a COW extent, so we must probe the page cache
  		 * because there could be dirty page cache being backed
@@@ -1257,8 -1344,9 +1381,13 @@@
  	imap.br_startblock = HOLESTARTBLOCK;
  	imap.br_state = XFS_EXT_NORM;
  done:
+ 	seq = xfs_iomap_inode_sequence(ip, 0);
  	xfs_trim_extent(&imap, offset_fsb, end_fsb);
++<<<<<<< HEAD
 +	error = xfs_bmbt_to_iomap(ip, iomap, &imap, 0);
++=======
+ 	error = xfs_bmbt_to_iomap(ip, iomap, &imap, flags, 0, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  out_unlock:
  	xfs_iunlock(ip, lockmode);
  	return error;
@@@ -1284,8 -1372,9 +1413,9 @@@ xfs_xattr_iomap_begin
  	struct xfs_bmbt_irec	imap;
  	int			nimaps = 1, error = 0;
  	unsigned		lockmode;
+ 	int			seq;
  
 -	if (xfs_is_shutdown(mp))
 +	if (XFS_FORCED_SHUTDOWN(mp))
  		return -EIO;
  
  	lockmode = xfs_ilock_attr_map_shared(ip);
@@@ -1305,7 -1396,7 +1437,11 @@@ out_unlock
  	if (error)
  		return error;
  	ASSERT(nimaps);
++<<<<<<< HEAD
 +	return xfs_bmbt_to_iomap(ip, iomap, &imap, 0);
++=======
+ 	return xfs_bmbt_to_iomap(ip, iomap, &imap, flags, IOMAP_F_XATTR, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  }
  
  const struct iomap_ops xfs_xattr_iomap_ops = {
diff --cc fs/xfs/xfs_iomap.h
index 7d3703556d0e,4da13440bae9..000000000000
--- a/fs/xfs/xfs_iomap.h
+++ b/fs/xfs/xfs_iomap.h
@@@ -12,13 -12,20 +12,29 @@@ struct xfs_inode
  struct xfs_bmbt_irec;
  
  int xfs_iomap_write_direct(struct xfs_inode *ip, xfs_fileoff_t offset_fsb,
++<<<<<<< HEAD
 +		xfs_fileoff_t count_fsb, struct xfs_bmbt_irec *imap);
++=======
+ 		xfs_fileoff_t count_fsb, unsigned int flags,
+ 		struct xfs_bmbt_irec *imap, u64 *sequence);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  int xfs_iomap_write_unwritten(struct xfs_inode *, xfs_off_t, xfs_off_t, bool);
  xfs_fileoff_t xfs_iomap_eof_align_last_fsb(struct xfs_inode *ip,
  		xfs_fileoff_t end_fsb);
  
++<<<<<<< HEAD
 +int xfs_bmbt_to_iomap(struct xfs_inode *, struct iomap *,
 +		struct xfs_bmbt_irec *, u16);
++=======
+ u64 xfs_iomap_inode_sequence(struct xfs_inode *ip, u16 iomap_flags);
+ int xfs_bmbt_to_iomap(struct xfs_inode *ip, struct iomap *iomap,
+ 		struct xfs_bmbt_irec *imap, unsigned int mapping_flags,
+ 		u16 iomap_flags, u64 sequence_cookie);
+ 
+ int xfs_zero_range(struct xfs_inode *ip, loff_t pos, loff_t len,
+ 		bool *did_zero);
+ int xfs_truncate_page(struct xfs_inode *ip, loff_t pos, bool *did_zero);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  
  static inline xfs_filblks_t
  xfs_aligned_fsb_count(
diff --cc fs/xfs/xfs_pnfs.c
index 33c5b166a191,38d23f0e703a..000000000000
--- a/fs/xfs/xfs_pnfs.c
+++ b/fs/xfs/xfs_pnfs.c
@@@ -91,8 -125,9 +91,9 @@@ xfs_fs_map_blocks
  	int			nimaps = 1;
  	uint			lock_flags;
  	int			error = 0;
+ 	u64			seq;
  
 -	if (xfs_is_shutdown(mp))
 +	if (XFS_FORCED_SHUTDOWN(mp))
  		return -EIO;
  
  	/*
@@@ -155,7 -191,7 +157,11 @@@
  		xfs_iunlock(ip, lock_flags);
  
  		error = xfs_iomap_write_direct(ip, offset_fsb,
++<<<<<<< HEAD
 +				end_fsb - offset_fsb, &imap);
++=======
+ 				end_fsb - offset_fsb, 0, &imap, &seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  		if (error)
  			goto out_unlock;
  
@@@ -175,7 -211,7 +181,11 @@@
  	}
  	xfs_iunlock(ip, XFS_IOLOCK_EXCL);
  
++<<<<<<< HEAD
 +	error = xfs_bmbt_to_iomap(ip, iomap, &imap, 0);
++=======
+ 	error = xfs_bmbt_to_iomap(ip, iomap, &imap, 0, 0, seq);
++>>>>>>> 304a68b9c63b (xfs: use iomap_valid method to detect stale cached iomaps)
  	*device_generation = mp->m_generation;
  	return error;
  out_unlock:
* Unmerged path fs/xfs/libxfs/xfs_bmap.c
* Unmerged path fs/xfs/xfs_aops.c
* Unmerged path fs/xfs/xfs_iomap.c
* Unmerged path fs/xfs/xfs_iomap.h
* Unmerged path fs/xfs/xfs_pnfs.c
