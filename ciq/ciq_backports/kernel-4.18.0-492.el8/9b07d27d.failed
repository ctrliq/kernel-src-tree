swiotlb: mark swiotlb_memblock_alloc() as __init

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-492.el8
commit-author Randy Dunlap <rdunlap@infradead.org>
commit 9b07d27d0fbb7f7441aa986859a0f53ec93a0335
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-492.el8/9b07d27d.failed

swiotlb_memblock_alloc() calls memblock_alloc(), which calls
(__init) memblock_alloc_try_nid(). However, swiotlb_membloc_alloc()
can be marked as __init since it is only called by swiotlb_init_remap(),
which is already marked as __init. This prevents a modpost build
warning/error:

WARNING: modpost: vmlinux.o: section mismatch in reference: swiotlb_memblock_alloc (section: .text) -> memblock_alloc_try_nid (section: .init.text)
WARNING: modpost: vmlinux.o: section mismatch in reference: swiotlb_memblock_alloc (section: .text) -> memblock_alloc_try_nid (section: .init.text)

This fixes the build warning/error seen on ARM64, PPC64, S390, i386,
and x86_64.

Fixes: 8d58aa484920 ("swiotlb: reduce the swiotlb buffer size on allocation failure")
	Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
	Cc: Alexey Kardashevskiy <aik@amd.com>
	Cc: Christoph Hellwig <hch@lst.de>
	Cc: iommu@lists.linux.dev
	Cc: Mike Rapoport <rppt@kernel.org>
	Cc: linux-mm@kvack.org
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 9b07d27d0fbb7f7441aa986859a0f53ec93a0335)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index 689f7d1d18b6,03e3251cd9d2..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -287,6 -292,38 +287,41 @@@ static void swiotlb_init_io_tlb_mem(str
  	return;
  }
  
++<<<<<<< HEAD
++=======
+ static void __init *swiotlb_memblock_alloc(unsigned long nslabs,
+ 		unsigned int flags,
+ 		int (*remap)(void *tlb, unsigned long nslabs))
+ {
+ 	size_t bytes = PAGE_ALIGN(nslabs << IO_TLB_SHIFT);
+ 	void *tlb;
+ 
+ 	/*
+ 	 * By default allocate the bounce buffer memory from low memory, but
+ 	 * allow to pick a location everywhere for hypervisors with guest
+ 	 * memory encryption.
+ 	 */
+ 	if (flags & SWIOTLB_ANY)
+ 		tlb = memblock_alloc(bytes, PAGE_SIZE);
+ 	else
+ 		tlb = memblock_alloc_low(bytes, PAGE_SIZE);
+ 
+ 	if (!tlb) {
+ 		pr_warn("%s: Failed to allocate %zu bytes tlb structure\n",
+ 			__func__, bytes);
+ 		return NULL;
+ 	}
+ 
+ 	if (remap && remap(tlb, nslabs) < 0) {
+ 		memblock_free(tlb, PAGE_ALIGN(bytes));
+ 		pr_warn("%s: Failed to remap %zu bytes\n", __func__, bytes);
+ 		return NULL;
+ 	}
+ 
+ 	return tlb;
+ }
+ 
++>>>>>>> 9b07d27d0fbb (swiotlb: mark swiotlb_memblock_alloc() as __init)
  /*
   * Statically reserve bounce buffer space and initialize bounce buffer data
   * structures for the software IO TLB used to implement the DMA API.
* Unmerged path kernel/dma/swiotlb.c
