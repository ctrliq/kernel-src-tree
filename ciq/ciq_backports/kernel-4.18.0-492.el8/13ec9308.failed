KVM: arm64: Retry fault if vma_lookup() results become invalid

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-492.el8
commit-author David Matlack <dmatlack@google.com>
commit 13ec9308a85702af7c31f3638a2720863848a7f2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-492.el8/13ec9308.failed

Read mmu_invalidate_seq before dropping the mmap_lock so that KVM can
detect if the results of vma_lookup() (e.g. vma_shift) become stale
before it acquires kvm->mmu_lock. This fixes a theoretical bug where a
VMA could be changed by userspace after vma_lookup() and before KVM
reads the mmu_invalidate_seq, causing KVM to install page table entries
based on a (possibly) no-longer-valid vma_shift.

Re-order the MMU cache top-up to earlier in user_mem_abort() so that it
is not done after KVM has read mmu_invalidate_seq (i.e. so as to avoid
inducing spurious fault retries).

This bug has existed since KVM/ARM's inception. It's unlikely that any
sane userspace currently modifies VMAs in such a way as to trigger this
race. And even with directed testing I was unable to reproduce it. But a
sufficiently motivated host userspace might be able to exploit this
race.

Fixes: 94f8e6418d39 ("KVM: ARM: Handle guest faults in KVM")
	Cc: stable@vger.kernel.org
	Reported-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: David Matlack <dmatlack@google.com>
	Reviewed-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20230313235454.2964067-1-dmatlack@google.com
	Signed-off-by: Oliver Upton <oliver.upton@linux.dev>
(cherry picked from commit 13ec9308a85702af7c31f3638a2720863848a7f2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kvm/mmu.c
diff --cc arch/arm64/kvm/mmu.c
index c48424af1146,f54408355d1d..000000000000
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@@ -1689,7 -1217,24 +1689,28 @@@ static int user_mem_abort(struct kvm_vc
  		return -EFAULT;
  	}
  
++<<<<<<< HEAD
 +	/* Let's check if we will get back a huge page backed by hugetlbfs */
++=======
+ 	/*
+ 	 * Permission faults just need to update the existing leaf entry,
+ 	 * and so normally don't require allocations from the memcache. The
+ 	 * only exception to this is when dirty logging is enabled at runtime
+ 	 * and a write fault needs to collapse a block entry into a table.
+ 	 */
+ 	if (fault_status != ESR_ELx_FSC_PERM ||
+ 	    (logging_active && write_fault)) {
+ 		ret = kvm_mmu_topup_memory_cache(memcache,
+ 						 kvm_mmu_cache_min_pages(kvm));
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	/*
+ 	 * Let's check if we will get back a huge page backed by hugetlbfs, or
+ 	 * get block mapping for device MMIO region.
+ 	 */
++>>>>>>> 13ec9308a857 (KVM: arm64: Retry fault if vma_lookup() results become invalid)
  	mmap_read_lock(current->mm);
  	vma = vma_lookup(current->mm, hva);
  	if (unlikely(!vma)) {
@@@ -1698,54 -1243,59 +1719,98 @@@
  		return -EFAULT;
  	}
  
 +	if (is_vm_hugetlb_page(vma))
 +		vma_shift = huge_page_shift(hstate_vma(vma));
 +	else
 +		vma_shift = PAGE_SHIFT;
 +
 +	vma_pagesize = 1ULL << vma_shift;
 +	if (logging_active ||
 +	    (vma->vm_flags & VM_PFNMAP) ||
 +	    !fault_supports_stage2_huge_mapping(memslot, hva, vma_pagesize)) {
 +		force_pte = true;
 +		vma_pagesize = PAGE_SIZE;
 +		vma_shift = PAGE_SHIFT;
 +	}
 +
  	/*
 -	 * logging_active is guaranteed to never be true for VM_PFNMAP
 -	 * memslots.
 +	 * The stage2 has a minimum of 2 level table (For arm64 see
 +	 * kvm_arm_setup_stage2()). Hence, we are guaranteed that we can
 +	 * use PMD_SIZE huge mappings (even when the PMD is folded into PGD).
 +	 * As for PUD huge maps, we must make sure that we have at least
 +	 * 3 levels, i.e, PMD is not folded.
  	 */
++<<<<<<< HEAD
 +	if (vma_pagesize == PMD_SIZE ||
 +	    (vma_pagesize == PUD_SIZE && kvm_stage2_has_pmd(kvm)))
 +		gfn = (fault_ipa & huge_page_mask(hstate_vma(vma))) >> PAGE_SHIFT;
 +	mmap_read_unlock(current->mm);
 +
 +	/* We need minimum second+third level pages */
 +	ret = kvm_mmu_topup_memory_cache(memcache, kvm_mmu_cache_min_pages(kvm));
 +	if (ret)
 +		return ret;
 +
 +	mmu_seq = vcpu->kvm->mmu_notifier_seq;
 +	/*
 +	 * Ensure the read of mmu_notifier_seq happens before we call
 +	 * gfn_to_pfn_prot (which calls get_user_pages), so that we don't risk
 +	 * the page we just got a reference to gets unmapped before we have a
 +	 * chance to grab the mmu_lock, which ensure that if the page gets
 +	 * unmapped afterwards, the call to kvm_unmap_gfn will take it away
 +	 * from us again properly. This smp_rmb() interacts with the smp_wmb()
 +	 * in kvm_mmu_notifier_invalidate_<page|range_end>.
++=======
+ 	if (logging_active) {
+ 		force_pte = true;
+ 		vma_shift = PAGE_SHIFT;
+ 	} else {
+ 		vma_shift = get_vma_page_shift(vma, hva);
+ 	}
+ 
+ 	switch (vma_shift) {
+ #ifndef __PAGETABLE_PMD_FOLDED
+ 	case PUD_SHIFT:
+ 		if (fault_supports_stage2_huge_mapping(memslot, hva, PUD_SIZE))
+ 			break;
+ 		fallthrough;
+ #endif
+ 	case CONT_PMD_SHIFT:
+ 		vma_shift = PMD_SHIFT;
+ 		fallthrough;
+ 	case PMD_SHIFT:
+ 		if (fault_supports_stage2_huge_mapping(memslot, hva, PMD_SIZE))
+ 			break;
+ 		fallthrough;
+ 	case CONT_PTE_SHIFT:
+ 		vma_shift = PAGE_SHIFT;
+ 		force_pte = true;
+ 		fallthrough;
+ 	case PAGE_SHIFT:
+ 		break;
+ 	default:
+ 		WARN_ONCE(1, "Unknown vma_shift %d", vma_shift);
+ 	}
+ 
+ 	vma_pagesize = 1UL << vma_shift;
+ 	if (vma_pagesize == PMD_SIZE || vma_pagesize == PUD_SIZE)
+ 		fault_ipa &= ~(vma_pagesize - 1);
+ 
+ 	gfn = fault_ipa >> PAGE_SHIFT;
+ 
+ 	/*
+ 	 * Read mmu_invalidate_seq so that KVM can detect if the results of
+ 	 * vma_lookup() or __gfn_to_pfn_memslot() become stale prior to
+ 	 * acquiring kvm->mmu_lock.
++>>>>>>> 13ec9308a857 (KVM: arm64: Retry fault if vma_lookup() results become invalid)
  	 *
- 	 * Besides, __gfn_to_pfn_memslot() instead of gfn_to_pfn_prot() is
- 	 * used to avoid unnecessary overhead introduced to locate the memory
- 	 * slot because it's always fixed even @gfn is adjusted for huge pages.
+ 	 * Rely on mmap_read_unlock() for an implicit smp_rmb(), which pairs
+ 	 * with the smp_wmb() in kvm_mmu_invalidate_end().
  	 */
- 	smp_rmb();
+ 	mmu_seq = vcpu->kvm->mmu_invalidate_seq;
+ 	mmap_read_unlock(current->mm);
  
 -	pfn = __gfn_to_pfn_memslot(memslot, gfn, false, false, NULL,
 +	pfn = __gfn_to_pfn_memslot(memslot, gfn, false, NULL,
  				   write_fault, &writable, NULL);
  	if (pfn == KVM_PFN_ERR_HWPOISON) {
  		kvm_send_hwpoison_signal(hva, vma_shift);
* Unmerged path arch/arm64/kvm/mmu.c
