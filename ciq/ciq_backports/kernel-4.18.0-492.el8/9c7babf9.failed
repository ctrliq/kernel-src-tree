xfs,iomap: move delalloc punching to iomap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-492.el8
commit-author Dave Chinner <dchinner@redhat.com>
commit 9c7babf94a0d686b552e53aded8d4703d1b8b92b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-492.el8/9c7babf9.failed

Because that's what Christoph wants for this error handling path
only XFS uses.

It requires a new iomap export for handling errors over delalloc
ranges. This is basically the XFS code as is stands, but even though
Christoph wants this as iomap funcitonality, we still have 
to call it from the filesystem specific ->iomap_end callback, and
call into the iomap code with yet another filesystem specific
callback to punch the delalloc extent within the defined ranges.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Darrick J. Wong <djwong@kernel.org>
(cherry picked from commit 9c7babf94a0d686b552e53aded8d4703d1b8b92b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_iomap.c
#	include/linux/iomap.h
diff --cc fs/xfs/xfs_iomap.c
index d8cd2583dedb,ea96e8a34868..000000000000
--- a/fs/xfs/xfs_iomap.c
+++ b/fs/xfs/xfs_iomap.c
@@@ -1076,6 -1121,20 +1076,23 @@@ out_unlock
  }
  
  static int
++<<<<<<< HEAD
++=======
+ xfs_buffered_write_delalloc_punch(
+ 	struct inode		*inode,
+ 	loff_t			offset,
+ 	loff_t			length)
+ {
+ 	struct xfs_mount	*mp = XFS_M(inode->i_sb);
+ 	xfs_fileoff_t		start_fsb = XFS_B_TO_FSBT(mp, offset);
+ 	xfs_fileoff_t		end_fsb = XFS_B_TO_FSB(mp, offset + length);
+ 
+ 	return xfs_bmap_punch_delalloc_range(XFS_I(inode), start_fsb,
+ 				end_fsb - start_fsb);
+ }
+ 
+ static int
++>>>>>>> 9c7babf94a0d (xfs,iomap: move delalloc punching to iomap)
  xfs_buffered_write_iomap_end(
  	struct inode		*inode,
  	loff_t			offset,
@@@ -1084,14 -1143,9 +1101,17 @@@
  	unsigned		flags,
  	struct iomap		*iomap)
  {
++<<<<<<< HEAD
 +	struct xfs_inode	*ip = XFS_I(inode);
 +	struct xfs_mount	*mp = ip->i_mount;
 +	xfs_fileoff_t		start_fsb;
 +	xfs_fileoff_t		end_fsb;
 +	int			error = 0;
++=======
++>>>>>>> 9c7babf94a0d (xfs,iomap: move delalloc punching to iomap)
  
- 	if (iomap->type != IOMAP_DELALLOC)
- 		return 0;
+ 	struct xfs_mount	*mp = XFS_M(inode->i_sb);
+ 	int			error;
  
  	/*
  	 * Behave as if the write failed if drop writes is enabled. Set the NEW
@@@ -1102,38 -1156,13 +1122,47 @@@
  		written = 0;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * start_fsb refers to the first unused block after a short write. If
 +	 * nothing was written, round offset down to point at the first block in
 +	 * the range.
 +	 */
 +	if (unlikely(!written))
 +		start_fsb = XFS_B_TO_FSBT(mp, offset);
 +	else
 +		start_fsb = XFS_B_TO_FSB(mp, offset + written);
 +	end_fsb = XFS_B_TO_FSB(mp, offset + length);
 +
 +	/*
 +	 * Trim delalloc blocks if they were allocated by this write and we
 +	 * didn't manage to write the whole range.
 +	 *
 +	 * We don't need to care about racing delalloc as we hold i_mutex
 +	 * across the reserve/allocate/unreserve calls. If there are delalloc
 +	 * blocks in the range, they are ours.
 +	 */
 +	if ((iomap->flags & IOMAP_F_NEW) && start_fsb < end_fsb) {
 +		truncate_pagecache_range(VFS_I(ip), XFS_FSB_TO_B(mp, start_fsb),
 +					 XFS_FSB_TO_B(mp, end_fsb) - 1);
 +
 +		error = xfs_bmap_punch_delalloc_range(ip, start_fsb,
 +					       end_fsb - start_fsb);
 +		if (error && !XFS_FORCED_SHUTDOWN(mp)) {
 +			xfs_alert(mp, "%s: unable to clean up ino %lld",
 +				__func__, ip->i_ino);
 +			return error;
 +		}
++=======
+ 	error = iomap_file_buffered_write_punch_delalloc(inode, iomap, offset,
+ 			length, written, &xfs_buffered_write_delalloc_punch);
+ 	if (error && !xfs_is_shutdown(mp)) {
+ 		xfs_alert(mp, "%s: unable to clean up ino 0x%llx",
+ 			__func__, XFS_I(inode)->i_ino);
+ 		return error;
++>>>>>>> 9c7babf94a0d (xfs,iomap: move delalloc punching to iomap)
  	}
 +
  	return 0;
  }
  
diff --cc include/linux/iomap.h
index 293e72d14e21,0698c4b8ce0e..000000000000
--- a/include/linux/iomap.h
+++ b/include/linux/iomap.h
@@@ -226,20 -226,15 +226,28 @@@ static inline struct iomap *iomap_iter_
  
  ssize_t iomap_file_buffered_write(struct kiocb *iocb, struct iov_iter *from,
  		const struct iomap_ops *ops);
++<<<<<<< HEAD
 +int iomap_readpage(struct page *page, const struct iomap_ops *ops);
++=======
+ int iomap_file_buffered_write_punch_delalloc(struct inode *inode,
+ 		struct iomap *iomap, loff_t pos, loff_t length, ssize_t written,
+ 		int (*punch)(struct inode *inode, loff_t pos, loff_t length));
+ 
+ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops);
++>>>>>>> 9c7babf94a0d (xfs,iomap: move delalloc punching to iomap)
  void iomap_readahead(struct readahead_control *, const struct iomap_ops *ops);
 -bool iomap_is_partially_uptodate(struct folio *, size_t from, size_t count);
 -bool iomap_release_folio(struct folio *folio, gfp_t gfp_flags);
 -void iomap_invalidate_folio(struct folio *folio, size_t offset, size_t len);
 +int iomap_set_page_dirty(struct page *page);
 +int iomap_is_partially_uptodate(struct page *page, unsigned long from,
 +		unsigned long count);
 +int iomap_releasepage(struct page *page, gfp_t gfp_mask);
 +void iomap_invalidatepage(struct page *page, unsigned int offset,
 +		unsigned int len);
 +#ifdef CONFIG_MIGRATION
 +int iomap_migrate_page(struct address_space *mapping, struct page *newpage,
 +		struct page *page, enum migrate_mode mode);
 +#else
 +#define iomap_migrate_page NULL
 +#endif
  int iomap_file_unshare(struct inode *inode, loff_t pos, loff_t len,
  		const struct iomap_ops *ops);
  int iomap_zero_range(struct inode *inode, loff_t pos, loff_t len,
diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c
index a9fecbda715a..e48d7a6e8744 100644
--- a/fs/iomap/buffered-io.c
+++ b/fs/iomap/buffered-io.c
@@ -891,6 +891,66 @@ iomap_file_buffered_write(struct kiocb *iocb, struct iov_iter *i,
 }
 EXPORT_SYMBOL_GPL(iomap_file_buffered_write);
 
+/*
+ * When a short write occurs, the filesystem may need to remove reserved space
+ * that was allocated in ->iomap_begin from it's ->iomap_end method. For
+ * filesystems that use delayed allocation, we need to punch out delalloc
+ * extents from the range that are not dirty in the page cache. As the write can
+ * race with page faults, there can be dirty pages over the delalloc extent
+ * outside the range of a short write but still within the delalloc extent
+ * allocated for this iomap.
+ *
+ * This function uses [start_byte, end_byte) intervals (i.e. open ended) to
+ * simplify range iterations, but converts them back to {offset,len} tuples for
+ * the punch callback.
+ */
+int iomap_file_buffered_write_punch_delalloc(struct inode *inode,
+		struct iomap *iomap, loff_t pos, loff_t length,
+		ssize_t written,
+		int (*punch)(struct inode *inode, loff_t pos, loff_t length))
+{
+	loff_t			start_byte;
+	loff_t			end_byte;
+	int			blocksize = i_blocksize(inode);
+	int			error = 0;
+
+	if (iomap->type != IOMAP_DELALLOC)
+		return 0;
+
+	/* If we didn't reserve the blocks, we're not allowed to punch them. */
+	if (!(iomap->flags & IOMAP_F_NEW))
+		return 0;
+
+	/*
+	 * start_byte refers to the first unused block after a short write. If
+	 * nothing was written, round offset down to point at the first block in
+	 * the range.
+	 */
+	if (unlikely(!written))
+		start_byte = round_down(pos, blocksize);
+	else
+		start_byte = round_up(pos + written, blocksize);
+	end_byte = round_up(pos + length, blocksize);
+
+	/* Nothing to do if we've written the entire delalloc extent */
+	if (start_byte >= end_byte)
+		return 0;
+
+	/*
+	 * Lock the mapping to avoid races with page faults re-instantiating
+	 * folios and dirtying them via ->page_mkwrite between the page cache
+	 * truncation and the delalloc extent removal. Failing to do this can
+	 * leave dirty pages with no space reservation in the cache.
+	 */
+	filemap_invalidate_lock(inode->i_mapping);
+	truncate_pagecache_range(inode, start_byte, end_byte - 1);
+	error = punch(inode, start_byte, end_byte - start_byte);
+	filemap_invalidate_unlock(inode->i_mapping);
+
+	return error;
+}
+EXPORT_SYMBOL_GPL(iomap_file_buffered_write_punch_delalloc);
+
 static loff_t iomap_unshare_iter(struct iomap_iter *iter)
 {
 	struct iomap *iomap = &iter->iomap;
* Unmerged path fs/xfs/xfs_iomap.c
* Unmerged path include/linux/iomap.h
