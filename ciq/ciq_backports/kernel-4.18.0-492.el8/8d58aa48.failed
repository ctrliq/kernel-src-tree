swiotlb: reduce the swiotlb buffer size on allocation failure

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-492.el8
commit-author Alexey Kardashevskiy <aik@amd.com>
commit 8d58aa484920c4f9be4834a7aeb446cdced21a37
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-492.el8/8d58aa48.failed

At the moment the AMD encrypted platform reserves 6% of RAM for SWIOTLB
or 1GB, whichever is less. However it is possible that there is no block
big enough in the low memory which make SWIOTLB allocation fail and
the kernel continues without DMA. In such case a VM hangs on DMA.

This moves alloc+remap to a helper and calls it from a loop where
the size is halved on each iteration.

This updates default_nslabs on successful allocation which looks like
an oversight as not doing so should have broken callers of
swiotlb_size_or_default().

	Signed-off-by: Alexey Kardashevskiy <aik@amd.com>
	Reviewed-by: Pankaj Gupta <pankaj.gupta@amd.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 8d58aa484920c4f9be4834a7aeb446cdced21a37)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index 3536691bc829,a34c38bbe28f..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -321,30 -356,16 +351,43 @@@ void __init swiotlb_init_remap(bool add
  		swiotlb_adjust_nareas(num_possible_cpus());
  
  	nslabs = default_nslabs;
++<<<<<<< HEAD
 +	/*
 +	 * By default allocate the bounce buffer memory from low memory, but
 +	 * allow to pick a location everywhere for hypervisors with guest
 +	 * memory encryption.
 +	 */
 +retry:
 +	bytes = PAGE_ALIGN(nslabs << IO_TLB_SHIFT);
 +	if (flags & SWIOTLB_ANY)
 +		tlb = memblock_alloc_nopanic(bytes, PAGE_SIZE);
 +	else
 +		tlb = memblock_alloc_low_nopanic(bytes, PAGE_SIZE);
 +	if (!tlb) {
 +		pr_warn("%s: failed to allocate tlb structure\n", __func__);
 +		return;
 +	}
 +
 +	if (remap && remap(tlb, nslabs) < 0) {
 +		memblock_free(__pa(tlb), PAGE_ALIGN(bytes));
 +
 +		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
 +		if (nslabs < IO_TLB_MIN_SLABS)
 +			panic("%s: Failed to remap %zu bytes\n",
 +			      __func__, bytes);
 +		goto retry;
++=======
+ 	while ((tlb = swiotlb_memblock_alloc(nslabs, flags, remap)) == NULL) {
+ 		if (nslabs <= IO_TLB_MIN_SLABS)
+ 			return;
+ 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
+ 	}
+ 
+ 	if (default_nslabs != nslabs) {
+ 		pr_info("SWIOTLB bounce buffer size adjusted %lu -> %lu slabs",
+ 			default_nslabs, nslabs);
+ 		default_nslabs = nslabs;
++>>>>>>> 8d58aa484920 (swiotlb: reduce the swiotlb buffer size on allocation failure)
  	}
  
  	alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
* Unmerged path kernel/dma/swiotlb.c
