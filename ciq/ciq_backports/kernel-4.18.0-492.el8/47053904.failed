KVM: arm64: timers: Convert per-vcpu virtual offset to a global value

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-492.el8
commit-author Marc Zyngier <maz@kernel.org>
commit 47053904e18282af4525a02e3e0f519f014fc7f9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-492.el8/47053904.failed

Having a per-vcpu virtual offset is a pain. It needs to be synchronized
on each update, and expands badly to a setup where different timers can
have different offsets, or have composite offsets (as with NV).

So let's start by replacing the use of the CNTVOFF_EL2 shadow register
(which we want to reclaim for NV anyway), and make the virtual timer
carry a pointer to a VM-wide offset.

This simplifies the code significantly. It also addresses two terrible bugs:

- The use of CNTVOFF_EL2 leads to some nice offset corruption
  when the sysreg gets reset, as reported by Joey.

- The kvm mutex is taken from a vcpu ioctl, which goes against
  the locking rules...

	Reported-by: Joey Gouly <joey.gouly@arm.com>
	Reviewed-by: Reiji Watanabe <reijiw@google.com>
	Signed-off-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20230224173915.GA17407@e124191.cambridge.arm.com
	Tested-by: Joey Gouly <joey.gouly@arm.com>
Link: https://lore.kernel.org/r/20230224191640.3396734-1-maz@kernel.org
	Signed-off-by: Oliver Upton <oliver.upton@linux.dev>
(cherry picked from commit 47053904e18282af4525a02e3e0f519f014fc7f9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kvm/hypercalls.c
diff --cc arch/arm64/kvm/hypercalls.c
index c0c0ef61fae4,5da884e11337..000000000000
--- a/arch/arm64/kvm/hypercalls.c
+++ b/arch/arm64/kvm/hypercalls.c
@@@ -9,10 -9,123 +9,125 @@@
  #include <kvm/arm_hypercalls.h>
  #include <kvm/arm_psci.h>
  
++<<<<<<< HEAD
++=======
+ #define KVM_ARM_SMCCC_STD_FEATURES				\
+ 	GENMASK(KVM_REG_ARM_STD_BMAP_BIT_COUNT - 1, 0)
+ #define KVM_ARM_SMCCC_STD_HYP_FEATURES				\
+ 	GENMASK(KVM_REG_ARM_STD_HYP_BMAP_BIT_COUNT - 1, 0)
+ #define KVM_ARM_SMCCC_VENDOR_HYP_FEATURES			\
+ 	GENMASK(KVM_REG_ARM_VENDOR_HYP_BMAP_BIT_COUNT - 1, 0)
+ 
+ static void kvm_ptp_get_time(struct kvm_vcpu *vcpu, u64 *val)
+ {
+ 	struct system_time_snapshot systime_snapshot;
+ 	u64 cycles = ~0UL;
+ 	u32 feature;
+ 
+ 	/*
+ 	 * system time and counter value must captured at the same
+ 	 * time to keep consistency and precision.
+ 	 */
+ 	ktime_get_snapshot(&systime_snapshot);
+ 
+ 	/*
+ 	 * This is only valid if the current clocksource is the
+ 	 * architected counter, as this is the only one the guest
+ 	 * can see.
+ 	 */
+ 	if (systime_snapshot.cs_id != CSID_ARM_ARCH_COUNTER)
+ 		return;
+ 
+ 	/*
+ 	 * The guest selects one of the two reference counters
+ 	 * (virtual or physical) with the first argument of the SMCCC
+ 	 * call. In case the identifier is not supported, error out.
+ 	 */
+ 	feature = smccc_get_arg1(vcpu);
+ 	switch (feature) {
+ 	case KVM_PTP_VIRT_COUNTER:
+ 		cycles = systime_snapshot.cycles - vcpu->kvm->arch.timer_data.voffset;
+ 		break;
+ 	case KVM_PTP_PHYS_COUNTER:
+ 		cycles = systime_snapshot.cycles;
+ 		break;
+ 	default:
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * This relies on the top bit of val[0] never being set for
+ 	 * valid values of system time, because that is *really* far
+ 	 * in the future (about 292 years from 1970, and at that stage
+ 	 * nobody will give a damn about it).
+ 	 */
+ 	val[0] = upper_32_bits(systime_snapshot.real);
+ 	val[1] = lower_32_bits(systime_snapshot.real);
+ 	val[2] = upper_32_bits(cycles);
+ 	val[3] = lower_32_bits(cycles);
+ }
+ 
+ static bool kvm_hvc_call_default_allowed(u32 func_id)
+ {
+ 	switch (func_id) {
+ 	/*
+ 	 * List of function-ids that are not gated with the bitmapped
+ 	 * feature firmware registers, and are to be allowed for
+ 	 * servicing the call by default.
+ 	 */
+ 	case ARM_SMCCC_VERSION_FUNC_ID:
+ 	case ARM_SMCCC_ARCH_FEATURES_FUNC_ID:
+ 		return true;
+ 	default:
+ 		/* PSCI 0.2 and up is in the 0:0x1f range */
+ 		if (ARM_SMCCC_OWNER_NUM(func_id) == ARM_SMCCC_OWNER_STANDARD &&
+ 		    ARM_SMCCC_FUNC_NUM(func_id) <= 0x1f)
+ 			return true;
+ 
+ 		/*
+ 		 * KVM's PSCI 0.1 doesn't comply with SMCCC, and has
+ 		 * its own function-id base and range
+ 		 */
+ 		if (func_id >= KVM_PSCI_FN(0) && func_id <= KVM_PSCI_FN(3))
+ 			return true;
+ 
+ 		return false;
+ 	}
+ }
+ 
+ static bool kvm_hvc_call_allowed(struct kvm_vcpu *vcpu, u32 func_id)
+ {
+ 	struct kvm_smccc_features *smccc_feat = &vcpu->kvm->arch.smccc_feat;
+ 
+ 	switch (func_id) {
+ 	case ARM_SMCCC_TRNG_VERSION:
+ 	case ARM_SMCCC_TRNG_FEATURES:
+ 	case ARM_SMCCC_TRNG_GET_UUID:
+ 	case ARM_SMCCC_TRNG_RND32:
+ 	case ARM_SMCCC_TRNG_RND64:
+ 		return test_bit(KVM_REG_ARM_STD_BIT_TRNG_V1_0,
+ 				&smccc_feat->std_bmap);
+ 	case ARM_SMCCC_HV_PV_TIME_FEATURES:
+ 	case ARM_SMCCC_HV_PV_TIME_ST:
+ 		return test_bit(KVM_REG_ARM_STD_HYP_BIT_PV_TIME,
+ 				&smccc_feat->std_hyp_bmap);
+ 	case ARM_SMCCC_VENDOR_HYP_KVM_FEATURES_FUNC_ID:
+ 	case ARM_SMCCC_VENDOR_HYP_CALL_UID_FUNC_ID:
+ 		return test_bit(KVM_REG_ARM_VENDOR_HYP_BIT_FUNC_FEAT,
+ 				&smccc_feat->vendor_hyp_bmap);
+ 	case ARM_SMCCC_VENDOR_HYP_KVM_PTP_FUNC_ID:
+ 		return test_bit(KVM_REG_ARM_VENDOR_HYP_BIT_PTP,
+ 				&smccc_feat->vendor_hyp_bmap);
+ 	default:
+ 		return kvm_hvc_call_default_allowed(func_id);
+ 	}
+ }
+ 
++>>>>>>> 47053904e182 (KVM: arm64: timers: Convert per-vcpu virtual offset to a global value)
  int kvm_hvc_call_handler(struct kvm_vcpu *vcpu)
  {
 -	struct kvm_smccc_features *smccc_feat = &vcpu->kvm->arch.smccc_feat;
  	u32 func_id = smccc_get_function(vcpu);
 -	u64 val[4] = {SMCCC_RET_NOT_SUPPORTED};
 +	long val = SMCCC_RET_NOT_SUPPORTED;
  	u32 feature;
  	gpa_t gpa;
  
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 1ff9818e9882..7f58bd229b69 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -96,6 +96,9 @@ struct kvm_arch {
 	/* Interrupt controller */
 	struct vgic_dist	vgic;
 
+	/* Timers */
+	struct arch_timer_vm_data timer_data;
+
 	/* Mandated version of PSCI */
 	u32 psci_version;
 
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index 82c2c15f2e5d..a427b1e673c9 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -95,14 +95,10 @@ u64 timer_get_cval(struct arch_timer_context *ctxt)
 
 static u64 timer_get_offset(struct arch_timer_context *ctxt)
 {
-	struct kvm_vcpu *vcpu = ctxt->vcpu;
+	if (ctxt->offset.vm_offset)
+		return *ctxt->offset.vm_offset;
 
-	switch(arch_timer_ctx_index(ctxt)) {
-	case TIMER_VTIMER:
-		return __vcpu_sys_reg(vcpu, CNTVOFF_EL2);
-	default:
-		return 0;
-	}
+	return 0;
 }
 
 static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
@@ -139,15 +135,12 @@ static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 
 static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 {
-	struct kvm_vcpu *vcpu = ctxt->vcpu;
-
-	switch(arch_timer_ctx_index(ctxt)) {
-	case TIMER_VTIMER:
-		__vcpu_sys_reg(vcpu, CNTVOFF_EL2) = offset;
-		break;
-	default:
+	if (!ctxt->offset.vm_offset) {
 		WARN(offset, "timer %ld\n", arch_timer_ctx_index(ctxt));
+		return;
 	}
+
+	WRITE_ONCE(*ctxt->offset.vm_offset, offset);
 }
 
 u64 kvm_phys_timer_read(void)
@@ -761,25 +754,6 @@ int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-/* Make the updates of cntvoff for all vtimer contexts atomic */
-static void update_vtimer_cntvoff(struct kvm_vcpu *vcpu, u64 cntvoff)
-{
-	unsigned long i;
-	struct kvm *kvm = vcpu->kvm;
-	struct kvm_vcpu *tmp;
-
-	mutex_lock(&kvm->lock);
-	kvm_for_each_vcpu(i, tmp, kvm)
-		timer_set_offset(vcpu_vtimer(tmp), cntvoff);
-
-	/*
-	 * When called from the vcpu create path, the CPU being created is not
-	 * included in the loop above, so we just set it here as well.
-	 */
-	timer_set_offset(vcpu_vtimer(vcpu), cntvoff);
-	mutex_unlock(&kvm->lock);
-}
-
 void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -787,10 +761,11 @@ void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 	struct arch_timer_context *ptimer = vcpu_ptimer(vcpu);
 
 	vtimer->vcpu = vcpu;
+	vtimer->offset.vm_offset = &vcpu->kvm->arch.timer_data.voffset;
 	ptimer->vcpu = vcpu;
 
 	/* Synchronize cntvoff across all vtimers of a VM. */
-	update_vtimer_cntvoff(vcpu, kvm_phys_timer_read());
+	timer_set_offset(vtimer, kvm_phys_timer_read());
 	timer_set_offset(ptimer, 0);
 
 	hrtimer_init(&timer->bg_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);
@@ -828,7 +803,7 @@ int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 		break;
 	case KVM_REG_ARM_TIMER_CNT:
 		timer = vcpu_vtimer(vcpu);
-		update_vtimer_cntvoff(vcpu, kvm_phys_timer_read() - value);
+		timer_set_offset(timer, kvm_phys_timer_read() - value);
 		break;
 	case KVM_REG_ARM_TIMER_CVAL:
 		timer = vcpu_vtimer(vcpu);
* Unmerged path arch/arm64/kvm/hypercalls.c
diff --git a/include/kvm/arm_arch_timer.h b/include/kvm/arm_arch_timer.h
index 2bb30e6c9fcd..94578340515a 100644
--- a/include/kvm/arm_arch_timer.h
+++ b/include/kvm/arm_arch_timer.h
@@ -35,6 +35,19 @@ enum kvm_arch_timer_regs {
 	TIMER_REG_CTL,
 };
 
+struct arch_timer_offset {
+	/*
+	 * If set, pointer to one of the offsets in the kvm's offset
+	 * structure. If NULL, assume a zero offset.
+	 */
+	u64	*vm_offset;
+};
+
+struct arch_timer_vm_data {
+	/* Offset applied to the virtual timer/counter */
+	u64	voffset;
+};
+
 struct arch_timer_context {
 	struct kvm_vcpu			*vcpu;
 
@@ -44,6 +57,8 @@ struct arch_timer_context {
 	/* Emulated Timer (may be unused) */
 	struct hrtimer			hrtimer;
 
+	/* Offset for this counter/timer */
+	struct arch_timer_offset	offset;
 	/*
 	 * We have multiple paths which can save/restore the timer state onto
 	 * the hardware, so we need some way of keeping track of where the
