bpf: Fix a sdiv overflow issue

jira LE-2177
cve CVE-2024-49888
Rebuild_History Non-Buildable kernel-5.14.0-503.19.1.el9_5
commit-author Yonghong Song <yonghong.song@linux.dev>
commit 7dd34d7b7dcf9309fc6224caf4dd5b35bedddcb7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-503.19.1.el9_5/7dd34d7b.failed

Zac Ecob reported a problem where a bpf program may cause kernel crash due
to the following error:
  Oops: divide error: 0000 [#1] PREEMPT SMP KASAN PTI

The failure is due to the below signed divide:
  LLONG_MIN/-1 where LLONG_MIN equals to -9,223,372,036,854,775,808.
LLONG_MIN/-1 is supposed to give a positive number 9,223,372,036,854,775,808,
but it is impossible since for 64-bit system, the maximum positive
number is 9,223,372,036,854,775,807. On x86_64, LLONG_MIN/-1 will
cause a kernel exception. On arm64, the result for LLONG_MIN/-1 is
LLONG_MIN.

Further investigation found all the following sdiv/smod cases may trigger
an exception when bpf program is running on x86_64 platform:
  - LLONG_MIN/-1 for 64bit operation
  - INT_MIN/-1 for 32bit operation
  - LLONG_MIN%-1 for 64bit operation
  - INT_MIN%-1 for 32bit operation
where -1 can be an immediate or in a register.

On arm64, there are no exceptions:
  - LLONG_MIN/-1 = LLONG_MIN
  - INT_MIN/-1 = INT_MIN
  - LLONG_MIN%-1 = 0
  - INT_MIN%-1 = 0
where -1 can be an immediate or in a register.

Insn patching is needed to handle the above cases and the patched codes
produced results aligned with above arm64 result. The below are pseudo
codes to handle sdiv/smod exceptions including both divisor -1 and divisor 0
and the divisor is stored in a register.

sdiv:
      tmp = rX
      tmp += 1 /* [-1, 0] -> [0, 1]
      if tmp >(unsigned) 1 goto L2
      if tmp == 0 goto L1
      rY = 0
  L1:
      rY = -rY;
      goto L3
  L2:
      rY /= rX
  L3:

smod:
      tmp = rX
      tmp += 1 /* [-1, 0] -> [0, 1]
      if tmp >(unsigned) 1 goto L1
      if tmp == 1 (is64 ? goto L2 : goto L3)
      rY = 0;
      goto L2
  L1:
      rY %= rX
  L2:
      goto L4  // only when !is64
  L3:
      wY = wY  // only when !is64
  L4:

  [1] https://lore.kernel.org/bpf/tPJLTEh7S_DxFEqAI2Ji5MBSoZVg7_G-Py2iaZpAaWtM961fFTWtsnlzwvTbzBzaUzwQAoNATXKUlt0LZOFgnDcIyKCswAnAGdUF3LBrhGQ=@protonmail.com/

	Reported-by: Zac Ecob <zacecob@protonmail.com>
	Signed-off-by: Yonghong Song <yonghong.song@linux.dev>
	Acked-by: Andrii Nakryiko <andrii@kernel.org>
Link: https://lore.kernel.org/r/20240913150326.1187788-1-yonghong.song@linux.dev
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
(cherry picked from commit 7dd34d7b7dcf9309fc6224caf4dd5b35bedddcb7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/bpf/verifier.c
diff --cc kernel/bpf/verifier.c
index 2ca877463352,068f763dcefb..000000000000
--- a/kernel/bpf/verifier.c
+++ b/kernel/bpf/verifier.c
@@@ -19321,8 -20482,55 +19321,60 @@@ static int do_misc_fixups(struct bpf_ve
  		mark_subprog_exc_cb(env, env->exception_callback_subprog);
  	}
  
++<<<<<<< HEAD
 +	for (i = 0; i < insn_cnt; i++, insn++) {
 +		/* Make divide-by-zero exceptions impossible. */
++=======
+ 	for (i = 0; i < insn_cnt;) {
+ 		if (insn->code == (BPF_ALU64 | BPF_MOV | BPF_X) && insn->imm) {
+ 			if ((insn->off == BPF_ADDR_SPACE_CAST && insn->imm == 1) ||
+ 			    (((struct bpf_map *)env->prog->aux->arena)->map_flags & BPF_F_NO_USER_CONV)) {
+ 				/* convert to 32-bit mov that clears upper 32-bit */
+ 				insn->code = BPF_ALU | BPF_MOV | BPF_X;
+ 				/* clear off and imm, so it's a normal 'wX = wY' from JIT pov */
+ 				insn->off = 0;
+ 				insn->imm = 0;
+ 			} /* cast from as(0) to as(1) should be handled by JIT */
+ 			goto next_insn;
+ 		}
+ 
+ 		if (env->insn_aux_data[i + delta].needs_zext)
+ 			/* Convert BPF_CLASS(insn->code) == BPF_ALU64 to 32-bit ALU */
+ 			insn->code = BPF_ALU | BPF_OP(insn->code) | BPF_SRC(insn->code);
+ 
+ 		/* Make sdiv/smod divide-by-minus-one exceptions impossible. */
+ 		if ((insn->code == (BPF_ALU64 | BPF_MOD | BPF_K) ||
+ 		     insn->code == (BPF_ALU64 | BPF_DIV | BPF_K) ||
+ 		     insn->code == (BPF_ALU | BPF_MOD | BPF_K) ||
+ 		     insn->code == (BPF_ALU | BPF_DIV | BPF_K)) &&
+ 		    insn->off == 1 && insn->imm == -1) {
+ 			bool is64 = BPF_CLASS(insn->code) == BPF_ALU64;
+ 			bool isdiv = BPF_OP(insn->code) == BPF_DIV;
+ 			struct bpf_insn *patchlet;
+ 			struct bpf_insn chk_and_sdiv[] = {
+ 				BPF_RAW_INSN((is64 ? BPF_ALU64 : BPF_ALU) |
+ 					     BPF_NEG | BPF_K, insn->dst_reg,
+ 					     0, 0, 0),
+ 			};
+ 			struct bpf_insn chk_and_smod[] = {
+ 				BPF_MOV32_IMM(insn->dst_reg, 0),
+ 			};
+ 
+ 			patchlet = isdiv ? chk_and_sdiv : chk_and_smod;
+ 			cnt = isdiv ? ARRAY_SIZE(chk_and_sdiv) : ARRAY_SIZE(chk_and_smod);
+ 
+ 			new_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);
+ 			if (!new_prog)
+ 				return -ENOMEM;
+ 
+ 			delta    += cnt - 1;
+ 			env->prog = prog = new_prog;
+ 			insn      = new_prog->insnsi + i + delta;
+ 			goto next_insn;
+ 		}
+ 
+ 		/* Make divide-by-zero and divide-by-minus-one exceptions impossible. */
++>>>>>>> 7dd34d7b7dcf (bpf: Fix a sdiv overflow issue)
  		if (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||
  		    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||
  		    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||
* Unmerged path kernel/bpf/verifier.c
