fsdax: Fix infinite loop in dax_iomap_rw()

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.21.1.rt7.284.el8_8
commit-author Li Jinlin <lijinlin3@huawei.com>
commit 17d9c15c9b9e7fb285f7ac5367dfb5f00ff575e3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.21.1.rt7.284.el8_8/17d9c15c.failed

I got an infinite loop and a WARNING report when executing a tail command
in virtiofs.

  WARNING: CPU: 10 PID: 964 at fs/iomap/iter.c:34 iomap_iter+0x3a2/0x3d0
  Modules linked in:
  CPU: 10 PID: 964 Comm: tail Not tainted 5.19.0-rc7
  Call Trace:
  <TASK>
  dax_iomap_rw+0xea/0x620
  ? __this_cpu_preempt_check+0x13/0x20
  fuse_dax_read_iter+0x47/0x80
  fuse_file_read_iter+0xae/0xd0
  new_sync_read+0xfe/0x180
  ? 0xffffffff81000000
  vfs_read+0x14d/0x1a0
  ksys_read+0x6d/0xf0
  __x64_sys_read+0x1a/0x20
  do_syscall_64+0x3b/0x90
  entry_SYSCALL_64_after_hwframe+0x63/0xcd

The tail command will call read() with a count of 0. In this case,
iomap_iter() will report this WARNING, and always return 1 which casuing
the infinite loop in dax_iomap_rw().

Fixing by checking count whether is 0 in dax_iomap_rw().

Fixes: ca289e0b95af ("fsdax: switch dax_iomap_rw to use iomap_iter")
	Signed-off-by: Li Jinlin <lijinlin3@huawei.com>
	Reviewed-by: Darrick J. Wong <djwong@kernel.org>
Link: https://lore.kernel.org/r/20220725032050.3873372-1-lijinlin3@huawei.com
	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
(cherry picked from commit 17d9c15c9b9e7fb285f7ac5367dfb5f00ff575e3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/dax.c
diff --cc fs/dax.c
index 7f8158986e50,7ab248ed21aa..000000000000
--- a/fs/dax.c
+++ b/fs/dax.c
@@@ -1473,35 -1035,437 +1473,466 @@@ fallback
  	trace_dax_pmd_load_hole_fallback(inode, vmf, zero_page, *entry);
  	return VM_FAULT_FALLBACK;
  }
++<<<<<<< HEAD
 +
 +static vm_fault_t dax_iomap_pmd_fault(struct vm_fault *vmf, pfn_t *pfnp,
 +			       const struct iomap_ops *ops)
++=======
+ #else
+ static vm_fault_t dax_pmd_load_hole(struct xa_state *xas, struct vm_fault *vmf,
+ 		const struct iomap *iomap, void **entry)
+ {
+ 	return VM_FAULT_FALLBACK;
+ }
+ #endif /* CONFIG_FS_DAX_PMD */
+ 
+ static int dax_memzero(struct dax_device *dax_dev, pgoff_t pgoff,
+ 		unsigned int offset, size_t size)
+ {
+ 	void *kaddr;
+ 	long ret;
+ 
+ 	ret = dax_direct_access(dax_dev, pgoff, 1, DAX_ACCESS, &kaddr, NULL);
+ 	if (ret > 0) {
+ 		memset(kaddr + offset, 0, size);
+ 		dax_flush(dax_dev, kaddr + offset, size);
+ 	}
+ 	return ret;
+ }
+ 
+ static s64 dax_zero_iter(struct iomap_iter *iter, bool *did_zero)
+ {
+ 	const struct iomap *iomap = &iter->iomap;
+ 	const struct iomap *srcmap = iomap_iter_srcmap(iter);
+ 	loff_t pos = iter->pos;
+ 	u64 length = iomap_length(iter);
+ 	s64 written = 0;
+ 
+ 	/* already zeroed?  we're done. */
+ 	if (srcmap->type == IOMAP_HOLE || srcmap->type == IOMAP_UNWRITTEN)
+ 		return length;
+ 
+ 	do {
+ 		unsigned offset = offset_in_page(pos);
+ 		unsigned size = min_t(u64, PAGE_SIZE - offset, length);
+ 		pgoff_t pgoff = dax_iomap_pgoff(iomap, pos);
+ 		long rc;
+ 		int id;
+ 
+ 		id = dax_read_lock();
+ 		if (IS_ALIGNED(pos, PAGE_SIZE) && size == PAGE_SIZE)
+ 			rc = dax_zero_page_range(iomap->dax_dev, pgoff, 1);
+ 		else
+ 			rc = dax_memzero(iomap->dax_dev, pgoff, offset, size);
+ 		dax_read_unlock(id);
+ 
+ 		if (rc < 0)
+ 			return rc;
+ 		pos += size;
+ 		length -= size;
+ 		written += size;
+ 		if (did_zero)
+ 			*did_zero = true;
+ 	} while (length > 0);
+ 
+ 	return written;
+ }
+ 
+ int dax_zero_range(struct inode *inode, loff_t pos, loff_t len, bool *did_zero,
+ 		const struct iomap_ops *ops)
+ {
+ 	struct iomap_iter iter = {
+ 		.inode		= inode,
+ 		.pos		= pos,
+ 		.len		= len,
+ 		.flags		= IOMAP_DAX | IOMAP_ZERO,
+ 	};
+ 	int ret;
+ 
+ 	while ((ret = iomap_iter(&iter, ops)) > 0)
+ 		iter.processed = dax_zero_iter(&iter, did_zero);
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(dax_zero_range);
+ 
+ int dax_truncate_page(struct inode *inode, loff_t pos, bool *did_zero,
+ 		const struct iomap_ops *ops)
+ {
+ 	unsigned int blocksize = i_blocksize(inode);
+ 	unsigned int off = pos & (blocksize - 1);
+ 
+ 	/* Block boundary? Nothing to do */
+ 	if (!off)
+ 		return 0;
+ 	return dax_zero_range(inode, pos, blocksize - off, did_zero, ops);
+ }
+ EXPORT_SYMBOL_GPL(dax_truncate_page);
+ 
+ static loff_t dax_iomap_iter(const struct iomap_iter *iomi,
+ 		struct iov_iter *iter)
+ {
+ 	const struct iomap *iomap = &iomi->iomap;
+ 	loff_t length = iomap_length(iomi);
+ 	loff_t pos = iomi->pos;
+ 	struct dax_device *dax_dev = iomap->dax_dev;
+ 	loff_t end = pos + length, done = 0;
+ 	ssize_t ret = 0;
+ 	size_t xfer;
+ 	int id;
+ 
+ 	if (iov_iter_rw(iter) == READ) {
+ 		end = min(end, i_size_read(iomi->inode));
+ 		if (pos >= end)
+ 			return 0;
+ 
+ 		if (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)
+ 			return iov_iter_zero(min(length, end - pos), iter);
+ 	}
+ 
+ 	if (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED))
+ 		return -EIO;
+ 
+ 	/*
+ 	 * Write can allocate block for an area which has a hole page mapped
+ 	 * into page tables. We have to tear down these mappings so that data
+ 	 * written by write(2) is visible in mmap.
+ 	 */
+ 	if (iomap->flags & IOMAP_F_NEW) {
+ 		invalidate_inode_pages2_range(iomi->inode->i_mapping,
+ 					      pos >> PAGE_SHIFT,
+ 					      (end - 1) >> PAGE_SHIFT);
+ 	}
+ 
+ 	id = dax_read_lock();
+ 	while (pos < end) {
+ 		unsigned offset = pos & (PAGE_SIZE - 1);
+ 		const size_t size = ALIGN(length + offset, PAGE_SIZE);
+ 		pgoff_t pgoff = dax_iomap_pgoff(iomap, pos);
+ 		ssize_t map_len;
+ 		bool recovery = false;
+ 		void *kaddr;
+ 
+ 		if (fatal_signal_pending(current)) {
+ 			ret = -EINTR;
+ 			break;
+ 		}
+ 
+ 		map_len = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size),
+ 				DAX_ACCESS, &kaddr, NULL);
+ 		if (map_len == -EIO && iov_iter_rw(iter) == WRITE) {
+ 			map_len = dax_direct_access(dax_dev, pgoff,
+ 					PHYS_PFN(size), DAX_RECOVERY_WRITE,
+ 					&kaddr, NULL);
+ 			if (map_len > 0)
+ 				recovery = true;
+ 		}
+ 		if (map_len < 0) {
+ 			ret = map_len;
+ 			break;
+ 		}
+ 
+ 		map_len = PFN_PHYS(map_len);
+ 		kaddr += offset;
+ 		map_len -= offset;
+ 		if (map_len > end - pos)
+ 			map_len = end - pos;
+ 
+ 		if (recovery)
+ 			xfer = dax_recovery_write(dax_dev, pgoff, kaddr,
+ 					map_len, iter);
+ 		else if (iov_iter_rw(iter) == WRITE)
+ 			xfer = dax_copy_from_iter(dax_dev, pgoff, kaddr,
+ 					map_len, iter);
+ 		else
+ 			xfer = dax_copy_to_iter(dax_dev, pgoff, kaddr,
+ 					map_len, iter);
+ 
+ 		pos += xfer;
+ 		length -= xfer;
+ 		done += xfer;
+ 
+ 		if (xfer == 0)
+ 			ret = -EFAULT;
+ 		if (xfer < map_len)
+ 			break;
+ 	}
+ 	dax_read_unlock(id);
+ 
+ 	return done ? done : ret;
+ }
+ 
+ /**
+  * dax_iomap_rw - Perform I/O to a DAX file
+  * @iocb:	The control block for this I/O
+  * @iter:	The addresses to do I/O from or to
+  * @ops:	iomap ops passed from the file system
+  *
+  * This function performs read and write operations to directly mapped
+  * persistent memory.  The callers needs to take care of read/write exclusion
+  * and evicting any page cache pages in the region under I/O.
+  */
+ ssize_t
+ dax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,
+ 		const struct iomap_ops *ops)
+ {
+ 	struct iomap_iter iomi = {
+ 		.inode		= iocb->ki_filp->f_mapping->host,
+ 		.pos		= iocb->ki_pos,
+ 		.len		= iov_iter_count(iter),
+ 		.flags		= IOMAP_DAX,
+ 	};
+ 	loff_t done = 0;
+ 	int ret;
+ 
+ 	if (!iomi.len)
+ 		return 0;
+ 
+ 	if (iov_iter_rw(iter) == WRITE) {
+ 		lockdep_assert_held_write(&iomi.inode->i_rwsem);
+ 		iomi.flags |= IOMAP_WRITE;
+ 	} else {
+ 		lockdep_assert_held(&iomi.inode->i_rwsem);
+ 	}
+ 
+ 	if (iocb->ki_flags & IOCB_NOWAIT)
+ 		iomi.flags |= IOMAP_NOWAIT;
+ 
+ 	while ((ret = iomap_iter(&iomi, ops)) > 0)
+ 		iomi.processed = dax_iomap_iter(&iomi, iter);
+ 
+ 	done = iomi.pos - iocb->ki_pos;
+ 	iocb->ki_pos = iomi.pos;
+ 	return done ? done : ret;
+ }
+ EXPORT_SYMBOL_GPL(dax_iomap_rw);
+ 
+ static vm_fault_t dax_fault_return(int error)
+ {
+ 	if (error == 0)
+ 		return VM_FAULT_NOPAGE;
+ 	return vmf_error(error);
+ }
+ 
+ /*
+  * MAP_SYNC on a dax mapping guarantees dirty metadata is
+  * flushed on write-faults (non-cow), but not read-faults.
+  */
+ static bool dax_fault_is_synchronous(unsigned long flags,
+ 		struct vm_area_struct *vma, const struct iomap *iomap)
+ {
+ 	return (flags & IOMAP_WRITE) && (vma->vm_flags & VM_SYNC)
+ 		&& (iomap->flags & IOMAP_F_DIRTY);
+ }
+ 
+ /*
+  * When handling a synchronous page fault and the inode need a fsync, we can
+  * insert the PTE/PMD into page tables only after that fsync happened. Skip
+  * insertion for now and return the pfn so that caller can insert it after the
+  * fsync is done.
+  */
+ static vm_fault_t dax_fault_synchronous_pfnp(pfn_t *pfnp, pfn_t pfn)
+ {
+ 	if (WARN_ON_ONCE(!pfnp))
+ 		return VM_FAULT_SIGBUS;
+ 	*pfnp = pfn;
+ 	return VM_FAULT_NEEDDSYNC;
+ }
+ 
+ static vm_fault_t dax_fault_cow_page(struct vm_fault *vmf,
+ 		const struct iomap_iter *iter)
+ {
+ 	vm_fault_t ret;
+ 	int error = 0;
+ 
+ 	switch (iter->iomap.type) {
+ 	case IOMAP_HOLE:
+ 	case IOMAP_UNWRITTEN:
+ 		clear_user_highpage(vmf->cow_page, vmf->address);
+ 		break;
+ 	case IOMAP_MAPPED:
+ 		error = copy_cow_page_dax(vmf, iter);
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		error = -EIO;
+ 		break;
+ 	}
+ 
+ 	if (error)
+ 		return dax_fault_return(error);
+ 
+ 	__SetPageUptodate(vmf->cow_page);
+ 	ret = finish_fault(vmf);
+ 	if (!ret)
+ 		return VM_FAULT_DONE_COW;
+ 	return ret;
+ }
+ 
+ /**
+  * dax_fault_iter - Common actor to handle pfn insertion in PTE/PMD fault.
+  * @vmf:	vm fault instance
+  * @iter:	iomap iter
+  * @pfnp:	pfn to be returned
+  * @xas:	the dax mapping tree of a file
+  * @entry:	an unlocked dax entry to be inserted
+  * @pmd:	distinguish whether it is a pmd fault
+  */
+ static vm_fault_t dax_fault_iter(struct vm_fault *vmf,
+ 		const struct iomap_iter *iter, pfn_t *pfnp,
+ 		struct xa_state *xas, void **entry, bool pmd)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	const struct iomap *iomap = &iter->iomap;
+ 	size_t size = pmd ? PMD_SIZE : PAGE_SIZE;
+ 	loff_t pos = (loff_t)xas->xa_index << PAGE_SHIFT;
+ 	bool write = vmf->flags & FAULT_FLAG_WRITE;
+ 	bool sync = dax_fault_is_synchronous(iter->flags, vmf->vma, iomap);
+ 	unsigned long entry_flags = pmd ? DAX_PMD : 0;
+ 	int err = 0;
+ 	pfn_t pfn;
+ 
+ 	if (!pmd && vmf->cow_page)
+ 		return dax_fault_cow_page(vmf, iter);
+ 
+ 	/* if we are reading UNWRITTEN and HOLE, return a hole. */
+ 	if (!write &&
+ 	    (iomap->type == IOMAP_UNWRITTEN || iomap->type == IOMAP_HOLE)) {
+ 		if (!pmd)
+ 			return dax_load_hole(xas, mapping, entry, vmf);
+ 		return dax_pmd_load_hole(xas, vmf, iomap, entry);
+ 	}
+ 
+ 	if (iomap->type != IOMAP_MAPPED) {
+ 		WARN_ON_ONCE(1);
+ 		return pmd ? VM_FAULT_FALLBACK : VM_FAULT_SIGBUS;
+ 	}
+ 
+ 	err = dax_iomap_pfn(&iter->iomap, pos, size, &pfn);
+ 	if (err)
+ 		return pmd ? VM_FAULT_FALLBACK : dax_fault_return(err);
+ 
+ 	*entry = dax_insert_entry(xas, mapping, vmf, *entry, pfn, entry_flags,
+ 				  write && !sync);
+ 
+ 	if (sync)
+ 		return dax_fault_synchronous_pfnp(pfnp, pfn);
+ 
+ 	/* insert PMD pfn */
+ 	if (pmd)
+ 		return vmf_insert_pfn_pmd(vmf, pfn, write);
+ 
+ 	/* insert PTE pfn */
+ 	if (write)
+ 		return vmf_insert_mixed_mkwrite(vmf->vma, vmf->address, pfn);
+ 	return vmf_insert_mixed(vmf->vma, vmf->address, pfn);
+ }
+ 
+ static vm_fault_t dax_iomap_pte_fault(struct vm_fault *vmf, pfn_t *pfnp,
+ 			       int *iomap_errp, const struct iomap_ops *ops)
+ {
+ 	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+ 	XA_STATE(xas, &mapping->i_pages, vmf->pgoff);
+ 	struct iomap_iter iter = {
+ 		.inode		= mapping->host,
+ 		.pos		= (loff_t)vmf->pgoff << PAGE_SHIFT,
+ 		.len		= PAGE_SIZE,
+ 		.flags		= IOMAP_DAX | IOMAP_FAULT,
+ 	};
+ 	vm_fault_t ret = 0;
+ 	void *entry;
+ 	int error;
+ 
+ 	trace_dax_pte_fault(iter.inode, vmf, ret);
+ 	/*
+ 	 * Check whether offset isn't beyond end of file now. Caller is supposed
+ 	 * to hold locks serializing us with truncate / punch hole so this is
+ 	 * a reliable test.
+ 	 */
+ 	if (iter.pos >= i_size_read(iter.inode)) {
+ 		ret = VM_FAULT_SIGBUS;
+ 		goto out;
+ 	}
+ 
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)
+ 		iter.flags |= IOMAP_WRITE;
+ 
+ 	entry = grab_mapping_entry(&xas, mapping, 0);
+ 	if (xa_is_internal(entry)) {
+ 		ret = xa_to_internal(entry);
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * It is possible, particularly with mixed reads & writes to private
+ 	 * mappings, that we have raced with a PMD fault that overlaps with
+ 	 * the PTE we need to set up.  If so just return and the fault will be
+ 	 * retried.
+ 	 */
+ 	if (pmd_trans_huge(*vmf->pmd) || pmd_devmap(*vmf->pmd)) {
+ 		ret = VM_FAULT_NOPAGE;
+ 		goto unlock_entry;
+ 	}
+ 
+ 	while ((error = iomap_iter(&iter, ops)) > 0) {
+ 		if (WARN_ON_ONCE(iomap_length(&iter) < PAGE_SIZE)) {
+ 			iter.processed = -EIO;	/* fs corruption? */
+ 			continue;
+ 		}
+ 
+ 		ret = dax_fault_iter(vmf, &iter, pfnp, &xas, &entry, false);
+ 		if (ret != VM_FAULT_SIGBUS &&
+ 		    (iter.iomap.flags & IOMAP_F_NEW)) {
+ 			count_vm_event(PGMAJFAULT);
+ 			count_memcg_event_mm(vmf->vma->vm_mm, PGMAJFAULT);
+ 			ret |= VM_FAULT_MAJOR;
+ 		}
+ 
+ 		if (!(ret & VM_FAULT_ERROR))
+ 			iter.processed = PAGE_SIZE;
+ 	}
+ 
+ 	if (iomap_errp)
+ 		*iomap_errp = error;
+ 	if (!ret && error)
+ 		ret = dax_fault_return(error);
+ 
+ unlock_entry:
+ 	dax_unlock_entry(&xas, entry);
+ out:
+ 	trace_dax_pte_fault_done(iter.inode, vmf, ret);
+ 	return ret;
+ }
+ 
+ #ifdef CONFIG_FS_DAX_PMD
+ static bool dax_fault_check_fallback(struct vm_fault *vmf, struct xa_state *xas,
+ 		pgoff_t max_pgoff)
++>>>>>>> 17d9c15c9b9e (fsdax: Fix infinite loop in dax_iomap_rw())
  {
 +	struct vm_area_struct *vma = vmf->vma;
 +	struct address_space *mapping = vma->vm_file->f_mapping;
 +	XA_STATE_ORDER(xas, &mapping->i_pages, vmf->pgoff, PMD_ORDER);
  	unsigned long pmd_addr = vmf->address & PMD_MASK;
  	bool write = vmf->flags & FAULT_FLAG_WRITE;
 +	bool sync;
 +	unsigned int iomap_flags = (write ? IOMAP_WRITE : 0) | IOMAP_FAULT;
 +	struct inode *inode = mapping->host;
 +	vm_fault_t result = VM_FAULT_FALLBACK;
 +	struct iomap iomap = { .type = IOMAP_HOLE };
 +	struct iomap srcmap = { .type = IOMAP_HOLE };
 +	pgoff_t max_pgoff;
 +	void *entry;
 +	loff_t pos;
 +	int error;
 +	pfn_t pfn;
 +
 +	/*
 +	 * Check whether offset isn't beyond end of file now. Caller is
 +	 * supposed to hold locks serializing us with truncate / punch hole so
 +	 * this is a reliable test.
 +	 */
 +	max_pgoff = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 +
 +	trace_dax_pmd_fault(inode, vmf, max_pgoff, 0);
  
  	/*
  	 * Make sure that the faulting address's PMD offset (color) matches
* Unmerged path fs/dax.c
