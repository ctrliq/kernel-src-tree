tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy'

jira LE-1907
cve CVE-2022-23825"
cve CVE-2022-29901
cve CVE-2022-29900
cve {CVE-2022-23816
cve 2090229]
cve [2103167
cve Long)
cve (Waiman
cve memcpy
cve mem
cve bench
cve perf
cve in
cve used
cve copies
cve "cpy,set_64.S
Rebuild_History Non-Buildable kernel-4.18.0-372.32.1.el8_6
commit-author Arnaldo Carvalho de Melo <acme@redhat.com>
commit db1a8b97a0a36155171dbb805fbcb276e07559f6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.32.1.el8_6/db1a8b97.failed

To bring in the change made in this cset:

  4d6ffa27b8e5116c ("x86/lib: Change .weak to SYM_FUNC_START_WEAK for arch/x86/lib/mem*_64.S")
  6dcc5627f6aec4cb ("x86/asm: Change all ENTRY+ENDPROC to SYM_FUNC_*")

I needed to define SYM_FUNC_START_LOCAL() as SYM_L_GLOBAL as
mem{cpy,set}_{orig,erms} are used by 'perf bench'.

This silences these perf tools build warnings:

  Warning: Kernel ABI header at 'tools/arch/x86/lib/memcpy_64.S' differs from latest version at 'arch/x86/lib/memcpy_64.S'
  diff -u tools/arch/x86/lib/memcpy_64.S arch/x86/lib/memcpy_64.S
  Warning: Kernel ABI header at 'tools/arch/x86/lib/memset_64.S' differs from latest version at 'arch/x86/lib/memset_64.S'
  diff -u tools/arch/x86/lib/memset_64.S arch/x86/lib/memset_64.S

	Cc: Adrian Hunter <adrian.hunter@intel.com>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Fangrui Song <maskray@google.com>
	Cc: Ian Rogers <irogers@google.com>
	Cc: Jiri Olsa <jolsa@kernel.org>
	Cc: Jiri Slaby <jirislaby@kernel.org>
	Cc: Namhyung Kim <namhyung@kernel.org>
	Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
(cherry picked from commit db1a8b97a0a36155171dbb805fbcb276e07559f6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/arch/x86/lib/memcpy_64.S
#	tools/arch/x86/lib/memset_64.S
#	tools/perf/util/include/linux/linkage.h
diff --cc tools/arch/x86/lib/memcpy_64.S
index 3b24dc05251c,1e299ac73c86..000000000000
--- a/tools/arch/x86/lib/memcpy_64.S
+++ b/tools/arch/x86/lib/memcpy_64.S
@@@ -27,8 -27,8 +25,13 @@@
   * Output:
   * rax original destination
   */
++<<<<<<< HEAD
 +ENTRY(__memcpy)
 +ENTRY(memcpy)
++=======
+ SYM_FUNC_START_ALIAS(__memcpy)
+ SYM_FUNC_START_WEAK(memcpy)
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  	ALTERNATIVE_2 "jmp memcpy_orig", "", X86_FEATURE_REP_GOOD, \
  		      "jmp memcpy_erms", X86_FEATURE_ERMS
  
@@@ -49,14 -49,14 +52,22 @@@ EXPORT_SYMBOL(__memcpy
   * memcpy_erms() - enhanced fast string memcpy. This is faster and
   * simpler than memcpy. Use memcpy_erms when possible.
   */
++<<<<<<< HEAD
 +ENTRY(memcpy_erms)
++=======
+ SYM_FUNC_START_LOCAL(memcpy_erms)
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  	movq %rdi, %rax
  	movq %rdx, %rcx
  	rep movsb
  	ret
 -SYM_FUNC_END(memcpy_erms)
 +ENDPROC(memcpy_erms)
  
++<<<<<<< HEAD
 +ENTRY(memcpy_orig)
++=======
+ SYM_FUNC_START_LOCAL(memcpy_orig)
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  	movq %rdi, %rax
  
  	cmpq $0x20, %rdx
diff --cc tools/arch/x86/lib/memset_64.S
index f8f3dc0a6690,0bfd26e4ca9e..000000000000
--- a/tools/arch/x86/lib/memset_64.S
+++ b/tools/arch/x86/lib/memset_64.S
@@@ -18,8 -17,8 +17,13 @@@
   *
   * rax   original destination
   */
++<<<<<<< HEAD
 +ENTRY(memset)
 +ENTRY(__memset)
++=======
+ SYM_FUNC_START_WEAK(memset)
+ SYM_FUNC_START(__memset)
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  	/*
  	 * Some CPUs support enhanced REP MOVSB/STOSB feature. It is recommended
  	 * to use it when possible. If not available, use fast string instructions.
@@@ -42,8 -41,10 +46,15 @@@
  	rep stosb
  	movq %r9,%rax
  	ret
++<<<<<<< HEAD
 +ENDPROC(memset)
 +ENDPROC(__memset)
++=======
+ SYM_FUNC_END(__memset)
+ SYM_FUNC_END_ALIAS(memset)
+ EXPORT_SYMBOL(memset)
+ EXPORT_SYMBOL(__memset)
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  
  /*
   * ISO C memset - set a memory block to a byte value. This function uses
@@@ -56,16 -57,16 +67,24 @@@
   *
   * rax   original destination
   */
++<<<<<<< HEAD
 +ENTRY(memset_erms)
++=======
+ SYM_FUNC_START_LOCAL(memset_erms)
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  	movq %rdi,%r9
  	movb %sil,%al
  	movq %rdx,%rcx
  	rep stosb
  	movq %r9,%rax
  	ret
 -SYM_FUNC_END(memset_erms)
 +ENDPROC(memset_erms)
  
++<<<<<<< HEAD
 +ENTRY(memset_orig)
++=======
+ SYM_FUNC_START_LOCAL(memset_orig)
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  	movq %rdi,%r10
  
  	/* expand byte value  */
diff --cc tools/perf/util/include/linux/linkage.h
index f01d48a8d707,5acf053fca7d..000000000000
--- a/tools/perf/util/include/linux/linkage.h
+++ b/tools/perf/util/include/linux/linkage.h
@@@ -5,10 -5,100 +5,108 @@@
  
  /* linkage.h ... for including arch/x86/lib/memcpy_64.S */
  
++<<<<<<< HEAD
 +#define ENTRY(name)				\
 +	.globl name;				\
++=======
+ /* Some toolchains use other characters (e.g. '`') to mark new line in macro */
+ #ifndef ASM_NL
+ #define ASM_NL		 ;
+ #endif
+ 
+ #ifndef __ALIGN
+ #define __ALIGN		.align 4,0x90
+ #define __ALIGN_STR	".align 4,0x90"
+ #endif
+ 
+ /* SYM_T_FUNC -- type used by assembler to mark functions */
+ #ifndef SYM_T_FUNC
+ #define SYM_T_FUNC				STT_FUNC
+ #endif
+ 
+ /* SYM_A_* -- align the symbol? */
+ #define SYM_A_ALIGN				ALIGN
+ 
+ /* SYM_L_* -- linkage of symbols */
+ #define SYM_L_GLOBAL(name)			.globl name
+ #define SYM_L_WEAK(name)			.weak name
+ #define SYM_L_LOCAL(name)			/* nothing */
+ 
+ #define ALIGN __ALIGN
+ 
+ /* === generic annotations === */
+ 
+ /* SYM_ENTRY -- use only if you have to for non-paired symbols */
+ #ifndef SYM_ENTRY
+ #define SYM_ENTRY(name, linkage, align...)		\
+ 	linkage(name) ASM_NL				\
+ 	align ASM_NL					\
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  	name:
 -#endif
  
++<<<<<<< HEAD
 +#define ENDPROC(name)
++=======
+ /* SYM_START -- use only if you have to */
+ #ifndef SYM_START
+ #define SYM_START(name, linkage, align...)		\
+ 	SYM_ENTRY(name, linkage, align)
+ #endif
+ 
+ /* SYM_END -- use only if you have to */
+ #ifndef SYM_END
+ #define SYM_END(name, sym_type)				\
+ 	.type name sym_type ASM_NL			\
+ 	.size name, .-name
+ #endif
+ 
+ /*
+  * SYM_FUNC_START_ALIAS -- use where there are two global names for one
+  * function
+  */
+ #ifndef SYM_FUNC_START_ALIAS
+ #define SYM_FUNC_START_ALIAS(name)			\
+ 	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)
+ #endif
+ 
+ /* SYM_FUNC_START -- use for global functions */
+ #ifndef SYM_FUNC_START
+ /*
+  * The same as SYM_FUNC_START_ALIAS, but we will need to distinguish these two
+  * later.
+  */
+ #define SYM_FUNC_START(name)				\
+ 	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)
+ #endif
+ 
+ /* SYM_FUNC_START_LOCAL -- use for local functions */
+ #ifndef SYM_FUNC_START_LOCAL
+ /* the same as SYM_FUNC_START_LOCAL_ALIAS, see comment near SYM_FUNC_START */
+ #define SYM_FUNC_START_LOCAL(name)			\
+ 	SYM_START(name, SYM_L_LOCAL, SYM_A_ALIGN)
+ #endif
+ 
+ /* SYM_FUNC_END_ALIAS -- the end of LOCAL_ALIASed or ALIASed function */
+ #ifndef SYM_FUNC_END_ALIAS
+ #define SYM_FUNC_END_ALIAS(name)			\
+ 	SYM_END(name, SYM_T_FUNC)
+ #endif
+ 
+ /* SYM_FUNC_START_WEAK -- use for weak functions */
+ #ifndef SYM_FUNC_START_WEAK
+ #define SYM_FUNC_START_WEAK(name)			\
+ 	SYM_START(name, SYM_L_WEAK, SYM_A_ALIGN)
+ #endif
+ 
+ /*
+  * SYM_FUNC_END -- the end of SYM_FUNC_START_LOCAL, SYM_FUNC_START,
+  * SYM_FUNC_START_WEAK, ...
+  */
+ #ifndef SYM_FUNC_END
+ /* the same as SYM_FUNC_END_ALIAS, see comment near SYM_FUNC_START */
+ #define SYM_FUNC_END(name)				\
+ 	SYM_END(name, SYM_T_FUNC)
+ #endif
++>>>>>>> db1a8b97a0a3 (tools arch: Update arch/x86/lib/mem{cpy,set}_64.S copies used in 'perf bench mem memcpy')
  
  #endif	/* PERF_LINUX_LINKAGE_H_ */
* Unmerged path tools/arch/x86/lib/memcpy_64.S
* Unmerged path tools/arch/x86/lib/memset_64.S
diff --git a/tools/perf/bench/mem-memcpy-x86-64-asm.S b/tools/perf/bench/mem-memcpy-x86-64-asm.S
index 9ad015a1e202..6eb45a2aa8db 100644
--- a/tools/perf/bench/mem-memcpy-x86-64-asm.S
+++ b/tools/perf/bench/mem-memcpy-x86-64-asm.S
@@ -2,6 +2,9 @@
 
 /* Various wrappers to make the kernel .S file build in user-space: */
 
+// memcpy_orig and memcpy_erms are being defined as SYM_L_LOCAL but we need it
+#define SYM_FUNC_START_LOCAL(name)                      \
+        SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)
 #define memcpy MEMCPY /* don't hide glibc's memcpy() */
 #define altinstr_replacement text
 #define globl p2align 4; .globl
diff --git a/tools/perf/bench/mem-memset-x86-64-asm.S b/tools/perf/bench/mem-memset-x86-64-asm.S
index d550bd526162..6f093c483842 100644
--- a/tools/perf/bench/mem-memset-x86-64-asm.S
+++ b/tools/perf/bench/mem-memset-x86-64-asm.S
@@ -1,4 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 */
+// memset_orig and memset_erms are being defined as SYM_L_LOCAL but we need it
+#define SYM_FUNC_START_LOCAL(name)                      \
+        SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)
 #define memset MEMSET /* don't hide glibc's memset() */
 #define altinstr_replacement text
 #define globl p2align 4; .globl
* Unmerged path tools/perf/util/include/linux/linkage.h
