x86: Add straight-line-speculation mitigation

jira LE-1907
cve CVE-2022-23825
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-4.18.0-372.32.1.el8_6
commit-author Peter Zijlstra <peterz@infradead.org>
commit e463a09af2f0677b9485a7e8e4e70b396b2ffb6f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.32.1.el8_6/e463a09a.failed

Make use of an upcoming GCC feature to mitigate
straight-line-speculation for x86:

  https://gcc.gnu.org/g:53a643f8568067d7700a9f2facc8ba39974973d3
  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=102952
  https://bugs.llvm.org/show_bug.cgi?id=52323

It's built tested on x86_64-allyesconfig using GCC-12 and GCC-11.

Maintenance overhead of this should be fairly low due to objtool
validation.

Size overhead of all these additional int3 instructions comes to:

     text	   data	    bss	    dec	    hex	filename
  22267751	6933356	2011368	31212475	1dc43bb	defconfig-build/vmlinux
  22804126	6933356	1470696	31208178	1dc32f2	defconfig-build/vmlinux.sls

Or roughly 2.4% additional text.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lore.kernel.org/r/20211204134908.140103474@infradead.org
(cherry picked from commit e463a09af2f0677b9485a7e8e4e70b396b2ffb6f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Makefile
#	arch/x86/include/asm/linkage.h
#	arch/x86/include/asm/static_call.h
#	arch/x86/kernel/ftrace.c
#	arch/x86/kernel/static_call.c
#	arch/x86/lib/memmove_64.S
#	arch/x86/lib/retpoline.S
#	scripts/Makefile.build
#	scripts/link-vmlinux.sh
diff --cc arch/x86/Makefile
index 7621048daf34,e84cdd409b64..000000000000
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@@ -259,6 -191,34 +259,37 @@@ endi
    endif
  endif
  
++<<<<<<< HEAD
++=======
+ ifdef CONFIG_SLS
+   KBUILD_CFLAGS += -mharden-sls=all
+ endif
+ 
+ KBUILD_LDFLAGS += -m elf_$(UTS_MACHINE)
+ 
+ ifdef CONFIG_LTO_CLANG
+ ifeq ($(shell test $(CONFIG_LLD_VERSION) -lt 130000; echo $$?),0)
+ KBUILD_LDFLAGS	+= -plugin-opt=-stack-alignment=$(if $(CONFIG_X86_32),4,8)
+ endif
+ endif
+ 
+ ifdef CONFIG_X86_NEED_RELOCS
+ LDFLAGS_vmlinux := --emit-relocs --discard-none
+ else
+ LDFLAGS_vmlinux :=
+ endif
+ 
+ #
+ # The 64-bit kernel must be aligned to 2MB.  Pass -z max-page-size=0x200000 to
+ # the linker to force 2MB page size regardless of the default page size used
+ # by the linker.
+ #
+ ifdef CONFIG_X86_64
+ LDFLAGS_vmlinux += -z max-page-size=0x200000
+ endif
+ 
+ 
++>>>>>>> e463a09af2f0 (x86: Add straight-line-speculation mitigation)
  archscripts: scripts_basic
  	$(Q)$(MAKE) $(build)=arch/x86/tools relocs
  
diff --cc arch/x86/include/asm/linkage.h
index e07188e8d763,030907922bd0..000000000000
--- a/arch/x86/include/asm/linkage.h
+++ b/arch/x86/include/asm/linkage.h
@@@ -26,6 -18,20 +26,23 @@@
  #define __ALIGN_STR	__stringify(__ALIGN)
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_SLS
+ #define RET	ret; int3
+ #else
+ #define RET	ret
+ #endif
+ 
+ #else /* __ASSEMBLY__ */
+ 
+ #ifdef CONFIG_SLS
+ #define ASM_RET	"ret; int3\n\t"
+ #else
+ #define ASM_RET	"ret\n\t"
+ #endif
+ 
++>>>>>>> e463a09af2f0 (x86: Add straight-line-speculation mitigation)
  #endif /* __ASSEMBLY__ */
  
  #endif /* _ASM_X86_LINKAGE_H */
diff --cc arch/x86/kernel/ftrace.c
index 65f676625c82,7cc540e6de0c..000000000000
--- a/arch/x86/kernel/ftrace.c
+++ b/arch/x86/kernel/ftrace.c
@@@ -739,6 -303,8 +739,11 @@@ union ftrace_op_code_union 
  	} __attribute__((packed));
  };
  
++<<<<<<< HEAD
++=======
+ #define RET_SIZE		1 + IS_ENABLED(CONFIG_SLS)
+ 
++>>>>>>> e463a09af2f0 (x86: Add straight-line-speculation mitigation)
  static unsigned long
  create_trampoline(struct ftrace_ops *ops, unsigned int *tramp_size)
  {
diff --cc arch/x86/lib/memmove_64.S
index 7ff00ea64e4f,50ea390df712..000000000000
--- a/arch/x86/lib/memmove_64.S
+++ b/arch/x86/lib/memmove_64.S
@@@ -42,7 -40,7 +42,11 @@@ SYM_FUNC_START(__memmove
  	/* FSRM implies ERMS => no length checks, do the copy directly */
  .Lmemmove_begin_forward:
  	ALTERNATIVE "cmp $0x20, %rdx; jb 1f", "", X86_FEATURE_FSRM
++<<<<<<< HEAD
 +	ALTERNATIVE "", "movq %rdx, %rcx; rep movsb; retq", X86_FEATURE_ERMS
++=======
+ 	ALTERNATIVE "", __stringify(movq %rdx, %rcx; rep movsb; RET), X86_FEATURE_ERMS
++>>>>>>> e463a09af2f0 (x86: Add straight-line-speculation mitigation)
  
  	/*
  	 * movsq instruction have many startup latency
diff --cc arch/x86/lib/retpoline.S
index 363ec132df7e,89b3fb244e15..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -4,18 -4,38 +4,29 @@@
  #include <linux/linkage.h>
  #include <asm/dwarf2.h>
  #include <asm/cpufeatures.h>
 -#include <asm/alternative.h>
 +#include <asm/alternative-asm.h>
  #include <asm/export.h>
  #include <asm/nospec-branch.h>
 -#include <asm/unwind_hints.h>
 -#include <asm/frame.h>
 -
 -	.section .text.__x86.indirect_thunk
 -
 -.macro RETPOLINE reg
 -	ANNOTATE_INTRA_FUNCTION_CALL
 -	call    .Ldo_rop_\@
 -.Lspec_trap_\@:
 -	UNWIND_HINT_EMPTY
 -	pause
 -	lfence
 -	jmp .Lspec_trap_\@
 -.Ldo_rop_\@:
 -	mov     %\reg, (%_ASM_SP)
 -	UNWIND_HINT_FUNC
 -	RET
 -.endm
  
  .macro THUNK reg
 +	.section .text.__x86.indirect_thunk
  
++<<<<<<< HEAD
 +SYM_FUNC_START(__x86_indirect_thunk_\reg)
 +	CFI_STARTPROC
 +	JMP_NOSPEC %\reg
 +	CFI_ENDPROC
 +SYM_FUNC_END(__x86_indirect_thunk_\reg)
++=======
+ 	.align RETPOLINE_THUNK_SIZE
+ SYM_INNER_LABEL(__x86_indirect_thunk_\reg, SYM_L_GLOBAL)
+ 	UNWIND_HINT_EMPTY
+ 
+ 	ALTERNATIVE_2 __stringify(ANNOTATE_RETPOLINE_SAFE; jmp *%\reg), \
+ 		      __stringify(RETPOLINE \reg), X86_FEATURE_RETPOLINE, \
+ 		      __stringify(lfence; ANNOTATE_RETPOLINE_SAFE; jmp *%\reg; int3), X86_FEATURE_RETPOLINE_AMD
+ 
++>>>>>>> e463a09af2f0 (x86: Add straight-line-speculation mitigation)
  .endm
  
  /*
diff --cc scripts/Makefile.build
index 67e764f0f806,a4b89b757287..000000000000
--- a/scripts/Makefile.build
+++ b/scripts/Makefile.build
@@@ -234,41 -219,35 +234,53 @@@ sub_cmd_record_mcount = set -e ; perl $
  	"$(if $(part-of-module),1,0)" "$(@)";
  recordmcount_source := $(srctree)/scripts/recordmcount.pl
  endif # BUILD_C_RECORDMCOUNT
 -cmd_record_mcount = $(if $(findstring $(strip $(CC_FLAGS_FTRACE)),$(_c_flags)),	\
 -	$(sub_cmd_record_mcount))
 -endif # CONFIG_FTRACE_MCOUNT_USE_RECORDMCOUNT
 +cmd_record_mcount =						\
 +	if [ "$(findstring $(CC_FLAGS_FTRACE),$(_c_flags))" =	\
 +	     "$(CC_FLAGS_FTRACE)" ]; then			\
 +		$(sub_cmd_record_mcount)			\
 +	fi;
 +endif # CC_USING_RECORD_MCOUNT
 +endif # CONFIG_FTRACE_MCOUNT_RECORD
  
  ifdef CONFIG_STACK_VALIDATION
 +ifneq ($(SKIP_STACK_VALIDATION),1)
  
 -objtool := $(objtree)/tools/objtool/objtool
 +__objtool_obj := $(objtree)/tools/objtool/objtool
  
++<<<<<<< HEAD
 +objtool_args = $(if $(CONFIG_UNWINDER_ORC),orc generate,check)
++=======
+ objtool_args =								\
+ 	$(if $(CONFIG_UNWINDER_ORC),orc generate,check)			\
+ 	$(if $(part-of-module), --module)				\
+ 	$(if $(CONFIG_FRAME_POINTER),, --no-fp)				\
+ 	$(if $(CONFIG_GCOV_KERNEL)$(CONFIG_LTO_CLANG), --no-unreachable)\
+ 	$(if $(CONFIG_RETPOLINE), --retpoline)				\
+ 	$(if $(CONFIG_X86_SMAP), --uaccess)				\
+ 	$(if $(CONFIG_FTRACE_MCOUNT_USE_OBJTOOL), --mcount)		\
+ 	$(if $(CONFIG_SLS), --sls)
++>>>>>>> e463a09af2f0 (x86: Add straight-line-speculation mitigation)
  
 -cmd_objtool = $(if $(objtool-enabled), ; $(objtool) $(objtool_args) $@)
 -cmd_gen_objtooldep = $(if $(objtool-enabled), { echo ; echo '$@: $$(wildcard $(objtool))' ; } >> $(dot-target).cmd)
 +objtool_args += $(if $(part-of-module), --module,)
  
 -endif # CONFIG_STACK_VALIDATION
 -
 -ifdef CONFIG_LTO_CLANG
 +ifndef CONFIG_FRAME_POINTER
 +objtool_args += --no-fp
 +endif
 +ifdef CONFIG_GCOV_KERNEL
 +objtool_args += --no-unreachable
 +else
 +objtool_args += $(call cc-ifversion, -lt, 0405, --no-unreachable)
 +endif
 +ifdef CONFIG_RETPOLINE
 +  objtool_args += --retpoline
 +endif
  
 -# Skip objtool for LLVM bitcode
 -$(obj)/%.o: objtool-enabled :=
  
 +ifdef CONFIG_MODVERSIONS
 +objtool_o = $(@D)/.tmp_$(@F)
  else
 +objtool_o = $(@)
 +endif
  
  # 'OBJECT_FILES_NON_STANDARD := y': skip objtool checking for a directory
  # 'OBJECT_FILES_NON_STANDARD_foo.o := 'y': skip objtool checking for a file
diff --cc scripts/link-vmlinux.sh
index 1c5f1f9c810a,9716f285e404..000000000000
--- a/scripts/link-vmlinux.sh
+++ b/scripts/link-vmlinux.sh
@@@ -75,7 -80,71 +75,75 @@@ modpost_link(
  		${KBUILD_VMLINUX_LIBS}				\
  		--end-group"
  
++<<<<<<< HEAD
 +	${LD} ${LDFLAGS} -r -o ${1} ${objects}
++=======
+ 	if [ -n "${CONFIG_LTO_CLANG}" ]; then
+ 		gen_initcalls
+ 		lds="-T .tmp_initcalls.lds"
+ 
+ 		if [ -n "${CONFIG_MODVERSIONS}" ]; then
+ 			gen_symversions
+ 			lds="${lds} -T .tmp_symversions.lds"
+ 		fi
+ 
+ 		# This might take a while, so indicate that we're doing
+ 		# an LTO link
+ 		info LTO ${1}
+ 	else
+ 		info LD ${1}
+ 	fi
+ 
+ 	${LD} ${KBUILD_LDFLAGS} -r -o ${1} ${lds} ${objects}
+ }
+ 
+ objtool_link()
+ {
+ 	local objtoolcmd;
+ 	local objtoolopt;
+ 
+ 	if [ "${CONFIG_LTO_CLANG} ${CONFIG_STACK_VALIDATION}" = "y y" ]; then
+ 		# Don't perform vmlinux validation unless explicitly requested,
+ 		# but run objtool on vmlinux.o now that we have an object file.
+ 		if [ -n "${CONFIG_UNWINDER_ORC}" ]; then
+ 			objtoolcmd="orc generate"
+ 		fi
+ 
+ 		objtoolopt="${objtoolopt} --duplicate"
+ 
+ 		if [ -n "${CONFIG_FTRACE_MCOUNT_USE_OBJTOOL}" ]; then
+ 			objtoolopt="${objtoolopt} --mcount"
+ 		fi
+ 	fi
+ 
+ 	if [ -n "${CONFIG_VMLINUX_VALIDATION}" ]; then
+ 		objtoolopt="${objtoolopt} --noinstr"
+ 	fi
+ 
+ 	if [ -n "${objtoolopt}" ]; then
+ 		if [ -z "${objtoolcmd}" ]; then
+ 			objtoolcmd="check"
+ 		fi
+ 		objtoolopt="${objtoolopt} --vmlinux"
+ 		if [ -z "${CONFIG_FRAME_POINTER}" ]; then
+ 			objtoolopt="${objtoolopt} --no-fp"
+ 		fi
+ 		if [ -n "${CONFIG_GCOV_KERNEL}" ] || [ -n "${CONFIG_LTO_CLANG}" ]; then
+ 			objtoolopt="${objtoolopt} --no-unreachable"
+ 		fi
+ 		if [ -n "${CONFIG_RETPOLINE}" ]; then
+ 			objtoolopt="${objtoolopt} --retpoline"
+ 		fi
+ 		if [ -n "${CONFIG_X86_SMAP}" ]; then
+ 			objtoolopt="${objtoolopt} --uaccess"
+ 		fi
+ 		if [ -n "${CONFIG_SLS}" ]; then
+ 			objtoolopt="${objtoolopt} --sls"
+ 		fi
+ 		info OBJTOOL ${1}
+ 		tools/objtool/objtool ${objtoolcmd} ${objtoolopt} ${1}
+ 	fi
++>>>>>>> e463a09af2f0 (x86: Add straight-line-speculation mitigation)
  }
  
  # Link of vmlinux
* Unmerged path arch/x86/include/asm/static_call.h
* Unmerged path arch/x86/kernel/static_call.c
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 249fb2c19ce3..7be1abd47c23 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -471,6 +471,18 @@ config RETPOLINE
 	  branches. Requires a compiler with -mindirect-branch=thunk-extern
 	  support for full protection. The kernel may run slower.
 
+config CC_HAS_SLS
+	def_bool $(cc-option,-mharden-sls=all)
+
+config SLS
+	bool "Mitigate Straight-Line-Speculation"
+	depends on CC_HAS_SLS && X86_64
+	default n
+	help
+	  Compile the kernel with straight-line-speculation options to guard
+	  against straight line speculation. The kernel image might be slightly
+	  larger.
+
 config X86_CPU_RESCTRL
 	bool "x86 CPU resource control support"
 	depends on X86 && (CPU_SUP_INTEL || CPU_SUP_AMD)
* Unmerged path arch/x86/Makefile
* Unmerged path arch/x86/include/asm/linkage.h
* Unmerged path arch/x86/include/asm/static_call.h
* Unmerged path arch/x86/kernel/ftrace.c
* Unmerged path arch/x86/kernel/static_call.c
* Unmerged path arch/x86/lib/memmove_64.S
* Unmerged path arch/x86/lib/retpoline.S
* Unmerged path scripts/Makefile.build
* Unmerged path scripts/link-vmlinux.sh
