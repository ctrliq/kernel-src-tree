x86/retbleed: Add fine grained Kconfig knobs

jira LE-1907
cve CVE-2022-23825
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-4.18.0-372.32.1.el8_6
commit-author Peter Zijlstra <peterz@infradead.org>
commit f43b9876e857c739d407bc56df288b0ebe1a9164
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.32.1.el8_6/f43b9876.failed

Do fine-grained Kconfig for all the various retbleed parts.

NOTE: if your compiler doesn't support return thunks this will
silently 'upgrade' your mitigation to IBPB, you might not like this.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit f43b9876e857c739d407bc56df288b0ebe1a9164)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/Makefile
#	arch/x86/entry/calling.h
#	arch/x86/include/asm/disabled-features.h
#	arch/x86/include/asm/linkage.h
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/include/asm/static_call.h
#	arch/x86/kernel/alternative.c
#	arch/x86/kernel/cpu/amd.c
#	arch/x86/kernel/cpu/bugs.c
#	arch/x86/kernel/static_call.c
#	arch/x86/kvm/emulate.c
#	arch/x86/lib/retpoline.S
#	scripts/Makefile.lib
#	scripts/Makefile.vmlinux_o
#	security/Kconfig
#	tools/objtool/builtin-check.c
#	tools/objtool/check.c
#	tools/objtool/include/objtool/builtin.h
diff --cc arch/x86/Kconfig
index 249fb2c19ce3,e58798f636d4..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -458,19 -459,9 +458,22 @@@ config X86_MPPARS
  	  (esp with 64bit cpus) with acpi support, MADT and DSDT will override it
  
  config GOLDFISH
 -	def_bool y
 -	depends on X86_GOLDFISH
 +       def_bool y
 +       depends on X86_GOLDFISH
  
++<<<<<<< HEAD
 +config RETPOLINE
 +	bool "Avoid speculative indirect branches in kernel"
 +	default y
 +	select STACK_VALIDATION if HAVE_STACK_VALIDATION
 +	help
 +	  Compile kernel with the retpoline compiler options to guard against
 +	  kernel-to-user data leaks by avoiding speculative indirect
 +	  branches. Requires a compiler with -mindirect-branch=thunk-extern
 +	  support for full protection. The kernel may run slower.
 +
++=======
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  config X86_CPU_RESCTRL
  	bool "x86 CPU resource control support"
  	depends on X86 && (CPU_SUP_INTEL || CPU_SUP_AMD)
@@@ -2482,33 -2430,97 +2485,118 @@@ source "kernel/livepatch/Kconfig
  
  endmenu
  
+ config CC_HAS_SLS
+ 	def_bool $(cc-option,-mharden-sls=all)
+ 
+ config CC_HAS_RETURN_THUNK
+ 	def_bool $(cc-option,-mfunction-return=thunk-extern)
+ 
+ menuconfig SPECULATION_MITIGATIONS
+ 	bool "Mitigations for speculative execution vulnerabilities"
+ 	default y
+ 	help
+ 	  Say Y here to enable options which enable mitigations for
+ 	  speculative execution hardware vulnerabilities.
+ 
+ 	  If you say N, all mitigations will be disabled. You really
+ 	  should know what you are doing to say so.
+ 
+ if SPECULATION_MITIGATIONS
+ 
+ config PAGE_TABLE_ISOLATION
+ 	bool "Remove the kernel mapping in user mode"
+ 	default y
+ 	depends on (X86_64 || X86_PAE)
+ 	help
+ 	  This feature reduces the number of hardware side channels by
+ 	  ensuring that the majority of kernel addresses are not mapped
+ 	  into userspace.
+ 
+ 	  See Documentation/x86/pti.rst for more details.
+ 
+ config RETPOLINE
+ 	bool "Avoid speculative indirect branches in kernel"
+ 	select OBJTOOL if HAVE_OBJTOOL
+ 	default y
+ 	help
+ 	  Compile kernel with the retpoline compiler options to guard against
+ 	  kernel-to-user data leaks by avoiding speculative indirect
+ 	  branches. Requires a compiler with -mindirect-branch=thunk-extern
+ 	  support for full protection. The kernel may run slower.
+ 
+ config RETHUNK
+ 	bool "Enable return-thunks"
+ 	depends on RETPOLINE && CC_HAS_RETURN_THUNK
+ 	select OBJTOOL if HAVE_OBJTOOL
+ 	default y
+ 	help
+ 	  Compile the kernel with the return-thunks compiler option to guard
+ 	  against kernel-to-user data leaks by avoiding return speculation.
+ 	  Requires a compiler with -mfunction-return=thunk-extern
+ 	  support for full protection. The kernel may run slower.
+ 
+ config CPU_UNRET_ENTRY
+ 	bool "Enable UNRET on kernel entry"
+ 	depends on CPU_SUP_AMD && RETHUNK
+ 	default y
+ 	help
+ 	  Compile the kernel with support for the retbleed=unret mitigation.
+ 
+ config CPU_IBPB_ENTRY
+ 	bool "Enable IBPB on kernel entry"
+ 	depends on CPU_SUP_AMD
+ 	default y
+ 	help
+ 	  Compile the kernel with support for the retbleed=ibpb mitigation.
+ 
+ config CPU_IBRS_ENTRY
+ 	bool "Enable IBRS on kernel entry"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Compile the kernel with support for the spectre_v2=ibrs mitigation.
+ 	  This mitigates both spectre_v2 and retbleed at great cost to
+ 	  performance.
+ 
+ config SLS
+ 	bool "Mitigate Straight-Line-Speculation"
+ 	depends on CC_HAS_SLS && X86_64
+ 	select OBJTOOL if HAVE_OBJTOOL
+ 	default n
+ 	help
+ 	  Compile the kernel with straight-line-speculation options to guard
+ 	  against straight line speculation. The kernel image might be slightly
+ 	  larger.
+ 
+ endif
+ 
  config ARCH_HAS_ADD_PAGES
  	def_bool y
 -	depends on ARCH_ENABLE_MEMORY_HOTPLUG
 +	depends on X86_64 && ARCH_ENABLE_MEMORY_HOTPLUG
 +
 +config ARCH_ENABLE_MEMORY_HOTPLUG
 +	def_bool y
 +	depends on X86_64 || (X86_32 && HIGHMEM)
 +
 +config ARCH_ENABLE_MEMORY_HOTREMOVE
 +	def_bool y
 +	depends on MEMORY_HOTPLUG
 +
 +config USE_PERCPU_NUMA_NODE_ID
 +	def_bool y
 +	depends on NUMA
 +
 +config ARCH_ENABLE_SPLIT_PMD_PTLOCK
 +	def_bool y
 +	depends on X86_64 || X86_PAE
 +
 +config ARCH_ENABLE_HUGEPAGE_MIGRATION
 +	def_bool y
 +	depends on X86_64 && HUGETLB_PAGE && MIGRATION
  
 -config ARCH_MHP_MEMMAP_ON_MEMORY_ENABLE
 +config ARCH_ENABLE_THP_MIGRATION
  	def_bool y
 +	depends on X86_64 && TRANSPARENT_HUGEPAGE
  
  menu "Power management and ACPI options"
  
diff --cc arch/x86/Makefile
index 7621048daf34,1f40dad30d50..000000000000
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@@ -12,6 -12,24 +12,27 @@@ els
          KBUILD_DEFCONFIG := $(ARCH)_defconfig
  endif
  
++<<<<<<< HEAD
++=======
+ ifdef CONFIG_CC_IS_GCC
+ RETPOLINE_CFLAGS	:= $(call cc-option,-mindirect-branch=thunk-extern -mindirect-branch-register)
+ RETPOLINE_CFLAGS	+= $(call cc-option,-mindirect-branch-cs-prefix)
+ RETPOLINE_VDSO_CFLAGS	:= $(call cc-option,-mindirect-branch=thunk-inline -mindirect-branch-register)
+ endif
+ ifdef CONFIG_CC_IS_CLANG
+ RETPOLINE_CFLAGS	:= -mretpoline-external-thunk
+ RETPOLINE_VDSO_CFLAGS	:= -mretpoline
+ endif
+ 
+ ifdef CONFIG_RETHUNK
+ RETHUNK_CFLAGS		:= -mfunction-return=thunk-extern
+ RETPOLINE_CFLAGS	+= $(RETHUNK_CFLAGS)
+ endif
+ 
+ export RETPOLINE_CFLAGS
+ export RETPOLINE_VDSO_CFLAGS
+ 
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  # For gcc stack alignment is specified with -mpreferred-stack-boundary,
  # clang has the option -mstack-alignment for that purpose.
  ifneq ($(call cc-option, -mpreferred-stack-boundary=4),)
diff --cc arch/x86/entry/calling.h
index d7bba29aa91d,f6907627172b..000000000000
--- a/arch/x86/entry/calling.h
+++ b/arch/x86/entry/calling.h
@@@ -317,6 -285,66 +317,69 @@@ For 32-bit we have the following conven
  #endif
  
  /*
++<<<<<<< HEAD
++=======
+  * IBRS kernel mitigation for Spectre_v2.
+  *
+  * Assumes full context is established (PUSH_REGS, CR3 and GS) and it clobbers
+  * the regs it uses (AX, CX, DX). Must be called before the first RET
+  * instruction (NOTE! UNTRAIN_RET includes a RET instruction)
+  *
+  * The optional argument is used to save/restore the current value,
+  * which is used on the paranoid paths.
+  *
+  * Assumes x86_spec_ctrl_{base,current} to have SPEC_CTRL_IBRS set.
+  */
+ .macro IBRS_ENTER save_reg
+ #ifdef CONFIG_CPU_IBRS_ENTRY
+ 	ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ 	movl	$MSR_IA32_SPEC_CTRL, %ecx
+ 
+ .ifnb \save_reg
+ 	rdmsr
+ 	shl	$32, %rdx
+ 	or	%rdx, %rax
+ 	mov	%rax, \save_reg
+ 	test	$SPEC_CTRL_IBRS, %eax
+ 	jz	.Ldo_wrmsr_\@
+ 	lfence
+ 	jmp	.Lend_\@
+ .Ldo_wrmsr_\@:
+ .endif
+ 
+ 	movq	PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ 	movl	%edx, %eax
+ 	shr	$32, %rdx
+ 	wrmsr
+ .Lend_\@:
+ #endif
+ .endm
+ 
+ /*
+  * Similar to IBRS_ENTER, requires KERNEL GS,CR3 and clobbers (AX, CX, DX)
+  * regs. Must be called after the last RET.
+  */
+ .macro IBRS_EXIT save_reg
+ #ifdef CONFIG_CPU_IBRS_ENTRY
+ 	ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ 	movl	$MSR_IA32_SPEC_CTRL, %ecx
+ 
+ .ifnb \save_reg
+ 	mov	\save_reg, %rdx
+ .else
+ 	movq	PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ 	andl	$(~SPEC_CTRL_IBRS), %edx
+ .endif
+ 
+ 	movl	%edx, %eax
+ 	shr	$32, %rdx
+ 	wrmsr
+ .Lend_\@:
+ #endif
+ .endm
+ 
+ /*
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
   * Mitigate Spectre v1 for conditional swapgs code paths.
   *
   * FENCE_SWAPGS_USER_ENTRY is used in the user entry swapgs code path, to
diff --cc arch/x86/include/asm/disabled-features.h
index 8ca9a5833aa4,33d2cd04d254..000000000000
--- a/arch/x86/include/asm/disabled-features.h
+++ b/arch/x86/include/asm/disabled-features.h
@@@ -56,8 -50,30 +56,35 @@@
  # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
  #endif
  
++<<<<<<< HEAD
 +/* Force disable because it's broken beyond repair */
 +#define DISABLE_ENQCMD		(1 << (X86_FEATURE_ENQCMD & 31))
++=======
+ #ifdef CONFIG_RETPOLINE
+ # define DISABLE_RETPOLINE	0
+ #else
+ # define DISABLE_RETPOLINE	((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETPOLINE_LFENCE & 31)))
+ #endif
+ 
+ #ifdef CONFIG_RETHUNK
+ # define DISABLE_RETHUNK	0
+ #else
+ # define DISABLE_RETHUNK	(1 << (X86_FEATURE_RETHUNK & 31))
+ #endif
+ 
+ #ifdef CONFIG_CPU_UNRET_ENTRY
+ # define DISABLE_UNRET		0
+ #else
+ # define DISABLE_UNRET		(1 << (X86_FEATURE_UNRET & 31))
+ #endif
+ 
+ #ifdef CONFIG_INTEL_IOMMU_SVM
+ # define DISABLE_ENQCMD		0
+ #else
+ # define DISABLE_ENQCMD		(1 << (X86_FEATURE_ENQCMD & 31))
+ #endif
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  
  #ifdef CONFIG_X86_SGX
  # define DISABLE_SGX	0
@@@ -76,10 -98,10 +103,14 @@@
  #define DISABLED_MASK5	0
  #define DISABLED_MASK6	0
  #define DISABLED_MASK7	(DISABLE_PTI)
 -#define DISABLED_MASK8	(DISABLE_TDX_GUEST)
 -#define DISABLED_MASK9	(DISABLE_SGX)
 +#define DISABLED_MASK8	0
 +#define DISABLED_MASK9	(DISABLE_MPX|DISABLE_SGX)
  #define DISABLED_MASK10	0
++<<<<<<< HEAD
 +#define DISABLED_MASK11	0
++=======
+ #define DISABLED_MASK11	(DISABLE_RETPOLINE|DISABLE_RETHUNK|DISABLE_UNRET)
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  #define DISABLED_MASK12	0
  #define DISABLED_MASK13	0
  #define DISABLED_MASK14	0
diff --cc arch/x86/include/asm/linkage.h
index e07188e8d763,73ca20049835..000000000000
--- a/arch/x86/include/asm/linkage.h
+++ b/arch/x86/include/asm/linkage.h
@@@ -26,7 -19,59 +26,32 @@@
  #define __ALIGN_STR	__stringify(__ALIGN)
  #endif
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_RETHUNK) && !defined(__DISABLE_EXPORTS) && !defined(BUILD_VDSO)
+ #define RET	jmp __x86_return_thunk
+ #else /* CONFIG_RETPOLINE */
+ #ifdef CONFIG_SLS
+ #define RET	ret; int3
+ #else
+ #define RET	ret
+ #endif
+ #endif /* CONFIG_RETPOLINE */
+ 
+ #else /* __ASSEMBLY__ */
+ 
+ #if defined(CONFIG_RETHUNK) && !defined(__DISABLE_EXPORTS) && !defined(BUILD_VDSO)
+ #define ASM_RET	"jmp __x86_return_thunk\n\t"
+ #else /* CONFIG_RETPOLINE */
+ #ifdef CONFIG_SLS
+ #define ASM_RET	"ret; int3\n\t"
+ #else
+ #define ASM_RET	"ret\n\t"
+ #endif
+ #endif /* CONFIG_RETPOLINE */
+ 
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  #endif /* __ASSEMBLY__ */
  
 -/* SYM_FUNC_START -- use for global functions */
 -#define SYM_FUNC_START(name)				\
 -	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_NOALIGN -- use for global functions, w/o alignment */
 -#define SYM_FUNC_START_NOALIGN(name)			\
 -	SYM_START(name, SYM_L_GLOBAL, SYM_A_NONE)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_LOCAL -- use for local functions */
 -#define SYM_FUNC_START_LOCAL(name)			\
 -	SYM_START(name, SYM_L_LOCAL, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_LOCAL_NOALIGN -- use for local functions, w/o alignment */
 -#define SYM_FUNC_START_LOCAL_NOALIGN(name)		\
 -	SYM_START(name, SYM_L_LOCAL, SYM_A_NONE)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_WEAK -- use for weak functions */
 -#define SYM_FUNC_START_WEAK(name)			\
 -	SYM_START(name, SYM_L_WEAK, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_WEAK_NOALIGN -- use for weak functions, w/o alignment */
 -#define SYM_FUNC_START_WEAK_NOALIGN(name)		\
 -	SYM_START(name, SYM_L_WEAK, SYM_A_NONE)		\
 -	ENDBR
 -
  #endif /* _ASM_X86_LINKAGE_H */
  
diff --cc arch/x86/include/asm/nospec-branch.h
index 797212d0a956,bb05ed4f46bd..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -147,6 -125,31 +147,34 @@@
  	ALTERNATIVE "jmp .Lskip_rsb_\@", "", \ftr
  	__FILL_RETURN_BUFFER(\reg,\nr,%_ASM_SP)
  .Lskip_rsb_\@:
++<<<<<<< HEAD
++=======
+ .endm
+ 
+ #ifdef CONFIG_CPU_UNRET_ENTRY
+ #define CALL_ZEN_UNTRAIN_RET	"call zen_untrain_ret"
+ #else
+ #define CALL_ZEN_UNTRAIN_RET	""
+ #endif
+ 
+ /*
+  * Mitigate RETBleed for AMD/Hygon Zen uarch. Requires KERNEL CR3 because the
+  * return thunk isn't mapped into the userspace tables (then again, AMD
+  * typically has NO_MELTDOWN).
+  *
+  * While zen_untrain_ret() doesn't clobber anything but requires stack,
+  * entry_ibpb() will clobber AX, CX, DX.
+  *
+  * As such, this must be placed after every *SWITCH_TO_KERNEL_CR3 at a point
+  * where we have a stack but before any RET instruction.
+  */
+ .macro UNTRAIN_RET
+ #if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY)
+ 	ANNOTATE_UNRET_END
+ 	ALTERNATIVE_2 "",						\
+ 	              CALL_ZEN_UNTRAIN_RET, X86_FEATURE_UNRET,		\
+ 		      "call entry_ibpb", X86_FEATURE_ENTRY_IBPB
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  #endif
  .endm
  
diff --cc arch/x86/kernel/alternative.c
index 0b65a0cb501d,d6858533e6e5..000000000000
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@@ -432,6 -508,115 +432,118 @@@ void __init_or_module noinline apply_al
  	}
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_RETHUNK
+ /*
+  * Rewrite the compiler generated return thunk tail-calls.
+  *
+  * For example, convert:
+  *
+  *   JMP __x86_return_thunk
+  *
+  * into:
+  *
+  *   RET
+  */
+ static int patch_return(void *addr, struct insn *insn, u8 *bytes)
+ {
+ 	int i = 0;
+ 
+ 	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+ 		return -1;
+ 
+ 	bytes[i++] = RET_INSN_OPCODE;
+ 
+ 	for (; i < insn->length;)
+ 		bytes[i++] = INT3_INSN_OPCODE;
+ 
+ 	return i;
+ }
+ 
+ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+ {
+ 	s32 *s;
+ 
+ 	for (s = start; s < end; s++) {
+ 		void *dest = NULL, *addr = (void *)s + *s;
+ 		struct insn insn;
+ 		int len, ret;
+ 		u8 bytes[16];
+ 		u8 op;
+ 
+ 		ret = insn_decode_kernel(&insn, addr);
+ 		if (WARN_ON_ONCE(ret < 0))
+ 			continue;
+ 
+ 		op = insn.opcode.bytes[0];
+ 		if (op == JMP32_INSN_OPCODE)
+ 			dest = addr + insn.length + insn.immediate.value;
+ 
+ 		if (__static_call_fixup(addr, op, dest) ||
+ 		    WARN_ON_ONCE(dest != &__x86_return_thunk))
+ 			continue;
+ 
+ 		DPRINTK("return thunk at: %pS (%px) len: %d to: %pS",
+ 			addr, addr, insn.length,
+ 			addr + insn.length + insn.immediate.value);
+ 
+ 		len = patch_return(addr, &insn, bytes);
+ 		if (len == insn.length) {
+ 			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
+ 			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
+ 			text_poke_early(addr, bytes, len);
+ 		}
+ 	}
+ }
+ #else
+ void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
+ #endif /* CONFIG_RETHUNK */
+ 
+ #else /* !CONFIG_RETPOLINE || !CONFIG_OBJTOOL */
+ 
+ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end) { }
+ void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
+ 
+ #endif /* CONFIG_RETPOLINE && CONFIG_OBJTOOL */
+ 
+ #ifdef CONFIG_X86_KERNEL_IBT
+ 
+ /*
+  * Generated by: objtool --ibt
+  */
+ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end)
+ {
+ 	s32 *s;
+ 
+ 	for (s = start; s < end; s++) {
+ 		u32 endbr, poison = gen_endbr_poison();
+ 		void *addr = (void *)s + *s;
+ 
+ 		if (WARN_ON_ONCE(get_kernel_nofault(endbr, addr)))
+ 			continue;
+ 
+ 		if (WARN_ON_ONCE(!is_endbr(endbr)))
+ 			continue;
+ 
+ 		DPRINTK("ENDBR at: %pS (%px)", addr, addr);
+ 
+ 		/*
+ 		 * When we have IBT, the lack of ENDBR will trigger #CP
+ 		 */
+ 		DUMP_BYTES(((u8*)addr), 4, "%px: orig: ", addr);
+ 		DUMP_BYTES(((u8*)&poison), 4, "%px: repl: ", addr);
+ 		text_poke_early(addr, &poison, 4);
+ 	}
+ }
+ 
+ #else
+ 
+ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end) { }
+ 
+ #endif /* CONFIG_X86_KERNEL_IBT */
+ 
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  #ifdef CONFIG_SMP
  static void alternatives_smp_lock(const s32 *start, const s32 *end,
  				  u8 *text, u8 *text_end)
diff --cc arch/x86/kernel/cpu/amd.c
index 45b1a646fe2e,35d5288394cb..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -846,6 -853,35 +846,38 @@@ static void init_amd_bd(struct cpuinfo_
  			wrmsrl_safe(MSR_F15H_IC_CFG, value);
  		}
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Some BIOS implementations do not restore proper RDRAND support
+ 	 * across suspend and resume. Check on whether to hide the RDRAND
+ 	 * instruction support via CPUID.
+ 	 */
+ 	clear_rdrand_cpuid_bit(c);
+ }
+ 
+ void init_spectral_chicken(struct cpuinfo_x86 *c)
+ {
+ #ifdef CONFIG_CPU_UNRET_ENTRY
+ 	u64 value;
+ 
+ 	/*
+ 	 * On Zen2 we offer this chicken (bit) on the altar of Speculation.
+ 	 *
+ 	 * This suppresses speculation from the middle of a basic block, i.e. it
+ 	 * suppresses non-branch predictions.
+ 	 *
+ 	 * We use STIBP as a heuristic to filter out Zen2 from the rest of F17H
+ 	 */
+ 	if (!cpu_has(c, X86_FEATURE_HYPERVISOR) && cpu_has(c, X86_FEATURE_AMD_STIBP)) {
+ 		if (!rdmsrl_safe(MSR_ZEN2_SPECTRAL_CHICKEN, &value)) {
+ 			value |= MSR_ZEN2_SPECTRAL_CHICKEN_BIT;
+ 			wrmsrl_safe(MSR_ZEN2_SPECTRAL_CHICKEN, value);
+ 		}
+ 	}
+ #endif
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  }
  
  static void init_amd_zn(struct cpuinfo_x86 *c)
diff --cc arch/x86/kernel/cpu/bugs.c
index ff842ff87312,cf08a1b8f3c7..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -720,6 -772,174 +720,177 @@@ static int __init nospectre_v1_cmdline(
  }
  early_param("nospectre_v1", nospectre_v1_cmdline);
  
++<<<<<<< HEAD
++=======
+ static enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =
+ 	SPECTRE_V2_NONE;
+ 
+ #undef pr_fmt
+ #define pr_fmt(fmt)     "RETBleed: " fmt
+ 
+ enum retbleed_mitigation {
+ 	RETBLEED_MITIGATION_NONE,
+ 	RETBLEED_MITIGATION_UNRET,
+ 	RETBLEED_MITIGATION_IBPB,
+ 	RETBLEED_MITIGATION_IBRS,
+ 	RETBLEED_MITIGATION_EIBRS,
+ };
+ 
+ enum retbleed_mitigation_cmd {
+ 	RETBLEED_CMD_OFF,
+ 	RETBLEED_CMD_AUTO,
+ 	RETBLEED_CMD_UNRET,
+ 	RETBLEED_CMD_IBPB,
+ };
+ 
+ const char * const retbleed_strings[] = {
+ 	[RETBLEED_MITIGATION_NONE]	= "Vulnerable",
+ 	[RETBLEED_MITIGATION_UNRET]	= "Mitigation: untrained return thunk",
+ 	[RETBLEED_MITIGATION_IBPB]	= "Mitigation: IBPB",
+ 	[RETBLEED_MITIGATION_IBRS]	= "Mitigation: IBRS",
+ 	[RETBLEED_MITIGATION_EIBRS]	= "Mitigation: Enhanced IBRS",
+ };
+ 
+ static enum retbleed_mitigation retbleed_mitigation __ro_after_init =
+ 	RETBLEED_MITIGATION_NONE;
+ static enum retbleed_mitigation_cmd retbleed_cmd __ro_after_init =
+ 	RETBLEED_CMD_AUTO;
+ 
+ static int __ro_after_init retbleed_nosmt = false;
+ 
+ static int __init retbleed_parse_cmdline(char *str)
+ {
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	while (str) {
+ 		char *next = strchr(str, ',');
+ 		if (next) {
+ 			*next = 0;
+ 			next++;
+ 		}
+ 
+ 		if (!strcmp(str, "off")) {
+ 			retbleed_cmd = RETBLEED_CMD_OFF;
+ 		} else if (!strcmp(str, "auto")) {
+ 			retbleed_cmd = RETBLEED_CMD_AUTO;
+ 		} else if (!strcmp(str, "unret")) {
+ 			retbleed_cmd = RETBLEED_CMD_UNRET;
+ 		} else if (!strcmp(str, "ibpb")) {
+ 			retbleed_cmd = RETBLEED_CMD_IBPB;
+ 		} else if (!strcmp(str, "nosmt")) {
+ 			retbleed_nosmt = true;
+ 		} else {
+ 			pr_err("Ignoring unknown retbleed option (%s).", str);
+ 		}
+ 
+ 		str = next;
+ 	}
+ 
+ 	return 0;
+ }
+ early_param("retbleed", retbleed_parse_cmdline);
+ 
+ #define RETBLEED_UNTRAIN_MSG "WARNING: BTB untrained return thunk mitigation is only effective on AMD/Hygon!\n"
+ #define RETBLEED_INTEL_MSG "WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!\n"
+ 
+ static void __init retbleed_select_mitigation(void)
+ {
+ 	bool mitigate_smt = false;
+ 
+ 	if (!boot_cpu_has_bug(X86_BUG_RETBLEED) || cpu_mitigations_off())
+ 		return;
+ 
+ 	switch (retbleed_cmd) {
+ 	case RETBLEED_CMD_OFF:
+ 		return;
+ 
+ 	case RETBLEED_CMD_UNRET:
+ 		if (IS_ENABLED(CONFIG_CPU_UNRET_ENTRY)) {
+ 			retbleed_mitigation = RETBLEED_MITIGATION_UNRET;
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_UNRET_ENTRY.\n");
+ 			goto do_cmd_auto;
+ 		}
+ 		break;
+ 
+ 	case RETBLEED_CMD_IBPB:
+ 		if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY)) {
+ 			retbleed_mitigation = RETBLEED_MITIGATION_IBPB;
+ 		} else {
+ 			pr_err("WARNING: kernel not compiled with CPU_IBPB_ENTRY.\n");
+ 			goto do_cmd_auto;
+ 		}
+ 		break;
+ 
+ do_cmd_auto:
+ 	case RETBLEED_CMD_AUTO:
+ 	default:
+ 		if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||
+ 		    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON) {
+ 			if (IS_ENABLED(CONFIG_CPU_UNRET_ENTRY))
+ 				retbleed_mitigation = RETBLEED_MITIGATION_UNRET;
+ 			else if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY))
+ 				retbleed_mitigation = RETBLEED_MITIGATION_IBPB;
+ 		}
+ 
+ 		/*
+ 		 * The Intel mitigation (IBRS or eIBRS) was already selected in
+ 		 * spectre_v2_select_mitigation().  'retbleed_mitigation' will
+ 		 * be set accordingly below.
+ 		 */
+ 
+ 		break;
+ 	}
+ 
+ 	switch (retbleed_mitigation) {
+ 	case RETBLEED_MITIGATION_UNRET:
+ 		setup_force_cpu_cap(X86_FEATURE_RETHUNK);
+ 		setup_force_cpu_cap(X86_FEATURE_UNRET);
+ 
+ 		if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&
+ 		    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)
+ 			pr_err(RETBLEED_UNTRAIN_MSG);
+ 
+ 		mitigate_smt = true;
+ 		break;
+ 
+ 	case RETBLEED_MITIGATION_IBPB:
+ 		setup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);
+ 		mitigate_smt = true;
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ 
+ 	if (mitigate_smt && !boot_cpu_has(X86_FEATURE_STIBP) &&
+ 	    (retbleed_nosmt || cpu_mitigations_auto_nosmt()))
+ 		cpu_smt_disable(false);
+ 
+ 	/*
+ 	 * Let IBRS trump all on Intel without affecting the effects of the
+ 	 * retbleed= cmdline option.
+ 	 */
+ 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) {
+ 		switch (spectre_v2_enabled) {
+ 		case SPECTRE_V2_IBRS:
+ 			retbleed_mitigation = RETBLEED_MITIGATION_IBRS;
+ 			break;
+ 		case SPECTRE_V2_EIBRS:
+ 		case SPECTRE_V2_EIBRS_RETPOLINE:
+ 		case SPECTRE_V2_EIBRS_LFENCE:
+ 			retbleed_mitigation = RETBLEED_MITIGATION_EIBRS;
+ 			break;
+ 		default:
+ 			pr_err(RETBLEED_INTEL_MSG);
+ 		}
+ 	}
+ 
+ 	pr_info("%s\n", retbleed_strings[retbleed_mitigation]);
+ }
+ 
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  #undef pr_fmt
  #define pr_fmt(fmt)     "Spectre V2 : " fmt
  
@@@ -1056,19 -1276,28 +1227,44 @@@ static enum spectre_v2_mitigation_cmd _
  		return SPECTRE_V2_CMD_AUTO;
  	}
  
++<<<<<<< HEAD
 +	if (cmd == SPECTRE_V2_CMD_IBRS ||
 +	    cmd == SPECTRE_V2_CMD_IBRS_ALWAYS ||
 +	    cmd == SPECTRE_V2_CMD_RETPOLINE_IBRS_USER) {
 +		if (!boot_cpu_has(X86_FEATURE_IBRS)) {
 +			pr_err("%s selected but CPU doesn't have IBRS. Switching to AUTO select\n",
 +				mitigation_options[i].option);
 +			return SPECTRE_V2_CMD_AUTO;
 +		}
 +		if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
 +			pr_err("%s selected but CPU has eIBRS. Fall back to use eIBRS instead\n",
 +				mitigation_options[i].option);
 +			return SPECTRE_V2_CMD_EIBRS;
 +		}
++=======
+ 	if (cmd == SPECTRE_V2_CMD_IBRS && !IS_ENABLED(CONFIG_CPU_IBRS_ENTRY)) {
+ 		pr_err("%s selected but not compiled in. Switching to AUTO select\n",
+ 		       mitigation_options[i].option);
+ 		return SPECTRE_V2_CMD_AUTO;
+ 	}
+ 
+ 	if (cmd == SPECTRE_V2_CMD_IBRS && boot_cpu_data.x86_vendor != X86_VENDOR_INTEL) {
+ 		pr_err("%s selected but not Intel CPU. Switching to AUTO select\n",
+ 		       mitigation_options[i].option);
+ 		return SPECTRE_V2_CMD_AUTO;
+ 	}
+ 
+ 	if (cmd == SPECTRE_V2_CMD_IBRS && !boot_cpu_has(X86_FEATURE_IBRS)) {
+ 		pr_err("%s selected but CPU doesn't have IBRS. Switching to AUTO select\n",
+ 		       mitigation_options[i].option);
+ 		return SPECTRE_V2_CMD_AUTO;
+ 	}
+ 
+ 	if (cmd == SPECTRE_V2_CMD_IBRS && boot_cpu_has(X86_FEATURE_XENPV)) {
+ 		pr_err("%s selected but running as XenPV guest. Switching to AUTO select\n",
+ 		       mitigation_options[i].option);
+ 		return SPECTRE_V2_CMD_AUTO;
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  	}
  
  	spec_v2_print_cond(mitigation_options[i].option,
@@@ -1110,12 -1339,13 +1306,22 @@@ static void __init spectre_v2_select_mi
  			break;
  		}
  
++<<<<<<< HEAD
 +		/*
 +		 * For Skylake, we print a warning if IBRS isn't chosen.
 +		 */
 +		if (is_skylake_era() && boot_cpu_has(X86_FEATURE_IBRS)) {
 +			pr_warn("Using retpoline on Skylake-generation processors may not fully mitigate the vulnerability.\n");
 +			pr_warn("Add the \"spectre_v2=ibrs\" kernel boot flag to enable IBRS on Skylake systems that need full mitigation.\n");
++=======
+ 		if (IS_ENABLED(CONFIG_CPU_IBRS_ENTRY) &&
+ 		    boot_cpu_has_bug(X86_BUG_RETBLEED) &&
+ 		    retbleed_cmd != RETBLEED_CMD_OFF &&
+ 		    boot_cpu_has(X86_FEATURE_IBRS) &&
+ 		    boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) {
+ 			mode = SPECTRE_V2_IBRS;
+ 			break;
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  		}
  
  		mode = spectre_v2_select_retpoline();
diff --cc arch/x86/kvm/emulate.c
index 7745708ea5f4,db96bf7d1122..000000000000
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@@ -427,18 -433,31 +427,36 @@@ static int fastop(struct x86_emulate_ct
  	FOP_END
  
  /* Special case for SETcc - 1 instruction per cc */
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Depending on .config the SETcc functions look like:
+  *
+  * ENDBR			[4 bytes; CONFIG_X86_KERNEL_IBT]
+  * SETcc %al			[3 bytes]
+  * RET | JMP __x86_return_thunk	[1,5 bytes; CONFIG_RETHUNK]
+  * INT3				[1 byte; CONFIG_SLS]
+  */
+ #define RET_LENGTH	(1 + (4 * IS_ENABLED(CONFIG_RETHUNK)) + \
+ 			 IS_ENABLED(CONFIG_SLS))
+ #define SETCC_LENGTH	(ENDBR_INSN_SIZE + 3 + RET_LENGTH)
+ #define SETCC_ALIGN	(4 << ((SETCC_LENGTH > 4) & 1) << ((SETCC_LENGTH > 8) & 1))
+ static_assert(SETCC_LENGTH <= SETCC_ALIGN);
+ 
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  #define FOP_SETCC(op) \
 -	".align " __stringify(SETCC_ALIGN) " \n\t" \
 +	".align 4 \n\t" \
  	".type " #op ", @function \n\t" \
  	#op ": \n\t" \
 -	ASM_ENDBR \
  	#op " %al \n\t" \
 -	__FOP_RET(#op) \
 -	".skip " __stringify(SETCC_ALIGN) " - (.-" #op "), 0xcc \n\t"
 +	__FOP_RET(#op)
 +
 +asm(".pushsection .fixup, \"ax\"\n"
 +    "kvm_fastop_exception: xor %esi, %esi; ret\n"
 +    ".popsection");
  
 -__FOP_START(setcc, SETCC_ALIGN)
 +FOP_START(setcc)
  FOP_SETCC(seto)
  FOP_SETCC(setno)
  FOP_SETCC(setc)
diff --cc arch/x86/lib/retpoline.S
index 363ec132df7e,073289a55f84..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -24,25 -45,98 +24,117 @@@ SYM_FUNC_END(__x86_indirect_thunk_\reg
   * only see one instance of "__x86_indirect_thunk_\reg" rather
   * than one per register with the correct names. So we do it
   * the simple and nasty way...
 - *
 - * Worse, you can only have a single EXPORT_SYMBOL per line,
 - * and CPP can't insert newlines, so we have to repeat everything
 - * at least twice.
   */
 -
 +#define __EXPORT_THUNK(sym) _ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 +#define EXPORT_THUNK(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 +#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)
 +
++<<<<<<< HEAD
 +GENERATE_THUNK(_ASM_AX)
 +GENERATE_THUNK(_ASM_BX)
 +GENERATE_THUNK(_ASM_CX)
 +GENERATE_THUNK(_ASM_DX)
 +GENERATE_THUNK(_ASM_SI)
 +GENERATE_THUNK(_ASM_DI)
 +GENERATE_THUNK(_ASM_BP)
 +#ifdef CONFIG_64BIT
 +GENERATE_THUNK(r8)
 +GENERATE_THUNK(r9)
 +GENERATE_THUNK(r10)
 +GENERATE_THUNK(r11)
 +GENERATE_THUNK(r12)
 +GENERATE_THUNK(r13)
 +GENERATE_THUNK(r14)
 +GENERATE_THUNK(r15)
 +#endif
++=======
+ #define __EXPORT_THUNK(sym)	_ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
+ #define EXPORT_THUNK(reg)	__EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
+ 
+ 	.align RETPOLINE_THUNK_SIZE
+ SYM_CODE_START(__x86_indirect_thunk_array)
+ 
+ #define GEN(reg) THUNK reg
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ 	.align RETPOLINE_THUNK_SIZE
+ SYM_CODE_END(__x86_indirect_thunk_array)
+ 
+ #define GEN(reg) EXPORT_THUNK(reg)
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ /*
+  * This function name is magical and is used by -mfunction-return=thunk-extern
+  * for the compiler to generate JMPs to it.
+  */
+ #ifdef CONFIG_RETHUNK
+ 
+ 	.section .text.__x86.return_thunk
+ 
+ /*
+  * Safety details here pertain to the AMD Zen{1,2} microarchitecture:
+  * 1) The RET at __x86_return_thunk must be on a 64 byte boundary, for
+  *    alignment within the BTB.
+  * 2) The instruction at zen_untrain_ret must contain, and not
+  *    end with, the 0xc3 byte of the RET.
+  * 3) STIBP must be enabled, or SMT disabled, to prevent the sibling thread
+  *    from re-poisioning the BTB prediction.
+  */
+ 	.align 64
+ 	.skip 63, 0xcc
+ SYM_FUNC_START_NOALIGN(zen_untrain_ret);
+ 
+ 	/*
+ 	 * As executed from zen_untrain_ret, this is:
+ 	 *
+ 	 *   TEST $0xcc, %bl
+ 	 *   LFENCE
+ 	 *   JMP __x86_return_thunk
+ 	 *
+ 	 * Executing the TEST instruction has a side effect of evicting any BTB
+ 	 * prediction (potentially attacker controlled) attached to the RET, as
+ 	 * __x86_return_thunk + 1 isn't an instruction boundary at the moment.
+ 	 */
+ 	.byte	0xf6
+ 
+ 	/*
+ 	 * As executed from __x86_return_thunk, this is a plain RET.
+ 	 *
+ 	 * As part of the TEST above, RET is the ModRM byte, and INT3 the imm8.
+ 	 *
+ 	 * We subsequently jump backwards and architecturally execute the RET.
+ 	 * This creates a correct BTB prediction (type=ret), but in the
+ 	 * meantime we suffer Straight Line Speculation (because the type was
+ 	 * no branch) which is halted by the INT3.
+ 	 *
+ 	 * With SMT enabled and STIBP active, a sibling thread cannot poison
+ 	 * RET's prediction to a type of its choice, but can evict the
+ 	 * prediction due to competitive sharing. If the prediction is
+ 	 * evicted, __x86_return_thunk will suffer Straight Line Speculation
+ 	 * which will be contained safely by the INT3.
+ 	 */
+ SYM_INNER_LABEL(__x86_return_thunk, SYM_L_GLOBAL)
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
+ 
+ 	/*
+ 	 * Ensure the TEST decoding / BTB invalidation is complete.
+ 	 */
+ 	lfence
+ 
+ 	/*
+ 	 * Jump back and execute the RET in the middle of the TEST instruction.
+ 	 * INT3 is for SLS protection.
+ 	 */
+ 	jmp __x86_return_thunk
+ 	int3
+ SYM_FUNC_END(zen_untrain_ret)
+ __EXPORT_THUNK(zen_untrain_ret)
+ 
+ EXPORT_SYMBOL(__x86_return_thunk)
+ 
+ #endif /* CONFIG_RETHUNK */
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
diff --cc scripts/Makefile.lib
index ec05e9cae8f6,3fb6a99e78c4..000000000000
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@@ -187,6 -225,33 +187,36 @@@ dtc_cpp_flags  = -Wp,-MD,$(depfile).pre
  		 $(addprefix -I,$(DTC_INCLUDE))                          \
  		 -undef -D__DTS__
  
++<<<<<<< HEAD
++=======
+ ifdef CONFIG_OBJTOOL
+ 
+ objtool := $(objtree)/tools/objtool/objtool
+ 
+ objtool_args =								\
+ 	$(if $(CONFIG_HAVE_JUMP_LABEL_HACK), --hacks=jump_label)	\
+ 	$(if $(CONFIG_HAVE_NOINSTR_HACK), --hacks=noinstr)		\
+ 	$(if $(CONFIG_X86_KERNEL_IBT), --ibt)				\
+ 	$(if $(CONFIG_FTRACE_MCOUNT_USE_OBJTOOL), --mcount)		\
+ 	$(if $(CONFIG_UNWINDER_ORC), --orc)				\
+ 	$(if $(CONFIG_RETPOLINE), --retpoline)				\
+ 	$(if $(CONFIG_RETHUNK), --rethunk)				\
+ 	$(if $(CONFIG_SLS), --sls)					\
+ 	$(if $(CONFIG_STACK_VALIDATION), --stackval)			\
+ 	$(if $(CONFIG_HAVE_STATIC_CALL_INLINE), --static-call)		\
+ 	$(if $(CONFIG_HAVE_UACCESS_VALIDATION), --uaccess)		\
+ 	$(if $(delay-objtool), --link)					\
+ 	$(if $(part-of-module), --module)				\
+ 	$(if $(CONFIG_GCOV_KERNEL), --no-unreachable)
+ 
+ delay-objtool := $(or $(CONFIG_LTO_CLANG),$(CONFIG_X86_KERNEL_IBT))
+ 
+ cmd_objtool = $(if $(objtool-enabled), ; $(objtool) $(objtool_args) $@)
+ cmd_gen_objtooldep = $(if $(objtool-enabled), { echo ; echo '$@: $$(wildcard $(objtool))' ; } >> $(dot-target).cmd)
+ 
+ endif # CONFIG_OBJTOOL
+ 
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  # Useful for describing the dependency of composite objects
  # Usage:
  #   $(call multi_depend, multi_used_targets, suffix_to_remove, suffix_to_add)
diff --cc security/Kconfig
index bb3e22d43295,e6db09a779b7..000000000000
--- a/security/Kconfig
+++ b/security/Kconfig
@@@ -54,17 -54,6 +54,20 @@@ config SECURITY_NETWOR
  	  implement socket and networking access controls.
  	  If you are unsure how to answer this question, answer N.
  
++<<<<<<< HEAD
 +config PAGE_TABLE_ISOLATION
 +	bool "Remove the kernel mapping in user mode"
 +	default y
 +	depends on X86_64 && !UML
 +	help
 +	  This feature reduces the number of hardware side channels by
 +	  ensuring that the majority of kernel addresses are not mapped
 +	  into userspace.
 +
 +	  See Documentation/x86/pti.txt for more details.
 +
++=======
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  config SECURITY_INFINIBAND
  	bool "Infiniband Security Hooks"
  	depends on SECURITY && INFINIBAND
diff --cc tools/objtool/builtin-check.c
index f3b378126011,24fbe803a0d3..000000000000
--- a/tools/objtool/builtin-check.c
+++ b/tools/objtool/builtin-check.c
@@@ -36,26 -21,192 +36,148 @@@ static const char * const check_usage[
  	NULL,
  };
  
 -static const char * const env_usage[] = {
 -	"OBJTOOL_ARGS=\"<options>\"",
 -	NULL,
 +const struct option check_options[] = {
++<<<<<<< HEAD
 +	OPT_BOOLEAN('f', "no-fp", &no_fp, "Skip frame pointer validation"),
 +	OPT_BOOLEAN('u', "no-unreachable", &no_unreachable, "Skip 'unreachable instruction' warnings"),
 +	OPT_BOOLEAN('r', "retpoline", &retpoline, "Validate retpoline assumptions"),
 +	OPT_BOOLEAN('m', "module", &module, "Indicates the object will be part of a kernel module"),
 +	OPT_BOOLEAN('b', "backtrace", &backtrace, "unwind on error"),
 +	OPT_BOOLEAN('a', "uaccess", &uaccess, "enable uaccess checking"),
 +	OPT_END(),
  };
  
 -static int parse_dump(const struct option *opt, const char *str, int unset)
 -{
 -	if (!str || !strcmp(str, "orc")) {
 -		opts.dump_orc = true;
 -		return 0;
 -	}
 -
 -	return -1;
 -}
 -
 -static int parse_hacks(const struct option *opt, const char *str, int unset)
 -{
 -	bool found = false;
 -
 -	/*
 -	 * Use strstr() as a lazy method of checking for comma-separated
 -	 * options.
 -	 *
 -	 * No string provided == enable all options.
 -	 */
 -
 -	if (!str || strstr(str, "jump_label")) {
 -		opts.hack_jump_label = true;
 -		found = true;
 -	}
 -
 -	if (!str || strstr(str, "noinstr")) {
 -		opts.hack_noinstr = true;
 -		found = true;
 -	}
 -
 -	return found ? 0 : -1;
 -}
 -
 -const struct option check_options[] = {
 +int cmd_check(int argc, const char **argv)
++=======
+ 	OPT_GROUP("Actions:"),
+ 	OPT_CALLBACK_OPTARG('h', "hacks", NULL, NULL, "jump_label,noinstr", "patch toolchain bugs/limitations", parse_hacks),
+ 	OPT_BOOLEAN('i', "ibt", &opts.ibt, "validate and annotate IBT"),
+ 	OPT_BOOLEAN('m', "mcount", &opts.mcount, "annotate mcount/fentry calls for ftrace"),
+ 	OPT_BOOLEAN('n', "noinstr", &opts.noinstr, "validate noinstr rules"),
+ 	OPT_BOOLEAN('o', "orc", &opts.orc, "generate ORC metadata"),
+ 	OPT_BOOLEAN('r', "retpoline", &opts.retpoline, "validate and annotate retpoline usage"),
+ 	OPT_BOOLEAN(0,   "rethunk", &opts.rethunk, "validate and annotate rethunk usage"),
+ 	OPT_BOOLEAN(0,   "unret", &opts.unret, "validate entry unret placement"),
+ 	OPT_BOOLEAN('l', "sls", &opts.sls, "validate straight-line-speculation mitigations"),
+ 	OPT_BOOLEAN('s', "stackval", &opts.stackval, "validate frame pointer rules"),
+ 	OPT_BOOLEAN('t', "static-call", &opts.static_call, "annotate static calls"),
+ 	OPT_BOOLEAN('u', "uaccess", &opts.uaccess, "validate uaccess rules for SMAP"),
+ 	OPT_CALLBACK_OPTARG(0, "dump", NULL, NULL, "orc", "dump metadata", parse_dump),
+ 
+ 	OPT_GROUP("Options:"),
+ 	OPT_BOOLEAN(0, "backtrace", &opts.backtrace, "unwind on error"),
+ 	OPT_BOOLEAN(0, "backup", &opts.backup, "create .orig files before modification"),
+ 	OPT_BOOLEAN(0, "dry-run", &opts.dryrun, "don't write modifications"),
+ 	OPT_BOOLEAN(0, "link", &opts.link, "object is a linked object"),
+ 	OPT_BOOLEAN(0, "module", &opts.module, "object is part of a kernel module"),
+ 	OPT_BOOLEAN(0, "no-unreachable", &opts.no_unreachable, "skip 'unreachable instruction' warnings"),
+ 	OPT_BOOLEAN(0, "sec-address", &opts.sec_address, "print section addresses in warnings"),
+ 	OPT_BOOLEAN(0, "stats", &opts.stats, "print statistics"),
+ 
+ 	OPT_END(),
+ };
+ 
+ int cmd_parse_options(int argc, const char **argv, const char * const usage[])
+ {
+ 	const char *envv[16] = { };
+ 	char *env;
+ 	int envc;
+ 
+ 	env = getenv("OBJTOOL_ARGS");
+ 	if (env) {
+ 		envv[0] = "OBJTOOL_ARGS";
+ 		for (envc = 1; envc < ARRAY_SIZE(envv); ) {
+ 			envv[envc++] = env;
+ 			env = strchr(env, ' ');
+ 			if (!env)
+ 				break;
+ 			*env = '\0';
+ 			env++;
+ 		}
+ 
+ 		parse_options(envc, envv, check_options, env_usage, 0);
+ 	}
+ 
+ 	argc = parse_options(argc, argv, check_options, usage, 0);
+ 	if (argc != 1)
+ 		usage_with_options(usage, check_options);
+ 	return argc;
+ }
+ 
+ static bool opts_valid(void)
+ {
+ 	if (opts.hack_jump_label	||
+ 	    opts.hack_noinstr		||
+ 	    opts.ibt			||
+ 	    opts.mcount			||
+ 	    opts.noinstr		||
+ 	    opts.orc			||
+ 	    opts.retpoline		||
+ 	    opts.rethunk		||
+ 	    opts.sls			||
+ 	    opts.stackval		||
+ 	    opts.static_call		||
+ 	    opts.uaccess) {
+ 		if (opts.dump_orc) {
+ 			ERROR("--dump can't be combined with other options");
+ 			return false;
+ 		}
+ 
+ 		return true;
+ 	}
+ 
+ 	if (opts.unret && !opts.rethunk) {
+ 		ERROR("--unret requires --rethunk");
+ 		return false;
+ 	}
+ 
+ 	if (opts.dump_orc)
+ 		return true;
+ 
+ 	ERROR("At least one command required");
+ 	return false;
+ }
+ 
+ static bool link_opts_valid(struct objtool_file *file)
+ {
+ 	if (opts.link)
+ 		return true;
+ 
+ 	if (has_multiple_files(file->elf)) {
+ 		ERROR("Linked object detected, forcing --link");
+ 		opts.link = true;
+ 		return true;
+ 	}
+ 
+ 	if (opts.noinstr) {
+ 		ERROR("--noinstr requires --link");
+ 		return false;
+ 	}
+ 
+ 	if (opts.ibt) {
+ 		ERROR("--ibt requires --link");
+ 		return false;
+ 	}
+ 
+ 	if (opts.unret) {
+ 		ERROR("--unret requires --link");
+ 		return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
+ int objtool_run(int argc, const char **argv)
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  {
  	const char *objname;
 -	struct objtool_file *file;
 -	int ret;
 -
 -	argc = cmd_parse_options(argc, argv, check_usage);
 -	objname = argv[0];
 -
 -	if (!opts_valid())
 -		return 1;
 -
 -	if (opts.dump_orc)
 -		return orc_dump(objname);
  
 -	file = objtool_open_read(objname);
 -	if (!file)
 -		return 1;
 +	argc = parse_options(argc, argv, check_options, check_usage, 0);
  
 -	if (!link_opts_valid(file))
 -		return 1;
 -
 -	ret = check(file);
 -	if (ret)
 -		return ret;
 +	if (argc != 1)
 +		usage_with_options(check_usage, check_options);
  
 -	if (file->elf->changed)
 -		return elf_write(file->elf);
 +	objname = argv[0];
  
 -	return 0;
 +	return check(objname, false);
  }
diff --cc tools/objtool/check.c
index 2178cad40b84,7bebdb8867cd..000000000000
--- a/tools/objtool/check.c
+++ b/tools/objtool/check.c
@@@ -2313,12 -3728,20 +2313,26 @@@ static int validate_retpoline(struct ob
  		 * loaded late, they very much do need retpoline in their
  		 * .init.text
  		 */
 -		if (!strcmp(insn->sec->name, ".init.text") && !opts.module)
 +		if (!strcmp(insn->sec->name, ".init.text") && !module)
  			continue;
  
++<<<<<<< HEAD
 +		WARN_FUNC("indirect %s found in RETPOLINE build",
 +			  insn->sec, insn->offset,
 +			  insn->type == INSN_JUMP_DYNAMIC ? "jump" : "call");
++=======
+ 		if (insn->type == INSN_RETURN) {
+ 			if (opts.rethunk) {
+ 				WARN_FUNC("'naked' return found in RETHUNK build",
+ 					  insn->sec, insn->offset);
+ 			} else
+ 				continue;
+ 		} else {
+ 			WARN_FUNC("indirect %s found in RETPOLINE build",
+ 				  insn->sec, insn->offset,
+ 				  insn->type == INSN_JUMP_DYNAMIC ? "jump" : "call");
+ 		}
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  
  		warnings++;
  	}
@@@ -2512,18 -4230,79 +2526,65 @@@ int check(const char *_objname, bool or
  		warnings += ret;
  	}
  
 -	if (opts.unret) {
 -		/*
 -		 * Must be after validate_branch() and friends, it plays
 -		 * further games with insn->visited.
 -		 */
 -		ret = validate_unret(file);
 -		if (ret < 0)
 -			return ret;
 -		warnings += ret;
 -	}
 -
 -	if (opts.ibt) {
 -		ret = validate_ibt(file);
 +	if (orc) {
 +		ret = create_orc(&file);
  		if (ret < 0)
  			goto out;
 -		warnings += ret;
 -	}
  
 -	if (opts.sls) {
 -		ret = validate_sls(file);
 +		ret = create_orc_sections(&file);
  		if (ret < 0)
  			goto out;
 -		warnings += ret;
 -	}
  
 -	if (opts.static_call) {
 -		ret = create_static_call_sections(file);
 +		ret = elf_write(file.elf);
  		if (ret < 0)
  			goto out;
++<<<<<<< HEAD
++=======
+ 		warnings += ret;
+ 	}
+ 
+ 	if (opts.retpoline) {
+ 		ret = create_retpoline_sites_sections(file);
+ 		if (ret < 0)
+ 			goto out;
+ 		warnings += ret;
+ 	}
+ 
+ 	if (opts.rethunk) {
+ 		ret = create_return_sites_sections(file);
+ 		if (ret < 0)
+ 			goto out;
+ 		warnings += ret;
+ 	}
+ 
+ 	if (opts.mcount) {
+ 		ret = create_mcount_loc_sections(file);
+ 		if (ret < 0)
+ 			goto out;
+ 		warnings += ret;
+ 	}
+ 
+ 	if (opts.ibt) {
+ 		ret = create_ibt_endbr_seal_sections(file);
+ 		if (ret < 0)
+ 			goto out;
+ 		warnings += ret;
+ 	}
+ 
+ 	if (opts.orc && !list_empty(&file->insn_list)) {
+ 		ret = orc_create(file);
+ 		if (ret < 0)
+ 			goto out;
+ 		warnings += ret;
+ 	}
+ 
+ 
+ 	if (opts.stats) {
+ 		printf("nr_insns_visited: %ld\n", nr_insns_visited);
+ 		printf("nr_cfi: %ld\n", nr_cfi);
+ 		printf("nr_cfi_reused: %ld\n", nr_cfi_reused);
+ 		printf("nr_cfi_cache: %ld\n", nr_cfi_cache);
++>>>>>>> f43b9876e857 (x86/retbleed: Add fine grained Kconfig knobs)
  	}
  
  out:
* Unmerged path arch/x86/include/asm/static_call.h
* Unmerged path arch/x86/kernel/static_call.c
* Unmerged path scripts/Makefile.vmlinux_o
* Unmerged path tools/objtool/include/objtool/builtin.h
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/Makefile
* Unmerged path arch/x86/entry/calling.h
* Unmerged path arch/x86/include/asm/disabled-features.h
* Unmerged path arch/x86/include/asm/linkage.h
* Unmerged path arch/x86/include/asm/nospec-branch.h
* Unmerged path arch/x86/include/asm/static_call.h
* Unmerged path arch/x86/kernel/alternative.c
* Unmerged path arch/x86/kernel/cpu/amd.c
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path arch/x86/kernel/static_call.c
* Unmerged path arch/x86/kvm/emulate.c
* Unmerged path arch/x86/lib/retpoline.S
* Unmerged path scripts/Makefile.lib
* Unmerged path scripts/Makefile.vmlinux_o
* Unmerged path security/Kconfig
* Unmerged path tools/objtool/builtin-check.c
* Unmerged path tools/objtool/check.c
* Unmerged path tools/objtool/include/objtool/builtin.h
