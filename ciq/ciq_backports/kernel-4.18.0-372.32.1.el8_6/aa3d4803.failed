x86: Use return-thunk in asm code

jira LE-1907
cve CVE-2022-23825
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-4.18.0-372.32.1.el8_6
commit-author Peter Zijlstra <peterz@infradead.org>
commit aa3d480315ba6c3025a60958e1981072ea37c3df
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.32.1.el8_6/aa3d4803.failed

Use the return thunk in asm code. If the thunk isn't needed, it will
get patched into a RET instruction during boot by apply_returns().

Since alternatives can't handle relocations outside of the first
instruction, putting a 'jmp __x86_return_thunk' in one is not valid,
therefore carve out the memmove ERMS path into a separate label and jump
to it.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit aa3d480315ba6c3025a60958e1981072ea37c3df)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/vdso/Makefile
#	arch/x86/include/asm/linkage.h
#	arch/x86/lib/memmove_64.S
diff --cc arch/x86/entry/vdso/Makefile
index e86e36e6f659,76cd790ed0bd..000000000000
--- a/arch/x86/entry/vdso/Makefile
+++ b/arch/x86/entry/vdso/Makefile
@@@ -90,7 -91,8 +90,12 @@@ ifneq ($(RETPOLINE_VDSO_CFLAGS),
  endif
  endif
  
++<<<<<<< HEAD
 +$(vobjs): KBUILD_CFLAGS := $(filter-out $(GCC_PLUGINS_CFLAGS) $(RETPOLINE_CFLAGS),$(KBUILD_CFLAGS)) $(CFL)
++=======
+ $(vobjs): KBUILD_CFLAGS := $(filter-out $(CC_FLAGS_LTO) $(RANDSTRUCT_CFLAGS) $(GCC_PLUGINS_CFLAGS) $(RETPOLINE_CFLAGS),$(KBUILD_CFLAGS)) $(CFL)
+ $(vobjs): KBUILD_AFLAGS += -DBUILD_VDSO
++>>>>>>> aa3d480315ba (x86: Use return-thunk in asm code)
  
  #
  # vDSO code runs in userspace and -pg doesn't help with profiling anyway.
diff --cc arch/x86/include/asm/linkage.h
index e07188e8d763,e3ae331cabb1..000000000000
--- a/arch/x86/include/asm/linkage.h
+++ b/arch/x86/include/asm/linkage.h
@@@ -26,7 -19,59 +26,32 @@@
  #define __ALIGN_STR	__stringify(__ALIGN)
  #endif
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_RETPOLINE) && !defined(__DISABLE_EXPORTS) && !defined(BUILD_VDSO)
+ #define RET	jmp __x86_return_thunk
+ #else /* CONFIG_RETPOLINE */
+ #ifdef CONFIG_SLS
+ #define RET	ret; int3
+ #else
+ #define RET	ret
+ #endif
+ #endif /* CONFIG_RETPOLINE */
+ 
+ #else /* __ASSEMBLY__ */
+ 
+ #if defined(CONFIG_RETPOLINE) && !defined(__DISABLE_EXPORTS) && !defined(BUILD_VDSO)
+ #define ASM_RET	"jmp __x86_return_thunk\n\t"
+ #else /* CONFIG_RETPOLINE */
+ #ifdef CONFIG_SLS
+ #define ASM_RET	"ret; int3\n\t"
+ #else
+ #define ASM_RET	"ret\n\t"
+ #endif
+ #endif /* CONFIG_RETPOLINE */
+ 
++>>>>>>> aa3d480315ba (x86: Use return-thunk in asm code)
  #endif /* __ASSEMBLY__ */
  
 -/* SYM_FUNC_START -- use for global functions */
 -#define SYM_FUNC_START(name)				\
 -	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_NOALIGN -- use for global functions, w/o alignment */
 -#define SYM_FUNC_START_NOALIGN(name)			\
 -	SYM_START(name, SYM_L_GLOBAL, SYM_A_NONE)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_LOCAL -- use for local functions */
 -#define SYM_FUNC_START_LOCAL(name)			\
 -	SYM_START(name, SYM_L_LOCAL, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_LOCAL_NOALIGN -- use for local functions, w/o alignment */
 -#define SYM_FUNC_START_LOCAL_NOALIGN(name)		\
 -	SYM_START(name, SYM_L_LOCAL, SYM_A_NONE)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_WEAK -- use for weak functions */
 -#define SYM_FUNC_START_WEAK(name)			\
 -	SYM_START(name, SYM_L_WEAK, SYM_A_ALIGN)	\
 -	ENDBR
 -
 -/* SYM_FUNC_START_WEAK_NOALIGN -- use for weak functions, w/o alignment */
 -#define SYM_FUNC_START_WEAK_NOALIGN(name)		\
 -	SYM_START(name, SYM_L_WEAK, SYM_A_NONE)		\
 -	ENDBR
 -
  #endif /* _ASM_X86_LINKAGE_H */
  
diff --cc arch/x86/lib/memmove_64.S
index 7ff00ea64e4f,724bbf83eb5b..000000000000
--- a/arch/x86/lib/memmove_64.S
+++ b/arch/x86/lib/memmove_64.S
@@@ -42,7 -39,7 +42,11 @@@ SYM_FUNC_START(__memmove
  	/* FSRM implies ERMS => no length checks, do the copy directly */
  .Lmemmove_begin_forward:
  	ALTERNATIVE "cmp $0x20, %rdx; jb 1f", "", X86_FEATURE_FSRM
++<<<<<<< HEAD
 +	ALTERNATIVE "", "movq %rdx, %rcx; rep movsb; retq", X86_FEATURE_ERMS
++=======
+ 	ALTERNATIVE "", "jmp .Lmemmove_erms", X86_FEATURE_ERMS
++>>>>>>> aa3d480315ba (x86: Use return-thunk in asm code)
  
  	/*
  	 * movsq instruction have many startup latency
@@@ -207,8 -204,14 +211,17 @@@
  	movb (%rsi), %r11b
  	movb %r11b, (%rdi)
  13:
++<<<<<<< HEAD
 +	retq
++=======
+ 	RET
+ 
+ .Lmemmove_erms:
+ 	movq %rdx, %rcx
+ 	rep movsb
+ 	RET
++>>>>>>> aa3d480315ba (x86: Use return-thunk in asm code)
  SYM_FUNC_END(__memmove)
 +SYM_FUNC_END_ALIAS(memmove)
  EXPORT_SYMBOL(__memmove)
 -
 -SYM_FUNC_ALIAS_WEAK(memmove, __memmove)
  EXPORT_SYMBOL(memmove)
* Unmerged path arch/x86/entry/vdso/Makefile
* Unmerged path arch/x86/include/asm/linkage.h
* Unmerged path arch/x86/lib/memmove_64.S
