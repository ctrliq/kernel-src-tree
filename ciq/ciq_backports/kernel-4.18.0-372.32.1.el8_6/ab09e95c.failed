x86/kprobes: Convert to text-patching.h

jira LE-1907
cve CVE-2022-23825
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-4.18.0-372.32.1.el8_6
commit-author Peter Zijlstra <peterz@infradead.org>
commit ab09e95ca0c697e67f986c4a45a179abf47e7bfc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.32.1.el8_6/ab09e95c.failed

Convert kprobes to the new text-poke naming.

	Tested-by: Alexei Starovoitov <ast@kernel.org>
	Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
	Acked-by: Alexei Starovoitov <ast@kernel.org>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brian Gerst <brgerst@gmail.com>
	Cc: Denys Vlasenko <dvlasenk@redhat.com>
	Cc: H. Peter Anvin <hpa@zytor.com>
	Cc: Linus Torvalds <torvalds@linux-foundation.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
Link: https://lkml.kernel.org/r/20191111132458.103959370@infradead.org
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
(cherry picked from commit ab09e95ca0c697e67f986c4a45a179abf47e7bfc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/text-patching.h
#	arch/x86/kernel/kprobes/opt.c
diff --cc arch/x86/include/asm/text-patching.h
index 47a8c6185e2b,4c09f4286b0c..000000000000
--- a/arch/x86/include/asm/text-patching.h
+++ b/arch/x86/include/asm/text-patching.h
@@@ -71,6 -61,67 +71,70 @@@ static inline void int3_emulate_jmp(str
  #define JMP8_INSN_SIZE		2
  #define JMP8_INSN_OPCODE	0xEB
  
++<<<<<<< HEAD
++=======
+ #define DISP32_SIZE		4
+ 
+ static inline int text_opcode_size(u8 opcode)
+ {
+ 	int size = 0;
+ 
+ #define __CASE(insn)	\
+ 	case insn##_INSN_OPCODE: size = insn##_INSN_SIZE; break
+ 
+ 	switch(opcode) {
+ 	__CASE(INT3);
+ 	__CASE(CALL);
+ 	__CASE(JMP32);
+ 	__CASE(JMP8);
+ 	}
+ 
+ #undef __CASE
+ 
+ 	return size;
+ }
+ 
+ union text_poke_insn {
+ 	u8 text[POKE_MAX_OPCODE_SIZE];
+ 	struct {
+ 		u8 opcode;
+ 		s32 disp;
+ 	} __attribute__((packed));
+ };
+ 
+ static __always_inline
+ void *text_gen_insn(u8 opcode, const void *addr, const void *dest)
+ {
+ 	static union text_poke_insn insn; /* per instance */
+ 	int size = text_opcode_size(opcode);
+ 
+ 	insn.opcode = opcode;
+ 
+ 	if (size > 1) {
+ 		insn.disp = (long)dest - (long)(addr + size);
+ 		if (size == 2) {
+ 			/*
+ 			 * Ensure that for JMP9 the displacement
+ 			 * actually fits the signed byte.
+ 			 */
+ 			BUG_ON((insn.disp >> 31) != (insn.disp >> 7));
+ 		}
+ 	}
+ 
+ 	return &insn.text;
+ }
+ 
+ extern int after_bootmem;
+ extern __ro_after_init struct mm_struct *poking_mm;
+ extern __ro_after_init unsigned long poking_addr;
+ 
+ #ifndef CONFIG_UML_X86
+ static inline void int3_emulate_jmp(struct pt_regs *regs, unsigned long ip)
+ {
+ 	regs->ip = ip;
+ }
+ 
++>>>>>>> ab09e95ca0c6 (x86/kprobes: Convert to text-patching.h)
  static inline void int3_emulate_push(struct pt_regs *regs, unsigned long val)
  {
  	/*
diff --cc arch/x86/kernel/kprobes/opt.c
index 0448efa0d2f1,0d9ea487d573..000000000000
--- a/arch/x86/kernel/kprobes/opt.c
+++ b/arch/x86/kernel/kprobes/opt.c
@@@ -459,19 -444,14 +457,28 @@@ void arch_optimize_kprobes(struct list_
  /* Replace a relative jump with a breakpoint (int3).  */
  void arch_unoptimize_kprobe(struct optimized_kprobe *op)
  {
++<<<<<<< HEAD
 +	u8 insn_buff[RELATIVEJUMP_SIZE];
 +	u8 emulate_buff[RELATIVEJUMP_SIZE];
++=======
+ 	u8 insn_buff[JMP32_INSN_SIZE];
++>>>>>>> ab09e95ca0c6 (x86/kprobes: Convert to text-patching.h)
  
  	/* Set int3 to first byte for kprobes */
- 	insn_buff[0] = BREAKPOINT_INSTRUCTION;
- 	memcpy(insn_buff + 1, op->optinsn.copied_insn, RELATIVE_ADDR_SIZE);
+ 	insn_buff[0] = INT3_INSN_OPCODE;
+ 	memcpy(insn_buff + 1, op->optinsn.copied_insn, DISP32_SIZE);
  
++<<<<<<< HEAD
 +	emulate_buff[0] = RELATIVEJUMP_OPCODE;
 +	*(s32 *)(&emulate_buff[1]) = (s32)((long)op->optinsn.insn -
 +			((long)op->kp.addr + RELATIVEJUMP_SIZE));
 +
 +	text_poke_bp(op->kp.addr, insn_buff, RELATIVEJUMP_SIZE,
 +		     emulate_buff);
++=======
+ 	text_poke_bp(op->kp.addr, insn_buff, JMP32_INSN_SIZE,
+ 		     text_gen_insn(JMP32_INSN_OPCODE, op->kp.addr, op->optinsn.insn));
++>>>>>>> ab09e95ca0c6 (x86/kprobes: Convert to text-patching.h)
  }
  
  /*
diff --git a/arch/x86/include/asm/kprobes.h b/arch/x86/include/asm/kprobes.h
index c8cec1b39b88..3d97883ee9d2 100644
--- a/arch/x86/include/asm/kprobes.h
+++ b/arch/x86/include/asm/kprobes.h
@@ -24,12 +24,11 @@
 
 #include <asm-generic/kprobes.h>
 
-#define BREAKPOINT_INSTRUCTION	0xcc
-
 #ifdef CONFIG_KPROBES
 #include <linux/types.h>
 #include <linux/ptrace.h>
 #include <linux/percpu.h>
+#include <asm/text-patching.h>
 #include <asm/insn.h>
 
 #define  __ARCH_WANT_KPROBES_INSN_SLOT
@@ -38,10 +37,7 @@ struct pt_regs;
 struct kprobe;
 
 typedef u8 kprobe_opcode_t;
-#define RELATIVEJUMP_OPCODE 0xe9
-#define RELATIVEJUMP_SIZE 5
-#define RELATIVECALL_OPCODE 0xe8
-#define RELATIVE_ADDR_SIZE 4
+
 #define MAX_STACK_SIZE 64
 #define CUR_STACK_SIZE(ADDR) \
 	(current_top_of_stack() - (unsigned long)(ADDR))
@@ -56,11 +52,11 @@ extern __visible kprobe_opcode_t optprobe_template_entry[];
 extern __visible kprobe_opcode_t optprobe_template_val[];
 extern __visible kprobe_opcode_t optprobe_template_call[];
 extern __visible kprobe_opcode_t optprobe_template_end[];
-#define MAX_OPTIMIZED_LENGTH (MAX_INSN_SIZE + RELATIVE_ADDR_SIZE)
+#define MAX_OPTIMIZED_LENGTH (MAX_INSN_SIZE + DISP32_SIZE)
 #define MAX_OPTINSN_SIZE 				\
 	(((unsigned long)optprobe_template_end -	\
 	  (unsigned long)optprobe_template_entry) +	\
-	 MAX_OPTIMIZED_LENGTH + RELATIVEJUMP_SIZE)
+	 MAX_OPTIMIZED_LENGTH + JMP32_INSN_SIZE)
 
 extern const int kretprobe_blacklist_size;
 
@@ -86,7 +82,7 @@ struct arch_specific_insn {
 
 struct arch_optimized_insn {
 	/* copy of the original instructions */
-	kprobe_opcode_t copied_insn[RELATIVE_ADDR_SIZE];
+	kprobe_opcode_t copied_insn[DISP32_SIZE];
 	/* detour code buffer */
 	kprobe_opcode_t *insn;
 	/* the size of instructions copied to detour code buffer */
* Unmerged path arch/x86/include/asm/text-patching.h
diff --git a/arch/x86/kernel/kprobes/core.c b/arch/x86/kernel/kprobes/core.c
index 914c7d77ba21..fa967605d01a 100644
--- a/arch/x86/kernel/kprobes/core.c
+++ b/arch/x86/kernel/kprobes/core.c
@@ -132,14 +132,14 @@ __synthesize_relative_insn(void *dest, void *from, void *to, u8 op)
 /* Insert a jump instruction at address 'from', which jumps to address 'to'.*/
 void synthesize_reljump(void *dest, void *from, void *to)
 {
-	__synthesize_relative_insn(dest, from, to, RELATIVEJUMP_OPCODE);
+	__synthesize_relative_insn(dest, from, to, JMP32_INSN_OPCODE);
 }
 NOKPROBE_SYMBOL(synthesize_reljump);
 
 /* Insert a call instruction at address 'from', which calls address 'to'.*/
 void synthesize_relcall(void *dest, void *from, void *to)
 {
-	__synthesize_relative_insn(dest, from, to, RELATIVECALL_OPCODE);
+	__synthesize_relative_insn(dest, from, to, CALL_INSN_OPCODE);
 }
 NOKPROBE_SYMBOL(synthesize_relcall);
 
@@ -314,7 +314,7 @@ static int can_probe(unsigned long paddr)
 		 * Another debugging subsystem might insert this breakpoint.
 		 * In that case, we can't recover it.
 		 */
-		if (insn.opcode.bytes[0] == BREAKPOINT_INSTRUCTION)
+		if (insn.opcode.bytes[0] == INT3_INSN_OPCODE)
 			return 0;
 		addr += insn.length;
 	}
@@ -370,7 +370,7 @@ int __copy_instruction(u8 *dest, u8 *src, u8 *real, struct insn *insn)
 		return 0;
 
 	/* Another subsystem puts a breakpoint, failed to recover */
-	if (insn->opcode.bytes[0] == BREAKPOINT_INSTRUCTION)
+	if (insn->opcode.bytes[0] == INT3_INSN_OPCODE)
 		return 0;
 
 	/* We should not singlestep on the exception masking instructions */
@@ -416,14 +416,14 @@ static int prepare_boost(kprobe_opcode_t *buf, struct kprobe *p,
 	int len = insn->length;
 
 	if (can_boost(insn, p->addr) &&
-	    MAX_INSN_SIZE - len >= RELATIVEJUMP_SIZE) {
+	    MAX_INSN_SIZE - len >= JMP32_INSN_SIZE) {
 		/*
 		 * These instructions can be executed directly if it
 		 * jumps back to correct address.
 		 */
 		synthesize_reljump(buf + len, p->ainsn.insn + len,
 				   p->addr + insn->length);
-		len += RELATIVEJUMP_SIZE;
+		len += JMP32_INSN_SIZE;
 		p->ainsn.boostable = true;
 	} else {
 		p->ainsn.boostable = false;
@@ -522,7 +522,7 @@ int arch_prepare_kprobe(struct kprobe *p)
 
 void arch_arm_kprobe(struct kprobe *p)
 {
-	text_poke(p->addr, ((unsigned char []){BREAKPOINT_INSTRUCTION}), 1);
+	text_poke(p->addr, ((unsigned char []){INT3_INSN_OPCODE}), 1);
 }
 
 void arch_disarm_kprobe(struct kprobe *p)
@@ -630,7 +630,7 @@ static void setup_singlestep(struct kprobe *p, struct pt_regs *regs,
 	regs->flags |= X86_EFLAGS_TF;
 	regs->flags &= ~X86_EFLAGS_IF;
 	/* single step inline if the instruction is an int3 */
-	if (p->opcode == BREAKPOINT_INSTRUCTION)
+	if (p->opcode == INT3_INSN_OPCODE)
 		regs->ip = (unsigned long)p->addr;
 	else
 		regs->ip = (unsigned long)p->ainsn.insn;
@@ -717,7 +717,7 @@ int kprobe_int3_handler(struct pt_regs *regs)
 				reset_current_kprobe();
 			return 1;
 		}
-	} else if (*addr != BREAKPOINT_INSTRUCTION) {
+	} else if (*addr != INT3_INSN_OPCODE) {
 		/*
 		 * The breakpoint instruction was removed right
 		 * after we hit it.  Another cpu has removed
* Unmerged path arch/x86/kernel/kprobes/opt.c
