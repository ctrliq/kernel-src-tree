x86/bpf: Add IBHF call at end of classic BPF

jira LE-4018
Rebuild_History Non-Buildable kernel-5.14.0-570.37.1.el9_6
commit-author Daniel Sneddon <daniel.sneddon@linux.intel.com>
commit 9f725eec8fc0b39bdc07dcc8897283c367c1a163
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-570.37.1.el9_6/9f725eec.failed

Classic BPF programs can be run by unprivileged users, allowing
unprivileged code to execute inside the kernel. Attackers can use this to
craft branch history in kernel mode that can influence the target of
indirect branches.

BHI_DIS_S provides user-kernel isolation of branch history, but cBPF can be
used to bypass this protection by crafting branch history in kernel mode.
To stop intra-mode attacks via cBPF programs, Intel created a new
instruction Indirect Branch History Fence (IBHF). IBHF prevents the
predicted targets of subsequent indirect branches from being influenced by
branch history prior to the IBHF. IBHF is only effective while BHI_DIS_S is
enabled.

Add the IBHF instruction to cBPF jitted code's exit path. Add the new fence
when the hardware mitigation is enabled (i.e., X86_FEATURE_CLEAR_BHB_HW is
set) or after the software sequence (X86_FEATURE_CLEAR_BHB_LOOP) is being
used in a virtual machine. Note that X86_FEATURE_CLEAR_BHB_HW and
X86_FEATURE_CLEAR_BHB_LOOP are mutually exclusive, so the JIT compiler will
only emit the new fence, not the SW sequence, when X86_FEATURE_CLEAR_BHB_HW
is set.

Hardware that enumerates BHI_NO basically has BHI_DIS_S protections always
enabled, regardless of the value of BHI_DIS_S. Since BHI_DIS_S doesn't
protect against intra-mode attacks, enumerate BHI bug on BHI_NO hardware as
well.

	Signed-off-by: Daniel Sneddon <daniel.sneddon@linux.intel.com>
	Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Acked-by: Daniel Borkmann <daniel@iogearbox.net>
	Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
(cherry picked from commit 9f725eec8fc0b39bdc07dcc8897283c367c1a163)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/net/bpf_jit_comp.c
diff --cc arch/x86/net/bpf_jit_comp.c
index 55fc2ebbda0f,e472572392ef..000000000000
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@@ -1412,6 -1500,51 +1414,54 @@@ static void emit_shiftx(u8 **pprog, u3
  #define LOAD_TAIL_CALL_CNT_PTR(stack)				\
  	__LOAD_TCC_PTR(BPF_TAIL_CALL_CNT_PTR_STACK_OFF(stack))
  
++<<<<<<< HEAD
++=======
+ /* Memory size/value to protect private stack overflow/underflow */
+ #define PRIV_STACK_GUARD_SZ    8
+ #define PRIV_STACK_GUARD_VAL   0xEB9F12345678eb9fULL
+ 
+ static int emit_spectre_bhb_barrier(u8 **pprog, u8 *ip,
+ 				    struct bpf_prog *bpf_prog)
+ {
+ 	u8 *prog = *pprog;
+ 	u8 *func;
+ 
+ 	if (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP)) {
+ 		/* The clearing sequence clobbers eax and ecx. */
+ 		EMIT1(0x50); /* push rax */
+ 		EMIT1(0x51); /* push rcx */
+ 		ip += 2;
+ 
+ 		func = (u8 *)clear_bhb_loop;
+ 		ip += x86_call_depth_emit_accounting(&prog, func, ip);
+ 
+ 		if (emit_call(&prog, func, ip))
+ 			return -EINVAL;
+ 		EMIT1(0x59); /* pop rcx */
+ 		EMIT1(0x58); /* pop rax */
+ 	}
+ 	/* Insert IBHF instruction */
+ 	if ((cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_LOOP) &&
+ 	     cpu_feature_enabled(X86_FEATURE_HYPERVISOR)) ||
+ 	    (cpu_feature_enabled(X86_FEATURE_CLEAR_BHB_HW) &&
+ 	     IS_ENABLED(CONFIG_X86_64))) {
+ 		/*
+ 		 * Add an Indirect Branch History Fence (IBHF). IBHF acts as a
+ 		 * fence preventing branch history from before the fence from
+ 		 * affecting indirect branches after the fence. This is
+ 		 * specifically used in cBPF jitted code to prevent Intra-mode
+ 		 * BHI attacks. The IBHF instruction is designed to be a NOP on
+ 		 * hardware that doesn't need or support it.  The REP and REX.W
+ 		 * prefixes are required by the microcode, and they also ensure
+ 		 * that the NOP is unlikely to be used in existing code.
+ 		 */
+ 		EMIT5(0xF3, 0x48, 0x0F, 0x1E, 0xF8); /* ibhf */
+ 	}
+ 	*pprog = prog;
+ 	return 0;
+ }
+ 
++>>>>>>> 9f725eec8fc0 (x86/bpf: Add IBHF call at end of classic BPF)
  static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image, u8 *rw_image,
  		  int oldproglen, struct jit_context *ctx, bool jmp_padding)
  {
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 0bb2204445f2..e0ff9c7d631e 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1483,9 +1483,12 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (vulnerable_to_rfds(x86_arch_cap_msr))
 		setup_force_cpu_bug(X86_BUG_RFDS);
 
-	/* When virtualized, eIBRS could be hidden, assume vulnerable */
-	if (!(x86_arch_cap_msr & ARCH_CAP_BHI_NO) &&
-	    !cpu_matches(cpu_vuln_whitelist, NO_BHI) &&
+	/*
+	 * Intel parts with eIBRS are vulnerable to BHI attacks. Parts with
+	 * BHI_NO still need to use the BHI mitigation to prevent Intra-mode
+	 * attacks.  When virtualized, eIBRS could be hidden, assume vulnerable.
+	 */
+	if (!cpu_matches(cpu_vuln_whitelist, NO_BHI) &&
 	    (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
 	     boot_cpu_has(X86_FEATURE_HYPERVISOR)))
 		setup_force_cpu_bug(X86_BUG_BHI);
* Unmerged path arch/x86/net/bpf_jit_comp.c
