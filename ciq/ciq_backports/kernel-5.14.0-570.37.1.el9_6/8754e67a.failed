x86/its: Add support for ITS-safe indirect thunk

jira LE-4018
cve CVE-2024-28956
Rebuild_History Non-Buildable kernel-5.14.0-570.37.1.el9_6
commit-author Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
commit 8754e67ad4ac692c67ff1f99c0d07156f04ae40c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-570.37.1.el9_6/8754e67a.failed

Due to ITS, indirect branches in the lower half of a cacheline may be
vulnerable to branch target injection attack.

Introduce ITS-safe thunks to patch indirect branches in the lower half of
cacheline with the thunk. Also thunk any eBPF generated indirect branches
in emit_indirect_jump().

Below category of indirect branches are not mitigated:

- Indirect branches in the .init section are not mitigated because they are
  discarded after boot.
- Indirect branches that are explicitly marked retpoline-safe.

Note that retpoline also mitigates the indirect branches against ITS. This
is because the retpoline sequence fills an RSB entry before RET, and it
does not suffer from RSB-underflow part of the ITS.

	Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
(cherry picked from commit 8754e67ad4ac692c67ff1f99c0d07156f04ae40c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
#	arch/x86/include/asm/cpufeatures.h
diff --cc arch/x86/Kconfig
index 5d4f050bd59f,a6bd0590c437..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -2628,6 -2609,118 +2628,121 @@@ config MITIGATION_SPECTRE_BH
  	  indirect branches.
  	  See <file:Documentation/admin-guide/hw-vuln/spectre.rst>
  
++<<<<<<< HEAD
++=======
+ config MITIGATION_MDS
+ 	bool "Mitigate Microarchitectural Data Sampling (MDS) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for Microarchitectural Data Sampling (MDS). MDS is
+ 	  a hardware vulnerability which allows unprivileged speculative access
+ 	  to data which is available in various CPU internal buffers.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/mds.rst>
+ 
+ config MITIGATION_TAA
+ 	bool "Mitigate TSX Asynchronous Abort (TAA) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for TSX Asynchronous Abort (TAA). TAA is a hardware
+ 	  vulnerability that allows unprivileged speculative access to data
+ 	  which is available in various CPU internal buffers by using
+ 	  asynchronous aborts within an Intel TSX transactional region.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/tsx_async_abort.rst>
+ 
+ config MITIGATION_MMIO_STALE_DATA
+ 	bool "Mitigate MMIO Stale Data hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for MMIO Stale Data hardware bugs.  Processor MMIO
+ 	  Stale Data Vulnerabilities are a class of memory-mapped I/O (MMIO)
+ 	  vulnerabilities that can expose data. The vulnerabilities require the
+ 	  attacker to have access to MMIO.
+ 	  See also
+ 	  <file:Documentation/admin-guide/hw-vuln/processor_mmio_stale_data.rst>
+ 
+ config MITIGATION_L1TF
+ 	bool "Mitigate L1 Terminal Fault (L1TF) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Mitigate L1 Terminal Fault (L1TF) hardware bug. L1 Terminal Fault is a
+ 	  hardware vulnerability which allows unprivileged speculative access to data
+ 	  available in the Level 1 Data Cache.
+ 	  See <file:Documentation/admin-guide/hw-vuln/l1tf.rst
+ 
+ config MITIGATION_RETBLEED
+ 	bool "Mitigate RETBleed hardware bug"
+ 	depends on (CPU_SUP_INTEL && MITIGATION_SPECTRE_V2) || MITIGATION_UNRET_ENTRY || MITIGATION_IBPB_ENTRY
+ 	default y
+ 	help
+ 	  Enable mitigation for RETBleed (Arbitrary Speculative Code Execution
+ 	  with Return Instructions) vulnerability.  RETBleed is a speculative
+ 	  execution attack which takes advantage of microarchitectural behavior
+ 	  in many modern microprocessors, similar to Spectre v2. An
+ 	  unprivileged attacker can use these flaws to bypass conventional
+ 	  memory security restrictions to gain read access to privileged memory
+ 	  that would otherwise be inaccessible.
+ 
+ config MITIGATION_SPECTRE_V1
+ 	bool "Mitigate SPECTRE V1 hardware bug"
+ 	default y
+ 	help
+ 	  Enable mitigation for Spectre V1 (Bounds Check Bypass). Spectre V1 is a
+ 	  class of side channel attacks that takes advantage of speculative
+ 	  execution that bypasses conditional branch instructions used for
+ 	  memory access bounds check.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/spectre.rst>
+ 
+ config MITIGATION_SPECTRE_V2
+ 	bool "Mitigate SPECTRE V2 hardware bug"
+ 	default y
+ 	help
+ 	  Enable mitigation for Spectre V2 (Branch Target Injection). Spectre
+ 	  V2 is a class of side channel attacks that takes advantage of
+ 	  indirect branch predictors inside the processor. In Spectre variant 2
+ 	  attacks, the attacker can steer speculative indirect branches in the
+ 	  victim to gadget code by poisoning the branch target buffer of a CPU
+ 	  used for predicting indirect branch addresses.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/spectre.rst>
+ 
+ config MITIGATION_SRBDS
+ 	bool "Mitigate Special Register Buffer Data Sampling (SRBDS) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for Special Register Buffer Data Sampling (SRBDS).
+ 	  SRBDS is a hardware vulnerability that allows Microarchitectural Data
+ 	  Sampling (MDS) techniques to infer values returned from special
+ 	  register accesses. An unprivileged user can extract values returned
+ 	  from RDRAND and RDSEED executed on another core or sibling thread
+ 	  using MDS techniques.
+ 	  See also
+ 	  <file:Documentation/admin-guide/hw-vuln/special-register-buffer-data-sampling.rst>
+ 
+ config MITIGATION_SSB
+ 	bool "Mitigate Speculative Store Bypass (SSB) hardware bug"
+ 	default y
+ 	help
+ 	  Enable mitigation for Speculative Store Bypass (SSB). SSB is a
+ 	  hardware security vulnerability and its exploitation takes advantage
+ 	  of speculative execution in a similar way to the Meltdown and Spectre
+ 	  security vulnerabilities.
+ 
+ config MITIGATION_ITS
+ 	bool "Enable Indirect Target Selection mitigation"
+ 	depends on CPU_SUP_INTEL && X86_64
+ 	depends on MITIGATION_RETPOLINE && MITIGATION_RETHUNK
+ 	default y
+ 	help
+ 	  Enable Indirect Target Selection (ITS) mitigation. ITS is a bug in
+ 	  BPU on some Intel CPUs that may allow Spectre V2 style attacks. If
+ 	  disabled, mitigation cannot be enabled via cmdline.
+ 	  See <file:Documentation/admin-guide/hw-vuln/indirect-target-selection.rst>
+ 
++>>>>>>> 8754e67ad4ac (x86/its: Add support for ITS-safe indirect thunk)
  endif
  
  config ARCH_HAS_ADD_PAGES
diff --cc arch/x86/include/asm/cpufeatures.h
index bfa40f5bcc4c,03af8191073a..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -474,12 -472,16 +474,25 @@@
   *
   * Reuse free bits when adding new feature flags!
   */
++<<<<<<< HEAD
 +#define X86_FEATURE_AMD_LBR_PMC_FREEZE	(21*32+ 0) /* AMD LBR and PMC Freeze */
 +#define X86_FEATURE_CLEAR_BHB_LOOP	(21*32+ 1) /* "" Clear branch history at syscall entry using SW loop */
 +#define X86_FEATURE_BHI_CTRL		(21*32+ 2) /* "" BHI_DIS_S HW control available */
 +#define X86_FEATURE_CLEAR_BHB_HW	(21*32+ 3) /* "" BHI_DIS_S HW control enabled */
 +#define X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT (21*32+ 4) /* "" Clear branch history at vmexit using SW loop */
 +#define X86_FEATURE_FAST_CPPC		(21*32 + 5) /* "" AMD Fast CPPC */
++=======
+ #define X86_FEATURE_AMD_LBR_PMC_FREEZE	(21*32+ 0) /* "amd_lbr_pmc_freeze" AMD LBR and PMC Freeze */
+ #define X86_FEATURE_CLEAR_BHB_LOOP	(21*32+ 1) /* Clear branch history at syscall entry using SW loop */
+ #define X86_FEATURE_BHI_CTRL		(21*32+ 2) /* BHI_DIS_S HW control available */
+ #define X86_FEATURE_CLEAR_BHB_HW	(21*32+ 3) /* BHI_DIS_S HW control enabled */
+ #define X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT (21*32+ 4) /* Clear branch history at vmexit using SW loop */
+ #define X86_FEATURE_AMD_FAST_CPPC	(21*32 + 5) /* Fast CPPC */
+ #define X86_FEATURE_AMD_HETEROGENEOUS_CORES (21*32 + 6) /* Heterogeneous Core Topology */
+ #define X86_FEATURE_AMD_WORKLOAD_CLASS	(21*32 + 7) /* Workload Classification */
+ #define X86_FEATURE_PREFER_YMM		(21*32 + 8) /* Avoid ZMM registers due to downclocking */
+ #define X86_FEATURE_INDIRECT_THUNK_ITS	(21*32 + 9) /* Use thunk for indirect branches in lower half of cacheline */
++>>>>>>> 8754e67ad4ac (x86/its: Add support for ITS-safe indirect thunk)
  
  /*
   * BUG word(s)
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/include/asm/cpufeatures.h
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index a2ed8f5680f2..070b43cef814 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -356,10 +356,14 @@
 	".long 999b\n\t"					\
 	".popsection\n\t"
 
+#define ITS_THUNK_SIZE	64
+
 typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
+typedef u8 its_thunk_t[ITS_THUNK_SIZE];
 extern retpoline_thunk_t __x86_indirect_thunk_array[];
 extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
 extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];
+extern its_thunk_t	 __x86_indirect_its_thunk_array[];
 
 #ifdef CONFIG_MITIGATION_RETHUNK
 extern void __x86_return_thunk(void);
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index 4fad1e17e3ce..49758d0368a8 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -522,7 +522,8 @@ static int emit_indirect(int op, int reg, u8 *bytes)
 	return i;
 }
 
-static int emit_call_track_retpoline(void *addr, struct insn *insn, int reg, u8 *bytes)
+static int __emit_trampoline(void *addr, struct insn *insn, u8 *bytes,
+			     void *call_dest, void *jmp_dest)
 {
 	u8 op = insn->opcode.bytes[0];
 	int i = 0;
@@ -543,7 +544,7 @@ static int emit_call_track_retpoline(void *addr, struct insn *insn, int reg, u8
 	switch (op) {
 	case CALL_INSN_OPCODE:
 		__text_gen_insn(bytes+i, op, addr+i,
-				__x86_indirect_call_thunk_array[reg],
+				call_dest,
 				CALL_INSN_SIZE);
 		i += CALL_INSN_SIZE;
 		break;
@@ -551,7 +552,7 @@ static int emit_call_track_retpoline(void *addr, struct insn *insn, int reg, u8
 	case JMP32_INSN_OPCODE:
 clang_jcc:
 		__text_gen_insn(bytes+i, op, addr+i,
-				__x86_indirect_jump_thunk_array[reg],
+				jmp_dest,
 				JMP32_INSN_SIZE);
 		i += JMP32_INSN_SIZE;
 		break;
@@ -566,6 +567,35 @@ static int emit_call_track_retpoline(void *addr, struct insn *insn, int reg, u8
 	return i;
 }
 
+static int emit_call_track_retpoline(void *addr, struct insn *insn, int reg, u8 *bytes)
+{
+	return __emit_trampoline(addr, insn, bytes,
+				 __x86_indirect_call_thunk_array[reg],
+				 __x86_indirect_jump_thunk_array[reg]);
+}
+
+#ifdef CONFIG_MITIGATION_ITS
+static int emit_its_trampoline(void *addr, struct insn *insn, int reg, u8 *bytes)
+{
+	return __emit_trampoline(addr, insn, bytes,
+				 __x86_indirect_its_thunk_array[reg],
+				 __x86_indirect_its_thunk_array[reg]);
+}
+
+/* Check if an indirect branch is at ITS-unsafe address */
+static bool cpu_wants_indirect_its_thunk_at(unsigned long addr, int reg)
+{
+	if (!cpu_feature_enabled(X86_FEATURE_INDIRECT_THUNK_ITS))
+		return false;
+
+	/* Indirect branch opcode is 2 or 3 bytes depending on reg */
+	addr += 1 + reg / 8;
+
+	/* Lower-half of the cacheline? */
+	return !(addr & 0x20);
+}
+#endif
+
 /*
  * Rewrite the compiler generated retpoline thunk calls.
  *
@@ -640,6 +670,15 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 		bytes[i++] = 0xe8; /* LFENCE */
 	}
 
+#ifdef CONFIG_MITIGATION_ITS
+	/*
+	 * Check if the address of last byte of emitted-indirect is in
+	 * lower-half of the cacheline. Such branches need ITS mitigation.
+	 */
+	if (cpu_wants_indirect_its_thunk_at((unsigned long)addr + i, reg))
+		return emit_its_trampoline(addr, insn, reg, bytes);
+#endif
+
 	ret = emit_indirect(op, reg, bytes + i);
 	if (ret < 0)
 		return ret;
diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S
index bf2cebfe91ce..945d77e0923e 100644
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@ -541,4 +541,10 @@ INIT_PER_CPU(irq_stack_backing_store);
 		"SRSO function pair won't alias");
 #endif
 
+#if defined(CONFIG_MITIGATION_ITS) && !defined(CONFIG_DEBUG_FORCE_FUNCTION_ALIGN_64B)
+. = ASSERT(__x86_indirect_its_thunk_rax & 0x20, "__x86_indirect_thunk_rax not in second half of cacheline");
+. = ASSERT(((__x86_indirect_its_thunk_rcx - __x86_indirect_its_thunk_rax) % 64) == 0, "Indirect thunks are not cacheline apart");
+. = ASSERT(__x86_indirect_its_thunk_array == __x86_indirect_its_thunk_rax, "Gap in ITS thunk array");
+#endif
+
 #endif /* CONFIG_X86_64 */
diff --git a/arch/x86/lib/retpoline.S b/arch/x86/lib/retpoline.S
index 8a99d5a29f90..2ccd014fe372 100644
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@ -366,6 +366,34 @@ SYM_FUNC_END(call_depth_return_thunk)
 
 #endif /* CONFIG_MITIGATION_CALL_DEPTH_TRACKING */
 
+#ifdef CONFIG_MITIGATION_ITS
+
+.macro ITS_THUNK reg
+
+SYM_INNER_LABEL(__x86_indirect_its_thunk_\reg, SYM_L_GLOBAL)
+	UNWIND_HINT_UNDEFINED
+	ANNOTATE_NOENDBR
+	ANNOTATE_RETPOLINE_SAFE
+	jmp *%\reg
+	int3
+	.align 32, 0xcc		/* fill to the end of the line */
+	.skip  32, 0xcc		/* skip to the next upper half */
+.endm
+
+/* ITS mitigation requires thunks be aligned to upper half of cacheline */
+.align 64, 0xcc
+.skip 32, 0xcc
+SYM_CODE_START(__x86_indirect_its_thunk_array)
+
+#define GEN(reg) ITS_THUNK reg
+#include <asm/GEN-for-each-reg.h>
+#undef GEN
+
+	.align 64, 0xcc
+SYM_CODE_END(__x86_indirect_its_thunk_array)
+
+#endif
+
 /*
  * This function name is magical and is used by -mfunction-return=thunk-extern
  * for the compiler to generate JMPs to it.
diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index 55fc2ebbda0f..53d46d4479ab 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -637,7 +637,10 @@ static void emit_indirect_jump(u8 **pprog, int reg, u8 *ip)
 {
 	u8 *prog = *pprog;
 
-	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	if (cpu_feature_enabled(X86_FEATURE_INDIRECT_THUNK_ITS)) {
+		OPTIMIZER_HIDE_VAR(reg);
+		emit_jump(&prog, &__x86_indirect_its_thunk_array[reg], ip);
+	} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
 		EMIT_LFENCE();
 		EMIT2(0xFF, 0xE0 + reg);
 	} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
