cifs: split out dfs code from cifs_reconnect()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-553.16.1.el8_10
commit-author Paulo Alcantara <pc@cjr.nz>
commit bbcce368044572d0802c3bbb8ef3fe98f581d803
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.16.1.el8_10/bbcce368.failed

Make two separate functions that handle dfs and non-dfs reconnect
logics since cifs_reconnect() became way too complex to handle both.
While at it, add some documentation.

	Signed-off-by: Paulo Alcantara (SUSE) <pc@cjr.nz>
	Reviewed-by: Shyam Prasad N <sprasad@microsoft.com>
	Signed-off-by: Steve French <stfrench@microsoft.com>
(cherry picked from commit bbcce368044572d0802c3bbb8ef3fe98f581d803)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/cifs/connect.c
diff --cc fs/cifs/connect.c
index 13237453ae19,f5c5731cda02..000000000000
--- a/fs/cifs/connect.c
+++ b/fs/cifs/connect.c
@@@ -159,56 -148,99 +159,152 @@@ static void cifs_resolve_server(struct 
  	mutex_unlock(&server->srv_mutex);
  }
  
++<<<<<<< HEAD
 +#ifdef CONFIG_CIFS_DFS_UPCALL
 +/* These functions must be called with server->srv_mutex held */
 +static void reconn_set_next_dfs_target(struct TCP_Server_Info *server,
 +				       struct cifs_sb_info *cifs_sb,
 +				       struct dfs_cache_tgt_list *tgt_list,
 +				       struct dfs_cache_tgt_iterator **tgt_it)
 +{
 +	const char *name;
 +	int rc;
 +
 +	if (!cifs_sb || !cifs_sb->origin_fullpath)
 +		return;
 +
 +	if (!*tgt_it) {
 +		*tgt_it = dfs_cache_get_tgt_iterator(tgt_list);
 +	} else {
 +		*tgt_it = dfs_cache_get_next_tgt(tgt_list, *tgt_it);
 +		if (!*tgt_it)
 +			*tgt_it = dfs_cache_get_tgt_iterator(tgt_list);
 +	}
 +
 +	cifs_dbg(FYI, "%s: UNC: %s\n", __func__, cifs_sb->origin_fullpath);
 +
 +	name = dfs_cache_get_tgt_name(*tgt_it);
 +
 +	kfree(server->hostname);
 +
 +	server->hostname = extract_hostname(name);
 +	if (IS_ERR(server->hostname)) {
 +		cifs_dbg(FYI,
 +			 "%s: failed to extract hostname from target: %ld\n",
 +			 __func__, PTR_ERR(server->hostname));
 +		return;
 +	}
 +
 +	rc = reconn_set_ipaddr_from_hostname(server);
 +	if (rc) {
 +		cifs_dbg(FYI, "%s: failed to resolve hostname: %d\n",
 +			 __func__, rc);
 +	}
 +}
 +
 +static inline int reconn_setup_dfs_targets(struct cifs_sb_info *cifs_sb,
 +					   struct dfs_cache_tgt_list *tl)
 +{
 +	if (!cifs_sb->origin_fullpath)
 +		return -EOPNOTSUPP;
 +	return dfs_cache_noreq_find(cifs_sb->origin_fullpath + 1, NULL, tl);
 +}
 +#endif
++=======
+ /**
+  * Mark all sessions and tcons for reconnect.
+  *
+  * @server needs to be previously set to CifsNeedReconnect.
+  */
+ static void cifs_mark_tcp_ses_conns_for_reconnect(struct TCP_Server_Info *server)
+ {
+ 	struct cifs_ses *ses;
+ 	struct cifs_tcon *tcon;
+ 	struct mid_q_entry *mid, *nmid;
+ 	struct list_head retry_list;
+ 
+ 	server->maxBuf = 0;
+ 	server->max_read = 0;
+ 
+ 	cifs_dbg(FYI, "Mark tcp session as need reconnect\n");
+ 	trace_smb3_reconnect(server->CurrentMid, server->conn_id, server->hostname);
+ 	/*
+ 	 * before reconnecting the tcp session, mark the smb session (uid) and the tid bad so they
+ 	 * are not used until reconnected.
+ 	 */
+ 	cifs_dbg(FYI, "%s: marking sessions and tcons for reconnect\n", __func__);
+ 	spin_lock(&cifs_tcp_ses_lock);
+ 	list_for_each_entry(ses, &server->smb_ses_list, smb_ses_list) {
+ 		ses->need_reconnect = true;
+ 		list_for_each_entry(tcon, &ses->tcon_list, tcon_list)
+ 			tcon->need_reconnect = true;
+ 		if (ses->tcon_ipc)
+ 			ses->tcon_ipc->need_reconnect = true;
+ 	}
+ 	spin_unlock(&cifs_tcp_ses_lock);
+ 
+ 	/* do not want to be sending data on a socket we are freeing */
+ 	cifs_dbg(FYI, "%s: tearing down socket\n", __func__);
+ 	mutex_lock(&server->srv_mutex);
+ 	if (server->ssocket) {
+ 		cifs_dbg(FYI, "State: 0x%x Flags: 0x%lx\n", server->ssocket->state,
+ 			 server->ssocket->flags);
+ 		kernel_sock_shutdown(server->ssocket, SHUT_WR);
+ 		cifs_dbg(FYI, "Post shutdown state: 0x%x Flags: 0x%lx\n", server->ssocket->state,
+ 			 server->ssocket->flags);
+ 		sock_release(server->ssocket);
+ 		server->ssocket = NULL;
+ 	}
+ 	server->sequence_number = 0;
+ 	server->session_estab = false;
+ 	kfree(server->session_key.response);
+ 	server->session_key.response = NULL;
+ 	server->session_key.len = 0;
+ 	server->lstrp = jiffies;
+ 
+ 	/* mark submitted MIDs for retry and issue callback */
+ 	INIT_LIST_HEAD(&retry_list);
+ 	cifs_dbg(FYI, "%s: moving mids to private list\n", __func__);
+ 	spin_lock(&GlobalMid_Lock);
+ 	list_for_each_entry_safe(mid, nmid, &server->pending_mid_q, qhead) {
+ 		kref_get(&mid->refcount);
+ 		if (mid->mid_state == MID_REQUEST_SUBMITTED)
+ 			mid->mid_state = MID_RETRY_NEEDED;
+ 		list_move(&mid->qhead, &retry_list);
+ 		mid->mid_flags |= MID_DELETED;
+ 	}
+ 	spin_unlock(&GlobalMid_Lock);
+ 	mutex_unlock(&server->srv_mutex);
+ 
+ 	cifs_dbg(FYI, "%s: issuing mid callbacks\n", __func__);
+ 	list_for_each_entry_safe(mid, nmid, &retry_list, qhead) {
+ 		list_del_init(&mid->qhead);
+ 		mid->callback(mid);
+ 		cifs_mid_q_entry_release(mid);
+ 	}
+ 
+ 	if (cifs_rdma_enabled(server)) {
+ 		mutex_lock(&server->srv_mutex);
+ 		smbd_destroy(server);
+ 		mutex_unlock(&server->srv_mutex);
+ 	}
+ }
+ 
+ static bool cifs_tcp_ses_needs_reconnect(struct TCP_Server_Info *server, int num_targets)
+ {
+ 	spin_lock(&GlobalMid_Lock);
+ 	server->nr_targets = num_targets;
+ 	if (server->tcpStatus == CifsExiting) {
+ 		/* the demux thread will exit normally next time through the loop */
+ 		spin_unlock(&GlobalMid_Lock);
+ 		wake_up(&server->response_q);
+ 		return false;
+ 	}
+ 	server->tcpStatus = CifsNeedReconnect;
+ 	spin_unlock(&GlobalMid_Lock);
+ 	return true;
+ }
++>>>>>>> bbcce3680445 (cifs: split out dfs code from cifs_reconnect())
  
  /*
   * cifs tcp session reconnection
@@@ -218,132 -250,14 +314,137 @@@
   * reconnect tcp session
   * wake up waiters on reconnection? - (not needed currently)
   */
- int
- cifs_reconnect(struct TCP_Server_Info *server)
+ static int __cifs_reconnect(struct TCP_Server_Info *server)
  {
  	int rc = 0;
++<<<<<<< HEAD
 +	struct list_head *tmp, *tmp2;
 +	struct cifs_ses *ses;
 +	struct cifs_tcon *tcon;
 +	struct mid_q_entry *mid_entry;
 +	struct list_head retry_list;
 +#ifdef CONFIG_CIFS_DFS_UPCALL
 +	struct super_block *sb = NULL;
 +	struct cifs_sb_info *cifs_sb = NULL;
 +	struct dfs_cache_tgt_list tgt_list = DFS_CACHE_TGT_LIST_INIT(tgt_list);
 +	struct dfs_cache_tgt_iterator *tgt_it = NULL;
 +#endif
 +
 +	spin_lock(&GlobalMid_Lock);
 +	server->nr_targets = 1;
 +#ifdef CONFIG_CIFS_DFS_UPCALL
 +	spin_unlock(&GlobalMid_Lock);
 +	sb = cifs_get_tcp_super(server);
 +	if (IS_ERR(sb)) {
 +		rc = PTR_ERR(sb);
 +		cifs_dbg(FYI, "%s: will not do DFS failover: rc = %d\n",
 +			 __func__, rc);
 +		sb = NULL;
 +	} else {
 +		cifs_sb = CIFS_SB(sb);
 +		rc = reconn_setup_dfs_targets(cifs_sb, &tgt_list);
 +		if (rc) {
 +			cifs_sb = NULL;
 +			if (rc != -EOPNOTSUPP) {
 +				cifs_server_dbg(VFS, "%s: no target servers for DFS failover\n",
 +						__func__);
 +			}
 +		} else {
 +			server->nr_targets = dfs_cache_get_nr_tgts(&tgt_list);
 +		}
 +	}
 +	cifs_dbg(FYI, "%s: will retry %d target(s)\n", __func__,
 +		 server->nr_targets);
 +	spin_lock(&GlobalMid_Lock);
 +#endif
 +	if (server->tcpStatus == CifsExiting) {
 +		/* the demux thread will exit normally
 +		next time through the loop */
 +		spin_unlock(&GlobalMid_Lock);
 +#ifdef CONFIG_CIFS_DFS_UPCALL
 +		dfs_cache_free_tgts(&tgt_list);
 +		cifs_put_tcp_super(sb);
 +#endif
 +		wake_up(&server->response_q);
 +		return rc;
 +	} else
 +		server->tcpStatus = CifsNeedReconnect;
 +	spin_unlock(&GlobalMid_Lock);
 +	server->maxBuf = 0;
 +	server->max_read = 0;
++=======
+ 
+ 	if (!cifs_tcp_ses_needs_reconnect(server, 1))
+ 		return 0;
++>>>>>>> bbcce3680445 (cifs: split out dfs code from cifs_reconnect())
  
 -	cifs_mark_tcp_ses_conns_for_reconnect(server);
 +	cifs_dbg(FYI, "Mark tcp session as need reconnect\n");
 +	trace_smb3_reconnect(server->CurrentMid, server->hostname);
 +
 +	/* before reconnecting the tcp session, mark the smb session (uid)
 +		and the tid bad so they are not used until reconnected */
 +	cifs_dbg(FYI, "%s: marking sessions and tcons for reconnect\n",
 +		 __func__);
 +	spin_lock(&cifs_tcp_ses_lock);
 +	list_for_each(tmp, &server->smb_ses_list) {
 +		ses = list_entry(tmp, struct cifs_ses, smb_ses_list);
 +		ses->need_reconnect = true;
 +		list_for_each(tmp2, &ses->tcon_list) {
 +			tcon = list_entry(tmp2, struct cifs_tcon, tcon_list);
 +			tcon->need_reconnect = true;
 +		}
 +		if (ses->tcon_ipc)
 +			ses->tcon_ipc->need_reconnect = true;
 +	}
 +	spin_unlock(&cifs_tcp_ses_lock);
 +
 +	/* do not want to be sending data on a socket we are freeing */
 +	cifs_dbg(FYI, "%s: tearing down socket\n", __func__);
 +	mutex_lock(&server->srv_mutex);
 +	if (server->ssocket) {
 +		cifs_dbg(FYI, "State: 0x%x Flags: 0x%lx\n",
 +			 server->ssocket->state, server->ssocket->flags);
 +		kernel_sock_shutdown(server->ssocket, SHUT_WR);
 +		cifs_dbg(FYI, "Post shutdown state: 0x%x Flags: 0x%lx\n",
 +			 server->ssocket->state, server->ssocket->flags);
 +		sock_release(server->ssocket);
 +		server->ssocket = NULL;
 +	}
 +	server->sequence_number = 0;
 +	server->session_estab = false;
 +	kfree(server->session_key.response);
 +	server->session_key.response = NULL;
 +	server->session_key.len = 0;
 +	server->lstrp = jiffies;
 +
 +	/* mark submitted MIDs for retry and issue callback */
 +	INIT_LIST_HEAD(&retry_list);
 +	cifs_dbg(FYI, "%s: moving mids to private list\n", __func__);
 +	spin_lock(&GlobalMid_Lock);
 +	list_for_each_safe(tmp, tmp2, &server->pending_mid_q) {
 +		mid_entry = list_entry(tmp, struct mid_q_entry, qhead);
 +		kref_get(&mid_entry->refcount);
 +		if (mid_entry->mid_state == MID_REQUEST_SUBMITTED)
 +			mid_entry->mid_state = MID_RETRY_NEEDED;
 +		list_move(&mid_entry->qhead, &retry_list);
 +		mid_entry->mid_flags |= MID_DELETED;
 +	}
 +	spin_unlock(&GlobalMid_Lock);
 +	mutex_unlock(&server->srv_mutex);
 +
 +	cifs_dbg(FYI, "%s: issuing mid callbacks\n", __func__);
 +	list_for_each_safe(tmp, tmp2, &retry_list) {
 +		mid_entry = list_entry(tmp, struct mid_q_entry, qhead);
 +		list_del_init(&mid_entry->qhead);
 +		mid_entry->callback(mid_entry);
 +		cifs_mid_q_entry_release(mid_entry);
 +	}
 +
 +	if (cifs_rdma_enabled(server)) {
 +		mutex_lock(&server->srv_mutex);
 +		smbd_destroy(server);
 +		mutex_unlock(&server->srv_mutex);
 +	}
  
  	do {
  		try_to_freeze();
* Unmerged path fs/cifs/connect.c
