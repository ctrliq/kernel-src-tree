x86/entry_64: Add VERW just before userspace transition

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-553.16.1.el8_10
commit-author Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
commit 3c7501722e6b31a6e56edd23cea5e77dbb9ffd1a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.16.1.el8_10/3c750172.failed

Mitigation for MDS is to use VERW instruction to clear any secrets in
CPU Buffers. Any memory accesses after VERW execution can still remain
in CPU buffers. It is safer to execute VERW late in return to user path
to minimize the window in which kernel data can end up in CPU buffers.
There are not many kernel secrets to be had after SWITCH_TO_USER_CR3.

Add support for deploying VERW mitigation after user register state is
restored. This helps minimize the chances of kernel data ending up into
CPU buffers after executing VERW.

Note that the mitigation at the new location is not yet enabled.

  Corner case not handled
  =======================
  Interrupts returning to kernel don't clear CPUs buffers since the
  exit-to-user path is expected to do that anyways. But, there could be
  a case when an NMI is generated in kernel after the exit-to-user path
  has cleared the buffers. This case is not handled and NMI returning to
  kernel don't clear CPU buffers because:

  1. It is rare to get an NMI after VERW, but before returning to userspace.
  2. For an unprivileged user, there is no known way to make that NMI
     less rare or target it.
  3. It would take a large number of these precisely-timed NMIs to mount
     an actual attack.  There's presumably not enough bandwidth.
  4. The NMI in question occurs after a VERW, i.e. when user state is
     restored and most interesting data is already scrubbed. Whats left
     is only the data that NMI touches, and that may or may not be of
     any interest.

	Suggested-by: Dave Hansen <dave.hansen@intel.com>
	Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
Link: https://lore.kernel.org/all/20240213-delay-verw-v8-2-a6216d83edb7%40linux.intel.com
(cherry picked from commit 3c7501722e6b31a6e56edd23cea5e77dbb9ffd1a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/entry_64.S
diff --cc arch/x86/entry/entry_64.S
index 20ae3965f651,9bb485977629..000000000000
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@@ -240,10 -158,14 +240,17 @@@ syscall_return_via_sysret
  
  	popq	%rdi
  	popq	%rsp
 +
  SYM_INNER_LABEL(entry_SYSRETQ_unsafe_stack, SYM_L_GLOBAL)
++<<<<<<< HEAD
 +	USERGS_SYSRET64
++=======
+ 	ANNOTATE_NOENDBR
+ 	swapgs
+ 	CLEAR_CPU_BUFFERS
+ 	sysretq
++>>>>>>> 3c7501722e6b (x86/entry_64: Add VERW just before userspace transition)
  SYM_INNER_LABEL(entry_SYSRETQ_end, SYM_L_GLOBAL)
 -	ANNOTATE_NOENDBR
 -	int3
  SYM_CODE_END(entry_SYSCALL_64)
  
  /*
@@@ -628,53 -511,77 +635,74 @@@ SYM_CODE_END(\asmsym
  .endm
  #endif
  
 -/*
 - * Double fault entry. Straight paranoid. No checks from which context
 - * this comes because for the espfix induced #DF this would do the wrong
 - * thing.
 - */
 -.macro idtentry_df vector asmsym cfunc
 -SYM_CODE_START(\asmsym)
 -	UNWIND_HINT_IRET_ENTRY offset=8
 -	ENDBR
 -	ASM_CLAC
 -	cld
 -
 -	/* paranoid_entry returns GS information for paranoid_exit in EBX. */
 -	call	paranoid_entry
 -	UNWIND_HINT_REGS
 -
 -	movq	%rsp, %rdi		/* pt_regs pointer into first argument */
 -	movq	ORIG_RAX(%rsp), %rsi	/* get error code into 2nd argument*/
 -	movq	$-1, ORIG_RAX(%rsp)	/* no syscall to restart */
 -	call	\cfunc
 -
 -	/* For some configurations \cfunc ends up being a noreturn. */
 -	REACHABLE
 -
 -	jmp	paranoid_exit
 -
 -_ASM_NOKPROBE(\asmsym)
 -SYM_CODE_END(\asmsym)
 -.endm
 +/* Interrupt entry/exit. */
  
  /*
 - * Include the defines which emit the idt entries which are shared
 - * shared between 32 and 64 bit and emit the __irqentry_text_* markers
 - * so the stacktrace boundary checks work.
 + * The interrupt stubs push (~vector+0x80) onto the stack and
 + * then jump to common_spurious/interrupt.
   */
 -	__ALIGN
 -	.globl __irqentry_text_start
 -__irqentry_text_start:
 +common_spurious:
 +	addq	$-0x80, (%rsp)			/* Adjust vector to [-256, -1] range */
 +	call	interrupt_entry
 +	UNWIND_HINT_REGS indirect=1
 +	call	smp_spurious_interrupt		/* rdi points to pt_regs */
 +	jmp	ret_from_intr
 +END(common_spurious)
 +_ASM_NOKPROBE(common_spurious)
 +
 +/* common_interrupt is a hotpath. Align it */
 +	.p2align CONFIG_X86_L1_CACHE_SHIFT
 +common_interrupt:
 +	addq	$-0x80, (%rsp)			/* Adjust vector to [-256, -1] range */
 +	call	interrupt_entry
 +	UNWIND_HINT_REGS indirect=1
 +	call	do_IRQ	/* rdi points to pt_regs */
 +	/* 0(%rsp): old RSP */
 +ret_from_intr:
 +	DISABLE_INTERRUPTS(CLBR_ANY)
 +	TRACE_IRQS_OFF
 +
 +	LEAVE_IRQ_STACK
  
 -#include <asm/idtentry.h>
 +	testb	$3, CS(%rsp)
 +	jz	retint_kernel
  
 -	__ALIGN
 -	.globl __irqentry_text_end
 -__irqentry_text_end:
 -	ANNOTATE_NOENDBR
 +	/* Interrupt came from user space */
 +.Lretint_user:
 +	mov	%rsp,%rdi
 +	call	prepare_exit_to_usermode
 +	TRACE_IRQS_IRETQ
  
 -SYM_CODE_START_LOCAL(common_interrupt_return)
  SYM_INNER_LABEL(swapgs_restore_regs_and_return_to_usermode, SYM_L_GLOBAL)
  	IBRS_EXIT
++<<<<<<< HEAD
 +#ifdef CONFIG_DEBUG_ENTRY
 +	/* Assert that pt_regs indicates user mode. */
 +	testb	$3, CS(%rsp)
 +	jnz	1f
++=======
+ #ifdef CONFIG_XEN_PV
+ 	ALTERNATIVE "", "jmp xenpv_restore_regs_and_return_to_usermode", X86_FEATURE_XENPV
+ #endif
+ #ifdef CONFIG_PAGE_TABLE_ISOLATION
+ 	ALTERNATIVE "", "jmp .Lpti_restore_regs_and_return_to_usermode", X86_FEATURE_PTI
+ #endif
+ 
+ 	STACKLEAK_ERASE
+ 	POP_REGS
+ 	add	$8, %rsp	/* orig_ax */
+ 	UNWIND_HINT_IRET_REGS
+ 
+ .Lswapgs_and_iret:
+ 	swapgs
+ 	CLEAR_CPU_BUFFERS
+ 	/* Assert that the IRET frame indicates user mode. */
+ 	testb	$3, 8(%rsp)
+ 	jnz	.Lnative_iret
++>>>>>>> 3c7501722e6b (x86/entry_64: Add VERW just before userspace transition)
  	ud2
 -
 -#ifdef CONFIG_PAGE_TABLE_ISOLATION
 -.Lpti_restore_regs_and_return_to_usermode:
 +1:
 +#endif
  	POP_REGS pop_rdi=0
  
  	/*
@@@ -1857,15 -1466,23 +1893,20 @@@ nmi_restore
  	 * about espfix64 on the way back to kernel mode.
  	 */
  	iretq
 -SYM_CODE_END(asm_exc_nmi)
 +SYM_CODE_END(nmi)
  
 -/*
 - * This handles SYSCALL from 32-bit code.  There is no way to program
 - * MSRs to fully disable 32-bit SYSCALL.
 - */
  SYM_CODE_START(entry_SYSCALL32_ignore)
 -	UNWIND_HINT_END_OF_STACK
 -	ENDBR
 +	UNWIND_HINT_EMPTY
  	mov	$-ENOSYS, %eax
++<<<<<<< HEAD
 +	sysret
++=======
+ 	CLEAR_CPU_BUFFERS
+ 	sysretl
++>>>>>>> 3c7501722e6b (x86/entry_64: Add VERW just before userspace transition)
  SYM_CODE_END(entry_SYSCALL32_ignore)
  
 -.pushsection .text, "ax"
 -	__FUNC_ALIGN
 -SYM_CODE_START_NOALIGN(rewind_stack_and_make_dead)
 +SYM_CODE_START(rewind_stack_do_exit)
  	UNWIND_HINT_FUNC
  	/* Prevent any naive code from trying to unwind to our caller. */
  	xorl	%ebp, %ebp
* Unmerged path arch/x86/entry/entry_64.S
diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S
index 0f87d59822e5..c32440e0a088 100644
--- a/arch/x86/entry/entry_64_compat.S
+++ b/arch/x86/entry/entry_64_compat.S
@@ -318,6 +318,7 @@ SYM_INNER_LABEL(entry_SYSRETL_compat_unsafe_stack, SYM_L_GLOBAL)
 	xorl	%r9d, %r9d
 	xorl	%r10d, %r10d
 	swapgs
+	CLEAR_CPU_BUFFERS
 	sysretl
 SYM_INNER_LABEL(entry_SYSRETL_compat_end, SYM_L_GLOBAL)
 SYM_CODE_END(entry_SYSCALL_compat)
