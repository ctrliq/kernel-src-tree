swiotlb: extend buffer pre-padding to alloc_align_mask if necessary

jira LE-1907
cve CVE-2024-35814
Rebuild_History Non-Buildable kernel-4.18.0-553.16.1.el8_10
commit-author Petr Tesarik <petr.tesarik1@huawei-partners.com>
commit af133562d5aff41fcdbe51f1a504ae04788b5fc0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.16.1.el8_10/af133562.failed

Allow a buffer pre-padding of up to alloc_align_mask, even if it requires
allocating additional IO TLB slots.

If the allocation alignment is bigger than IO_TLB_SIZE and min_align_mask
covers any non-zero bits in the original address between IO_TLB_SIZE and
alloc_align_mask, these bits are not preserved in the swiotlb buffer
address.

To fix this case, increase the allocation size and use a larger offset
within the allocated buffer. As a result, extra padding slots may be
allocated before the mapping start address.

Leave orig_addr in these padding slots initialized to INVALID_PHYS_ADDR.
These slots do not correspond to any CPU buffer, so attempts to sync the
data should be ignored.

The padding slots should be automatically released when the buffer is
unmapped. However, swiotlb_tbl_unmap_single() takes only the address of the
DMA buffer slot, not the first padding slot. Save the number of padding
slots in struct io_tlb_slot and use it to adjust the slot index in
swiotlb_release_slots(), so all allocated slots are properly freed.

Fixes: 2fd4fa5d3fb5 ("swiotlb: Fix alignment checks when both allocation and DMA masks are present")
Link: https://lore.kernel.org/linux-iommu/20240311210507.217daf8b@meshulam.tesarici.cz/
	Signed-off-by: Petr Tesarik <petr.tesarik1@huawei-partners.com>
	Reviewed-by: Michael Kelley <mhklinux@outlook.com>
	Tested-by: Michael Kelley <mhklinux@outlook.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit af133562d5aff41fcdbe51f1a504ae04788b5fc0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index c0e227dcb45e,d7a8cb93ef2d..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -61,6 -63,15 +61,18 @@@
  
  #define INVALID_PHYS_ADDR (~(phys_addr_t)0)
  
++<<<<<<< HEAD
++=======
+ /**
+  * struct io_tlb_slot - IO TLB slot descriptor
+  * @orig_addr:	The original address corresponding to a mapped entry.
+  * @alloc_size:	Size of the allocated buffer.
+  * @list:	The free list describing the number of free entries available
+  *		from each index.
+  * @pad_slots:	Number of preceding padding slots. Valid only in the first
+  *		allocated non-padding slot.
+  */
++>>>>>>> af133562d5af (swiotlb: extend buffer pre-padding to alloc_align_mask if necessary)
  struct io_tlb_slot {
  	phys_addr_t orig_addr;
  	size_t alloc_size;
@@@ -262,9 -286,11 +275,10 @@@ static void swiotlb_init_io_tlb_mem(str
  	}
  
  	for (i = 0; i < mem->nslabs; i++) {
 -		mem->slots[i].list = min(IO_TLB_SEGSIZE - io_tlb_offset(i),
 -					 mem->nslabs - i);
 +		mem->slots[i].list = IO_TLB_SEGSIZE - io_tlb_offset(i);
  		mem->slots[i].orig_addr = INVALID_PHYS_ADDR;
  		mem->slots[i].alloc_size = 0;
+ 		mem->slots[i].pad_slots = 0;
  	}
  
  	memset(vaddr, 0, bytes);
@@@ -484,12 -557,298 +498,303 @@@ void __init swiotlb_exit(void
  	memset(mem, 0, sizeof(*mem));
  }
  
++<<<<<<< HEAD
 +/*
 + * Return the offset into a iotlb slot required to keep the device happy.
++=======
+ #ifdef CONFIG_SWIOTLB_DYNAMIC
+ 
+ /**
+  * alloc_dma_pages() - allocate pages to be used for DMA
+  * @gfp:	GFP flags for the allocation.
+  * @bytes:	Size of the buffer.
+  * @phys_limit:	Maximum allowed physical address of the buffer.
+  *
+  * Allocate pages from the buddy allocator. If successful, make the allocated
+  * pages decrypted that they can be used for DMA.
+  *
+  * Return: Decrypted pages, %NULL on allocation failure, or ERR_PTR(-EAGAIN)
+  * if the allocated physical address was above @phys_limit.
+  */
+ static struct page *alloc_dma_pages(gfp_t gfp, size_t bytes, u64 phys_limit)
+ {
+ 	unsigned int order = get_order(bytes);
+ 	struct page *page;
+ 	phys_addr_t paddr;
+ 	void *vaddr;
+ 
+ 	page = alloc_pages(gfp, order);
+ 	if (!page)
+ 		return NULL;
+ 
+ 	paddr = page_to_phys(page);
+ 	if (paddr + bytes - 1 > phys_limit) {
+ 		__free_pages(page, order);
+ 		return ERR_PTR(-EAGAIN);
+ 	}
+ 
+ 	vaddr = phys_to_virt(paddr);
+ 	if (set_memory_decrypted((unsigned long)vaddr, PFN_UP(bytes)))
+ 		goto error;
+ 	return page;
+ 
+ error:
+ 	/* Intentional leak if pages cannot be encrypted again. */
+ 	if (!set_memory_encrypted((unsigned long)vaddr, PFN_UP(bytes)))
+ 		__free_pages(page, order);
+ 	return NULL;
+ }
+ 
+ /**
+  * swiotlb_alloc_tlb() - allocate a dynamic IO TLB buffer
+  * @dev:	Device for which a memory pool is allocated.
+  * @bytes:	Size of the buffer.
+  * @phys_limit:	Maximum allowed physical address of the buffer.
+  * @gfp:	GFP flags for the allocation.
+  *
+  * Return: Allocated pages, or %NULL on allocation failure.
+  */
+ static struct page *swiotlb_alloc_tlb(struct device *dev, size_t bytes,
+ 		u64 phys_limit, gfp_t gfp)
+ {
+ 	struct page *page;
+ 
+ 	/*
+ 	 * Allocate from the atomic pools if memory is encrypted and
+ 	 * the allocation is atomic, because decrypting may block.
+ 	 */
+ 	if (!gfpflags_allow_blocking(gfp) && dev && force_dma_unencrypted(dev)) {
+ 		void *vaddr;
+ 
+ 		if (!IS_ENABLED(CONFIG_DMA_COHERENT_POOL))
+ 			return NULL;
+ 
+ 		return dma_alloc_from_pool(dev, bytes, &vaddr, gfp,
+ 					   dma_coherent_ok);
+ 	}
+ 
+ 	gfp &= ~GFP_ZONEMASK;
+ 	if (phys_limit <= DMA_BIT_MASK(zone_dma_bits))
+ 		gfp |= __GFP_DMA;
+ 	else if (phys_limit <= DMA_BIT_MASK(32))
+ 		gfp |= __GFP_DMA32;
+ 
+ 	while (IS_ERR(page = alloc_dma_pages(gfp, bytes, phys_limit))) {
+ 		if (IS_ENABLED(CONFIG_ZONE_DMA32) &&
+ 		    phys_limit < DMA_BIT_MASK(64) &&
+ 		    !(gfp & (__GFP_DMA32 | __GFP_DMA)))
+ 			gfp |= __GFP_DMA32;
+ 		else if (IS_ENABLED(CONFIG_ZONE_DMA) &&
+ 			 !(gfp & __GFP_DMA))
+ 			gfp = (gfp & ~__GFP_DMA32) | __GFP_DMA;
+ 		else
+ 			return NULL;
+ 	}
+ 
+ 	return page;
+ }
+ 
+ /**
+  * swiotlb_free_tlb() - free a dynamically allocated IO TLB buffer
+  * @vaddr:	Virtual address of the buffer.
+  * @bytes:	Size of the buffer.
+  */
+ static void swiotlb_free_tlb(void *vaddr, size_t bytes)
+ {
+ 	if (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&
+ 	    dma_free_from_pool(NULL, vaddr, bytes))
+ 		return;
+ 
+ 	/* Intentional leak if pages cannot be encrypted again. */
+ 	if (!set_memory_encrypted((unsigned long)vaddr, PFN_UP(bytes)))
+ 		__free_pages(virt_to_page(vaddr), get_order(bytes));
+ }
+ 
+ /**
+  * swiotlb_alloc_pool() - allocate a new IO TLB memory pool
+  * @dev:	Device for which a memory pool is allocated.
+  * @minslabs:	Minimum number of slabs.
+  * @nslabs:	Desired (maximum) number of slabs.
+  * @nareas:	Number of areas.
+  * @phys_limit:	Maximum DMA buffer physical address.
+  * @gfp:	GFP flags for the allocations.
+  *
+  * Allocate and initialize a new IO TLB memory pool. The actual number of
+  * slabs may be reduced if allocation of @nslabs fails. If even
+  * @minslabs cannot be allocated, this function fails.
+  *
+  * Return: New memory pool, or %NULL on allocation failure.
+  */
+ static struct io_tlb_pool *swiotlb_alloc_pool(struct device *dev,
+ 		unsigned long minslabs, unsigned long nslabs,
+ 		unsigned int nareas, u64 phys_limit, gfp_t gfp)
+ {
+ 	struct io_tlb_pool *pool;
+ 	unsigned int slot_order;
+ 	struct page *tlb;
+ 	size_t pool_size;
+ 	size_t tlb_size;
+ 
+ 	if (nslabs > SLABS_PER_PAGE << MAX_PAGE_ORDER) {
+ 		nslabs = SLABS_PER_PAGE << MAX_PAGE_ORDER;
+ 		nareas = limit_nareas(nareas, nslabs);
+ 	}
+ 
+ 	pool_size = sizeof(*pool) + array_size(sizeof(*pool->areas), nareas);
+ 	pool = kzalloc(pool_size, gfp);
+ 	if (!pool)
+ 		goto error;
+ 	pool->areas = (void *)pool + sizeof(*pool);
+ 
+ 	tlb_size = nslabs << IO_TLB_SHIFT;
+ 	while (!(tlb = swiotlb_alloc_tlb(dev, tlb_size, phys_limit, gfp))) {
+ 		if (nslabs <= minslabs)
+ 			goto error_tlb;
+ 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
+ 		nareas = limit_nareas(nareas, nslabs);
+ 		tlb_size = nslabs << IO_TLB_SHIFT;
+ 	}
+ 
+ 	slot_order = get_order(array_size(sizeof(*pool->slots), nslabs));
+ 	pool->slots = (struct io_tlb_slot *)
+ 		__get_free_pages(gfp, slot_order);
+ 	if (!pool->slots)
+ 		goto error_slots;
+ 
+ 	swiotlb_init_io_tlb_pool(pool, page_to_phys(tlb), nslabs, true, nareas);
+ 	return pool;
+ 
+ error_slots:
+ 	swiotlb_free_tlb(page_address(tlb), tlb_size);
+ error_tlb:
+ 	kfree(pool);
+ error:
+ 	return NULL;
+ }
+ 
+ /**
+  * swiotlb_dyn_alloc() - dynamic memory pool allocation worker
+  * @work:	Pointer to dyn_alloc in struct io_tlb_mem.
+  */
+ static void swiotlb_dyn_alloc(struct work_struct *work)
+ {
+ 	struct io_tlb_mem *mem =
+ 		container_of(work, struct io_tlb_mem, dyn_alloc);
+ 	struct io_tlb_pool *pool;
+ 
+ 	pool = swiotlb_alloc_pool(NULL, IO_TLB_MIN_SLABS, default_nslabs,
+ 				  default_nareas, mem->phys_limit, GFP_KERNEL);
+ 	if (!pool) {
+ 		pr_warn_ratelimited("Failed to allocate new pool");
+ 		return;
+ 	}
+ 
+ 	add_mem_pool(mem, pool);
+ }
+ 
+ /**
+  * swiotlb_dyn_free() - RCU callback to free a memory pool
+  * @rcu:	RCU head in the corresponding struct io_tlb_pool.
+  */
+ static void swiotlb_dyn_free(struct rcu_head *rcu)
+ {
+ 	struct io_tlb_pool *pool = container_of(rcu, struct io_tlb_pool, rcu);
+ 	size_t slots_size = array_size(sizeof(*pool->slots), pool->nslabs);
+ 	size_t tlb_size = pool->end - pool->start;
+ 
+ 	free_pages((unsigned long)pool->slots, get_order(slots_size));
+ 	swiotlb_free_tlb(pool->vaddr, tlb_size);
+ 	kfree(pool);
+ }
+ 
+ /**
+  * swiotlb_find_pool() - find the IO TLB pool for a physical address
+  * @dev:        Device which has mapped the DMA buffer.
+  * @paddr:      Physical address within the DMA buffer.
+  *
+  * Find the IO TLB memory pool descriptor which contains the given physical
+  * address, if any.
+  *
+  * Return: Memory pool which contains @paddr, or %NULL if none.
+  */
+ struct io_tlb_pool *swiotlb_find_pool(struct device *dev, phys_addr_t paddr)
+ {
+ 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
+ 	struct io_tlb_pool *pool;
+ 
+ 	rcu_read_lock();
+ 	list_for_each_entry_rcu(pool, &mem->pools, node) {
+ 		if (paddr >= pool->start && paddr < pool->end)
+ 			goto out;
+ 	}
+ 
+ 	list_for_each_entry_rcu(pool, &dev->dma_io_tlb_pools, node) {
+ 		if (paddr >= pool->start && paddr < pool->end)
+ 			goto out;
+ 	}
+ 	pool = NULL;
+ out:
+ 	rcu_read_unlock();
+ 	return pool;
+ }
+ 
+ /**
+  * swiotlb_del_pool() - remove an IO TLB pool from a device
+  * @dev:	Owning device.
+  * @pool:	Memory pool to be removed.
+  */
+ static void swiotlb_del_pool(struct device *dev, struct io_tlb_pool *pool)
+ {
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&dev->dma_io_tlb_lock, flags);
+ 	list_del_rcu(&pool->node);
+ 	spin_unlock_irqrestore(&dev->dma_io_tlb_lock, flags);
+ 
+ 	call_rcu(&pool->rcu, swiotlb_dyn_free);
+ }
+ 
+ #endif	/* CONFIG_SWIOTLB_DYNAMIC */
+ 
+ /**
+  * swiotlb_dev_init() - initialize swiotlb fields in &struct device
+  * @dev:	Device to be initialized.
   */
- static unsigned int swiotlb_align_offset(struct device *dev, u64 addr)
+ void swiotlb_dev_init(struct device *dev)
  {
- 	return addr & dma_get_min_align_mask(dev) & (IO_TLB_SIZE - 1);
+ 	dev->dma_io_tlb_mem = &io_tlb_default_mem;
+ #ifdef CONFIG_SWIOTLB_DYNAMIC
+ 	INIT_LIST_HEAD(&dev->dma_io_tlb_pools);
+ 	spin_lock_init(&dev->dma_io_tlb_lock);
+ 	dev->dma_uses_io_tlb = false;
+ #endif
+ }
+ 
+ /**
+  * swiotlb_align_offset() - Get required offset into an IO TLB allocation.
+  * @dev:         Owning device.
+  * @align_mask:  Allocation alignment mask.
+  * @addr:        DMA address.
+  *
+  * Return the minimum offset from the start of an IO TLB allocation which is
+  * required for a given buffer address and allocation alignment to keep the
+  * device happy.
+  *
+  * First, the address bits covered by min_align_mask must be identical in the
+  * original address and the bounce buffer address. High bits are preserved by
+  * choosing a suitable IO TLB slot, but bits below IO_TLB_SHIFT require extra
+  * padding bytes before the bounce buffer.
+  *
+  * Second, @align_mask specifies which bits of the first allocated slot must
+  * be zero. This may require allocating additional padding slots, and then the
+  * offset (in bytes) from the first such padding slot is returned.
++>>>>>>> af133562d5af (swiotlb: extend buffer pre-padding to alloc_align_mask if necessary)
+  */
+ static unsigned int swiotlb_align_offset(struct device *dev,
+ 					 unsigned int align_mask, u64 addr)
+ {
+ 	return addr & dma_get_min_align_mask(dev) &
+ 		(align_mask | (IO_TLB_SIZE - 1));
  }
  
  /*
@@@ -588,23 -945,89 +893,23 @@@ static unsigned int wrap_area_index(str
  }
  
  /*
 - * Track the total used slots with a global atomic value in order to have
 - * correct information to determine the high water mark. The mem_used()
 - * function gives imprecise results because there's no locking across
 - * multiple areas.
 + * Find a suitable number of IO TLB entries size that will fit this request and
 + * allocate a buffer from that IO TLB pool.
   */
 -#ifdef CONFIG_DEBUG_FS
 -static void inc_used_and_hiwater(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -	unsigned long old_hiwater, new_used;
 -
 -	new_used = atomic_long_add_return(nslots, &mem->total_used);
 -	old_hiwater = atomic_long_read(&mem->used_hiwater);
 -	do {
 -		if (new_used <= old_hiwater)
 -			break;
 -	} while (!atomic_long_try_cmpxchg(&mem->used_hiwater,
 -					  &old_hiwater, new_used));
 -}
 -
 -static void dec_used(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -	atomic_long_sub(nslots, &mem->total_used);
 -}
 -
 -#else /* !CONFIG_DEBUG_FS */
 -static void inc_used_and_hiwater(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -}
 -static void dec_used(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -}
 -#endif /* CONFIG_DEBUG_FS */
 -
 -#ifdef CONFIG_SWIOTLB_DYNAMIC
 -#ifdef CONFIG_DEBUG_FS
 -static void inc_transient_used(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -	atomic_long_add(nslots, &mem->transient_nslabs);
 -}
 -
 -static void dec_transient_used(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -	atomic_long_sub(nslots, &mem->transient_nslabs);
 -}
 -
 -#else /* !CONFIG_DEBUG_FS */
 -static void inc_transient_used(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -}
 -static void dec_transient_used(struct io_tlb_mem *mem, unsigned int nslots)
 -{
 -}
 -#endif /* CONFIG_DEBUG_FS */
 -#endif /* CONFIG_SWIOTLB_DYNAMIC */
 -
 -/**
 - * swiotlb_search_pool_area() - search one memory area in one pool
 - * @dev:	Device which maps the buffer.
 - * @pool:	Memory pool to be searched.
 - * @area_index:	Index of the IO TLB memory area to be searched.
 - * @orig_addr:	Original (non-bounced) IO buffer address.
 - * @alloc_size: Total requested size of the bounce buffer,
 - *		including initial alignment padding.
 - * @alloc_align_mask:	Required alignment of the allocated buffer.
 - *
 - * Find a suitable sequence of IO TLB entries for the request and allocate
 - * a buffer from the given IO TLB memory area.
 - * This function takes care of locking.
 - *
 - * Return: Index of the first allocated slot, or -1 on error.
 - */
 -static int swiotlb_search_pool_area(struct device *dev, struct io_tlb_pool *pool,
 -		int area_index, phys_addr_t orig_addr, size_t alloc_size,
 +static int swiotlb_do_find_slots(struct device *dev, int area_index,
 +		phys_addr_t orig_addr, size_t alloc_size,
  		unsigned int alloc_align_mask)
  {
 -	struct io_tlb_area *area = pool->areas + area_index;
 +	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
 +	struct io_tlb_area *area = mem->areas + area_index;
  	unsigned long boundary_mask = dma_get_seg_boundary(dev);
  	dma_addr_t tbl_dma_addr =
 -		phys_to_dma_unencrypted(dev, pool->start) & boundary_mask;
 +		phys_to_dma_unencrypted(dev, mem->start) & boundary_mask;
  	unsigned long max_slots = get_max_slots(boundary_mask);
 -	unsigned int iotlb_align_mask = dma_get_min_align_mask(dev);
 +	unsigned int iotlb_align_mask =
 +		dma_get_min_align_mask(dev) | alloc_align_mask;
  	unsigned int nslots = nr_slots(alloc_size), stride;
- 	unsigned int offset = swiotlb_align_offset(dev, orig_addr);
+ 	unsigned int offset = swiotlb_align_offset(dev, 0, orig_addr);
  	unsigned int index, slots_checked, count = 0, i;
  	unsigned long flags;
  	unsigned int slot_base;
@@@ -719,7 -1350,8 +1024,12 @@@ phys_addr_t swiotlb_tbl_map_single(stru
  		unsigned long attrs)
  {
  	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
++<<<<<<< HEAD
 +	unsigned int offset = swiotlb_align_offset(dev, orig_addr);
++=======
+ 	unsigned int offset;
+ 	struct io_tlb_pool *pool;
++>>>>>>> af133562d5af (swiotlb: extend buffer pre-padding to alloc_align_mask if necessary)
  	unsigned int i;
  	int index;
  	phys_addr_t tlb_addr;
@@@ -739,8 -1372,9 +1050,9 @@@
  		return (phys_addr_t)DMA_MAPPING_ERROR;
  	}
  
+ 	offset = swiotlb_align_offset(dev, alloc_align_mask, orig_addr);
  	index = swiotlb_find_slots(dev, orig_addr,
 -				   alloc_size + offset, alloc_align_mask, &pool);
 +				   alloc_size + offset, alloc_align_mask);
  	if (index == -1) {
  		if (!(attrs & DMA_ATTR_NO_WARN))
  			dev_warn_ratelimited(dev,
@@@ -754,15 -1388,21 +1066,19 @@@
  	 * This is needed when we sync the memory.  Then we sync the buffer if
  	 * needed.
  	 */
+ 	pad_slots = offset >> IO_TLB_SHIFT;
+ 	offset &= (IO_TLB_SIZE - 1);
+ 	index += pad_slots;
+ 	pool->slots[index].pad_slots = pad_slots;
  	for (i = 0; i < nr_slots(alloc_size + offset); i++)
 -		pool->slots[index + i].orig_addr = slot_addr(orig_addr, i);
 -	tlb_addr = slot_addr(pool->start, index) + offset;
 +		mem->slots[index + i].orig_addr = slot_addr(orig_addr, i);
 +	tlb_addr = slot_addr(mem->start, index) + offset;
  	/*
 -	 * When the device is writing memory, i.e. dir == DMA_FROM_DEVICE, copy
 -	 * the original buffer to the TLB buffer before initiating DMA in order
 -	 * to preserve the original's data if the device does a partial write,
 -	 * i.e. if the device doesn't overwrite the entire buffer.  Preserving
 -	 * the original data, even if it's garbage, is necessary to match
 -	 * hardware behavior.  Use of swiotlb is supposed to be transparent,
 -	 * i.e. swiotlb must not corrupt memory by clobbering unwritten bytes.
 +	 * When dir == DMA_FROM_DEVICE we could omit the copy from the orig
 +	 * to the tlb buffer, if we knew for sure the device will
 +	 * overwirte the entire current content. But we don't. Thus
 +	 * unconditional bounce may prevent leaking swiotlb content (i.e.
 +	 * kernel memory) to user-space.
  	 */
  	swiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);
  	return tlb_addr;
@@@ -770,15 -1410,19 +1086,19 @@@
  
  static void swiotlb_release_slots(struct device *dev, phys_addr_t tlb_addr)
  {
 -	struct io_tlb_pool *mem = swiotlb_find_pool(dev, tlb_addr);
 +	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
  	unsigned long flags;
- 	unsigned int offset = swiotlb_align_offset(dev, tlb_addr);
- 	int index = (tlb_addr - offset - mem->start) >> IO_TLB_SHIFT;
- 	int nslots = nr_slots(mem->slots[index].alloc_size + offset);
- 	int aindex = index / mem->area_nslabs;
- 	struct io_tlb_area *area = &mem->areas[aindex];
+ 	unsigned int offset = swiotlb_align_offset(dev, 0, tlb_addr);
+ 	int index, nslots, aindex;
+ 	struct io_tlb_area *area;
  	int count, i;
  
+ 	index = (tlb_addr - offset - mem->start) >> IO_TLB_SHIFT;
+ 	index -= mem->slots[index].pad_slots;
+ 	nslots = nr_slots(mem->slots[index].alloc_size + offset);
+ 	aindex = index / mem->area_nslabs;
+ 	area = &mem->areas[aindex];
+ 
  	/*
  	 * Return the buffer to the free list by setting the corresponding
  	 * entries to indicate the number of contiguous entries available.
* Unmerged path kernel/dma/swiotlb.c
