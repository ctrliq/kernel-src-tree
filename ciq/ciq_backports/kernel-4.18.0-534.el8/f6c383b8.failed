netfilter: nf_tables: adapt set backend to use GC transaction API

jira LE-1907
cve CVE-2023-4244
Rebuild_History Non-Buildable kernel-4.18.0-534.el8
commit-author Pablo Neira Ayuso <pablo@netfilter.org>
commit f6c383b8c31a93752a52697f8430a71dcbc46adf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-534.el8/f6c383b8.failed

Use the GC transaction API to replace the old and buggy gc API and the
busy mark approach.

No set elements are removed from async garbage collection anymore,
instead the _DEAD bit is set on so the set element is not visible from
lookup path anymore. Async GC enqueues transaction work that might be
aborted and retried later.

rbtree and pipapo set backends does not set on the _DEAD bit from the
sync GC path since this runs in control plane path where mutex is held.
In this case, set elements are deactivated, removed and then released
via RCU callback, sync GC never fails.

Fixes: 3c4287f62044 ("nf_tables: Add set type for arbitrary concatenation of ranges")
Fixes: 8d8540c4f5e0 ("netfilter: nft_set_rbtree: add timeout support")
Fixes: 9d0982927e79 ("netfilter: nft_hash: add support for timeouts")
	Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
(cherry picked from commit f6c383b8c31a93752a52697f8430a71dcbc46adf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/netfilter/nf_tables_api.c
#	net/netfilter/nft_set_hash.c
#	net/netfilter/nft_set_pipapo.c
#	net/netfilter/nft_set_rbtree.c
diff --cc net/netfilter/nf_tables_api.c
index 65617e020aff,fd4b5da7ac3c..000000000000
--- a/net/netfilter/nf_tables_api.c
+++ b/net/netfilter/nf_tables_api.c
@@@ -5238,6 -6277,191 +5238,194 @@@ err_elem_expr_setup
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
++=======
+ struct nft_set_ext *nft_set_catchall_lookup(const struct net *net,
+ 					    const struct nft_set *set)
+ {
+ 	struct nft_set_elem_catchall *catchall;
+ 	u8 genmask = nft_genmask_cur(net);
+ 	struct nft_set_ext *ext;
+ 
+ 	list_for_each_entry_rcu(catchall, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 		if (nft_set_elem_active(ext, genmask) &&
+ 		    !nft_set_elem_expired(ext) &&
+ 		    !nft_set_elem_is_dead(ext))
+ 			return ext;
+ 	}
+ 
+ 	return NULL;
+ }
+ EXPORT_SYMBOL_GPL(nft_set_catchall_lookup);
+ 
+ void *nft_set_catchall_gc(const struct nft_set *set)
+ {
+ 	struct nft_set_elem_catchall *catchall, *next;
+ 	struct nft_set_ext *ext;
+ 	void *elem = NULL;
+ 
+ 	list_for_each_entry_safe(catchall, next, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 
+ 		if (!nft_set_elem_expired(ext) ||
+ 		    nft_set_elem_mark_busy(ext))
+ 			continue;
+ 
+ 		elem = catchall->elem;
+ 		list_del_rcu(&catchall->list);
+ 		kfree_rcu(catchall, rcu);
+ 		break;
+ 	}
+ 
+ 	return elem;
+ }
+ EXPORT_SYMBOL_GPL(nft_set_catchall_gc);
+ 
+ static int nft_setelem_catchall_insert(const struct net *net,
+ 				       struct nft_set *set,
+ 				       const struct nft_set_elem *elem,
+ 				       struct nft_set_ext **pext)
+ {
+ 	struct nft_set_elem_catchall *catchall;
+ 	u8 genmask = nft_genmask_next(net);
+ 	struct nft_set_ext *ext;
+ 
+ 	list_for_each_entry(catchall, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 		if (nft_set_elem_active(ext, genmask)) {
+ 			*pext = ext;
+ 			return -EEXIST;
+ 		}
+ 	}
+ 
+ 	catchall = kmalloc(sizeof(*catchall), GFP_KERNEL);
+ 	if (!catchall)
+ 		return -ENOMEM;
+ 
+ 	catchall->elem = elem->priv;
+ 	list_add_tail_rcu(&catchall->list, &set->catchall_list);
+ 
+ 	return 0;
+ }
+ 
+ static int nft_setelem_insert(const struct net *net,
+ 			      struct nft_set *set,
+ 			      const struct nft_set_elem *elem,
+ 			      struct nft_set_ext **ext, unsigned int flags)
+ {
+ 	int ret;
+ 
+ 	if (flags & NFT_SET_ELEM_CATCHALL)
+ 		ret = nft_setelem_catchall_insert(net, set, elem, ext);
+ 	else
+ 		ret = set->ops->insert(net, set, elem, ext);
+ 
+ 	return ret;
+ }
+ 
+ static bool nft_setelem_is_catchall(const struct nft_set *set,
+ 				    const struct nft_set_elem *elem)
+ {
+ 	struct nft_set_ext *ext = nft_set_elem_ext(set, elem->priv);
+ 
+ 	if (nft_set_ext_exists(ext, NFT_SET_EXT_FLAGS) &&
+ 	    *nft_set_ext_flags(ext) & NFT_SET_ELEM_CATCHALL)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static void nft_setelem_activate(struct net *net, struct nft_set *set,
+ 				 struct nft_set_elem *elem)
+ {
+ 	struct nft_set_ext *ext = nft_set_elem_ext(set, elem->priv);
+ 
+ 	if (nft_setelem_is_catchall(set, elem)) {
+ 		nft_set_elem_change_active(net, set, ext);
+ 	} else {
+ 		set->ops->activate(net, set, elem);
+ 	}
+ }
+ 
+ static int nft_setelem_catchall_deactivate(const struct net *net,
+ 					   struct nft_set *set,
+ 					   struct nft_set_elem *elem)
+ {
+ 	struct nft_set_elem_catchall *catchall;
+ 	struct nft_set_ext *ext;
+ 
+ 	list_for_each_entry(catchall, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 		if (!nft_is_active(net, ext))
+ 			continue;
+ 
+ 		kfree(elem->priv);
+ 		elem->priv = catchall->elem;
+ 		nft_set_elem_change_active(net, set, ext);
+ 		return 0;
+ 	}
+ 
+ 	return -ENOENT;
+ }
+ 
+ static int __nft_setelem_deactivate(const struct net *net,
+ 				    struct nft_set *set,
+ 				    struct nft_set_elem *elem)
+ {
+ 	void *priv;
+ 
+ 	priv = set->ops->deactivate(net, set, elem);
+ 	if (!priv)
+ 		return -ENOENT;
+ 
+ 	kfree(elem->priv);
+ 	elem->priv = priv;
+ 	set->ndeact++;
+ 
+ 	return 0;
+ }
+ 
+ static int nft_setelem_deactivate(const struct net *net,
+ 				  struct nft_set *set,
+ 				  struct nft_set_elem *elem, u32 flags)
+ {
+ 	int ret;
+ 
+ 	if (flags & NFT_SET_ELEM_CATCHALL)
+ 		ret = nft_setelem_catchall_deactivate(net, set, elem);
+ 	else
+ 		ret = __nft_setelem_deactivate(net, set, elem);
+ 
+ 	return ret;
+ }
+ 
+ static void nft_setelem_catchall_remove(const struct net *net,
+ 					const struct nft_set *set,
+ 					const struct nft_set_elem *elem)
+ {
+ 	struct nft_set_elem_catchall *catchall, *next;
+ 
+ 	list_for_each_entry_safe(catchall, next, &set->catchall_list, list) {
+ 		if (catchall->elem == elem->priv) {
+ 			list_del_rcu(&catchall->list);
+ 			kfree_rcu(catchall, rcu);
+ 			break;
+ 		}
+ 	}
+ }
+ 
+ static void nft_setelem_remove(const struct net *net,
+ 			       const struct nft_set *set,
+ 			       const struct nft_set_elem *elem)
+ {
+ 	if (nft_setelem_is_catchall(set, elem))
+ 		nft_setelem_catchall_remove(net, set, elem);
+ 	else
+ 		set->ops->remove(net, set, elem);
+ }
+ 
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  static bool nft_setelem_valid_key_end(const struct nft_set *set,
  				      struct nlattr **nla, u32 flags)
  {
@@@ -5824,13 -7077,72 +6012,76 @@@ err1
  	return err;
  }
  
 -static int __nft_set_catchall_flush(const struct nft_ctx *ctx,
 -				    struct nft_set *set,
 -				    struct nft_set_elem *elem)
 +static int nf_tables_delsetelem(struct net *net, struct sock *nlsk,
 +				struct sk_buff *skb, const struct nlmsghdr *nlh,
 +				const struct nlattr * const nla[],
 +				struct netlink_ext_ack *extack)
  {
++<<<<<<< HEAD
 +	u8 genmask = nft_genmask_next(net);
++=======
+ 	struct nft_trans *trans;
+ 
+ 	trans = nft_trans_alloc_gfp(ctx, NFT_MSG_DELSETELEM,
+ 				    sizeof(struct nft_trans_elem), GFP_KERNEL);
+ 	if (!trans)
+ 		return -ENOMEM;
+ 
+ 	nft_setelem_data_deactivate(ctx->net, set, elem);
+ 	nft_trans_elem_set(trans) = set;
+ 	nft_trans_elem(trans) = *elem;
+ 	nft_trans_commit_list_add_tail(ctx->net, trans);
+ 
+ 	return 0;
+ }
+ 
+ static int nft_set_catchall_flush(const struct nft_ctx *ctx,
+ 				  struct nft_set *set)
+ {
+ 	u8 genmask = nft_genmask_next(ctx->net);
+ 	struct nft_set_elem_catchall *catchall;
+ 	struct nft_set_elem elem;
+ 	struct nft_set_ext *ext;
+ 	int ret = 0;
+ 
+ 	list_for_each_entry_rcu(catchall, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 		if (!nft_set_elem_active(ext, genmask))
+ 			continue;
+ 
+ 		elem.priv = catchall->elem;
+ 		ret = __nft_set_catchall_flush(ctx, set, &elem);
+ 		if (ret < 0)
+ 			break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int nft_set_flush(struct nft_ctx *ctx, struct nft_set *set, u8 genmask)
+ {
+ 	struct nft_set_iter iter = {
+ 		.genmask	= genmask,
+ 		.fn		= nft_setelem_flush,
+ 	};
+ 
+ 	set->ops->walk(ctx, set, &iter);
+ 	if (!iter.err)
+ 		iter.err = nft_set_catchall_flush(ctx, set);
+ 
+ 	return iter.err;
+ }
+ 
+ static int nf_tables_delsetelem(struct sk_buff *skb,
+ 				const struct nfnl_info *info,
+ 				const struct nlattr * const nla[])
+ {
+ 	struct netlink_ext_ack *extack = info->extack;
+ 	u8 genmask = nft_genmask_next(info->net);
+ 	u8 family = info->nfmsg->nfgen_family;
+ 	struct net *net = info->net;
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  	const struct nlattr *attr;
 -	struct nft_table *table;
  	struct nft_set *set;
  	struct nft_ctx ctx;
  	int rem, err = 0;
diff --cc net/netfilter/nft_set_hash.c
index 037a2a90e4be,2f067e4596b0..000000000000
--- a/net/netfilter/nft_set_hash.c
+++ b/net/netfilter/nft_set_hash.c
@@@ -335,7 -374,10 +358,14 @@@ try_later
  	rhashtable_walk_stop(&hti);
  	rhashtable_walk_exit(&hti);
  
++<<<<<<< HEAD
 +	nft_set_gc_batch_complete(gcb);
++=======
+ 	if (gc)
+ 		nft_trans_gc_queue_async_done(gc);
+ 
+ done:
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  	queue_delayed_work(system_power_efficient_wq, &priv->gc_work,
  			   nft_set_gc_interval(set));
  }
@@@ -376,19 -418,30 +406,18 @@@ static int nft_rhash_init(const struct 
  	return 0;
  }
  
 -struct nft_rhash_ctx {
 -	const struct nft_ctx	ctx;
 -	const struct nft_set	*set;
 -};
 -
  static void nft_rhash_elem_destroy(void *ptr, void *arg)
  {
 -	struct nft_rhash_ctx *rhash_ctx = arg;
 -
 -	nf_tables_set_elem_destroy(&rhash_ctx->ctx, rhash_ctx->set, ptr);
 +	nft_set_elem_destroy(arg, ptr, true);
  }
  
 -static void nft_rhash_destroy(const struct nft_ctx *ctx,
 -			      const struct nft_set *set)
 +static void nft_rhash_destroy(const struct nft_set *set)
  {
  	struct nft_rhash *priv = nft_set_priv(set);
 -	struct nft_rhash_ctx rhash_ctx = {
 -		.ctx	= *ctx,
 -		.set	= set,
 -	};
  
  	cancel_delayed_work_sync(&priv->gc_work);
- 	rcu_barrier();
  	rhashtable_free_and_destroy(&priv->ht, nft_rhash_elem_destroy,
 -				    (void *)&rhash_ctx);
 +				    (void *)set);
  }
  
  /* Number of buckets is stored in u32, so cap our result to 1U<<31 */
diff --cc net/netfilter/nft_set_pipapo.c
index f7ee23bf2b06,a5b8301afe4a..000000000000
--- a/net/netfilter/nft_set_pipapo.c
+++ b/net/netfilter/nft_set_pipapo.c
@@@ -1442,10 -1552,18 +1453,21 @@@ static void nft_pipapo_gc_deactivate(st
   * @set:	nftables API set representation
   * @m:		Matching data
   */
- static void pipapo_gc(const struct nft_set *set, struct nft_pipapo_match *m)
+ static void pipapo_gc(const struct nft_set *_set, struct nft_pipapo_match *m)
  {
+ 	struct nft_set *set = (struct nft_set *) _set;
  	struct nft_pipapo *priv = nft_set_priv(set);
+ 	struct net *net = read_pnet(&set->net);
  	int rules_f0, first_rule = 0;
++<<<<<<< HEAD
++=======
+ 	struct nft_pipapo_elem *e;
+ 	struct nft_trans_gc *gc;
+ 
+ 	gc = nft_trans_gc_alloc(set, 0, GFP_KERNEL);
+ 	if (!gc)
+ 		return;
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  
  	while ((rules_f0 = pipapo_rules_same_key(m->f, first_rule))) {
  		union nft_pipapo_map_bucket rulemap[NFT_PIPAPO_MAX_FIELDS];
@@@ -1486,7 -1610,11 +1515,15 @@@
  		}
  	}
  
++<<<<<<< HEAD
 +	priv->last_gc = jiffies;
++=======
+ 	gc = nft_trans_gc_catchall(gc, 0);
+ 	if (gc) {
+ 		nft_trans_gc_queue_sync_done(gc);
+ 		priv->last_gc = jiffies;
+ 	}
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  }
  
  /**
@@@ -1584,9 -1739,6 +1621,12 @@@ static void nft_pipapo_activate(const s
  		return;
  
  	nft_set_elem_change_active(net, set, &e->ext);
++<<<<<<< HEAD
 +	nft_set_elem_clear_busy(&e->ext);
 +
 +	pipapo_commit(set);
++=======
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  }
  
  /**
diff --cc net/netfilter/nft_set_rbtree.c
index c80ee6034fca,f9d4c8fcbbf8..000000000000
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@@ -41,12 -38,20 +41,18 @@@ static bool nft_rbtree_interval_start(c
  	return !nft_rbtree_interval_end(rbe);
  }
  
 -static int nft_rbtree_cmp(const struct nft_set *set,
 -			  const struct nft_rbtree_elem *e1,
 -			  const struct nft_rbtree_elem *e2)
 +static bool nft_rbtree_equal(const struct nft_set *set, const void *this,
 +			     const struct nft_rbtree_elem *interval)
  {
 -	return memcmp(nft_set_ext_key(&e1->ext), nft_set_ext_key(&e2->ext),
 -		      set->klen);
 +	return memcmp(this, nft_set_ext_key(&interval->ext), set->klen) == 0;
  }
  
+ static bool nft_rbtree_elem_expired(const struct nft_rbtree_elem *rbe)
+ {
+ 	return nft_set_elem_expired(&rbe->ext) ||
+ 	       nft_set_elem_is_dead(&rbe->ext);
+ }
+ 
  static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
  				const u32 *key, const struct nft_set_ext **ext,
  				unsigned int seq)
@@@ -217,6 -221,90 +223,93 @@@ static void *nft_rbtree_get(const struc
  	return rbe;
  }
  
++<<<<<<< HEAD
++=======
+ static void nft_rbtree_gc_remove(struct net *net, struct nft_set *set,
+ 				 struct nft_rbtree *priv,
+ 				 struct nft_rbtree_elem *rbe)
+ {
+ 	struct nft_set_elem elem = {
+ 		.priv	= rbe,
+ 	};
+ 
+ 	nft_setelem_data_deactivate(net, set, &elem);
+ 	rb_erase(&rbe->node, &priv->root);
+ }
+ 
+ static int nft_rbtree_gc_elem(const struct nft_set *__set,
+ 			      struct nft_rbtree *priv,
+ 			      struct nft_rbtree_elem *rbe,
+ 			      u8 genmask)
+ {
+ 	struct nft_set *set = (struct nft_set *)__set;
+ 	struct rb_node *prev = rb_prev(&rbe->node);
+ 	struct net *net = read_pnet(&set->net);
+ 	struct nft_rbtree_elem *rbe_prev;
+ 	struct nft_trans_gc *gc;
+ 
+ 	gc = nft_trans_gc_alloc(set, 0, GFP_ATOMIC);
+ 	if (!gc)
+ 		return -ENOMEM;
+ 
+ 	/* search for end interval coming before this element.
+ 	 * end intervals don't carry a timeout extension, they
+ 	 * are coupled with the interval start element.
+ 	 */
+ 	while (prev) {
+ 		rbe_prev = rb_entry(prev, struct nft_rbtree_elem, node);
+ 		if (nft_rbtree_interval_end(rbe_prev) &&
+ 		    nft_set_elem_active(&rbe_prev->ext, genmask))
+ 			break;
+ 
+ 		prev = rb_prev(prev);
+ 	}
+ 
+ 	if (prev) {
+ 		rbe_prev = rb_entry(prev, struct nft_rbtree_elem, node);
+ 		nft_rbtree_gc_remove(net, set, priv, rbe_prev);
+ 
+ 		/* There is always room in this trans gc for this element,
+ 		 * memory allocation never actually happens, hence, the warning
+ 		 * splat in such case. No need to set NFT_SET_ELEM_DEAD_BIT,
+ 		 * this is synchronous gc which never fails.
+ 		 */
+ 		gc = nft_trans_gc_queue_sync(gc, GFP_ATOMIC);
+ 		if (WARN_ON_ONCE(!gc))
+ 			return -ENOMEM;
+ 
+ 		nft_trans_gc_elem_add(gc, rbe_prev);
+ 	}
+ 
+ 	nft_rbtree_gc_remove(net, set, priv, rbe);
+ 	gc = nft_trans_gc_queue_sync(gc, GFP_ATOMIC);
+ 	if (WARN_ON_ONCE(!gc))
+ 		return -ENOMEM;
+ 
+ 	nft_trans_gc_elem_add(gc, rbe);
+ 
+ 	nft_trans_gc_queue_sync_done(gc);
+ 
+ 	return 0;
+ }
+ 
+ static bool nft_rbtree_update_first(const struct nft_set *set,
+ 				    struct nft_rbtree_elem *rbe,
+ 				    struct rb_node *first)
+ {
+ 	struct nft_rbtree_elem *first_elem;
+ 
+ 	first_elem = rb_entry(first, struct nft_rbtree_elem, node);
+ 	/* this element is closest to where the new element is to be inserted:
+ 	 * update the first element for the node list path.
+ 	 */
+ 	if (nft_rbtree_cmp(set, rbe, first_elem) < 0)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
  			       struct nft_rbtree_elem *new,
  			       struct nft_set_ext **ext)
@@@ -528,8 -669,9 +626,14 @@@ try_later
  	write_seqcount_end(&priv->count);
  	write_unlock_bh(&priv->lock);
  
++<<<<<<< HEAD
 +	nft_set_gc_batch_complete(gcb);
 +
++=======
+ 	if (gc)
+ 		nft_trans_gc_queue_async_done(gc);
+ done:
++>>>>>>> f6c383b8c31a (netfilter: nf_tables: adapt set backend to use GC transaction API)
  	queue_delayed_work(system_power_efficient_wq, &priv->gc_work,
  			   nft_set_gc_interval(set));
  }
* Unmerged path net/netfilter/nf_tables_api.c
* Unmerged path net/netfilter/nft_set_hash.c
* Unmerged path net/netfilter/nft_set_pipapo.c
* Unmerged path net/netfilter/nft_set_rbtree.c
