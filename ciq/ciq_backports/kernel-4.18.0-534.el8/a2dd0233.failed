netfilter: nf_tables: remove busy mark and gc batch API

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-534.el8
commit-author Pablo Neira Ayuso <pablo@netfilter.org>
commit a2dd0233cbc4d8a0abb5f64487487ffc9265beb5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-534.el8/a2dd0233.failed

Ditch it, it has been replace it by the GC transaction API and it has no
clients anymore.

	Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
(cherry picked from commit a2dd0233cbc4d8a0abb5f64487487ffc9265beb5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/net/netfilter/nf_tables.h
#	net/netfilter/nf_tables_api.c
diff --cc include/net/netfilter/nf_tables.h
index afd104d4fe75,35870858ddf2..000000000000
--- a/include/net/netfilter/nf_tables.h
+++ b/include/net/netfilter/nf_tables.h
@@@ -522,9 -597,14 +522,15 @@@ struct nft_set *nft_set_lookup_global(c
  				      const struct nlattr *nla_set_id,
  				      u8 genmask);
  
++<<<<<<< HEAD
++=======
+ struct nft_set_ext *nft_set_catchall_lookup(const struct net *net,
+ 					    const struct nft_set *set);
+ 
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  static inline unsigned long nft_set_gc_interval(const struct nft_set *set)
  {
 -	u32 gc_int = READ_ONCE(set->gc_int);
 -
 -	return gc_int ? msecs_to_jiffies(gc_int) : HZ;
 +	return set->gc_int ? msecs_to_jiffies(set->gc_int) : HZ;
  }
  
  /**
@@@ -732,63 -812,9 +738,7 @@@ int nft_set_elem_expr_clone(const struc
  			    struct nft_expr *expr_array[]);
  void nft_set_elem_destroy(const struct nft_set *set, void *elem,
  			  bool destroy_expr);
 -void nf_tables_set_elem_destroy(const struct nft_ctx *ctx,
 -				const struct nft_set *set, void *elem);
  
- /**
-  *	struct nft_set_gc_batch_head - nf_tables set garbage collection batch
-  *
-  *	@rcu: rcu head
-  *	@set: set the elements belong to
-  *	@cnt: count of elements
-  */
- struct nft_set_gc_batch_head {
- 	struct rcu_head			rcu;
- 	const struct nft_set		*set;
- 	unsigned int			cnt;
- };
- 
- #define NFT_SET_GC_BATCH_SIZE	((PAGE_SIZE -				  \
- 				  sizeof(struct nft_set_gc_batch_head)) / \
- 				 sizeof(void *))
- 
- /**
-  *	struct nft_set_gc_batch - nf_tables set garbage collection batch
-  *
-  * 	@head: GC batch head
-  * 	@elems: garbage collection elements
-  */
- struct nft_set_gc_batch {
- 	struct nft_set_gc_batch_head	head;
- 	void				*elems[NFT_SET_GC_BATCH_SIZE];
- };
- 
- struct nft_set_gc_batch *nft_set_gc_batch_alloc(const struct nft_set *set,
- 						gfp_t gfp);
- void nft_set_gc_batch_release(struct rcu_head *rcu);
- 
- static inline void nft_set_gc_batch_complete(struct nft_set_gc_batch *gcb)
- {
- 	if (gcb != NULL)
- 		call_rcu(&gcb->head.rcu, nft_set_gc_batch_release);
- }
- 
- static inline struct nft_set_gc_batch *
- nft_set_gc_batch_check(const struct nft_set *set, struct nft_set_gc_batch *gcb,
- 		       gfp_t gfp)
- {
- 	if (gcb != NULL) {
- 		if (gcb->head.cnt + 1 < ARRAY_SIZE(gcb->elems))
- 			return gcb;
- 		nft_set_gc_batch_complete(gcb);
- 	}
- 	return nft_set_gc_batch_alloc(set, gfp);
- }
- 
- static inline void nft_set_gc_batch_add(struct nft_set_gc_batch *gcb,
- 					void *elem)
- {
- 	gcb->elems[gcb->head.cnt++] = elem;
- }
- 
  struct nft_expr_ops;
  /**
   *	struct nft_expr_type - nf_tables expression type
@@@ -1382,39 -1501,32 +1332,65 @@@ static inline void nft_set_elem_change_
  	ext->genmask ^= nft_genmask_next(net);
  }
  
++<<<<<<< HEAD
 +/*
 + * We use a free bit in the genmask field to indicate the element
 + * is busy, meaning it is currently being processed either by
 + * the netlink API or GC.
 + *
 + * Even though the genmask is only a single byte wide, this works
 + * because the extension structure if fully constant once initialized,
 + * so there are no non-atomic write accesses unless it is already
 + * marked busy.
 + */
 +#define NFT_SET_ELEM_BUSY_MASK	(1 << 2)
++=======
+ #endif /* IS_ENABLED(CONFIG_NF_TABLES) */
+ 
+ #define NFT_SET_ELEM_DEAD_MASK	(1 << 2)
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  
  #if defined(__LITTLE_ENDIAN_BITFIELD)
- #define NFT_SET_ELEM_BUSY_BIT	2
+ #define NFT_SET_ELEM_DEAD_BIT	2
  #elif defined(__BIG_ENDIAN_BITFIELD)
++<<<<<<< HEAD
 +#define NFT_SET_ELEM_BUSY_BIT	(BITS_PER_LONG - BITS_PER_BYTE + 2)
++=======
+ #define NFT_SET_ELEM_DEAD_BIT	(BITS_PER_LONG - BITS_PER_BYTE + 2)
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  #else
  #error
  #endif
  
++<<<<<<< HEAD
 +static inline int nft_set_elem_mark_busy(struct nft_set_ext *ext)
++=======
+ static inline void nft_set_elem_dead(struct nft_set_ext *ext)
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  {
  	unsigned long *word = (unsigned long *)ext;
  
  	BUILD_BUG_ON(offsetof(struct nft_set_ext, genmask) != 0);
++<<<<<<< HEAD
 +	return test_and_set_bit(NFT_SET_ELEM_BUSY_BIT, word);
 +}
 +
 +static inline void nft_set_elem_clear_busy(struct nft_set_ext *ext)
 +{
 +	unsigned long *word = (unsigned long *)ext;
 +
 +	clear_bit(NFT_SET_ELEM_BUSY_BIT, word);
++=======
+ 	set_bit(NFT_SET_ELEM_DEAD_BIT, word);
+ }
+ 
+ static inline int nft_set_elem_is_dead(const struct nft_set_ext *ext)
+ {
+ 	unsigned long *word = (unsigned long *)ext;
+ 
+ 	BUILD_BUG_ON(offsetof(struct nft_set_ext, genmask) != 0);
+ 	return test_bit(NFT_SET_ELEM_DEAD_BIT, word);
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  }
  
  /**
diff --cc net/netfilter/nf_tables_api.c
index 65617e020aff,c62227ae7746..000000000000
--- a/net/netfilter/nf_tables_api.c
+++ b/net/netfilter/nf_tables_api.c
@@@ -5238,6 -6277,168 +5238,171 @@@ err_elem_expr_setup
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
++=======
+ struct nft_set_ext *nft_set_catchall_lookup(const struct net *net,
+ 					    const struct nft_set *set)
+ {
+ 	struct nft_set_elem_catchall *catchall;
+ 	u8 genmask = nft_genmask_cur(net);
+ 	struct nft_set_ext *ext;
+ 
+ 	list_for_each_entry_rcu(catchall, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 		if (nft_set_elem_active(ext, genmask) &&
+ 		    !nft_set_elem_expired(ext) &&
+ 		    !nft_set_elem_is_dead(ext))
+ 			return ext;
+ 	}
+ 
+ 	return NULL;
+ }
+ EXPORT_SYMBOL_GPL(nft_set_catchall_lookup);
+ 
+ static int nft_setelem_catchall_insert(const struct net *net,
+ 				       struct nft_set *set,
+ 				       const struct nft_set_elem *elem,
+ 				       struct nft_set_ext **pext)
+ {
+ 	struct nft_set_elem_catchall *catchall;
+ 	u8 genmask = nft_genmask_next(net);
+ 	struct nft_set_ext *ext;
+ 
+ 	list_for_each_entry(catchall, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 		if (nft_set_elem_active(ext, genmask)) {
+ 			*pext = ext;
+ 			return -EEXIST;
+ 		}
+ 	}
+ 
+ 	catchall = kmalloc(sizeof(*catchall), GFP_KERNEL);
+ 	if (!catchall)
+ 		return -ENOMEM;
+ 
+ 	catchall->elem = elem->priv;
+ 	list_add_tail_rcu(&catchall->list, &set->catchall_list);
+ 
+ 	return 0;
+ }
+ 
+ static int nft_setelem_insert(const struct net *net,
+ 			      struct nft_set *set,
+ 			      const struct nft_set_elem *elem,
+ 			      struct nft_set_ext **ext, unsigned int flags)
+ {
+ 	int ret;
+ 
+ 	if (flags & NFT_SET_ELEM_CATCHALL)
+ 		ret = nft_setelem_catchall_insert(net, set, elem, ext);
+ 	else
+ 		ret = set->ops->insert(net, set, elem, ext);
+ 
+ 	return ret;
+ }
+ 
+ static bool nft_setelem_is_catchall(const struct nft_set *set,
+ 				    const struct nft_set_elem *elem)
+ {
+ 	struct nft_set_ext *ext = nft_set_elem_ext(set, elem->priv);
+ 
+ 	if (nft_set_ext_exists(ext, NFT_SET_EXT_FLAGS) &&
+ 	    *nft_set_ext_flags(ext) & NFT_SET_ELEM_CATCHALL)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static void nft_setelem_activate(struct net *net, struct nft_set *set,
+ 				 struct nft_set_elem *elem)
+ {
+ 	struct nft_set_ext *ext = nft_set_elem_ext(set, elem->priv);
+ 
+ 	if (nft_setelem_is_catchall(set, elem)) {
+ 		nft_set_elem_change_active(net, set, ext);
+ 	} else {
+ 		set->ops->activate(net, set, elem);
+ 	}
+ }
+ 
+ static int nft_setelem_catchall_deactivate(const struct net *net,
+ 					   struct nft_set *set,
+ 					   struct nft_set_elem *elem)
+ {
+ 	struct nft_set_elem_catchall *catchall;
+ 	struct nft_set_ext *ext;
+ 
+ 	list_for_each_entry(catchall, &set->catchall_list, list) {
+ 		ext = nft_set_elem_ext(set, catchall->elem);
+ 		if (!nft_is_active(net, ext))
+ 			continue;
+ 
+ 		kfree(elem->priv);
+ 		elem->priv = catchall->elem;
+ 		nft_set_elem_change_active(net, set, ext);
+ 		return 0;
+ 	}
+ 
+ 	return -ENOENT;
+ }
+ 
+ static int __nft_setelem_deactivate(const struct net *net,
+ 				    struct nft_set *set,
+ 				    struct nft_set_elem *elem)
+ {
+ 	void *priv;
+ 
+ 	priv = set->ops->deactivate(net, set, elem);
+ 	if (!priv)
+ 		return -ENOENT;
+ 
+ 	kfree(elem->priv);
+ 	elem->priv = priv;
+ 	set->ndeact++;
+ 
+ 	return 0;
+ }
+ 
+ static int nft_setelem_deactivate(const struct net *net,
+ 				  struct nft_set *set,
+ 				  struct nft_set_elem *elem, u32 flags)
+ {
+ 	int ret;
+ 
+ 	if (flags & NFT_SET_ELEM_CATCHALL)
+ 		ret = nft_setelem_catchall_deactivate(net, set, elem);
+ 	else
+ 		ret = __nft_setelem_deactivate(net, set, elem);
+ 
+ 	return ret;
+ }
+ 
+ static void nft_setelem_catchall_remove(const struct net *net,
+ 					const struct nft_set *set,
+ 					const struct nft_set_elem *elem)
+ {
+ 	struct nft_set_elem_catchall *catchall, *next;
+ 
+ 	list_for_each_entry_safe(catchall, next, &set->catchall_list, list) {
+ 		if (catchall->elem == elem->priv) {
+ 			list_del_rcu(&catchall->list);
+ 			kfree_rcu(catchall, rcu);
+ 			break;
+ 		}
+ 	}
+ }
+ 
+ static void nft_setelem_remove(const struct net *net,
+ 			       const struct nft_set *set,
+ 			       const struct nft_set_elem *elem)
+ {
+ 	if (nft_setelem_is_catchall(set, elem))
+ 		nft_setelem_catchall_remove(net, set, elem);
+ 	else
+ 		set->ops->remove(net, set, elem);
+ }
+ 
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  static bool nft_setelem_valid_key_end(const struct nft_set *set,
  				      struct nlattr **nla, u32 flags)
  {
@@@ -5552,11 -6763,12 +5717,17 @@@ static int nft_add_set_elem(struct nft_
  	trans = nft_trans_elem_alloc(ctx, NFT_MSG_NEWSETELEM, set);
  	if (trans == NULL) {
  		err = -ENOMEM;
 -		goto err_elem_free;
 +		goto err_elem_expr;
  	}
  
++<<<<<<< HEAD
 +	ext->genmask = nft_genmask_cur(ctx->net) | NFT_SET_ELEM_BUSY_MASK;
 +	err = set->ops->insert(ctx->net, set, &elem, &ext2);
++=======
+ 	ext->genmask = nft_genmask_cur(ctx->net);
+ 
+ 	err = nft_setelem_insert(ctx->net, set, &elem, &ext2, flags);
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  	if (err) {
  		if (err == -EEXIST) {
  			if (nft_set_ext_exists(ext, NFT_SET_EXT_DATA) ^
@@@ -5868,31 -7158,6 +6039,34 @@@ static int nf_tables_delsetelem(struct 
  	return err;
  }
  
++<<<<<<< HEAD
 +void nft_set_gc_batch_release(struct rcu_head *rcu)
 +{
 +	struct nft_set_gc_batch *gcb;
 +	unsigned int i;
 +
 +	gcb = container_of(rcu, struct nft_set_gc_batch, head.rcu);
 +	for (i = 0; i < gcb->head.cnt; i++)
 +		nft_set_elem_destroy(gcb->head.set, gcb->elems[i], true);
 +	kfree(gcb);
 +}
 +EXPORT_SYMBOL_GPL(nft_set_gc_batch_release);
 +
 +struct nft_set_gc_batch *nft_set_gc_batch_alloc(const struct nft_set *set,
 +						gfp_t gfp)
 +{
 +	struct nft_set_gc_batch *gcb;
 +
 +	gcb = kzalloc(sizeof(*gcb), gfp);
 +	if (gcb == NULL)
 +		return gcb;
 +	gcb->head.set = set;
 +	return gcb;
 +}
 +EXPORT_SYMBOL_GPL(nft_set_gc_batch_alloc);
 +
++=======
++>>>>>>> a2dd0233cbc4 (netfilter: nf_tables: remove busy mark and gc batch API)
  /*
   * Stateful objects
   */
* Unmerged path include/net/netfilter/nf_tables.h
* Unmerged path net/netfilter/nf_tables_api.c
