tools/testing/vma: add missing function stub

jira LE-3822
Rebuild_History Non-Buildable kernel-6.12.0-55.27.1.el10_0
commit-author Lorenzo Stoakes <lorenzo.stoakes@oracle.com>
commit 918850c13608c7b138512c2ecbfd3436b7a51797
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-6.12.0-55.27.1.el10_0/918850c1.failed

The hugetlb fix introduced in commit ee40c9920ac2 ("mm: fix copy_vma()
error handling for hugetlb mappings") mistakenly did not provide a stub
for the VMA userland testing, which results in a compile error when trying
to build this.

Provide this stub to resolve the issue.

Link: https://lkml.kernel.org/r/20250528-fix-vma-test-v1-1-c8a5f533b38f@oracle.com
Fixes: ee40c9920ac2 ("mm: fix copy_vma() error handling for hugetlb mappings")
	Signed-off-by: Lorenzo Stoakes <lorenzo.stoakes@oracle.com>
	Reviewed-by:  Liam R. Howlett <Liam.Howlett@oracle.com>
	Reviewed-by: Pedro Falcato <pfalcato@suse.de>
	Cc: Jann Horn <jannh@google.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 918850c13608c7b138512c2ecbfd3436b7a51797)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/vma/vma_internal.h
diff --cc tools/testing/vma/vma_internal.h
index c5b9da034511,441feb21aa5a..000000000000
--- a/tools/testing/vma/vma_internal.h
+++ b/tools/testing/vma/vma_internal.h
@@@ -920,4 -1122,348 +920,351 @@@ static inline bool signal_pending(void 
  	return false;
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool is_file_hugepages(struct file *)
+ {
+ 	return false;
+ }
+ 
+ static inline int security_vm_enough_memory_mm(struct mm_struct *, long)
+ {
+ 	return 0;
+ }
+ 
+ static inline bool may_expand_vm(struct mm_struct *, vm_flags_t, unsigned long)
+ {
+ 	return true;
+ }
+ 
+ static inline void vm_flags_init(struct vm_area_struct *vma,
+ 				 vm_flags_t flags)
+ {
+ 	vma->__vm_flags = flags;
+ }
+ 
+ static inline void vm_flags_set(struct vm_area_struct *vma,
+ 				vm_flags_t flags)
+ {
+ 	vma_start_write(vma);
+ 	vma->__vm_flags |= flags;
+ }
+ 
+ static inline void vm_flags_clear(struct vm_area_struct *vma,
+ 				  vm_flags_t flags)
+ {
+ 	vma_start_write(vma);
+ 	vma->__vm_flags &= ~flags;
+ }
+ 
+ static inline int shmem_zero_setup(struct vm_area_struct *)
+ {
+ 	return 0;
+ }
+ 
+ static inline void vma_set_anonymous(struct vm_area_struct *vma)
+ {
+ 	vma->vm_ops = NULL;
+ }
+ 
+ static inline void ksm_add_vma(struct vm_area_struct *)
+ {
+ }
+ 
+ static inline void perf_event_mmap(struct vm_area_struct *)
+ {
+ }
+ 
+ static inline bool vma_is_dax(struct vm_area_struct *)
+ {
+ 	return false;
+ }
+ 
+ static inline struct vm_area_struct *get_gate_vma(struct mm_struct *)
+ {
+ 	return NULL;
+ }
+ 
+ bool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot);
+ 
+ /* Update vma->vm_page_prot to reflect vma->vm_flags. */
+ static inline void vma_set_page_prot(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_flags = vma->vm_flags;
+ 	pgprot_t vm_page_prot;
+ 
+ 	/* testing: we inline vm_pgprot_modify() to avoid clash with vma.h. */
+ 	vm_page_prot = pgprot_modify(vma->vm_page_prot, vm_get_page_prot(vm_flags));
+ 
+ 	if (vma_wants_writenotify(vma, vm_page_prot)) {
+ 		vm_flags &= ~VM_SHARED;
+ 		/* testing: we inline vm_pgprot_modify() to avoid clash with vma.h. */
+ 		vm_page_prot = pgprot_modify(vm_page_prot, vm_get_page_prot(vm_flags));
+ 	}
+ 	/* remove_protection_ptes reads vma->vm_page_prot without mmap_lock */
+ 	WRITE_ONCE(vma->vm_page_prot, vm_page_prot);
+ }
+ 
+ static inline bool arch_validate_flags(unsigned long)
+ {
+ 	return true;
+ }
+ 
+ static inline void vma_close(struct vm_area_struct *)
+ {
+ }
+ 
+ static inline int mmap_file(struct file *, struct vm_area_struct *)
+ {
+ 	return 0;
+ }
+ 
+ static inline unsigned long stack_guard_start_gap(struct vm_area_struct *vma)
+ {
+ 	if (vma->vm_flags & VM_GROWSDOWN)
+ 		return stack_guard_gap;
+ 
+ 	/* See reasoning around the VM_SHADOW_STACK definition */
+ 	if (vma->vm_flags & VM_SHADOW_STACK)
+ 		return PAGE_SIZE;
+ 
+ 	return 0;
+ }
+ 
+ static inline unsigned long vm_start_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long gap = stack_guard_start_gap(vma);
+ 	unsigned long vm_start = vma->vm_start;
+ 
+ 	vm_start -= gap;
+ 	if (vm_start > vma->vm_start)
+ 		vm_start = 0;
+ 	return vm_start;
+ }
+ 
+ static inline unsigned long vm_end_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_end = vma->vm_end;
+ 
+ 	if (vma->vm_flags & VM_GROWSUP) {
+ 		vm_end += stack_guard_gap;
+ 		if (vm_end < vma->vm_end)
+ 			vm_end = -PAGE_SIZE;
+ 	}
+ 	return vm_end;
+ }
+ 
+ static inline int is_hugepage_only_range(struct mm_struct *mm,
+ 					unsigned long addr, unsigned long len)
+ {
+ 	return 0;
+ }
+ 
+ static inline bool vma_is_accessible(struct vm_area_struct *vma)
+ {
+ 	return vma->vm_flags & VM_ACCESS_FLAGS;
+ }
+ 
+ static inline bool capable(int cap)
+ {
+ 	return true;
+ }
+ 
+ static inline bool mlock_future_ok(struct mm_struct *mm, unsigned long flags,
+ 			unsigned long bytes)
+ {
+ 	unsigned long locked_pages, limit_pages;
+ 
+ 	if (!(flags & VM_LOCKED) || capable(CAP_IPC_LOCK))
+ 		return true;
+ 
+ 	locked_pages = bytes >> PAGE_SHIFT;
+ 	locked_pages += mm->locked_vm;
+ 
+ 	limit_pages = rlimit(RLIMIT_MEMLOCK);
+ 	limit_pages >>= PAGE_SHIFT;
+ 
+ 	return locked_pages <= limit_pages;
+ }
+ 
+ static inline int __anon_vma_prepare(struct vm_area_struct *vma)
+ {
+ 	struct anon_vma *anon_vma = calloc(1, sizeof(struct anon_vma));
+ 
+ 	if (!anon_vma)
+ 		return -ENOMEM;
+ 
+ 	anon_vma->root = anon_vma;
+ 	vma->anon_vma = anon_vma;
+ 
+ 	return 0;
+ }
+ 
+ static inline int anon_vma_prepare(struct vm_area_struct *vma)
+ {
+ 	if (likely(vma->anon_vma))
+ 		return 0;
+ 
+ 	return __anon_vma_prepare(vma);
+ }
+ 
+ static inline void userfaultfd_unmap_complete(struct mm_struct *mm,
+ 					      struct list_head *uf)
+ {
+ }
+ 
+ /*
+  * Denies creating a writable executable mapping or gaining executable permissions.
+  *
+  * This denies the following:
+  *
+  *     a)      mmap(PROT_WRITE | PROT_EXEC)
+  *
+  *     b)      mmap(PROT_WRITE)
+  *             mprotect(PROT_EXEC)
+  *
+  *     c)      mmap(PROT_WRITE)
+  *             mprotect(PROT_READ)
+  *             mprotect(PROT_EXEC)
+  *
+  * But allows the following:
+  *
+  *     d)      mmap(PROT_READ | PROT_EXEC)
+  *             mmap(PROT_READ | PROT_EXEC | PROT_BTI)
+  *
+  * This is only applicable if the user has set the Memory-Deny-Write-Execute
+  * (MDWE) protection mask for the current process.
+  *
+  * @old specifies the VMA flags the VMA originally possessed, and @new the ones
+  * we propose to set.
+  *
+  * Return: false if proposed change is OK, true if not ok and should be denied.
+  */
+ static inline bool map_deny_write_exec(unsigned long old, unsigned long new)
+ {
+ 	/* If MDWE is disabled, we have nothing to deny. */
+ 	if (!test_bit(MMF_HAS_MDWE, &current->mm->flags))
+ 		return false;
+ 
+ 	/* If the new VMA is not executable, we have nothing to deny. */
+ 	if (!(new & VM_EXEC))
+ 		return false;
+ 
+ 	/* Under MDWE we do not accept newly writably executable VMAs... */
+ 	if (new & VM_WRITE)
+ 		return true;
+ 
+ 	/* ...nor previously non-executable VMAs becoming executable. */
+ 	if (!(old & VM_EXEC))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ static inline int mapping_map_writable(struct address_space *mapping)
+ {
+ 	int c = atomic_read(&mapping->i_mmap_writable);
+ 
+ 	/* Derived from the raw_atomic_inc_unless_negative() implementation. */
+ 	do {
+ 		if (c < 0)
+ 			return -EPERM;
+ 	} while (!__sync_bool_compare_and_swap(&mapping->i_mmap_writable, c, c+1));
+ 
+ 	return 0;
+ }
+ 
+ static inline unsigned long move_page_tables(struct pagetable_move_control *pmc)
+ {
+ 	(void)pmc;
+ 
+ 	return 0;
+ }
+ 
+ static inline void free_pgd_range(struct mmu_gather *tlb,
+ 			unsigned long addr, unsigned long end,
+ 			unsigned long floor, unsigned long ceiling)
+ {
+ 	(void)tlb;
+ 	(void)addr;
+ 	(void)end;
+ 	(void)floor;
+ 	(void)ceiling;
+ }
+ 
+ static inline int ksm_execve(struct mm_struct *mm)
+ {
+ 	(void)mm;
+ 
+ 	return 0;
+ }
+ 
+ static inline void ksm_exit(struct mm_struct *mm)
+ {
+ 	(void)mm;
+ }
+ 
+ static inline void vma_lock_init(struct vm_area_struct *vma, bool reset_refcnt)
+ {
+ 	(void)vma;
+ 	(void)reset_refcnt;
+ }
+ 
+ static inline void vma_numab_state_init(struct vm_area_struct *vma)
+ {
+ 	(void)vma;
+ }
+ 
+ static inline void vma_numab_state_free(struct vm_area_struct *vma)
+ {
+ 	(void)vma;
+ }
+ 
+ static inline void dup_anon_vma_name(struct vm_area_struct *orig_vma,
+ 				     struct vm_area_struct *new_vma)
+ {
+ 	(void)orig_vma;
+ 	(void)new_vma;
+ }
+ 
+ static inline void free_anon_vma_name(struct vm_area_struct *vma)
+ {
+ 	(void)vma;
+ }
+ 
+ /* Did the driver provide valid mmap hook configuration? */
+ static inline bool file_has_valid_mmap_hooks(struct file *file)
+ {
+ 	bool has_mmap = file->f_op->mmap;
+ 	bool has_mmap_prepare = file->f_op->mmap_prepare;
+ 
+ 	/* Hooks are mutually exclusive. */
+ 	if (WARN_ON_ONCE(has_mmap && has_mmap_prepare))
+ 		return false;
+ 	if (WARN_ON_ONCE(!has_mmap && !has_mmap_prepare))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static inline int call_mmap(struct file *file, struct vm_area_struct *vma)
+ {
+ 	if (WARN_ON_ONCE(file->f_op->mmap_prepare))
+ 		return -EINVAL;
+ 
+ 	return file->f_op->mmap(file, vma);
+ }
+ 
+ static inline int __call_mmap_prepare(struct file *file,
+ 		struct vm_area_desc *desc)
+ {
+ 	return file->f_op->mmap_prepare(desc);
+ }
+ 
+ static inline void fixup_hugetlb_reservations(struct vm_area_struct *vma)
+ {
+ 	(void)vma;
+ }
+ 
++>>>>>>> 918850c13608 (tools/testing/vma: add missing function stub)
  #endif	/* __MM_VMA_INTERNAL_H */
* Unmerged path tools/testing/vma/vma_internal.h
