qed: Update qed_hsi.h for fw 8.59.1.0

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-372.32.1.rt7.189.el8_6
commit-author Prabhakar Kushwaha <pkushwaha@marvell.com>
commit fe40a830dcded26f012739fd6dac0da9c805bc38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-372.32.1.rt7.189.el8_6/fe40a830.failed

The qed_hsi.h has been updated to support new FW version 8.59.1.0 with
changes.
 - Updates FW HSI (Hardware Software interface) structures.
 - Addition/update in function declaration and defines as per HSI.
 - Add generic infrastructure for FW error reporting as part of
   common event queue handling.
 - Move malicious VF error reporting to FW error reporting
   infrastructure.
 - Move consolidation queue initialization from FW context to ramrod
   message.

qed_hsi.h header file changes lead to change in many files to ensure
compilation.

This patch also fixes the existing checkpatch warnings and few important
checks.

	Signed-off-by: Ariel Elior <aelior@marvell.com>
	Signed-off-by: Shai Malin <smalin@marvell.com>
	Signed-off-by: Omkar Kulkarni <okulkarni@marvell.com>
	Signed-off-by: Prabhakar Kushwaha <pkushwaha@marvell.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit fe40a830dcded26f012739fd6dac0da9c805bc38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/qlogic/qed/qed_hsi.h
#	drivers/net/ethernet/qlogic/qed/qed_sriov.h
diff --cc drivers/net/ethernet/qlogic/qed/qed_hsi.h
index 467813418a64,f2cedbd9489c..000000000000
--- a/drivers/net/ethernet/qlogic/qed/qed_hsi.h
+++ b/drivers/net/ethernet/qlogic/qed/qed_hsi.h
@@@ -3745,8 -2375,25 +3838,17 @@@ enum dbg_status qed_dbg_parse_attn(stru
  /* Win 13 */
  #define GTT_BAR0_MAP_REG_PSDM_RAM	0x01a000UL
  
+ /* Returns the VOQ based on port and TC */
+ #define VOQ(port, tc, max_phys_tcs_per_port)   ((tc) ==                       \
+ 						PURE_LB_TC ? NUM_OF_PHYS_TCS *\
+ 						MAX_NUM_PORTS_BB +            \
+ 						(port) : (port) *             \
+ 						(max_phys_tcs_per_port) + (tc))
+ 
+ struct init_qm_pq_params;
+ 
  /**
 - * qed_qm_pf_mem_size(): Prepare QM ILT sizes.
 - *
 - * @num_pf_cids: Number of connections used by this PF.
 - * @num_vf_cids: Number of connections used by VFs of this PF.
 - * @num_tids: Number of tasks used by this PF.
 - * @num_pf_pqs: Number of PQs used by this PF.
 - * @num_vf_pqs: Number of PQs used by VFs of this PF.
 - *
 - * Return: The required host memory size in 4KB units.
 + * @brief qed_qm_pf_mem_size - prepare QM ILT sizes
   *
   * Returns the required host memory size in 4KB units.
   * Must be called before all QM init HSI functions.
@@@ -3789,25 -2439,38 +3902,38 @@@ struct qed_qm_pf_rt_init_params 
  	u16 num_vf_pqs;
  	u16 start_vport;
  	u16 num_vports;
+ 	u16 start_rl;
+ 	u16 num_rls;
  	u16 pf_wfq;
  	u32 pf_rl;
+ 	u32 link_speed;
  	struct init_qm_pq_params *pq_params;
  	struct init_qm_vport_params *vport_params;
+ 	struct init_qm_rl_params *rl_params;
  };
  
+ /**
+  * qed_qm_pf_rt_init(): Prepare QM runtime init values for the PF phase.
+  *
+  * @p_hwfn:  HW device data.
+  * @p_ptt: Ptt window used for writing the registers
+  * @p_params: Parameters.
+  *
+  * Return: 0 on success, -1 on error.
+  */
  int qed_qm_pf_rt_init(struct qed_hwfn *p_hwfn,
- 	struct qed_ptt *p_ptt,
- 	struct qed_qm_pf_rt_init_params *p_params);
+ 		      struct qed_ptt *p_ptt,
+ 		      struct qed_qm_pf_rt_init_params *p_params);
  
  /**
 - * qed_init_pf_wfq(): Initializes the WFQ weight of the specified PF.
 + * @brief qed_init_pf_wfq - Initializes the WFQ weight of the specified PF
   *
 - * @p_hwfn: HW device data.
 - * @p_ptt: Ptt window used for writing the registers
 - * @pf_id: PF ID
 - * @pf_wfq: WFQ weight. Must be non-zero.
 + * @param p_hwfn
 + * @param p_ptt - ptt window used for writing the registers
 + * @param pf_id - PF ID
 + * @param pf_wfq - WFQ weight. Must be non-zero.
   *
 - * Return: 0 on success, -1 on error.
 + * @return 0 on success, -1 on error.
   */
  int qed_init_pf_wfq(struct qed_hwfn *p_hwfn,
  		    struct qed_ptt *p_ptt, u8 pf_id, u16 pf_wfq);
@@@ -3842,32 -2505,50 +3968,60 @@@ int qed_init_vport_wfq(struct qed_hwfn 
  		       u16 first_tx_pq_id[NUM_OF_TCS], u16 wfq);
  
  /**
++<<<<<<< HEAD
 + * @brief qed_init_global_rl - Initializes the rate limit of the specified
 + * rate limiter
 + *
 + * @param p_hwfn
 + * @param p_ptt - ptt window used for writing the registers
 + * @param rl_id - RL ID
 + * @param rate_limit - rate limit in Mb/sec units
++=======
+  * qed_init_vport_tc_wfq(): Initializes the WFQ weight of the specified
+  *                          VPORT and TC.
+  *
+  * @p_hwfn: HW device data.
+  * @p_ptt: Ptt window used for writing the registers.
+  * @first_tx_pq_id: The first Tx PQ ID associated with the VPORT and TC.
+  *                  (filled by qed_qm_pf_rt_init).
+  * @weight: VPORT+TC WFQ weight.
+  *
+  * Return: 0 on success, -1 on error.
+  */
+ int qed_init_vport_tc_wfq(struct qed_hwfn *p_hwfn,
+ 			  struct qed_ptt *p_ptt,
+ 			  u16 first_tx_pq_id, u16 weight);
+ 
+ /**
+  * qed_init_global_rl():  Initializes the rate limit of the specified
+  * rate limiter.
+  *
+  * @p_hwfn: HW device data.
+  * @p_ptt: Ptt window used for writing the registers.
+  * @rl_id: RL ID.
+  * @rate_limit: Rate limit in Mb/sec units
+  * @vport_rl_type: Vport RL type.
++>>>>>>> fe40a830dcde (qed: Update qed_hsi.h for fw 8.59.1.0)
   *
 - * Return: 0 on success, -1 on error.
 + * @return 0 on success, -1 on error.
   */
  int qed_init_global_rl(struct qed_hwfn *p_hwfn,
  		       struct qed_ptt *p_ptt,
- 		       u16 rl_id, u32 rate_limit);
+ 		       u16 rl_id, u32 rate_limit,
+ 		       enum init_qm_rl_type vport_rl_type);
  
  /**
 - * qed_send_qm_stop_cmd(): Sends a stop command to the QM.
 + * @brief qed_send_qm_stop_cmd  Sends a stop command to the QM
   *
 - * @p_hwfn: HW device data.
 - * @p_ptt: Ptt window used for writing the registers.
 - * @is_release_cmd: true for release, false for stop.
 - * @is_tx_pq: true for Tx PQs, false for Other PQs.
 - * @start_pq: first PQ ID to stop
 - * @num_pqs: Number of PQs to stop, starting from start_pq.
 + * @param p_hwfn
 + * @param p_ptt
 + * @param is_release_cmd - true for release, false for stop.
 + * @param is_tx_pq - true for Tx PQs, false for Other PQs.
 + * @param start_pq - first PQ ID to stop
 + * @param num_pqs - Number of PQs to stop, starting from start_pq.
   *
 - * Return: Bool, true if successful, false if timeout occurred while waiting
 - *         for QM command done.
 + * @return bool, true if successful, false if timeout occurred while waiting for
 + *	QM command done.
   */
  bool qed_send_qm_stop_cmd(struct qed_hwfn *p_hwfn,
  			  struct qed_ptt *p_ptt,
@@@ -4056,342 -2763,28 +4210,354 @@@ void qed_fw_overlay_init_ram(struct qed
  			     struct phys_mem_desc *fw_overlay_mem);
  
  /**
 - * qed_fw_overlay_mem_free(): Frees the FW overlay memory.
 - *
 - * @p_hwfn: HW device data.
 - * @fw_overlay_mem: The allocated FW overlay memory to free.
 + * @brief qed_fw_overlay_mem_free - Frees the FW overlay memory.
   *
 - * Return: Void.
 + * @param p_hwfn - HW device data.
 + * @param fw_overlay_mem - the allocated FW overlay memory to free.
   */
  void qed_fw_overlay_mem_free(struct qed_hwfn *p_hwfn,
- 			     struct phys_mem_desc *fw_overlay_mem);
+ 			     struct phys_mem_desc **fw_overlay_mem);
+ 
+ #define PCICFG_OFFSET					0x2000
+ #define GRC_CONFIG_REG_PF_INIT_VF			0x624
+ 
+ /* First VF_NUM for PF is encoded in this register.
+  * The number of VFs assigned to a PF is assumed to be a multiple of 8.
+  * Software should program these bits based on Total Number of VFs programmed
+  * for each PF.
+  * Since registers from 0x000-0x7ff are spilt across functions, each PF will
+  * have the same location for the same 4 bits
+  */
+ #define GRC_CR_PF_INIT_VF_PF_FIRST_VF_NUM_MASK		0xff
  
 +/* Ystorm flow control mode. Use enum fw_flow_ctrl_mode */
 +#define YSTORM_FLOW_CONTROL_MODE_OFFSET			(IRO[0].base)
 +#define YSTORM_FLOW_CONTROL_MODE_SIZE			(IRO[0].size)
 +
 +/* Tstorm port statistics */
 +#define TSTORM_PORT_STAT_OFFSET(port_id) \
 +	(IRO[1].base + ((port_id) * IRO[1].m1))
 +#define TSTORM_PORT_STAT_SIZE				(IRO[1].size)
 +
 +/* Tstorm ll2 port statistics */
 +#define TSTORM_LL2_PORT_STAT_OFFSET(port_id) \
 +	(IRO[2].base + ((port_id) * IRO[2].m1))
 +#define TSTORM_LL2_PORT_STAT_SIZE			(IRO[2].size)
 +
 +/* Ustorm VF-PF Channel ready flag */
 +#define USTORM_VF_PF_CHANNEL_READY_OFFSET(vf_id) \
 +	(IRO[3].base + ((vf_id) * IRO[3].m1))
 +#define USTORM_VF_PF_CHANNEL_READY_SIZE			(IRO[3].size)
 +
 +/* Ustorm Final flr cleanup ack */
 +#define USTORM_FLR_FINAL_ACK_OFFSET(pf_id) \
 +	(IRO[4].base + ((pf_id) * IRO[4].m1))
 +#define USTORM_FLR_FINAL_ACK_SIZE			(IRO[4].size)
 +
 +/* Ustorm Event ring consumer */
 +#define USTORM_EQE_CONS_OFFSET(pf_id) \
 +	(IRO[5].base + ((pf_id) * IRO[5].m1))
 +#define USTORM_EQE_CONS_SIZE				(IRO[5].size)
 +
 +/* Ustorm eth queue zone */
 +#define USTORM_ETH_QUEUE_ZONE_OFFSET(queue_zone_id) \
 +	(IRO[6].base + ((queue_zone_id) * IRO[6].m1))
 +#define USTORM_ETH_QUEUE_ZONE_SIZE			(IRO[6].size)
 +
 +/* Ustorm Common Queue ring consumer */
 +#define USTORM_COMMON_QUEUE_CONS_OFFSET(queue_zone_id) \
 +	(IRO[7].base + ((queue_zone_id) * IRO[7].m1))
 +#define USTORM_COMMON_QUEUE_CONS_SIZE			(IRO[7].size)
 +
 +/* Xstorm common PQ info */
 +#define XSTORM_PQ_INFO_OFFSET(pq_id) \
 +	(IRO[8].base + ((pq_id) * IRO[8].m1))
 +#define XSTORM_PQ_INFO_SIZE				(IRO[8].size)
 +
 +/* Xstorm Integration Test Data */
 +#define XSTORM_INTEG_TEST_DATA_OFFSET			(IRO[9].base)
 +#define XSTORM_INTEG_TEST_DATA_SIZE			(IRO[9].size)
 +
 +/* Ystorm Integration Test Data */
 +#define YSTORM_INTEG_TEST_DATA_OFFSET			(IRO[10].base)
 +#define YSTORM_INTEG_TEST_DATA_SIZE			(IRO[10].size)
 +
 +/* Pstorm Integration Test Data */
 +#define PSTORM_INTEG_TEST_DATA_OFFSET			(IRO[11].base)
 +#define PSTORM_INTEG_TEST_DATA_SIZE			(IRO[11].size)
 +
 +/* Tstorm Integration Test Data */
 +#define TSTORM_INTEG_TEST_DATA_OFFSET			(IRO[12].base)
 +#define TSTORM_INTEG_TEST_DATA_SIZE			(IRO[12].size)
 +
 +/* Mstorm Integration Test Data */
 +#define MSTORM_INTEG_TEST_DATA_OFFSET			(IRO[13].base)
 +#define MSTORM_INTEG_TEST_DATA_SIZE			(IRO[13].size)
 +
 +/* Ustorm Integration Test Data */
 +#define USTORM_INTEG_TEST_DATA_OFFSET			(IRO[14].base)
 +#define USTORM_INTEG_TEST_DATA_SIZE			(IRO[14].size)
 +
 +/* Xstorm overlay buffer host address */
 +#define XSTORM_OVERLAY_BUF_ADDR_OFFSET			(IRO[15].base)
 +#define XSTORM_OVERLAY_BUF_ADDR_SIZE			(IRO[15].size)
 +
 +/* Ystorm overlay buffer host address */
 +#define YSTORM_OVERLAY_BUF_ADDR_OFFSET			(IRO[16].base)
 +#define YSTORM_OVERLAY_BUF_ADDR_SIZE			(IRO[16].size)
 +
 +/* Pstorm overlay buffer host address */
 +#define PSTORM_OVERLAY_BUF_ADDR_OFFSET			(IRO[17].base)
 +#define PSTORM_OVERLAY_BUF_ADDR_SIZE			(IRO[17].size)
 +
 +/* Tstorm overlay buffer host address */
 +#define TSTORM_OVERLAY_BUF_ADDR_OFFSET			(IRO[18].base)
 +#define TSTORM_OVERLAY_BUF_ADDR_SIZE			(IRO[18].size)
 +
 +/* Mstorm overlay buffer host address */
 +#define MSTORM_OVERLAY_BUF_ADDR_OFFSET			(IRO[19].base)
 +#define MSTORM_OVERLAY_BUF_ADDR_SIZE			(IRO[19].size)
 +
 +/* Ustorm overlay buffer host address */
 +#define USTORM_OVERLAY_BUF_ADDR_OFFSET			(IRO[20].base)
 +#define USTORM_OVERLAY_BUF_ADDR_SIZE			(IRO[20].size)
 +
 +/* Tstorm producers */
 +#define TSTORM_LL2_RX_PRODS_OFFSET(core_rx_queue_id) \
 +	(IRO[21].base + ((core_rx_queue_id) * IRO[21].m1))
 +#define TSTORM_LL2_RX_PRODS_SIZE			(IRO[21].size)
 +
 +/* Tstorm LightL2 queue statistics */
 +#define CORE_LL2_TSTORM_PER_QUEUE_STAT_OFFSET(core_rx_queue_id) \
 +	(IRO[22].base + ((core_rx_queue_id) * IRO[22].m1))
 +#define CORE_LL2_TSTORM_PER_QUEUE_STAT_SIZE		(IRO[22].size)
 +
 +/* Ustorm LiteL2 queue statistics */
 +#define CORE_LL2_USTORM_PER_QUEUE_STAT_OFFSET(core_rx_queue_id) \
 +	(IRO[23].base + ((core_rx_queue_id) * IRO[23].m1))
 +#define CORE_LL2_USTORM_PER_QUEUE_STAT_SIZE		(IRO[23].size)
 +
 +/* Pstorm LiteL2 queue statistics */
 +#define CORE_LL2_PSTORM_PER_QUEUE_STAT_OFFSET(core_tx_stats_id) \
 +	(IRO[24].base + ((core_tx_stats_id) * IRO[24].m1))
 +#define CORE_LL2_PSTORM_PER_QUEUE_STAT_SIZE		(IRO[24].size)
 +
 +/* Mstorm queue statistics */
 +#define MSTORM_QUEUE_STAT_OFFSET(stat_counter_id) \
 +	(IRO[25].base + ((stat_counter_id) * IRO[25].m1))
 +#define MSTORM_QUEUE_STAT_SIZE				(IRO[25].size)
 +
 +/* TPA agregation timeout in us resolution (on ASIC) */
 +#define MSTORM_TPA_TIMEOUT_US_OFFSET			(IRO[26].base)
 +#define MSTORM_TPA_TIMEOUT_US_SIZE			(IRO[26].size)
 +
 +/* Mstorm ETH VF queues producers offset in RAM. Used in default VF zone size
 + * mode
 + */
 +#define MSTORM_ETH_VF_PRODS_OFFSET(vf_id, vf_queue_id) \
 +	(IRO[27].base + ((vf_id) * IRO[27].m1) + ((vf_queue_id) * IRO[27].m2))
 +#define MSTORM_ETH_VF_PRODS_SIZE			(IRO[27].size)
 +
 +/* Mstorm ETH PF queues producers */
 +#define MSTORM_ETH_PF_PRODS_OFFSET(queue_id) \
 +	(IRO[28].base + ((queue_id) * IRO[28].m1))
 +#define MSTORM_ETH_PF_PRODS_SIZE			(IRO[28].size)
 +
 +/* Mstorm pf statistics */
 +#define MSTORM_ETH_PF_STAT_OFFSET(pf_id) \
 +	(IRO[29].base + ((pf_id) * IRO[29].m1))
 +#define MSTORM_ETH_PF_STAT_SIZE				(IRO[29].size)
 +
 +/* Ustorm queue statistics */
 +#define USTORM_QUEUE_STAT_OFFSET(stat_counter_id) \
 +	(IRO[30].base + ((stat_counter_id) * IRO[30].m1))
 +#define USTORM_QUEUE_STAT_SIZE				(IRO[30].size)
 +
 +/* Ustorm pf statistics */
 +#define USTORM_ETH_PF_STAT_OFFSET(pf_id) \
 +	(IRO[31].base + ((pf_id) * IRO[31].m1))
 +#define USTORM_ETH_PF_STAT_SIZE				(IRO[31].size)
 +
 +/* Pstorm queue statistics */
 +#define PSTORM_QUEUE_STAT_OFFSET(stat_counter_id)	\
 +	(IRO[32].base + ((stat_counter_id) * IRO[32].m1))
 +#define PSTORM_QUEUE_STAT_SIZE				(IRO[32].size)
 +
 +/* Pstorm pf statistics */
 +#define PSTORM_ETH_PF_STAT_OFFSET(pf_id) \
 +	(IRO[33].base + ((pf_id) * IRO[33].m1))
 +#define PSTORM_ETH_PF_STAT_SIZE				(IRO[33].size)
 +
 +/* Control frame's EthType configuration for TX control frame security */
 +#define PSTORM_CTL_FRAME_ETHTYPE_OFFSET(eth_type_id)	\
 +	(IRO[34].base + ((eth_type_id) * IRO[34].m1))
 +#define PSTORM_CTL_FRAME_ETHTYPE_SIZE			(IRO[34].size)
 +
 +/* Tstorm last parser message */
 +#define TSTORM_ETH_PRS_INPUT_OFFSET			(IRO[35].base)
 +#define TSTORM_ETH_PRS_INPUT_SIZE			(IRO[35].size)
 +
 +/* Tstorm Eth limit Rx rate */
 +#define ETH_RX_RATE_LIMIT_OFFSET(pf_id)	\
 +	(IRO[36].base + ((pf_id) * IRO[36].m1))
 +#define ETH_RX_RATE_LIMIT_SIZE				(IRO[36].size)
 +
 +/* RSS indirection table entry update command per PF offset in TSTORM PF BAR0.
 + * Use eth_tstorm_rss_update_data for update
 + */
 +#define TSTORM_ETH_RSS_UPDATE_OFFSET(pf_id) \
 +	(IRO[37].base + ((pf_id) * IRO[37].m1))
 +#define TSTORM_ETH_RSS_UPDATE_SIZE			(IRO[37].size)
 +
 +/* Xstorm queue zone */
 +#define XSTORM_ETH_QUEUE_ZONE_OFFSET(queue_id) \
 +	(IRO[38].base + ((queue_id) * IRO[38].m1))
 +#define XSTORM_ETH_QUEUE_ZONE_SIZE			(IRO[38].size)
 +
 +/* Ystorm cqe producer */
 +#define YSTORM_TOE_CQ_PROD_OFFSET(rss_id) \
 +	(IRO[39].base + ((rss_id) * IRO[39].m1))
 +#define YSTORM_TOE_CQ_PROD_SIZE				(IRO[39].size)
 +
 +/* Ustorm cqe producer */
 +#define USTORM_TOE_CQ_PROD_OFFSET(rss_id) \
 +	(IRO[40].base + ((rss_id) * IRO[40].m1))
 +#define USTORM_TOE_CQ_PROD_SIZE				(IRO[40].size)
 +
 +/* Ustorm grq producer */
 +#define USTORM_TOE_GRQ_PROD_OFFSET(pf_id) \
 +	(IRO[41].base + ((pf_id) * IRO[41].m1))
 +#define USTORM_TOE_GRQ_PROD_SIZE			(IRO[41].size)
 +
 +/* Tstorm cmdq-cons of given command queue-id */
 +#define TSTORM_SCSI_CMDQ_CONS_OFFSET(cmdq_queue_id) \
 +	(IRO[42].base + ((cmdq_queue_id) * IRO[42].m1))
 +#define TSTORM_SCSI_CMDQ_CONS_SIZE			(IRO[42].size)
 +
 +/* Tstorm (reflects M-Storm) bdq-external-producer of given function ID,
 + * BDqueue-id
 + */
 +#define TSTORM_SCSI_BDQ_EXT_PROD_OFFSET(storage_func_id, bdq_id) \
 +	(IRO[43].base + ((storage_func_id) * IRO[43].m1) + \
 +	 ((bdq_id) * IRO[43].m2))
 +#define TSTORM_SCSI_BDQ_EXT_PROD_SIZE			(IRO[43].size)
 +
 +/* Mstorm bdq-external-producer of given BDQ resource ID, BDqueue-id */
 +#define MSTORM_SCSI_BDQ_EXT_PROD_OFFSET(storage_func_id, bdq_id) \
 +	(IRO[44].base + ((storage_func_id) * IRO[44].m1) + \
 +	 ((bdq_id) * IRO[44].m2))
 +#define MSTORM_SCSI_BDQ_EXT_PROD_SIZE			(IRO[44].size)
 +
 +/* Tstorm iSCSI RX stats */
 +#define TSTORM_ISCSI_RX_STATS_OFFSET(storage_func_id) \
 +	(IRO[45].base + ((storage_func_id) * IRO[45].m1))
 +#define TSTORM_ISCSI_RX_STATS_SIZE			(IRO[45].size)
 +
 +/* Mstorm iSCSI RX stats */
 +#define MSTORM_ISCSI_RX_STATS_OFFSET(storage_func_id) \
 +	(IRO[46].base + ((storage_func_id) * IRO[46].m1))
 +#define MSTORM_ISCSI_RX_STATS_SIZE			(IRO[46].size)
 +
 +/* Ustorm iSCSI RX stats */
 +#define USTORM_ISCSI_RX_STATS_OFFSET(storage_func_id) \
 +	(IRO[47].base + ((storage_func_id) * IRO[47].m1))
 +#define USTORM_ISCSI_RX_STATS_SIZE			(IRO[47].size)
 +
 +/* Xstorm iSCSI TX stats */
 +#define XSTORM_ISCSI_TX_STATS_OFFSET(storage_func_id) \
 +	(IRO[48].base + ((storage_func_id) * IRO[48].m1))
 +#define XSTORM_ISCSI_TX_STATS_SIZE			(IRO[48].size)
 +
 +/* Ystorm iSCSI TX stats */
 +#define YSTORM_ISCSI_TX_STATS_OFFSET(storage_func_id) \
 +	(IRO[49].base + ((storage_func_id) * IRO[49].m1))
 +#define YSTORM_ISCSI_TX_STATS_SIZE			(IRO[49].size)
 +
 +/* Pstorm iSCSI TX stats */
 +#define PSTORM_ISCSI_TX_STATS_OFFSET(storage_func_id) \
 +	(IRO[50].base + ((storage_func_id) * IRO[50].m1))
 +#define PSTORM_ISCSI_TX_STATS_SIZE			(IRO[50].size)
 +
 +/* Tstorm FCoE RX stats */
 +#define TSTORM_FCOE_RX_STATS_OFFSET(pf_id) \
 +	(IRO[51].base + ((pf_id) * IRO[51].m1))
 +#define TSTORM_FCOE_RX_STATS_SIZE			(IRO[51].size)
 +
 +/* Pstorm FCoE TX stats */
 +#define PSTORM_FCOE_TX_STATS_OFFSET(pf_id) \
 +	(IRO[52].base + ((pf_id) * IRO[52].m1))
 +#define PSTORM_FCOE_TX_STATS_SIZE			(IRO[52].size)
 +
 +/* Pstorm RDMA queue statistics */
 +#define PSTORM_RDMA_QUEUE_STAT_OFFSET(rdma_stat_counter_id) \
 +	(IRO[53].base + ((rdma_stat_counter_id) * IRO[53].m1))
 +#define PSTORM_RDMA_QUEUE_STAT_SIZE			(IRO[53].size)
 +
 +/* Tstorm RDMA queue statistics */
 +#define TSTORM_RDMA_QUEUE_STAT_OFFSET(rdma_stat_counter_id) \
 +	(IRO[54].base + ((rdma_stat_counter_id) * IRO[54].m1))
 +#define TSTORM_RDMA_QUEUE_STAT_SIZE			(IRO[54].size)
 +
 +/* Xstorm error level for assert */
 +#define XSTORM_RDMA_ASSERT_LEVEL_OFFSET(pf_id) \
 +	(IRO[55].base + ((pf_id) * IRO[55].m1))
 +#define XSTORM_RDMA_ASSERT_LEVEL_SIZE			(IRO[55].size)
 +
 +/* Ystorm error level for assert */
 +#define YSTORM_RDMA_ASSERT_LEVEL_OFFSET(pf_id) \
 +	(IRO[56].base + ((pf_id) * IRO[56].m1))
 +#define YSTORM_RDMA_ASSERT_LEVEL_SIZE			(IRO[56].size)
 +
 +/* Pstorm error level for assert */
 +#define PSTORM_RDMA_ASSERT_LEVEL_OFFSET(pf_id) \
 +	(IRO[57].base + ((pf_id) * IRO[57].m1))
 +#define PSTORM_RDMA_ASSERT_LEVEL_SIZE			(IRO[57].size)
 +
 +/* Tstorm error level for assert */
 +#define TSTORM_RDMA_ASSERT_LEVEL_OFFSET(pf_id) \
 +	(IRO[58].base + ((pf_id) * IRO[58].m1))
 +#define TSTORM_RDMA_ASSERT_LEVEL_SIZE			(IRO[58].size)
 +
 +/* Mstorm error level for assert */
 +#define MSTORM_RDMA_ASSERT_LEVEL_OFFSET(pf_id) \
 +	(IRO[59].base + ((pf_id) * IRO[59].m1))
 +#define MSTORM_RDMA_ASSERT_LEVEL_SIZE			(IRO[59].size)
 +
 +/* Ustorm error level for assert */
 +#define USTORM_RDMA_ASSERT_LEVEL_OFFSET(pf_id) \
 +	(IRO[60].base + ((pf_id) * IRO[60].m1))
 +#define USTORM_RDMA_ASSERT_LEVEL_SIZE			(IRO[60].size)
 +
 +/* Xstorm iWARP rxmit stats */
 +#define XSTORM_IWARP_RXMIT_STATS_OFFSET(pf_id) \
 +	(IRO[61].base + ((pf_id) * IRO[61].m1))
 +#define XSTORM_IWARP_RXMIT_STATS_SIZE			(IRO[61].size)
 +
 +/* Tstorm RoCE Event Statistics */
 +#define TSTORM_ROCE_EVENTS_STAT_OFFSET(roce_pf_id)	\
 +	(IRO[62].base + ((roce_pf_id) * IRO[62].m1))
 +#define TSTORM_ROCE_EVENTS_STAT_SIZE			(IRO[62].size)
 +
 +/* DCQCN Received Statistics */
 +#define YSTORM_ROCE_DCQCN_RECEIVED_STATS_OFFSET(roce_pf_id)\
 +	(IRO[63].base + ((roce_pf_id) * IRO[63].m1))
 +#define YSTORM_ROCE_DCQCN_RECEIVED_STATS_SIZE		(IRO[63].size)
 +
 +/* RoCE Error Statistics */
 +#define YSTORM_ROCE_ERROR_STATS_OFFSET(roce_pf_id)	\
 +	(IRO[64].base + ((roce_pf_id) * IRO[64].m1))
 +#define YSTORM_ROCE_ERROR_STATS_SIZE			(IRO[64].size)
 +
 +/* DCQCN Sent Statistics */
 +#define PSTORM_ROCE_DCQCN_SENT_STATS_OFFSET(roce_pf_id)	\
 +	(IRO[65].base + ((roce_pf_id) * IRO[65].m1))
 +#define PSTORM_ROCE_DCQCN_SENT_STATS_SIZE		(IRO[65].size)
 +
 +/* RoCE CQEs Statistics */
 +#define USTORM_ROCE_CQE_STATS_OFFSET(roce_pf_id)	\
 +	(IRO[66].base + ((roce_pf_id) * IRO[66].m1))
 +#define USTORM_ROCE_CQE_STATS_SIZE			(IRO[66].size)
 +
  /* Runtime array offsets */
  #define DORQ_REG_PF_MAX_ICID_0_RT_OFFSET				0
  #define DORQ_REG_PF_MAX_ICID_1_RT_OFFSET				1
diff --cc drivers/net/ethernet/qlogic/qed/qed_sriov.h
index 64a359d8b1e6,1edf9c44dc67..000000000000
--- a/drivers/net/ethernet/qlogic/qed/qed_sriov.h
+++ b/drivers/net/ethernet/qlogic/qed/qed_sriov.h
@@@ -310,11 -314,36 +310,40 @@@ void *qed_add_tlv(struct qed_hwfn *p_hw
  void qed_dp_tlv_list(struct qed_hwfn *p_hwfn, void *tlvs_list);
  
  /**
++<<<<<<< HEAD
 + * @brief qed_iov_alloc - allocate sriov related resources
++=======
+  * qed_sriov_vfpf_malicious(): Handle malicious VF/PF.
+  *
+  * @p_hwfn: HW device data.
+  * @p_data: Pointer to data.
+  *
+  * Return: Void.
+  */
+ void qed_sriov_vfpf_malicious(struct qed_hwfn *p_hwfn,
+ 			      struct fw_err_data *p_data);
+ 
+ /**
+  * qed_sriov_eqe_event(): Callback for SRIOV events.
+  *
+  * @p_hwfn: HW device data.
+  * @opcode: Opcode.
+  * @echo: Echo.
+  * @data: data
+  * @fw_return_code: FW return code.
+  *
+  * Return: Int.
+  */
+ int qed_sriov_eqe_event(struct qed_hwfn *p_hwfn, u8 opcode, __le16 echo,
+ 			union event_ring_data *data, u8  fw_return_code);
+ 
+ /**
+  * qed_iov_alloc(): allocate sriov related resources
++>>>>>>> fe40a830dcde (qed: Update qed_hsi.h for fw 8.59.1.0)
   *
 - * @p_hwfn: HW device data.
 + * @param p_hwfn
   *
 - * Return: Int.
 + * @return int
   */
  int qed_iov_alloc(struct qed_hwfn *p_hwfn);
  
diff --git a/drivers/net/ethernet/qlogic/qed/qed_dev.c b/drivers/net/ethernet/qlogic/qed/qed_dev.c
index f8c6f753e831..5bf06cb48111 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_dev.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_dev.c
@@ -1396,12 +1396,13 @@ void qed_resc_free(struct qed_dev *cdev)
 			qed_rdma_info_free(p_hwfn);
 		}
 
+		qed_spq_unregister_async_cb(p_hwfn, PROTOCOLID_COMMON);
 		qed_iov_free(p_hwfn);
 		qed_l2_free(p_hwfn);
 		qed_dmae_info_free(p_hwfn);
 		qed_dcbx_info_free(p_hwfn);
 		qed_dbg_user_data_free(p_hwfn);
-		qed_fw_overlay_mem_free(p_hwfn, p_hwfn->fw_overlay_mem);
+		qed_fw_overlay_mem_free(p_hwfn, &p_hwfn->fw_overlay_mem);
 
 		/* Destroy doorbell recovery mechanism */
 		qed_db_recovery_teardown(p_hwfn);
@@ -1483,8 +1484,8 @@ static u16 qed_init_qm_get_num_pf_rls(struct qed_hwfn *p_hwfn)
 	u16 num_pf_rls, num_vfs = qed_init_qm_get_num_vfs(p_hwfn);
 
 	/* num RLs can't exceed resource amount of rls or vports */
-	num_pf_rls = (u16) min_t(u32, RESC_NUM(p_hwfn, QED_RL),
-				 RESC_NUM(p_hwfn, QED_VPORT));
+	num_pf_rls = (u16)min_t(u32, RESC_NUM(p_hwfn, QED_RL),
+				RESC_NUM(p_hwfn, QED_VPORT));
 
 	/* Make sure after we reserve there's something left */
 	if (num_pf_rls < num_vfs + NUM_DEFAULT_RLS)
@@ -1532,8 +1533,8 @@ static void qed_init_qm_params(struct qed_hwfn *p_hwfn)
 	bool four_port;
 
 	/* pq and vport bases for this PF */
-	qm_info->start_pq = (u16) RESC_START(p_hwfn, QED_PQ);
-	qm_info->start_vport = (u8) RESC_START(p_hwfn, QED_VPORT);
+	qm_info->start_pq = (u16)RESC_START(p_hwfn, QED_PQ);
+	qm_info->start_vport = (u8)RESC_START(p_hwfn, QED_VPORT);
 
 	/* rate limiting and weighted fair queueing are always enabled */
 	qm_info->vport_rl_en = true;
@@ -1628,9 +1629,9 @@ static void qed_init_qm_advance_vport(struct qed_hwfn *p_hwfn)
  */
 
 /* flags for pq init */
-#define PQ_INIT_SHARE_VPORT     (1 << 0)
-#define PQ_INIT_PF_RL           (1 << 1)
-#define PQ_INIT_VF_RL           (1 << 2)
+#define PQ_INIT_SHARE_VPORT     BIT(0)
+#define PQ_INIT_PF_RL           BIT(1)
+#define PQ_INIT_VF_RL           BIT(2)
 
 /* defines for pq init */
 #define PQ_INIT_DEFAULT_WRR_GROUP       1
@@ -2290,7 +2291,7 @@ int qed_resc_alloc(struct qed_dev *cdev)
 			goto alloc_no_mem;
 		}
 
-		rc = qed_eq_alloc(p_hwfn, (u16) n_eqes);
+		rc = qed_eq_alloc(p_hwfn, (u16)n_eqes);
 		if (rc)
 			goto alloc_err;
 
@@ -2375,6 +2376,49 @@ int qed_resc_alloc(struct qed_dev *cdev)
 	return rc;
 }
 
+static int qed_fw_err_handler(struct qed_hwfn *p_hwfn,
+			      u8 opcode,
+			      u16 echo,
+			      union event_ring_data *data, u8 fw_return_code)
+{
+	if (fw_return_code != COMMON_ERR_CODE_ERROR)
+		goto eqe_unexpected;
+
+	if (data->err_data.recovery_scope == ERR_SCOPE_FUNC &&
+	    le16_to_cpu(data->err_data.entity_id) >= MAX_NUM_PFS) {
+		qed_sriov_vfpf_malicious(p_hwfn, &data->err_data);
+		return 0;
+	}
+
+eqe_unexpected:
+	DP_ERR(p_hwfn,
+	       "Skipping unexpected eqe 0x%02x, FW return code 0x%x, echo 0x%x\n",
+	       opcode, fw_return_code, echo);
+	return -EINVAL;
+}
+
+static int qed_common_eqe_event(struct qed_hwfn *p_hwfn,
+				u8 opcode,
+				__le16 echo,
+				union event_ring_data *data,
+				u8 fw_return_code)
+{
+	switch (opcode) {
+	case COMMON_EVENT_VF_PF_CHANNEL:
+	case COMMON_EVENT_VF_FLR:
+		return qed_sriov_eqe_event(p_hwfn, opcode, echo, data,
+					   fw_return_code);
+	case COMMON_EVENT_FW_ERROR:
+		return qed_fw_err_handler(p_hwfn, opcode,
+					  le16_to_cpu(echo), data,
+					  fw_return_code);
+	default:
+		DP_INFO(p_hwfn->cdev, "Unknown eqe event 0x%02x, echo 0x%x\n",
+			opcode, echo);
+		return -EINVAL;
+	}
+}
+
 void qed_resc_setup(struct qed_dev *cdev)
 {
 	int i;
@@ -2403,6 +2447,8 @@ void qed_resc_setup(struct qed_dev *cdev)
 
 		qed_l2_setup(p_hwfn);
 		qed_iov_setup(p_hwfn);
+		qed_spq_register_async_cb(p_hwfn, PROTOCOLID_COMMON,
+					  qed_common_eqe_event);
 #ifdef CONFIG_QED_LL2
 		if (p_hwfn->using_ll2)
 			qed_ll2_setup(p_hwfn);
@@ -2592,7 +2638,7 @@ static void qed_init_cache_line_size(struct qed_hwfn *p_hwfn,
 			cache_line_size);
 	}
 
-	if (L1_CACHE_BYTES > wr_mbs)
+	if (wr_mbs < L1_CACHE_BYTES)
 		DP_INFO(p_hwfn,
 			"The cache line size for padding is suboptimal for performance [OS cache line size 0x%x, wr mbs 0x%x]\n",
 			L1_CACHE_BYTES, wr_mbs);
@@ -2608,13 +2654,21 @@ static int qed_hw_init_common(struct qed_hwfn *p_hwfn,
 			      struct qed_ptt *p_ptt, int hw_mode)
 {
 	struct qed_qm_info *qm_info = &p_hwfn->qm_info;
-	struct qed_qm_common_rt_init_params params;
+	struct qed_qm_common_rt_init_params *params;
 	struct qed_dev *cdev = p_hwfn->cdev;
 	u8 vf_id, max_num_vfs;
 	u16 num_pfs, pf_id;
 	u32 concrete_fid;
 	int rc = 0;
 
+	params = kzalloc(sizeof(*params), GFP_KERNEL);
+	if (!params) {
+		DP_NOTICE(p_hwfn->cdev,
+			  "Failed to allocate common init params\n");
+
+		return -ENOMEM;
+	}
+
 	qed_init_cau_rt_data(cdev);
 
 	/* Program GTT windows */
@@ -2627,16 +2681,15 @@ static int qed_hw_init_common(struct qed_hwfn *p_hwfn,
 			qm_info->pf_wfq_en = true;
 	}
 
-	memset(&params, 0, sizeof(params));
-	params.max_ports_per_engine = p_hwfn->cdev->num_ports_in_engine;
-	params.max_phys_tcs_per_port = qm_info->max_phys_tcs_per_port;
-	params.pf_rl_en = qm_info->pf_rl_en;
-	params.pf_wfq_en = qm_info->pf_wfq_en;
-	params.global_rl_en = qm_info->vport_rl_en;
-	params.vport_wfq_en = qm_info->vport_wfq_en;
-	params.port_params = qm_info->qm_port_params;
+	params->max_ports_per_engine = p_hwfn->cdev->num_ports_in_engine;
+	params->max_phys_tcs_per_port = qm_info->max_phys_tcs_per_port;
+	params->pf_rl_en = qm_info->pf_rl_en;
+	params->pf_wfq_en = qm_info->pf_wfq_en;
+	params->global_rl_en = qm_info->vport_rl_en;
+	params->vport_wfq_en = qm_info->vport_wfq_en;
+	params->port_params = qm_info->qm_port_params;
 
-	qed_qm_common_rt_init(p_hwfn, &params);
+	qed_qm_common_rt_init(p_hwfn, params);
 
 	qed_cxt_hw_init_common(p_hwfn);
 
@@ -2644,7 +2697,7 @@ static int qed_hw_init_common(struct qed_hwfn *p_hwfn,
 
 	rc = qed_init_run(p_hwfn, p_ptt, PHASE_ENGINE, ANY_PHASE_ID, hw_mode);
 	if (rc)
-		return rc;
+		goto out;
 
 	qed_wr(p_hwfn, p_ptt, PSWRQ2_REG_L2P_VALIDATE_VFID, 0);
 	qed_wr(p_hwfn, p_ptt, PGLUE_B_REG_USE_CLIENTID_IN_TAG, 1);
@@ -2663,7 +2716,7 @@ static int qed_hw_init_common(struct qed_hwfn *p_hwfn,
 	max_num_vfs = QED_IS_AH(cdev) ? MAX_NUM_VFS_K2 : MAX_NUM_VFS_BB;
 	for (vf_id = 0; vf_id < max_num_vfs; vf_id++) {
 		concrete_fid = qed_vfid_to_concrete(p_hwfn, vf_id);
-		qed_fid_pretend(p_hwfn, p_ptt, (u16) concrete_fid);
+		qed_fid_pretend(p_hwfn, p_ptt, (u16)concrete_fid);
 		qed_wr(p_hwfn, p_ptt, CCFC_REG_STRONG_ENABLE_VF, 0x1);
 		qed_wr(p_hwfn, p_ptt, CCFC_REG_WEAK_ENABLE_VF, 0x0);
 		qed_wr(p_hwfn, p_ptt, TCFC_REG_STRONG_ENABLE_VF, 0x1);
@@ -2672,6 +2725,9 @@ static int qed_hw_init_common(struct qed_hwfn *p_hwfn,
 	/* pretend to original PF */
 	qed_fid_pretend(p_hwfn, p_ptt, p_hwfn->rel_pf_id);
 
+out:
+	kfree(params);
+
 	return rc;
 }
 
@@ -2784,7 +2840,7 @@ qed_hw_init_pf_doorbell_bar(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)
 			qed_rdma_dpm_bar(p_hwfn, p_ptt);
 	}
 
-	p_hwfn->wid_count = (u16) n_cpus;
+	p_hwfn->wid_count = (u16)n_cpus;
 
 	DP_INFO(p_hwfn,
 		"doorbell bar: normal_region_size=%d, pwm_region_size=%d, dpi_size=%d, dpi_count=%d, roce_edpm=%s, page_size=%lu\n",
@@ -3503,8 +3559,8 @@ static void qed_hw_hwfn_prepare(struct qed_hwfn *p_hwfn)
 static void get_function_id(struct qed_hwfn *p_hwfn)
 {
 	/* ME Register */
-	p_hwfn->hw_info.opaque_fid = (u16) REG_RD(p_hwfn,
-						  PXP_PF_ME_OPAQUE_ADDR);
+	p_hwfn->hw_info.opaque_fid = (u16)REG_RD(p_hwfn,
+						 PXP_PF_ME_OPAQUE_ADDR);
 
 	p_hwfn->hw_info.concrete_fid = REG_RD(p_hwfn, PXP_PF_ME_CONCRETE_ADDR);
 
@@ -3670,12 +3726,14 @@ u32 qed_get_hsi_def_val(struct qed_dev *cdev, enum qed_hsi_def_type type)
 
 	return qed_hsi_def_val[type][chip_id];
 }
+
 static int
 qed_hw_set_soft_resc_size(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)
 {
 	u32 resc_max_val, mcp_resp;
 	u8 res_id;
 	int rc;
+
 	for (res_id = 0; res_id < QED_MAX_RESC; res_id++) {
 		switch (res_id) {
 		case QED_LL2_RAM_QUEUE:
@@ -3921,7 +3979,7 @@ static int qed_hw_get_resc(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)
 	 * resources allocation queries should be atomic. Since several PFs can
 	 * run in parallel - a resource lock is needed.
 	 * If either the resource lock or resource set value commands are not
-	 * supported - skip the the max values setting, release the lock if
+	 * supported - skip the max values setting, release the lock if
 	 * needed, and proceed to the queries. Other failures, including a
 	 * failure to acquire the lock, will cause this function to fail.
 	 */
@@ -4775,7 +4833,7 @@ int qed_fw_l2_queue(struct qed_hwfn *p_hwfn, u16 src_id, u16 *dst_id)
 	if (src_id >= RESC_NUM(p_hwfn, QED_L2_QUEUE)) {
 		u16 min, max;
 
-		min = (u16) RESC_START(p_hwfn, QED_L2_QUEUE);
+		min = (u16)RESC_START(p_hwfn, QED_L2_QUEUE);
 		max = min + RESC_NUM(p_hwfn, QED_L2_QUEUE);
 		DP_NOTICE(p_hwfn,
 			  "l2_queue id [%d] is not valid, available indices [%d - %d]\n",
* Unmerged path drivers/net/ethernet/qlogic/qed/qed_hsi.h
diff --git a/drivers/net/ethernet/qlogic/qed/qed_init_fw_funcs.c b/drivers/net/ethernet/qlogic/qed/qed_init_fw_funcs.c
index 30c0b5502670..dcec56479117 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_init_fw_funcs.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_init_fw_funcs.c
@@ -919,7 +919,8 @@ int qed_init_vport_wfq(struct qed_hwfn *p_hwfn,
 }
 
 int qed_init_global_rl(struct qed_hwfn *p_hwfn,
-		       struct qed_ptt *p_ptt, u16 rl_id, u32 rate_limit)
+		       struct qed_ptt *p_ptt, u16 rl_id, u32 rate_limit,
+		       enum init_qm_rl_type vport_rl_type)
 {
 	u32 inc_val;
 
@@ -1644,7 +1645,7 @@ struct phys_mem_desc *qed_fw_overlay_mem_alloc(struct qed_hwfn *p_hwfn,
 
 	/* If memory allocation has failed, free all allocated memory */
 	if (buf_offset < buf_size) {
-		qed_fw_overlay_mem_free(p_hwfn, allocated_mem);
+		qed_fw_overlay_mem_free(p_hwfn, &allocated_mem);
 		return NULL;
 	}
 
@@ -1678,16 +1679,16 @@ void qed_fw_overlay_init_ram(struct qed_hwfn *p_hwfn,
 }
 
 void qed_fw_overlay_mem_free(struct qed_hwfn *p_hwfn,
-			     struct phys_mem_desc *fw_overlay_mem)
+			     struct phys_mem_desc **fw_overlay_mem)
 {
 	u8 storm_id;
 
-	if (!fw_overlay_mem)
+	if (!fw_overlay_mem || !(*fw_overlay_mem))
 		return;
 
 	for (storm_id = 0; storm_id < NUM_STORMS; storm_id++) {
 		struct phys_mem_desc *storm_mem_desc =
-		    (struct phys_mem_desc *)fw_overlay_mem + storm_id;
+		    (struct phys_mem_desc *)*fw_overlay_mem + storm_id;
 
 		/* Free Storm's physical memory */
 		if (storm_mem_desc->virt_addr)
@@ -1698,5 +1699,6 @@ void qed_fw_overlay_mem_free(struct qed_hwfn *p_hwfn,
 	}
 
 	/* Free allocated virtual memory */
-	kfree(fw_overlay_mem);
+	kfree(*fw_overlay_mem);
+	*fw_overlay_mem = NULL;
 }
diff --git a/drivers/net/ethernet/qlogic/qed/qed_l2.c b/drivers/net/ethernet/qlogic/qed/qed_l2.c
index ba8c7a31cce1..c3aeb2df0652 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_l2.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_l2.c
@@ -37,7 +37,6 @@
 #include "qed_sp.h"
 #include "qed_sriov.h"
 
-
 #define QED_MAX_SGES_NUM 16
 #define CRC32_POLY 0x1edc6f41
 
@@ -1111,7 +1110,6 @@ qed_eth_pf_tx_queue_start(struct qed_hwfn *p_hwfn,
 {
 	int rc;
 
-
 	rc = qed_eth_txq_start_ramrod(p_hwfn, p_cid,
 				      pbl_addr, pbl_size,
 				      qed_get_cm_pq_idx_mcos(p_hwfn, tc));
@@ -2010,7 +2008,7 @@ qed_configure_rfs_ntuple_filter(struct qed_hwfn *p_hwfn,
 				struct qed_spq_comp_cb *p_cb,
 				struct qed_ntuple_filter_params *p_params)
 {
-	struct rx_update_gft_filter_data *p_ramrod = NULL;
+	struct rx_update_gft_filter_ramrod_data *p_ramrod = NULL;
 	struct qed_spq_entry *p_ent = NULL;
 	struct qed_sp_init_data init_data;
 	u16 abs_rx_q_id = 0;
@@ -2031,7 +2029,7 @@ qed_configure_rfs_ntuple_filter(struct qed_hwfn *p_hwfn,
 	}
 
 	rc = qed_sp_init_request(p_hwfn, &p_ent,
-				 ETH_RAMROD_GFT_UPDATE_FILTER,
+				 ETH_RAMROD_RX_UPDATE_GFT_FILTER,
 				 PROTOCOLID_ETH, &init_data);
 	if (rc)
 		return rc;
diff --git a/drivers/net/ethernet/qlogic/qed/qed_l2.h b/drivers/net/ethernet/qlogic/qed/qed_l2.h
index 8eceeebb1a7b..b187c7e8350c 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_l2.h
+++ b/drivers/net/ethernet/qlogic/qed/qed_l2.h
@@ -146,7 +146,6 @@ struct qed_sp_vport_start_params {
 int qed_sp_eth_vport_start(struct qed_hwfn *p_hwfn,
 			   struct qed_sp_vport_start_params *p_params);
 
-
 struct qed_filter_accept_flags {
 	u8	update_rx_mode_config;
 	u8	update_tx_mode_config;
diff --git a/drivers/net/ethernet/qlogic/qed/qed_sp.h b/drivers/net/ethernet/qlogic/qed/qed_sp.h
index 60ff3222bf55..7d0abc1a34d4 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_sp.h
+++ b/drivers/net/ethernet/qlogic/qed/qed_sp.h
@@ -23,9 +23,9 @@ enum spq_mode {
 };
 
 struct qed_spq_comp_cb {
-	void	(*function)(struct qed_hwfn *,
-			    void *,
-			    union event_ring_data *,
+	void	(*function)(struct qed_hwfn *p_hwfn,
+			    void *cookie,
+			    union event_ring_data *data,
 			    u8 fw_return_code);
 	void	*cookie;
 };
@@ -58,7 +58,7 @@ union ramrod_data {
 	struct tx_queue_stop_ramrod_data tx_queue_stop;
 	struct vport_start_ramrod_data vport_start;
 	struct vport_stop_ramrod_data vport_stop;
-	struct rx_update_gft_filter_data rx_update_gft;
+	struct rx_update_gft_filter_ramrod_data rx_update_gft;
 	struct vport_update_ramrod_data vport_update;
 	struct core_rx_start_ramrod_data core_rx_queue_start;
 	struct core_rx_stop_ramrod_data core_rx_queue_stop;
diff --git a/drivers/net/ethernet/qlogic/qed/qed_sp_commands.c b/drivers/net/ethernet/qlogic/qed/qed_sp_commands.c
index b4ed54ffef9b..648176dfb871 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_sp_commands.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_sp_commands.c
@@ -369,8 +369,12 @@ int qed_sp_pf_start(struct qed_hwfn *p_hwfn,
 		       qed_chain_get_pbl_phys(&p_hwfn->p_eq->chain));
 	page_cnt = (u8)qed_chain_get_page_cnt(&p_hwfn->p_eq->chain);
 	p_ramrod->event_ring_num_pages = page_cnt;
-	DMA_REGPAIR_LE(p_ramrod->consolid_q_pbl_addr,
+
+	/* Place consolidation queue address in ramrod */
+	DMA_REGPAIR_LE(p_ramrod->consolid_q_pbl_base_addr,
 		       qed_chain_get_pbl_phys(&p_hwfn->p_consq->chain));
+	page_cnt = (u8)qed_chain_get_page_cnt(&p_hwfn->p_consq->chain);
+	p_ramrod->consolid_q_num_pages = page_cnt;
 
 	qed_tunn_set_pf_start_params(p_hwfn, p_tunn, &p_ramrod->tunnel_config);
 
@@ -401,8 +405,8 @@ int qed_sp_pf_start(struct qed_hwfn *p_hwfn,
 	if (p_hwfn->cdev->p_iov_info) {
 		struct qed_hw_sriov_info *p_iov = p_hwfn->cdev->p_iov_info;
 
-		p_ramrod->base_vf_id = (u8) p_iov->first_vf_in_pf;
-		p_ramrod->num_vfs = (u8) p_iov->total_vfs;
+		p_ramrod->base_vf_id = (u8)p_iov->first_vf_in_pf;
+		p_ramrod->num_vfs = (u8)p_iov->total_vfs;
 	}
 	p_ramrod->hsi_fp_ver.major_ver_arr[ETH_VER_KEY] = ETH_HSI_VER_MAJOR;
 	p_ramrod->hsi_fp_ver.minor_ver_arr[ETH_VER_KEY] = ETH_HSI_VER_MINOR;
diff --git a/drivers/net/ethernet/qlogic/qed/qed_spq.c b/drivers/net/ethernet/qlogic/qed/qed_spq.c
index fa8385178538..e670ace87aa2 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_spq.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_spq.c
@@ -31,8 +31,8 @@
 #include "qed_rdma.h"
 
 /***************************************************************************
-* Structures & Definitions
-***************************************************************************/
+ * Structures & Definitions
+ ***************************************************************************/
 
 #define SPQ_HIGH_PRI_RESERVE_DEFAULT    (1)
 
@@ -42,8 +42,8 @@
 #define SPQ_BLOCK_SLEEP_MS              (5)
 
 /***************************************************************************
-* Blocking Imp. (BLOCK/EBLOCK mode)
-***************************************************************************/
+ * Blocking Imp. (BLOCK/EBLOCK mode)
+ ***************************************************************************/
 static void qed_spq_blocking_cb(struct qed_hwfn *p_hwfn,
 				void *cookie,
 				union event_ring_data *data, u8 fw_return_code)
@@ -149,8 +149,8 @@ static int qed_spq_block(struct qed_hwfn *p_hwfn,
 }
 
 /***************************************************************************
-* SPQ entries inner API
-***************************************************************************/
+ * SPQ entries inner API
+ ***************************************************************************/
 static int qed_spq_fill_entry(struct qed_hwfn *p_hwfn,
 			      struct qed_spq_entry *p_ent)
 {
@@ -184,8 +184,8 @@ static int qed_spq_fill_entry(struct qed_hwfn *p_hwfn,
 }
 
 /***************************************************************************
-* HSI access
-***************************************************************************/
+ * HSI access
+ ***************************************************************************/
 static void qed_spq_hw_initialize(struct qed_hwfn *p_hwfn,
 				  struct qed_spq *p_spq)
 {
@@ -217,13 +217,10 @@ static void qed_spq_hw_initialize(struct qed_hwfn *p_hwfn,
 	physical_q = qed_get_cm_pq_idx(p_hwfn, PQ_FLAGS_LB);
 	p_cxt->xstorm_ag_context.physical_q0 = cpu_to_le16(physical_q);
 
-	p_cxt->xstorm_st_context.spq_base_lo =
+	p_cxt->xstorm_st_context.spq_base_addr.lo =
 		DMA_LO_LE(p_spq->chain.p_phys_addr);
-	p_cxt->xstorm_st_context.spq_base_hi =
+	p_cxt->xstorm_st_context.spq_base_addr.hi =
 		DMA_HI_LE(p_spq->chain.p_phys_addr);
-
-	DMA_REGPAIR_LE(p_cxt->xstorm_st_context.consolid_base_addr,
-		       p_hwfn->p_consq->chain.p_phys_addr);
 }
 
 static int qed_spq_hw_post(struct qed_hwfn *p_hwfn,
@@ -265,8 +262,8 @@ static int qed_spq_hw_post(struct qed_hwfn *p_hwfn,
 }
 
 /***************************************************************************
-* Asynchronous events
-***************************************************************************/
+ * Asynchronous events
+ ***************************************************************************/
 static int
 qed_async_event_completion(struct qed_hwfn *p_hwfn,
 			   struct event_ring_entry *p_eqe)
@@ -311,8 +308,8 @@ qed_spq_unregister_async_cb(struct qed_hwfn *p_hwfn,
 }
 
 /***************************************************************************
-* EQ API
-***************************************************************************/
+ * EQ API
+ ***************************************************************************/
 void qed_eq_prod_update(struct qed_hwfn *p_hwfn, u16 prod)
 {
 	u32 addr = GTT_BAR0_MAP_REG_USDM_RAM +
@@ -433,8 +430,8 @@ void qed_eq_free(struct qed_hwfn *p_hwfn)
 }
 
 /***************************************************************************
-* CQE API - manipulate EQ functionality
-***************************************************************************/
+ * CQE API - manipulate EQ functionality
+ ***************************************************************************/
 static int qed_cqe_completion(struct qed_hwfn *p_hwfn,
 			      struct eth_slow_path_rx_cqe *cqe,
 			      enum protocol_type protocol)
@@ -464,8 +461,8 @@ int qed_eth_cqe_completion(struct qed_hwfn *p_hwfn,
 }
 
 /***************************************************************************
-* Slow hwfn Queue (spq)
-***************************************************************************/
+ * Slow hwfn Queue (spq)
+ ***************************************************************************/
 void qed_spq_setup(struct qed_hwfn *p_hwfn)
 {
 	struct qed_spq *p_spq = p_hwfn->p_spq;
@@ -548,7 +545,7 @@ int qed_spq_alloc(struct qed_hwfn *p_hwfn)
 	int ret;
 
 	/* SPQ struct */
-	p_spq = kzalloc(sizeof(struct qed_spq), GFP_KERNEL);
+	p_spq = kzalloc(sizeof(*p_spq), GFP_KERNEL);
 	if (!p_spq)
 		return -ENOMEM;
 
@@ -676,7 +673,6 @@ static int qed_spq_add_entry(struct qed_hwfn *p_hwfn,
 	struct qed_spq *p_spq = p_hwfn->p_spq;
 
 	if (p_ent->queue == &p_spq->unlimited_pending) {
-
 		if (list_empty(&p_spq->free_pool)) {
 			list_add_tail(&p_ent->list, &p_spq->unlimited_pending);
 			p_spq->unlimited_pending_count++;
@@ -725,8 +721,8 @@ static int qed_spq_add_entry(struct qed_hwfn *p_hwfn,
 }
 
 /***************************************************************************
-* Accessor
-***************************************************************************/
+ * Accessor
+ ***************************************************************************/
 u32 qed_spq_get_cid(struct qed_hwfn *p_hwfn)
 {
 	if (!p_hwfn->p_spq)
@@ -735,8 +731,8 @@ u32 qed_spq_get_cid(struct qed_hwfn *p_hwfn)
 }
 
 /***************************************************************************
-* Posting new Ramrods
-***************************************************************************/
+ * Posting new Ramrods
+ ***************************************************************************/
 static int qed_spq_post_list(struct qed_hwfn *p_hwfn,
 			     struct list_head *head, u32 keep_reserve)
 {
diff --git a/drivers/net/ethernet/qlogic/qed/qed_sriov.c b/drivers/net/ethernet/qlogic/qed/qed_sriov.c
index 08d92711c7a2..7347f6a6053c 100644
--- a/drivers/net/ethernet/qlogic/qed/qed_sriov.c
+++ b/drivers/net/ethernet/qlogic/qed/qed_sriov.c
@@ -19,12 +19,13 @@
 #include "qed_sp.h"
 #include "qed_sriov.h"
 #include "qed_vf.h"
-static int qed_sriov_eqe_event(struct qed_hwfn *p_hwfn,
-			       u8 opcode,
-			       __le16 echo,
-			       union event_ring_data *data, u8 fw_return_code);
 static int qed_iov_bulletin_set_mac(struct qed_hwfn *p_hwfn, u8 *mac, int vfid);
 
+static u16 qed_vf_from_entity_id(__le16 entity_id)
+{
+	return le16_to_cpu(entity_id) - MAX_NUM_PFS;
+}
+
 static u8 qed_vf_calculate_legacy(struct qed_vf_info *p_vf)
 {
 	u8 legacy = 0;
@@ -169,8 +170,8 @@ static struct qed_vf_info *qed_iov_get_vf_info(struct qed_hwfn *p_hwfn,
 				  b_enabled_only, false))
 		vf = &p_hwfn->pf_iov_info->vfs_array[relative_vf_id];
 	else
-		DP_ERR(p_hwfn, "qed_iov_get_vf_info: VF[%d] is not enabled\n",
-		       relative_vf_id);
+		DP_ERR(p_hwfn, "%s: VF[%d] is not enabled\n",
+		       __func__, relative_vf_id);
 
 	return vf;
 }
@@ -308,7 +309,7 @@ static int qed_iov_post_vf_bulletin(struct qed_hwfn *p_hwfn,
 	struct qed_dmae_params params;
 	struct qed_vf_info *p_vf;
 
-	p_vf = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	p_vf = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!p_vf)
 		return -EINVAL;
 
@@ -420,7 +421,7 @@ static void qed_iov_setup_vfdb(struct qed_hwfn *p_hwfn)
 	bulletin_p = p_iov_info->bulletins_phys;
 	if (!p_req_virt_addr || !p_reply_virt_addr || !p_bulletin_virt) {
 		DP_ERR(p_hwfn,
-		       "qed_iov_setup_vfdb called without allocating mem first\n");
+		       "%s called without allocating mem first\n", __func__);
 		return;
 	}
 
@@ -464,7 +465,7 @@ static int qed_iov_allocate_vfdb(struct qed_hwfn *p_hwfn)
 	num_vfs = p_hwfn->cdev->p_iov_info->total_vfs;
 
 	DP_VERBOSE(p_hwfn, QED_MSG_IOV,
-		   "qed_iov_allocate_vfdb for %d VFs\n", num_vfs);
+		   "%s for %d VFs\n", __func__, num_vfs);
 
 	/* Allocate PF Mailbox buffer (per-VF) */
 	p_iov_info->mbx_msg_size = sizeof(union vfpf_tlvs) * num_vfs;
@@ -500,10 +501,10 @@ static int qed_iov_allocate_vfdb(struct qed_hwfn *p_hwfn)
 		   QED_MSG_IOV,
 		   "PF's Requests mailbox [%p virt 0x%llx phys],  Response mailbox [%p virt 0x%llx phys] Bulletins [%p virt 0x%llx phys]\n",
 		   p_iov_info->mbx_msg_virt_addr,
-		   (u64) p_iov_info->mbx_msg_phys_addr,
+		   (u64)p_iov_info->mbx_msg_phys_addr,
 		   p_iov_info->mbx_reply_virt_addr,
-		   (u64) p_iov_info->mbx_reply_phys_addr,
-		   p_iov_info->p_bulletins, (u64) p_iov_info->bulletins_phys);
+		   (u64)p_iov_info->mbx_reply_phys_addr,
+		   p_iov_info->p_bulletins, (u64)p_iov_info->bulletins_phys);
 
 	return 0;
 }
@@ -608,7 +609,7 @@ int qed_iov_hw_info(struct qed_hwfn *p_hwfn)
 	if (rc)
 		return rc;
 
-	/* We want PF IOV to be synonemous with the existance of p_iov_info;
+	/* We want PF IOV to be synonemous with the existence of p_iov_info;
 	 * In case the capability is published but there are no VFs, simply
 	 * de-allocate the struct.
 	 */
@@ -714,12 +715,12 @@ static void qed_iov_vf_igu_reset(struct qed_hwfn *p_hwfn,
 	int i;
 
 	/* Set VF masks and configuration - pretend */
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) vf->concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)vf->concrete_fid);
 
 	qed_wr(p_hwfn, p_ptt, IGU_REG_STATISTIC_NUM_VF_MSG_SENT, 0);
 
 	/* unpretend */
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) p_hwfn->hw_info.concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)p_hwfn->hw_info.concrete_fid);
 
 	/* iterate over all queues, clear sb consumer */
 	for (i = 0; i < vf->num_sbs; i++)
@@ -734,7 +735,7 @@ static void qed_iov_vf_igu_set_int(struct qed_hwfn *p_hwfn,
 {
 	u32 igu_vf_conf;
 
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) vf->concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)vf->concrete_fid);
 
 	igu_vf_conf = qed_rd(p_hwfn, p_ptt, IGU_REG_VF_CONFIGURATION);
 
@@ -746,7 +747,7 @@ static void qed_iov_vf_igu_set_int(struct qed_hwfn *p_hwfn,
 	qed_wr(p_hwfn, p_ptt, IGU_REG_VF_CONFIGURATION, igu_vf_conf);
 
 	/* unpretend */
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) p_hwfn->hw_info.concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)p_hwfn->hw_info.concrete_fid);
 }
 
 static int
@@ -807,7 +808,7 @@ static int qed_iov_enable_vf_access(struct qed_hwfn *p_hwfn,
 	if (rc)
 		return rc;
 
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) vf->concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)vf->concrete_fid);
 
 	SET_FIELD(igu_vf_conf, IGU_VF_CONF_PARENT, p_hwfn->rel_pf_id);
 	STORE_RT_REG(p_hwfn, IGU_REG_VF_CONFIGURATION_RT_OFFSET, igu_vf_conf);
@@ -816,7 +817,7 @@ static int qed_iov_enable_vf_access(struct qed_hwfn *p_hwfn,
 		     p_hwfn->hw_info.hw_mode);
 
 	/* unpretend */
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) p_hwfn->hw_info.concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)p_hwfn->hw_info.concrete_fid);
 
 	vf->state = VF_FREE;
 
@@ -904,7 +905,7 @@ static u8 qed_iov_alloc_vf_igu_sbs(struct qed_hwfn *p_hwfn,
 				  p_block->igu_sb_id * sizeof(u64), 2, NULL);
 	}
 
-	vf->num_sbs = (u8) num_rx_queues;
+	vf->num_sbs = (u8)num_rx_queues;
 
 	return vf->num_sbs;
 }
@@ -988,7 +989,7 @@ static int qed_iov_init_hw_for_vf(struct qed_hwfn *p_hwfn,
 
 	vf = qed_iov_get_vf_info(p_hwfn, p_params->rel_vf_id, false);
 	if (!vf) {
-		DP_ERR(p_hwfn, "qed_iov_init_hw_for_vf : vf is NULL\n");
+		DP_ERR(p_hwfn, "%s : vf is NULL\n", __func__);
 		return -EINVAL;
 	}
 
@@ -1092,7 +1093,7 @@ static int qed_iov_release_hw_for_vf(struct qed_hwfn *p_hwfn,
 
 	vf = qed_iov_get_vf_info(p_hwfn, rel_vf_id, true);
 	if (!vf) {
-		DP_ERR(p_hwfn, "qed_iov_release_hw_for_vf : vf is NULL\n");
+		DP_ERR(p_hwfn, "%s : vf is NULL\n", __func__);
 		return -EINVAL;
 	}
 
@@ -1545,7 +1546,7 @@ static void qed_iov_vf_mbx_acquire(struct qed_hwfn *p_hwfn,
 	memset(resp, 0, sizeof(*resp));
 
 	/* Write the PF version so that VF would know which version
-	 * is supported - might be later overriden. This guarantees that
+	 * is supported - might be later overridden. This guarantees that
 	 * VF could recognize legacy PF based on lack of versions in reply.
 	 */
 	pfdev_info->major_fp_hsi = ETH_HSI_VER_MAJOR;
@@ -1897,7 +1898,7 @@ static void qed_iov_vf_mbx_start_vport(struct qed_hwfn *p_hwfn,
 	int sb_id;
 	int rc;
 
-	vf_info = qed_iov_get_vf_info(p_hwfn, (u16) vf->relative_vf_id, true);
+	vf_info = qed_iov_get_vf_info(p_hwfn, (u16)vf->relative_vf_id, true);
 	if (!vf_info) {
 		DP_NOTICE(p_hwfn->cdev,
 			  "Failed to get VF info, invalid vfid [%d]\n",
@@ -1957,7 +1958,7 @@ static void qed_iov_vf_mbx_start_vport(struct qed_hwfn *p_hwfn,
 	rc = qed_sp_eth_vport_start(p_hwfn, &params);
 	if (rc) {
 		DP_ERR(p_hwfn,
-		       "qed_iov_vf_mbx_start_vport returned error %d\n", rc);
+		       "%s returned error %d\n", __func__, rc);
 		status = PFVF_STATUS_FAILURE;
 	} else {
 		vf->vport_instance++;
@@ -1993,8 +1994,8 @@ static void qed_iov_vf_mbx_stop_vport(struct qed_hwfn *p_hwfn,
 
 	rc = qed_sp_vport_stop(p_hwfn, vf->opaque_fid, vf->vport_id);
 	if (rc) {
-		DP_ERR(p_hwfn, "qed_iov_vf_mbx_stop_vport returned error %d\n",
-		       rc);
+		DP_ERR(p_hwfn, "%s returned error %d\n",
+		       __func__, rc);
 		status = PFVF_STATUS_FAILURE;
 	}
 
@@ -3030,7 +3031,7 @@ static void qed_iov_vf_mbx_vport_update(struct qed_hwfn *p_hwfn,
 		goto out;
 	}
 	p_rss_params = vzalloc(sizeof(*p_rss_params));
-	if (p_rss_params == NULL) {
+	if (!p_rss_params) {
 		status = PFVF_STATUS_FAILURE;
 		goto out;
 	}
@@ -3550,6 +3551,7 @@ static void qed_iov_vf_pf_set_coalesce(struct qed_hwfn *p_hwfn,
 	qed_iov_prepare_resp(p_hwfn, p_ptt, vf, CHANNEL_TLV_COALESCE_UPDATE,
 			     sizeof(struct pfvf_def_resp_tlv), status);
 }
+
 static int
 qed_iov_vf_flr_poll_dorq(struct qed_hwfn *p_hwfn,
 			 struct qed_vf_info *p_vf, struct qed_ptt *p_ptt)
@@ -3557,7 +3559,7 @@ qed_iov_vf_flr_poll_dorq(struct qed_hwfn *p_hwfn,
 	int cnt;
 	u32 val;
 
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) p_vf->concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)p_vf->concrete_fid);
 
 	for (cnt = 0; cnt < 50; cnt++) {
 		val = qed_rd(p_hwfn, p_ptt, DORQ_REG_VF_USAGE_CNT);
@@ -3565,7 +3567,7 @@ qed_iov_vf_flr_poll_dorq(struct qed_hwfn *p_hwfn,
 			break;
 		msleep(20);
 	}
-	qed_fid_pretend(p_hwfn, p_ptt, (u16) p_hwfn->hw_info.concrete_fid);
+	qed_fid_pretend(p_hwfn, p_ptt, (u16)p_hwfn->hw_info.concrete_fid);
 
 	if (cnt == 50) {
 		DP_ERR(p_hwfn,
@@ -3842,7 +3844,7 @@ static void qed_iov_process_mbx_req(struct qed_hwfn *p_hwfn,
 	struct qed_iov_vf_mbx *mbx;
 	struct qed_vf_info *p_vf;
 
-	p_vf = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	p_vf = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!p_vf)
 		return;
 
@@ -3979,7 +3981,7 @@ static void qed_iov_pf_get_pending_events(struct qed_hwfn *p_hwfn, u64 *events)
 static struct qed_vf_info *qed_sriov_get_vf_from_absid(struct qed_hwfn *p_hwfn,
 						       u16 abs_vfid)
 {
-	u8 min = (u8) p_hwfn->cdev->p_iov_info->first_vf_in_pf;
+	u8 min = (u8)p_hwfn->cdev->p_iov_info->first_vf_in_pf;
 
 	if (!_qed_iov_pf_sanity_check(p_hwfn, (int)abs_vfid - min, false)) {
 		DP_VERBOSE(p_hwfn,
@@ -3989,7 +3991,7 @@ static struct qed_vf_info *qed_sriov_get_vf_from_absid(struct qed_hwfn *p_hwfn,
 		return NULL;
 	}
 
-	return &p_hwfn->pf_iov_info->vfs_array[(u8) abs_vfid - min];
+	return &p_hwfn->pf_iov_info->vfs_array[(u8)abs_vfid - min];
 }
 
 static int qed_sriov_vfpf_msg(struct qed_hwfn *p_hwfn,
@@ -4013,13 +4015,13 @@ static int qed_sriov_vfpf_msg(struct qed_hwfn *p_hwfn,
 	return 0;
 }
 
-static void qed_sriov_vfpf_malicious(struct qed_hwfn *p_hwfn,
-				     struct malicious_vf_eqe_data *p_data)
+void qed_sriov_vfpf_malicious(struct qed_hwfn *p_hwfn,
+			      struct fw_err_data *p_data)
 {
 	struct qed_vf_info *p_vf;
 
-	p_vf = qed_sriov_get_vf_from_absid(p_hwfn, p_data->vf_id);
-
+	p_vf = qed_sriov_get_vf_from_absid(p_hwfn, qed_vf_from_entity_id
+					   (p_data->entity_id));
 	if (!p_vf)
 		return;
 
@@ -4036,16 +4038,13 @@ static void qed_sriov_vfpf_malicious(struct qed_hwfn *p_hwfn,
 	}
 }
 
-static int qed_sriov_eqe_event(struct qed_hwfn *p_hwfn, u8 opcode, __le16 echo,
-			       union event_ring_data *data, u8 fw_return_code)
+int qed_sriov_eqe_event(struct qed_hwfn *p_hwfn, u8 opcode, __le16 echo,
+			union event_ring_data *data, u8 fw_return_code)
 {
 	switch (opcode) {
 	case COMMON_EVENT_VF_PF_CHANNEL:
 		return qed_sriov_vfpf_msg(p_hwfn, le16_to_cpu(echo),
 					  &data->vf_pf_channel.msg_addr);
-	case COMMON_EVENT_MALICIOUS_VF:
-		qed_sriov_vfpf_malicious(p_hwfn, &data->malicious_vf);
-		return 0;
 	default:
 		DP_INFO(p_hwfn->cdev, "Unknown sriov eqe event 0x%02x\n",
 			opcode);
@@ -4075,7 +4074,7 @@ static int qed_iov_copy_vf_msg(struct qed_hwfn *p_hwfn, struct qed_ptt *ptt,
 	struct qed_dmae_params params;
 	struct qed_vf_info *vf_info;
 
-	vf_info = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	vf_info = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!vf_info)
 		return -EINVAL;
 
@@ -4176,7 +4175,7 @@ static void qed_iov_bulletin_set_forced_vlan(struct qed_hwfn *p_hwfn,
 	struct qed_vf_info *vf_info;
 	u64 feature;
 
-	vf_info = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	vf_info = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!vf_info) {
 		DP_NOTICE(p_hwfn->cdev,
 			  "Can not set forced MAC, invalid vfid [%d]\n", vfid);
@@ -4226,7 +4225,7 @@ static bool qed_iov_vf_has_vport_instance(struct qed_hwfn *p_hwfn, int vfid)
 {
 	struct qed_vf_info *p_vf_info;
 
-	p_vf_info = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	p_vf_info = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!p_vf_info)
 		return false;
 
@@ -4237,7 +4236,7 @@ static bool qed_iov_is_vf_stopped(struct qed_hwfn *p_hwfn, int vfid)
 {
 	struct qed_vf_info *p_vf_info;
 
-	p_vf_info = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	p_vf_info = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!p_vf_info)
 		return true;
 
@@ -4248,7 +4247,7 @@ static bool qed_iov_spoofchk_get(struct qed_hwfn *p_hwfn, int vfid)
 {
 	struct qed_vf_info *vf_info;
 
-	vf_info = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	vf_info = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!vf_info)
 		return false;
 
@@ -4266,7 +4265,7 @@ static int qed_iov_spoofchk_set(struct qed_hwfn *p_hwfn, int vfid, bool val)
 		goto out;
 	}
 
-	vf = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	vf = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!vf)
 		goto out;
 
@@ -4345,7 +4344,8 @@ static int qed_iov_configure_tx_rate(struct qed_hwfn *p_hwfn,
 		return rc;
 
 	rl_id = abs_vp_id;	/* The "rl_id" is set as the "vport_id" */
-	return qed_init_global_rl(p_hwfn, p_ptt, rl_id, (u32)val);
+	return qed_init_global_rl(p_hwfn, p_ptt, rl_id, (u32)val,
+				  QM_RL_TYPE_NORMAL);
 }
 
 static int
@@ -4376,7 +4376,7 @@ static int qed_iov_get_vf_min_rate(struct qed_hwfn *p_hwfn, int vfid)
 	struct qed_wfq_data *vf_vp_wfq;
 	struct qed_vf_info *vf_info;
 
-	vf_info = qed_iov_get_vf_info(p_hwfn, (u16) vfid, true);
+	vf_info = qed_iov_get_vf_info(p_hwfn, (u16)vfid, true);
 	if (!vf_info)
 		return 0;
 
@@ -4395,8 +4395,10 @@ static int qed_iov_get_vf_min_rate(struct qed_hwfn *p_hwfn, int vfid)
  */
 void qed_schedule_iov(struct qed_hwfn *hwfn, enum qed_iov_wq_flag flag)
 {
+	/* Memory barrier for setting atomic bit */
 	smp_mb__before_atomic();
 	set_bit(flag, &hwfn->iov_task_flags);
+	/* Memory barrier after setting atomic bit */
 	smp_mb__after_atomic();
 	DP_VERBOSE(hwfn, QED_MSG_IOV, "Scheduling iov task [Flag: %d]\n", flag);
 	queue_delayed_work(hwfn->iov_wq, &hwfn->iov_task, 0);
@@ -4407,8 +4409,8 @@ void qed_vf_start_iov_wq(struct qed_dev *cdev)
 	int i;
 
 	for_each_hwfn(cdev, i)
-	    queue_delayed_work(cdev->hwfns[i].iov_wq,
-			       &cdev->hwfns[i].iov_task, 0);
+		queue_delayed_work(cdev->hwfns[i].iov_wq,
+				   &cdev->hwfns[i].iov_task, 0);
 }
 
 int qed_sriov_disable(struct qed_dev *cdev, bool pci_enabled)
@@ -4416,8 +4418,8 @@ int qed_sriov_disable(struct qed_dev *cdev, bool pci_enabled)
 	int i, j;
 
 	for_each_hwfn(cdev, i)
-	    if (cdev->hwfns[i].iov_wq)
-		flush_workqueue(cdev->hwfns[i].iov_wq);
+		if (cdev->hwfns[i].iov_wq)
+			flush_workqueue(cdev->hwfns[i].iov_wq);
 
 	/* Mark VFs for disablement */
 	qed_iov_set_vfs_to_disable(cdev, true);
@@ -5010,7 +5012,7 @@ static void qed_handle_bulletin_post(struct qed_hwfn *hwfn)
 	}
 
 	qed_for_each_vf(hwfn, i)
-	    qed_iov_post_vf_bulletin(hwfn, i, ptt);
+		qed_iov_post_vf_bulletin(hwfn, i, ptt);
 
 	qed_ptt_release(hwfn, ptt);
 }
* Unmerged path drivers/net/ethernet/qlogic/qed/qed_sriov.h
diff --git a/include/linux/qed/eth_common.h b/include/linux/qed/eth_common.h
index cd1207ad4ada..c84e08bc6802 100644
--- a/include/linux/qed/eth_common.h
+++ b/include/linux/qed/eth_common.h
@@ -67,6 +67,7 @@
 /* Ethernet vport update constants */
 #define ETH_FILTER_RULES_COUNT		10
 #define ETH_RSS_IND_TABLE_ENTRIES_NUM	128
+#define ETH_RSS_IND_TABLE_MASK_SIZE_REGS    (ETH_RSS_IND_TABLE_ENTRIES_NUM / 32)
 #define ETH_RSS_KEY_SIZE_REGS		10
 #define ETH_RSS_ENGINE_NUM_K2		207
 #define ETH_RSS_ENGINE_NUM_BB		127
diff --git a/include/linux/qed/rdma_common.h b/include/linux/qed/rdma_common.h
index bab078b25834..6dfed163ab6c 100644
--- a/include/linux/qed/rdma_common.h
+++ b/include/linux/qed/rdma_common.h
@@ -27,6 +27,7 @@
 #define RDMA_MAX_PDS			(64 * 1024)
 #define RDMA_MAX_XRC_SRQS                       (1024)
 #define RDMA_MAX_SRQS                           (32 * 1024)
+#define RDMA_MAX_IRQ_ELEMS_IN_PAGE      (128)
 
 #define RDMA_NUM_STATISTIC_COUNTERS	MAX_NUM_VPORTS
 #define RDMA_NUM_STATISTIC_COUNTERS_K2	MAX_NUM_VPORTS_K2
