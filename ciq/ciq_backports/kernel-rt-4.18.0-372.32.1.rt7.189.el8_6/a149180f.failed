x86: Add magic AMD return-thunk

jira LE-1907
cve CVE-2022-23825
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-rt-4.18.0-372.32.1.rt7.189.el8_6
commit-author Peter Zijlstra <peterz@infradead.org>
commit a149180fbcf336e97ce4eb2cdc13672727feb94d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-372.32.1.rt7.189.el8_6/a149180f.failed

Note: needs to be in a section distinct from Retpolines such that the
Retpoline RET substitution cannot possibly use immediate jumps.

ORC unwinding for zen_untrain_ret() and __x86_return_thunk() is a
little tricky but works due to the fact that zen_untrain_ret() doesn't
have any stack ops and as such will emit a single ORC entry at the
start (+0x3f).

Meanwhile, unwinding an IP, including the __x86_return_thunk() one
(+0x40) will search for the largest ORC entry smaller or equal to the
IP, these will find the one ORC entry (+0x3f) and all works.

  [ Alexandre: SVM part. ]
  [ bp: Build fix, massages. ]

	Suggested-by: Andrew Cooper <Andrew.Cooper3@citrix.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit a149180fbcf336e97ce4eb2cdc13672727feb94d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/entry/entry_64.S
#	arch/x86/entry/entry_64_compat.S
#	arch/x86/include/asm/cpufeatures.h
#	arch/x86/include/asm/disabled-features.h
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/lib/retpoline.S
#	tools/objtool/check.c
diff --cc arch/x86/entry/entry_64.S
index f6cd4f44ad1c,1f4b18c8909b..000000000000
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@@ -121,6 -95,8 +121,11 @@@ SYM_CODE_START(entry_SYSCALL_64
  	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
  
  SYM_INNER_LABEL(entry_SYSCALL_64_safe_stack, SYM_L_GLOBAL)
++<<<<<<< HEAD
++=======
+ 	ANNOTATE_NOENDBR
+ 	UNTRAIN_RET
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  	/* Construct struct pt_regs on stack */
  	pushq	$__USER_DS				/* pt_regs->ss */
@@@ -799,8 -716,9 +804,9 @@@ native_irq_return_ldt
  	 */
  
  	pushq	%rdi				/* Stash user RDI */
 -	swapgs					/* to kernel GS */
 +	SWAPGS					/* to kernel GS */
  	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi	/* to kernel CR3 */
+ 	UNTRAIN_RET
  
  	movq	PER_CPU_VAR(espfix_waddr), %rdi
  	movq	%rax, (0*8)(%rdi)		/* user RAX */
@@@ -1393,31 -1023,13 +1400,35 @@@ ENTRY(error_entry
  	FENCE_SWAPGS_USER_ENTRY
  	/* We have user CR3.  Change to kernel CR3. */
  	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax
++<<<<<<< HEAD
 +	IBRS_ENTRY_CLOBBER
++=======
+ 	UNTRAIN_RET
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
 -	leaq	8(%rsp), %rdi			/* arg0 = pt_regs pointer */
  .Lerror_entry_from_usermode_after_swapgs:
  	/* Put us onto the real thread stack. */
 +	popq	%r12				/* save return addr in %12 */
 +	movq	%rsp, %rdi			/* arg0 = pt_regs pointer */
  	call	sync_regs
 -	RET
 +	movq	%rax, %rsp			/* switch stack */
 +	ENCODE_FRAME_POINTER
 +	pushq	%r12
 +
 +	/*
 +	 * We need to tell lockdep that IRQs are off.  We can't do this until
 +	 * we fix gsbase, and we should do it before enter_from_user_mode
 +	 * (which can take locks).
 +	 */
 +	TRACE_IRQS_OFF
 +	CALL_enter_from_user_mode
 +	ret
 +
 +.Lerror_entry_done_lfence:
 +	FENCE_SWAPGS_KERNEL_ENTRY
 +.Lerror_entry_done:
 +	TRACE_IRQS_OFF
 +	ret
  
  	/*
  	 * There are two places in the kernel that can potentially fault with
@@@ -1456,10 -1073,10 +1467,14 @@@
  	 * We came from an IRET to user mode, so we have user
  	 * gsbase and CR3.  Switch to kernel gsbase and CR3:
  	 */
 -	swapgs
 +	SWAPGS
  	FENCE_SWAPGS_USER_ENTRY
  	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax
++<<<<<<< HEAD
 +	IBRS_ENTRY
++=======
+ 	UNTRAIN_RET
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  	/*
  	 * Pretend that the exception came from user mode: set up pt_regs
diff --cc arch/x86/entry/entry_64_compat.S
index 8ea9c49ab0f3,03d74c5153fb..000000000000
--- a/arch/x86/entry/entry_64_compat.S
+++ b/arch/x86/entry/entry_64_compat.S
@@@ -14,7 -14,7 +14,11 @@@
  #include <asm/irqflags.h>
  #include <asm/asm.h>
  #include <asm/smap.h>
++<<<<<<< HEAD
 +#include <asm/spec_ctrl.h>
++=======
+ #include <asm/nospec-branch.h>
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  #include <linux/linkage.h>
  #include <linux/err.h>
  
@@@ -56,7 -59,21 +60,25 @@@ SYM_FUNC_START(entry_SYSENTER_compat
  
  	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
  
++<<<<<<< HEAD
 +	IBRS_ENTRY
++=======
+ 	/* Construct struct pt_regs on stack */
+ 	pushq	$__USER32_DS		/* pt_regs->ss */
+ 	pushq	$0			/* pt_regs->sp = 0 (placeholder) */
+ 
+ 	/*
+ 	 * Push flags.  This is nasty.  First, interrupts are currently
+ 	 * off, but we need pt_regs->flags to have IF set.  Second, if TS
+ 	 * was set in usermode, it's still set, and we're singlestepping
+ 	 * through this code.  do_SYSENTER_32() will fix up IF.
+ 	 */
+ 	pushfq				/* pt_regs->flags (except IF = 0) */
+ 	pushq	$__USER32_CS		/* pt_regs->cs */
+ 	pushq	$0			/* pt_regs->ip = 0 (placeholder) */
+ SYM_INNER_LABEL(entry_SYSENTER_compat_after_hwframe, SYM_L_GLOBAL)
+ 	UNTRAIN_RET
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  	/*
  	 * User tracing code (ptrace or signal handlers) might assume that
@@@ -212,7 -190,9 +234,13 @@@ SYM_CODE_START(entry_SYSCALL_compat
  	/* Switch to the kernel stack */
  	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
  
++<<<<<<< HEAD
 +	IBRS_ENTRY
++=======
+ SYM_INNER_LABEL(entry_SYSCALL_compat_safe_stack, SYM_L_GLOBAL)
+ 	ANNOTATE_NOENDBR
+ 	UNTRAIN_RET
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  	/* Construct struct pt_regs on stack */
  	pushq	$__USER32_DS		/* pt_regs->ss */
@@@ -358,63 -317,31 +386,69 @@@ SYM_CODE_START(entry_INT80_compat
  
  	/* switch to thread stack expects orig_ax and rdi to be pushed */
  	pushq	%rax			/* pt_regs->orig_ax */
 +	pushq	%rdi			/* pt_regs->di */
  
  	/* Need to switch before accessing the thread stack. */
 -	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax
 -
 -	/* In the Xen PV case we already run on the thread stack. */
 -	ALTERNATIVE "", "jmp .Lint80_keep_stack", X86_FEATURE_XENPV
 -
 -	movq	%rsp, %rax
 +	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi
 +	movq	%rsp, %rdi
  	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
  
 -	pushq	5*8(%rax)		/* regs->ss */
 -	pushq	4*8(%rax)		/* regs->rsp */
 -	pushq	3*8(%rax)		/* regs->eflags */
 -	pushq	2*8(%rax)		/* regs->cs */
 -	pushq	1*8(%rax)		/* regs->ip */
 -	pushq	0*8(%rax)		/* regs->orig_ax */
 -.Lint80_keep_stack:
 +	IBRS_ENTRY
  
++<<<<<<< HEAD
 +	pushq	6*8(%rdi)		/* regs->ss */
 +	pushq	5*8(%rdi)		/* regs->rsp */
 +	pushq	4*8(%rdi)		/* regs->eflags */
 +	pushq	3*8(%rdi)		/* regs->cs */
 +	pushq	2*8(%rdi)		/* regs->ip */
 +	pushq	1*8(%rdi)		/* regs->orig_ax */
++=======
+ 	UNTRAIN_RET
+ 	PUSH_AND_CLEAR_REGS rax=$-ENOSYS
+ 	UNWIND_HINT_REGS
 -
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
 +
 +	pushq	(%rdi)			/* pt_regs->di */
 +	pushq	%rsi			/* pt_regs->si */
 +	xorl	%esi, %esi		/* nospec   si */
 +	pushq	%rdx			/* pt_regs->dx */
 +	xorl	%edx, %edx		/* nospec   dx */
 +	pushq	%rcx			/* pt_regs->cx */
 +	xorl	%ecx, %ecx		/* nospec   cx */
 +	pushq	$-ENOSYS		/* pt_regs->ax */
 +	pushq   %r8			/* pt_regs->r8 */
 +	xorl	%r8d, %r8d		/* nospec   r8 */
 +	pushq   %r9			/* pt_regs->r9 */
 +	xorl	%r9d, %r9d		/* nospec   r9 */
 +	pushq   %r10			/* pt_regs->r10*/
 +	xorl	%r10d, %r10d		/* nospec   r10 */
 +	pushq   %r11			/* pt_regs->r11 */
 +	xorl	%r11d, %r11d		/* nospec   r11 */
 +	pushq   %rbx                    /* pt_regs->rbx */
 +	xorl	%ebx, %ebx		/* nospec   rbx */
 +	pushq   %rbp                    /* pt_regs->rbp */
 +	xorl	%ebp, %ebp		/* nospec   rbp */
 +	pushq   %r12                    /* pt_regs->r12 */
 +	xorl	%r12d, %r12d		/* nospec   r12 */
 +	pushq   %r13                    /* pt_regs->r13 */
 +	xorl	%r13d, %r13d		/* nospec   r13 */
 +	pushq   %r14                    /* pt_regs->r14 */
 +	xorl	%r14d, %r14d		/* nospec   r14 */
 +	pushq   %r15                    /* pt_regs->r15 */
 +	xorl	%r15d, %r15d		/* nospec   r15 */
  	cld
  
 +	/*
 +	 * User mode is traced as though IRQs are on, and the interrupt
 +	 * gate turned them off.
 +	 */
 +	TRACE_IRQS_OFF
 +
  	movq	%rsp, %rdi
  	call	do_int80_syscall_32
 +.Lsyscall_32_done:
 +
 +	/* Go back to user mode. */
 +	TRACE_IRQS_ON
  	jmp	swapgs_restore_regs_and_return_to_usermode
  SYM_CODE_END(entry_INT80_compat)
diff --cc arch/x86/include/asm/cpufeatures.h
index 46b0bb0c4e55,fa3d0db1470e..000000000000
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -294,6 -300,8 +294,11 @@@
  /* FREE!				(11*32+11) */
  #define X86_FEATURE_RETPOLINE		(11*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
  #define X86_FEATURE_RETPOLINE_LFENCE	(11*32+13) /* "" Use LFENCE for Spectre variant 2 */
++<<<<<<< HEAD
++=======
+ #define X86_FEATURE_RETHUNK		(11*32+14) /* "" Use REturn THUNK */
+ #define X86_FEATURE_UNRET		(11*32+15) /* "" AMD BTB untrain return */
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  /* Intel-defined CPU features, CPUID level 0x00000007:1 (EAX), word 12 */
  #define X86_FEATURE_AVX_VNNI		(12*32+ 4) /* AVX VNNI instructions */
diff --cc arch/x86/include/asm/disabled-features.h
index 8ca9a5833aa4,db75da511a36..000000000000
--- a/arch/x86/include/asm/disabled-features.h
+++ b/arch/x86/include/asm/disabled-features.h
@@@ -56,8 -50,20 +56,25 @@@
  # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
  #endif
  
++<<<<<<< HEAD
 +/* Force disable because it's broken beyond repair */
 +#define DISABLE_ENQCMD		(1 << (X86_FEATURE_ENQCMD & 31))
++=======
+ #ifdef CONFIG_RETPOLINE
+ # define DISABLE_RETPOLINE	0
+ #else
+ # define DISABLE_RETPOLINE	((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETPOLINE_LFENCE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETHUNK & 31)) | \
+ 				 (1 << (X86_FEATURE_UNRET & 31)))
+ #endif
+ 
+ #ifdef CONFIG_INTEL_IOMMU_SVM
+ # define DISABLE_ENQCMD		0
+ #else
+ # define DISABLE_ENQCMD		(1 << (X86_FEATURE_ENQCMD & 31))
+ #endif
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  #ifdef CONFIG_X86_SGX
  # define DISABLE_SGX	0
diff --cc arch/x86/include/asm/nospec-branch.h
index 12722d2e7709,5ca60ae0d14f..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -158,7 -136,19 +174,16 @@@
  	_ASM_PTR " 999b\n\t"					\
  	".popsection\n\t"
  
++<<<<<<< HEAD
++=======
+ typedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];
+ extern retpoline_thunk_t __x86_indirect_thunk_array[];
+ 
+ extern void __x86_return_thunk(void);
+ extern void zen_untrain_ret(void);
+ 
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  #ifdef CONFIG_RETPOLINE
 -
 -#define GEN(reg) \
 -	extern retpoline_thunk_t __x86_indirect_thunk_ ## reg;
 -#include <asm/GEN-for-each-reg.h>
 -#undef GEN
 -
  #ifdef CONFIG_X86_64
  
  /*
diff --cc arch/x86/lib/retpoline.S
index 363ec132df7e,fdd16163b996..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -24,25 -45,94 +24,113 @@@ SYM_FUNC_END(__x86_indirect_thunk_\reg
   * only see one instance of "__x86_indirect_thunk_\reg" rather
   * than one per register with the correct names. So we do it
   * the simple and nasty way...
 - *
 - * Worse, you can only have a single EXPORT_SYMBOL per line,
 - * and CPP can't insert newlines, so we have to repeat everything
 - * at least twice.
   */
 -
 +#define __EXPORT_THUNK(sym) _ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
 +#define EXPORT_THUNK(reg) __EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
 +#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)
 +
++<<<<<<< HEAD
 +GENERATE_THUNK(_ASM_AX)
 +GENERATE_THUNK(_ASM_BX)
 +GENERATE_THUNK(_ASM_CX)
 +GENERATE_THUNK(_ASM_DX)
 +GENERATE_THUNK(_ASM_SI)
 +GENERATE_THUNK(_ASM_DI)
 +GENERATE_THUNK(_ASM_BP)
 +#ifdef CONFIG_64BIT
 +GENERATE_THUNK(r8)
 +GENERATE_THUNK(r9)
 +GENERATE_THUNK(r10)
 +GENERATE_THUNK(r11)
 +GENERATE_THUNK(r12)
 +GENERATE_THUNK(r13)
 +GENERATE_THUNK(r14)
 +GENERATE_THUNK(r15)
 +#endif
++=======
+ #define __EXPORT_THUNK(sym)	_ASM_NOKPROBE(sym); EXPORT_SYMBOL(sym)
+ #define EXPORT_THUNK(reg)	__EXPORT_THUNK(__x86_indirect_thunk_ ## reg)
+ 
+ 	.align RETPOLINE_THUNK_SIZE
+ SYM_CODE_START(__x86_indirect_thunk_array)
+ 
+ #define GEN(reg) THUNK reg
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ 	.align RETPOLINE_THUNK_SIZE
+ SYM_CODE_END(__x86_indirect_thunk_array)
+ 
+ #define GEN(reg) EXPORT_THUNK(reg)
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ /*
+  * This function name is magical and is used by -mfunction-return=thunk-extern
+  * for the compiler to generate JMPs to it.
+  */
+ 	.section .text.__x86.return_thunk
+ 
+ /*
+  * Safety details here pertain to the AMD Zen{1,2} microarchitecture:
+  * 1) The RET at __x86_return_thunk must be on a 64 byte boundary, for
+  *    alignment within the BTB.
+  * 2) The instruction at zen_untrain_ret must contain, and not
+  *    end with, the 0xc3 byte of the RET.
+  * 3) STIBP must be enabled, or SMT disabled, to prevent the sibling thread
+  *    from re-poisioning the BTB prediction.
+  */
+ 	.align 64
+ 	.skip 63, 0xcc
+ SYM_FUNC_START_NOALIGN(zen_untrain_ret);
+ 
+ 	/*
+ 	 * As executed from zen_untrain_ret, this is:
+ 	 *
+ 	 *   TEST $0xcc, %bl
+ 	 *   LFENCE
+ 	 *   JMP __x86_return_thunk
+ 	 *
+ 	 * Executing the TEST instruction has a side effect of evicting any BTB
+ 	 * prediction (potentially attacker controlled) attached to the RET, as
+ 	 * __x86_return_thunk + 1 isn't an instruction boundary at the moment.
+ 	 */
+ 	.byte	0xf6
+ 
+ 	/*
+ 	 * As executed from __x86_return_thunk, this is a plain RET.
+ 	 *
+ 	 * As part of the TEST above, RET is the ModRM byte, and INT3 the imm8.
+ 	 *
+ 	 * We subsequently jump backwards and architecturally execute the RET.
+ 	 * This creates a correct BTB prediction (type=ret), but in the
+ 	 * meantime we suffer Straight Line Speculation (because the type was
+ 	 * no branch) which is halted by the INT3.
+ 	 *
+ 	 * With SMT enabled and STIBP active, a sibling thread cannot poison
+ 	 * RET's prediction to a type of its choice, but can evict the
+ 	 * prediction due to competitive sharing. If the prediction is
+ 	 * evicted, __x86_return_thunk will suffer Straight Line Speculation
+ 	 * which will be contained safely by the INT3.
+ 	 */
+ SYM_INNER_LABEL(__x86_return_thunk, SYM_L_GLOBAL)
+ 	ret
+ 	int3
+ SYM_CODE_END(__x86_return_thunk)
+ 
+ 	/*
+ 	 * Ensure the TEST decoding / BTB invalidation is complete.
+ 	 */
+ 	lfence
+ 
+ 	/*
+ 	 * Jump back and execute the RET in the middle of the TEST instruction.
+ 	 * INT3 is for SLS protection.
+ 	 */
+ 	jmp __x86_return_thunk
+ 	int3
+ SYM_FUNC_END(zen_untrain_ret)
+ __EXPORT_THUNK(zen_untrain_ret)
+ 
+ EXPORT_SYMBOL(__x86_return_thunk)
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
diff --cc tools/objtool/check.c
index 2178cad40b84,4252cd05dfc4..000000000000
--- a/tools/objtool/check.c
+++ b/tools/objtool/check.c
@@@ -552,6 -1125,217 +552,220 @@@ static int add_ignore_alternatives(stru
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ __weak bool arch_is_retpoline(struct symbol *sym)
+ {
+ 	return false;
+ }
+ 
+ __weak bool arch_is_rethunk(struct symbol *sym)
+ {
+ 	return false;
+ }
+ 
+ #define NEGATIVE_RELOC	((void *)-1L)
+ 
+ static struct reloc *insn_reloc(struct objtool_file *file, struct instruction *insn)
+ {
+ 	if (insn->reloc == NEGATIVE_RELOC)
+ 		return NULL;
+ 
+ 	if (!insn->reloc) {
+ 		if (!file)
+ 			return NULL;
+ 
+ 		insn->reloc = find_reloc_by_dest_range(file->elf, insn->sec,
+ 						       insn->offset, insn->len);
+ 		if (!insn->reloc) {
+ 			insn->reloc = NEGATIVE_RELOC;
+ 			return NULL;
+ 		}
+ 	}
+ 
+ 	return insn->reloc;
+ }
+ 
+ static void remove_insn_ops(struct instruction *insn)
+ {
+ 	struct stack_op *op, *tmp;
+ 
+ 	list_for_each_entry_safe(op, tmp, &insn->stack_ops, list) {
+ 		list_del(&op->list);
+ 		free(op);
+ 	}
+ }
+ 
+ static void annotate_call_site(struct objtool_file *file,
+ 			       struct instruction *insn, bool sibling)
+ {
+ 	struct reloc *reloc = insn_reloc(file, insn);
+ 	struct symbol *sym = insn->call_dest;
+ 
+ 	if (!sym)
+ 		sym = reloc->sym;
+ 
+ 	/*
+ 	 * Alternative replacement code is just template code which is
+ 	 * sometimes copied to the original instruction. For now, don't
+ 	 * annotate it. (In the future we might consider annotating the
+ 	 * original instruction if/when it ever makes sense to do so.)
+ 	 */
+ 	if (!strcmp(insn->sec->name, ".altinstr_replacement"))
+ 		return;
+ 
+ 	if (sym->static_call_tramp) {
+ 		list_add_tail(&insn->call_node, &file->static_call_list);
+ 		return;
+ 	}
+ 
+ 	if (sym->retpoline_thunk) {
+ 		list_add_tail(&insn->call_node, &file->retpoline_call_list);
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * Many compilers cannot disable KCOV or sanitizer calls with a function
+ 	 * attribute so they need a little help, NOP out any such calls from
+ 	 * noinstr text.
+ 	 */
+ 	if (opts.hack_noinstr && insn->sec->noinstr && sym->profiling_func) {
+ 		if (reloc) {
+ 			reloc->type = R_NONE;
+ 			elf_write_reloc(file->elf, reloc);
+ 		}
+ 
+ 		elf_write_insn(file->elf, insn->sec,
+ 			       insn->offset, insn->len,
+ 			       sibling ? arch_ret_insn(insn->len)
+ 			               : arch_nop_insn(insn->len));
+ 
+ 		insn->type = sibling ? INSN_RETURN : INSN_NOP;
+ 
+ 		if (sibling) {
+ 			/*
+ 			 * We've replaced the tail-call JMP insn by two new
+ 			 * insn: RET; INT3, except we only have a single struct
+ 			 * insn here. Mark it retpoline_safe to avoid the SLS
+ 			 * warning, instead of adding another insn.
+ 			 */
+ 			insn->retpoline_safe = true;
+ 		}
+ 
+ 		return;
+ 	}
+ 
+ 	if (opts.mcount && sym->fentry) {
+ 		if (sibling)
+ 			WARN_FUNC("Tail call to __fentry__ !?!?", insn->sec, insn->offset);
+ 
+ 		if (reloc) {
+ 			reloc->type = R_NONE;
+ 			elf_write_reloc(file->elf, reloc);
+ 		}
+ 
+ 		elf_write_insn(file->elf, insn->sec,
+ 			       insn->offset, insn->len,
+ 			       arch_nop_insn(insn->len));
+ 
+ 		insn->type = INSN_NOP;
+ 
+ 		list_add_tail(&insn->call_node, &file->mcount_loc_list);
+ 		return;
+ 	}
+ 
+ 	if (!sibling && dead_end_function(file, sym))
+ 		insn->dead_end = true;
+ }
+ 
+ static void add_call_dest(struct objtool_file *file, struct instruction *insn,
+ 			  struct symbol *dest, bool sibling)
+ {
+ 	insn->call_dest = dest;
+ 	if (!dest)
+ 		return;
+ 
+ 	/*
+ 	 * Whatever stack impact regular CALLs have, should be undone
+ 	 * by the RETURN of the called function.
+ 	 *
+ 	 * Annotated intra-function calls retain the stack_ops but
+ 	 * are converted to JUMP, see read_intra_function_calls().
+ 	 */
+ 	remove_insn_ops(insn);
+ 
+ 	annotate_call_site(file, insn, sibling);
+ }
+ 
+ static void add_retpoline_call(struct objtool_file *file, struct instruction *insn)
+ {
+ 	/*
+ 	 * Retpoline calls/jumps are really dynamic calls/jumps in disguise,
+ 	 * so convert them accordingly.
+ 	 */
+ 	switch (insn->type) {
+ 	case INSN_CALL:
+ 		insn->type = INSN_CALL_DYNAMIC;
+ 		break;
+ 	case INSN_JUMP_UNCONDITIONAL:
+ 		insn->type = INSN_JUMP_DYNAMIC;
+ 		break;
+ 	case INSN_JUMP_CONDITIONAL:
+ 		insn->type = INSN_JUMP_DYNAMIC_CONDITIONAL;
+ 		break;
+ 	default:
+ 		return;
+ 	}
+ 
+ 	insn->retpoline_safe = true;
+ 
+ 	/*
+ 	 * Whatever stack impact regular CALLs have, should be undone
+ 	 * by the RETURN of the called function.
+ 	 *
+ 	 * Annotated intra-function calls retain the stack_ops but
+ 	 * are converted to JUMP, see read_intra_function_calls().
+ 	 */
+ 	remove_insn_ops(insn);
+ 
+ 	annotate_call_site(file, insn, false);
+ }
+ 
+ static void add_return_call(struct objtool_file *file, struct instruction *insn, bool add)
+ {
+ 	/*
+ 	 * Return thunk tail calls are really just returns in disguise,
+ 	 * so convert them accordingly.
+ 	 */
+ 	insn->type = INSN_RETURN;
+ 	insn->retpoline_safe = true;
+ 
+ 	if (add)
+ 		list_add_tail(&insn->call_node, &file->return_thunk_list);
+ }
+ 
+ static bool same_function(struct instruction *insn1, struct instruction *insn2)
+ {
+ 	return insn1->func->pfunc == insn2->func->pfunc;
+ }
+ 
+ static bool is_first_func_insn(struct objtool_file *file, struct instruction *insn)
+ {
+ 	if (insn->offset == insn->func->offset)
+ 		return true;
+ 
+ 	if (opts.ibt) {
+ 		struct instruction *prev = prev_insn_same_sym(file, insn);
+ 
+ 		if (prev && prev->type == INSN_ENDBR &&
+ 		    insn->offset == insn->func->offset + prev->len)
+ 			return true;
+ 	}
+ 
+ 	return false;
+ }
+ 
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  /*
   * Find the destination instructions for all jumps.
   */
@@@ -563,49 -1347,61 +777,82 @@@ static int add_jump_destinations(struc
  	unsigned long dest_off;
  
  	for_each_insn(file, insn) {
 -		if (insn->jump_dest) {
 -			/*
 -			 * handle_group_alt() may have previously set
 -			 * 'jump_dest' for some alternatives.
 -			 */
 +		if (insn->type != INSN_JUMP_CONDITIONAL &&
 +		    insn->type != INSN_JUMP_UNCONDITIONAL)
  			continue;
 -		}
 -		if (!is_static_jump(insn))
 +
 +		if (insn->ignore || insn->offset == FAKE_JUMP_OFFSET)
  			continue;
  
 -		reloc = insn_reloc(file, insn);
 -		if (!reloc) {
 +		rela = find_rela_by_dest_range(insn->sec, insn->offset,
 +					       insn->len);
 +		if (!rela) {
  			dest_sec = insn->sec;
++<<<<<<< HEAD
 +			dest_off = insn->offset + insn->len + insn->immediate;
 +		} else if (rela->sym->type == STT_SECTION) {
 +			dest_sec = rela->sym->sec;
 +			dest_off = rela->addend + 4;
 +		} else if (rela->sym->sec->idx) {
 +			dest_sec = rela->sym->sec;
 +			dest_off = rela->sym->sym.st_value + rela->addend + 4;
 +		} else if (strstr(rela->sym->name, "_indirect_thunk_")) {
++=======
+ 			dest_off = arch_jump_destination(insn);
+ 		} else if (reloc->sym->type == STT_SECTION) {
+ 			dest_sec = reloc->sym->sec;
+ 			dest_off = arch_dest_reloc_offset(reloc->addend);
+ 		} else if (reloc->sym->retpoline_thunk) {
+ 			add_retpoline_call(file, insn);
+ 			continue;
+ 		} else if (reloc->sym->return_thunk) {
+ 			add_return_call(file, insn, true);
+ 			continue;
+ 		} else if (insn->func) {
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  			/*
 -			 * External sibling call or internal sibling call with
 -			 * STT_FUNC reloc.
 +			 * Retpoline jumps are really dynamic jumps in
 +			 * disguise, so convert them accordingly.
  			 */
 -			add_call_dest(file, insn, reloc->sym, true);
 +			insn->type = INSN_JUMP_DYNAMIC;
 +			insn->retpoline_safe = true;
  			continue;
 -		} else if (reloc->sym->sec->idx) {
 -			dest_sec = reloc->sym->sec;
 -			dest_off = reloc->sym->sym.st_value +
 -				   arch_dest_reloc_offset(reloc->addend);
  		} else {
 -			/* non-func asm code jumping to another file */
 +			/* sibling call */
 +			insn->call_dest = rela->sym;
 +			insn->jump_dest = NULL;
  			continue;
  		}
  
++<<<<<<< HEAD
 +		insn->jump_dest = find_insn(file, dest_sec, dest_off);
 +		if (!insn->jump_dest) {
 +
 +			/*
 +			 * This is a special case where an alt instruction
 +			 * jumps past the end of the section.  These are
 +			 * handled later in handle_group_alt().
 +			 */
 +			if (!strcmp(insn->sec->name, ".altinstr_replacement"))
 +				continue;
++=======
+ 		jump_dest = find_insn(file, dest_sec, dest_off);
+ 		if (!jump_dest) {
+ 			struct symbol *sym = find_symbol_by_offset(dest_sec, dest_off);
+ 
+ 			/*
+ 			 * This is a special case for zen_untrain_ret().
+ 			 * It jumps to __x86_return_thunk(), but objtool
+ 			 * can't find the thunk's starting RET
+ 			 * instruction, because the RET is also in the
+ 			 * middle of another instruction.  Objtool only
+ 			 * knows about the outer instruction.
+ 			 */
+ 			if (sym && sym->return_thunk) {
+ 				add_return_call(file, insn, false);
+ 				continue;
+ 			}
++>>>>>>> a149180fbcf3 (x86: Add magic AMD return-thunk)
  
  			WARN_FUNC("can't find jump dest instruction at %s+0x%lx",
  				  insn->sec, insn->offset, dest_sec->name,
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/entry/entry_64_compat.S
* Unmerged path arch/x86/include/asm/cpufeatures.h
* Unmerged path arch/x86/include/asm/disabled-features.h
* Unmerged path arch/x86/include/asm/nospec-branch.h
diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S
index 45d0dd83ad4d..57b7108df4e7 100644
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@ -135,7 +135,7 @@ SECTIONS
 
 #ifdef CONFIG_RETPOLINE
 		__indirect_thunk_start = .;
-		*(.text.__x86.indirect_thunk)
+		*(.text.__x86.*)
 		__indirect_thunk_end = .;
 #endif
 
diff --git a/arch/x86/kvm/svm/vmenter.S b/arch/x86/kvm/svm/vmenter.S
index 4fa17df123cd..0a101f41d543 100644
--- a/arch/x86/kvm/svm/vmenter.S
+++ b/arch/x86/kvm/svm/vmenter.S
@@ -110,6 +110,15 @@ SYM_FUNC_START(__svm_vcpu_run)
 	mov %r15, VCPU_R15(%_ASM_AX)
 #endif
 
+	/*
+	 * Mitigate RETBleed for AMD/Hygon Zen uarch. RET should be
+	 * untrained as soon as we exit the VM and are back to the
+	 * kernel. This should be done before re-enabling interrupts
+	 * because interrupt handlers won't sanitize 'ret' if the return is
+	 * from the kernel.
+	 */
+	UNTRAIN_RET
+
 	/*
 	 * Clear all general purpose registers except RSP and RAX to prevent
 	 * speculative use of the guest's values, even those that are reloaded
@@ -190,6 +199,15 @@ SYM_FUNC_START(__svm_sev_es_vcpu_run)
 	FILL_RETURN_BUFFER %_ASM_AX, RSB_CLEAR_LOOPS, X86_FEATURE_RETPOLINE
 #endif
 
+	/*
+	 * Mitigate RETBleed for AMD/Hygon Zen uarch. RET should be
+	 * untrained as soon as we exit the VM and are back to the
+	 * kernel. This should be done before re-enabling interrupts
+	 * because interrupt handlers won't sanitize RET if the return is
+	 * from the kernel.
+	 */
+	UNTRAIN_RET
+
 	pop %_ASM_BX
 
 #ifdef CONFIG_X86_64
* Unmerged path arch/x86/lib/retpoline.S
* Unmerged path tools/objtool/check.c
