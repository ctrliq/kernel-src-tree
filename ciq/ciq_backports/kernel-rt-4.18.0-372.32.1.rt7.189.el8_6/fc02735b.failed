KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS

jira LE-1907
cve CVE-2022-23825
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-rt-4.18.0-372.32.1.rt7.189.el8_6
commit-author Josh Poimboeuf <jpoimboe@kernel.org>
commit fc02735b14fff8c6678b521d324ade27b1a3d4cf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-372.32.1.rt7.189.el8_6/fc02735b.failed

On eIBRS systems, the returns in the vmexit return path from
__vmx_vcpu_run() to vmx_vcpu_run() are exposed to RSB poisoning attacks.

Fix that by moving the post-vmexit spec_ctrl handling to immediately
after the vmexit.

	Signed-off-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit fc02735b14fff8c6678b521d324ade27b1a3d4cf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/nospec-branch.h
#	arch/x86/kvm/vmx/run_flags.h
#	arch/x86/kvm/vmx/vmenter.S
#	arch/x86/kvm/vmx/vmx.c
#	arch/x86/kvm/vmx/vmx.h
diff --cc arch/x86/include/asm/nospec-branch.h
index 797212d0a956,ccde87e6eabb..000000000000
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@@ -267,6 -274,9 +267,12 @@@ static inline void indirect_branch_pred
  
  /* The Intel SPEC CTRL MSR base value cache */
  extern u64 x86_spec_ctrl_base;
++<<<<<<< HEAD
++=======
+ extern u64 x86_spec_ctrl_current;
+ extern void write_spec_ctrl_current(u64 val, bool force);
+ extern u64 spec_ctrl_current(void);
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
  
  /*
   * With retpoline, we must use IBRS to restrict branch prediction
diff --cc arch/x86/kvm/vmx/vmenter.S
index fd0a4aadb374,8641ea74a307..000000000000
--- a/arch/x86/kvm/vmx/vmenter.S
+++ b/arch/x86/kvm/vmx/vmenter.S
@@@ -30,73 -31,12 +30,78 @@@
  
  .section .noinstr.text, "ax"
  
 +/**
 + * vmx_vmenter - VM-Enter the current loaded VMCS
 + *
 + * %RFLAGS.ZF:	!VMCS.LAUNCHED, i.e. controls VMLAUNCH vs. VMRESUME
 + *
 + * Returns:
 + *	%RFLAGS.CF is set on VM-Fail Invalid
 + *	%RFLAGS.ZF is set on VM-Fail Valid
 + *	%RFLAGS.{CF,ZF} are cleared on VM-Success, i.e. VM-Exit
 + *
 + * Note that VMRESUME/VMLAUNCH fall-through and return directly if
 + * they VM-Fail, whereas a successful VM-Enter + VM-Exit will jump
 + * to vmx_vmexit.
 + */
 +SYM_FUNC_START_LOCAL(vmx_vmenter)
 +	/* EFLAGS.ZF is set if VMCS.LAUNCHED == 0 */
 +	je 2f
 +
 +1:	vmresume
 +	ret
 +
 +2:	vmlaunch
 +	ret
 +
 +3:	cmpb $0, kvm_rebooting
 +	je 4f
 +	ret
 +4:	ud2
 +
 +	_ASM_EXTABLE(1b, 3b)
 +	_ASM_EXTABLE(2b, 3b)
 +
 +SYM_FUNC_END(vmx_vmenter)
 +
 +/**
 + * vmx_vmexit - Handle a VMX VM-Exit
 + *
 + * Returns:
 + *	%RFLAGS.{CF,ZF} are cleared on VM-Success, i.e. VM-Exit
 + *
 + * This is vmx_vmenter's partner in crime.  On a VM-Exit, control will jump
 + * here after hardware loads the host's state, i.e. this is the destination
 + * referred to by VMCS.HOST_RIP.
 + */
 +SYM_FUNC_START(vmx_vmexit)
 +#ifdef CONFIG_RETPOLINE
 +	ALTERNATIVE "jmp .Lvmexit_skip_rsb", "", X86_FEATURE_RETPOLINE
 +	/* Preserve guest's RAX, it's used to stuff the RSB. */
 +	push %_ASM_AX
 +
 +	/* IMPORTANT: Stuff the RSB immediately after VM-Exit, before RET! */
 +	FILL_RETURN_BUFFER %_ASM_AX, RSB_CLEAR_LOOPS, X86_FEATURE_RETPOLINE
 +
 +	/* Clear RFLAGS.CF and RFLAGS.ZF to preserve VM-Exit, i.e. !VM-Fail. */
 +	or $1, %_ASM_AX
 +
 +	pop %_ASM_AX
 +.Lvmexit_skip_rsb:
 +#endif
 +	ret
 +SYM_FUNC_END(vmx_vmexit)
 +
  /**
   * __vmx_vcpu_run - Run a vCPU via a transition to VMX guest mode
-  * @vmx:	struct vcpu_vmx * (forwarded to vmx_update_host_rsp)
+  * @vmx:	struct vcpu_vmx *
   * @regs:	unsigned long * (to guest registers)
++<<<<<<< HEAD
 + * @launched:	%true if the VMCS has been launched
++=======
+  * @flags:	VMX_RUN_VMRESUME:	use VMRESUME instead of VMLAUNCH
+  *		VMX_RUN_SAVE_SPEC_CTRL: save guest SPEC_CTRL into vmx->spec_ctrl
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
   *
   * Returns:
   *	0 on VM-Exit, 1 on VM-Fail
@@@ -185,21 -156,23 +196,31 @@@ SYM_FUNC_START(__vmx_vcpu_run
  	mov %r15, VCPU_R15(%_ASM_AX)
  #endif
  
++<<<<<<< HEAD
 +	/* Clear RAX to indicate VM-Exit (as opposed to VM-Fail). */
 +	xor %eax, %eax
++=======
+ 	/* Clear return value to indicate VM-Exit (as opposed to VM-Fail). */
+ 	xor %ebx, %ebx
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
  
 -.Lclear_regs:
  	/*
- 	 * Clear all general purpose registers except RSP and RAX to prevent
+ 	 * Clear all general purpose registers except RSP and RBX to prevent
  	 * speculative use of the guest's values, even those that are reloaded
  	 * via the stack.  In theory, an L1 cache miss when restoring registers
  	 * could lead to speculative execution with the guest's values.
  	 * Zeroing XORs are dirt cheap, i.e. the extra paranoia is essentially
  	 * free.  RSP and RAX are exempt as RSP is restored by hardware during
- 	 * VM-Exit and RAX is explicitly loaded with 0 or 1 to return VM-Fail.
+ 	 * VM-Exit and RBX is explicitly loaded with 0 or 1 to hold the return
+ 	 * value.
  	 */
++<<<<<<< HEAD
 +1:	xor %ecx, %ecx
++=======
+ 	xor %eax, %eax
+ 	xor %ecx, %ecx
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
  	xor %edx, %edx
- 	xor %ebx, %ebx
  	xor %ebp, %ebp
  	xor %esi, %esi
  	xor %edi, %edi
@@@ -216,8 -189,30 +237,34 @@@
  
  	/* "POP" @regs. */
  	add $WORD_SIZE, %_ASM_SP
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * IMPORTANT: RSB filling and SPEC_CTRL handling must be done before
+ 	 * the first unbalanced RET after vmexit!
+ 	 *
+ 	 * For retpoline, RSB filling is needed to prevent poisoned RSB entries
+ 	 * and (in some cases) RSB underflow.
+ 	 *
+ 	 * eIBRS has its own protection against poisoned RSB, so it doesn't
+ 	 * need the RSB filling sequence.  But it does need to be enabled
+ 	 * before the first unbalanced RET.
+          */
+ 
+ 	FILL_RETURN_BUFFER %_ASM_CX, RSB_CLEAR_LOOPS, X86_FEATURE_RETPOLINE
+ 
+ 	pop %_ASM_ARG2	/* @flags */
+ 	pop %_ASM_ARG1	/* @vmx */
+ 
+ 	call vmx_spec_ctrl_restore_host
+ 
+ 	/* Put return value in AX */
+ 	mov %_ASM_BX, %_ASM_AX
+ 
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
  	pop %_ASM_BX
 +
  #ifdef CONFIG_X86_64
  	pop %r12
  	pop %r13
@@@ -228,11 -223,17 +275,24 @@@
  	pop %edi
  #endif
  	pop %_ASM_BP
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ 
+ .Lfixup:
+ 	cmpb $0, kvm_rebooting
+ 	jne .Lvmfail
+ 	ud2
+ .Lvmfail:
+ 	/* VM-Fail: set return value to 1 */
+ 	mov $1, %_ASM_BX
+ 	jmp .Lclear_regs
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
  
 +	/* VM-Fail.  Out-of-line to avoid a taken Jcc after VM-Exit. */
 +2:	mov $1, %eax
 +	jmp 1b
  SYM_FUNC_END(__vmx_vcpu_run)
  
  
diff --cc arch/x86/kvm/vmx/vmx.c
index e42640520968,b81000cc826a..000000000000
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@@ -844,16 -835,26 +844,31 @@@ static bool msr_write_intercepted(struc
  	if (!(exec_controls_get(vmx) & CPU_BASED_USE_MSR_BITMAPS))
  		return true;
  
 -	return vmx_test_msr_bitmap_write(vmx->loaded_vmcs->msr_bitmap,
 -					 MSR_IA32_SPEC_CTRL);
 -}
 +	msr_bitmap = vmx->loaded_vmcs->msr_bitmap;
  
 -unsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx)
 -{
 -	unsigned int flags = 0;
 +	if (msr <= 0x1fff) {
 +		return !!test_bit(msr, msr_bitmap + 0x800 / f);
 +	} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {
 +		msr &= 0x1fff;
 +		return !!test_bit(msr, msr_bitmap + 0xc00 / f);
 +	}
  
++<<<<<<< HEAD
 +	return true;
++=======
+ 	if (vmx->loaded_vmcs->launched)
+ 		flags |= VMX_RUN_VMRESUME;
+ 
+ 	/*
+ 	 * If writes to the SPEC_CTRL MSR aren't intercepted, the guest is free
+ 	 * to change it directly without causing a vmexit.  In that case read
+ 	 * it after vmexit and store it in vmx->spec_ctrl.
+ 	 */
+ 	if (unlikely(!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL)))
+ 		flags |= VMX_RUN_SAVE_SPEC_CTRL;
+ 
+ 	return flags;
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
  }
  
  static void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,
@@@ -6838,28 -6992,8 +6873,8 @@@ static fastpath_t vmx_vcpu_run(struct k
  	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
  
  	/* The actual VMENTER/EXIT is in the .noinstr.text section. */
 -	vmx_vcpu_enter_exit(vcpu, vmx, __vmx_vcpu_run_flags(vmx));
 +	vmx_vcpu_enter_exit(vcpu, vmx);
  
- 	/*
- 	 * We do not use IBRS in the kernel. If this vCPU has used the
- 	 * SPEC_CTRL MSR it may have left it on; save the value and
- 	 * turn it off. This is much more efficient than blindly adding
- 	 * it to the atomic save/restore list. Especially as the former
- 	 * (Saving guest MSRs on vmexit) doesn't even exist in KVM.
- 	 *
- 	 * For non-nested case:
- 	 * If the L01 MSR bitmap does not intercept the MSR, then we need to
- 	 * save it.
- 	 *
- 	 * For nested case:
- 	 * If the L02 MSR bitmap does not intercept the MSR, then we need to
- 	 * save it.
- 	 */
- 	if (unlikely(!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL)))
- 		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
- 
- 	x86_spec_ctrl_restore_host(vmx->spec_ctrl, 0);
- 
  	/* All fields are clean at this point */
  	if (static_branch_unlikely(&enable_evmcs)) {
  		current_evmcs->hv_clean_fields |=
diff --cc arch/x86/kvm/vmx/vmx.h
index f120ccd14f3d,da654af12ccb..000000000000
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@@ -381,7 -405,10 +381,14 @@@ void vmx_set_virtual_apic_mode(struct k
  struct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr);
  void pt_update_intercept_for_msr(struct kvm_vcpu *vcpu);
  void vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp);
++<<<<<<< HEAD
 +bool __vmx_vcpu_run(struct vcpu_vmx *vmx, unsigned long *regs, bool launched);
++=======
+ void vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx, unsigned int flags);
+ unsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx);
+ bool __vmx_vcpu_run(struct vcpu_vmx *vmx, unsigned long *regs,
+ 		    unsigned int flags);
++>>>>>>> fc02735b14ff (KVM: VMX: Prevent guest RSB poisoning attacks with eIBRS)
  int vmx_find_loadstore_msr_slot(struct vmx_msrs *m, u32 msr);
  void vmx_ept_load_pdptrs(struct kvm_vcpu *vcpu);
  
* Unmerged path arch/x86/kvm/vmx/run_flags.h
* Unmerged path arch/x86/include/asm/nospec-branch.h
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index ff842ff87312..e3d24aff83cb 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -157,6 +157,10 @@ void __init check_bugs(void)
 #endif
 }
 
+/*
+ * NOTE: For VMX, this function is not called in the vmexit path.
+ * It uses vmx_spec_ctrl_restore_host() instead.
+ */
 void
 x86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)
 {
* Unmerged path arch/x86/kvm/vmx/run_flags.h
* Unmerged path arch/x86/kvm/vmx/vmenter.S
* Unmerged path arch/x86/kvm/vmx/vmx.c
* Unmerged path arch/x86/kvm/vmx/vmx.h
