x86: Prepare asm files for straight-line-speculation

jira LE-1907
cve CVE-2022-23825
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-rt-4.18.0-372.32.1.rt7.189.el8_6
commit-author Peter Zijlstra <peterz@infradead.org>
commit f94909ceb1ed4bfdb2ada72f93236305e6d6951f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-372.32.1.rt7.189.el8_6/f94909ce.failed

Replace all ret/retq instructions with RET in preparation of making
RET a macro. Since AS is case insensitive it's a big no-op without
RET defined.

  find arch/x86/ -name \*.S | while read file
  do
	sed -i 's/\<ret[q]*\>/RET/' $file
  done

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lore.kernel.org/r/20211204134907.905503893@infradead.org
(cherry picked from commit f94909ceb1ed4bfdb2ada72f93236305e6d6951f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/efi_thunk_64.S
#	arch/x86/boot/compressed/head_64.S
#	arch/x86/boot/compressed/mem_encrypt.S
#	arch/x86/crypto/aesni-intel_asm.S
#	arch/x86/crypto/blake2s-core.S
#	arch/x86/crypto/chacha-avx2-x86_64.S
#	arch/x86/crypto/chacha-avx512vl-x86_64.S
#	arch/x86/crypto/chacha20-ssse3-x86_64.S
#	arch/x86/crypto/crct10dif-pcl-asm_64.S
#	arch/x86/crypto/nh-avx2-x86_64.S
#	arch/x86/crypto/nh-sse2-x86_64.S
#	arch/x86/crypto/serpent-sse2-i586-asm_32.S
#	arch/x86/crypto/sha512-avx2-asm.S
#	arch/x86/crypto/sm4-aesni-avx-asm_64.S
#	arch/x86/crypto/sm4-aesni-avx2-asm_64.S
#	arch/x86/crypto/twofish-i586-asm_32.S
#	arch/x86/entry/entry_32.S
#	arch/x86/entry/entry_64.S
#	arch/x86/entry/thunk_64.S
#	arch/x86/kernel/acpi/wakeup_32.S
#	arch/x86/kernel/ftrace_32.S
#	arch/x86/kernel/ftrace_64.S
#	arch/x86/kernel/verify_cpu.S
#	arch/x86/lib/atomic64_386_32.S
#	arch/x86/lib/atomic64_cx8_32.S
#	arch/x86/lib/checksum_32.S
#	arch/x86/lib/copy_mc_64.S
#	arch/x86/lib/copy_page_64.S
#	arch/x86/lib/csum-copy_64.S
#	arch/x86/lib/getuser.S
#	arch/x86/lib/memcpy_64.S
#	arch/x86/lib/memset_64.S
#	arch/x86/lib/retpoline.S
#	arch/x86/math-emu/div_small.S
#	arch/x86/math-emu/mul_Xsig.S
#	arch/x86/math-emu/polynom_Xsig.S
#	arch/x86/math-emu/reg_norm.S
#	arch/x86/math-emu/reg_u_sub.S
#	arch/x86/math-emu/round_Xsig.S
#	arch/x86/math-emu/shr_Xsig.S
#	arch/x86/math-emu/wm_shrx.S
#	arch/x86/platform/efi/efi_stub_32.S
#	arch/x86/platform/efi/efi_stub_64.S
#	arch/x86/power/hibernate_asm_32.S
#	arch/x86/power/hibernate_asm_64.S
#	arch/x86/xen/xen-asm.S
#	arch/x86/xen/xen-head.S
diff --cc arch/x86/boot/compressed/efi_thunk_64.S
index 5af41b130925,70052779b235..000000000000
--- a/arch/x86/boot/compressed/efi_thunk_64.S
+++ b/arch/x86/boot/compressed/efi_thunk_64.S
@@@ -85,26 -88,13 +85,31 @@@ SYM_FUNC_START(efi64_thunk
  	/*
  	 * Convert 32-bit status code into 64-bit.
  	 */
 -	roll	$1, %eax
 -	rorq	$1, %rax
 -
 +	test	%rax, %rax
 +	jz	1f
 +	movl	%eax, %ecx
 +	andl	$0x0fffffff, %ecx
 +	andl	$0xf0000000, %eax
 +	shl	$32, %rax
 +	or	%rcx, %rax
 +1:
 +	addq	$8, %rsp
  	pop	%rbx
  	pop	%rbp
++<<<<<<< HEAD
 +	ret
 +SYM_FUNC_END(efi64_thunk)
 +
 +ENTRY(efi_exit32)
 +	movq	func_rt_ptr(%rip), %rax
 +	push	%rax
 +	mov	%rdi, %rax
 +	ret
 +ENDPROC(efi_exit32)
++=======
+ 	RET
+ SYM_FUNC_END(__efi64_thunk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	.code32
  /*
diff --cc arch/x86/boot/compressed/head_64.S
index 80b99129936a,fd9441f40457..000000000000
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@@ -659,24 -721,241 +659,245 @@@ SYM_DATA_START(boot_idt
  	.endr
  SYM_DATA_END_LABEL(boot_idt, SYM_L_GLOBAL, boot_idt_end)
  
 -#ifdef CONFIG_AMD_MEM_ENCRYPT
 -SYM_DATA_START(boot32_idt_desc)
 -	.word   boot32_idt_end - boot32_idt - 1
 -	.long   0
 -SYM_DATA_END(boot32_idt_desc)
 -	.balign 8
 -SYM_DATA_START(boot32_idt)
 -	.rept 32
 -	.quad 0
 -	.endr
 -SYM_DATA_END_LABEL(boot32_idt, SYM_L_GLOBAL, boot32_idt_end)
 -#endif
 -
  #ifdef CONFIG_EFI_STUB
 -SYM_DATA(image_offset, .long 0)
 -#endif
 +efi_config:
 +	.quad	0
 +
  #ifdef CONFIG_EFI_MIXED
++<<<<<<< HEAD
 +	.global efi32_config
 +efi32_config:
 +	.fill	5,8,0
 +	.quad	efi64_thunk
 +	.byte	0
 +#endif
 +
 +	.global efi64_config
 +efi64_config:
 +	.fill	5,8,0
 +	.quad	efi_call
 +	.byte	1
 +#endif /* CONFIG_EFI_STUB */
++=======
+ SYM_DATA_LOCAL(efi32_boot_args, .long 0, 0, 0)
+ SYM_DATA(efi_is64, .byte 1)
+ 
+ #define ST32_boottime		60 // offsetof(efi_system_table_32_t, boottime)
+ #define BS32_handle_protocol	88 // offsetof(efi_boot_services_32_t, handle_protocol)
+ #define LI32_image_base		32 // offsetof(efi_loaded_image_32_t, image_base)
+ 
+ 	__HEAD
+ 	.code32
+ SYM_FUNC_START(efi32_pe_entry)
+ /*
+  * efi_status_t efi32_pe_entry(efi_handle_t image_handle,
+  *			       efi_system_table_32_t *sys_table)
+  */
+ 
+ 	pushl	%ebp
+ 	movl	%esp, %ebp
+ 	pushl	%eax				// dummy push to allocate loaded_image
+ 
+ 	pushl	%ebx				// save callee-save registers
+ 	pushl	%edi
+ 
+ 	call	verify_cpu			// check for long mode support
+ 	testl	%eax, %eax
+ 	movl	$0x80000003, %eax		// EFI_UNSUPPORTED
+ 	jnz	2f
+ 
+ 	call	1f
+ 1:	pop	%ebx
+ 	subl	$ rva(1b), %ebx
+ 
+ 	/* Get the loaded image protocol pointer from the image handle */
+ 	leal	-4(%ebp), %eax
+ 	pushl	%eax				// &loaded_image
+ 	leal	rva(loaded_image_proto)(%ebx), %eax
+ 	pushl	%eax				// pass the GUID address
+ 	pushl	8(%ebp)				// pass the image handle
+ 
+ 	/*
+ 	 * Note the alignment of the stack frame.
+ 	 *   sys_table
+ 	 *   handle             <-- 16-byte aligned on entry by ABI
+ 	 *   return address
+ 	 *   frame pointer
+ 	 *   loaded_image       <-- local variable
+ 	 *   saved %ebx		<-- 16-byte aligned here
+ 	 *   saved %edi
+ 	 *   &loaded_image
+ 	 *   &loaded_image_proto
+ 	 *   handle             <-- 16-byte aligned for call to handle_protocol
+ 	 */
+ 
+ 	movl	12(%ebp), %eax			// sys_table
+ 	movl	ST32_boottime(%eax), %eax	// sys_table->boottime
+ 	call	*BS32_handle_protocol(%eax)	// sys_table->boottime->handle_protocol
+ 	addl	$12, %esp			// restore argument space
+ 	testl	%eax, %eax
+ 	jnz	2f
+ 
+ 	movl	8(%ebp), %ecx			// image_handle
+ 	movl	12(%ebp), %edx			// sys_table
+ 	movl	-4(%ebp), %esi			// loaded_image
+ 	movl	LI32_image_base(%esi), %esi	// loaded_image->image_base
+ 	movl	%ebx, %ebp			// startup_32 for efi32_pe_stub_entry
+ 	/*
+ 	 * We need to set the image_offset variable here since startup_32() will
+ 	 * use it before we get to the 64-bit efi_pe_entry() in C code.
+ 	 */
+ 	subl	%esi, %ebx
+ 	movl	%ebx, rva(image_offset)(%ebp)	// save image_offset
+ 	jmp	efi32_pe_stub_entry
+ 
+ 2:	popl	%edi				// restore callee-save registers
+ 	popl	%ebx
+ 	leave
+ 	RET
+ SYM_FUNC_END(efi32_pe_entry)
+ 
+ 	.section ".rodata"
+ 	/* EFI loaded image protocol GUID */
+ 	.balign 4
+ SYM_DATA_START_LOCAL(loaded_image_proto)
+ 	.long	0x5b1b31a1
+ 	.word	0x9562, 0x11d2
+ 	.byte	0x8e, 0x3f, 0x00, 0xa0, 0xc9, 0x69, 0x72, 0x3b
+ SYM_DATA_END(loaded_image_proto)
+ #endif
+ 
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ 	__HEAD
+ 	.code32
+ /*
+  * Write an IDT entry into boot32_idt
+  *
+  * Parameters:
+  *
+  * %eax:	Handler address
+  * %edx:	Vector number
+  *
+  * Physical offset is expected in %ebp
+  */
+ SYM_FUNC_START(startup32_set_idt_entry)
+ 	push    %ebx
+ 	push    %ecx
+ 
+ 	/* IDT entry address to %ebx */
+ 	leal    rva(boot32_idt)(%ebp), %ebx
+ 	shl	$3, %edx
+ 	addl    %edx, %ebx
+ 
+ 	/* Build IDT entry, lower 4 bytes */
+ 	movl    %eax, %edx
+ 	andl    $0x0000ffff, %edx	# Target code segment offset [15:0]
+ 	movl    $__KERNEL32_CS, %ecx	# Target code segment selector
+ 	shl     $16, %ecx
+ 	orl     %ecx, %edx
+ 
+ 	/* Store lower 4 bytes to IDT */
+ 	movl    %edx, (%ebx)
+ 
+ 	/* Build IDT entry, upper 4 bytes */
+ 	movl    %eax, %edx
+ 	andl    $0xffff0000, %edx	# Target code segment offset [31:16]
+ 	orl     $0x00008e00, %edx	# Present, Type 32-bit Interrupt Gate
+ 
+ 	/* Store upper 4 bytes to IDT */
+ 	movl    %edx, 4(%ebx)
+ 
+ 	pop     %ecx
+ 	pop     %ebx
+ 	RET
+ SYM_FUNC_END(startup32_set_idt_entry)
+ #endif
+ 
+ SYM_FUNC_START(startup32_load_idt)
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ 	/* #VC handler */
+ 	leal    rva(startup32_vc_handler)(%ebp), %eax
+ 	movl    $X86_TRAP_VC, %edx
+ 	call    startup32_set_idt_entry
+ 
+ 	/* Load IDT */
+ 	leal	rva(boot32_idt)(%ebp), %eax
+ 	movl	%eax, rva(boot32_idt_desc+2)(%ebp)
+ 	lidt    rva(boot32_idt_desc)(%ebp)
+ #endif
+ 	RET
+ SYM_FUNC_END(startup32_load_idt)
+ 
+ /*
+  * Check for the correct C-bit position when the startup_32 boot-path is used.
+  *
+  * The check makes use of the fact that all memory is encrypted when paging is
+  * disabled. The function creates 64 bits of random data using the RDRAND
+  * instruction. RDRAND is mandatory for SEV guests, so always available. If the
+  * hypervisor violates that the kernel will crash right here.
+  *
+  * The 64 bits of random data are stored to a memory location and at the same
+  * time kept in the %eax and %ebx registers. Since encryption is always active
+  * when paging is off the random data will be stored encrypted in main memory.
+  *
+  * Then paging is enabled. When the C-bit position is correct all memory is
+  * still mapped encrypted and comparing the register values with memory will
+  * succeed. An incorrect C-bit position will map all memory unencrypted, so that
+  * the compare will use the encrypted random data and fail.
+  */
+ SYM_FUNC_START(startup32_check_sev_cbit)
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ 	pushl	%eax
+ 	pushl	%ebx
+ 	pushl	%ecx
+ 	pushl	%edx
+ 
+ 	/* Check for non-zero sev_status */
+ 	movl	rva(sev_status)(%ebp), %eax
+ 	testl	%eax, %eax
+ 	jz	4f
+ 
+ 	/*
+ 	 * Get two 32-bit random values - Don't bail out if RDRAND fails
+ 	 * because it is better to prevent forward progress if no random value
+ 	 * can be gathered.
+ 	 */
+ 1:	rdrand	%eax
+ 	jnc	1b
+ 2:	rdrand	%ebx
+ 	jnc	2b
+ 
+ 	/* Store to memory and keep it in the registers */
+ 	movl	%eax, rva(sev_check_data)(%ebp)
+ 	movl	%ebx, rva(sev_check_data+4)(%ebp)
+ 
+ 	/* Enable paging to see if encryption is active */
+ 	movl	%cr0, %edx			 /* Backup %cr0 in %edx */
+ 	movl	$(X86_CR0_PG | X86_CR0_PE), %ecx /* Enable Paging and Protected mode */
+ 	movl	%ecx, %cr0
+ 
+ 	cmpl	%eax, rva(sev_check_data)(%ebp)
+ 	jne	3f
+ 	cmpl	%ebx, rva(sev_check_data+4)(%ebp)
+ 	jne	3f
+ 
+ 	movl	%edx, %cr0	/* Restore previous %cr0 */
+ 
+ 	jmp	4f
+ 
+ 3:	/* Check failed - hlt the machine */
+ 	hlt
+ 	jmp	3b
+ 
+ 4:
+ 	popl	%edx
+ 	popl	%ecx
+ 	popl	%ebx
+ 	popl	%eax
+ #endif
+ 	RET
+ SYM_FUNC_END(startup32_check_sev_cbit)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * Stack and heap for uncompression
diff --cc arch/x86/boot/compressed/mem_encrypt.S
index b0a29b2ef0fb,a63424d13627..000000000000
--- a/arch/x86/boot/compressed/mem_encrypt.S
+++ b/arch/x86/boot/compressed/mem_encrypt.S
@@@ -67,9 -58,132 +67,135 @@@ SYM_FUNC_START(get_sev_encryption_bit
  
  #endif	/* CONFIG_AMD_MEM_ENCRYPT */
  
- 	ret
+ 	RET
  SYM_FUNC_END(get_sev_encryption_bit)
  
++<<<<<<< HEAD
++=======
+ /**
+  * sev_es_req_cpuid - Request a CPUID value from the Hypervisor using
+  *		      the GHCB MSR protocol
+  *
+  * @%eax:	Register to request (0=EAX, 1=EBX, 2=ECX, 3=EDX)
+  * @%edx:	CPUID Function
+  *
+  * Returns 0 in %eax on success, non-zero on failure
+  * %edx returns CPUID value on success
+  */
+ SYM_CODE_START_LOCAL(sev_es_req_cpuid)
+ 	shll	$30, %eax
+ 	orl     $0x00000004, %eax
+ 	movl    $MSR_AMD64_SEV_ES_GHCB, %ecx
+ 	wrmsr
+ 	rep; vmmcall		# VMGEXIT
+ 	rdmsr
+ 
+ 	/* Check response */
+ 	movl	%eax, %ecx
+ 	andl	$0x3ffff000, %ecx	# Bits [12-29] MBZ
+ 	jnz	2f
+ 
+ 	/* Check return code */
+ 	andl    $0xfff, %eax
+ 	cmpl    $5, %eax
+ 	jne	2f
+ 
+ 	/* All good - return success */
+ 	xorl	%eax, %eax
+ 1:
+ 	RET
+ 2:
+ 	movl	$-1, %eax
+ 	jmp	1b
+ SYM_CODE_END(sev_es_req_cpuid)
+ 
+ SYM_CODE_START(startup32_vc_handler)
+ 	pushl	%eax
+ 	pushl	%ebx
+ 	pushl	%ecx
+ 	pushl	%edx
+ 
+ 	/* Keep CPUID function in %ebx */
+ 	movl	%eax, %ebx
+ 
+ 	/* Check if error-code == SVM_EXIT_CPUID */
+ 	cmpl	$0x72, 16(%esp)
+ 	jne	.Lfail
+ 
+ 	movl	$0, %eax		# Request CPUID[fn].EAX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 12(%esp)		# Store result
+ 
+ 	movl	$1, %eax		# Request CPUID[fn].EBX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 8(%esp)		# Store result
+ 
+ 	movl	$2, %eax		# Request CPUID[fn].ECX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 4(%esp)		# Store result
+ 
+ 	movl	$3, %eax		# Request CPUID[fn].EDX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 0(%esp)		# Store result
+ 
+ 	/*
+ 	 * Sanity check CPUID results from the Hypervisor. See comment in
+ 	 * do_vc_no_ghcb() for more details on why this is necessary.
+ 	 */
+ 
+ 	/* Fail if SEV leaf not available in CPUID[0x80000000].EAX */
+ 	cmpl    $0x80000000, %ebx
+ 	jne     .Lcheck_sev
+ 	cmpl    $0x8000001f, 12(%esp)
+ 	jb      .Lfail
+ 	jmp     .Ldone
+ 
+ .Lcheck_sev:
+ 	/* Fail if SEV bit not set in CPUID[0x8000001f].EAX[1] */
+ 	cmpl    $0x8000001f, %ebx
+ 	jne     .Ldone
+ 	btl     $1, 12(%esp)
+ 	jnc     .Lfail
+ 
+ .Ldone:
+ 	popl	%edx
+ 	popl	%ecx
+ 	popl	%ebx
+ 	popl	%eax
+ 
+ 	/* Remove error code */
+ 	addl	$4, %esp
+ 
+ 	/* Jump over CPUID instruction */
+ 	addl	$2, (%esp)
+ 
+ 	iret
+ .Lfail:
+ 	/* Send terminate request to Hypervisor */
+ 	movl    $0x100, %eax
+ 	xorl    %edx, %edx
+ 	movl    $MSR_AMD64_SEV_ES_GHCB, %ecx
+ 	wrmsr
+ 	rep; vmmcall
+ 
+ 	/* If request fails, go to hlt loop */
+ 	hlt
+ 	jmp .Lfail
+ SYM_CODE_END(startup32_vc_handler)
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  	.code64
  
  #include "../../kernel/sev_verify_cbit.S"
diff --cc arch/x86/crypto/aesni-intel_asm.S
index 15bba68f28fe,363699dd7220..000000000000
--- a/arch/x86/crypto/aesni-intel_asm.S
+++ b/arch/x86/crypto/aesni-intel_asm.S
@@@ -2004,26 -1995,26 +2004,31 @@@ SYM_FUNC_START_LOCAL(_aesni_enc1
  .align 4
  .Lenc128:
  	movaps -0x20(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps -0x10(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps (TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x10(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x20(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x30(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x40(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x50(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x60(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESENCLAST KEY STATE
 +	ret
++=======
+ 	aesenclast KEY, STATE
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END(_aesni_enc1)
  
  /*
@@@ -2082,56 -2073,56 +2087,64 @@@ SYM_FUNC_START_LOCAL(_aesni_enc4
  #.align 4
  .L4enc128:
  	movaps -0x20(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps -0x10(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps (TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x10(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x20(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x30(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x40(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x50(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x60(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESENCLAST KEY STATE1		# last round
 +	AESENCLAST KEY STATE2
 +	AESENCLAST KEY STATE3
 +	AESENCLAST KEY STATE4
 +	ret
++=======
+ 	aesenclast KEY, STATE1		# last round
+ 	aesenclast KEY, STATE2
+ 	aesenclast KEY, STATE3
+ 	aesenclast KEY, STATE4
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END(_aesni_enc4)
  
  /*
@@@ -2194,26 -2185,26 +2207,31 @@@ SYM_FUNC_START_LOCAL(_aesni_dec1
  .align 4
  .Ldec128:
  	movaps -0x20(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps -0x10(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps (TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x10(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x20(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x30(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x40(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x50(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x60(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESDECLAST KEY STATE
 +	ret
++=======
+ 	aesdeclast KEY, STATE
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END(_aesni_dec1)
  
  /*
@@@ -2272,56 -2263,56 +2290,64 @@@ SYM_FUNC_START_LOCAL(_aesni_dec4
  .align 4
  .L4dec128:
  	movaps -0x20(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps -0x10(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps (TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x10(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x20(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x30(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x40(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x50(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x60(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESDECLAST KEY STATE1		# last round
 +	AESDECLAST KEY STATE2
 +	AESDECLAST KEY STATE3
 +	AESDECLAST KEY STATE4
 +	ret
++=======
+ 	aesdeclast KEY, STATE1		# last round
+ 	aesdeclast KEY, STATE2
+ 	aesdeclast KEY, STATE3
+ 	aesdeclast KEY, STATE4
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END(_aesni_dec4)
  
  /*
@@@ -2579,16 -2570,143 +2605,137 @@@ SYM_FUNC_START(aesni_cbc_dec
  	popl IVP
  #endif
  	FRAME_END
- 	ret
+ 	RET
  SYM_FUNC_END(aesni_cbc_dec)
  
++<<<<<<< HEAD
 +#ifdef __x86_64__
++=======
+ /*
+  * void aesni_cts_cbc_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
+  *			  size_t len, u8 *iv)
+  */
+ SYM_FUNC_START(aesni_cts_cbc_enc)
+ 	FRAME_BEGIN
+ #ifndef __x86_64__
+ 	pushl IVP
+ 	pushl LEN
+ 	pushl KEYP
+ 	pushl KLEN
+ 	movl (FRAME_OFFSET+20)(%esp), KEYP	# ctx
+ 	movl (FRAME_OFFSET+24)(%esp), OUTP	# dst
+ 	movl (FRAME_OFFSET+28)(%esp), INP	# src
+ 	movl (FRAME_OFFSET+32)(%esp), LEN	# len
+ 	movl (FRAME_OFFSET+36)(%esp), IVP	# iv
+ 	lea .Lcts_permute_table, T1
+ #else
+ 	lea .Lcts_permute_table(%rip), T1
+ #endif
+ 	mov 480(KEYP), KLEN
+ 	movups (IVP), STATE
+ 	sub $16, LEN
+ 	mov T1, IVP
+ 	add $32, IVP
+ 	add LEN, T1
+ 	sub LEN, IVP
+ 	movups (T1), %xmm4
+ 	movups (IVP), %xmm5
+ 
+ 	movups (INP), IN1
+ 	add LEN, INP
+ 	movups (INP), IN2
+ 
+ 	pxor IN1, STATE
+ 	call _aesni_enc1
+ 
+ 	pshufb %xmm5, IN2
+ 	pxor STATE, IN2
+ 	pshufb %xmm4, STATE
+ 	add OUTP, LEN
+ 	movups STATE, (LEN)
+ 
+ 	movaps IN2, STATE
+ 	call _aesni_enc1
+ 	movups STATE, (OUTP)
+ 
+ #ifndef __x86_64__
+ 	popl KLEN
+ 	popl KEYP
+ 	popl LEN
+ 	popl IVP
+ #endif
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(aesni_cts_cbc_enc)
+ 
+ /*
+  * void aesni_cts_cbc_dec(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
+  *			  size_t len, u8 *iv)
+  */
+ SYM_FUNC_START(aesni_cts_cbc_dec)
+ 	FRAME_BEGIN
+ #ifndef __x86_64__
+ 	pushl IVP
+ 	pushl LEN
+ 	pushl KEYP
+ 	pushl KLEN
+ 	movl (FRAME_OFFSET+20)(%esp), KEYP	# ctx
+ 	movl (FRAME_OFFSET+24)(%esp), OUTP	# dst
+ 	movl (FRAME_OFFSET+28)(%esp), INP	# src
+ 	movl (FRAME_OFFSET+32)(%esp), LEN	# len
+ 	movl (FRAME_OFFSET+36)(%esp), IVP	# iv
+ 	lea .Lcts_permute_table, T1
+ #else
+ 	lea .Lcts_permute_table(%rip), T1
+ #endif
+ 	mov 480(KEYP), KLEN
+ 	add $240, KEYP
+ 	movups (IVP), IV
+ 	sub $16, LEN
+ 	mov T1, IVP
+ 	add $32, IVP
+ 	add LEN, T1
+ 	sub LEN, IVP
+ 	movups (T1), %xmm4
+ 
+ 	movups (INP), STATE
+ 	add LEN, INP
+ 	movups (INP), IN1
+ 
+ 	call _aesni_dec1
+ 	movaps STATE, IN2
+ 	pshufb %xmm4, STATE
+ 	pxor IN1, STATE
+ 
+ 	add OUTP, LEN
+ 	movups STATE, (LEN)
+ 
+ 	movups (IVP), %xmm0
+ 	pshufb %xmm0, IN1
+ 	pblendvb IN2, IN1
+ 	movaps IN1, STATE
+ 	call _aesni_dec1
+ 
+ 	pxor IV, STATE
+ 	movups STATE, (OUTP)
+ 
+ #ifndef __x86_64__
+ 	popl KLEN
+ 	popl KEYP
+ 	popl LEN
+ 	popl IVP
+ #endif
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(aesni_cts_cbc_dec)
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  .pushsection .rodata
  .align 16
 -.Lcts_permute_table:
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -	.byte		0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07
 -	.byte		0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -#ifdef __x86_64__
  .Lbswap_mask:
  	.byte 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0
 -#endif
  .popsection
  
 -#ifdef __x86_64__
  /*
   * _aesni_inc_init:	internal ABI
   *	setup registers used by _aesni_inc
@@@ -2603,11 -2721,11 +2750,17 @@@
  SYM_FUNC_START_LOCAL(_aesni_inc_init)
  	movaps .Lbswap_mask, BSWAP_MASK
  	movaps IV, CTR
 -	pshufb BSWAP_MASK, CTR
 +	PSHUFB_XMM BSWAP_MASK CTR
  	mov $1, TCTR_LOW
++<<<<<<< HEAD
 +	MOVQ_R64_XMM TCTR_LOW INC
 +	MOVQ_R64_XMM CTR TCTR_LOW
 +	ret
++=======
+ 	movq TCTR_LOW, INC
+ 	movq CTR, TCTR_LOW
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END(_aesni_inc_init)
  
  /*
@@@ -2634,8 -2752,8 +2787,13 @@@ SYM_FUNC_START_LOCAL(_aesni_inc
  	psrldq $8, INC
  .Linc_low:
  	movaps CTR, IV
++<<<<<<< HEAD
 +	PSHUFB_XMM BSWAP_MASK IV
 +	ret
++=======
+ 	pshufb BSWAP_MASK, IV
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END(_aesni_inc)
  
  /*
@@@ -2698,9 -2816,17 +2856,9 @@@ SYM_FUNC_START(aesni_ctr_enc
  	movups IV, (IVP)
  .Lctr_enc_just_ret:
  	FRAME_END
- 	ret
+ 	RET
  SYM_FUNC_END(aesni_ctr_enc)
  
 -#endif
 -
 -.section	.rodata.cst16.gf128mul_x_ble_mask, "aM", @progbits, 16
 -.align 16
 -.Lgf128mul_x_ble_mask:
 -	.octa 0x00000000000000010000000000000087
 -.previous
 -
  /*
   * _aesni_gf128mul_x_ble:		internal ABI
   *	Multiply in GF(2^128) for XTS IVs
@@@ -2758,25 -2892,151 +2916,146 @@@ SYM_FUNC_START(aesni_xts_crypt8
  
  	_aesni_gf128mul_x_ble()
  	movdqa IV, STATE4
 -	movdqu 0x30(INP), IN
 -	pxor IN, STATE4
 +	movdqu 0x30(INP), INC
 +	pxor INC, STATE4
  	movdqu IV, 0x30(OUTP)
  
 -	call _aesni_enc4
 +	CALL_NOSPEC %r11
  
 -	movdqu 0x00(OUTP), IN
 -	pxor IN, STATE1
 +	movdqu 0x00(OUTP), INC
 +	pxor INC, STATE1
  	movdqu STATE1, 0x00(OUTP)
  
 -	movdqu 0x10(OUTP), IN
 -	pxor IN, STATE2
 -	movdqu STATE2, 0x10(OUTP)
 -
 -	movdqu 0x20(OUTP), IN
 -	pxor IN, STATE3
 -	movdqu STATE3, 0x20(OUTP)
 -
 -	movdqu 0x30(OUTP), IN
 -	pxor IN, STATE4
 -	movdqu STATE4, 0x30(OUTP)
 -
  	_aesni_gf128mul_x_ble()
++<<<<<<< HEAD
++=======
+ 
+ 	add $64, INP
+ 	add $64, OUTP
+ 	test LEN, LEN
+ 	jnz .Lxts_enc_loop4
+ 
+ .Lxts_enc_ret_iv:
+ 	movups IV, (IVP)
+ 
+ .Lxts_enc_ret:
+ #ifndef __x86_64__
+ 	popl KLEN
+ 	popl KEYP
+ 	popl LEN
+ 	popl IVP
+ #endif
+ 	FRAME_END
+ 	RET
+ 
+ .Lxts_enc_1x:
+ 	add $64, LEN
+ 	jz .Lxts_enc_ret_iv
+ 	sub $16, LEN
+ 	jl .Lxts_enc_cts4
+ 
+ .Lxts_enc_loop1:
+ 	movdqu (INP), STATE
+ 	pxor IV, STATE
+ 	call _aesni_enc1
+ 	pxor IV, STATE
+ 	_aesni_gf128mul_x_ble()
+ 
+ 	test LEN, LEN
+ 	jz .Lxts_enc_out
+ 
+ 	add $16, INP
+ 	sub $16, LEN
+ 	jl .Lxts_enc_cts1
+ 
+ 	movdqu STATE, (OUTP)
+ 	add $16, OUTP
+ 	jmp .Lxts_enc_loop1
+ 
+ .Lxts_enc_out:
+ 	movdqu STATE, (OUTP)
+ 	jmp .Lxts_enc_ret_iv
+ 
+ .Lxts_enc_cts4:
+ 	movdqa STATE4, STATE
+ 	sub $16, OUTP
+ 
+ .Lxts_enc_cts1:
+ #ifndef __x86_64__
+ 	lea .Lcts_permute_table, T1
+ #else
+ 	lea .Lcts_permute_table(%rip), T1
+ #endif
+ 	add LEN, INP		/* rewind input pointer */
+ 	add $16, LEN		/* # bytes in final block */
+ 	movups (INP), IN1
+ 
+ 	mov T1, IVP
+ 	add $32, IVP
+ 	add LEN, T1
+ 	sub LEN, IVP
+ 	add OUTP, LEN
+ 
+ 	movups (T1), %xmm4
+ 	movaps STATE, IN2
+ 	pshufb %xmm4, STATE
+ 	movups STATE, (LEN)
+ 
+ 	movups (IVP), %xmm0
+ 	pshufb %xmm0, IN1
+ 	pblendvb IN2, IN1
+ 	movaps IN1, STATE
+ 
+ 	pxor IV, STATE
+ 	call _aesni_enc1
+ 	pxor IV, STATE
+ 
+ 	movups STATE, (OUTP)
+ 	jmp .Lxts_enc_ret
+ SYM_FUNC_END(aesni_xts_encrypt)
+ 
+ /*
+  * void aesni_xts_decrypt(const struct crypto_aes_ctx *ctx, u8 *dst,
+  *			  const u8 *src, unsigned int len, le128 *iv)
+  */
+ SYM_FUNC_START(aesni_xts_decrypt)
+ 	FRAME_BEGIN
+ #ifndef __x86_64__
+ 	pushl IVP
+ 	pushl LEN
+ 	pushl KEYP
+ 	pushl KLEN
+ 	movl (FRAME_OFFSET+20)(%esp), KEYP	# ctx
+ 	movl (FRAME_OFFSET+24)(%esp), OUTP	# dst
+ 	movl (FRAME_OFFSET+28)(%esp), INP	# src
+ 	movl (FRAME_OFFSET+32)(%esp), LEN	# len
+ 	movl (FRAME_OFFSET+36)(%esp), IVP	# iv
+ 	movdqa .Lgf128mul_x_ble_mask, GF128MUL_MASK
+ #else
+ 	movdqa .Lgf128mul_x_ble_mask(%rip), GF128MUL_MASK
+ #endif
+ 	movups (IVP), IV
+ 
+ 	mov 480(KEYP), KLEN
+ 	add $240, KEYP
+ 
+ 	test $15, LEN
+ 	jz .Lxts_dec_loop4
+ 	sub $16, LEN
+ 
+ .Lxts_dec_loop4:
+ 	sub $64, LEN
+ 	jl .Lxts_dec_1x
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  	movdqa IV, STATE1
 -	movdqu 0x00(INP), IN
 -	pxor IN, STATE1
 -	movdqu IV, 0x00(OUTP)
 +	movdqu 0x40(INP), INC
 +	pxor INC, STATE1
 +	movdqu IV, 0x40(OUTP)
 +
 +	movdqu 0x10(OUTP), INC
 +	pxor INC, STATE2
 +	movdqu STATE2, 0x10(OUTP)
  
  	_aesni_gf128mul_x_ble()
  	movdqa IV, STATE2
@@@ -2799,34 -3075,89 +3078,38 @@@
  	movdqu STATE4, 0x30(OUTP)
  
  	_aesni_gf128mul_x_ble()
 +	movdqa IV, STATE4
 +	movdqu 0x70(INP), INC
 +	pxor INC, STATE4
 +	movdqu IV, 0x70(OUTP)
  
 -	add $64, INP
 -	add $64, OUTP
 -	test LEN, LEN
 -	jnz .Lxts_dec_loop4
 -
 -.Lxts_dec_ret_iv:
 +	_aesni_gf128mul_x_ble()
  	movups IV, (IVP)
  
 -.Lxts_dec_ret:
 -#ifndef __x86_64__
 -	popl KLEN
 -	popl KEYP
 -	popl LEN
 -	popl IVP
 -#endif
 -	FRAME_END
 -	RET
 +	CALL_NOSPEC %r11
  
 -.Lxts_dec_1x:
 -	add $64, LEN
 -	jz .Lxts_dec_ret_iv
 +	movdqu 0x40(OUTP), INC
 +	pxor INC, STATE1
 +	movdqu STATE1, 0x40(OUTP)
  
 -.Lxts_dec_loop1:
 -	movdqu (INP), STATE
 +	movdqu 0x50(OUTP), INC
 +	pxor INC, STATE2
 +	movdqu STATE2, 0x50(OUTP)
  
 -	add $16, INP
 -	sub $16, LEN
 -	jl .Lxts_dec_cts1
 +	movdqu 0x60(OUTP), INC
 +	pxor INC, STATE3
 +	movdqu STATE3, 0x60(OUTP)
  
 -	pxor IV, STATE
 -	call _aesni_dec1
 -	pxor IV, STATE
 -	_aesni_gf128mul_x_ble()
 +	movdqu 0x70(OUTP), INC
 +	pxor INC, STATE4
 +	movdqu STATE4, 0x70(OUTP)
  
 -	test LEN, LEN
 -	jz .Lxts_dec_out
 -
 -	movdqu STATE, (OUTP)
 -	add $16, OUTP
 -	jmp .Lxts_dec_loop1
 -
 -.Lxts_dec_out:
 -	movdqu STATE, (OUTP)
 -	jmp .Lxts_dec_ret_iv
 -
 -.Lxts_dec_cts1:
 -	movdqa IV, STATE4
 -	_aesni_gf128mul_x_ble()
 -
 -	pxor IV, STATE
 -	call _aesni_dec1
 -	pxor IV, STATE
 +	FRAME_END
++<<<<<<< HEAD
 +	ret
 +SYM_FUNC_END(aesni_xts_crypt8)
++=======
++	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -#ifndef __x86_64__
 -	lea .Lcts_permute_table, T1
 -#else
 -	lea .Lcts_permute_table(%rip), T1
  #endif
 -	add LEN, INP		/* rewind input pointer */
 -	add $16, LEN		/* # bytes in final block */
 -	movups (INP), IN1
 -
 -	mov T1, IVP
 -	add $32, IVP
 -	add LEN, T1
 -	sub LEN, IVP
 -	add OUTP, LEN
 -
 -	movups (T1), %xmm4
 -	movaps STATE, IN2
 -	pshufb %xmm4, STATE
 -	movups STATE, (LEN)
 -
 -	movups (IVP), %xmm0
 -	pshufb %xmm0, IN1
 -	pblendvb IN2, IN1
 -	movaps IN1, STATE
 -
 -	pxor STATE4, STATE
 -	call _aesni_dec1
 -	pxor STATE4, STATE
 -
 -	movups STATE, (OUTP)
 -	jmp .Lxts_dec_ret
 -SYM_FUNC_END(aesni_xts_decrypt)
diff --cc arch/x86/crypto/chacha20-ssse3-x86_64.S
index 512a2b500fd1,7111949cd5b9..000000000000
--- a/arch/x86/crypto/chacha20-ssse3-x86_64.S
+++ b/arch/x86/crypto/chacha20-ssse3-x86_64.S
@@@ -118,39 -105,129 +118,98 @@@ ENTRY(chacha20_block_xor_ssse3
  	# x3 = shuffle32(x3, MASK(0, 3, 2, 1))
  	pshufd		$0x39,%xmm3,%xmm3
  
 -	sub		$2,%r8d
 +	dec		%ecx
  	jnz		.Ldoubleround
  
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
++=======
+ 	RET
+ SYM_FUNC_END(chacha_permute)
+ 
+ SYM_FUNC_START(chacha_block_xor_ssse3)
+ 	# %rdi: Input state matrix, s
+ 	# %rsi: up to 1 data block output, o
+ 	# %rdx: up to 1 data block input, i
+ 	# %rcx: input/output length in bytes
+ 	# %r8d: nrounds
+ 	FRAME_BEGIN
+ 
+ 	# x0..3 = s0..3
+ 	movdqu		0x00(%rdi),%xmm0
+ 	movdqu		0x10(%rdi),%xmm1
+ 	movdqu		0x20(%rdi),%xmm2
+ 	movdqu		0x30(%rdi),%xmm3
+ 	movdqa		%xmm0,%xmm8
+ 	movdqa		%xmm1,%xmm9
+ 	movdqa		%xmm2,%xmm10
+ 	movdqa		%xmm3,%xmm11
+ 
+ 	mov		%rcx,%rax
+ 	call		chacha_permute
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation):arch/x86/crypto/chacha-ssse3-x86_64.S
  	# o0 = i0 ^ (x0 + s0)
 -	paddd		%xmm8,%xmm0
 -	cmp		$0x10,%rax
 -	jl		.Lxorpart
  	movdqu		0x00(%rdx),%xmm4
 +	paddd		%xmm8,%xmm0
  	pxor		%xmm4,%xmm0
  	movdqu		%xmm0,0x00(%rsi)
  	# o1 = i1 ^ (x1 + s1)
 +	movdqu		0x10(%rdx),%xmm5
  	paddd		%xmm9,%xmm1
 -	movdqa		%xmm1,%xmm0
 -	cmp		$0x20,%rax
 -	jl		.Lxorpart
 -	movdqu		0x10(%rdx),%xmm0
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0x10(%rsi)
 +	pxor		%xmm5,%xmm1
 +	movdqu		%xmm1,0x10(%rsi)
  	# o2 = i2 ^ (x2 + s2)
 +	movdqu		0x20(%rdx),%xmm6
  	paddd		%xmm10,%xmm2
 -	movdqa		%xmm2,%xmm0
 -	cmp		$0x30,%rax
 -	jl		.Lxorpart
 -	movdqu		0x20(%rdx),%xmm0
 -	pxor		%xmm2,%xmm0
 -	movdqu		%xmm0,0x20(%rsi)
 +	pxor		%xmm6,%xmm2
 +	movdqu		%xmm2,0x20(%rsi)
  	# o3 = i3 ^ (x3 + s3)
 +	movdqu		0x30(%rdx),%xmm7
  	paddd		%xmm11,%xmm3
 -	movdqa		%xmm3,%xmm0
 -	cmp		$0x40,%rax
 -	jl		.Lxorpart
 -	movdqu		0x30(%rdx),%xmm0
 -	pxor		%xmm3,%xmm0
 -	movdqu		%xmm0,0x30(%rsi)
 +	pxor		%xmm7,%xmm3
 +	movdqu		%xmm3,0x30(%rsi)
  
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
 +	ret
 +ENDPROC(chacha20_block_xor_ssse3)
++=======
+ .Ldone:
+ 	FRAME_END
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation):arch/x86/crypto/chacha-ssse3-x86_64.S
  
 -.Lxorpart:
 -	# xor remaining bytes from partial register into output
 -	mov		%rax,%r9
 -	and		$0x0f,%r9
 -	jz		.Ldone
 -	and		$~0x0f,%rax
 -
 -	mov		%rsi,%r11
 -
 -	lea		8(%rsp),%r10
 -	sub		$0x10,%rsp
 -	and		$~31,%rsp
 -
 -	lea		(%rdx,%rax),%rsi
 -	mov		%rsp,%rdi
 -	mov		%r9,%rcx
 -	rep movsb
 -
 -	pxor		0x00(%rsp),%xmm0
 -	movdqa		%xmm0,0x00(%rsp)
 -
 -	mov		%rsp,%rsi
 -	lea		(%r11,%rax),%rdi
 -	mov		%r9,%rcx
 -	rep movsb
 -
 -	lea		-8(%r10),%rsp
 -	jmp		.Ldone
 -
 -SYM_FUNC_END(chacha_block_xor_ssse3)
 -
 -SYM_FUNC_START(hchacha_block_ssse3)
 +ENTRY(chacha20_4block_xor_ssse3)
  	# %rdi: Input state matrix, s
 -	# %rsi: output (8 32-bit words)
 -	# %edx: nrounds
 -	FRAME_BEGIN
 +	# %rsi: 4 data blocks output, o
 +	# %rdx: 4 data blocks input, i
  
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
 +	# This function encrypts four consecutive ChaCha20 blocks by loading the
++=======
+ 	movdqu		0x00(%rdi),%xmm0
+ 	movdqu		0x10(%rdi),%xmm1
+ 	movdqu		0x20(%rdi),%xmm2
+ 	movdqu		0x30(%rdi),%xmm3
+ 
+ 	mov		%edx,%r8d
+ 	call		chacha_permute
+ 
+ 	movdqu		%xmm0,0x00(%rsi)
+ 	movdqu		%xmm3,0x10(%rsi)
+ 
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(hchacha_block_ssse3)
+ 
+ SYM_FUNC_START(chacha_4block_xor_ssse3)
+ 	# %rdi: Input state matrix, s
+ 	# %rsi: up to 4 data blocks output, o
+ 	# %rdx: up to 4 data blocks input, i
+ 	# %rcx: input/output length in bytes
+ 	# %r8d: nrounds
+ 
+ 	# This function encrypts four consecutive ChaCha blocks by loading the
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation):arch/x86/crypto/chacha-ssse3-x86_64.S
  	# the state matrix in SSE registers four times. As we need some scratch
  	# registers, we save the first four registers on the stack. The
  	# algorithm performs each operation on the corresponding word of each
@@@ -588,43 -736,56 +647,73 @@@
  	movdqu		0xc0(%rdx),%xmm1
  	pxor		%xmm1,%xmm0
  	movdqu		%xmm0,0xc0(%rsi)
 -
 -	movdqu		%xmm7,%xmm0
 -	cmp		$0xe0,%rax
 -	jl		.Lxorpart4
 +	movdqu		0x10(%rdx),%xmm1
 +	pxor		%xmm1,%xmm4
 +	movdqu		%xmm4,0x10(%rsi)
 +	movdqu		0x90(%rdx),%xmm1
 +	pxor		%xmm1,%xmm5
 +	movdqu		%xmm5,0x90(%rsi)
 +	movdqu		0x50(%rdx),%xmm1
 +	pxor		%xmm1,%xmm6
 +	movdqu		%xmm6,0x50(%rsi)
  	movdqu		0xd0(%rdx),%xmm1
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0xd0(%rsi)
 -
 -	movdqu		%xmm11,%xmm0
 -	cmp		$0xf0,%rax
 -	jl		.Lxorpart4
 +	pxor		%xmm1,%xmm7
 +	movdqu		%xmm7,0xd0(%rsi)
 +	movdqu		0x20(%rdx),%xmm1
 +	pxor		%xmm1,%xmm8
 +	movdqu		%xmm8,0x20(%rsi)
 +	movdqu		0xa0(%rdx),%xmm1
 +	pxor		%xmm1,%xmm9
 +	movdqu		%xmm9,0xa0(%rsi)
 +	movdqu		0x60(%rdx),%xmm1
 +	pxor		%xmm1,%xmm10
 +	movdqu		%xmm10,0x60(%rsi)
  	movdqu		0xe0(%rdx),%xmm1
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0xe0(%rsi)
 -
 -	movdqu		%xmm15,%xmm0
 -	cmp		$0x100,%rax
 -	jl		.Lxorpart4
 +	pxor		%xmm1,%xmm11
 +	movdqu		%xmm11,0xe0(%rsi)
 +	movdqu		0x30(%rdx),%xmm1
 +	pxor		%xmm1,%xmm12
 +	movdqu		%xmm12,0x30(%rsi)
 +	movdqu		0xb0(%rdx),%xmm1
 +	pxor		%xmm1,%xmm13
 +	movdqu		%xmm13,0xb0(%rsi)
 +	movdqu		0x70(%rdx),%xmm1
 +	pxor		%xmm1,%xmm14
 +	movdqu		%xmm14,0x70(%rsi)
  	movdqu		0xf0(%rdx),%xmm1
 -	pxor		%xmm1,%xmm0
 -	movdqu		%xmm0,0xf0(%rsi)
 +	pxor		%xmm1,%xmm15
 +	movdqu		%xmm15,0xf0(%rsi)
  
 -.Ldone4:
  	lea		-8(%r10),%rsp
++<<<<<<< HEAD:arch/x86/crypto/chacha20-ssse3-x86_64.S
 +	ret
 +ENDPROC(chacha20_4block_xor_ssse3)
++=======
+ 	RET
+ 
+ .Lxorpart4:
+ 	# xor remaining bytes from partial register into output
+ 	mov		%rax,%r9
+ 	and		$0x0f,%r9
+ 	jz		.Ldone4
+ 	and		$~0x0f,%rax
+ 
+ 	mov		%rsi,%r11
+ 
+ 	lea		(%rdx,%rax),%rsi
+ 	mov		%rsp,%rdi
+ 	mov		%r9,%rcx
+ 	rep movsb
+ 
+ 	pxor		0x00(%rsp),%xmm0
+ 	movdqa		%xmm0,0x00(%rsp)
+ 
+ 	mov		%rsp,%rsi
+ 	lea		(%r11,%rax),%rdi
+ 	mov		%r9,%rcx
+ 	rep movsb
+ 
+ 	jmp		.Ldone4
+ 
+ SYM_FUNC_END(chacha_4block_xor_ssse3)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation):arch/x86/crypto/chacha-ssse3-x86_64.S
diff --cc arch/x86/crypto/crct10dif-pcl-asm_64.S
index bd3f57e00a09,721474abfb71..000000000000
--- a/arch/x86/crypto/crct10dif-pcl-asm_64.S
+++ b/arch/x86/crypto/crct10dif-pcl-asm_64.S
@@@ -71,487 -92,194 +71,514 @@@
  .align 16
  SYM_FUNC_START(crc_t10dif_pcl)
  
 -	movdqa	.Lbswap_mask(%rip), BSWAP_MASK
 -
 -	# For sizes less than 256 bytes, we can't fold 128 bytes at a time.
 -	cmp	$256, len
 -	jl	.Lless_than_256_bytes
 -
 -	# Load the first 128 data bytes.  Byte swapping is necessary to make the
 -	# bit order match the polynomial coefficient order.
 -	movdqu	16*0(buf), %xmm0
 -	movdqu	16*1(buf), %xmm1
 -	movdqu	16*2(buf), %xmm2
 -	movdqu	16*3(buf), %xmm3
 -	movdqu	16*4(buf), %xmm4
 -	movdqu	16*5(buf), %xmm5
 -	movdqu	16*6(buf), %xmm6
 -	movdqu	16*7(buf), %xmm7
 -	add	$128, buf
 -	pshufb	BSWAP_MASK, %xmm0
 -	pshufb	BSWAP_MASK, %xmm1
 -	pshufb	BSWAP_MASK, %xmm2
 -	pshufb	BSWAP_MASK, %xmm3
 -	pshufb	BSWAP_MASK, %xmm4
 -	pshufb	BSWAP_MASK, %xmm5
 -	pshufb	BSWAP_MASK, %xmm6
 -	pshufb	BSWAP_MASK, %xmm7
 +	# adjust the 16-bit initial_crc value, scale it to 32 bits
 +	shl	$16, arg1_low32
 +
 +	# Allocate Stack Space
 +	mov     %rsp, %rcx
 +	sub	$16*2, %rsp
 +	# align stack to 16 byte boundary
 +	and     $~(0x10 - 1), %rsp
 +
 +	# check if smaller than 256
 +	cmp	$256, arg3
 +
 +	# for sizes less than 128, we can't fold 64B at a time...
 +	jl	_less_than_128
 +
 +
 +	# load the initial crc value
 +	movd	arg1_low32, %xmm10	# initial crc
 +
 +	# crc value does not need to be byte-reflected, but it needs
 +	# to be moved to the high part of the register.
 +	# because data will be byte-reflected and will align with
 +	# initial crc at correct place.
 +	pslldq	$12, %xmm10
 +
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +	# receive the initial 64B data, xor the initial crc value
 +	movdqu	16*0(arg2), %xmm0
 +	movdqu	16*1(arg2), %xmm1
 +	movdqu	16*2(arg2), %xmm2
 +	movdqu	16*3(arg2), %xmm3
 +	movdqu	16*4(arg2), %xmm4
 +	movdqu	16*5(arg2), %xmm5
 +	movdqu	16*6(arg2), %xmm6
 +	movdqu	16*7(arg2), %xmm7
 +
 +	pshufb	%xmm11, %xmm0
 +	# XOR the initial_crc value
 +	pxor	%xmm10, %xmm0
 +	pshufb	%xmm11, %xmm1
 +	pshufb	%xmm11, %xmm2
 +	pshufb	%xmm11, %xmm3
 +	pshufb	%xmm11, %xmm4
 +	pshufb	%xmm11, %xmm5
 +	pshufb	%xmm11, %xmm6
 +	pshufb	%xmm11, %xmm7
 +
 +	movdqa	rk3(%rip), %xmm10	#xmm10 has rk3 and rk4
 +					#imm value of pclmulqdq instruction
 +					#will determine which constant to use
 +
 +	#################################################################
 +	# we subtract 256 instead of 128 to save one instruction from the loop
 +	sub	$256, arg3
 +
 +	# at this section of the code, there is 64*x+y (0<=y<64) bytes of
 +	# buffer. The _fold_64_B_loop will fold 64B at a time
 +	# until we have 64+y Bytes of buffer
 +
 +
 +	# fold 64B at a time. This section of the code folds 4 xmm
 +	# registers in parallel
 +_fold_64_B_loop:
 +
 +	# update the buffer pointer
 +	add	$128, arg2		#    buf += 64#
 +
 +	movdqu	16*0(arg2), %xmm9
 +	movdqu	16*1(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm0, %xmm8
 +	movdqa	%xmm1, %xmm13
 +	pclmulqdq	$0x0 , %xmm10, %xmm0
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0 , %xmm10, %xmm1
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm0
 +	xorps	%xmm8 , %xmm0
 +	pxor	%xmm12, %xmm1
 +	xorps	%xmm13, %xmm1
 +
 +	movdqu	16*2(arg2), %xmm9
 +	movdqu	16*3(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm2, %xmm8
 +	movdqa	%xmm3, %xmm13
 +	pclmulqdq	$0x0, %xmm10, %xmm2
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0, %xmm10, %xmm3
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm2
 +	xorps	%xmm8 , %xmm2
 +	pxor	%xmm12, %xmm3
 +	xorps	%xmm13, %xmm3
 +
 +	movdqu	16*4(arg2), %xmm9
 +	movdqu	16*5(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm4, %xmm8
 +	movdqa	%xmm5, %xmm13
 +	pclmulqdq	$0x0,  %xmm10, %xmm4
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0,  %xmm10, %xmm5
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 ,  %xmm4
 +	xorps	%xmm8 ,  %xmm4
 +	pxor	%xmm12,  %xmm5
 +	xorps	%xmm13,  %xmm5
 +
 +	movdqu	16*6(arg2), %xmm9
 +	movdqu	16*7(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm6 , %xmm8
 +	movdqa	%xmm7 , %xmm13
 +	pclmulqdq	$0x0 , %xmm10, %xmm6
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0 , %xmm10, %xmm7
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm6
 +	xorps	%xmm8 , %xmm6
 +	pxor	%xmm12, %xmm7
 +	xorps	%xmm13, %xmm7
 +
 +	sub	$128, arg3
 +
 +	# check if there is another 64B in the buffer to be able to fold
 +	jge	_fold_64_B_loop
 +	##################################################################
 +
 +
 +	add	$128, arg2
 +	# at this point, the buffer pointer is pointing at the last y Bytes
 +	# of the buffer the 64B of folded data is in 4 of the xmm
 +	# registers: xmm0, xmm1, xmm2, xmm3
 +
 +
 +	# fold the 8 xmm registers to 1 xmm register with different constants
 +
 +	movdqa	rk9(%rip), %xmm10
 +	movdqa	%xmm0, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm0
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm0, %xmm7
  
 -	# XOR the first 16 data *bits* with the initial CRC value.
 -	pxor	%xmm8, %xmm8
 -	pinsrw	$7, init_crc, %xmm8
 -	pxor	%xmm8, %xmm0
 -
 -	movdqa	.Lfold_across_128_bytes_consts(%rip), FOLD_CONSTS
 -
 -	# Subtract 128 for the 128 data bytes just consumed.  Subtract another
 -	# 128 to simplify the termination condition of the following loop.
 -	sub	$256, len
 -
 -	# While >= 128 data bytes remain (not counting xmm0-7), fold the 128
 -	# bytes xmm0-7 into them, storing the result back into xmm0-7.
 -.Lfold_128_bytes_loop:
 -	fold_32_bytes	0, %xmm0, %xmm1
 -	fold_32_bytes	32, %xmm2, %xmm3
 -	fold_32_bytes	64, %xmm4, %xmm5
 -	fold_32_bytes	96, %xmm6, %xmm7
 -	add	$128, buf
 -	sub	$128, len
 -	jge	.Lfold_128_bytes_loop
 -
 -	# Now fold the 112 bytes in xmm0-xmm6 into the 16 bytes in xmm7.
 -
 -	# Fold across 64 bytes.
 -	movdqa	.Lfold_across_64_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm0, %xmm4
 -	fold_16_bytes	%xmm1, %xmm5
 -	fold_16_bytes	%xmm2, %xmm6
 -	fold_16_bytes	%xmm3, %xmm7
 -	# Fold across 32 bytes.
 -	movdqa	.Lfold_across_32_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm4, %xmm6
 -	fold_16_bytes	%xmm5, %xmm7
 -	# Fold across 16 bytes.
 -	movdqa	.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm6, %xmm7
 -
 -	# Add 128 to get the correct number of data bytes remaining in 0...127
 -	# (not counting xmm7), following the previous extra subtraction by 128.
 -	# Then subtract 16 to simplify the termination condition of the
 -	# following loop.
 -	add	$128-16, len
 -
 -	# While >= 16 data bytes remain (not counting xmm7), fold the 16 bytes
 -	# xmm7 into them, storing the result back into xmm7.
 -	jl	.Lfold_16_bytes_loop_done
 -.Lfold_16_bytes_loop:
 +	movdqa	rk11(%rip), %xmm10
 +	movdqa	%xmm1, %xmm8
 +	pclmulqdq	 $0x11, %xmm10, %xmm1
 +	pclmulqdq	 $0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm1, %xmm7
 +
 +	movdqa	rk13(%rip), %xmm10
 +	movdqa	%xmm2, %xmm8
 +	pclmulqdq	 $0x11, %xmm10, %xmm2
 +	pclmulqdq	 $0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm2, %xmm7
 +
 +	movdqa	rk15(%rip), %xmm10
 +	movdqa	%xmm3, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm3
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm3, %xmm7
 +
 +	movdqa	rk17(%rip), %xmm10
 +	movdqa	%xmm4, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm4
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm4, %xmm7
 +
 +	movdqa	rk19(%rip), %xmm10
 +	movdqa	%xmm5, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm5
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm5, %xmm7
 +
 +	movdqa	rk1(%rip), %xmm10	#xmm10 has rk1 and rk2
 +					#imm value of pclmulqdq instruction
 +					#will determine which constant to use
 +	movdqa	%xmm6, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm6
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm6, %xmm7
 +
 +
 +	# instead of 64, we add 48 to the loop counter to save 1 instruction
 +	# from the loop instead of a cmp instruction, we use the negative
 +	# flag with the jl instruction
 +	add	$128-16, arg3
 +	jl	_final_reduction_for_128
 +
 +	# now we have 16+y bytes left to reduce. 16 Bytes is in register xmm7
 +	# and the rest is in memory. We can fold 16 bytes at a time if y>=16
 +	# continue folding 16B at a time
 +
 +_16B_reduction_loop:
  	movdqa	%xmm7, %xmm8
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
  	pxor	%xmm8, %xmm7
 -	movdqu	(buf), %xmm0
 -	pshufb	BSWAP_MASK, %xmm0
 +	movdqu	(arg2), %xmm0
 +	pshufb	%xmm11, %xmm0
  	pxor	%xmm0 , %xmm7
 -	add	$16, buf
 -	sub	$16, len
 -	jge	.Lfold_16_bytes_loop
 -
 -.Lfold_16_bytes_loop_done:
 -	# Add 16 to get the correct number of data bytes remaining in 0...15
 -	# (not counting xmm7), following the previous extra subtraction by 16.
 -	add	$16, len
 -	je	.Lreduce_final_16_bytes
 -
 -.Lhandle_partial_segment:
 -	# Reduce the last '16 + len' bytes where 1 <= len <= 15 and the first 16
 -	# bytes are in xmm7 and the rest are the remaining data in 'buf'.  To do
 -	# this without needing a fold constant for each possible 'len', redivide
 -	# the bytes into a first chunk of 'len' bytes and a second chunk of 16
 -	# bytes, then fold the first chunk into the second.
 -
 +	add	$16, arg2
 +	sub	$16, arg3
 +	# instead of a cmp instruction, we utilize the flags with the
 +	# jge instruction equivalent of: cmp arg3, 16-16
 +	# check if there is any more 16B in the buffer to be able to fold
 +	jge	_16B_reduction_loop
 +
 +	#now we have 16+z bytes left to reduce, where 0<= z < 16.
 +	#first, we reduce the data in the xmm7 register
 +
 +
 +_final_reduction_for_128:
 +	# check if any more data to fold. If not, compute the CRC of
 +	# the final 128 bits
 +	add	$16, arg3
 +	je	_128_done
 +
 +	# here we are getting data that is less than 16 bytes.
 +	# since we know that there was data before the pointer, we can
 +	# offset the input pointer before the actual point, to receive
 +	# exactly 16 bytes. after that the registers need to be adjusted.
 +_get_last_two_xmms:
  	movdqa	%xmm7, %xmm2
  
 -	# xmm1 = last 16 original data bytes
 -	movdqu	-16(buf, len), %xmm1
 -	pshufb	BSWAP_MASK, %xmm1
 +	movdqu	-16(arg2, arg3), %xmm1
 +	pshufb	%xmm11, %xmm1
  
 -	# xmm2 = high order part of second chunk: xmm7 left-shifted by 'len' bytes.
 -	lea	.Lbyteshift_table+16(%rip), %rax
 -	sub	len, %rax
 +	# get rid of the extra data that was loaded before
 +	# load the shift constant
 +	lea	pshufb_shf_table+16(%rip), %rax
 +	sub	arg3, %rax
  	movdqu	(%rax), %xmm0
 +
 +	# shift xmm2 to the left by arg3 bytes
  	pshufb	%xmm0, %xmm2
  
 -	# xmm7 = first chunk: xmm7 right-shifted by '16-len' bytes.
 -	pxor	.Lmask1(%rip), %xmm0
 +	# shift xmm7 to the right by 16-arg3 bytes
 +	pxor	mask1(%rip), %xmm0
  	pshufb	%xmm0, %xmm7
 -
 -	# xmm1 = second chunk: 'len' bytes from xmm1 (low-order bytes),
 -	# then '16-len' bytes from xmm2 (high-order bytes).
  	pblendvb	%xmm2, %xmm1	#xmm0 is implicit
  
 -	# Fold the first chunk into the second chunk, storing the result in xmm7.
 +	# fold 16 Bytes
 +	movdqa	%xmm1, %xmm2
  	movdqa	%xmm7, %xmm8
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
  	pxor	%xmm8, %xmm7
 -	pxor	%xmm1, %xmm7
 -
 -.Lreduce_final_16_bytes:
 -	# Reduce the 128-bit value M(x), stored in xmm7, to the final 16-bit CRC
 -
 -	# Load 'x^48 * (x^48 mod G(x))' and 'x^48 * (x^80 mod G(x))'.
 -	movdqa	.Lfinal_fold_consts(%rip), FOLD_CONSTS
 +	pxor	%xmm2, %xmm7
  
 -	# Fold the high 64 bits into the low 64 bits, while also multiplying by
 -	# x^64.  This produces a 128-bit value congruent to x^64 * M(x) and
 -	# whose low 48 bits are 0.
 +_128_done:
 +	# compute crc of a 128-bit value
 +	movdqa	rk5(%rip), %xmm10	# rk5 and rk6 in xmm10
  	movdqa	%xmm7, %xmm0
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7 # high bits * x^48 * (x^80 mod G(x))
 -	pslldq	$8, %xmm0
 -	pxor	%xmm0, %xmm7			  # + low bits * x^64
  
 -	# Fold the high 32 bits into the low 96 bits.  This produces a 96-bit
 -	# value congruent to x^64 * M(x) and whose low 48 bits are 0.
 +	#64b fold
 +	pclmulqdq	$0x1, %xmm10, %xmm7
 +	pslldq	$8   ,  %xmm0
 +	pxor	%xmm0,  %xmm7
 +
 +	#32b fold
  	movdqa	%xmm7, %xmm0
 -	pand	.Lmask2(%rip), %xmm0		  # zero high 32 bits
 -	psrldq	$12, %xmm7			  # extract high 32 bits
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm7 # high 32 bits * x^48 * (x^48 mod G(x))
 -	pxor	%xmm0, %xmm7			  # + low bits
  
 -	# Load G(x) and floor(x^48 / G(x)).
 -	movdqa	.Lbarrett_reduction_consts(%rip), FOLD_CONSTS
 +	pand	mask2(%rip), %xmm0
  
++<<<<<<< HEAD
 +	psrldq	$12, %xmm7
 +	pclmulqdq	$0x10, %xmm10, %xmm7
++=======
+ 	# Use Barrett reduction to compute the final CRC value.
+ 	movdqa	%xmm7, %xmm0
+ 	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7 # high 32 bits * floor(x^48 / G(x))
+ 	psrlq	$32, %xmm7			  # /= x^32
+ 	pclmulqdq	$0x00, FOLD_CONSTS, %xmm7 # *= G(x)
+ 	psrlq	$48, %xmm0
+ 	pxor	%xmm7, %xmm0		     # + low 16 nonzero bits
+ 	# Final CRC value (x^16 * M(x)) mod G(x) is in low 16 bits of xmm0.
+ 
+ 	pextrw	$0, %xmm0, %eax
+ 	RET
+ 
+ .align 16
+ .Lless_than_256_bytes:
+ 	# Checksumming a buffer of length 16...255 bytes
+ 
+ 	# Load the first 16 data bytes.
+ 	movdqu	(buf), %xmm7
+ 	pshufb	BSWAP_MASK, %xmm7
+ 	add	$16, buf
+ 
+ 	# XOR the first 16 data *bits* with the initial CRC value.
+ 	pxor	%xmm0, %xmm0
+ 	pinsrw	$7, init_crc, %xmm0
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +	pxor	%xmm0, %xmm7
 +
 +	#barrett reduction
 +_barrett:
 +	movdqa	rk7(%rip), %xmm10	# rk7 and rk8 in xmm10
 +	movdqa	%xmm7, %xmm0
 +	pclmulqdq	$0x01, %xmm10, %xmm7
 +	pslldq	$4, %xmm7
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +
 +	pslldq	$4, %xmm7
 +	pxor	%xmm0, %xmm7
 +	pextrd	$1, %xmm7, %eax
 +
 +_cleanup:
 +	# scale the result back to 16 bits
 +	shr	$16, %eax
 +	mov     %rcx, %rsp
 +	ret
 +
 +########################################################################
 +
 +.align 16
 +_less_than_128:
 +
 +	# check if there is enough buffer to be able to fold 16B at a time
 +	cmp	$32, arg3
 +	jl	_less_than_32
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +
 +	# now if there is, load the constants
 +	movdqa	rk1(%rip), %xmm10	# rk1 and rk2 in xmm10
 +
 +	movd	arg1_low32, %xmm0	# get the initial crc value
 +	pslldq	$12, %xmm0	# align it to its correct place
 +	movdqu	(arg2), %xmm7	# load the plaintext
 +	pshufb	%xmm11, %xmm7	# byte-reflect the plaintext
  	pxor	%xmm0, %xmm7
  
 -	movdqa	.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS
 -	cmp	$16, len
 -	je	.Lreduce_final_16_bytes		# len == 16
 -	sub	$32, len
 -	jge	.Lfold_16_bytes_loop		# 32 <= len <= 255
 -	add	$16, len
 -	jmp	.Lhandle_partial_segment	# 17 <= len <= 31
 +
 +	# update the buffer pointer
 +	add	$16, arg2
 +
 +	# update the counter. subtract 32 instead of 16 to save one
 +	# instruction from the loop
 +	sub	$32, arg3
 +
 +	jmp	_16B_reduction_loop
 +
 +
 +.align 16
 +_less_than_32:
 +	# mov initial crc to the return value. this is necessary for
 +	# zero-length buffers.
 +	mov	arg1_low32, %eax
 +	test	arg3, arg3
 +	je	_cleanup
 +
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +
 +	movd	arg1_low32, %xmm0	# get the initial crc value
 +	pslldq	$12, %xmm0	# align it to its correct place
 +
 +	cmp	$16, arg3
 +	je	_exact_16_left
 +	jl	_less_than_16_left
 +
 +	movdqu	(arg2), %xmm7	# load the plaintext
 +	pshufb	%xmm11, %xmm7	# byte-reflect the plaintext
 +	pxor	%xmm0 , %xmm7	# xor the initial crc value
 +	add	$16, arg2
 +	sub	$16, arg3
 +	movdqa	rk1(%rip), %xmm10	# rk1 and rk2 in xmm10
 +	jmp	_get_last_two_xmms
 +
 +
 +.align 16
 +_less_than_16_left:
 +	# use stack space to load data less than 16 bytes, zero-out
 +	# the 16B in memory first.
 +
 +	pxor	%xmm1, %xmm1
 +	mov	%rsp, %r11
 +	movdqa	%xmm1, (%r11)
 +
 +	cmp	$4, arg3
 +	jl	_only_less_than_4
 +
 +	# backup the counter value
 +	mov	arg3, %r9
 +	cmp	$8, arg3
 +	jl	_less_than_8_left
 +
 +	# load 8 Bytes
 +	mov	(arg2), %rax
 +	mov	%rax, (%r11)
 +	add	$8, %r11
 +	sub	$8, arg3
 +	add	$8, arg2
 +_less_than_8_left:
 +
 +	cmp	$4, arg3
 +	jl	_less_than_4_left
 +
 +	# load 4 Bytes
 +	mov	(arg2), %eax
 +	mov	%eax, (%r11)
 +	add	$4, %r11
 +	sub	$4, arg3
 +	add	$4, arg2
 +_less_than_4_left:
 +
 +	cmp	$2, arg3
 +	jl	_less_than_2_left
 +
 +	# load 2 Bytes
 +	mov	(arg2), %ax
 +	mov	%ax, (%r11)
 +	add	$2, %r11
 +	sub	$2, arg3
 +	add	$2, arg2
 +_less_than_2_left:
 +	cmp     $1, arg3
 +        jl      _zero_left
 +
 +	# load 1 Byte
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +_zero_left:
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7	# xor the initial crc value
 +
 +	# shl r9, 4
 +	lea	pshufb_shf_table+16(%rip), %rax
 +	sub	%r9, %rax
 +	movdqu	(%rax), %xmm0
 +	pxor	mask1(%rip), %xmm0
 +
 +	pshufb	%xmm0, %xmm7
 +	jmp	_128_done
 +
 +.align 16
 +_exact_16_left:
 +	movdqu	(arg2), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	jmp	_128_done
 +
 +_only_less_than_4:
 +	cmp	$3, arg3
 +	jl	_only_less_than_3
 +
 +	# load 3 Bytes
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	mov	1(arg2), %al
 +	mov	%al, 1(%r11)
 +
 +	mov	2(arg2), %al
 +	mov	%al, 2(%r11)
 +
 +	movdqa	 (%rsp), %xmm7
 +	pshufb	 %xmm11, %xmm7
 +	pxor	 %xmm0 , %xmm7  # xor the initial crc value
 +
 +	psrldq	$5, %xmm7
 +
 +	jmp	_barrett
 +_only_less_than_3:
 +	cmp	$2, arg3
 +	jl	_only_less_than_2
 +
 +	# load 2 Bytes
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	mov	1(arg2), %al
 +	mov	%al, 1(%r11)
 +
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	psrldq	$6, %xmm7
 +
 +	jmp	_barrett
 +_only_less_than_2:
 +
 +	# load 1 Byte
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	psrldq	$7, %xmm7
 +
 +	jmp	_barrett
 +
  SYM_FUNC_END(crc_t10dif_pcl)
  
  .section	.rodata, "a", @progbits
diff --cc arch/x86/crypto/serpent-sse2-i586-asm_32.S
index d348f1553a79,8ccb03ad7cef..000000000000
--- a/arch/x86/crypto/serpent-sse2-i586-asm_32.S
+++ b/arch/x86/crypto/serpent-sse2-i586-asm_32.S
@@@ -573,10 -558,10 +573,15 @@@ ENTRY(__serpent_enc_blk_4way
  .L__enc_xor4:
  	xor_blocks(%eax, RA, RB, RC, RD, RT0, RT1, RE);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__serpent_enc_blk_4way)
++=======
+ 	RET;
+ SYM_FUNC_END(__serpent_enc_blk_4way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_dec_blk_4way)
 +ENTRY(serpent_dec_blk_4way)
  	/* input:
  	 *	arg_ctx(%esp): ctx, CTX
  	 *	arg_dst(%esp): dst
@@@ -627,5 -612,5 +632,10 @@@
  	movl arg_dst(%esp), %eax;
  	write_blocks(%eax, RC, RD, RB, RE, RT0, RT1, RA);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_dec_blk_4way)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_dec_blk_4way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/sha512-avx2-asm.S
index 80d830e7ee09,5cdaab7d6901..000000000000
--- a/arch/x86/crypto/sha512-avx2-asm.S
+++ b/arch/x86/crypto/sha512-avx2-asm.S
@@@ -672,16 -668,18 +672,30 @@@ loop2
  
  done_hash:
  
 +# Restore GPRs
 +	mov	8*0+frame_GPRSAVE(%rsp), %rbx
 +	mov	8*1+frame_GPRSAVE(%rsp), %r12
 +	mov	8*2+frame_GPRSAVE(%rsp), %r13
 +	mov	8*3+frame_GPRSAVE(%rsp), %r14
 +	mov	8*4+frame_GPRSAVE(%rsp), %r15
 +
  	# Restore Stack Pointer
++<<<<<<< HEAD
 +	mov	frame_RSPSAVE(%rsp), %rsp
 +	ret
++=======
+ 	mov	%rbp, %rsp
+ 	pop	%rbp
+ 
+ 	# Restore GPRs
+ 	pop	%r15
+ 	pop	%r14
+ 	pop	%r13
+ 	pop	%r12
+ 	pop	%rbx
+ 
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END(sha512_transform_rorx)
  
  ########################################################################
diff --cc arch/x86/crypto/twofish-i586-asm_32.S
index 694ea4587ba7,3abcad661884..000000000000
--- a/arch/x86/crypto/twofish-i586-asm_32.S
+++ b/arch/x86/crypto/twofish-i586-asm_32.S
@@@ -273,10 -260,10 +273,15 @@@ ENTRY(twofish_enc_blk
  	pop	%ebx
  	pop	%ebp
  	mov	$1,	%eax
++<<<<<<< HEAD
 +	ret
 +ENDPROC(twofish_enc_blk)
++=======
+ 	RET
+ SYM_FUNC_END(twofish_enc_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(twofish_dec_blk)
 +ENTRY(twofish_dec_blk)
  	push	%ebp			/* save registers according to calling convention*/
  	push    %ebx
  	push    %esi
@@@ -330,5 -317,5 +335,10 @@@
  	pop	%ebx
  	pop	%ebp
  	mov	$1,	%eax
++<<<<<<< HEAD
 +	ret
 +ENDPROC(twofish_dec_blk)
++=======
+ 	RET
+ SYM_FUNC_END(twofish_dec_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/entry/entry_32.S
index ea11e444c691,00413e37feee..000000000000
--- a/arch/x86/entry/entry_32.S
+++ b/arch/x86/entry/entry_32.S
@@@ -775,8 -740,10 +775,15 @@@ ENTRY(schedule_tail_wrapper
  	popl	%eax
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(schedule_tail_wrapper)
++=======
+ 	RET
+ SYM_FUNC_END(schedule_tail_wrapper)
+ .popsection
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  /*
   * A newly forked process directly context switches into this address.
   *
diff --cc arch/x86/entry/entry_64.S
index f6cd4f44ad1c,e23319ad3f42..000000000000
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@@ -1082,13 -736,11 +1082,19 @@@ SYM_FUNC_START(native_load_gs_index
  .Lgs_change:
  	movl	%edi, %gs
  2:	ALTERNATIVE "", "mfence", X86_BUG_SWAPGS_FENCE
 -	swapgs
 +	SWAPGS
 +	TRACE_IRQS_FLAGS (%rsp)
 +	popfq
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +SYM_FUNC_END(native_load_gs_index)
 +EXPORT_SYMBOL(native_load_gs_index)
++=======
+ 	RET
+ SYM_FUNC_END(asm_load_gs_index)
+ EXPORT_SYMBOL(asm_load_gs_index)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	_ASM_EXTABLE(.Lgs_change, .Lbad_gs)
  	.section .fixup, "ax"
@@@ -1289,7 -889,7 +1295,11 @@@ ENTRY(paranoid_entry
  	 * is needed here.
  	 */
  	SAVE_AND_SET_GSBASE scratch_reg=%rax save_reg=%rbx
++<<<<<<< HEAD
 +	jmp	.Librs_entry
++=======
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .Lparanoid_entry_checkgs:
  	/* EBX = 1 -> kernel GSBASE active, no restore required */
@@@ -1316,11 -906,12 +1326,17 @@@
  
  	/* EBX = 0 -> SWAPGS required on exit */
  	xorl	%ebx, %ebx
 -	swapgs
 -.Lparanoid_kernel_gsbase:
  
++<<<<<<< HEAD
 +.Librs_entry:
 +	IBRS_ENTRY_SAVE_AND_CLOBBER  save_reg=%r13d
 +	ret
 +END(paranoid_entry)
++=======
+ 	FENCE_SWAPGS_KERNEL_ENTRY
+ 	RET
+ SYM_CODE_END(paranoid_entry)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * "Paranoid" exit path from exception stack.  This is invoked
@@@ -1403,21 -989,7 +1419,25 @@@ ENTRY(error_entry
  	movq	%rax, %rsp			/* switch stack */
  	ENCODE_FRAME_POINTER
  	pushq	%r12
++<<<<<<< HEAD
 +
 +	/*
 +	 * We need to tell lockdep that IRQs are off.  We can't do this until
 +	 * we fix gsbase, and we should do it before enter_from_user_mode
 +	 * (which can take locks).
 +	 */
 +	TRACE_IRQS_OFF
 +	CALL_enter_from_user_mode
 +	ret
 +
 +.Lerror_entry_done_lfence:
 +	FENCE_SWAPGS_KERNEL_ENTRY
 +.Lerror_entry_done:
 +	TRACE_IRQS_OFF
 +	ret
++=======
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	/*
  	 * There are two places in the kernel that can potentially fault with
@@@ -1441,10 -1013,14 +1461,21 @@@
  	 * .Lgs_change's error handler with kernel gsbase.
  	 */
  	SWAPGS
++<<<<<<< HEAD
 +	FENCE_SWAPGS_USER_ENTRY
 +	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax
 +	IBRS_ENTRY
 +	jmp .Lerror_entry_done
++=======
+ 
+ 	/*
+ 	 * Issue an LFENCE to prevent GS speculation, regardless of whether it is a
+ 	 * kernel or user gsbase.
+ 	 */
+ .Lerror_entry_done_lfence:
+ 	FENCE_SWAPGS_KERNEL_ENTRY
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .Lbstep_iret:
  	/* Fix truncated RIP */
diff --cc arch/x86/entry/thunk_64.S
index b211ebc8e586,505b488fcc65..000000000000
--- a/arch/x86/entry/thunk_64.S
+++ b/arch/x86/entry/thunk_64.S
@@@ -67,6 -50,7 +67,12 @@@
  	popq %rsi
  	popq %rdi
  	popq %rbp
++<<<<<<< HEAD
 +	ret
 +	_ASM_NOKPROBE(.L_restore)
++=======
+ 	RET
+ 	_ASM_NOKPROBE(__thunk_restore)
+ SYM_CODE_END(__thunk_restore)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #endif
diff --cc arch/x86/kernel/acpi/wakeup_32.S
index 4203d4f0c68d,cf69081073b5..000000000000
--- a/arch/x86/kernel/acpi/wakeup_32.S
+++ b/arch/x86/kernel/acpi/wakeup_32.S
@@@ -69,9 -70,9 +69,9 @@@ restore_registers
  	movl	saved_context_edi, %edi
  	pushl	saved_context_eflags
  	popfl
- 	ret
+ 	RET
  
 -SYM_CODE_START(do_suspend_lowlevel)
 +ENTRY(do_suspend_lowlevel)
  	call	save_processor_state
  	call	save_registers
  	pushl	$3
@@@ -85,7 -86,8 +85,12 @@@
  ret_point:
  	call	restore_registers
  	call	restore_processor_state
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(do_suspend_lowlevel)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .data
  ALIGN
diff --cc arch/x86/kernel/ftrace_32.S
index e0061dc976e1,a0ed0e4a2c0c..000000000000
--- a/arch/x86/kernel/ftrace_32.S
+++ b/arch/x86/kernel/ftrace_32.S
@@@ -21,11 -18,12 +21,18 @@@ EXPORT_SYMBOL(__fentry__
  # define MCOUNT_FRAME			0	/* using frame = false */
  #endif
  
++<<<<<<< HEAD
 +ENTRY(function_hook)
 +	ret
 +END(function_hook)
++=======
+ SYM_FUNC_START(__fentry__)
+ 	RET
+ SYM_FUNC_END(__fentry__)
+ EXPORT_SYMBOL(__fentry__)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_CODE_START(ftrace_caller)
 +ENTRY(ftrace_caller)
  
  #ifdef CONFIG_FRAME_POINTER
  	/*
@@@ -85,11 -83,11 +92,17 @@@ ftrace_graph_call
  #endif
  
  /* This is weak to keep gas from relaxing the jumps */
++<<<<<<< HEAD
 +WEAK(ftrace_stub)
 +	ret
 +END(ftrace_caller)
++=======
+ SYM_INNER_LABEL_ALIGN(ftrace_stub, SYM_L_WEAK)
+ 	RET
+ SYM_CODE_END(ftrace_caller)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_CODE_START(ftrace_regs_caller)
 +ENTRY(ftrace_regs_caller)
  	/*
  	 * We're here from an mcount/fentry CALL, and the stack frame looks like:
  	 *
@@@ -178,8 -177,8 +191,13 @@@ ENTRY(ftrace_graph_caller
  	popl	%edx
  	popl	%ecx
  	popl	%eax
++<<<<<<< HEAD
 +	ret
 +END(ftrace_graph_caller)
++=======
+ 	RET
+ SYM_CODE_END(ftrace_graph_caller)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .globl return_to_handler
  return_to_handler:
diff --cc arch/x86/kernel/ftrace_64.S
index 8144bb7316a4,11ac028e30e4..000000000000
--- a/arch/x86/kernel/ftrace_64.S
+++ b/arch/x86/kernel/ftrace_64.S
@@@ -134,9 -131,10 +134,16 @@@ EXPORT_SYMBOL(__fentry__
  
  #ifdef CONFIG_DYNAMIC_FTRACE
  
++<<<<<<< HEAD
 +SYM_FUNC_START(function_hook)
 +	retq
 +SYM_FUNC_END(function_hook)
++=======
+ SYM_FUNC_START(__fentry__)
+ 	RET
+ SYM_FUNC_END(__fentry__)
+ EXPORT_SYMBOL(__fentry__)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  SYM_FUNC_START(ftrace_caller)
  	/* save_mcount_regs fills in first two parameters */
@@@ -162,17 -168,20 +169,29 @@@ SYM_INNER_LABEL(ftrace_call, SYM_L_GLOB
  	 * think twice before adding any new code or changing the
  	 * layout here.
  	 */
 -SYM_INNER_LABEL(ftrace_caller_end, SYM_L_GLOBAL)
 +SYM_INNER_LABEL(ftrace_epilogue, SYM_L_GLOBAL)
  
 -	jmp ftrace_epilogue
 -SYM_FUNC_END(ftrace_caller);
 +#ifdef CONFIG_FUNCTION_GRAPH_TRACER
 +SYM_INNER_LABEL(ftrace_graph_call, SYM_L_GLOBAL)
 +	jmp ftrace_stub
 +#endif
  
++<<<<<<< HEAD
 +/* This is weak to keep gas from relaxing the jumps */
 +WEAK(ftrace_stub)
 +	retq
 +SYM_FUNC_END(ftrace_caller)
++=======
+ SYM_FUNC_START(ftrace_epilogue)
+ /*
+  * This is weak to keep gas from relaxing the jumps.
+  * It is also used to copy the RET for trampolines.
+  */
+ SYM_INNER_LABEL_ALIGN(ftrace_stub, SYM_L_WEAK)
+ 	UNWIND_HINT_FUNC
+ 	RET
+ SYM_FUNC_END(ftrace_epilogue)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  SYM_FUNC_START(ftrace_regs_caller)
  	/* Save the current flags before any operations that can change them */
@@@ -282,17 -283,8 +301,17 @@@ SYM_FUNC_START(function_hook
  	cmpq $ftrace_stub, ftrace_trace_function
  	jnz trace
  
 +fgraph_trace:
 +#ifdef CONFIG_FUNCTION_GRAPH_TRACER
 +	cmpq $ftrace_stub, ftrace_graph_return
 +	jnz ftrace_graph_caller
 +
 +	cmpq $ftrace_graph_entry_stub, ftrace_graph_entry
 +	jnz ftrace_graph_caller
 +#endif
 +
  SYM_INNER_LABEL(ftrace_stub, SYM_L_GLOBAL)
- 	retq
+ 	RET
  
  trace:
  	/* save_mcount_regs fills in first two parameters */
diff --cc arch/x86/kernel/verify_cpu.S
index 3d3c2f71f617,1258a5872d12..000000000000
--- a/arch/x86/kernel/verify_cpu.S
+++ b/arch/x86/kernel/verify_cpu.S
@@@ -138,5 -136,5 +138,10 @@@ ENTRY(verify_cpu
  .Lverify_cpu_sse_ok:
  	popf				# Restore caller passed flags
  	xorl %eax, %eax
++<<<<<<< HEAD
 +	ret
 +ENDPROC(verify_cpu)
++=======
+ 	RET
+ SYM_FUNC_END(verify_cpu)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/atomic64_386_32.S
index 9b0ca8fe80fc,e768815e58ae..000000000000
--- a/arch/x86/lib/atomic64_386_32.S
+++ b/arch/x86/lib/atomic64_386_32.S
@@@ -32,19 -28,16 +32,25 @@@ ENTRY(atomic64_##op##_386); 
  
  #define ENDP endp
  
++<<<<<<< HEAD
 +#define RET \
 +	UNLOCK v; \
 +	ret
++=======
+ #define RET_IRQ_RESTORE \
+ 	IRQ_RESTORE v; \
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +#define RET_ENDP \
 +	RET; \
 +	ENDP
  
  #define v %ecx
 -BEGIN_IRQ_SAVE(read)
 +BEGIN(read)
  	movl  (v), %eax
  	movl 4(v), %edx
 -	RET_IRQ_RESTORE
 -ENDP
 +RET_ENDP
  #undef v
  
  #define v %esi
diff --cc arch/x86/lib/atomic64_cx8_32.S
index db3ae85440ff,90afb488b396..000000000000
--- a/arch/x86/lib/atomic64_cx8_32.S
+++ b/arch/x86/lib/atomic64_cx8_32.S
@@@ -20,32 -16,32 +20,47 @@@
  	cmpxchg8b (\reg)
  .endm
  
 -SYM_FUNC_START(atomic64_read_cx8)
 +ENTRY(atomic64_read_cx8)
  	read64 %ecx
++<<<<<<< HEAD
 +	ret
 +ENDPROC(atomic64_read_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_read_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(atomic64_set_cx8)
 +ENTRY(atomic64_set_cx8)
  1:
  /* we don't need LOCK_PREFIX since aligned 64-bit writes
   * are atomic on 586 and newer */
  	cmpxchg8b (%esi)
  	jne 1b
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(atomic64_set_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_set_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(atomic64_xchg_cx8)
 +ENTRY(atomic64_xchg_cx8)
  1:
  	LOCK_PREFIX
  	cmpxchg8b (%esi)
  	jne 1b
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(atomic64_xchg_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_xchg_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .macro addsub_return func ins insc
 -SYM_FUNC_START(atomic64_\func\()_return_cx8)
 +ENTRY(atomic64_\func\()_return_cx8)
  	pushl %ebp
  	pushl %ebx
  	pushl %esi
@@@ -72,8 -68,8 +87,13 @@@
  	popl %esi
  	popl %ebx
  	popl %ebp
++<<<<<<< HEAD
 +	ret
 +ENDPROC(atomic64_\func\()_return_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_\func\()_return_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  .endm
  
  addsub_return add add adc
@@@ -97,8 -93,8 +117,13 @@@ ENTRY(atomic64_\func\()_return_cx8
  	movl %ebx, %eax
  	movl %ecx, %edx
  	popl %ebx
++<<<<<<< HEAD
 +	ret
 +ENDPROC(atomic64_\func\()_return_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_\func\()_return_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  .endm
  
  incdec_return inc add adc
@@@ -122,10 -118,10 +147,15 @@@ ENTRY(atomic64_dec_if_positive_cx8
  	movl %ebx, %eax
  	movl %ecx, %edx
  	popl %ebx
++<<<<<<< HEAD
 +	ret
 +ENDPROC(atomic64_dec_if_positive_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_dec_if_positive_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(atomic64_add_unless_cx8)
 +ENTRY(atomic64_add_unless_cx8)
  	pushl %ebp
  	pushl %ebx
  /* these just push these two parameters on the stack */
@@@ -180,5 -176,5 +210,10 @@@ ENTRY(atomic64_inc_not_zero_cx8
  	movl $1, %eax
  3:
  	popl %ebx
++<<<<<<< HEAD
 +	ret
 +ENDPROC(atomic64_inc_not_zero_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_inc_not_zero_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/checksum_32.S
index db1d1dd5ae35,929ad1747dea..000000000000
--- a/arch/x86/lib/checksum_32.S
+++ b/arch/x86/lib/checksum_32.S
@@@ -131,8 -127,8 +131,13 @@@ ENTRY(csum_partial
  8:
  	popl %ebx
  	popl %esi
++<<<<<<< HEAD
 +	ret
 +ENDPROC(csum_partial)
++=======
+ 	RET
+ SYM_FUNC_END(csum_partial)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  #else
  
@@@ -249,8 -245,8 +254,13 @@@ ENTRY(csum_partial
  90: 
  	popl %ebx
  	popl %esi
++<<<<<<< HEAD
 +	ret
 +ENDPROC(csum_partial)
++=======
+ 	RET
+ SYM_FUNC_END(csum_partial)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  				
  #endif
  EXPORT_SYMBOL(csum_partial)
diff --cc arch/x86/lib/copy_page_64.S
index 7d2148c14713,30ea644bf446..000000000000
--- a/arch/x86/lib/copy_page_64.S
+++ b/arch/x86/lib/copy_page_64.S
@@@ -85,5 -85,5 +85,10 @@@ ENTRY(copy_page_regs
  	movq	(%rsp), %rbx
  	movq	1*8(%rsp), %r12
  	addq	$2*8, %rsp
++<<<<<<< HEAD
 +	ret
 +ENDPROC(copy_page_regs)
++=======
+ 	RET
+ SYM_FUNC_END(copy_page_regs)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/csum-copy_64.S
index 3394a8ff7fd0,d9e16a2cf285..000000000000
--- a/arch/x86/lib/csum-copy_64.S
+++ b/arch/x86/lib/csum-copy_64.S
@@@ -203,26 -192,65 +203,67 @@@ SYM_FUNC_START(csum_partial_copy_generi
  	adcl %r9d, %eax		/* carry */
  
  .Lende:
++<<<<<<< HEAD
 +	movq 2*8(%rsp), %rbx
 +	movq 3*8(%rsp), %r12
 +	movq 4*8(%rsp), %r14
 +	movq 5*8(%rsp), %r13
 +	movq 6*8(%rsp), %r15
 +	addq $7*8, %rsp
 +	ret
++=======
+ 	testq %r10, %r10
+ 	js  .Lwas_odd
+ .Lout:
+ 	movq 0*8(%rsp), %rbx
+ 	movq 1*8(%rsp), %r12
+ 	movq 2*8(%rsp), %r14
+ 	movq 3*8(%rsp), %r13
+ 	movq 4*8(%rsp), %r15
+ 	addq $5*8, %rsp
+ 	RET
+ .Lshort:
+ 	movl %ecx, %r10d
+ 	jmp  .L1
+ .Lunaligned:
+ 	xorl %ebx, %ebx
+ 	testb $1, %sil
+ 	jne  .Lodd
+ 1:	testb $2, %sil
+ 	je   2f
+ 	source
+ 	movw (%rdi), %bx
+ 	dest
+ 	movw %bx, (%rsi)
+ 	leaq 2(%rdi), %rdi
+ 	subq $2, %rcx
+ 	leaq 2(%rsi), %rsi
+ 	addq %rbx, %rax
+ 2:	testb $4, %sil
+ 	je .Laligned
+ 	source
+ 	movl (%rdi), %ebx
+ 	dest
+ 	movl %ebx, (%rsi)
+ 	leaq 4(%rdi), %rdi
+ 	subq $4, %rcx
+ 	leaq 4(%rsi), %rsi
+ 	addq %rbx, %rax
+ 	jmp .Laligned
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -.Lodd:
 -	source
 -	movb (%rdi), %bl
 -	dest
 -	movb %bl, (%rsi)
 -	leaq 1(%rdi), %rdi
 -	leaq 1(%rsi), %rsi
 -	/* decrement, set MSB */
 -	leaq -1(%rcx, %rcx), %rcx
 -	rorq $1, %rcx
 -	shll $8, %ebx
 -	addq %rbx, %rax
 -	jmp 1b
 -
 -.Lwas_odd:
 -	roll $8, %eax
 -	jmp .Lout
 +	/* Exception handlers. Very simple, zeroing is done in the wrappers */
 +.Lbad_source:
 +	movq (%rsp), %rax
 +	testq %rax, %rax
 +	jz   .Lende
 +	movl $-EFAULT, (%rax)
 +	jmp  .Lende
  
 -	/* Exception: just return 0 */
 -.Lfault:
 -	xorl %eax, %eax
 -	jmp  .Lout
 +.Lbad_dest:
 +	movq 8(%rsp), %rax
 +	testq %rax, %rax
 +	jz   .Lende
 +	movl $-EFAULT, (%rax)
 +	jmp .Lende
  SYM_FUNC_END(csum_partial_copy_generic)
diff --cc arch/x86/lib/getuser.S
index a21ee2882611,b70d98d79a9d..000000000000
--- a/arch/x86/lib/getuser.S
+++ b/arch/x86/lib/getuser.S
@@@ -59,10 -68,10 +59,10 @@@ SYM_FUNC_START(__get_user_2
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -2:	movzwl (%_ASM_AX),%edx
 +2:	movzwl -1(%_ASM_AX),%edx
  	xor %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  SYM_FUNC_END(__get_user_2)
  EXPORT_SYMBOL(__get_user_2)
  
@@@ -75,10 -82,10 +75,10 @@@ SYM_FUNC_START(__get_user_4
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -3:	movl (%_ASM_AX),%edx
 +3:	movl -3(%_ASM_AX),%edx
  	xor %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  SYM_FUNC_END(__get_user_4)
  EXPORT_SYMBOL(__get_user_4)
  
@@@ -92,46 -97,93 +92,107 @@@ SYM_FUNC_START(__get_user_8
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -4:	movq (%_ASM_AX),%rdx
 +4:	movq -7(%_ASM_AX),%rdx
  	xor %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  #else
 -	LOAD_TASK_SIZE_MINUS_N(7)
 -	cmp %_ASM_DX,%_ASM_AX
 +	add $7,%_ASM_AX
 +	jc bad_get_user_8
 +	mov PER_CPU_VAR(current_task), %_ASM_DX
 +	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX
  	jae bad_get_user_8
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -4:	movl (%_ASM_AX),%edx
 -5:	movl 4(%_ASM_AX),%ecx
 +4:	movl -7(%_ASM_AX),%edx
 +5:	movl -3(%_ASM_AX),%ecx
  	xor %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  #endif
  SYM_FUNC_END(__get_user_8)
  EXPORT_SYMBOL(__get_user_8)
  
++<<<<<<< HEAD
 +
 +.Lbad_get_user_clac:
++=======
+ /* .. and the same for __get_user, just without the range checks */
+ SYM_FUNC_START(__get_user_nocheck_1)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ 6:	movzbl (%_ASM_AX),%edx
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_1)
+ EXPORT_SYMBOL(__get_user_nocheck_1)
+ 
+ SYM_FUNC_START(__get_user_nocheck_2)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ 7:	movzwl (%_ASM_AX),%edx
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_2)
+ EXPORT_SYMBOL(__get_user_nocheck_2)
+ 
+ SYM_FUNC_START(__get_user_nocheck_4)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ 8:	movl (%_ASM_AX),%edx
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_4)
+ EXPORT_SYMBOL(__get_user_nocheck_4)
+ 
+ SYM_FUNC_START(__get_user_nocheck_8)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ #ifdef CONFIG_X86_64
+ 9:	movq (%_ASM_AX),%rdx
+ #else
+ 9:	movl (%_ASM_AX),%edx
+ 10:	movl 4(%_ASM_AX),%ecx
+ #endif
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_8)
+ EXPORT_SYMBOL(__get_user_nocheck_8)
+ 
+ 
+ SYM_CODE_START_LOCAL(.Lbad_get_user_clac)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  	ASM_CLAC
  bad_get_user:
  	xor %edx,%edx
  	mov $(-EFAULT),%_ASM_AX
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(.Lbad_get_user_clac)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  #ifdef CONFIG_X86_32
 -SYM_CODE_START_LOCAL(.Lbad_get_user_8_clac)
 +.Lbad_get_user_8_clac:
  	ASM_CLAC
  bad_get_user_8:
  	xor %edx,%edx
  	xor %ecx,%ecx
  	mov $(-EFAULT),%_ASM_AX
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(.Lbad_get_user_8_clac)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #endif
  
 -/* get_user */
  	_ASM_EXTABLE_UA(1b, .Lbad_get_user_clac)
  	_ASM_EXTABLE_UA(2b, .Lbad_get_user_clac)
  	_ASM_EXTABLE_UA(3b, .Lbad_get_user_clac)
diff --cc arch/x86/lib/memcpy_64.S
index 57cb82a8f8bb,59cf2343f3d9..000000000000
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@@ -39,8 -39,8 +39,13 @@@ ENTRY(memcpy
  	rep movsq
  	movl %edx, %ecx
  	rep movsb
++<<<<<<< HEAD
 +	ret
 +ENDPROC(memcpy)
++=======
+ 	RET
+ SYM_FUNC_END(memcpy)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  SYM_FUNC_END_ALIAS(__memcpy)
  EXPORT_SYMBOL(memcpy)
  EXPORT_SYMBOL(__memcpy)
@@@ -53,10 -53,10 +58,15 @@@ ENTRY(memcpy_erms
  	movq %rdi, %rax
  	movq %rdx, %rcx
  	rep movsb
++<<<<<<< HEAD
 +	ret
 +ENDPROC(memcpy_erms)
++=======
+ 	RET
+ SYM_FUNC_END(memcpy_erms)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START_LOCAL(memcpy_orig)
 +ENTRY(memcpy_orig)
  	movq %rdi, %rax
  
  	cmpq $0x20, %rdx
@@@ -180,118 -180,7 +190,123 @@@
  	movb %cl, (%rdi)
  
  .Lend:
++<<<<<<< HEAD
 +	retq
 +ENDPROC(memcpy_orig)
++=======
+ 	RET
+ SYM_FUNC_END(memcpy_orig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +#ifndef CONFIG_UML
 +
 +MCSAFE_TEST_CTL
 +
 +/*
 + * __memcpy_mcsafe - memory copy with machine check exception handling
 + * Note that we only catch machine checks when reading the source addresses.
 + * Writes to target are posted and don't generate machine checks.
 + */
 +SYM_FUNC_START(__memcpy_mcsafe)
 +	cmpl $8, %edx
 +	/* Less than 8 bytes? Go to byte copy loop */
 +	jb .L_no_whole_words
 +
 +	/* Check for bad alignment of source */
 +	testl $7, %esi
 +	/* Already aligned */
 +	jz .L_8byte_aligned
 +
 +	/* Copy one byte at a time until source is 8-byte aligned */
 +	movl %esi, %ecx
 +	andl $7, %ecx
 +	subl $8, %ecx
 +	negl %ecx
 +	subl %ecx, %edx
 +.L_read_leading_bytes:
 +	movb (%rsi), %al
 +	MCSAFE_TEST_SRC %rsi 1 .E_leading_bytes
 +	MCSAFE_TEST_DST %rdi 1 .E_leading_bytes
 +.L_write_leading_bytes:
 +	movb %al, (%rdi)
 +	incq %rsi
 +	incq %rdi
 +	decl %ecx
 +	jnz .L_read_leading_bytes
 +
 +.L_8byte_aligned:
 +	movl %edx, %ecx
 +	andl $7, %edx
 +	shrl $3, %ecx
 +	jz .L_no_whole_words
 +
 +.L_read_words:
 +	movq (%rsi), %r8
 +	MCSAFE_TEST_SRC %rsi 8 .E_read_words
 +	MCSAFE_TEST_DST %rdi 8 .E_write_words
 +.L_write_words:
 +	movq %r8, (%rdi)
 +	addq $8, %rsi
 +	addq $8, %rdi
 +	decl %ecx
 +	jnz .L_read_words
 +
 +	/* Any trailing bytes? */
 +.L_no_whole_words:
 +	andl %edx, %edx
 +	jz .L_done_memcpy_trap
 +
 +	/* Copy trailing bytes */
 +	movl %edx, %ecx
 +.L_read_trailing_bytes:
 +	movb (%rsi), %al
 +	MCSAFE_TEST_SRC %rsi 1 .E_trailing_bytes
 +	MCSAFE_TEST_DST %rdi 1 .E_trailing_bytes
 +.L_write_trailing_bytes:
 +	movb %al, (%rdi)
 +	incq %rsi
 +	incq %rdi
 +	decl %ecx
 +	jnz .L_read_trailing_bytes
 +
 +	/* Copy successful. Return zero */
 +.L_done_memcpy_trap:
 +	xorl %eax, %eax
 +	ret
 +SYM_FUNC_END(__memcpy_mcsafe)
 +EXPORT_SYMBOL_GPL(__memcpy_mcsafe)
 +
 +	.section .fixup, "ax"
 +	/*
 +	 * Return number of bytes not copied for any failure. Note that
 +	 * there is no "tail" handling since the source buffer is 8-byte
 +	 * aligned and poison is cacheline aligned.
 +	 */
 +.E_read_words:
 +	shll	$3, %ecx
 +.E_leading_bytes:
 +	addl	%edx, %ecx
 +.E_trailing_bytes:
 +	mov	%ecx, %eax
 +	ret
 +
 +	/*
 +	 * For write fault handling, given the destination is unaligned,
 +	 * we handle faults on multi-byte writes with a byte-by-byte
 +	 * copy up to the write-protected page.
 +	 */
 +.E_write_words:
 +	shll	$3, %ecx
 +	addl	%edx, %ecx
 +	movl	%ecx, %edx
 +	jmp mcsafe_handle_tail
 +
 +	.previous
  
 -.popsection
 +	_ASM_EXTABLE_FAULT(.L_read_leading_bytes, .E_leading_bytes)
 +	_ASM_EXTABLE_FAULT(.L_read_words, .E_read_words)
 +	_ASM_EXTABLE_FAULT(.L_read_trailing_bytes, .E_trailing_bytes)
 +	_ASM_EXTABLE(.L_write_leading_bytes, .E_leading_bytes)
 +	_ASM_EXTABLE(.L_write_words, .E_write_words)
 +	_ASM_EXTABLE(.L_write_trailing_bytes, .E_trailing_bytes)
 +#endif
diff --cc arch/x86/lib/memset_64.S
index 858983b54a26,d624f2bc42f1..000000000000
--- a/arch/x86/lib/memset_64.S
+++ b/arch/x86/lib/memset_64.S
@@@ -65,10 -63,10 +65,15 @@@ ENTRY(memset_erms
  	movq %rdx,%rcx
  	rep stosb
  	movq %r9,%rax
++<<<<<<< HEAD
 +	ret
 +ENDPROC(memset_erms)
++=======
+ 	RET
+ SYM_FUNC_END(memset_erms)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START_LOCAL(memset_orig)
 +ENTRY(memset_orig)
  	movq %rdi,%r10
  
  	/* expand byte value  */
diff --cc arch/x86/lib/retpoline.S
index 363ec132df7e,a842866062c8..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -4,18 -4,38 +4,39 @@@
  #include <linux/linkage.h>
  #include <asm/dwarf2.h>
  #include <asm/cpufeatures.h>
 -#include <asm/alternative.h>
 +#include <asm/alternative-asm.h>
  #include <asm/export.h>
  #include <asm/nospec-branch.h>
++<<<<<<< HEAD
++=======
+ #include <asm/unwind_hints.h>
+ #include <asm/frame.h>
+ 
+ 	.section .text.__x86.indirect_thunk
+ 
+ .macro RETPOLINE reg
+ 	ANNOTATE_INTRA_FUNCTION_CALL
+ 	call    .Ldo_rop_\@
+ .Lspec_trap_\@:
+ 	UNWIND_HINT_EMPTY
+ 	pause
+ 	lfence
+ 	jmp .Lspec_trap_\@
+ .Ldo_rop_\@:
+ 	mov     %\reg, (%_ASM_SP)
+ 	UNWIND_HINT_FUNC
+ 	RET
+ .endm
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .macro THUNK reg
 +	.section .text.__x86.indirect_thunk
  
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_INNER_LABEL(__x86_indirect_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_EMPTY
 -
 -	ALTERNATIVE_2 __stringify(ANNOTATE_RETPOLINE_SAFE; jmp *%\reg), \
 -		      __stringify(RETPOLINE \reg), X86_FEATURE_RETPOLINE, \
 -		      __stringify(lfence; ANNOTATE_RETPOLINE_SAFE; jmp *%\reg), X86_FEATURE_RETPOLINE_AMD
 -
 +SYM_FUNC_START(__x86_indirect_thunk_\reg)
 +	CFI_STARTPROC
 +	JMP_NOSPEC %\reg
 +	CFI_ENDPROC
 +SYM_FUNC_END(__x86_indirect_thunk_\reg)
  .endm
  
  /*
diff --cc arch/x86/math-emu/div_small.S
index 8f5025c80ee0,637439bfefa4..000000000000
--- a/arch/x86/math-emu/div_small.S
+++ b/arch/x86/math-emu/div_small.S
@@@ -44,5 -44,5 +44,10 @@@ ENTRY(FPU_div_small
  	popl	%esi
  
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(FPU_div_small)
++=======
+ 	RET
+ SYM_FUNC_END(FPU_div_small)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/mul_Xsig.S
index 3e489122a2b0,54a031b66142..000000000000
--- a/arch/x86/math-emu/mul_Xsig.S
+++ b/arch/x86/math-emu/mul_Xsig.S
@@@ -62,11 -62,11 +62,16 @@@ ENTRY(mul32_Xsig
  
  	popl %esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(mul32_Xsig)
++=======
+ 	RET
+ SYM_FUNC_END(mul32_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
 -SYM_FUNC_START(mul64_Xsig)
 +ENTRY(mul64_Xsig)
  	pushl %ebp
  	movl %esp,%ebp
  	subl $16,%esp
@@@ -115,8 -115,8 +120,13 @@@
  
  	popl %esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(mul64_Xsig)
++=======
+ 	RET
+ SYM_FUNC_END(mul64_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
  
@@@ -175,5 -175,5 +185,10 @@@ ENTRY(mul_Xsig_Xsig
  
  	popl %esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(mul_Xsig_Xsig)
++=======
+ 	RET
+ SYM_FUNC_END(mul_Xsig_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/polynom_Xsig.S
index 604f0b2d17e8,35fd723fc0df..000000000000
--- a/arch/x86/math-emu/polynom_Xsig.S
+++ b/arch/x86/math-emu/polynom_Xsig.S
@@@ -133,5 -133,5 +133,10 @@@ L_accum_done
  	popl	%edi
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(polynomial_Xsig)
++=======
+ 	RET
+ SYM_FUNC_END(polynomial_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/reg_norm.S
index 7f6b4392a15d,594936eeed67..000000000000
--- a/arch/x86/math-emu/reg_norm.S
+++ b/arch/x86/math-emu/reg_norm.S
@@@ -146,5 -146,5 +146,10 @@@ L_exit_nuo_zero
  
  	popl	%ebx
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(FPU_normalize_nuo)
++=======
+ 	RET
+ SYM_FUNC_END(FPU_normalize_nuo)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/reg_u_sub.S
index f05dea7dec38,4c900c29e4ff..000000000000
--- a/arch/x86/math-emu/reg_u_sub.S
+++ b/arch/x86/math-emu/reg_u_sub.S
@@@ -270,5 -270,5 +270,10 @@@ L_exit
  	popl	%edi
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(FPU_u_sub)
++=======
+ 	RET
+ SYM_FUNC_END(FPU_u_sub)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/round_Xsig.S
index 226a51e991f1,126c40473bad..000000000000
--- a/arch/x86/math-emu/round_Xsig.S
+++ b/arch/x86/math-emu/round_Xsig.S
@@@ -78,8 -78,8 +78,13 @@@ L_exit
  	popl	%esi
  	popl	%ebx
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(round_Xsig)
++=======
+ 	RET
+ SYM_FUNC_END(round_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
  
@@@ -138,5 -138,5 +143,10 @@@ L_n_exit
  	popl	%esi
  	popl	%ebx
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(norm_Xsig)
++=======
+ 	RET
+ SYM_FUNC_END(norm_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/shr_Xsig.S
index 96f4779aa9c1,f726bf6f6396..000000000000
--- a/arch/x86/math-emu/shr_Xsig.S
+++ b/arch/x86/math-emu/shr_Xsig.S
@@@ -85,5 -85,5 +85,10 @@@ L_more_than_95
  	movl	%eax,8(%esi)
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(shr_Xsig)
++=======
+ 	RET
+ SYM_FUNC_END(shr_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/wm_shrx.S
index d588874eb6fb,f608a28a4c43..000000000000
--- a/arch/x86/math-emu/wm_shrx.S
+++ b/arch/x86/math-emu/wm_shrx.S
@@@ -92,8 -92,8 +92,13 @@@ L_more_than_95
  	movl	%eax,4(%esi)
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(FPU_shrx)
++=======
+ 	RET
+ SYM_FUNC_END(FPU_shrx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
  /*---------------------------------------------------------------------------+
@@@ -203,5 -203,5 +208,10 @@@ Ls_more_than_95
  	popl	%ebx
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
 +ENDPROC(FPU_shrxs)
++=======
+ 	RET
+ SYM_FUNC_END(FPU_shrxs)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/platform/efi/efi_stub_32.S
index ab2e91e76894,f3cfdb1c9a35..000000000000
--- a/arch/x86/platform/efi/efi_stub_32.S
+++ b/arch/x86/platform/efi/efi_stub_32.S
@@@ -89,36 -53,8 +89,43 @@@ ENTRY(efi_call_phys
  	movl	%cr0, %edx
  	orl	$0x80000000, %edx
  	movl	%edx, %cr0
 +	jmp	1f
 +1:
 +	/*
 +	 * 8. Now restore the virtual mode from flat mode by
 +	 * adding EIP with PAGE_OFFSET.
 +	 */
 +	movl	$1f, %edx
 +	jmp	*%edx
 +1:
 +
++<<<<<<< HEAD
 +	/*
 +	 * 9. Balance the stack. And because EAX contain the return value,
 +	 * we'd better not clobber it.
 +	 */
 +	leal	efi_rt_function_ptr, %edx
 +	movl	(%edx), %ecx
 +	pushl	%ecx
 +
 +	/*
 +	 * 10. Push the saved return address onto the stack and return.
 +	 */
 +	leal	saved_return_addr, %edx
 +	movl	(%edx), %ecx
 +	pushl	%ecx
 +	ret
 +ENDPROC(efi_call_phys)
 +.previous
  
 +.data
 +saved_return_addr:
 +	.long 0
 +efi_rt_function_ptr:
 +	.long 0
++=======
+ 	movl	16(%esp), %ebx
+ 	leave
+ 	RET
+ SYM_FUNC_END(efi_call_svam)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/platform/efi/efi_stub_64.S
index e7e1020f4ccb,2206b8bc47b8..000000000000
--- a/arch/x86/platform/efi/efi_stub_64.S
+++ b/arch/x86/platform/efi/efi_stub_64.S
@@@ -21,7 -21,7 +21,12 @@@ SYM_FUNC_START(efi_call
  	mov %r8, %r9
  	mov %rcx, %r8
  	mov %rsi, %rcx
 -	CALL_NOSPEC rdi
 +	CALL_NOSPEC %rdi
  	leave
++<<<<<<< HEAD
 +	ret
 +SYM_FUNC_END(efi_call)
++=======
+ 	RET
+ SYM_FUNC_END(__efi_call)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/power/hibernate_asm_32.S
index 14f9cd4f9dd3,5606a15cf9a1..000000000000
--- a/arch/x86/power/hibernate_asm_32.S
+++ b/arch/x86/power/hibernate_asm_32.S
@@@ -32,10 -32,10 +32,15 @@@ ENTRY(swsusp_arch_suspend
  	FRAME_BEGIN
  	call swsusp_save
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(swsusp_arch_suspend)
++=======
+ 	RET
+ SYM_FUNC_END(swsusp_arch_suspend)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_CODE_START(restore_image)
 +ENTRY(restore_image)
  	/* prepare to jump to the image kernel */
  	movl	restore_jump_address, %ebx
  	movl	restore_cr3, %ebp
@@@ -97,5 -105,8 +102,13 @@@ ENTRY(restore_registers
  
  	xorl	%eax, %eax
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(restore_registers)
++=======
+ 	/* tell the hibernation core that we've just restored the memory */
+ 	movl	%eax, in_suspend
+ 
+ 	RET
+ SYM_FUNC_END(restore_registers)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/power/hibernate_asm_64.S
index 04eff2fe7b6d,0a0539e1cc81..000000000000
--- a/arch/x86/power/hibernate_asm_64.S
+++ b/arch/x86/power/hibernate_asm_64.S
@@@ -144,5 -66,85 +144,88 @@@ SYM_FUNC_START(restore_registers
  	/* tell the hibernation core that we've just restored the memory */
  	movq	%rax, in_suspend(%rip)
  
- 	ret
+ 	RET
  SYM_FUNC_END(restore_registers)
++<<<<<<< HEAD
++=======
+ 
+ SYM_FUNC_START(swsusp_arch_suspend)
+ 	movq	$saved_context, %rax
+ 	movq	%rsp, pt_regs_sp(%rax)
+ 	movq	%rbp, pt_regs_bp(%rax)
+ 	movq	%rsi, pt_regs_si(%rax)
+ 	movq	%rdi, pt_regs_di(%rax)
+ 	movq	%rbx, pt_regs_bx(%rax)
+ 	movq	%rcx, pt_regs_cx(%rax)
+ 	movq	%rdx, pt_regs_dx(%rax)
+ 	movq	%r8, pt_regs_r8(%rax)
+ 	movq	%r9, pt_regs_r9(%rax)
+ 	movq	%r10, pt_regs_r10(%rax)
+ 	movq	%r11, pt_regs_r11(%rax)
+ 	movq	%r12, pt_regs_r12(%rax)
+ 	movq	%r13, pt_regs_r13(%rax)
+ 	movq	%r14, pt_regs_r14(%rax)
+ 	movq	%r15, pt_regs_r15(%rax)
+ 	pushfq
+ 	popq	pt_regs_flags(%rax)
+ 
+ 	/* save cr3 */
+ 	movq	%cr3, %rax
+ 	movq	%rax, restore_cr3(%rip)
+ 
+ 	FRAME_BEGIN
+ 	call swsusp_save
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(swsusp_arch_suspend)
+ 
+ SYM_FUNC_START(restore_image)
+ 	/* prepare to jump to the image kernel */
+ 	movq	restore_jump_address(%rip), %r8
+ 	movq	restore_cr3(%rip), %r9
+ 
+ 	/* prepare to switch to temporary page tables */
+ 	movq	temp_pgt(%rip), %rax
+ 	movq	mmu_cr4_features(%rip), %rbx
+ 
+ 	/* prepare to copy image data to their original locations */
+ 	movq	restore_pblist(%rip), %rdx
+ 
+ 	/* jump to relocated restore code */
+ 	movq	relocated_restore_code(%rip), %rcx
+ 	ANNOTATE_RETPOLINE_SAFE
+ 	jmpq	*%rcx
+ SYM_FUNC_END(restore_image)
+ 
+ 	/* code below has been relocated to a safe page */
+ SYM_FUNC_START(core_restore_code)
+ 	/* switch to temporary page tables */
+ 	movq	%rax, %cr3
+ 	/* flush TLB */
+ 	movq	%rbx, %rcx
+ 	andq	$~(X86_CR4_PGE), %rcx
+ 	movq	%rcx, %cr4;  # turn off PGE
+ 	movq	%cr3, %rcx;  # flush TLB
+ 	movq	%rcx, %cr3;
+ 	movq	%rbx, %cr4;  # turn PGE back on
+ .Lloop:
+ 	testq	%rdx, %rdx
+ 	jz	.Ldone
+ 
+ 	/* get addresses from the pbe and copy the page */
+ 	movq	pbe_address(%rdx), %rsi
+ 	movq	pbe_orig_address(%rdx), %rdi
+ 	movq	$(PAGE_SIZE >> 3), %rcx
+ 	rep
+ 	movsq
+ 
+ 	/* progress to the next pbe */
+ 	movq	pbe_next(%rdx), %rdx
+ 	jmp	.Lloop
+ 
+ .Ldone:
+ 	/* jump to the restore_registers address from the image header */
+ 	ANNOTATE_RETPOLINE_SAFE
+ 	jmpq	*%r8
+ SYM_FUNC_END(core_restore_code)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/xen/xen-asm.S
index d7bf6d5cfcb9,e730e6200e64..000000000000
--- a/arch/x86/xen/xen-asm.S
+++ b/arch/x86/xen/xen-asm.S
@@@ -9,9 -9,57 +9,52 @@@
  #include <asm/asm-offsets.h>
  #include <asm/percpu.h>
  #include <asm/processor-flags.h>
 -#include <asm/segment.h>
 -#include <asm/thread_info.h>
 -#include <asm/asm.h>
  #include <asm/frame.h>
 -#include <asm/unwind_hints.h>
  
 -#include <xen/interface/xen.h>
 -
 -#include <linux/init.h>
  #include <linux/linkage.h>
++<<<<<<< HEAD
++=======
+ #include <../entry/calling.h>
+ 
+ .pushsection .noinstr.text, "ax"
+ /*
+  * Disabling events is simply a matter of making the event mask
+  * non-zero.
+  */
+ SYM_FUNC_START(xen_irq_disable_direct)
+ 	movb $1, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
+ 	RET
+ SYM_FUNC_END(xen_irq_disable_direct)
+ 
+ /*
+  * Force an event check by making a hypercall, but preserve regs
+  * before making the call.
+  */
+ SYM_FUNC_START(check_events)
+ 	FRAME_BEGIN
+ 	push %rax
+ 	push %rcx
+ 	push %rdx
+ 	push %rsi
+ 	push %rdi
+ 	push %r8
+ 	push %r9
+ 	push %r10
+ 	push %r11
+ 	call xen_force_evtchn_callback
+ 	pop %r11
+ 	pop %r10
+ 	pop %r9
+ 	pop %r8
+ 	pop %rdi
+ 	pop %rsi
+ 	pop %rdx
+ 	pop %rcx
+ 	pop %rax
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(check_events)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * Enable events.  This clears the event mask and tests the pending
@@@ -36,19 -84,9 +79,19 @@@ SYM_FUNC_START(xen_irq_enable_direct
  	call check_events
  1:
  	FRAME_END
- 	ret
+ 	RET
  SYM_FUNC_END(xen_irq_enable_direct)
  
 +
 +/*
 + * Disabling events is simply a matter of making the event mask
 + * non-zero.
 + */
 +SYM_FUNC_START(xen_irq_disable_direct)
 +	movb $1, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
 +	ret
 +SYM_FUNC_END(xen_irq_disable_direct)
 +
  /*
   * (xen_)save_fl is used to get the current interrupt enable status.
   * Callers expect the status to be in X86_EFLAGS_IF, and other bits
@@@ -62,76 -100,200 +105,127 @@@ SYM_FUNC_START(xen_save_fl_direct
  	testb $0xff, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
  	setz %ah
  	addb %ah, %ah
- 	ret
+ 	RET
  SYM_FUNC_END(xen_save_fl_direct)
  
 -SYM_FUNC_START(xen_read_cr2)
 +
 +/*
 + * In principle the caller should be passing us a value return from
 + * xen_save_fl_direct, but for robustness sake we test only the
 + * X86_EFLAGS_IF flag rather than the whole byte. After setting the
 + * interrupt mask state, it checks for unmasked pending events and
 + * enters the hypervisor to get them delivered if so.
 + */
 +SYM_FUNC_START(xen_restore_fl_direct)
  	FRAME_BEGIN
++<<<<<<< HEAD
 +#ifdef CONFIG_X86_64
 +	testw $X86_EFLAGS_IF, %di
 +#else
 +	testb $X86_EFLAGS_IF>>8, %ah
++=======
+ 	_ASM_MOV PER_CPU_VAR(xen_vcpu), %_ASM_AX
+ 	_ASM_MOV XEN_vcpu_info_arch_cr2(%_ASM_AX), %_ASM_AX
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(xen_read_cr2);
+ 
+ SYM_FUNC_START(xen_read_cr2_direct)
+ 	FRAME_BEGIN
+ 	_ASM_MOV PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_arch_cr2, %_ASM_AX
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(xen_read_cr2_direct);
+ .popsection
+ 
+ .macro xen_pv_trap name
+ SYM_CODE_START(xen_\name)
+ 	UNWIND_HINT_EMPTY
+ 	pop %rcx
+ 	pop %r11
+ 	jmp  \name
+ SYM_CODE_END(xen_\name)
+ _ASM_NOKPROBE(xen_\name)
+ .endm
+ 
+ xen_pv_trap asm_exc_divide_error
+ xen_pv_trap asm_xenpv_exc_debug
+ xen_pv_trap asm_exc_int3
+ xen_pv_trap asm_xenpv_exc_nmi
+ xen_pv_trap asm_exc_overflow
+ xen_pv_trap asm_exc_bounds
+ xen_pv_trap asm_exc_invalid_op
+ xen_pv_trap asm_exc_device_not_available
+ xen_pv_trap asm_xenpv_exc_double_fault
+ xen_pv_trap asm_exc_coproc_segment_overrun
+ xen_pv_trap asm_exc_invalid_tss
+ xen_pv_trap asm_exc_segment_not_present
+ xen_pv_trap asm_exc_stack_segment
+ xen_pv_trap asm_exc_general_protection
+ xen_pv_trap asm_exc_page_fault
+ xen_pv_trap asm_exc_spurious_interrupt_bug
+ xen_pv_trap asm_exc_coprocessor_error
+ xen_pv_trap asm_exc_alignment_check
+ #ifdef CONFIG_X86_MCE
+ xen_pv_trap asm_xenpv_exc_machine_check
+ #endif /* CONFIG_X86_MCE */
+ xen_pv_trap asm_exc_simd_coprocessor_error
+ #ifdef CONFIG_IA32_EMULATION
+ xen_pv_trap entry_INT80_compat
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #endif
 -xen_pv_trap asm_exc_xen_unknown_trap
 -xen_pv_trap asm_exc_xen_hypervisor_callback
 -
 -	__INIT
 -SYM_CODE_START(xen_early_idt_handler_array)
 -	i = 0
 -	.rept NUM_EXCEPTION_VECTORS
 -	UNWIND_HINT_EMPTY
 -	pop %rcx
 -	pop %r11
 -	jmp early_idt_handler_array + i*EARLY_IDT_HANDLER_SIZE
 -	i = i + 1
 -	.fill xen_early_idt_handler_array + i*XEN_EARLY_IDT_HANDLER_SIZE - ., 1, 0xcc
 -	.endr
 -SYM_CODE_END(xen_early_idt_handler_array)
 -	__FINIT
 -
 -hypercall_iret = hypercall_page + __HYPERVISOR_iret * 32
 -/*
 - * Xen64 iret frame:
 - *
 - *	ss
 - *	rsp
 - *	rflags
 - *	cs
 - *	rip		<-- standard iret frame
 - *
 - *	flags
 - *
 - *	rcx		}
 - *	r11		}<-- pushed by hypercall page
 - * rsp->rax		}
 - */
 -SYM_CODE_START(xen_iret)
 -	UNWIND_HINT_EMPTY
 -	pushq $0
 -	jmp hypercall_iret
 -SYM_CODE_END(xen_iret)
 -
 -/*
 - * XEN pv doesn't use trampoline stack, PER_CPU_VAR(cpu_tss_rw + TSS_sp0) is
 - * also the kernel stack.  Reusing swapgs_restore_regs_and_return_to_usermode()
 - * in XEN pv would cause %rsp to move up to the top of the kernel stack and
 - * leave the IRET frame below %rsp, which is dangerous to be corrupted if #NMI
 - * interrupts. And swapgs_restore_regs_and_return_to_usermode() pushing the IRET
 - * frame at the same address is useless.
 - */
 -SYM_CODE_START(xenpv_restore_regs_and_return_to_usermode)
 -	UNWIND_HINT_REGS
 -	POP_REGS
 -
 -	/* stackleak_erase() can work safely on the kernel stack. */
 -	STACKLEAK_ERASE_NOCLOBBER
 -
 -	addq	$8, %rsp	/* skip regs->orig_ax */
 -	jmp xen_iret
 -SYM_CODE_END(xenpv_restore_regs_and_return_to_usermode)
 -
 -/*
 - * Xen handles syscall callbacks much like ordinary exceptions, which
 - * means we have:
 - * - kernel gs
 - * - kernel rsp
 - * - an iret-like stack frame on the stack (including rcx and r11):
 - *	ss
 - *	rsp
 - *	rflags
 - *	cs
 - *	rip
 - *	r11
 - * rsp->rcx
 - */
 -
 -/* Normal 64-bit system call target */
 -SYM_CODE_START(xen_syscall_target)
 -	UNWIND_HINT_EMPTY
 -	popq %rcx
 -	popq %r11
 -
 -	/*
 -	 * Neither Xen nor the kernel really knows what the old SS and
 -	 * CS were.  The kernel expects __USER_DS and __USER_CS, so
 -	 * report those values even though Xen will guess its own values.
 -	 */
 -	movq $__USER_DS, 4*8(%rsp)
 -	movq $__USER_CS, 1*8(%rsp)
 -
 -	jmp entry_SYSCALL_64_after_hwframe
 -SYM_CODE_END(xen_syscall_target)
 -
 -#ifdef CONFIG_IA32_EMULATION
 -
 -/* 32-bit compat syscall target */
 -SYM_CODE_START(xen_syscall32_target)
 -	UNWIND_HINT_EMPTY
 -	popq %rcx
 -	popq %r11
 -
 -	/*
 -	 * Neither Xen nor the kernel really knows what the old SS and
 -	 * CS were.  The kernel expects __USER32_DS and __USER32_CS, so
 -	 * report those values even though Xen will guess its own values.
 -	 */
 -	movq $__USER32_DS, 4*8(%rsp)
 -	movq $__USER32_CS, 1*8(%rsp)
 -
 -	jmp entry_SYSCALL_compat_after_hwframe
 -SYM_CODE_END(xen_syscall32_target)
 -
 -/* 32-bit compat sysenter target */
 -SYM_CODE_START(xen_sysenter_target)
 -	UNWIND_HINT_EMPTY
 +	setz PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
  	/*
 -	 * NB: Xen is polite and clears TF from EFLAGS for us.  This means
 -	 * that we don't need to guard against single step exceptions here.
 -	 */
 -	popq %rcx
 -	popq %r11
 -
 -	/*
 -	 * Neither Xen nor the kernel really knows what the old SS and
 -	 * CS were.  The kernel expects __USER32_DS and __USER32_CS, so
 -	 * report those values even though Xen will guess its own values.
 +	 * Preempt here doesn't matter because that will deal with any
 +	 * pending interrupts.  The pending check may end up being run
 +	 * on the wrong CPU, but that doesn't hurt.
  	 */
 -	movq $__USER32_DS, 4*8(%rsp)
 -	movq $__USER32_CS, 1*8(%rsp)
 -
 -	jmp entry_SYSENTER_compat_after_hwframe
 -SYM_CODE_END(xen_sysenter_target)
  
 -#else /* !CONFIG_IA32_EMULATION */
 +	/* check for unmasked and pending */
 +	cmpw $0x0001, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_pending
 +	jnz 1f
 +	call check_events
 +1:
 +	FRAME_END
 +	ret
 +SYM_FUNC_END(xen_restore_fl_direct)
  
 -SYM_CODE_START(xen_syscall32_target)
 -SYM_CODE_START(xen_sysenter_target)
 -	UNWIND_HINT_EMPTY
 -	lea 16(%rsp), %rsp	/* strip %rcx, %r11 */
 -	mov $-ENOSYS, %rax
 -	pushq $0
 -	jmp hypercall_iret
 -SYM_CODE_END(xen_sysenter_target)
 -SYM_CODE_END(xen_syscall32_target)
  
 -#endif	/* CONFIG_IA32_EMULATION */
 +/*
 + * Force an event check by making a hypercall, but preserve regs
 + * before making the call.
 + */
 +SYM_FUNC_START(check_events)
 +	FRAME_BEGIN
 +#ifdef CONFIG_X86_32
 +	push %eax
 +	push %ecx
 +	push %edx
 +	call xen_force_evtchn_callback
 +	pop %edx
 +	pop %ecx
 +	pop %eax
 +#else
 +	push %rax
 +	push %rcx
 +	push %rdx
 +	push %rsi
 +	push %rdi
 +	push %r8
 +	push %r9
 +	push %r10
 +	push %r11
 +	call xen_force_evtchn_callback
 +	pop %r11
 +	pop %r10
 +	pop %r9
 +	pop %r8
 +	pop %rdi
 +	pop %rsi
 +	pop %rdx
 +	pop %rcx
 +	pop %rax
 +#endif
 +	FRAME_END
 +	ret
 +SYM_FUNC_END(check_events)
diff --cc arch/x86/xen/xen-head.S
index 1d0cee3163e4,11d286529fe5..000000000000
--- a/arch/x86/xen/xen-head.S
+++ b/arch/x86/xen/xen-head.S
@@@ -20,6 -20,23 +20,26 @@@
  #include <xen/interface/xen-mca.h>
  #include <asm/xen/interface.h>
  
++<<<<<<< HEAD
++=======
+ .pushsection .noinstr.text, "ax"
+ 	.balign PAGE_SIZE
+ SYM_CODE_START(hypercall_page)
+ 	.rept (PAGE_SIZE / 32)
+ 		UNWIND_HINT_FUNC
+ 		.skip 31, 0x90
+ 		RET
+ 	.endr
+ 
+ #define HYPERCALL(n) \
+ 	.equ xen_hypercall_##n, hypercall_page + __HYPERVISOR_##n * 32; \
+ 	.type xen_hypercall_##n, @function; .size xen_hypercall_##n, 32
+ #include <asm/xen-hypercalls.h>
+ #undef HYPERCALL
+ SYM_CODE_END(hypercall_page)
+ .popsection
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #ifdef CONFIG_XEN_PV
  	__INIT
  SYM_CODE_START(startup_xen)
* Unmerged path arch/x86/crypto/blake2s-core.S
* Unmerged path arch/x86/crypto/chacha-avx2-x86_64.S
* Unmerged path arch/x86/crypto/chacha-avx512vl-x86_64.S
* Unmerged path arch/x86/crypto/nh-avx2-x86_64.S
* Unmerged path arch/x86/crypto/nh-sse2-x86_64.S
* Unmerged path arch/x86/crypto/sm4-aesni-avx-asm_64.S
* Unmerged path arch/x86/crypto/sm4-aesni-avx2-asm_64.S
* Unmerged path arch/x86/lib/copy_mc_64.S
* Unmerged path arch/x86/boot/compressed/efi_thunk_64.S
* Unmerged path arch/x86/boot/compressed/head_64.S
* Unmerged path arch/x86/boot/compressed/mem_encrypt.S
diff --git a/arch/x86/crypto/aegis128-aesni-asm.S b/arch/x86/crypto/aegis128-aesni-asm.S
index 4565c2ac888a..8a73756a9f14 100644
--- a/arch/x86/crypto/aegis128-aesni-asm.S
+++ b/arch/x86/crypto/aegis128-aesni-asm.S
@@ -125,7 +125,7 @@ SYM_FUNC_START_LOCAL(__load_partial)
 	pxor T0, MSG
 
 .Lld_partial_8:
-	ret
+	RET
 SYM_FUNC_END(__load_partial)
 
 /*
@@ -183,7 +183,7 @@ SYM_FUNC_START_LOCAL(__store_partial)
 	mov %r10b, (%r9)
 
 .Lst_partial_1:
-	ret
+	RET
 SYM_FUNC_END(__store_partial)
 
 /*
@@ -228,7 +228,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_init)
 	movdqu STATE4, 0x40(STATEP)
 
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(crypto_aegis128_aesni_init)
 
 /*
@@ -340,7 +340,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_ad)
 	movdqu STATE3, 0x30(STATEP)
 	movdqu STATE4, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lad_out_1:
 	movdqu STATE4, 0x00(STATEP)
@@ -349,7 +349,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_ad)
 	movdqu STATE2, 0x30(STATEP)
 	movdqu STATE3, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lad_out_2:
 	movdqu STATE3, 0x00(STATEP)
@@ -358,7 +358,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_ad)
 	movdqu STATE1, 0x30(STATEP)
 	movdqu STATE2, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lad_out_3:
 	movdqu STATE2, 0x00(STATEP)
@@ -367,7 +367,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_ad)
 	movdqu STATE0, 0x30(STATEP)
 	movdqu STATE1, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lad_out_4:
 	movdqu STATE1, 0x00(STATEP)
@@ -376,11 +376,11 @@ SYM_FUNC_START(crypto_aegis128_aesni_ad)
 	movdqu STATE4, 0x30(STATEP)
 	movdqu STATE0, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lad_out:
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(crypto_aegis128_aesni_ad)
 
 .macro encrypt_block a s0 s1 s2 s3 s4 i
@@ -455,7 +455,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_enc)
 	movdqu STATE2, 0x30(STATEP)
 	movdqu STATE3, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lenc_out_1:
 	movdqu STATE3, 0x00(STATEP)
@@ -464,7 +464,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_enc)
 	movdqu STATE1, 0x30(STATEP)
 	movdqu STATE2, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lenc_out_2:
 	movdqu STATE2, 0x00(STATEP)
@@ -473,7 +473,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_enc)
 	movdqu STATE0, 0x30(STATEP)
 	movdqu STATE1, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lenc_out_3:
 	movdqu STATE1, 0x00(STATEP)
@@ -482,7 +482,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_enc)
 	movdqu STATE4, 0x30(STATEP)
 	movdqu STATE0, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lenc_out_4:
 	movdqu STATE0, 0x00(STATEP)
@@ -491,11 +491,11 @@ SYM_FUNC_START(crypto_aegis128_aesni_enc)
 	movdqu STATE3, 0x30(STATEP)
 	movdqu STATE4, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Lenc_out:
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(crypto_aegis128_aesni_enc)
 
 /*
@@ -535,7 +535,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_enc_tail)
 	movdqu STATE3, 0x40(STATEP)
 
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(crypto_aegis128_aesni_enc_tail)
 
 .macro decrypt_block a s0 s1 s2 s3 s4 i
@@ -609,7 +609,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_dec)
 	movdqu STATE2, 0x30(STATEP)
 	movdqu STATE3, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Ldec_out_1:
 	movdqu STATE3, 0x00(STATEP)
@@ -618,7 +618,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_dec)
 	movdqu STATE1, 0x30(STATEP)
 	movdqu STATE2, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Ldec_out_2:
 	movdqu STATE2, 0x00(STATEP)
@@ -627,7 +627,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_dec)
 	movdqu STATE0, 0x30(STATEP)
 	movdqu STATE1, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Ldec_out_3:
 	movdqu STATE1, 0x00(STATEP)
@@ -636,7 +636,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_dec)
 	movdqu STATE4, 0x30(STATEP)
 	movdqu STATE0, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Ldec_out_4:
 	movdqu STATE0, 0x00(STATEP)
@@ -645,11 +645,11 @@ SYM_FUNC_START(crypto_aegis128_aesni_dec)
 	movdqu STATE3, 0x30(STATEP)
 	movdqu STATE4, 0x40(STATEP)
 	FRAME_END
-	ret
+	RET
 
 .Ldec_out:
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(crypto_aegis128_aesni_dec)
 
 /*
@@ -699,7 +699,7 @@ SYM_FUNC_START(crypto_aegis128_aesni_dec_tail)
 	movdqu STATE3, 0x40(STATEP)
 
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(crypto_aegis128_aesni_dec_tail)
 
 /*
@@ -746,5 +746,5 @@ SYM_FUNC_START(crypto_aegis128_aesni_final)
 	movdqu MSG, (%rsi)
 
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(crypto_aegis128_aesni_final)
diff --git a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
index ec437db1fa54..161d4e949234 100644
--- a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
+++ b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
@@ -534,7 +534,7 @@ ddq_add_8:
 	/* return updated IV */
 	vpshufb	xbyteswap, xcounter, xcounter
 	vmovdqu	xcounter, (p_iv)
-	ret
+	RET
 .endm
 
 /*
* Unmerged path arch/x86/crypto/aesni-intel_asm.S
diff --git a/arch/x86/crypto/aesni-intel_avx-x86_64.S b/arch/x86/crypto/aesni-intel_avx-x86_64.S
index bfa1c0b3e5b4..a4e4b01ff7d1 100644
--- a/arch/x86/crypto/aesni-intel_avx-x86_64.S
+++ b/arch/x86/crypto/aesni-intel_avx-x86_64.S
@@ -1779,7 +1779,7 @@ SYM_FUNC_START(aesni_gcm_init_avx_gen2)
         FUNC_SAVE
         INIT GHASH_MUL_AVX, PRECOMPUTE_AVX
         FUNC_RESTORE
-        ret
+        RET
 SYM_FUNC_END(aesni_gcm_init_avx_gen2)
 
 ###############################################################################
@@ -1800,15 +1800,15 @@ SYM_FUNC_START(aesni_gcm_enc_update_avx_gen2)
         # must be 192
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 11
         FUNC_RESTORE
-        ret
+        RET
 key_128_enc_update:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 9
         FUNC_RESTORE
-        ret
+        RET
 key_256_enc_update:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 13
         FUNC_RESTORE
-        ret
+        RET
 SYM_FUNC_END(aesni_gcm_enc_update_avx_gen2)
 
 ###############################################################################
@@ -1829,15 +1829,15 @@ SYM_FUNC_START(aesni_gcm_dec_update_avx_gen2)
         # must be 192
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 11
         FUNC_RESTORE
-        ret
+        RET
 key_128_dec_update:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 9
         FUNC_RESTORE
-        ret
+        RET
 key_256_dec_update:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 13
         FUNC_RESTORE
-        ret
+        RET
 SYM_FUNC_END(aesni_gcm_dec_update_avx_gen2)
 
 ###############################################################################
@@ -1858,15 +1858,15 @@ SYM_FUNC_START(aesni_gcm_finalize_avx_gen2)
         # must be 192
         GCM_COMPLETE GHASH_MUL_AVX, 11, arg3, arg4
         FUNC_RESTORE
-        ret
+        RET
 key_128_finalize:
         GCM_COMPLETE GHASH_MUL_AVX, 9, arg3, arg4
         FUNC_RESTORE
-        ret
+        RET
 key_256_finalize:
         GCM_COMPLETE GHASH_MUL_AVX, 13, arg3, arg4
         FUNC_RESTORE
-        ret
+        RET
 SYM_FUNC_END(aesni_gcm_finalize_avx_gen2)
 
 #endif /* CONFIG_AS_AVX */
@@ -2750,7 +2750,7 @@ SYM_FUNC_START(aesni_gcm_init_avx_gen4)
         FUNC_SAVE
         INIT GHASH_MUL_AVX2, PRECOMPUTE_AVX2
         FUNC_RESTORE
-        ret
+        RET
 SYM_FUNC_END(aesni_gcm_init_avx_gen4)
 
 ###############################################################################
@@ -2771,15 +2771,15 @@ SYM_FUNC_START(aesni_gcm_enc_update_avx_gen4)
         # must be 192
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 11
         FUNC_RESTORE
-	ret
+	RET
 key_128_enc_update4:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 9
         FUNC_RESTORE
-	ret
+	RET
 key_256_enc_update4:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 13
         FUNC_RESTORE
-	ret
+	RET
 SYM_FUNC_END(aesni_gcm_enc_update_avx_gen4)
 
 ###############################################################################
@@ -2800,15 +2800,15 @@ SYM_FUNC_START(aesni_gcm_dec_update_avx_gen4)
         # must be 192
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 11
         FUNC_RESTORE
-        ret
+        RET
 key_128_dec_update4:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 9
         FUNC_RESTORE
-        ret
+        RET
 key_256_dec_update4:
         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 13
         FUNC_RESTORE
-        ret
+        RET
 SYM_FUNC_END(aesni_gcm_dec_update_avx_gen4)
 
 ###############################################################################
@@ -2829,15 +2829,15 @@ SYM_FUNC_START(aesni_gcm_finalize_avx_gen4)
         # must be 192
         GCM_COMPLETE GHASH_MUL_AVX2, 11, arg3, arg4
         FUNC_RESTORE
-        ret
+        RET
 key_128_finalize4:
         GCM_COMPLETE GHASH_MUL_AVX2, 9, arg3, arg4
         FUNC_RESTORE
-        ret
+        RET
 key_256_finalize4:
         GCM_COMPLETE GHASH_MUL_AVX2, 13, arg3, arg4
         FUNC_RESTORE
-        ret
+        RET
 SYM_FUNC_END(aesni_gcm_finalize_avx_gen4)
 
 #endif /* CONFIG_AS_AVX2 */
* Unmerged path arch/x86/crypto/blake2s-core.S
diff --git a/arch/x86/crypto/blowfish-x86_64-asm_64.S b/arch/x86/crypto/blowfish-x86_64-asm_64.S
index 70c34850ee0b..64c1f701b766 100644
--- a/arch/x86/crypto/blowfish-x86_64-asm_64.S
+++ b/arch/x86/crypto/blowfish-x86_64-asm_64.S
@@ -150,10 +150,10 @@ SYM_FUNC_START(__blowfish_enc_blk)
 	jnz .L__enc_xor;
 
 	write_block();
-	ret;
+	RET;
 .L__enc_xor:
 	xor_block();
-	ret;
+	RET;
 SYM_FUNC_END(__blowfish_enc_blk)
 
 SYM_FUNC_START(blowfish_dec_blk)
@@ -185,7 +185,7 @@ SYM_FUNC_START(blowfish_dec_blk)
 
 	movq %r11, %r12;
 
-	ret;
+	RET;
 SYM_FUNC_END(blowfish_dec_blk)
 
 /**********************************************************************
@@ -337,14 +337,14 @@ SYM_FUNC_START(__blowfish_enc_blk_4way)
 
 	popq %rbx;
 	popq %r12;
-	ret;
+	RET;
 
 .L__enc_xor4:
 	xor_block4();
 
 	popq %rbx;
 	popq %r12;
-	ret;
+	RET;
 SYM_FUNC_END(__blowfish_enc_blk_4way)
 
 SYM_FUNC_START(blowfish_dec_blk_4way)
@@ -379,5 +379,5 @@ SYM_FUNC_START(blowfish_dec_blk_4way)
 	popq %rbx;
 	popq %r12;
 
-	ret;
+	RET;
 SYM_FUNC_END(blowfish_dec_blk_4way)
diff --git a/arch/x86/crypto/camellia-aesni-avx-asm_64.S b/arch/x86/crypto/camellia-aesni-avx-asm_64.S
index d01ddd73de65..932036f922db 100644
--- a/arch/x86/crypto/camellia-aesni-avx-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx-asm_64.S
@@ -193,7 +193,7 @@ SYM_FUNC_START_LOCAL(roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_c
 	roundsm16(%xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7,
 		  %xmm8, %xmm9, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm15,
 		  %rcx, (%r9));
-	ret;
+	RET;
 SYM_FUNC_END(roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
 
 .align 8
@@ -201,7 +201,7 @@ SYM_FUNC_START_LOCAL(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_a
 	roundsm16(%xmm4, %xmm5, %xmm6, %xmm7, %xmm0, %xmm1, %xmm2, %xmm3,
 		  %xmm12, %xmm13, %xmm14, %xmm15, %xmm8, %xmm9, %xmm10, %xmm11,
 		  %rax, (%r9));
-	ret;
+	RET;
 SYM_FUNC_END(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 
 /*
@@ -787,7 +787,7 @@ SYM_FUNC_START_LOCAL(__camellia_enc_blk16)
 		    %xmm15, (key_table)(CTX, %r8, 8), (%rax), 1 * 16(%rax));
 
 	FRAME_END
-	ret;
+	RET;
 
 .align 8
 .Lenc_max32:
@@ -874,7 +874,7 @@ SYM_FUNC_START_LOCAL(__camellia_dec_blk16)
 		    %xmm15, (key_table)(CTX), (%rax), 1 * 16(%rax));
 
 	FRAME_END
-	ret;
+	RET;
 
 .align 8
 .Ldec_max32:
@@ -915,7 +915,7 @@ SYM_FUNC_START(camellia_ecb_enc_16way)
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(camellia_ecb_enc_16way)
 
 SYM_FUNC_START(camellia_ecb_dec_16way)
@@ -945,7 +945,7 @@ SYM_FUNC_START(camellia_ecb_dec_16way)
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(camellia_ecb_dec_16way)
 
 SYM_FUNC_START(camellia_cbc_dec_16way)
@@ -996,7 +996,7 @@ SYM_FUNC_START(camellia_cbc_dec_16way)
 		     %xmm8, %rsi);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(camellia_cbc_dec_16way)
 
 #define inc_le128(x, minus_one, tmp) \
diff --git a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
index 85f0a265dee8..4b5cf557d1f3 100644
--- a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
@@ -232,7 +232,7 @@ SYM_FUNC_START_LOCAL(roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_c
 	roundsm32(%ymm0, %ymm1, %ymm2, %ymm3, %ymm4, %ymm5, %ymm6, %ymm7,
 		  %ymm8, %ymm9, %ymm10, %ymm11, %ymm12, %ymm13, %ymm14, %ymm15,
 		  %rcx, (%r9));
-	ret;
+	RET;
 SYM_FUNC_END(roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
 
 .align 8
@@ -240,7 +240,7 @@ SYM_FUNC_START_LOCAL(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_a
 	roundsm32(%ymm4, %ymm5, %ymm6, %ymm7, %ymm0, %ymm1, %ymm2, %ymm3,
 		  %ymm12, %ymm13, %ymm14, %ymm15, %ymm8, %ymm9, %ymm10, %ymm11,
 		  %rax, (%r9));
-	ret;
+	RET;
 SYM_FUNC_END(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 
 /*
@@ -830,7 +830,7 @@ SYM_FUNC_START_LOCAL(__camellia_enc_blk32)
 		    %ymm15, (key_table)(CTX, %r8, 8), (%rax), 1 * 32(%rax));
 
 	FRAME_END
-	ret;
+	RET;
 
 .align 8
 .Lenc_max32:
@@ -917,7 +917,7 @@ SYM_FUNC_START_LOCAL(__camellia_dec_blk32)
 		    %ymm15, (key_table)(CTX), (%rax), 1 * 32(%rax));
 
 	FRAME_END
-	ret;
+	RET;
 
 .align 8
 .Ldec_max32:
@@ -962,7 +962,7 @@ SYM_FUNC_START(camellia_ecb_enc_32way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(camellia_ecb_enc_32way)
 
 SYM_FUNC_START(camellia_ecb_dec_32way)
@@ -996,7 +996,7 @@ SYM_FUNC_START(camellia_ecb_dec_32way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(camellia_ecb_dec_32way)
 
 SYM_FUNC_START(camellia_cbc_dec_32way)
@@ -1064,7 +1064,7 @@ SYM_FUNC_START(camellia_cbc_dec_32way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(camellia_cbc_dec_32way)
 
 #define inc_le128(x, minus_one, tmp) \
diff --git a/arch/x86/crypto/camellia-x86_64-asm_64.S b/arch/x86/crypto/camellia-x86_64-asm_64.S
index 4d77c9dcddbd..1059b84d2f90 100644
--- a/arch/x86/crypto/camellia-x86_64-asm_64.S
+++ b/arch/x86/crypto/camellia-x86_64-asm_64.S
@@ -228,13 +228,13 @@ SYM_FUNC_START(__camellia_enc_blk)
 	enc_outunpack(mov, RT1);
 
 	movq RR12, %r12;
-	ret;
+	RET;
 
 .L__enc_xor:
 	enc_outunpack(xor, RT1);
 
 	movq RR12, %r12;
-	ret;
+	RET;
 SYM_FUNC_END(__camellia_enc_blk)
 
 SYM_FUNC_START(camellia_dec_blk)
@@ -272,7 +272,7 @@ SYM_FUNC_START(camellia_dec_blk)
 	dec_outunpack();
 
 	movq RR12, %r12;
-	ret;
+	RET;
 SYM_FUNC_END(camellia_dec_blk)
 
 /**********************************************************************
@@ -463,14 +463,14 @@ SYM_FUNC_START(__camellia_enc_blk_2way)
 
 	movq RR12, %r12;
 	popq %rbx;
-	ret;
+	RET;
 
 .L__enc2_xor:
 	enc_outunpack2(xor, RT2);
 
 	movq RR12, %r12;
 	popq %rbx;
-	ret;
+	RET;
 SYM_FUNC_END(__camellia_enc_blk_2way)
 
 SYM_FUNC_START(camellia_dec_blk_2way)
@@ -510,5 +510,5 @@ SYM_FUNC_START(camellia_dec_blk_2way)
 
 	movq RR12, %r12;
 	movq RXOR, %rbx;
-	ret;
+	RET;
 SYM_FUNC_END(camellia_dec_blk_2way)
diff --git a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
index 3789c61f6166..0de44ddc8beb 100644
--- a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
@@ -294,7 +294,7 @@ SYM_FUNC_START_LOCAL(__cast5_enc_blk16)
 	outunpack_blocks(RR3, RL3, RTMP, RX, RKM);
 	outunpack_blocks(RR4, RL4, RTMP, RX, RKM);
 
-	ret;
+	RET;
 SYM_FUNC_END(__cast5_enc_blk16)
 
 .align 16
@@ -367,7 +367,7 @@ SYM_FUNC_START_LOCAL(__cast5_dec_blk16)
 	outunpack_blocks(RR3, RL3, RTMP, RX, RKM);
 	outunpack_blocks(RR4, RL4, RTMP, RX, RKM);
 
-	ret;
+	RET;
 
 .L__skip_dec:
 	vpsrldq $4, RKR, RKR;
@@ -408,7 +408,7 @@ SYM_FUNC_START(cast5_ecb_enc_16way)
 
 	popq %r15;
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(cast5_ecb_enc_16way)
 
 SYM_FUNC_START(cast5_ecb_dec_16way)
@@ -446,7 +446,7 @@ SYM_FUNC_START(cast5_ecb_dec_16way)
 
 	popq %r15;
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(cast5_ecb_dec_16way)
 
 SYM_FUNC_START(cast5_cbc_dec_16way)
@@ -498,7 +498,7 @@ SYM_FUNC_START(cast5_cbc_dec_16way)
 	popq %r15;
 	popq %r12;
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(cast5_cbc_dec_16way)
 
 SYM_FUNC_START(cast5_ctr_16way)
@@ -574,5 +574,5 @@ SYM_FUNC_START(cast5_ctr_16way)
 	popq %r15;
 	popq %r12;
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(cast5_ctr_16way)
diff --git a/arch/x86/crypto/cast6-avx-x86_64-asm_64.S b/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
index e38ab4571a6b..a72dbeb6a49b 100644
--- a/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
@@ -306,7 +306,7 @@ SYM_FUNC_START_LOCAL(__cast6_enc_blk8)
 	outunpack_blocks(RA1, RB1, RC1, RD1, RTMP, RX, RKRF, RKM);
 	outunpack_blocks(RA2, RB2, RC2, RD2, RTMP, RX, RKRF, RKM);
 
-	ret;
+	RET;
 SYM_FUNC_END(__cast6_enc_blk8)
 
 .align 8
@@ -353,7 +353,7 @@ SYM_FUNC_START_LOCAL(__cast6_dec_blk8)
 	outunpack_blocks(RA1, RB1, RC1, RD1, RTMP, RX, RKRF, RKM);
 	outunpack_blocks(RA2, RB2, RC2, RD2, RTMP, RX, RKRF, RKM);
 
-	ret;
+	RET;
 SYM_FUNC_END(__cast6_dec_blk8)
 
 SYM_FUNC_START(cast6_ecb_enc_8way)
@@ -376,7 +376,7 @@ SYM_FUNC_START(cast6_ecb_enc_8way)
 
 	popq %r15;
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(cast6_ecb_enc_8way)
 
 SYM_FUNC_START(cast6_ecb_dec_8way)
@@ -399,7 +399,7 @@ SYM_FUNC_START(cast6_ecb_dec_8way)
 
 	popq %r15;
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(cast6_ecb_dec_8way)
 
 SYM_FUNC_START(cast6_cbc_dec_8way)
@@ -425,7 +425,7 @@ SYM_FUNC_START(cast6_cbc_dec_8way)
 	popq %r15;
 	popq %r12;
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(cast6_cbc_dec_8way)
 
 SYM_FUNC_START(cast6_ctr_8way)
* Unmerged path arch/x86/crypto/chacha-avx2-x86_64.S
* Unmerged path arch/x86/crypto/chacha-avx512vl-x86_64.S
* Unmerged path arch/x86/crypto/chacha20-ssse3-x86_64.S
diff --git a/arch/x86/crypto/crc32-pclmul_asm.S b/arch/x86/crypto/crc32-pclmul_asm.S
index 9fd28ff65bc2..0fbf626ebb7f 100644
--- a/arch/x86/crypto/crc32-pclmul_asm.S
+++ b/arch/x86/crypto/crc32-pclmul_asm.S
@@ -237,5 +237,5 @@ fold_64:
 	pxor    %xmm2, %xmm1
 	PEXTRD  0x01, %xmm1, %eax
 
-	ret
+	RET
 SYM_FUNC_END(crc32_pclmul_le_16)
diff --git a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
index 0e6690e3618c..ad7cf78ab623 100644
--- a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
+++ b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
@@ -310,7 +310,7 @@ do_return:
 	popq    %rsi
 	popq    %rdi
 	popq    %rbx
-        ret
+        RET
 SYM_FUNC_END(crc_pcl)
 
 .section	.rodata, "a", @progbits
* Unmerged path arch/x86/crypto/crct10dif-pcl-asm_64.S
diff --git a/arch/x86/crypto/des3_ede-asm_64.S b/arch/x86/crypto/des3_ede-asm_64.S
index 82779c08029b..285775d32b72 100644
--- a/arch/x86/crypto/des3_ede-asm_64.S
+++ b/arch/x86/crypto/des3_ede-asm_64.S
@@ -252,7 +252,7 @@ SYM_FUNC_START(des3_ede_x86_64_crypt_blk)
 	popq %r12;
 	popq %rbx;
 
-	ret;
+	RET;
 SYM_FUNC_END(des3_ede_x86_64_crypt_blk)
 
 /***********************************************************************
@@ -537,7 +537,7 @@ SYM_FUNC_START(des3_ede_x86_64_crypt_blk_3way)
 	popq %r12;
 	popq %rbx;
 
-	ret;
+	RET;
 SYM_FUNC_END(des3_ede_x86_64_crypt_blk_3way)
 
 .section	.rodata, "a", @progbits
diff --git a/arch/x86/crypto/ghash-clmulni-intel_asm.S b/arch/x86/crypto/ghash-clmulni-intel_asm.S
index 12e3a850257b..71dad110dbf9 100644
--- a/arch/x86/crypto/ghash-clmulni-intel_asm.S
+++ b/arch/x86/crypto/ghash-clmulni-intel_asm.S
@@ -89,7 +89,7 @@ SYM_FUNC_START_LOCAL(__clmul_gf128mul_ble)
 	psrlq $1, T2
 	pxor T2, T1
 	pxor T1, DATA
-	ret
+	RET
 SYM_FUNC_END(__clmul_gf128mul_ble)
 
 /* void clmul_ghash_mul(char *dst, const u128 *shash) */
@@ -103,7 +103,7 @@ SYM_FUNC_START(clmul_ghash_mul)
 	PSHUFB_XMM BSWAP DATA
 	movups DATA, (%rdi)
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(clmul_ghash_mul)
 
 /*
@@ -132,5 +132,5 @@ SYM_FUNC_START(clmul_ghash_update)
 	movups DATA, (%rdi)
 .Lupdate_just_ret:
 	FRAME_END
-	ret
+	RET
 SYM_FUNC_END(clmul_ghash_update)
* Unmerged path arch/x86/crypto/nh-avx2-x86_64.S
* Unmerged path arch/x86/crypto/nh-sse2-x86_64.S
diff --git a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
index 72de86a8091e..ef403d100bd5 100644
--- a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
@@ -620,7 +620,7 @@ SYM_FUNC_START_LOCAL(__serpent_enc_blk8_avx)
 	write_blocks(RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	write_blocks(RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(__serpent_enc_blk8_avx)
 
 .align 8
@@ -674,7 +674,7 @@ SYM_FUNC_START_LOCAL(__serpent_dec_blk8_avx)
 	write_blocks(RC1, RD1, RB1, RE1, RK0, RK1, RK2);
 	write_blocks(RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(__serpent_dec_blk8_avx)
 
 SYM_FUNC_START(serpent_ecb_enc_8way_avx)
@@ -692,7 +692,7 @@ SYM_FUNC_START(serpent_ecb_enc_8way_avx)
 	store_8way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(serpent_ecb_enc_8way_avx)
 
 SYM_FUNC_START(serpent_ecb_dec_8way_avx)
@@ -710,7 +710,7 @@ SYM_FUNC_START(serpent_ecb_dec_8way_avx)
 	store_8way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(serpent_ecb_dec_8way_avx)
 
 SYM_FUNC_START(serpent_cbc_dec_8way_avx)
@@ -728,7 +728,7 @@ SYM_FUNC_START(serpent_cbc_dec_8way_avx)
 	store_cbc_8way(%rdx, %rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(serpent_cbc_dec_8way_avx)
 
 SYM_FUNC_START(serpent_ctr_8way_avx)
diff --git a/arch/x86/crypto/serpent-avx2-asm_64.S b/arch/x86/crypto/serpent-avx2-asm_64.S
index b866f1632803..36e5fc4168cb 100644
--- a/arch/x86/crypto/serpent-avx2-asm_64.S
+++ b/arch/x86/crypto/serpent-avx2-asm_64.S
@@ -616,7 +616,7 @@ SYM_FUNC_START_LOCAL(__serpent_enc_blk16)
 	write_blocks(RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	write_blocks(RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(__serpent_enc_blk16)
 
 .align 8
@@ -670,7 +670,7 @@ SYM_FUNC_START_LOCAL(__serpent_dec_blk16)
 	write_blocks(RC1, RD1, RB1, RE1, RK0, RK1, RK2);
 	write_blocks(RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(__serpent_dec_blk16)
 
 SYM_FUNC_START(serpent_ecb_enc_16way)
@@ -692,7 +692,7 @@ SYM_FUNC_START(serpent_ecb_enc_16way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(serpent_ecb_enc_16way)
 
 SYM_FUNC_START(serpent_ecb_dec_16way)
@@ -714,7 +714,7 @@ SYM_FUNC_START(serpent_ecb_dec_16way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(serpent_ecb_dec_16way)
 
 SYM_FUNC_START(serpent_cbc_dec_16way)
@@ -737,7 +737,7 @@ SYM_FUNC_START(serpent_cbc_dec_16way)
 	vzeroupper;
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(serpent_cbc_dec_16way)
 
 SYM_FUNC_START(serpent_ctr_16way)
* Unmerged path arch/x86/crypto/serpent-sse2-i586-asm_32.S
diff --git a/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S b/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
index bdeee900df63..6a1216c89b90 100644
--- a/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
@@ -690,13 +690,13 @@ SYM_FUNC_START(__serpent_enc_blk_8way)
 	write_blocks(%rsi, RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	write_blocks(%rax, RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	RET;
 
 .L__enc_xor8:
 	xor_blocks(%rsi, RA1, RB1, RC1, RD1, RK0, RK1, RK2);
 	xor_blocks(%rax, RA2, RB2, RC2, RD2, RK0, RK1, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(__serpent_enc_blk_8way)
 
 SYM_FUNC_START(serpent_dec_blk_8way)
@@ -750,5 +750,5 @@ SYM_FUNC_START(serpent_dec_blk_8way)
 	write_blocks(%rsi, RC1, RD1, RB1, RE1, RK0, RK1, RK2);
 	write_blocks(%rax, RC2, RD2, RB2, RE2, RK0, RK1, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(serpent_dec_blk_8way)
diff --git a/arch/x86/crypto/sha1_avx2_x86_64_asm.S b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
index 6decc85ef7b7..4d0ce173a5a3 100644
--- a/arch/x86/crypto/sha1_avx2_x86_64_asm.S
+++ b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
@@ -674,7 +674,7 @@ _loop3:
 	pop	%r12
 	pop	%rbx
 
-	ret
+	RET
 
 	SYM_FUNC_END(\name)
 .endm
diff --git a/arch/x86/crypto/sha1_ni_asm.S b/arch/x86/crypto/sha1_ni_asm.S
index 11efe3a45a1f..b59f3ca62837 100644
--- a/arch/x86/crypto/sha1_ni_asm.S
+++ b/arch/x86/crypto/sha1_ni_asm.S
@@ -290,7 +290,7 @@ SYM_FUNC_START(sha1_ni_transform)
 .Ldone_hash:
 	mov		RSPSAVE, %rsp
 
-	ret
+	RET
 SYM_FUNC_END(sha1_ni_transform)
 
 .section	.rodata.cst16.PSHUFFLE_BYTE_FLIP_MASK, "aM", @progbits, 16
diff --git a/arch/x86/crypto/sha1_ssse3_asm.S b/arch/x86/crypto/sha1_ssse3_asm.S
index 265caba3d113..2940c6e3a412 100644
--- a/arch/x86/crypto/sha1_ssse3_asm.S
+++ b/arch/x86/crypto/sha1_ssse3_asm.S
@@ -103,7 +103,7 @@
 	pop	%rbp
 	pop	%r12
 	pop	%rbx
-	ret
+	RET
 
 	SYM_FUNC_END(\name)
 .endm
diff --git a/arch/x86/crypto/sha256-avx-asm.S b/arch/x86/crypto/sha256-avx-asm.S
index 22e14c8dd2e4..71320f9a8b36 100644
--- a/arch/x86/crypto/sha256-avx-asm.S
+++ b/arch/x86/crypto/sha256-avx-asm.S
@@ -459,7 +459,7 @@ done_hash:
 	popq    %r13
 	popq	%r12
 	popq    %rbx
-	ret
+	RET
 SYM_FUNC_END(sha256_transform_avx)
 
 .section	.rodata.cst256.K256, "aM", @progbits, 256
diff --git a/arch/x86/crypto/sha256-avx2-asm.S b/arch/x86/crypto/sha256-avx2-asm.S
index 519b551ad576..75eac9f99ac9 100644
--- a/arch/x86/crypto/sha256-avx2-asm.S
+++ b/arch/x86/crypto/sha256-avx2-asm.S
@@ -712,7 +712,7 @@ done_hash:
 	popq	%r13
 	popq	%r12
 	popq	%rbx
-	ret
+	RET
 SYM_FUNC_END(sha256_transform_rorx)
 
 .section	.rodata.cst512.K256, "aM", @progbits, 512
diff --git a/arch/x86/crypto/sha256-ssse3-asm.S b/arch/x86/crypto/sha256-ssse3-asm.S
index 69cc2f91dc4c..7157fc706c9a 100644
--- a/arch/x86/crypto/sha256-ssse3-asm.S
+++ b/arch/x86/crypto/sha256-ssse3-asm.S
@@ -470,7 +470,7 @@ done_hash:
 	popq    %r12
 	popq    %rbx
 
-	ret
+	RET
 SYM_FUNC_END(sha256_transform_ssse3)
 
 .section	.rodata.cst256.K256, "aM", @progbits, 256
diff --git a/arch/x86/crypto/sha256_ni_asm.S b/arch/x86/crypto/sha256_ni_asm.S
index 7abade04a3a3..94d50dd27cb5 100644
--- a/arch/x86/crypto/sha256_ni_asm.S
+++ b/arch/x86/crypto/sha256_ni_asm.S
@@ -326,7 +326,7 @@ SYM_FUNC_START(sha256_ni_transform)
 
 .Ldone_hash:
 
-	ret
+	RET
 SYM_FUNC_END(sha256_ni_transform)
 
 .section	.rodata.cst256.K256, "aM", @progbits, 256
diff --git a/arch/x86/crypto/sha512-avx-asm.S b/arch/x86/crypto/sha512-avx-asm.S
index 3704ddd7e5d5..07beb4ad145e 100644
--- a/arch/x86/crypto/sha512-avx-asm.S
+++ b/arch/x86/crypto/sha512-avx-asm.S
@@ -364,7 +364,7 @@ updateblock:
 	mov	frame_RSPSAVE(%rsp), %rsp
 
 nowork:
-	ret
+	RET
 SYM_FUNC_END(sha512_transform_avx)
 
 ########################################################################
* Unmerged path arch/x86/crypto/sha512-avx2-asm.S
diff --git a/arch/x86/crypto/sha512-ssse3-asm.S b/arch/x86/crypto/sha512-ssse3-asm.S
index 838f984e95d9..34d371746918 100644
--- a/arch/x86/crypto/sha512-ssse3-asm.S
+++ b/arch/x86/crypto/sha512-ssse3-asm.S
@@ -363,7 +363,7 @@ updateblock:
 	mov	frame_RSPSAVE(%rsp), %rsp
 
 nowork:
-	ret
+	RET
 SYM_FUNC_END(sha512_transform_ssse3)
 
 ########################################################################
* Unmerged path arch/x86/crypto/sm4-aesni-avx-asm_64.S
* Unmerged path arch/x86/crypto/sm4-aesni-avx2-asm_64.S
diff --git a/arch/x86/crypto/twofish-avx-x86_64-asm_64.S b/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
index 16e53c98e6a0..f65fe8672b0c 100644
--- a/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
@@ -287,7 +287,7 @@ SYM_FUNC_START_LOCAL(__twofish_enc_blk8)
 	outunpack_blocks(RC1, RD1, RA1, RB1, RK1, RX0, RY0, RK2);
 	outunpack_blocks(RC2, RD2, RA2, RB2, RK1, RX0, RY0, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(__twofish_enc_blk8)
 
 .align 8
@@ -327,7 +327,7 @@ SYM_FUNC_START_LOCAL(__twofish_dec_blk8)
 	outunpack_blocks(RA1, RB1, RC1, RD1, RK1, RX0, RY0, RK2);
 	outunpack_blocks(RA2, RB2, RC2, RD2, RK1, RX0, RY0, RK2);
 
-	ret;
+	RET;
 SYM_FUNC_END(__twofish_dec_blk8)
 
 SYM_FUNC_START(twofish_ecb_enc_8way)
@@ -347,7 +347,7 @@ SYM_FUNC_START(twofish_ecb_enc_8way)
 	store_8way(%r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(twofish_ecb_enc_8way)
 
 SYM_FUNC_START(twofish_ecb_dec_8way)
@@ -367,7 +367,7 @@ SYM_FUNC_START(twofish_ecb_dec_8way)
 	store_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(twofish_ecb_dec_8way)
 
 SYM_FUNC_START(twofish_cbc_dec_8way)
@@ -392,7 +392,7 @@ SYM_FUNC_START(twofish_cbc_dec_8way)
 	popq %r12;
 
 	FRAME_END
-	ret;
+	RET;
 SYM_FUNC_END(twofish_cbc_dec_8way)
 
 SYM_FUNC_START(twofish_ctr_8way)
* Unmerged path arch/x86/crypto/twofish-i586-asm_32.S
diff --git a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
index c830aef77070..7cb074d40154 100644
--- a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
@@ -273,7 +273,7 @@ SYM_FUNC_START(__twofish_enc_blk_3way)
 	popq %rbx;
 	popq %r12;
 	popq %r13;
-	ret;
+	RET;
 
 .L__enc_xor3:
 	outunpack_enc3(xor);
@@ -281,7 +281,7 @@ SYM_FUNC_START(__twofish_enc_blk_3way)
 	popq %rbx;
 	popq %r12;
 	popq %r13;
-	ret;
+	RET;
 SYM_FUNC_END(__twofish_enc_blk_3way)
 
 SYM_FUNC_START(twofish_dec_blk_3way)
@@ -316,5 +316,5 @@ SYM_FUNC_START(twofish_dec_blk_3way)
 	popq %rbx;
 	popq %r12;
 	popq %r13;
-	ret;
+	RET;
 SYM_FUNC_END(twofish_dec_blk_3way)
diff --git a/arch/x86/crypto/twofish-x86_64-asm_64.S b/arch/x86/crypto/twofish-x86_64-asm_64.S
index 74ef6c55d75f..7203cc2742b8 100644
--- a/arch/x86/crypto/twofish-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64.S
@@ -265,7 +265,7 @@ SYM_FUNC_START(twofish_enc_blk)
 
 	popq	R1
 	movl	$1,%eax
-	ret
+	RET
 SYM_FUNC_END(twofish_enc_blk)
 
 SYM_FUNC_START(twofish_dec_blk)
@@ -317,5 +317,5 @@ SYM_FUNC_START(twofish_dec_blk)
 
 	popq	R1
 	movl	$1,%eax
-	ret
+	RET
 SYM_FUNC_END(twofish_dec_blk)
* Unmerged path arch/x86/entry/entry_32.S
* Unmerged path arch/x86/entry/entry_64.S
diff --git a/arch/x86/entry/thunk_32.S b/arch/x86/entry/thunk_32.S
index 8118553a1e1e..dca337d6963a 100644
--- a/arch/x86/entry/thunk_32.S
+++ b/arch/x86/entry/thunk_32.S
@@ -25,7 +25,7 @@
 	popl %edx
 	popl %ecx
 	popl %eax
-	ret
+	RET
 	_ASM_NOKPROBE(\name)
 	.endm
 
* Unmerged path arch/x86/entry/thunk_64.S
diff --git a/arch/x86/entry/vdso/vdso32/system_call.S b/arch/x86/entry/vdso/vdso32/system_call.S
index de1fff7188aa..a1ee5ebd56d4 100644
--- a/arch/x86/entry/vdso/vdso32/system_call.S
+++ b/arch/x86/entry/vdso/vdso32/system_call.S
@@ -78,7 +78,7 @@ SYM_INNER_LABEL(int80_landing_pad, SYM_L_GLOBAL)
 	popl	%ecx
 	CFI_RESTORE		ecx
 	CFI_ADJUST_CFA_OFFSET	-4
-	ret
+	RET
 	CFI_ENDPROC
 
 	.size __kernel_vsyscall,.-__kernel_vsyscall
diff --git a/arch/x86/entry/vdso/vsgx.S b/arch/x86/entry/vdso/vsgx.S
index 86a0e94f68df..cff209dd3787 100644
--- a/arch/x86/entry/vdso/vsgx.S
+++ b/arch/x86/entry/vdso/vsgx.S
@@ -81,7 +81,7 @@ SYM_FUNC_START(__vdso_sgx_enter_enclave)
 	pop	%rbx
 	leave
 	.cfi_def_cfa		%rsp, 8
-	ret
+	RET
 
 	/* The out-of-line code runs with the pre-leave stack frame. */
 	.cfi_def_cfa		%rbp, 16
diff --git a/arch/x86/entry/vsyscall/vsyscall_emu_64.S b/arch/x86/entry/vsyscall/vsyscall_emu_64.S
index c9596a9af159..dfbf8791cf4d 100644
--- a/arch/x86/entry/vsyscall/vsyscall_emu_64.S
+++ b/arch/x86/entry/vsyscall/vsyscall_emu_64.S
@@ -20,17 +20,17 @@ __vsyscall_page:
 
 	mov $__NR_gettimeofday, %rax
 	syscall
-	ret
+	RET
 
 	.balign 1024, 0xcc
 	mov $__NR_time, %rax
 	syscall
-	ret
+	RET
 
 	.balign 1024, 0xcc
 	mov $__NR_getcpu, %rax
 	syscall
-	ret
+	RET
 
 	.balign 4096, 0xcc
 
* Unmerged path arch/x86/kernel/acpi/wakeup_32.S
* Unmerged path arch/x86/kernel/ftrace_32.S
* Unmerged path arch/x86/kernel/ftrace_64.S
diff --git a/arch/x86/kernel/head_32.S b/arch/x86/kernel/head_32.S
index abe6df15a8fb..86c79155d7e6 100644
--- a/arch/x86/kernel/head_32.S
+++ b/arch/x86/kernel/head_32.S
@@ -390,7 +390,7 @@ setup_once:
 #endif
 
 	andl $0,setup_once_ref	/* Once is enough, thanks */
-	ret
+	RET
 
 ENTRY(early_idt_handler_array)
 	# 36(%esp) %eflags
diff --git a/arch/x86/kernel/irqflags.S b/arch/x86/kernel/irqflags.S
index 0db0375235b4..6e64de13ae0b 100644
--- a/arch/x86/kernel/irqflags.S
+++ b/arch/x86/kernel/irqflags.S
@@ -10,7 +10,7 @@
 SYM_FUNC_START(native_save_fl)
 	pushf
 	pop %_ASM_AX
-	ret
+	RET
 SYM_FUNC_END(native_save_fl)
 EXPORT_SYMBOL(native_save_fl)
 
diff --git a/arch/x86/kernel/relocate_kernel_32.S b/arch/x86/kernel/relocate_kernel_32.S
index 74d7891fc026..5670536e7152 100644
--- a/arch/x86/kernel/relocate_kernel_32.S
+++ b/arch/x86/kernel/relocate_kernel_32.S
@@ -93,7 +93,7 @@ SYM_CODE_START_NOALIGN(relocate_kernel)
 	movl    %edi, %eax
 	addl    $(identity_mapped - relocate_kernel), %eax
 	pushl   %eax
-	ret
+	RET
 SYM_CODE_END(relocate_kernel)
 
 SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
@@ -161,7 +161,7 @@ SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
 	xorl    %edx, %edx
 	xorl    %esi, %esi
 	xorl    %ebp, %ebp
-	ret
+	RET
 1:
 	popl	%edx
 	movl	CP_PA_SWAP_PAGE(%edi), %esp
@@ -192,7 +192,7 @@ SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
 	movl	%edi, %eax
 	addl	$(virtual_mapped - relocate_kernel), %eax
 	pushl	%eax
-	ret
+	RET
 SYM_CODE_END(identity_mapped)
 
 SYM_CODE_START_LOCAL_NOALIGN(virtual_mapped)
@@ -210,7 +210,7 @@ SYM_CODE_START_LOCAL_NOALIGN(virtual_mapped)
 	popl	%edi
 	popl	%esi
 	popl	%ebx
-	ret
+	RET
 SYM_CODE_END(virtual_mapped)
 
 	/* Do the copies */
@@ -273,7 +273,7 @@ SYM_CODE_START_LOCAL_NOALIGN(swap_pages)
 	popl	%edi
 	popl	%ebx
 	popl	%ebp
-	ret
+	RET
 SYM_CODE_END(swap_pages)
 
 	.globl kexec_control_code_size
diff --git a/arch/x86/kernel/relocate_kernel_64.S b/arch/x86/kernel/relocate_kernel_64.S
index a7d3126f1dac..f4f66bdaf6f1 100644
--- a/arch/x86/kernel/relocate_kernel_64.S
+++ b/arch/x86/kernel/relocate_kernel_64.S
@@ -106,7 +106,7 @@ SYM_CODE_START_NOALIGN(relocate_kernel)
 	/* jump to identity mapped page */
 	addq	$(identity_mapped - relocate_kernel), %r8
 	pushq	%r8
-	ret
+	RET
 SYM_CODE_END(relocate_kernel)
 
 SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
@@ -193,7 +193,7 @@ SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
 	xorl	%r14d, %r14d
 	xorl	%r15d, %r15d
 
-	ret
+	RET
 
 1:
 	popq	%rdx
@@ -215,7 +215,7 @@ SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
 	call	swap_pages
 	movq	$virtual_mapped, %rax
 	pushq	%rax
-	ret
+	RET
 SYM_CODE_END(identity_mapped)
 
 SYM_CODE_START_LOCAL_NOALIGN(virtual_mapped)
@@ -236,7 +236,7 @@ SYM_CODE_START_LOCAL_NOALIGN(virtual_mapped)
 	popq	%r12
 	popq	%rbp
 	popq	%rbx
-	ret
+	RET
 SYM_CODE_END(virtual_mapped)
 
 	/* Do the copies */
@@ -293,7 +293,7 @@ SYM_CODE_START_LOCAL_NOALIGN(swap_pages)
 	lea	PAGE_SIZE(%rax), %rsi
 	jmp	0b
 3:
-	ret
+	RET
 SYM_CODE_END(swap_pages)
 
 	.globl kexec_control_code_size
diff --git a/arch/x86/kernel/sev_verify_cbit.S b/arch/x86/kernel/sev_verify_cbit.S
index ee04941a6546..3355e27c69eb 100644
--- a/arch/x86/kernel/sev_verify_cbit.S
+++ b/arch/x86/kernel/sev_verify_cbit.S
@@ -85,5 +85,5 @@ SYM_FUNC_START(sev_verify_cbit)
 #endif
 	/* Return page-table pointer */
 	movq	%rdi, %rax
-	ret
+	RET
 SYM_FUNC_END(sev_verify_cbit)
* Unmerged path arch/x86/kernel/verify_cpu.S
diff --git a/arch/x86/kvm/svm/vmenter.S b/arch/x86/kvm/svm/vmenter.S
index 4fa17df123cd..dfaeb47fcf2a 100644
--- a/arch/x86/kvm/svm/vmenter.S
+++ b/arch/x86/kvm/svm/vmenter.S
@@ -148,7 +148,7 @@ SYM_FUNC_START(__svm_vcpu_run)
 	pop %edi
 #endif
 	pop %_ASM_BP
-	ret
+	RET
 
 3:	cmpb $0, kvm_rebooting
 	jne 2b
@@ -202,7 +202,7 @@ SYM_FUNC_START(__svm_sev_es_vcpu_run)
 	pop %edi
 #endif
 	pop %_ASM_BP
-	ret
+	RET
 
 3:	cmpb $0, kvm_rebooting
 	jne 2b
diff --git a/arch/x86/kvm/vmx/vmenter.S b/arch/x86/kvm/vmx/vmenter.S
index fd0a4aadb374..6e6b634ef585 100644
--- a/arch/x86/kvm/vmx/vmenter.S
+++ b/arch/x86/kvm/vmx/vmenter.S
@@ -49,14 +49,14 @@ SYM_FUNC_START_LOCAL(vmx_vmenter)
 	je 2f
 
 1:	vmresume
-	ret
+	RET
 
 2:	vmlaunch
-	ret
+	RET
 
 3:	cmpb $0, kvm_rebooting
 	je 4f
-	ret
+	RET
 4:	ud2
 
 	_ASM_EXTABLE(1b, 3b)
@@ -89,7 +89,7 @@ SYM_FUNC_START(vmx_vmexit)
 	pop %_ASM_AX
 .Lvmexit_skip_rsb:
 #endif
-	ret
+	RET
 SYM_FUNC_END(vmx_vmexit)
 
 /**
@@ -228,7 +228,7 @@ SYM_FUNC_START(__vmx_vcpu_run)
 	pop %edi
 #endif
 	pop %_ASM_BP
-	ret
+	RET
 
 	/* VM-Fail.  Out-of-line to avoid a taken Jcc after VM-Exit. */
 2:	mov $1, %eax
@@ -293,7 +293,7 @@ SYM_FUNC_START(vmread_error_trampoline)
 	pop %_ASM_AX
 	pop %_ASM_BP
 
-	ret
+	RET
 SYM_FUNC_END(vmread_error_trampoline)
 
 SYM_FUNC_START(vmx_do_interrupt_nmi_irqoff)
@@ -326,5 +326,5 @@ SYM_FUNC_START(vmx_do_interrupt_nmi_irqoff)
 	 */
 	mov %_ASM_BP, %_ASM_SP
 	pop %_ASM_BP
-	ret
+	RET
 SYM_FUNC_END(vmx_do_interrupt_nmi_irqoff)
* Unmerged path arch/x86/lib/atomic64_386_32.S
* Unmerged path arch/x86/lib/atomic64_cx8_32.S
* Unmerged path arch/x86/lib/checksum_32.S
diff --git a/arch/x86/lib/clear_page_64.S b/arch/x86/lib/clear_page_64.S
index 47aa2830010b..8ef8af838662 100644
--- a/arch/x86/lib/clear_page_64.S
+++ b/arch/x86/lib/clear_page_64.S
@@ -16,7 +16,7 @@ SYM_FUNC_START(clear_page_rep)
 	movl $4096/8,%ecx
 	xorl %eax,%eax
 	rep stosq
-	ret
+	RET
 SYM_FUNC_END(clear_page_rep)
 EXPORT_SYMBOL_GPL(clear_page_rep)
 
@@ -38,7 +38,7 @@ SYM_FUNC_START(clear_page_orig)
 	leaq	64(%rdi),%rdi
 	jnz	.Lloop
 	nop
-	ret
+	RET
 SYM_FUNC_END(clear_page_orig)
 EXPORT_SYMBOL_GPL(clear_page_orig)
 
@@ -46,6 +46,6 @@ SYM_FUNC_START(clear_page_erms)
 	movl $4096,%ecx
 	xorl %eax,%eax
 	rep stosb
-	ret
+	RET
 SYM_FUNC_END(clear_page_erms)
 EXPORT_SYMBOL_GPL(clear_page_erms)
diff --git a/arch/x86/lib/cmpxchg16b_emu.S b/arch/x86/lib/cmpxchg16b_emu.S
index b6ba6360b3ca..e15c058a6572 100644
--- a/arch/x86/lib/cmpxchg16b_emu.S
+++ b/arch/x86/lib/cmpxchg16b_emu.S
@@ -43,11 +43,11 @@ SYM_FUNC_START(this_cpu_cmpxchg16b_emu)
 
 	popfq
 	mov $1, %al
-	ret
+	RET
 
 .Lnot_same:
 	popfq
 	xor %al,%al
-	ret
+	RET
 
 SYM_FUNC_END(this_cpu_cmpxchg16b_emu)
diff --git a/arch/x86/lib/cmpxchg8b_emu.S b/arch/x86/lib/cmpxchg8b_emu.S
index 77aa18db3968..b1823efb5512 100644
--- a/arch/x86/lib/cmpxchg8b_emu.S
+++ b/arch/x86/lib/cmpxchg8b_emu.S
@@ -38,7 +38,7 @@ SYM_FUNC_START(cmpxchg8b_emu)
 	movl %ecx, 4(%esi)
 
 	popfl
-	ret
+	RET
 
 .Lnot_same:
 	movl  (%esi), %eax
@@ -46,7 +46,7 @@ SYM_FUNC_START(cmpxchg8b_emu)
 	movl 4(%esi), %edx
 
 	popfl
-	ret
+	RET
 
 SYM_FUNC_END(cmpxchg8b_emu)
 EXPORT_SYMBOL(cmpxchg8b_emu)
* Unmerged path arch/x86/lib/copy_mc_64.S
* Unmerged path arch/x86/lib/copy_page_64.S
diff --git a/arch/x86/lib/copy_user_64.S b/arch/x86/lib/copy_user_64.S
index a41fa4de85f2..d7ef6ea786fd 100644
--- a/arch/x86/lib/copy_user_64.S
+++ b/arch/x86/lib/copy_user_64.S
@@ -104,7 +104,7 @@ SYM_FUNC_START(copy_user_generic_unrolled)
 	jnz 21b
 23:	xor %eax,%eax
 	ASM_CLAC
-	ret
+	RET
 
 	.section .fixup,"ax"
 30:	shll $6,%ecx
@@ -172,7 +172,7 @@ SYM_FUNC_START(copy_user_generic_string)
 	movsb
 	xorl %eax,%eax
 	ASM_CLAC
-	ret
+	RET
 
 	.section .fixup,"ax"
 11:	leal (%rdx,%rcx,8),%ecx
@@ -206,7 +206,7 @@ SYM_FUNC_START(copy_user_enhanced_fast_string)
 	movsb
 	xorl %eax,%eax
 	ASM_CLAC
-	ret
+	RET
 
 	.section .fixup,"ax"
 12:	movl %ecx,%edx		/* ecx is zerorest also */
@@ -236,7 +236,7 @@ ALIGN;
 1:	rep movsb
 2:	mov %ecx,%eax
 	ASM_CLAC
-	ret
+	RET
 
 	_ASM_EXTABLE_UA(1b, 2b)
 END(.Lcopy_user_handle_tail)
@@ -347,7 +347,7 @@ SYM_FUNC_START(__copy_user_nocache)
 	xorl %eax,%eax
 	ASM_CLAC
 	sfence
-	ret
+	RET
 
 	.section .fixup,"ax"
 .L_fixup_4x8b_copy:
* Unmerged path arch/x86/lib/csum-copy_64.S
* Unmerged path arch/x86/lib/getuser.S
diff --git a/arch/x86/lib/hweight.S b/arch/x86/lib/hweight.S
index dbf8cc97b7f5..12c16c6aa44a 100644
--- a/arch/x86/lib/hweight.S
+++ b/arch/x86/lib/hweight.S
@@ -32,7 +32,7 @@ SYM_FUNC_START(__sw_hweight32)
 	imull $0x01010101, %eax, %eax		# w_tmp *= 0x01010101
 	shrl $24, %eax				# w = w_tmp >> 24
 	__ASM_SIZE(pop,) %__ASM_REG(dx)
-	ret
+	RET
 SYM_FUNC_END(__sw_hweight32)
 EXPORT_SYMBOL(__sw_hweight32)
 
@@ -65,7 +65,7 @@ SYM_FUNC_START(__sw_hweight64)
 
 	popq    %rdx
 	popq    %rdi
-	ret
+	RET
 #else /* CONFIG_X86_32 */
 	/* We're getting an u64 arg in (%eax,%edx): unsigned long hweight64(__u64 w) */
 	pushl   %ecx
@@ -77,7 +77,7 @@ SYM_FUNC_START(__sw_hweight64)
 	addl    %ecx, %eax                      # result
 
 	popl    %ecx
-	ret
+	RET
 #endif
 SYM_FUNC_END(__sw_hweight64)
 EXPORT_SYMBOL(__sw_hweight64)
diff --git a/arch/x86/lib/iomap_copy_64.S b/arch/x86/lib/iomap_copy_64.S
index 2246fbf32fa8..402f40e98aab 100644
--- a/arch/x86/lib/iomap_copy_64.S
+++ b/arch/x86/lib/iomap_copy_64.S
@@ -23,5 +23,5 @@
 SYM_FUNC_START(__iowrite32_copy)
 	movl %edx,%ecx
 	rep movsd
-	ret
+	RET
 SYM_FUNC_END(__iowrite32_copy)
* Unmerged path arch/x86/lib/memcpy_64.S
diff --git a/arch/x86/lib/memmove_64.S b/arch/x86/lib/memmove_64.S
index 7ff00ea64e4f..67e806c9c63d 100644
--- a/arch/x86/lib/memmove_64.S
+++ b/arch/x86/lib/memmove_64.S
@@ -42,7 +42,7 @@ SYM_FUNC_START(__memmove)
 	/* FSRM implies ERMS => no length checks, do the copy directly */
 .Lmemmove_begin_forward:
 	ALTERNATIVE "cmp $0x20, %rdx; jb 1f", "", X86_FEATURE_FSRM
-	ALTERNATIVE "", "movq %rdx, %rcx; rep movsb; retq", X86_FEATURE_ERMS
+	ALTERNATIVE "", "movq %rdx, %rcx; rep movsb; RET", X86_FEATURE_ERMS
 
 	/*
 	 * movsq instruction have many startup latency
@@ -207,7 +207,7 @@ SYM_FUNC_START(__memmove)
 	movb (%rsi), %r11b
 	movb %r11b, (%rdi)
 13:
-	retq
+	RET
 SYM_FUNC_END(__memmove)
 SYM_FUNC_END_ALIAS(memmove)
 EXPORT_SYMBOL(__memmove)
* Unmerged path arch/x86/lib/memset_64.S
diff --git a/arch/x86/lib/msr-reg.S b/arch/x86/lib/msr-reg.S
index a2b9caa5274c..ebd259f31496 100644
--- a/arch/x86/lib/msr-reg.S
+++ b/arch/x86/lib/msr-reg.S
@@ -35,7 +35,7 @@ SYM_FUNC_START(\op\()_safe_regs)
 	movl    %edi, 28(%r10)
 	popq %r12
 	popq %rbx
-	ret
+	RET
 3:
 	movl    $-EIO, %r11d
 	jmp     2b
@@ -77,7 +77,7 @@ SYM_FUNC_START(\op\()_safe_regs)
 	popl %esi
 	popl %ebp
 	popl %ebx
-	ret
+	RET
 3:
 	movl    $-EIO, 4(%esp)
 	jmp     2b
diff --git a/arch/x86/lib/putuser.S b/arch/x86/lib/putuser.S
index 742ebfaf8b56..5ec38b064297 100644
--- a/arch/x86/lib/putuser.S
+++ b/arch/x86/lib/putuser.S
@@ -42,7 +42,7 @@ SYM_FUNC_START(__put_user_1)
 1:	movb %al,(%_ASM_CX)
 	xor %eax,%eax
 	ASM_CLAC
-	ret
+	RET
 SYM_FUNC_END(__put_user_1)
 EXPORT_SYMBOL(__put_user_1)
 
@@ -56,7 +56,7 @@ SYM_FUNC_START(__put_user_2)
 2:	movw %ax,(%_ASM_CX)
 	xor %eax,%eax
 	ASM_CLAC
-	ret
+	RET
 SYM_FUNC_END(__put_user_2)
 EXPORT_SYMBOL(__put_user_2)
 
@@ -70,7 +70,7 @@ SYM_FUNC_START(__put_user_4)
 3:	movl %eax,(%_ASM_CX)
 	xor %eax,%eax
 	ASM_CLAC
-	ret
+	RET
 SYM_FUNC_END(__put_user_4)
 EXPORT_SYMBOL(__put_user_4)
 
* Unmerged path arch/x86/lib/retpoline.S
diff --git a/arch/x86/math-emu/div_Xsig.S b/arch/x86/math-emu/div_Xsig.S
index ee08449d20fd..ade02edfb83e 100644
--- a/arch/x86/math-emu/div_Xsig.S
+++ b/arch/x86/math-emu/div_Xsig.S
@@ -341,7 +341,7 @@ L_exit:
 	popl	%esi
 
 	leave
-	ret
+	RET
 
 
 #ifdef PARANOID
* Unmerged path arch/x86/math-emu/div_small.S
* Unmerged path arch/x86/math-emu/mul_Xsig.S
* Unmerged path arch/x86/math-emu/polynom_Xsig.S
* Unmerged path arch/x86/math-emu/reg_norm.S
diff --git a/arch/x86/math-emu/reg_round.S b/arch/x86/math-emu/reg_round.S
index 04563421ee7d..24cf35c10dc6 100644
--- a/arch/x86/math-emu/reg_round.S
+++ b/arch/x86/math-emu/reg_round.S
@@ -437,7 +437,7 @@ fpu_Arith_exit:
 	popl	%edi
 	popl	%esi
 	leave
-	ret
+	RET
 
 
 /*
diff --git a/arch/x86/math-emu/reg_u_add.S b/arch/x86/math-emu/reg_u_add.S
index 50fe9f8c893c..ba1e7d638de6 100644
--- a/arch/x86/math-emu/reg_u_add.S
+++ b/arch/x86/math-emu/reg_u_add.S
@@ -164,6 +164,6 @@ L_exit:
 	popl	%edi
 	popl	%esi
 	leave
-	ret
+	RET
 #endif /* PARANOID */
 ENDPROC(FPU_u_add)
diff --git a/arch/x86/math-emu/reg_u_div.S b/arch/x86/math-emu/reg_u_div.S
index 94d545e118e4..7b2442179880 100644
--- a/arch/x86/math-emu/reg_u_div.S
+++ b/arch/x86/math-emu/reg_u_div.S
@@ -468,7 +468,7 @@ L_exit:
 	popl	%esi
 
 	leave
-	ret
+	RET
 #endif /* PARANOID */ 
 
 ENDPROC(FPU_u_div)
diff --git a/arch/x86/math-emu/reg_u_mul.S b/arch/x86/math-emu/reg_u_mul.S
index 21cde47fb3e5..2bab04237843 100644
--- a/arch/x86/math-emu/reg_u_mul.S
+++ b/arch/x86/math-emu/reg_u_mul.S
@@ -144,7 +144,7 @@ L_exit:
 	popl	%edi
 	popl	%esi
 	leave
-	ret
+	RET
 #endif /* PARANOID */ 
 
 ENDPROC(FPU_u_mul)
* Unmerged path arch/x86/math-emu/reg_u_sub.S
* Unmerged path arch/x86/math-emu/round_Xsig.S
* Unmerged path arch/x86/math-emu/shr_Xsig.S
* Unmerged path arch/x86/math-emu/wm_shrx.S
diff --git a/arch/x86/mm/mem_encrypt_boot.S b/arch/x86/mm/mem_encrypt_boot.S
index 2c0a6fbd4fe8..d1f87ea050d5 100644
--- a/arch/x86/mm/mem_encrypt_boot.S
+++ b/arch/x86/mm/mem_encrypt_boot.S
@@ -68,7 +68,7 @@ SYM_FUNC_START(sme_encrypt_execute)
 	movq	%rbp, %rsp		/* Restore original stack pointer */
 	pop	%rbp
 
-	ret
+	RET
 SYM_FUNC_END(sme_encrypt_execute)
 
 SYM_FUNC_START(__enc_copy)
@@ -154,6 +154,6 @@ SYM_FUNC_START(__enc_copy)
 	pop	%r12
 	pop	%r15
 
-	ret
+	RET
 .L__enc_copy_end:
 SYM_FUNC_END(__enc_copy)
* Unmerged path arch/x86/platform/efi/efi_stub_32.S
* Unmerged path arch/x86/platform/efi/efi_stub_64.S
diff --git a/arch/x86/platform/efi/efi_thunk_64.S b/arch/x86/platform/efi/efi_thunk_64.S
index ab2753b8bbcb..adf372633d49 100644
--- a/arch/x86/platform/efi/efi_thunk_64.S
+++ b/arch/x86/platform/efi/efi_thunk_64.S
@@ -63,7 +63,7 @@ SYM_FUNC_START(efi64_thunk)
 1:	movq	24(%rsp), %rsp
 	pop	%rbx
 	pop	%rbp
-	retq
+	RET
 
 	.code32
 2:	pushl	$__KERNEL_CS
diff --git a/arch/x86/platform/olpc/xo1-wakeup.S b/arch/x86/platform/olpc/xo1-wakeup.S
index 75f4faff8468..3a5abffe5660 100644
--- a/arch/x86/platform/olpc/xo1-wakeup.S
+++ b/arch/x86/platform/olpc/xo1-wakeup.S
@@ -77,7 +77,7 @@ save_registers:
 	pushfl
 	popl saved_context_eflags
 
-	ret
+	RET
 
 restore_registers:
 	movl saved_context_ebp, %ebp
@@ -88,7 +88,7 @@ restore_registers:
 	pushl saved_context_eflags
 	popfl
 
-	ret
+	RET
 
 SYM_CODE_START(do_olpc_suspend_lowlevel)
 	call	save_processor_state
@@ -109,7 +109,7 @@ ret_point:
 
 	call	restore_registers
 	call	restore_processor_state
-	ret
+	RET
 SYM_CODE_END(do_olpc_suspend_lowlevel)
 
 .data
* Unmerged path arch/x86/power/hibernate_asm_32.S
* Unmerged path arch/x86/power/hibernate_asm_64.S
diff --git a/arch/x86/um/checksum_32.S b/arch/x86/um/checksum_32.S
index b9933eb9274a..846e355c7a92 100644
--- a/arch/x86/um/checksum_32.S
+++ b/arch/x86/um/checksum_32.S
@@ -114,7 +114,7 @@ csum_partial:
 7:	
 	popl %ebx
 	popl %esi
-	ret
+	RET
 
 #else
 
@@ -212,7 +212,7 @@ csum_partial:
 80: 
 	popl %ebx
 	popl %esi
-	ret
+	RET
 				
 #endif
 	EXPORT_SYMBOL(csum_partial)
diff --git a/arch/x86/um/setjmp_32.S b/arch/x86/um/setjmp_32.S
index 62eaf8c80e04..2d991ddbcca5 100644
--- a/arch/x86/um/setjmp_32.S
+++ b/arch/x86/um/setjmp_32.S
@@ -34,7 +34,7 @@ kernel_setjmp:
 	movl %esi,12(%edx)
 	movl %edi,16(%edx)
 	movl %ecx,20(%edx)		# Return address
-	ret
+	RET
 
 	.size kernel_setjmp,.-kernel_setjmp
 
diff --git a/arch/x86/um/setjmp_64.S b/arch/x86/um/setjmp_64.S
index 1b5d40d4ff46..b46acb6a8ebd 100644
--- a/arch/x86/um/setjmp_64.S
+++ b/arch/x86/um/setjmp_64.S
@@ -33,7 +33,7 @@ kernel_setjmp:
 	movq %r14,40(%rdi)
 	movq %r15,48(%rdi)
 	movq %rsi,56(%rdi)		# Return address
-	ret
+	RET
 
 	.size kernel_setjmp,.-kernel_setjmp
 
* Unmerged path arch/x86/xen/xen-asm.S
* Unmerged path arch/x86/xen/xen-head.S
