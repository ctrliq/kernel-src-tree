gfs2: Be more careful with the quota sync generation

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-553.16.1.rt7.357.el8_10
commit-author Andreas Gruenbacher <agruenba@redhat.com>
commit d9a75a60699dedaac17d2b5170bb2e3cdc03481e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-553.16.1.rt7.357.el8_10/d9a75a60.failed

The quota sync generation is only ever updated under sd_quota_sync_mutex
by gfs2_quota_sync(), but its current value is fetched ouside of that
mutex, so use WRITE_ONCE() and READ_ONCE() when accessing it without
holding that mutex.

Pass the current sync generation to do_sync() from its callers to ensure
that we're not recording the wrong generation when the syncing is
done.  Also, make sure that qd->qd_sync_gen only ever moves forward.

In gfs2_quota_sync(), only write the new sync generation when we know
that there are changes.  This eliminates the need for function
sd_changed(), which we will remove in the next commit.

	Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
(cherry picked from commit d9a75a60699dedaac17d2b5170bb2e3cdc03481e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/gfs2/quota.c
diff --cc fs/gfs2/quota.c
index a5ead0a8fccb,4f2caa06ca93..000000000000
--- a/fs/gfs2/quota.c
+++ b/fs/gfs2/quota.c
@@@ -890,9 -891,10 +890,10 @@@ static int gfs2_adjust_quota(struct gfs
  	return err;
  }
  
- static int do_sync(unsigned int num_qd, struct gfs2_quota_data **qda)
+ static int do_sync(unsigned int num_qd, struct gfs2_quota_data **qda,
+ 		   u64 sync_gen)
  {
 -	struct gfs2_sbd *sdp = (*qda)->qd_sbd;
 +	struct gfs2_sbd *sdp = (*qda)->qd_gl->gl_name.ln_sbd;
  	struct gfs2_inode *ip = GFS2_I(sdp->sd_quota_inode);
  	struct gfs2_alloc_parms ap = {};
  	unsigned int data_blocks, ind_blocks;
@@@ -1296,36 -1331,57 +1304,62 @@@ int gfs2_quota_sync(struct super_block 
  	struct gfs2_sbd *sdp = sb->s_fs_info;
  	struct gfs2_quota_data **qda;
  	unsigned int max_qd = PAGE_SIZE / sizeof(struct gfs2_holder);
++<<<<<<< HEAD
 +	unsigned int num_qd;
 +	unsigned int x;
++=======
+ 	u64 sync_gen;
++>>>>>>> d9a75a60699d (gfs2: Be more careful with the quota sync generation)
  	int error = 0;
  
 -	if (sb_rdonly(sdp->sd_vfs))
 -		return 0;
 -	if (!qd_changed(sdp))
 -		return 0;
 -
  	qda = kcalloc(max_qd, sizeof(struct gfs2_quota_data *), GFP_KERNEL);
  	if (!qda)
  		return -ENOMEM;
  
  	mutex_lock(&sdp->sd_quota_sync_mutex);
- 	sdp->sd_quota_sync_gen++;
+ 	sync_gen = sdp->sd_quota_sync_gen + 1;
  
  	do {
 -		struct gfs2_quota_data *iter;
 -		unsigned int num_qd = 0;
 -		unsigned int x;
 +		num_qd = 0;
  
++<<<<<<< HEAD
 +		for (;;) {
 +			error = qd_fish(sdp, qda + num_qd);
 +			if (error || !qda[num_qd])
 +				break;
 +			if (++num_qd == max_qd)
 +				break;
++=======
+ 		spin_lock(&qd_lock);
+ 		list_for_each_entry(iter, &sdp->sd_quota_list, qd_list) {
+ 			if (qd_grab_sync(sdp, iter, sync_gen)) {
+ 				qda[num_qd++] = iter;
+ 				if (num_qd == max_qd)
+ 					break;
+ 			}
++>>>>>>> d9a75a60699d (gfs2: Be more careful with the quota sync generation)
  		}
 -		spin_unlock(&qd_lock);
 -
 -		if (!num_qd)
 -			break;
  
 -		for (x = 0; x < num_qd; x++) {
 -			error = bh_get(qda[x]);
 +		if (num_qd) {
  			if (!error)
 -				continue;
 +				error = do_sync(num_qd, qda);
  
 -			while (x < num_qd)
 -				qd_ungrab_sync(qda[--num_qd]);
 -			break;
 +			for (x = 0; x < num_qd; x++)
 +				qd_unlock(qda[x]);
  		}
++<<<<<<< HEAD
 +	} while (!error && num_qd == max_qd);
++=======
+ 
+ 		if (!error) {
+ 			WRITE_ONCE(sdp->sd_quota_sync_gen, sync_gen);
+ 			error = do_sync(num_qd, qda, sync_gen);
+ 		}
+ 
+ 		for (x = 0; x < num_qd; x++)
+ 			qd_unlock(qda[x]);
+ 	} while (!error);
++>>>>>>> d9a75a60699d (gfs2: Be more careful with the quota sync generation)
  
  	mutex_unlock(&sdp->sd_quota_sync_mutex);
  	kfree(qda);
* Unmerged path fs/gfs2/quota.c
