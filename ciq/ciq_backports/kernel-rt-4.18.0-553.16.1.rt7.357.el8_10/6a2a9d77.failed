xfs: pass alloc flags through to xfs_extent_busy_flush()

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-553.16.1.rt7.357.el8_10
commit-author Dave Chinner <dchinner@redhat.com>
commit 6a2a9d776c4ae24a797e25eed2b9f7f33f756296
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-553.16.1.rt7.357.el8_10/6a2a9d77.failed

To avoid blocking in xfs_extent_busy_flush() when freeing extents
and the only busy extents are held by the current transaction, we
need to pass the XFS_ALLOC_FLAG_FREEING flag context all the way
into xfs_extent_busy_flush().

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Darrick J. Wong <djwong@kernel.org>
	Signed-off-by: Darrick J. Wong <djwong@kernel.org>
	Reviewed-by: Chandan Babu R <chandan.babu@oracle.com>
(cherry picked from commit 6a2a9d776c4ae24a797e25eed2b9f7f33f756296)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/libxfs/xfs_alloc.c
diff --cc fs/xfs/libxfs/xfs_alloc.c
index 2debb6bf24be,155ed3cce52c..000000000000
--- a/fs/xfs/libxfs/xfs_alloc.c
+++ b/fs/xfs/libxfs/xfs_alloc.c
@@@ -1684,21 -1636,22 +1685,36 @@@ out
   * and of the form k * prod + mod unless there's nothing that large.
   * Return the starting a.g. block, or NULLAGBLOCK if we can't do it.
   */
- STATIC int				/* error */
+ static int
  xfs_alloc_ag_vextent_size(
- 	xfs_alloc_arg_t	*args)		/* allocation argument structure */
+ 	struct xfs_alloc_arg	*args,
+ 	uint32_t		alloc_flags)
  {
++<<<<<<< HEAD
 +	struct xfs_agf	*agf = args->agbp->b_addr;
 +	xfs_btree_cur_t	*bno_cur;	/* cursor for bno btree */
 +	xfs_btree_cur_t	*cnt_cur;	/* cursor for cnt btree */
 +	int		error;		/* error result */
 +	xfs_agblock_t	fbno;		/* start of found freespace */
 +	xfs_extlen_t	flen;		/* length of found freespace */
 +	int		i;		/* temp status variable */
 +	xfs_agblock_t	rbno;		/* returned block number */
 +	xfs_extlen_t	rlen;		/* length of returned extent */
 +	bool		busy;
 +	unsigned	busy_gen;
++=======
+ 	struct xfs_agf		*agf = args->agbp->b_addr;
+ 	struct xfs_btree_cur	*bno_cur;
+ 	struct xfs_btree_cur	*cnt_cur;
+ 	xfs_agblock_t		fbno;		/* start of found freespace */
+ 	xfs_extlen_t		flen;		/* length of found freespace */
+ 	xfs_agblock_t		rbno;		/* returned block number */
+ 	xfs_extlen_t		rlen;		/* length of returned extent */
+ 	bool			busy;
+ 	unsigned		busy_gen;
+ 	int			error;
+ 	int			i;
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  
  restart:
  	/*
@@@ -2567,8 -2590,8 +2584,13 @@@ xfs_alloc_fix_freelist
  	/* deferred ops (AGFL block frees) require permanent transactions */
  	ASSERT(tp->t_flags & XFS_TRANS_PERM_LOG_RES);
  
++<<<<<<< HEAD
 +	if (!pag->pagf_init) {
 +		error = xfs_alloc_read_agf(mp, tp, args->agno, flags, &agbp);
++=======
+ 	if (!xfs_perag_initialised_agf(pag)) {
+ 		error = xfs_alloc_read_agf(pag, tp, alloc_flags, &agbp);
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  		if (error) {
  			/* Couldn't lock the AGF so skip this AG. */
  			if (error == -EAGAIN)
@@@ -2582,9 -2605,10 +2604,16 @@@
  	 * somewhere else if we are not being asked to try harder at this
  	 * point
  	 */
++<<<<<<< HEAD
 +	if (pag->pagf_metadata && (args->datatype & XFS_ALLOC_USERDATA) &&
 +	    (flags & XFS_ALLOC_FLAG_TRYLOCK)) {
 +		ASSERT(!(flags & XFS_ALLOC_FLAG_FREEING));
++=======
+ 	if (xfs_perag_prefers_metadata(pag) &&
+ 	    (args->datatype & XFS_ALLOC_USERDATA) &&
+ 	    (alloc_flags & XFS_ALLOC_FLAG_TRYLOCK)) {
+ 		ASSERT(!(alloc_flags & XFS_ALLOC_FLAG_FREEING));
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  		goto out_agbp_relse;
  	}
  
@@@ -2598,7 -2622,7 +2627,11 @@@
  	 * Can fail if we're not blocking on locks, and it's held.
  	 */
  	if (!agbp) {
++<<<<<<< HEAD
 +		error = xfs_alloc_read_agf(mp, tp, args->agno, flags, &agbp);
++=======
+ 		error = xfs_alloc_read_agf(pag, tp, alloc_flags, &agbp);
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  		if (error) {
  			/* Couldn't lock the AGF so skip this AG. */
  			if (error == -EAGAIN)
@@@ -2655,8 -2679,9 +2688,14 @@@
  		targs.oinfo = XFS_RMAP_OINFO_SKIP_UPDATE;
  	else
  		targs.oinfo = XFS_RMAP_OINFO_AG;
++<<<<<<< HEAD
 +	while (!(flags & XFS_ALLOC_FLAG_NOSHRINK) && pag->pagf_flcount > need) {
 +		error = xfs_alloc_get_freelist(tp, agbp, &bno, 0);
++=======
+ 	while (!(alloc_flags & XFS_ALLOC_FLAG_NOSHRINK) &&
+ 			pag->pagf_flcount > need) {
+ 		error = xfs_alloc_get_freelist(pag, tp, agbp, &bno, 0);
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  		if (error)
  			goto out_agbp_relse;
  
@@@ -2682,7 -2708,7 +2721,11 @@@
  		targs.resv = XFS_AG_RESV_AGFL;
  
  		/* Allocate as many blocks as possible at once. */
++<<<<<<< HEAD
 +		error = xfs_alloc_ag_vextent(&targs);
++=======
+ 		error = xfs_alloc_ag_vextent_size(&targs, alloc_flags);
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  		if (error)
  			goto out_agflbp_relse;
  
@@@ -3175,183 -3207,82 +3218,228 @@@ xfs_alloc_vextent
  	ASSERT(args->minlen <= args->maxlen);
  	ASSERT(args->minlen <= agsize);
  	ASSERT(args->mod < args->prod);
 -
 -	if (XFS_FSB_TO_AGNO(mp, target) >= mp->m_sb.sb_agcount ||
 -	    XFS_FSB_TO_AGBNO(mp, target) >= agsize ||
 +	if (XFS_FSB_TO_AGNO(mp, args->fsbno) >= mp->m_sb.sb_agcount ||
 +	    XFS_FSB_TO_AGBNO(mp, args->fsbno) >= agsize ||
  	    args->minlen > args->maxlen || args->minlen > agsize ||
  	    args->mod >= args->prod) {
 +		args->fsbno = NULLFSBLOCK;
  		trace_xfs_alloc_vextent_badargs(args);
++<<<<<<< HEAD
++=======
+ 		return -ENOSPC;
+ 	}
+ 
+ 	if (args->agno != NULLAGNUMBER && *minimum_agno > args->agno) {
+ 		trace_xfs_alloc_vextent_skip_deadlock(args);
+ 		return -ENOSPC;
+ 	}
+ 	return 0;
+ 
+ }
+ 
+ /*
+  * Prepare an AG for allocation. If the AG is not prepared to accept the
+  * allocation, return failure.
+  *
+  * XXX(dgc): The complexity of "need_pag" will go away as all caller paths are
+  * modified to hold their own perag references.
+  */
+ static int
+ xfs_alloc_vextent_prepare_ag(
+ 	struct xfs_alloc_arg	*args,
+ 	uint32_t		alloc_flags)
+ {
+ 	bool			need_pag = !args->pag;
+ 	int			error;
+ 
+ 	if (need_pag)
+ 		args->pag = xfs_perag_get(args->mp, args->agno);
+ 
+ 	args->agbp = NULL;
+ 	error = xfs_alloc_fix_freelist(args, alloc_flags);
+ 	if (error) {
+ 		trace_xfs_alloc_vextent_nofix(args);
+ 		if (need_pag)
+ 			xfs_perag_put(args->pag);
+ 		args->agbno = NULLAGBLOCK;
+ 		return error;
+ 	}
+ 	if (!args->agbp) {
+ 		/* cannot allocate in this AG at all */
+ 		trace_xfs_alloc_vextent_noagbp(args);
+ 		args->agbno = NULLAGBLOCK;
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  		return 0;
  	}
 -	args->wasfromfl = 0;
 -	return 0;
 -}
  
 -/*
 - * Post-process allocation results to account for the allocation if it succeed
 - * and set the allocated block number correctly for the caller.
 - *
 - * XXX: we should really be returning ENOSPC for ENOSPC, not
 - * hiding it behind a "successful" NULLFSBLOCK allocation.
 - */
 -static int
 -xfs_alloc_vextent_finish(
 -	struct xfs_alloc_arg	*args,
 -	xfs_agnumber_t		minimum_agno,
 -	int			alloc_error,
 -	bool			drop_perag)
 -{
 -	struct xfs_mount	*mp = args->mp;
 -	int			error = 0;
 +	switch (type) {
 +	case XFS_ALLOCTYPE_THIS_AG:
 +	case XFS_ALLOCTYPE_NEAR_BNO:
 +	case XFS_ALLOCTYPE_THIS_BNO:
 +		/*
 +		 * These three force us into a single a.g.
 +		 */
 +		args->agno = XFS_FSB_TO_AGNO(mp, args->fsbno);
 +		args->pag = xfs_perag_get(mp, args->agno);
 +
 +		if (minimum_agno > args->agno) {
 +			trace_xfs_alloc_vextent_skip_deadlock(args);
 +			error = 0;
 +			break;
 +		}
 +
 +		error = xfs_alloc_fix_freelist(args, 0);
 +		if (error) {
 +			trace_xfs_alloc_vextent_nofix(args);
 +			goto error0;
 +		}
 +		if (!args->agbp) {
 +			trace_xfs_alloc_vextent_noagbp(args);
 +			break;
 +		}
 +		args->agbno = XFS_FSB_TO_AGBNO(mp, args->fsbno);
 +		if ((error = xfs_alloc_ag_vextent(args)))
 +			goto error0;
 +		break;
 +	case XFS_ALLOCTYPE_START_BNO:
 +		/*
 +		 * Try near allocation first, then anywhere-in-ag after
 +		 * the first a.g. fails.
 +		 */
 +		if ((args->datatype & XFS_ALLOC_INITIAL_USER_DATA) &&
 +		    (mp->m_flags & XFS_MOUNT_32BITINODES)) {
 +			args->fsbno = XFS_AGB_TO_FSB(mp,
 +					((mp->m_agfrotor / rotorstep) %
 +					mp->m_sb.sb_agcount), 0);
 +			bump_rotor = 1;
 +		}
 +		args->agbno = XFS_FSB_TO_AGBNO(mp, args->fsbno);
 +		args->type = XFS_ALLOCTYPE_NEAR_BNO;
 +		fallthrough;
 +	case XFS_ALLOCTYPE_FIRST_AG:
 +		/*
 +		 * Rotate through the allocation groups looking for a winner.
 +		 * If we are blocking, we must obey minimum_agno contraints for
 +		 * avoiding ABBA deadlocks on AGF locking.
 +		 */
 +		if (type == XFS_ALLOCTYPE_FIRST_AG) {
 +			/*
 +			 * Start with allocation group given by bno.
 +			 */
 +			args->agno = XFS_FSB_TO_AGNO(mp, args->fsbno);
 +			args->type = XFS_ALLOCTYPE_THIS_AG;
 +			sagno = minimum_agno;
 +			flags = 0;
 +		} else {
 +			/*
 +			 * Start with the given allocation group.
 +			 */
 +			args->agno = sagno = XFS_FSB_TO_AGNO(mp, args->fsbno);
 +			flags = XFS_ALLOC_FLAG_TRYLOCK;
 +		}
 +
 +		/*
 +		 * Loop over allocation groups twice; first time with
 +		 * trylock set, second time without.
 +		 */
 +		for (;;) {
 +			args->pag = xfs_perag_get(mp, args->agno);
 +			error = xfs_alloc_fix_freelist(args, flags);
 +			if (error) {
 +				trace_xfs_alloc_vextent_nofix(args);
 +				goto error0;
 +			}
 +			/*
 +			 * If we get a buffer back then the allocation will fly.
 +			 */
 +			if (args->agbp) {
 +				if ((error = xfs_alloc_ag_vextent(args)))
 +					goto error0;
 +				break;
 +			}
 +
 +			trace_xfs_alloc_vextent_loopfailed(args);
 +
 +			/*
 +			 * Didn't work, figure out the next iteration.
 +			 */
 +			if (args->agno == sagno &&
 +			    type == XFS_ALLOCTYPE_START_BNO)
 +				args->type = XFS_ALLOCTYPE_THIS_AG;
 +
 +			/*
 +			 * If we are try-locking, we can't deadlock on AGF
 +			 * locks, so we can wrap all the way back to the first
 +			 * AG. Otherwise, wrap back to the start AG so we can't
 +			 * deadlock, and let the end of scan handler decide what
 +			 * to do next.
 +			 */
 +			if (++(args->agno) == mp->m_sb.sb_agcount) {
 +				if (flags & XFS_ALLOC_FLAG_TRYLOCK)
 +					args->agno = 0;
 +				else
 +					args->agno = sagno;
 +			}
 +
 +			/*
 +			 * Reached the starting a.g., must either be done
 +			 * or switch to non-trylock mode.
 +			 */
 +			if (args->agno == sagno) {
 +				if (flags == 0) {
 +					args->agbno = NULLAGBLOCK;
 +					trace_xfs_alloc_vextent_allfailed(args);
 +					break;
 +				}
 +
 +				/*
 +				 * Blocking pass next, so we must obey minimum
 +				 * agno constraints to avoid ABBA AGF deadlocks.
 +				 */
 +				flags = 0;
 +				if (minimum_agno > sagno)
 +					sagno = minimum_agno;
 +
 +				if (type == XFS_ALLOCTYPE_START_BNO) {
 +					args->agbno = XFS_FSB_TO_AGBNO(mp,
 +						args->fsbno);
 +					args->type = XFS_ALLOCTYPE_NEAR_BNO;
 +				}
 +			}
 +			xfs_perag_put(args->pag);
 +		}
 +		if (bump_rotor) {
 +			if (args->agno == sagno)
 +				mp->m_agfrotor = (mp->m_agfrotor + 1) %
 +					(mp->m_sb.sb_agcount * rotorstep);
 +			else
 +				mp->m_agfrotor = (args->agno * rotorstep + 1) %
 +					(mp->m_sb.sb_agcount * rotorstep);
 +		}
 +		break;
 +	default:
 +		ASSERT(0);
 +		/* NOTREACHED */
 +	}
 +	if (args->agbno == NULLAGBLOCK) {
 +		args->fsbno = NULLFSBLOCK;
 +	} else {
 +		args->fsbno = XFS_AGB_TO_FSB(mp, args->agno, args->agbno);
 +#ifdef DEBUG
 +		ASSERT(args->len >= args->minlen);
 +		ASSERT(args->len <= args->maxlen);
 +		ASSERT(args->agbno % args->alignment == 0);
 +		XFS_AG_CHECK_DADDR(mp, XFS_FSB_TO_DADDR(mp, args->fsbno),
 +			args->len);
 +#endif
 +
 +	}
  
  	/*
 -	 * We can end up here with a locked AGF. If we failed, the caller is
 -	 * likely going to try to allocate again with different parameters, and
 -	 * that can widen the AGs that are searched for free space. If we have
 -	 * to do BMBT block allocation, we have to do a new allocation.
 +	 * We end up here with a locked AGF. If we failed, the caller is likely
 +	 * going to try to allocate again with different parameters, and that
 +	 * can widen the AGs that are searched for free space. If we have to do
 +	 * BMBT block allocation, we have to do a new allocation.
  	 *
  	 * Hence leaving this function with the AGF locked opens up potential
  	 * ABBA AGF deadlocks because a future allocation attempt in this
@@@ -3364,16 -3295,354 +3452,352 @@@
  	 * deadlocks.
  	 */
  	if (args->agbp &&
 -	    (args->tp->t_highest_agno == NULLAGNUMBER ||
 -	     args->agno > minimum_agno))
 -		args->tp->t_highest_agno = args->agno;
 -
 -	/*
 -	 * If the allocation failed with an error or we had an ENOSPC result,
 -	 * preserve the returned error whilst also marking the allocation result
 -	 * as "no extent allocated". This ensures that callers that fail to
 -	 * capture the error will still treat it as a failed allocation.
 -	 */
 -	if (alloc_error || args->agbno == NULLAGBLOCK) {
 -		args->fsbno = NULLFSBLOCK;
 -		error = alloc_error;
 -		goto out_drop_perag;
 +	    (args->tp->t_firstblock == NULLFSBLOCK ||
 +	     args->pag->pag_agno > minimum_agno)) {
 +		args->tp->t_firstblock = XFS_AGB_TO_FSB(mp,
 +					args->pag->pag_agno, 0);
  	}
++<<<<<<< HEAD
 +	xfs_perag_put(args->pag);
 +	return 0;
 +error0:
 +	xfs_perag_put(args->pag);
 +	return error;
++=======
+ 
+ 	args->fsbno = XFS_AGB_TO_FSB(mp, args->agno, args->agbno);
+ 
+ 	ASSERT(args->len >= args->minlen);
+ 	ASSERT(args->len <= args->maxlen);
+ 	ASSERT(args->agbno % args->alignment == 0);
+ 	XFS_AG_CHECK_DADDR(mp, XFS_FSB_TO_DADDR(mp, args->fsbno), args->len);
+ 
+ 	/* if not file data, insert new block into the reverse map btree */
+ 	if (!xfs_rmap_should_skip_owner_update(&args->oinfo)) {
+ 		error = xfs_rmap_alloc(args->tp, args->agbp, args->pag,
+ 				       args->agbno, args->len, &args->oinfo);
+ 		if (error)
+ 			goto out_drop_perag;
+ 	}
+ 
+ 	if (!args->wasfromfl) {
+ 		error = xfs_alloc_update_counters(args->tp, args->agbp,
+ 						  -((long)(args->len)));
+ 		if (error)
+ 			goto out_drop_perag;
+ 
+ 		ASSERT(!xfs_extent_busy_search(mp, args->pag, args->agbno,
+ 				args->len));
+ 	}
+ 
+ 	xfs_ag_resv_alloc_extent(args->pag, args->resv, args);
+ 
+ 	XFS_STATS_INC(mp, xs_allocx);
+ 	XFS_STATS_ADD(mp, xs_allocb, args->len);
+ 
+ 	trace_xfs_alloc_vextent_finish(args);
+ 
+ out_drop_perag:
+ 	if (drop_perag && args->pag) {
+ 		xfs_perag_rele(args->pag);
+ 		args->pag = NULL;
+ 	}
+ 	return error;
+ }
+ 
+ /*
+  * Allocate within a single AG only. This uses a best-fit length algorithm so if
+  * you need an exact sized allocation without locality constraints, this is the
+  * fastest way to do it.
+  *
+  * Caller is expected to hold a perag reference in args->pag.
+  */
+ int
+ xfs_alloc_vextent_this_ag(
+ 	struct xfs_alloc_arg	*args,
+ 	xfs_agnumber_t		agno)
+ {
+ 	struct xfs_mount	*mp = args->mp;
+ 	xfs_agnumber_t		minimum_agno;
+ 	uint32_t		alloc_flags = 0;
+ 	int			error;
+ 
+ 	ASSERT(args->pag != NULL);
+ 	ASSERT(args->pag->pag_agno == agno);
+ 
+ 	args->agno = agno;
+ 	args->agbno = 0;
+ 
+ 	trace_xfs_alloc_vextent_this_ag(args);
+ 
+ 	error = xfs_alloc_vextent_check_args(args, XFS_AGB_TO_FSB(mp, agno, 0),
+ 			&minimum_agno);
+ 	if (error) {
+ 		if (error == -ENOSPC)
+ 			return 0;
+ 		return error;
+ 	}
+ 
+ 	error = xfs_alloc_vextent_prepare_ag(args, alloc_flags);
+ 	if (!error && args->agbp)
+ 		error = xfs_alloc_ag_vextent_size(args, alloc_flags);
+ 
+ 	return xfs_alloc_vextent_finish(args, minimum_agno, error, false);
+ }
+ 
+ /*
+  * Iterate all AGs trying to allocate an extent starting from @start_ag.
+  *
+  * If the incoming allocation type is XFS_ALLOCTYPE_NEAR_BNO, it means the
+  * allocation attempts in @start_agno have locality information. If we fail to
+  * allocate in that AG, then we revert to anywhere-in-AG for all the other AGs
+  * we attempt to allocation in as there is no locality optimisation possible for
+  * those allocations.
+  *
+  * On return, args->pag may be left referenced if we finish before the "all
+  * failed" return point. The allocation finish still needs the perag, and
+  * so the caller will release it once they've finished the allocation.
+  *
+  * When we wrap the AG iteration at the end of the filesystem, we have to be
+  * careful not to wrap into AGs below ones we already have locked in the
+  * transaction if we are doing a blocking iteration. This will result in an
+  * out-of-order locking of AGFs and hence can cause deadlocks.
+  */
+ static int
+ xfs_alloc_vextent_iterate_ags(
+ 	struct xfs_alloc_arg	*args,
+ 	xfs_agnumber_t		minimum_agno,
+ 	xfs_agnumber_t		start_agno,
+ 	xfs_agblock_t		target_agbno,
+ 	uint32_t		alloc_flags)
+ {
+ 	struct xfs_mount	*mp = args->mp;
+ 	xfs_agnumber_t		restart_agno = minimum_agno;
+ 	xfs_agnumber_t		agno;
+ 	int			error = 0;
+ 
+ 	if (alloc_flags & XFS_ALLOC_FLAG_TRYLOCK)
+ 		restart_agno = 0;
+ restart:
+ 	for_each_perag_wrap_range(mp, start_agno, restart_agno,
+ 			mp->m_sb.sb_agcount, agno, args->pag) {
+ 		args->agno = agno;
+ 		error = xfs_alloc_vextent_prepare_ag(args, alloc_flags);
+ 		if (error)
+ 			break;
+ 		if (!args->agbp) {
+ 			trace_xfs_alloc_vextent_loopfailed(args);
+ 			continue;
+ 		}
+ 
+ 		/*
+ 		 * Allocation is supposed to succeed now, so break out of the
+ 		 * loop regardless of whether we succeed or not.
+ 		 */
+ 		if (args->agno == start_agno && target_agbno) {
+ 			args->agbno = target_agbno;
+ 			error = xfs_alloc_ag_vextent_near(args, alloc_flags);
+ 		} else {
+ 			args->agbno = 0;
+ 			error = xfs_alloc_ag_vextent_size(args, alloc_flags);
+ 		}
+ 		break;
+ 	}
+ 	if (error) {
+ 		xfs_perag_rele(args->pag);
+ 		args->pag = NULL;
+ 		return error;
+ 	}
+ 	if (args->agbp)
+ 		return 0;
+ 
+ 	/*
+ 	 * We didn't find an AG we can alloation from. If we were given
+ 	 * constraining flags by the caller, drop them and retry the allocation
+ 	 * without any constraints being set.
+ 	 */
+ 	if (alloc_flags & XFS_ALLOC_FLAG_TRYLOCK) {
+ 		alloc_flags &= ~XFS_ALLOC_FLAG_TRYLOCK;
+ 		restart_agno = minimum_agno;
+ 		goto restart;
+ 	}
+ 
+ 	ASSERT(args->pag == NULL);
+ 	trace_xfs_alloc_vextent_allfailed(args);
+ 	return 0;
+ }
+ 
+ /*
+  * Iterate from the AGs from the start AG to the end of the filesystem, trying
+  * to allocate blocks. It starts with a near allocation attempt in the initial
+  * AG, then falls back to anywhere-in-ag after the first AG fails. It will wrap
+  * back to zero if allowed by previous allocations in this transaction,
+  * otherwise will wrap back to the start AG and run a second blocking pass to
+  * the end of the filesystem.
+  */
+ int
+ xfs_alloc_vextent_start_ag(
+ 	struct xfs_alloc_arg	*args,
+ 	xfs_fsblock_t		target)
+ {
+ 	struct xfs_mount	*mp = args->mp;
+ 	xfs_agnumber_t		minimum_agno;
+ 	xfs_agnumber_t		start_agno;
+ 	xfs_agnumber_t		rotorstep = xfs_rotorstep;
+ 	bool			bump_rotor = false;
+ 	uint32_t		alloc_flags = XFS_ALLOC_FLAG_TRYLOCK;
+ 	int			error;
+ 
+ 	ASSERT(args->pag == NULL);
+ 
+ 	args->agno = NULLAGNUMBER;
+ 	args->agbno = NULLAGBLOCK;
+ 
+ 	trace_xfs_alloc_vextent_start_ag(args);
+ 
+ 	error = xfs_alloc_vextent_check_args(args, target, &minimum_agno);
+ 	if (error) {
+ 		if (error == -ENOSPC)
+ 			return 0;
+ 		return error;
+ 	}
+ 
+ 	if ((args->datatype & XFS_ALLOC_INITIAL_USER_DATA) &&
+ 	    xfs_is_inode32(mp)) {
+ 		target = XFS_AGB_TO_FSB(mp,
+ 				((mp->m_agfrotor / rotorstep) %
+ 				mp->m_sb.sb_agcount), 0);
+ 		bump_rotor = 1;
+ 	}
+ 
+ 	start_agno = max(minimum_agno, XFS_FSB_TO_AGNO(mp, target));
+ 	error = xfs_alloc_vextent_iterate_ags(args, minimum_agno, start_agno,
+ 			XFS_FSB_TO_AGBNO(mp, target), alloc_flags);
+ 
+ 	if (bump_rotor) {
+ 		if (args->agno == start_agno)
+ 			mp->m_agfrotor = (mp->m_agfrotor + 1) %
+ 				(mp->m_sb.sb_agcount * rotorstep);
+ 		else
+ 			mp->m_agfrotor = (args->agno * rotorstep + 1) %
+ 				(mp->m_sb.sb_agcount * rotorstep);
+ 	}
+ 
+ 	return xfs_alloc_vextent_finish(args, minimum_agno, error, true);
+ }
+ 
+ /*
+  * Iterate from the agno indicated via @target through to the end of the
+  * filesystem attempting blocking allocation. This does not wrap or try a second
+  * pass, so will not recurse into AGs lower than indicated by the target.
+  */
+ int
+ xfs_alloc_vextent_first_ag(
+ 	struct xfs_alloc_arg	*args,
+ 	xfs_fsblock_t		target)
+  {
+ 	struct xfs_mount	*mp = args->mp;
+ 	xfs_agnumber_t		minimum_agno;
+ 	xfs_agnumber_t		start_agno;
+ 	uint32_t		alloc_flags = XFS_ALLOC_FLAG_TRYLOCK;
+ 	int			error;
+ 
+ 	ASSERT(args->pag == NULL);
+ 
+ 	args->agno = NULLAGNUMBER;
+ 	args->agbno = NULLAGBLOCK;
+ 
+ 	trace_xfs_alloc_vextent_first_ag(args);
+ 
+ 	error = xfs_alloc_vextent_check_args(args, target, &minimum_agno);
+ 	if (error) {
+ 		if (error == -ENOSPC)
+ 			return 0;
+ 		return error;
+ 	}
+ 
+ 	start_agno = max(minimum_agno, XFS_FSB_TO_AGNO(mp, target));
+ 	error = xfs_alloc_vextent_iterate_ags(args, minimum_agno, start_agno,
+ 			XFS_FSB_TO_AGBNO(mp, target), alloc_flags);
+ 	return xfs_alloc_vextent_finish(args, minimum_agno, error, true);
+ }
+ 
+ /*
+  * Allocate at the exact block target or fail. Caller is expected to hold a
+  * perag reference in args->pag.
+  */
+ int
+ xfs_alloc_vextent_exact_bno(
+ 	struct xfs_alloc_arg	*args,
+ 	xfs_fsblock_t		target)
+ {
+ 	struct xfs_mount	*mp = args->mp;
+ 	xfs_agnumber_t		minimum_agno;
+ 	int			error;
+ 
+ 	ASSERT(args->pag != NULL);
+ 	ASSERT(args->pag->pag_agno == XFS_FSB_TO_AGNO(mp, target));
+ 
+ 	args->agno = XFS_FSB_TO_AGNO(mp, target);
+ 	args->agbno = XFS_FSB_TO_AGBNO(mp, target);
+ 
+ 	trace_xfs_alloc_vextent_exact_bno(args);
+ 
+ 	error = xfs_alloc_vextent_check_args(args, target, &minimum_agno);
+ 	if (error) {
+ 		if (error == -ENOSPC)
+ 			return 0;
+ 		return error;
+ 	}
+ 
+ 	error = xfs_alloc_vextent_prepare_ag(args, 0);
+ 	if (!error && args->agbp)
+ 		error = xfs_alloc_ag_vextent_exact(args);
+ 
+ 	return xfs_alloc_vextent_finish(args, minimum_agno, error, false);
+ }
+ 
+ /*
+  * Allocate an extent as close to the target as possible. If there are not
+  * viable candidates in the AG, then fail the allocation.
+  *
+  * Caller may or may not have a per-ag reference in args->pag.
+  */
+ int
+ xfs_alloc_vextent_near_bno(
+ 	struct xfs_alloc_arg	*args,
+ 	xfs_fsblock_t		target)
+ {
+ 	struct xfs_mount	*mp = args->mp;
+ 	xfs_agnumber_t		minimum_agno;
+ 	bool			needs_perag = args->pag == NULL;
+ 	uint32_t		alloc_flags = 0;
+ 	int			error;
+ 
+ 	if (!needs_perag)
+ 		ASSERT(args->pag->pag_agno == XFS_FSB_TO_AGNO(mp, target));
+ 
+ 	args->agno = XFS_FSB_TO_AGNO(mp, target);
+ 	args->agbno = XFS_FSB_TO_AGBNO(mp, target);
+ 
+ 	trace_xfs_alloc_vextent_near_bno(args);
+ 
+ 	error = xfs_alloc_vextent_check_args(args, target, &minimum_agno);
+ 	if (error) {
+ 		if (error == -ENOSPC)
+ 			return 0;
+ 		return error;
+ 	}
+ 
+ 	if (needs_perag)
+ 		args->pag = xfs_perag_grab(mp, args->agno);
+ 
+ 	error = xfs_alloc_vextent_prepare_ag(args, alloc_flags);
+ 	if (!error && args->agbp)
+ 		error = xfs_alloc_ag_vextent_near(args, alloc_flags);
+ 
+ 	return xfs_alloc_vextent_finish(args, minimum_agno, error, needs_perag);
++>>>>>>> 6a2a9d776c4a (xfs: pass alloc flags through to xfs_extent_busy_flush())
  }
  
  /* Ensure that the freelist is at full capacity. */
* Unmerged path fs/xfs/libxfs/xfs_alloc.c
diff --git a/fs/xfs/libxfs/xfs_alloc.h b/fs/xfs/libxfs/xfs_alloc.h
index 8f05e92ddf79..21f803d107bf 100644
--- a/fs/xfs/libxfs/xfs_alloc.h
+++ b/fs/xfs/libxfs/xfs_alloc.h
@@ -212,7 +212,7 @@ int xfs_alloc_read_agfl(struct xfs_mount *mp, struct xfs_trans *tp,
 			xfs_agnumber_t agno, struct xfs_buf **bpp);
 int xfs_free_agfl_block(struct xfs_trans *, xfs_agnumber_t, xfs_agblock_t,
 			struct xfs_buf *, struct xfs_owner_info *);
-int xfs_alloc_fix_freelist(struct xfs_alloc_arg *args, int flags);
+int xfs_alloc_fix_freelist(struct xfs_alloc_arg *args, uint32_t alloc_flags);
 int xfs_free_extent_fix_freelist(struct xfs_trans *tp, struct xfs_perag *pag,
 		struct xfs_buf **agbp);
 
diff --git a/fs/xfs/xfs_extent_busy.c b/fs/xfs/xfs_extent_busy.c
index 38503bfe9638..17e6113d366d 100644
--- a/fs/xfs/xfs_extent_busy.c
+++ b/fs/xfs/xfs_extent_busy.c
@@ -571,7 +571,8 @@ void
 xfs_extent_busy_flush(
 	struct xfs_mount	*mp,
 	struct xfs_perag	*pag,
-	unsigned		busy_gen)
+	unsigned		busy_gen,
+	uint32_t		alloc_flags)
 {
 	DEFINE_WAIT		(wait);
 	int			error;
diff --git a/fs/xfs/xfs_extent_busy.h b/fs/xfs/xfs_extent_busy.h
index 8031617bb6ef..9610b79687b3 100644
--- a/fs/xfs/xfs_extent_busy.h
+++ b/fs/xfs/xfs_extent_busy.h
@@ -53,7 +53,7 @@ xfs_extent_busy_trim(struct xfs_alloc_arg *args, xfs_agblock_t *bno,
 
 void
 xfs_extent_busy_flush(struct xfs_mount *mp, struct xfs_perag *pag,
-	unsigned busy_gen);
+	unsigned busy_gen, uint32_t alloc_flags);
 
 void
 xfs_extent_busy_wait_all(struct xfs_mount *mp);
