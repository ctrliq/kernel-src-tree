net/mlx5: Fix error print in case of IRQ request failed

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Shay Drory <shayd@nvidia.com>
commit aa968f922039706f6d13e8870b49e424d0a8d9ad
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/aa968f92.failed

In case IRQ layer failed to find or to request irq, the driver is
printing the first cpu of the provided affinity as part of the error
print. Empty affinity is a valid input for the IRQ layer, and it is
an error to call cpumask_first() on empty affinity.

Remove the first cpu print from the error message.

Fixes: c36326d38d93 ("net/mlx5: Round-Robin EQs over IRQs")
	Signed-off-by: Shay Drory <shayd@nvidia.com>
	Reviewed-by: Moshe Shemesh <moshe@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit aa968f922039706f6d13e8870b49e424d0a8d9ad)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
index 9ba149578ea8,bcee30f5de0a..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
@@@ -250,6 -293,126 +250,129 @@@ struct cpumask *mlx5_irq_get_affinity_m
  	return irq->mask;
  }
  
++<<<<<<< HEAD
++=======
+ int mlx5_irq_get_index(struct mlx5_irq *irq)
+ {
+ 	return irq->index;
+ }
+ 
+ /* irq_pool API */
+ 
+ /* creating an irq from irq_pool */
+ static struct mlx5_irq *irq_pool_create_irq(struct mlx5_irq_pool *pool,
+ 					    struct cpumask *affinity)
+ {
+ 	struct mlx5_irq *irq;
+ 	u32 irq_index;
+ 	int err;
+ 
+ 	err = xa_alloc(&pool->irqs, &irq_index, NULL, pool->xa_num_irqs,
+ 		       GFP_KERNEL);
+ 	if (err)
+ 		return ERR_PTR(err);
+ 	irq = irq_request(pool, irq_index);
+ 	if (IS_ERR(irq))
+ 		return irq;
+ 	cpumask_copy(irq->mask, affinity);
+ 	irq_set_affinity_hint(irq->irqn, irq->mask);
+ 	return irq;
+ }
+ 
+ /* looking for the irq with the smallest refcount and the same affinity */
+ static struct mlx5_irq *irq_pool_find_least_loaded(struct mlx5_irq_pool *pool,
+ 						   struct cpumask *affinity)
+ {
+ 	int start = pool->xa_num_irqs.min;
+ 	int end = pool->xa_num_irqs.max;
+ 	struct mlx5_irq *irq = NULL;
+ 	struct mlx5_irq *iter;
+ 	unsigned long index;
+ 
+ 	lockdep_assert_held(&pool->lock);
+ 	xa_for_each_range(&pool->irqs, index, iter, start, end) {
+ 		if (!cpumask_equal(iter->mask, affinity))
+ 			continue;
+ 		if (iter->refcount < pool->min_threshold)
+ 			return iter;
+ 		if (!irq || iter->refcount < irq->refcount)
+ 			irq = iter;
+ 	}
+ 	return irq;
+ }
+ 
+ /* requesting an irq from a given pool according to given affinity */
+ static struct mlx5_irq *irq_pool_request_affinity(struct mlx5_irq_pool *pool,
+ 						  struct cpumask *affinity)
+ {
+ 	struct mlx5_irq *least_loaded_irq, *new_irq;
+ 
+ 	mutex_lock(&pool->lock);
+ 	least_loaded_irq = irq_pool_find_least_loaded(pool, affinity);
+ 	if (least_loaded_irq &&
+ 	    least_loaded_irq->refcount < pool->min_threshold)
+ 		goto out;
+ 	new_irq = irq_pool_create_irq(pool, affinity);
+ 	if (IS_ERR(new_irq)) {
+ 		if (!least_loaded_irq) {
+ 			mlx5_core_err(pool->dev, "Didn't find a matching IRQ. err = %ld\n",
+ 				      PTR_ERR(new_irq));
+ 			mutex_unlock(&pool->lock);
+ 			return new_irq;
+ 		}
+ 		/* We failed to create a new IRQ for the requested affinity,
+ 		 * sharing existing IRQ.
+ 		 */
+ 		goto out;
+ 	}
+ 	least_loaded_irq = new_irq;
+ 	goto unlock;
+ out:
+ 	irq_get_locked(least_loaded_irq);
+ 	if (least_loaded_irq->refcount > pool->max_threshold)
+ 		mlx5_core_dbg(pool->dev, "IRQ %u overloaded, pool_name: %s, %u EQs on this irq\n",
+ 			      least_loaded_irq->irqn, pool->name,
+ 			      least_loaded_irq->refcount / MLX5_EQ_REFS_PER_IRQ);
+ unlock:
+ 	mutex_unlock(&pool->lock);
+ 	return least_loaded_irq;
+ }
+ 
+ /* requesting an irq from a given pool according to given index */
+ static struct mlx5_irq *
+ irq_pool_request_vector(struct mlx5_irq_pool *pool, int vecidx,
+ 			struct cpumask *affinity)
+ {
+ 	struct mlx5_irq *irq;
+ 
+ 	mutex_lock(&pool->lock);
+ 	irq = xa_load(&pool->irqs, vecidx);
+ 	if (irq) {
+ 		irq_get_locked(irq);
+ 		goto unlock;
+ 	}
+ 	irq = irq_request(pool, vecidx);
+ 	if (IS_ERR(irq) || !affinity)
+ 		goto unlock;
+ 	cpumask_copy(irq->mask, affinity);
+ 	if (!irq_pool_is_sf_pool(pool) && !pool->xa_num_irqs.max &&
+ 	    cpumask_empty(irq->mask))
+ 		cpumask_set_cpu(cpumask_first(cpu_online_mask), irq->mask);
+ 	irq_set_affinity_hint(irq->irqn, irq->mask);
+ unlock:
+ 	mutex_unlock(&pool->lock);
+ 	return irq;
+ }
+ 
+ static struct mlx5_irq_pool *find_sf_irq_pool(struct mlx5_irq_table *irq_table,
+ 					      int i, struct cpumask *affinity)
+ {
+ 	if (cpumask_empty(affinity) && i == MLX5_IRQ_EQ_CTRL)
+ 		return irq_table->sf_ctrl_pool;
+ 	return irq_table->sf_comp_pool;
+ }
+ 
++>>>>>>> aa968f922039 (net/mlx5: Fix error print in case of IRQ request failed)
  /**
   * mlx5_irq_release - release an IRQ back to the system.
   * @irq: irq to be released.
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
