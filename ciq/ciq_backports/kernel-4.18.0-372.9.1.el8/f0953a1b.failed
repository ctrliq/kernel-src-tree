mm: fix typos in comments

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Ingo Molnar <mingo@kernel.org>
commit f0953a1bbaca71e1ebbcb9864eb1b273156157ed
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/f0953a1b.failed

Fix ~94 single-word typos in locking code comments, plus a few
very obvious grammar mistakes.

Link: https://lkml.kernel.org/r/20210322212624.GA1963421@gmail.com
Link: https://lore.kernel.org/r/20210322205203.GB1959563@gmail.com
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
	Reviewed-by: Randy Dunlap <rdunlap@infradead.org>
	Cc: Bhaskar Chowdhury <unixbhaskar@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f0953a1bbaca71e1ebbcb9864eb1b273156157ed)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/balloon_compaction.c
#	mm/filemap.c
#	mm/gup.c
#	mm/kfence/report.c
#	mm/madvise.c
#	mm/memcontrol.c
#	mm/memory.c
#	mm/mremap.c
#	mm/pgalloc-track.h
diff --cc mm/balloon_compaction.c
index ef858d547e2d,907fefde2572..000000000000
--- a/mm/balloon_compaction.c
+++ b/mm/balloon_compaction.c
@@@ -10,14 -11,115 +10,117 @@@
  #include <linux/export.h>
  #include <linux/balloon_compaction.h>
  
++<<<<<<< HEAD
++=======
+ static void balloon_page_enqueue_one(struct balloon_dev_info *b_dev_info,
+ 				     struct page *page)
+ {
+ 	/*
+ 	 * Block others from accessing the 'page' when we get around to
+ 	 * establishing additional references. We should be the only one
+ 	 * holding a reference to the 'page' at this point. If we are not, then
+ 	 * memory corruption is possible and we should stop execution.
+ 	 */
+ 	BUG_ON(!trylock_page(page));
+ 	balloon_page_insert(b_dev_info, page);
+ 	unlock_page(page);
+ 	__count_vm_event(BALLOON_INFLATE);
+ }
+ 
+ /**
+  * balloon_page_list_enqueue() - inserts a list of pages into the balloon page
+  *				 list.
+  * @b_dev_info: balloon device descriptor where we will insert a new page to
+  * @pages: pages to enqueue - allocated using balloon_page_alloc.
+  *
+  * Driver must call this function to properly enqueue balloon pages before
+  * definitively removing them from the guest system.
+  *
+  * Return: number of pages that were enqueued.
+  */
+ size_t balloon_page_list_enqueue(struct balloon_dev_info *b_dev_info,
+ 				 struct list_head *pages)
+ {
+ 	struct page *page, *tmp;
+ 	unsigned long flags;
+ 	size_t n_pages = 0;
+ 
+ 	spin_lock_irqsave(&b_dev_info->pages_lock, flags);
+ 	list_for_each_entry_safe(page, tmp, pages, lru) {
+ 		list_del(&page->lru);
+ 		balloon_page_enqueue_one(b_dev_info, page);
+ 		n_pages++;
+ 	}
+ 	spin_unlock_irqrestore(&b_dev_info->pages_lock, flags);
+ 	return n_pages;
+ }
+ EXPORT_SYMBOL_GPL(balloon_page_list_enqueue);
+ 
+ /**
+  * balloon_page_list_dequeue() - removes pages from balloon's page list and
+  *				 returns a list of the pages.
+  * @b_dev_info: balloon device descriptor where we will grab a page from.
+  * @pages: pointer to the list of pages that would be returned to the caller.
+  * @n_req_pages: number of requested pages.
+  *
+  * Driver must call this function to properly de-allocate a previous enlisted
+  * balloon pages before definitively releasing it back to the guest system.
+  * This function tries to remove @n_req_pages from the ballooned pages and
+  * return them to the caller in the @pages list.
+  *
+  * Note that this function may fail to dequeue some pages even if the balloon
+  * isn't empty - since the page list can be temporarily empty due to compaction
+  * of isolated pages.
+  *
+  * Return: number of pages that were added to the @pages list.
+  */
+ size_t balloon_page_list_dequeue(struct balloon_dev_info *b_dev_info,
+ 				 struct list_head *pages, size_t n_req_pages)
+ {
+ 	struct page *page, *tmp;
+ 	unsigned long flags;
+ 	size_t n_pages = 0;
+ 
+ 	spin_lock_irqsave(&b_dev_info->pages_lock, flags);
+ 	list_for_each_entry_safe(page, tmp, &b_dev_info->pages, lru) {
+ 		if (n_pages == n_req_pages)
+ 			break;
+ 
+ 		/*
+ 		 * Block others from accessing the 'page' while we get around to
+ 		 * establishing additional references and preparing the 'page'
+ 		 * to be released by the balloon driver.
+ 		 */
+ 		if (!trylock_page(page))
+ 			continue;
+ 
+ 		if (IS_ENABLED(CONFIG_BALLOON_COMPACTION) &&
+ 		    PageIsolated(page)) {
+ 			/* raced with isolation */
+ 			unlock_page(page);
+ 			continue;
+ 		}
+ 		balloon_page_delete(page);
+ 		__count_vm_event(BALLOON_DEFLATE);
+ 		list_add(&page->lru, pages);
+ 		unlock_page(page);
+ 		n_pages++;
+ 	}
+ 	spin_unlock_irqrestore(&b_dev_info->pages_lock, flags);
+ 
+ 	return n_pages;
+ }
+ EXPORT_SYMBOL_GPL(balloon_page_list_dequeue);
+ 
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  /*
   * balloon_page_alloc - allocates a new page for insertion into the balloon
 - *			page list.
 + *			  page list.
   *
 - * Driver must call this function to properly allocate a new balloon page.
 - * Driver must call balloon_page_enqueue before definitively removing the page
 - * from the guest system.
 - *
 - * Return: struct page for the allocated page or NULL on allocation failure.
 + * Driver must call it to properly allocate a new enlisted balloon page.
 + * Driver must call balloon_page_enqueue before definitively removing it from
 + * the guest system.  This function returns the page address for the recently
 + * allocated page or NULL in the case we fail to allocate a new page this turn.
   */
  struct page *balloon_page_alloc(void)
  {
@@@ -59,14 -156,23 +162,19 @@@ EXPORT_SYMBOL_GPL(balloon_page_enqueue)
  
  /*
   * balloon_page_dequeue - removes a page from balloon's page list and returns
++<<<<<<< HEAD
 + *			  the its address to allow the driver release the page.
 + * @b_dev_info: balloon device decriptor where we will grab a page from.
++=======
+  *			  its address to allow the driver to release the page.
+  * @b_dev_info: balloon device descriptor where we will grab a page from.
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
   *
 - * Driver must call this function to properly dequeue a previously enqueued page
 - * before definitively releasing it back to the guest system.
 - *
 - * Caller must perform its own accounting to ensure that this
 - * function is called only if some pages are actually enqueued.
 - *
 - * Note that this function may fail to dequeue some pages even if there are
 - * some enqueued pages - since the page list can be temporarily empty due to
 - * the compaction of isolated pages.
 - *
 - * TODO: remove the caller accounting requirements, and allow caller to wait
 - * until all pages can be dequeued.
 - *
 - * Return: struct page for the dequeued page, or NULL if no page was dequeued.
 + * Driver must call it to properly de-allocate a previous enlisted balloon page
 + * before definetively releasing it back to the guest system.
 + * This function returns the page address for the recently dequeued page or
 + * NULL in the case we find balloon's page list temporarily empty due to
 + * compaction isolated pages.
   */
  struct page *balloon_page_dequeue(struct balloon_dev_info *b_dev_info)
  {
diff --cc mm/filemap.c
index 37e2ef20fa83,66f7e9fdfbc4..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -2436,6 -2702,110 +2436,113 @@@ out
  }
  EXPORT_SYMBOL(generic_file_read_iter);
  
++<<<<<<< HEAD
++=======
+ static inline loff_t page_seek_hole_data(struct xa_state *xas,
+ 		struct address_space *mapping, struct page *page,
+ 		loff_t start, loff_t end, bool seek_data)
+ {
+ 	const struct address_space_operations *ops = mapping->a_ops;
+ 	size_t offset, bsz = i_blocksize(mapping->host);
+ 
+ 	if (xa_is_value(page) || PageUptodate(page))
+ 		return seek_data ? start : end;
+ 	if (!ops->is_partially_uptodate)
+ 		return seek_data ? end : start;
+ 
+ 	xas_pause(xas);
+ 	rcu_read_unlock();
+ 	lock_page(page);
+ 	if (unlikely(page->mapping != mapping))
+ 		goto unlock;
+ 
+ 	offset = offset_in_thp(page, start) & ~(bsz - 1);
+ 
+ 	do {
+ 		if (ops->is_partially_uptodate(page, offset, bsz) == seek_data)
+ 			break;
+ 		start = (start + bsz) & ~(bsz - 1);
+ 		offset += bsz;
+ 	} while (offset < thp_size(page));
+ unlock:
+ 	unlock_page(page);
+ 	rcu_read_lock();
+ 	return start;
+ }
+ 
+ static inline
+ unsigned int seek_page_size(struct xa_state *xas, struct page *page)
+ {
+ 	if (xa_is_value(page))
+ 		return PAGE_SIZE << xa_get_order(xas->xa, xas->xa_index);
+ 	return thp_size(page);
+ }
+ 
+ /**
+  * mapping_seek_hole_data - Seek for SEEK_DATA / SEEK_HOLE in the page cache.
+  * @mapping: Address space to search.
+  * @start: First byte to consider.
+  * @end: Limit of search (exclusive).
+  * @whence: Either SEEK_HOLE or SEEK_DATA.
+  *
+  * If the page cache knows which blocks contain holes and which blocks
+  * contain data, your filesystem can use this function to implement
+  * SEEK_HOLE and SEEK_DATA.  This is useful for filesystems which are
+  * entirely memory-based such as tmpfs, and filesystems which support
+  * unwritten extents.
+  *
+  * Return: The requested offset on success, or -ENXIO if @whence specifies
+  * SEEK_DATA and there is no data after @start.  There is an implicit hole
+  * after @end - 1, so SEEK_HOLE returns @end if all the bytes between @start
+  * and @end contain data.
+  */
+ loff_t mapping_seek_hole_data(struct address_space *mapping, loff_t start,
+ 		loff_t end, int whence)
+ {
+ 	XA_STATE(xas, &mapping->i_pages, start >> PAGE_SHIFT);
+ 	pgoff_t max = (end - 1) >> PAGE_SHIFT;
+ 	bool seek_data = (whence == SEEK_DATA);
+ 	struct page *page;
+ 
+ 	if (end <= start)
+ 		return -ENXIO;
+ 
+ 	rcu_read_lock();
+ 	while ((page = find_get_entry(&xas, max, XA_PRESENT))) {
+ 		loff_t pos = (u64)xas.xa_index << PAGE_SHIFT;
+ 		unsigned int seek_size;
+ 
+ 		if (start < pos) {
+ 			if (!seek_data)
+ 				goto unlock;
+ 			start = pos;
+ 		}
+ 
+ 		seek_size = seek_page_size(&xas, page);
+ 		pos = round_up(pos + 1, seek_size);
+ 		start = page_seek_hole_data(&xas, mapping, page, start, pos,
+ 				seek_data);
+ 		if (start < pos)
+ 			goto unlock;
+ 		if (start >= end)
+ 			break;
+ 		if (seek_size > PAGE_SIZE)
+ 			xas_set(&xas, pos >> PAGE_SHIFT);
+ 		if (!xa_is_value(page))
+ 			put_page(page);
+ 	}
+ 	if (seek_data)
+ 		start = -ENXIO;
+ unlock:
+ 	rcu_read_unlock();
+ 	if (page && !xa_is_value(page))
+ 		put_page(page);
+ 	if (start > end)
+ 		return end;
+ 	return start;
+ }
+ 
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  #ifdef CONFIG_MMU
  #define MMAP_LOTSAMISS  (100)
  /*
diff --cc mm/gup.c
index 13c5ba8d2fa6,0697134b6a12..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -1649,40 -1565,60 +1649,59 @@@ finish_or_fault
  }
  #endif /* !CONFIG_MMU */
  
++<<<<<<< HEAD
 +#if defined(CONFIG_FS_DAX) || defined (CONFIG_CMA)
 +static bool check_dax_vmas(struct vm_area_struct **vmas, long nr_pages)
++=======
+ /**
+  * get_dump_page() - pin user page in memory while writing it to core dump
+  * @addr: user address
+  *
+  * Returns struct page pointer of user page pinned for dump,
+  * to be freed afterwards by put_page().
+  *
+  * Returns NULL on any kind of failure - a hole must then be inserted into
+  * the corefile, to preserve alignment with its headers; and also returns
+  * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -
+  * allowing a hole to be left in the corefile to save disk space.
+  *
+  * Called without mmap_lock (takes and releases the mmap_lock by itself).
+  */
+ #ifdef CONFIG_ELF_CORE
+ struct page *get_dump_page(unsigned long addr)
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  {
 -	struct mm_struct *mm = current->mm;
 -	struct page *page;
 -	int locked = 1;
 -	int ret;
 +	long i;
 +	struct vm_area_struct *vma_prev = NULL;
  
 -	if (mmap_read_lock_killable(mm))
 -		return NULL;
 -	ret = __get_user_pages_locked(mm, addr, 1, &page, NULL, &locked,
 -				      FOLL_FORCE | FOLL_DUMP | FOLL_GET);
 -	if (locked)
 -		mmap_read_unlock(mm);
 +	for (i = 0; i < nr_pages; i++) {
 +		struct vm_area_struct *vma = vmas[i];
  
 -	if (ret == 1 && is_page_poisoned(page))
 -		return NULL;
 +		if (vma == vma_prev)
 +			continue;
 +
 +		vma_prev = vma;
  
 -	return (ret == 1) ? page : NULL;
 +		if (vma_is_fsdax(vma))
 +			return true;
 +	}
 +	return false;
  }
 -#endif /* CONFIG_ELF_CORE */
  
 -#ifdef CONFIG_MIGRATION
 -/*
 - * Check whether all pages are pinnable, if so return number of pages.  If some
 - * pages are not pinnable, migrate them, and unpin all pages. Return zero if
 - * pages were migrated, or if some pages were not successfully isolated.
 - * Return negative error if migration fails.
 - */
 -static long check_and_migrate_movable_pages(unsigned long nr_pages,
 -					    struct page **pages,
 -					    unsigned int gup_flags)
 -{
 -	unsigned long i;
 -	unsigned long isolation_error_count = 0;
 -	bool drain_allow = true;
 -	LIST_HEAD(movable_page_list);
 -	long ret = 0;
 -	struct page *prev_head = NULL;
 -	struct page *head;
 +#ifdef CONFIG_CMA
 +static long check_and_migrate_cma_pages(struct task_struct *tsk,
 +					struct mm_struct *mm,
 +					unsigned long start,
 +					unsigned long nr_pages,
 +					struct page **pages,
 +					struct vm_area_struct **vmas,
 +					unsigned int gup_flags)
 +{
 +	unsigned long i, isolation_error_count;
 +	bool drain_allow;
 +	LIST_HEAD(cma_page_list);
 +	long ret = nr_pages;
 +	struct page *prev_head, *head;
  	struct migration_target_control mtc = {
  		.nid = NUMA_NO_NODE,
  		.gfp_mask = GFP_USER | __GFP_NOWARN,
diff --cc mm/madvise.c
index eb9bc6466890,63e489e5bfdb..000000000000
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@@ -1041,6 -1037,11 +1041,14 @@@ madvise_behavior_valid(int behavior
   *  MADV_DONTDUMP - the application wants to prevent pages in the given range
   *		from being included in its core dump.
   *  MADV_DODUMP - cancel MADV_DONTDUMP: no longer exclude from core dump.
++<<<<<<< HEAD
++=======
+  *  MADV_COLD - the application is not expected to use this memory soon,
+  *		deactivate pages in this range so that they can be reclaimed
+  *		easily if memory pressure happens.
+  *  MADV_PAGEOUT - the application is not expected to use this memory soon,
+  *		page out the pages in this range immediately.
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
   *
   * return values:
   *  zero    - success
diff --cc mm/memcontrol.c
index 26bfe691ce9d,64ada9e650a5..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -925,23 -901,20 +925,32 @@@ struct mem_cgroup *get_mem_cgroup_from_
  	if (mem_cgroup_disabled())
  		return NULL;
  
 +	/*
 +	 * Page cache insertions can happen without an
 +	 * actual mm context, e.g. during disk probing
 +	 * on boot, loopback IO, acct() writes etc.
 +	 *
 +	 * No need to css_get on root memcg as the reference
 +	 * counting is disabled on the root level in the
 +	 * cgroup core. See CSS_NO_REF.
 +	 */
 +	if (unlikely(!mm))
 +		return root_mem_cgroup;
 +
  	rcu_read_lock();
  	do {
++<<<<<<< HEAD
 +		memcg = mem_cgroup_from_task(rcu_dereference(mm->owner));
 +		if (unlikely(!memcg))
++=======
+ 		/*
+ 		 * Page cache insertions can happen without an
+ 		 * actual mm context, e.g. during disk probing
+ 		 * on boot, loopback IO, acct() writes etc.
+ 		 */
+ 		if (unlikely(!mm))
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  			memcg = root_mem_cgroup;
 -		else {
 -			memcg = mem_cgroup_from_task(rcu_dereference(mm->owner));
 -			if (unlikely(!memcg))
 -				memcg = root_mem_cgroup;
 -		}
  	} while (!css_tryget(&memcg->css));
  	rcu_read_unlock();
  	return memcg;
@@@ -1734,8 -1712,8 +1743,13 @@@ static void mem_cgroup_unmark_under_oom
  	struct mem_cgroup *iter;
  
  	/*
++<<<<<<< HEAD
 +	 * When a new child is created while the hierarchy is under oom,
 +	 * mem_cgroup_oom_lock() may not be called. Watch for underflow.
++=======
+ 	 * Be careful about under_oom underflows because a child memcg
+ 	 * could have been added after mem_cgroup_mark_under_oom.
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  	 */
  	spin_lock(&memcg_oom_lock);
  	for_each_mem_cgroup_tree(iter, memcg)
diff --cc mm/memory.c
index ea354c711d3e,730daa00952b..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -4269,6 -4502,67 +4269,70 @@@ retry_pud
  	return handle_pte_fault(&vmf);
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * mm_account_fault - Do page fault accounting
+  *
+  * @regs: the pt_regs struct pointer.  When set to NULL, will skip accounting
+  *        of perf event counters, but we'll still do the per-task accounting to
+  *        the task who triggered this page fault.
+  * @address: the faulted address.
+  * @flags: the fault flags.
+  * @ret: the fault retcode.
+  *
+  * This will take care of most of the page fault accounting.  Meanwhile, it
+  * will also include the PERF_COUNT_SW_PAGE_FAULTS_[MAJ|MIN] perf counter
+  * updates.  However, note that the handling of PERF_COUNT_SW_PAGE_FAULTS should
+  * still be in per-arch page fault handlers at the entry of page fault.
+  */
+ static inline void mm_account_fault(struct pt_regs *regs,
+ 				    unsigned long address, unsigned int flags,
+ 				    vm_fault_t ret)
+ {
+ 	bool major;
+ 
+ 	/*
+ 	 * We don't do accounting for some specific faults:
+ 	 *
+ 	 * - Unsuccessful faults (e.g. when the address wasn't valid).  That
+ 	 *   includes arch_vma_access_permitted() failing before reaching here.
+ 	 *   So this is not a "this many hardware page faults" counter.  We
+ 	 *   should use the hw profiling for that.
+ 	 *
+ 	 * - Incomplete faults (VM_FAULT_RETRY).  They will only be counted
+ 	 *   once they're completed.
+ 	 */
+ 	if (ret & (VM_FAULT_ERROR | VM_FAULT_RETRY))
+ 		return;
+ 
+ 	/*
+ 	 * We define the fault as a major fault when the final successful fault
+ 	 * is VM_FAULT_MAJOR, or if it retried (which implies that we couldn't
+ 	 * handle it immediately previously).
+ 	 */
+ 	major = (ret & VM_FAULT_MAJOR) || (flags & FAULT_FLAG_TRIED);
+ 
+ 	if (major)
+ 		current->maj_flt++;
+ 	else
+ 		current->min_flt++;
+ 
+ 	/*
+ 	 * If the fault is done for GUP, regs will be NULL.  We only do the
+ 	 * accounting for the per thread fault counters who triggered the
+ 	 * fault, and we skip the perf event updates.
+ 	 */
+ 	if (!regs)
+ 		return;
+ 
+ 	if (major)
+ 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);
+ 	else
+ 		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);
+ }
+ 
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  /*
   * By the time we get here, we already hold the mm semaphore
   *
@@@ -4560,6 -4845,18 +4624,21 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ /**
+  * generic_access_phys - generic implementation for iomem mmap access
+  * @vma: the vma to access
+  * @addr: userspace address, not relative offset within @vma
+  * @buf: buffer to read/write
+  * @len: length of transfer
+  * @write: set to FOLL_WRITE when writing, otherwise reading
+  *
+  * This is a generic implementation for &vm_operations_struct.access for an
+  * iomem mapping. This callback is used by access_process_vm() when the @vma is
+  * not page based.
+  */
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
  			void *buf, int len, int write)
  {
diff --cc mm/mremap.c
index 3c8a797d5693,47c255b60150..000000000000
--- a/mm/mremap.c
+++ b/mm/mremap.c
@@@ -537,9 -721,28 +537,34 @@@ static unsigned long mremap_to(unsigne
  	if (addr + old_len > new_addr && new_addr + new_len > addr)
  		goto out;
  
++<<<<<<< HEAD
 +	ret = do_munmap(mm, new_addr, new_len, uf_unmap_early);
 +	if (ret)
 +		goto out;
++=======
+ 	/*
+ 	 * move_vma() need us to stay 4 maps below the threshold, otherwise
+ 	 * it will bail out at the very beginning.
+ 	 * That is a problem if we have already unmaped the regions here
+ 	 * (new_addr, and old_addr), because userspace will not know the
+ 	 * state of the vma's after it gets -ENOMEM.
+ 	 * So, to avoid such scenario we can pre-compute if the whole
+ 	 * operation has high chances to success map-wise.
+ 	 * Worst-scenario case is when both vma's (new_addr and old_addr) get
+ 	 * split in 3 before unmapping it.
+ 	 * That means 2 more maps (1 for each) to the ones we already hold.
+ 	 * Check whether current map count plus 2 still leads us to 4 maps below
+ 	 * the threshold, otherwise return -ENOMEM here to be more safe.
+ 	 */
+ 	if ((mm->map_count + 2) >= sysctl_max_map_count - 3)
+ 		return -ENOMEM;
+ 
+ 	if (flags & MREMAP_FIXED) {
+ 		ret = do_munmap(mm, new_addr, new_len, uf_unmap_early);
+ 		if (ret)
+ 			goto out;
+ 	}
++>>>>>>> f0953a1bbaca (mm: fix typos in comments)
  
  	if (old_len >= new_len) {
  		ret = do_munmap(mm, addr+new_len, old_len - new_len, uf_unmap);
* Unmerged path mm/kfence/report.c
* Unmerged path mm/pgalloc-track.h
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 230c8d984885..7d3f29498362 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -104,7 +104,7 @@ extern int mmap_rnd_compat_bits __read_mostly;
  * embedding these tags into addresses that point to these memory regions, and
  * checking that the memory and the pointer tags match on memory accesses)
  * redefine this macro to strip tags from pointers.
- * It's defined as noop for arcitectures that don't support memory tagging.
+ * It's defined as noop for architectures that don't support memory tagging.
  */
 #ifndef untagged_addr
 #define untagged_addr(addr) (addr)
diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index f3340d38d002..395f1f7b2d6e 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -30,7 +30,7 @@ struct notifier_block;		/* in notifier.h */
  *
  * If IS_ENABLED(CONFIG_KASAN_VMALLOC), VM_KASAN is set on a vm_struct after
  * shadow memory has been mapped. It's used to handle allocation errors so that
- * we don't try to poision shadow on free if it was never allocated.
+ * we don't try to poison shadow on free if it was never allocated.
  *
  * Otherwise, VM_KASAN is set for kasan_module_alloc() allocations and used to
  * determine which allocations need the module shadow freed.
@@ -40,7 +40,7 @@ struct notifier_block;		/* in notifier.h */
 
 /*
  * Maximum alignment for ioremap() regions.
- * Can be overriden by arch-specific value.
+ * Can be overridden by arch-specific value.
  */
 #ifndef IOREMAP_MAX_ORDER
 #define IOREMAP_MAX_ORDER	(7 + PAGE_SHIFT)	/* 128 pages */
* Unmerged path mm/balloon_compaction.c
diff --git a/mm/compaction.c b/mm/compaction.c
index c5f9cdb35464..5d09d64a1558 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -1971,8 +1971,8 @@ static unsigned int fragmentation_score_wmark(pg_data_t *pgdat, bool low)
 	unsigned int wmark_low;
 
 	/*
-	 * Cap the low watermak to avoid excessive compaction
-	 * activity in case a user sets the proactivess tunable
+	 * Cap the low watermark to avoid excessive compaction
+	 * activity in case a user sets the proactiveness tunable
 	 * close to 100 (maximum).
 	 */
 	wmark_low = max(100U - sysctl_compaction_proactiveness, 5U);
* Unmerged path mm/filemap.c
* Unmerged path mm/gup.c
diff --git a/mm/highmem.c b/mm/highmem.c
index a0fd8e26047b..8685d6f666c8 100644
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@ -455,7 +455,7 @@ void *__kmap_local_pfn_prot(unsigned long pfn, pgprot_t prot)
 
 	/*
 	 * Disable migration so resulting virtual address is stable
-	 * accross preemption.
+	 * across preemption.
 	 */
 	migrate_disable();
 	preempt_disable();
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 512c64ed80b5..4adc209feb91 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1804,8 +1804,8 @@ bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,
 /*
  * Returns
  *  - 0 if PMD could not be locked
- *  - 1 if PMD was locked but protections unchange and TLB flush unnecessary
- *  - HPAGE_PMD_NR is protections changed and TLB flush necessary
+ *  - 1 if PMD was locked but protections unchanged and TLB flush unnecessary
+ *  - HPAGE_PMD_NR if protections changed and TLB flush necessary
  */
 int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 		unsigned long addr, pgprot_t newprot, unsigned long cp_flags)
@@ -2500,7 +2500,7 @@ static void __split_huge_page(struct page *page, struct list_head *list,
 		xa_lock(&swap_cache->i_pages);
 	}
 
-	/* lock lru list/PageCompound, ref freezed by page_ref_freeze */
+	/* lock lru list/PageCompound, ref frozen by page_ref_freeze */
 	lruvec = lock_page_lruvec(head);
 
 	for (i = nr - 1; i >= 1; i--) {
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 44b3e484b504..01ca2bdcfd8f 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -442,7 +442,7 @@ static int allocate_file_region_entries(struct resv_map *resv,
 			      resv->region_cache_count;
 
 		/* At this point, we should have enough entries in the cache
-		 * for all the existings adds_in_progress. We should only be
+		 * for all the existing adds_in_progress. We should only be
 		 * needing to allocate for regions_needed.
 		 */
 		VM_BUG_ON(resv->region_cache_count < resv->adds_in_progress);
@@ -5391,8 +5391,8 @@ void adjust_range_if_pmd_sharing_possible(struct vm_area_struct *vma,
 		v_end = ALIGN_DOWN(vma->vm_end, PUD_SIZE);
 
 	/*
-	 * vma need span at least one aligned PUD size and the start,end range
-	 * must at least partialy within it.
+	 * vma needs to span at least one aligned PUD size, and the range
+	 * must be at least partially within in.
 	 */
 	if (!(vma->vm_flags & VM_MAYSHARE) || !(v_end > v_start) ||
 		(*end <= v_start) || (*start >= v_end))
diff --git a/mm/internal.h b/mm/internal.h
index 6a87ec14c712..2ac406c6f27a 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -313,7 +313,7 @@ static inline bool is_exec_mapping(vm_flags_t flags)
 }
 
 /*
- * Stack area - atomatically grows in one direction
+ * Stack area - automatically grows in one direction
  *
  * VM_GROWSUP / VM_GROWSDOWN VMAs are always private anonymous:
  * do_mmap() forbids all other combinations.
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 03f6affad90a..d9620097fc1f 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -37,9 +37,9 @@ extern bool kasan_flag_panic __ro_after_init;
 #define KASAN_TAG_MAX		0xFD /* maximum value for random tags */
 
 #ifdef CONFIG_KASAN_HW_TAGS
-#define KASAN_TAG_MIN		0xF0 /* mimimum value for random tags */
+#define KASAN_TAG_MIN		0xF0 /* minimum value for random tags */
 #else
-#define KASAN_TAG_MIN		0x00 /* mimimum value for random tags */
+#define KASAN_TAG_MIN		0x00 /* minimum value for random tags */
 #endif
 
 #ifdef CONFIG_KASAN_GENERIC
@@ -345,7 +345,7 @@ static inline bool kasan_byte_accessible(const void *addr)
 #else /* CONFIG_KASAN_HW_TAGS */
 
 /**
- * kasan_poison - mark the memory range as unaccessible
+ * kasan_poison - mark the memory range as inaccessible
  * @addr - range start address, must be aligned to KASAN_GRANULE_SIZE
  * @size - range size, must be aligned to KASAN_GRANULE_SIZE
  * @value - value that's written to metadata for the range
@@ -374,7 +374,7 @@ bool kasan_byte_accessible(const void *addr);
 
 /**
  * kasan_poison_last_granule - mark the last granule of the memory range as
- * unaccessible
+ * inaccessible
  * @addr - range start address, must be aligned to KASAN_GRANULE_SIZE
  * @size - range size
  *
diff --git a/mm/kasan/quarantine.c b/mm/kasan/quarantine.c
index 728fb24c5683..d8ccff4c1275 100644
--- a/mm/kasan/quarantine.c
+++ b/mm/kasan/quarantine.c
@@ -27,7 +27,7 @@
 /* Data structure and operations for quarantine queues. */
 
 /*
- * Each queue is a signle-linked list, which also stores the total size of
+ * Each queue is a single-linked list, which also stores the total size of
  * objects inside of it.
  */
 struct qlist_head {
@@ -138,7 +138,7 @@ static void qlink_free(struct qlist_node *qlink, struct kmem_cache *cache)
 		local_irq_save(flags);
 
 	/*
-	 * As the object now gets freed from the quaratine, assume that its
+	 * As the object now gets freed from the quarantine, assume that its
 	 * free track is no longer valid.
 	 */
 	*(u8 *)kasan_mem_to_shadow(object) = KASAN_KMALLOC_FREE;
diff --git a/mm/kasan/shadow.c b/mm/kasan/shadow.c
index a457dfbcc47c..a0a6c4669344 100644
--- a/mm/kasan/shadow.c
+++ b/mm/kasan/shadow.c
@@ -303,7 +303,7 @@ int kasan_populate_vmalloc(unsigned long addr, unsigned long size)
 	 * // rest of vmalloc process		<data dependency>
 	 * STORE p, a				LOAD shadow(x+99)
 	 *
-	 * If there is no barrier between the end of unpoisioning the shadow
+	 * If there is no barrier between the end of unpoisoning the shadow
 	 * and the store of the result to p, the stores could be committed
 	 * in a different order by CPU#0, and CPU#1 could erroneously observe
 	 * poison in the shadow.
@@ -371,7 +371,7 @@ static int kasan_depopulate_vmalloc_pte(pte_t *ptep, unsigned long addr,
  * How does this work?
  * -------------------
  *
- * We have a region that is page aligned, labelled as A.
+ * We have a region that is page aligned, labeled as A.
  * That might not map onto the shadow in a way that is page-aligned:
  *
  *                    start                     end
* Unmerged path mm/kfence/report.c
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index 2f4c07eb32d9..30a2b4effa96 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -626,7 +626,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 		 *
 		 * The page table that maps the page has been already unlinked
 		 * from the page table tree and this process cannot get
-		 * an additinal pin on the page.
+		 * an additional pin on the page.
 		 *
 		 * New pins can come later if the page is shared across fork,
 		 * but not from this process. The other process cannot write to
diff --git a/mm/ksm.c b/mm/ksm.c
index 256526925e41..289cca963b36 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -1052,7 +1052,7 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,
 		/*
 		 * Ok this is tricky, when get_user_pages_fast() run it doesn't
 		 * take any lock, therefore the check that we are going to make
-		 * with the pagecount against the mapcount is racey and
+		 * with the pagecount against the mapcount is racy and
 		 * O_DIRECT can happen right after the check.
 		 * So we clear the pte and flush the tlb before the check
 		 * this assure us that no O_DIRECT can happen after the check
@@ -1422,7 +1422,7 @@ static struct page *stable_node_dup(struct stable_node **_stable_node_dup,
 			 */
 			*_stable_node = found;
 			/*
-			 * Just for robustneess as stable_node is
+			 * Just for robustness, as stable_node is
 			 * otherwise left as a stable pointer, the
 			 * compiler shall optimize it away at build
 			 * time.
* Unmerged path mm/madvise.c
* Unmerged path mm/memcontrol.c
diff --git a/mm/memory-failure.c b/mm/memory-failure.c
index 7fd26577ba6a..cae688ca79ad 100644
--- a/mm/memory-failure.c
+++ b/mm/memory-failure.c
@@ -80,7 +80,7 @@ static bool page_handle_poison(struct page *page, bool hugepage_or_freepage, boo
 		if (dissolve_free_huge_page(page) || !take_page_off_buddy(page))
 			/*
 			 * We could fail to take off the target page from buddy
-			 * for example due to racy page allocaiton, but that's
+			 * for example due to racy page allocation, but that's
 			 * acceptable because soft-offlined page is not broken
 			 * and if someone really want to use it, they should
 			 * take it.
* Unmerged path mm/memory.c
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index b09080ea18df..a44c944f6c05 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -1812,7 +1812,7 @@ static int apply_policy_zone(struct mempolicy *policy, enum zone_type zone)
 	 * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.
 	 *
 	 * policy->v.nodes is intersect with node_states[N_MEMORY].
-	 * so if the following test faile, it implies
+	 * so if the following test fails, it implies
 	 * policy->v.nodes has movable memory only.
 	 */
 	if (!nodes_intersects(policy->v.nodes, node_states[N_HIGH_MEMORY]))
@@ -2045,7 +2045,7 @@ bool init_nodemask_of_mempolicy(nodemask_t *mask)
  *
  * If tsk's mempolicy is "default" [NULL], return 'true' to indicate default
  * policy.  Otherwise, check for intersection between mask and the policy
- * nodemask for 'bind' or 'interleave' policy.  For 'perferred' or 'local'
+ * nodemask for 'bind' or 'interleave' policy.  For 'preferred' or 'local'
  * policy, always return true since it may allocate elsewhere on fallback.
  *
  * Takes task_lock(tsk) to prevent freeing of its mempolicy.
diff --git a/mm/migrate.c b/mm/migrate.c
index 0b645897027e..48ad4d4233fe 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -2718,11 +2718,11 @@ static void migrate_vma_unmap(struct migrate_vma *migrate)
  *
  * For empty entries inside CPU page table (pte_none() or pmd_none() is true) we
  * do set MIGRATE_PFN_MIGRATE flag inside the corresponding source array thus
- * allowing the caller to allocate device memory for those unback virtual
- * address.  For this the caller simply has to allocate device memory and
+ * allowing the caller to allocate device memory for those unbacked virtual
+ * addresses.  For this the caller simply has to allocate device memory and
  * properly set the destination entry like for regular migration.  Note that
- * this can still fails and thus inside the device driver must check if the
- * migration was successful for those entries after calling migrate_vma_pages()
+ * this can still fail, and thus inside the device driver you must check if the
+ * migration was successful for those entries after calling migrate_vma_pages(),
  * just like for regular migration.
  *
  * After that, the callers must call migrate_vma_pages() to go over each entry
diff --git a/mm/mmap.c b/mm/mmap.c
index 0737dbcb38fe..61d21fd2374b 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -562,7 +562,7 @@ static unsigned long count_vma_pages_range(struct mm_struct *mm,
 	unsigned long nr_pages = 0;
 	struct vm_area_struct *vma;
 
-	/* Find first overlaping mapping */
+	/* Find first overlapping mapping */
 	vma = find_vma_intersection(mm, addr, end);
 	if (!vma)
 		return 0;
@@ -2810,7 +2810,7 @@ int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 	if (unlikely(uf)) {
 		/*
 		 * If userfaultfd_unmap_prep returns an error the vmas
-		 * will remain splitted, but userland will get a
+		 * will remain split, but userland will get a
 		 * highly unexpected error anyway. This is no
 		 * different than the case where the first of the two
 		 * __split_vma fails, but we don't undo the first
diff --git a/mm/mprotect.c b/mm/mprotect.c
index e122e5d70e71..f9e4b328d73f 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -692,7 +692,7 @@ SYSCALL_DEFINE1(pkey_free, int, pkey)
 	mmap_write_unlock(current->mm);
 
 	/*
-	 * We could provie warnings or errors if any VMA still
+	 * We could provide warnings or errors if any VMA still
 	 * has the pkey set here.
 	 */
 	return ret;
* Unmerged path mm/mremap.c
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index 5e9a62446b75..a24a30287e6e 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -65,7 +65,7 @@ static inline bool is_memcg_oom(struct oom_control *oc)
 
 #ifdef CONFIG_NUMA
 /**
- * oom_cpuset_eligible() - check task eligiblity for kill
+ * oom_cpuset_eligible() - check task eligibility for kill
  * @start: task struct of which task to consider
  * @oc: pointer to struct oom_control
  *
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index a2ff3d049246..893086790d7b 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -1818,7 +1818,7 @@ static void balance_dirty_pages(struct bdi_writeback *wb,
 			break;
 
 		/*
-		 * In the case of an unresponding NFS server and the NFS dirty
+		 * In the case of an unresponsive NFS server and the NFS dirty
 		 * pages exceeds dirty_thresh, give the other good wb's a pipe
 		 * to go through, so that tasks on them still remain responsive.
 		 *
@@ -2232,7 +2232,7 @@ int write_cache_pages(struct address_space *mapping,
 			 * Page truncated or invalidated. We can freely skip it
 			 * then, even for data integrity operations: the page
 			 * has disappeared concurrently, so there could be no
-			 * real expectation of this data interity operation
+			 * real expectation of this data integrity operation
 			 * even if there is now a new, dirty page at the same
 			 * pagecache address.
 			 */
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index cf39c1a7978e..a55d1799b50d 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -875,7 +875,7 @@ compaction_capture(struct capture_control *capc, struct page *page,
 		return false;
 
 	/*
-	 * Do not let lower order allocations polluate a movable pageblock.
+	 * Do not let lower order allocations pollute a movable pageblock.
 	 * This might let an unmovable request use a reclaimable pageblock
 	 * and vice-versa but no more than normal fallback logic which can
 	 * have trouble finding a high-order free page.
@@ -2717,7 +2717,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 			/*
 			 * In page freeing path, migratetype change is racy so
 			 * we can counter several free pages in a pageblock
-			 * in this loop althoug we changed the pageblock type
+			 * in this loop although we changed the pageblock type
 			 * from highatomic to ac->migratetype. So we should
 			 * adjust the count once.
 			 */
@@ -3002,7 +3002,7 @@ static void drain_local_pages_wq(struct work_struct *work)
 	 * drain_all_pages doesn't use proper cpu hotplug protection so
 	 * we can race with cpu offline when the WQ can move this from
 	 * a cpu pinned worker to an unbound one. We can operate on a different
-	 * cpu which is allright but we also have to make sure to not move to
+	 * cpu which is alright but we also have to make sure to not move to
 	 * a different one.
 	 */
 	preempt_disable();
@@ -5650,7 +5650,7 @@ static int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)
 static int __parse_numa_zonelist_order(char *s)
 {
 	/*
-	 * We used to support different zonlists modes but they turned
+	 * We used to support different zonelists modes but they turned
 	 * out to be just not useful. Let's keep the warning in place
 	 * if somebody still use the cmd line parameter so that we do
 	 * not fail it silently
@@ -7446,7 +7446,7 @@ static void check_for_memory(pg_data_t *pgdat, int nid)
 }
 
 /*
- * Some architecturs, e.g. ARC may have ZONE_HIGHMEM below ZONE_NORMAL. For
+ * Some architectures, e.g. ARC may have ZONE_HIGHMEM below ZONE_NORMAL. For
  * such cases we allow max_zone_pfn sorted in the descending order
  */
 bool __weak arch_has_descending_max_zone_pfns(void)
@@ -8490,7 +8490,7 @@ static int __alloc_contig_migrate_range(struct compact_control *cc,
  * alloc_contig_range() -- tries to allocate given range of pages
  * @start:	start PFN to allocate
  * @end:	one-past-the-last PFN to allocate
- * @migratetype:	migratetype of the underlaying pageblocks (either
+ * @migratetype:	migratetype of the underlying pageblocks (either
  *			#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks
  *			in range must have the same migratetype and it must
  *			be either of the two.
@@ -8658,7 +8658,7 @@ void free_contig_range(unsigned long pfn, unsigned int nr_pages)
 #ifdef CONFIG_MEMORY_HOTPLUG
 /*
  * The zone indicated has a new number of managed_pages; batch sizes and percpu
- * page high values need to be recalulated.
+ * page high values need to be recalculated.
  */
 void __meminit zone_pcp_update(struct zone *zone)
 {
diff --git a/mm/page_owner.c b/mm/page_owner.c
index 8834fca99b64..b7777659be90 100644
--- a/mm/page_owner.c
+++ b/mm/page_owner.c
@@ -240,7 +240,7 @@ void __copy_page_owner(struct page *oldpage, struct page *newpage)
 	/*
 	 * We don't clear the bit on the oldpage as it's going to be freed
 	 * after migration. Until then, the info can be useful in case of
-	 * a bug, and the overal stats will be off a bit only temporarily.
+	 * a bug, and the overall stats will be off a bit only temporarily.
 	 * Also, migrate_misplaced_transhuge_page() can still fail the
 	 * migration and then we want the oldpage to retain the info. But
 	 * in that case we also don't need to explicitly clear the info from
diff --git a/mm/percpu-internal.h b/mm/percpu-internal.h
index b6dc22904088..639662c20c82 100644
--- a/mm/percpu-internal.h
+++ b/mm/percpu-internal.h
@@ -124,7 +124,7 @@ struct percpu_stats {
 	u64 nr_max_alloc;	/* max # of live allocations */
 	u32 nr_chunks;		/* current # of live chunks */
 	u32 nr_max_chunks;	/* max # of live chunks */
-	size_t min_alloc_size;	/* min allocaiton size */
+	size_t min_alloc_size;	/* min allocation size */
 	size_t max_alloc_size;	/* max allocation size */
 };
 
diff --git a/mm/percpu.c b/mm/percpu.c
index 6f174f5daedf..a95c439fe14c 100644
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@ -1909,7 +1909,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 			pr_info("limit reached, disable warning\n");
 	}
 	if (is_atomic) {
-		/* see the flag handling in pcpu_blance_workfn() */
+		/* see the flag handling in pcpu_balance_workfn() */
 		pcpu_atomic_alloc_failed = true;
 		pcpu_schedule_balance_work();
 	} else {
* Unmerged path mm/pgalloc-track.h
diff --git a/mm/slab.c b/mm/slab.c
index 6fc25c913ad7..880883837167 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -258,7 +258,7 @@ static void kmem_cache_node_init(struct kmem_cache_node *parent)
 
 #define BATCHREFILL_LIMIT	16
 /*
- * Optimization question: fewer reaps means less probability for unnessary
+ * Optimization question: fewer reaps means less probability for unnecessary
  * cpucache drain/refill cycles.
  *
  * OTOH the cpuarrays can contain lots of objects,
@@ -2405,8 +2405,8 @@ union freelist_init_state {
 };
 
 /*
- * Initialize the state based on the randomization methode available.
- * return true if the pre-computed list is available, false otherwize.
+ * Initialize the state based on the randomization method available.
+ * return true if the pre-computed list is available, false otherwise.
  */
 static bool freelist_state_initialize(union freelist_init_state *state,
 				struct kmem_cache *cachep,
diff --git a/mm/slub.c b/mm/slub.c
index aad4165cd235..855fae506efe 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -3653,7 +3653,7 @@ EXPORT_SYMBOL(kmem_cache_alloc_bulk);
  */
 
 /*
- * Mininum / Maximum order of slab pages. This influences locking overhead
+ * Minimum / Maximum order of slab pages. This influences locking overhead
  * and slab fragmentation. A higher order reduces the number of partial slabs
  * and increases the number of allocations possible without having to
  * take the list_lock.
diff --git a/mm/swap_slots.c b/mm/swap_slots.c
index a791411fed71..e7fe13e6f17e 100644
--- a/mm/swap_slots.c
+++ b/mm/swap_slots.c
@@ -16,7 +16,7 @@
  * to local caches without needing to acquire swap_info
  * lock.  We do not reuse the returned slots directly but
  * move them back to the global pool in a batch.  This
- * allows the slots to coaellesce and reduce fragmentation.
+ * allows the slots to coalesce and reduce fragmentation.
  *
  * The swap entry allocated is marked with SWAP_HAS_CACHE
  * flag in map_count that prevents it from being allocated
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index c075d9d96171..3567138a4a85 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -1251,7 +1251,7 @@ static unsigned long lazy_max_pages(void)
 static atomic_long_t vmap_lazy_nr = ATOMIC_LONG_INIT(0);
 
 /*
- * Serialize vmap purging.  There is no actual criticial section protected
+ * Serialize vmap purging.  There is no actual critical section protected
  * by this look, but we want to avoid concurrent calls for performance
  * reasons and to make the pcpu_get_vm_areas more deterministic.
  */
@@ -2285,7 +2285,7 @@ void vfree_atomic(const void *addr)
  * May sleep if called *not* from interrupt context.
  * Must not be called in NMI context (strictly speaking, it could be
  * if we have CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG, but making the calling
- * conventions for vfree() arch-depenedent would be a really bad idea).
+ * conventions for vfree() arch-dependent would be a really bad idea).
  */
 void vfree(const void *addr)
 {
@@ -2804,7 +2804,7 @@ static int aligned_vread(char *buf, char *addr, unsigned long count)
 		/*
 		 * To do safe access to this _mapped_ area, we need
 		 * lock. But adding lock here means that we need to add
-		 * overhead of vmalloc()/vfree() calles for this _debug_
+		 * overhead of vmalloc()/vfree() calls for this _debug_
 		 * interface, rarely used. Instead of that, we'll use
 		 * kmap() and get small overhead in this access function.
 		 */
diff --git a/mm/vmstat.c b/mm/vmstat.c
index e7be43fb1962..7fdbc3c18024 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -969,7 +969,7 @@ void cpu_vm_stats_fold(int cpu)
 
 /*
  * this is only called if !populated_zone(zone), which implies no other users of
- * pset->vm_stat_diff[] exsist.
+ * pset->vm_stat_diff[] exist.
  */
 void drain_zonestat(struct zone *zone, struct per_cpu_pageset *pset)
 {
diff --git a/mm/zpool.c b/mm/zpool.c
index 01a771e304fa..1b166e4e4ba1 100644
--- a/mm/zpool.c
+++ b/mm/zpool.c
@@ -317,7 +317,7 @@ int zpool_shrink(struct zpool *zpool, unsigned int pages,
  * This may hold locks, disable interrupts, and/or preemption,
  * and the zpool_unmap_handle() must be called to undo those
  * actions.  The code that uses the mapped handle should complete
- * its operatons on the mapped handle memory quickly and unmap
+ * its operations on the mapped handle memory quickly and unmap
  * as soon as possible.  As the implementation may use per-cpu
  * data, multiple handles should not be mapped concurrently on
  * any cpu.
diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
index f215c1e017eb..c68df7ebbe18 100644
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@ -1317,7 +1317,7 @@ EXPORT_SYMBOL_GPL(zs_get_total_pages);
  * zs_map_object - get address of allocated object from handle.
  * @pool: pool from which the object was allocated
  * @handle: handle returned from zs_malloc
- * @mm: maping mode to use
+ * @mm: mapping mode to use
  *
  * Before using an object allocated from zs_malloc, it must be mapped using
  * this function. When done with the object, it must be unmapped using
