ice: support for indirect notification

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
commit 195bb48fccdef4965a65579ef05db8a8fcba8dca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/195bb48f.failed

Implement indirect notification mechanism to support offloading TC rules
on tunnel devices.

Keep indirect block list in netdev priv. Notification will call setting
tc cls flower function. For now we can offload only ingress type. Return
not supported for other flow block binder.

	Signed-off-by: Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
	Acked-by: Paul Menzel <pmenzel@molgen.mpg.de>
	Tested-by: Sandeep Penigalapati <sandeep.penigalapati@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 195bb48fccdef4965a65579ef05db8a8fcba8dca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_main.c
#	drivers/net/ethernet/intel/ice/ice_tc_lib.h
diff --cc drivers/net/ethernet/intel/ice/ice_main.c
index 8b6a4047a03a,e6a8a07d30e5..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_main.c
+++ b/drivers/net/ethernet/intel/ice/ice_main.c
@@@ -59,6 -55,15 +59,18 @@@ static void ice_rebuild(struct ice_pf *
  
  static void ice_vsi_release_all(struct ice_pf *pf);
  
++<<<<<<< HEAD
++=======
+ static int ice_rebuild_channels(struct ice_pf *pf);
+ static void ice_remove_q_channels(struct ice_vsi *vsi, bool rem_adv_fltr);
+ 
+ static int
+ ice_indr_setup_tc_cb(struct net_device *netdev, struct Qdisc *sch,
+ 		     void *cb_priv, enum tc_setup_type type, void *type_data,
+ 		     void *data,
+ 		     void (*cleanup)(struct flow_block_cb *block_cb));
+ 
++>>>>>>> 195bb48fccde (ice: support for indirect notification)
  bool netif_is_ice(struct net_device *dev)
  {
  	return dev && (dev->netdev_ops == &ice_netdev_ops);
@@@ -7085,6 -7301,1045 +7165,1048 @@@ static void ice_tx_timeout(struct net_d
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * ice_setup_tc_cls_flower - flower classifier offloads
+  * @np: net device to configure
+  * @filter_dev: device on which filter is added
+  * @cls_flower: offload data
+  */
+ static int
+ ice_setup_tc_cls_flower(struct ice_netdev_priv *np,
+ 			struct net_device *filter_dev,
+ 			struct flow_cls_offload *cls_flower)
+ {
+ 	struct ice_vsi *vsi = np->vsi;
+ 
+ 	if (cls_flower->common.chain_index)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (cls_flower->command) {
+ 	case FLOW_CLS_REPLACE:
+ 		return ice_add_cls_flower(filter_dev, vsi, cls_flower);
+ 	case FLOW_CLS_DESTROY:
+ 		return ice_del_cls_flower(vsi, cls_flower);
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
+ /**
+  * ice_setup_tc_block_cb - callback handler registered for TC block
+  * @type: TC SETUP type
+  * @type_data: TC flower offload data that contains user input
+  * @cb_priv: netdev private data
+  */
+ static int
+ ice_setup_tc_block_cb(enum tc_setup_type type, void *type_data, void *cb_priv)
+ {
+ 	struct ice_netdev_priv *np = cb_priv;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_CLSFLOWER:
+ 		return ice_setup_tc_cls_flower(np, np->vsi->netdev,
+ 					       type_data);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ /**
+  * ice_validate_mqprio_qopt - Validate TCF input parameters
+  * @vsi: Pointer to VSI
+  * @mqprio_qopt: input parameters for mqprio queue configuration
+  *
+  * This function validates MQPRIO params, such as qcount (power of 2 wherever
+  * needed), and make sure user doesn't specify qcount and BW rate limit
+  * for TCs, which are more than "num_tc"
+  */
+ static int
+ ice_validate_mqprio_qopt(struct ice_vsi *vsi,
+ 			 struct tc_mqprio_qopt_offload *mqprio_qopt)
+ {
+ 	u64 sum_max_rate = 0, sum_min_rate = 0;
+ 	int non_power_of_2_qcount = 0;
+ 	struct ice_pf *pf = vsi->back;
+ 	int max_rss_q_cnt = 0;
+ 	struct device *dev;
+ 	int i, speed;
+ 	u8 num_tc;
+ 
+ 	if (vsi->type != ICE_VSI_PF)
+ 		return -EINVAL;
+ 
+ 	if (mqprio_qopt->qopt.offset[0] != 0 ||
+ 	    mqprio_qopt->qopt.num_tc < 1 ||
+ 	    mqprio_qopt->qopt.num_tc > ICE_CHNL_MAX_TC)
+ 		return -EINVAL;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	vsi->ch_rss_size = 0;
+ 	num_tc = mqprio_qopt->qopt.num_tc;
+ 
+ 	for (i = 0; num_tc; i++) {
+ 		int qcount = mqprio_qopt->qopt.count[i];
+ 		u64 max_rate, min_rate, rem;
+ 
+ 		if (!qcount)
+ 			return -EINVAL;
+ 
+ 		if (is_power_of_2(qcount)) {
+ 			if (non_power_of_2_qcount &&
+ 			    qcount > non_power_of_2_qcount) {
+ 				dev_err(dev, "qcount[%d] cannot be greater than non power of 2 qcount[%d]\n",
+ 					qcount, non_power_of_2_qcount);
+ 				return -EINVAL;
+ 			}
+ 			if (qcount > max_rss_q_cnt)
+ 				max_rss_q_cnt = qcount;
+ 		} else {
+ 			if (non_power_of_2_qcount &&
+ 			    qcount != non_power_of_2_qcount) {
+ 				dev_err(dev, "Only one non power of 2 qcount allowed[%d,%d]\n",
+ 					qcount, non_power_of_2_qcount);
+ 				return -EINVAL;
+ 			}
+ 			if (qcount < max_rss_q_cnt) {
+ 				dev_err(dev, "non power of 2 qcount[%d] cannot be less than other qcount[%d]\n",
+ 					qcount, max_rss_q_cnt);
+ 				return -EINVAL;
+ 			}
+ 			max_rss_q_cnt = qcount;
+ 			non_power_of_2_qcount = qcount;
+ 		}
+ 
+ 		/* TC command takes input in K/N/Gbps or K/M/Gbit etc but
+ 		 * converts the bandwidth rate limit into Bytes/s when
+ 		 * passing it down to the driver. So convert input bandwidth
+ 		 * from Bytes/s to Kbps
+ 		 */
+ 		max_rate = mqprio_qopt->max_rate[i];
+ 		max_rate = div_u64(max_rate, ICE_BW_KBPS_DIVISOR);
+ 		sum_max_rate += max_rate;
+ 
+ 		/* min_rate is minimum guaranteed rate and it can't be zero */
+ 		min_rate = mqprio_qopt->min_rate[i];
+ 		min_rate = div_u64(min_rate, ICE_BW_KBPS_DIVISOR);
+ 		sum_min_rate += min_rate;
+ 
+ 		if (min_rate && min_rate < ICE_MIN_BW_LIMIT) {
+ 			dev_err(dev, "TC%d: min_rate(%llu Kbps) < %u Kbps\n", i,
+ 				min_rate, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		iter_div_u64_rem(min_rate, ICE_MIN_BW_LIMIT, &rem);
+ 		if (rem) {
+ 			dev_err(dev, "TC%d: Min Rate not multiple of %u Kbps",
+ 				i, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		iter_div_u64_rem(max_rate, ICE_MIN_BW_LIMIT, &rem);
+ 		if (rem) {
+ 			dev_err(dev, "TC%d: Max Rate not multiple of %u Kbps",
+ 				i, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		/* min_rate can't be more than max_rate, except when max_rate
+ 		 * is zero (implies max_rate sought is max line rate). In such
+ 		 * a case min_rate can be more than max.
+ 		 */
+ 		if (max_rate && min_rate > max_rate) {
+ 			dev_err(dev, "min_rate %llu Kbps can't be more than max_rate %llu Kbps\n",
+ 				min_rate, max_rate);
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (i >= mqprio_qopt->qopt.num_tc - 1)
+ 			break;
+ 		if (mqprio_qopt->qopt.offset[i + 1] !=
+ 		    (mqprio_qopt->qopt.offset[i] + qcount))
+ 			return -EINVAL;
+ 	}
+ 	if (vsi->num_rxq <
+ 	    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))
+ 		return -EINVAL;
+ 	if (vsi->num_txq <
+ 	    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))
+ 		return -EINVAL;
+ 
+ 	speed = ice_get_link_speed_kbps(vsi);
+ 	if (sum_max_rate && sum_max_rate > (u64)speed) {
+ 		dev_err(dev, "Invalid max Tx rate(%llu) Kbps > speed(%u) Kbps specified\n",
+ 			sum_max_rate, speed);
+ 		return -EINVAL;
+ 	}
+ 	if (sum_min_rate && sum_min_rate > (u64)speed) {
+ 		dev_err(dev, "Invalid min Tx rate(%llu) Kbps > speed (%u) Kbps specified\n",
+ 			sum_min_rate, speed);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* make sure vsi->ch_rss_size is set correctly based on TC's qcount */
+ 	vsi->ch_rss_size = max_rss_q_cnt;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_add_channel - add a channel by adding VSI
+  * @pf: ptr to PF device
+  * @sw_id: underlying HW switching element ID
+  * @ch: ptr to channel structure
+  *
+  * Add a channel (VSI) using add_vsi and queue_map
+  */
+ static int ice_add_channel(struct ice_pf *pf, u16 sw_id, struct ice_channel *ch)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vsi *vsi;
+ 
+ 	if (ch->type != ICE_VSI_CHNL) {
+ 		dev_err(dev, "add new VSI failed, ch->type %d\n", ch->type);
+ 		return -EINVAL;
+ 	}
+ 
+ 	vsi = ice_chnl_vsi_setup(pf, pf->hw.port_info, ch);
+ 	if (!vsi || vsi->type != ICE_VSI_CHNL) {
+ 		dev_err(dev, "create chnl VSI failure\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	ch->sw_id = sw_id;
+ 	ch->vsi_num = vsi->vsi_num;
+ 	ch->info.mapping_flags = vsi->info.mapping_flags;
+ 	ch->ch_vsi = vsi;
+ 	/* set the back pointer of channel for newly created VSI */
+ 	vsi->ch = ch;
+ 
+ 	memcpy(&ch->info.q_mapping, &vsi->info.q_mapping,
+ 	       sizeof(vsi->info.q_mapping));
+ 	memcpy(&ch->info.tc_mapping, vsi->info.tc_mapping,
+ 	       sizeof(vsi->info.tc_mapping));
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_chnl_cfg_res
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  *
+  * Configure channel specific resources such as rings, vector.
+  */
+ static void ice_chnl_cfg_res(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ch->num_txq; i++) {
+ 		struct ice_q_vector *tx_q_vector, *rx_q_vector;
+ 		struct ice_ring_container *rc;
+ 		struct ice_tx_ring *tx_ring;
+ 		struct ice_rx_ring *rx_ring;
+ 
+ 		tx_ring = vsi->tx_rings[ch->base_q + i];
+ 		rx_ring = vsi->rx_rings[ch->base_q + i];
+ 		if (!tx_ring || !rx_ring)
+ 			continue;
+ 
+ 		/* setup ring being channel enabled */
+ 		tx_ring->ch = ch;
+ 		rx_ring->ch = ch;
+ 
+ 		/* following code block sets up vector specific attributes */
+ 		tx_q_vector = tx_ring->q_vector;
+ 		rx_q_vector = rx_ring->q_vector;
+ 		if (!tx_q_vector && !rx_q_vector)
+ 			continue;
+ 
+ 		if (tx_q_vector) {
+ 			tx_q_vector->ch = ch;
+ 			/* setup Tx and Rx ITR setting if DIM is off */
+ 			rc = &tx_q_vector->tx;
+ 			if (!ITR_IS_DYNAMIC(rc))
+ 				ice_write_itr(rc, rc->itr_setting);
+ 		}
+ 		if (rx_q_vector) {
+ 			rx_q_vector->ch = ch;
+ 			/* setup Tx and Rx ITR setting if DIM is off */
+ 			rc = &rx_q_vector->rx;
+ 			if (!ITR_IS_DYNAMIC(rc))
+ 				ice_write_itr(rc, rc->itr_setting);
+ 		}
+ 	}
+ 
+ 	/* it is safe to assume that, if channel has non-zero num_t[r]xq, then
+ 	 * GLINT_ITR register would have written to perform in-context
+ 	 * update, hence perform flush
+ 	 */
+ 	if (ch->num_txq || ch->num_rxq)
+ 		ice_flush(&vsi->back->hw);
+ }
+ 
+ /**
+  * ice_cfg_chnl_all_res - configure channel resources
+  * @vsi: pte to main_vsi
+  * @ch: ptr to channel structure
+  *
+  * This function configures channel specific resources such as flow-director
+  * counter index, and other resources such as queues, vectors, ITR settings
+  */
+ static void
+ ice_cfg_chnl_all_res(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	/* configure channel (aka ADQ) resources such as queues, vectors,
+ 	 * ITR settings for channel specific vectors and anything else
+ 	 */
+ 	ice_chnl_cfg_res(vsi, ch);
+ }
+ 
+ /**
+  * ice_setup_hw_channel - setup new channel
+  * @pf: ptr to PF device
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  * @sw_id: underlying HW switching element ID
+  * @type: type of channel to be created (VMDq2/VF)
+  *
+  * Setup new channel (VSI) based on specified type (VMDq2/VF)
+  * and configures Tx rings accordingly
+  */
+ static int
+ ice_setup_hw_channel(struct ice_pf *pf, struct ice_vsi *vsi,
+ 		     struct ice_channel *ch, u16 sw_id, u8 type)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int ret;
+ 
+ 	ch->base_q = vsi->next_base_q;
+ 	ch->type = type;
+ 
+ 	ret = ice_add_channel(pf, sw_id, ch);
+ 	if (ret) {
+ 		dev_err(dev, "failed to add_channel using sw_id %u\n", sw_id);
+ 		return ret;
+ 	}
+ 
+ 	/* configure/setup ADQ specific resources */
+ 	ice_cfg_chnl_all_res(vsi, ch);
+ 
+ 	/* make sure to update the next_base_q so that subsequent channel's
+ 	 * (aka ADQ) VSI queue map is correct
+ 	 */
+ 	vsi->next_base_q = vsi->next_base_q + ch->num_rxq;
+ 	dev_dbg(dev, "added channel: vsi_num %u, num_rxq %u\n", ch->vsi_num,
+ 		ch->num_rxq);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_setup_channel - setup new channel using uplink element
+  * @pf: ptr to PF device
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  *
+  * Setup new channel (VSI) based on specified type (VMDq2/VF)
+  * and uplink switching element
+  */
+ static bool
+ ice_setup_channel(struct ice_pf *pf, struct ice_vsi *vsi,
+ 		  struct ice_channel *ch)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	u16 sw_id;
+ 	int ret;
+ 
+ 	if (vsi->type != ICE_VSI_PF) {
+ 		dev_err(dev, "unsupported parent VSI type(%d)\n", vsi->type);
+ 		return false;
+ 	}
+ 
+ 	sw_id = pf->first_sw->sw_id;
+ 
+ 	/* create channel (VSI) */
+ 	ret = ice_setup_hw_channel(pf, vsi, ch, sw_id, ICE_VSI_CHNL);
+ 	if (ret) {
+ 		dev_err(dev, "failed to setup hw_channel\n");
+ 		return false;
+ 	}
+ 	dev_dbg(dev, "successfully created channel()\n");
+ 
+ 	return ch->ch_vsi ? true : false;
+ }
+ 
+ /**
+  * ice_set_bw_limit - setup BW limit for Tx traffic based on max_tx_rate
+  * @vsi: VSI to be configured
+  * @max_tx_rate: max Tx rate in Kbps to be configured as maximum BW limit
+  * @min_tx_rate: min Tx rate in Kbps to be configured as minimum BW limit
+  */
+ static int
+ ice_set_bw_limit(struct ice_vsi *vsi, u64 max_tx_rate, u64 min_tx_rate)
+ {
+ 	int err;
+ 
+ 	err = ice_set_min_bw_limit(vsi, min_tx_rate);
+ 	if (err)
+ 		return err;
+ 
+ 	return ice_set_max_bw_limit(vsi, max_tx_rate);
+ }
+ 
+ /**
+  * ice_create_q_channel - function to create channel
+  * @vsi: VSI to be configured
+  * @ch: ptr to channel (it contains channel specific params)
+  *
+  * This function creates channel (VSI) using num_queues specified by user,
+  * reconfigs RSS if needed.
+  */
+ static int ice_create_q_channel(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct device *dev;
+ 
+ 	if (!ch)
+ 		return -EINVAL;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	if (!ch->num_txq || !ch->num_rxq) {
+ 		dev_err(dev, "Invalid num_queues requested: %d\n", ch->num_rxq);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!vsi->cnt_q_avail || vsi->cnt_q_avail < ch->num_txq) {
+ 		dev_err(dev, "cnt_q_avail (%u) less than num_queues %d\n",
+ 			vsi->cnt_q_avail, ch->num_txq);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!ice_setup_channel(pf, vsi, ch)) {
+ 		dev_info(dev, "Failed to setup channel\n");
+ 		return -EINVAL;
+ 	}
+ 	/* configure BW rate limit */
+ 	if (ch->ch_vsi && (ch->max_tx_rate || ch->min_tx_rate)) {
+ 		int ret;
+ 
+ 		ret = ice_set_bw_limit(ch->ch_vsi, ch->max_tx_rate,
+ 				       ch->min_tx_rate);
+ 		if (ret)
+ 			dev_err(dev, "failed to set Tx rate of %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->ch_vsi->vsi_num);
+ 		else
+ 			dev_dbg(dev, "set Tx rate of %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->ch_vsi->vsi_num);
+ 	}
+ 
+ 	vsi->cnt_q_avail -= ch->num_txq;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_rem_all_chnl_fltrs - removes all channel filters
+  * @pf: ptr to PF, TC-flower based filter are tracked at PF level
+  *
+  * Remove all advanced switch filters only if they are channel specific
+  * tc-flower based filter
+  */
+ static void ice_rem_all_chnl_fltrs(struct ice_pf *pf)
+ {
+ 	struct ice_tc_flower_fltr *fltr;
+ 	struct hlist_node *node;
+ 
+ 	/* to remove all channel filters, iterate an ordered list of filters */
+ 	hlist_for_each_entry_safe(fltr, node,
+ 				  &pf->tc_flower_fltr_list,
+ 				  tc_flower_node) {
+ 		struct ice_rule_query_data rule;
+ 		int status;
+ 
+ 		/* for now process only channel specific filters */
+ 		if (!ice_is_chnl_fltr(fltr))
+ 			continue;
+ 
+ 		rule.rid = fltr->rid;
+ 		rule.rule_id = fltr->rule_id;
+ 		rule.vsi_handle = fltr->dest_id;
+ 		status = ice_rem_adv_rule_by_id(&pf->hw, &rule);
+ 		if (status) {
+ 			if (status == -ENOENT)
+ 				dev_dbg(ice_pf_to_dev(pf), "TC flower filter (rule_id %u) does not exist\n",
+ 					rule.rule_id);
+ 			else
+ 				dev_err(ice_pf_to_dev(pf), "failed to delete TC flower filter, status %d\n",
+ 					status);
+ 		} else if (fltr->dest_vsi) {
+ 			/* update advanced switch filter count */
+ 			if (fltr->dest_vsi->type == ICE_VSI_CHNL) {
+ 				u32 flags = fltr->flags;
+ 
+ 				fltr->dest_vsi->num_chnl_fltr--;
+ 				if (flags & (ICE_TC_FLWR_FIELD_DST_MAC |
+ 					     ICE_TC_FLWR_FIELD_ENC_DST_MAC))
+ 					pf->num_dmac_chnl_fltrs--;
+ 			}
+ 		}
+ 
+ 		hlist_del(&fltr->tc_flower_node);
+ 		kfree(fltr);
+ 	}
+ }
+ 
+ /**
+  * ice_remove_q_channels - Remove queue channels for the TCs
+  * @vsi: VSI to be configured
+  * @rem_fltr: delete advanced switch filter or not
+  *
+  * Remove queue channels for the TCs
+  */
+ static void ice_remove_q_channels(struct ice_vsi *vsi, bool rem_fltr)
+ {
+ 	struct ice_channel *ch, *ch_tmp;
+ 	struct ice_pf *pf = vsi->back;
+ 	int i;
+ 
+ 	/* remove all tc-flower based filter if they are channel filters only */
+ 	if (rem_fltr)
+ 		ice_rem_all_chnl_fltrs(pf);
+ 
+ 	/* perform cleanup for channels if they exist */
+ 	list_for_each_entry_safe(ch, ch_tmp, &vsi->ch_list, list) {
+ 		struct ice_vsi *ch_vsi;
+ 
+ 		list_del(&ch->list);
+ 		ch_vsi = ch->ch_vsi;
+ 		if (!ch_vsi) {
+ 			kfree(ch);
+ 			continue;
+ 		}
+ 
+ 		/* Reset queue contexts */
+ 		for (i = 0; i < ch->num_rxq; i++) {
+ 			struct ice_tx_ring *tx_ring;
+ 			struct ice_rx_ring *rx_ring;
+ 
+ 			tx_ring = vsi->tx_rings[ch->base_q + i];
+ 			rx_ring = vsi->rx_rings[ch->base_q + i];
+ 			if (tx_ring) {
+ 				tx_ring->ch = NULL;
+ 				if (tx_ring->q_vector)
+ 					tx_ring->q_vector->ch = NULL;
+ 			}
+ 			if (rx_ring) {
+ 				rx_ring->ch = NULL;
+ 				if (rx_ring->q_vector)
+ 					rx_ring->q_vector->ch = NULL;
+ 			}
+ 		}
+ 
+ 		/* clear the VSI from scheduler tree */
+ 		ice_rm_vsi_lan_cfg(ch->ch_vsi->port_info, ch->ch_vsi->idx);
+ 
+ 		/* Delete VSI from FW */
+ 		ice_vsi_delete(ch->ch_vsi);
+ 
+ 		/* Delete VSI from PF and HW VSI arrays */
+ 		ice_vsi_clear(ch->ch_vsi);
+ 
+ 		/* free the channel */
+ 		kfree(ch);
+ 	}
+ 
+ 	/* clear the channel VSI map which is stored in main VSI */
+ 	ice_for_each_chnl_tc(i)
+ 		vsi->tc_map_vsi[i] = NULL;
+ 
+ 	/* reset main VSI's all TC information */
+ 	vsi->all_enatc = 0;
+ 	vsi->all_numtc = 0;
+ }
+ 
+ /**
+  * ice_rebuild_channels - rebuild channel
+  * @pf: ptr to PF
+  *
+  * Recreate channel VSIs and replay filters
+  */
+ static int ice_rebuild_channels(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vsi *main_vsi;
+ 	bool rem_adv_fltr = true;
+ 	struct ice_channel *ch;
+ 	struct ice_vsi *vsi;
+ 	int tc_idx = 1;
+ 	int i, err;
+ 
+ 	main_vsi = ice_get_main_vsi(pf);
+ 	if (!main_vsi)
+ 		return 0;
+ 
+ 	if (!test_bit(ICE_FLAG_TC_MQPRIO, pf->flags) ||
+ 	    main_vsi->old_numtc == 1)
+ 		return 0; /* nothing to be done */
+ 
+ 	/* reconfigure main VSI based on old value of TC and cached values
+ 	 * for MQPRIO opts
+ 	 */
+ 	err = ice_vsi_cfg_tc(main_vsi, main_vsi->old_ena_tc);
+ 	if (err) {
+ 		dev_err(dev, "failed configuring TC(ena_tc:0x%02x) for HW VSI=%u\n",
+ 			main_vsi->old_ena_tc, main_vsi->vsi_num);
+ 		return err;
+ 	}
+ 
+ 	/* rebuild ADQ VSIs */
+ 	ice_for_each_vsi(pf, i) {
+ 		enum ice_vsi_type type;
+ 
+ 		vsi = pf->vsi[i];
+ 		if (!vsi || vsi->type != ICE_VSI_CHNL)
+ 			continue;
+ 
+ 		type = vsi->type;
+ 
+ 		/* rebuild ADQ VSI */
+ 		err = ice_vsi_rebuild(vsi, true);
+ 		if (err) {
+ 			dev_err(dev, "VSI (type:%s) at index %d rebuild failed, err %d\n",
+ 				ice_vsi_type_str(type), vsi->idx, err);
+ 			goto cleanup;
+ 		}
+ 
+ 		/* Re-map HW VSI number, using VSI handle that has been
+ 		 * previously validated in ice_replay_vsi() call above
+ 		 */
+ 		vsi->vsi_num = ice_get_hw_vsi_num(&pf->hw, vsi->idx);
+ 
+ 		/* replay filters for the VSI */
+ 		err = ice_replay_vsi(&pf->hw, vsi->idx);
+ 		if (err) {
+ 			dev_err(dev, "VSI (type:%s) replay failed, err %d, VSI index %d\n",
+ 				ice_vsi_type_str(type), err, vsi->idx);
+ 			rem_adv_fltr = false;
+ 			goto cleanup;
+ 		}
+ 		dev_info(dev, "VSI (type:%s) at index %d rebuilt successfully\n",
+ 			 ice_vsi_type_str(type), vsi->idx);
+ 
+ 		/* store ADQ VSI at correct TC index in main VSI's
+ 		 * map of TC to VSI
+ 		 */
+ 		main_vsi->tc_map_vsi[tc_idx++] = vsi;
+ 	}
+ 
+ 	/* ADQ VSI(s) has been rebuilt successfully, so setup
+ 	 * channel for main VSI's Tx and Rx rings
+ 	 */
+ 	list_for_each_entry(ch, &main_vsi->ch_list, list) {
+ 		struct ice_vsi *ch_vsi;
+ 
+ 		ch_vsi = ch->ch_vsi;
+ 		if (!ch_vsi)
+ 			continue;
+ 
+ 		/* reconfig channel resources */
+ 		ice_cfg_chnl_all_res(main_vsi, ch);
+ 
+ 		/* replay BW rate limit if it is non-zero */
+ 		if (!ch->max_tx_rate && !ch->min_tx_rate)
+ 			continue;
+ 
+ 		err = ice_set_bw_limit(ch_vsi, ch->max_tx_rate,
+ 				       ch->min_tx_rate);
+ 		if (err)
+ 			dev_err(dev, "failed (err:%d) to rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\n",
+ 				err, ch->max_tx_rate, ch->min_tx_rate,
+ 				ch_vsi->vsi_num);
+ 		else
+ 			dev_dbg(dev, "successfully rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->min_tx_rate,
+ 				ch_vsi->vsi_num);
+ 	}
+ 
+ 	/* reconfig RSS for main VSI */
+ 	if (main_vsi->ch_rss_size)
+ 		ice_vsi_cfg_rss_lut_key(main_vsi);
+ 
+ 	return 0;
+ 
+ cleanup:
+ 	ice_remove_q_channels(main_vsi, rem_adv_fltr);
+ 	return err;
+ }
+ 
+ /**
+  * ice_create_q_channels - Add queue channel for the given TCs
+  * @vsi: VSI to be configured
+  *
+  * Configures queue channel mapping to the given TCs
+  */
+ static int ice_create_q_channels(struct ice_vsi *vsi)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct ice_channel *ch;
+ 	int ret = 0, i;
+ 
+ 	ice_for_each_chnl_tc(i) {
+ 		if (!(vsi->all_enatc & BIT(i)))
+ 			continue;
+ 
+ 		ch = kzalloc(sizeof(*ch), GFP_KERNEL);
+ 		if (!ch) {
+ 			ret = -ENOMEM;
+ 			goto err_free;
+ 		}
+ 		INIT_LIST_HEAD(&ch->list);
+ 		ch->num_rxq = vsi->mqprio_qopt.qopt.count[i];
+ 		ch->num_txq = vsi->mqprio_qopt.qopt.count[i];
+ 		ch->base_q = vsi->mqprio_qopt.qopt.offset[i];
+ 		ch->max_tx_rate = vsi->mqprio_qopt.max_rate[i];
+ 		ch->min_tx_rate = vsi->mqprio_qopt.min_rate[i];
+ 
+ 		/* convert to Kbits/s */
+ 		if (ch->max_tx_rate)
+ 			ch->max_tx_rate = div_u64(ch->max_tx_rate,
+ 						  ICE_BW_KBPS_DIVISOR);
+ 		if (ch->min_tx_rate)
+ 			ch->min_tx_rate = div_u64(ch->min_tx_rate,
+ 						  ICE_BW_KBPS_DIVISOR);
+ 
+ 		ret = ice_create_q_channel(vsi, ch);
+ 		if (ret) {
+ 			dev_err(ice_pf_to_dev(pf),
+ 				"failed creating channel TC:%d\n", i);
+ 			kfree(ch);
+ 			goto err_free;
+ 		}
+ 		list_add_tail(&ch->list, &vsi->ch_list);
+ 		vsi->tc_map_vsi[i] = ch->ch_vsi;
+ 		dev_dbg(ice_pf_to_dev(pf),
+ 			"successfully created channel: VSI %pK\n", ch->ch_vsi);
+ 	}
+ 	return 0;
+ 
+ err_free:
+ 	ice_remove_q_channels(vsi, false);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * ice_setup_tc_mqprio_qdisc - configure multiple traffic classes
+  * @netdev: net device to configure
+  * @type_data: TC offload data
+  */
+ static int ice_setup_tc_mqprio_qdisc(struct net_device *netdev, void *type_data)
+ {
+ 	struct tc_mqprio_qopt_offload *mqprio_qopt = type_data;
+ 	struct ice_netdev_priv *np = netdev_priv(netdev);
+ 	struct ice_vsi *vsi = np->vsi;
+ 	struct ice_pf *pf = vsi->back;
+ 	u16 mode, ena_tc_qdisc = 0;
+ 	int cur_txq, cur_rxq;
+ 	u8 hw = 0, num_tcf;
+ 	struct device *dev;
+ 	int ret, i;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	num_tcf = mqprio_qopt->qopt.num_tc;
+ 	hw = mqprio_qopt->qopt.hw;
+ 	mode = mqprio_qopt->mode;
+ 	if (!hw) {
+ 		clear_bit(ICE_FLAG_TC_MQPRIO, pf->flags);
+ 		vsi->ch_rss_size = 0;
+ 		memcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));
+ 		goto config_tcf;
+ 	}
+ 
+ 	/* Generate queue region map for number of TCF requested */
+ 	for (i = 0; i < num_tcf; i++)
+ 		ena_tc_qdisc |= BIT(i);
+ 
+ 	switch (mode) {
+ 	case TC_MQPRIO_MODE_CHANNEL:
+ 
+ 		ret = ice_validate_mqprio_qopt(vsi, mqprio_qopt);
+ 		if (ret) {
+ 			netdev_err(netdev, "failed to validate_mqprio_qopt(), ret %d\n",
+ 				   ret);
+ 			return ret;
+ 		}
+ 		memcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));
+ 		set_bit(ICE_FLAG_TC_MQPRIO, pf->flags);
+ 		/* don't assume state of hw_tc_offload during driver load
+ 		 * and set the flag for TC flower filter if hw_tc_offload
+ 		 * already ON
+ 		 */
+ 		if (vsi->netdev->features & NETIF_F_HW_TC)
+ 			set_bit(ICE_FLAG_CLS_FLOWER, pf->flags);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ config_tcf:
+ 
+ 	/* Requesting same TCF configuration as already enabled */
+ 	if (ena_tc_qdisc == vsi->tc_cfg.ena_tc &&
+ 	    mode != TC_MQPRIO_MODE_CHANNEL)
+ 		return 0;
+ 
+ 	/* Pause VSI queues */
+ 	ice_dis_vsi(vsi, true);
+ 
+ 	if (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))
+ 		ice_remove_q_channels(vsi, true);
+ 
+ 	if (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {
+ 		vsi->req_txq = min_t(int, ice_get_avail_txq_count(pf),
+ 				     num_online_cpus());
+ 		vsi->req_rxq = min_t(int, ice_get_avail_rxq_count(pf),
+ 				     num_online_cpus());
+ 	} else {
+ 		/* logic to rebuild VSI, same like ethtool -L */
+ 		u16 offset = 0, qcount_tx = 0, qcount_rx = 0;
+ 
+ 		for (i = 0; i < num_tcf; i++) {
+ 			if (!(ena_tc_qdisc & BIT(i)))
+ 				continue;
+ 
+ 			offset = vsi->mqprio_qopt.qopt.offset[i];
+ 			qcount_rx = vsi->mqprio_qopt.qopt.count[i];
+ 			qcount_tx = vsi->mqprio_qopt.qopt.count[i];
+ 		}
+ 		vsi->req_txq = offset + qcount_tx;
+ 		vsi->req_rxq = offset + qcount_rx;
+ 
+ 		/* store away original rss_size info, so that it gets reused
+ 		 * form ice_vsi_rebuild during tc-qdisc delete stage - to
+ 		 * determine, what should be the rss_sizefor main VSI
+ 		 */
+ 		vsi->orig_rss_size = vsi->rss_size;
+ 	}
+ 
+ 	/* save current values of Tx and Rx queues before calling VSI rebuild
+ 	 * for fallback option
+ 	 */
+ 	cur_txq = vsi->num_txq;
+ 	cur_rxq = vsi->num_rxq;
+ 
+ 	/* proceed with rebuild main VSI using correct number of queues */
+ 	ret = ice_vsi_rebuild(vsi, false);
+ 	if (ret) {
+ 		/* fallback to current number of queues */
+ 		dev_info(dev, "Rebuild failed with new queues, try with current number of queues\n");
+ 		vsi->req_txq = cur_txq;
+ 		vsi->req_rxq = cur_rxq;
+ 		clear_bit(ICE_RESET_FAILED, pf->state);
+ 		if (ice_vsi_rebuild(vsi, false)) {
+ 			dev_err(dev, "Rebuild of main VSI failed again\n");
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	vsi->all_numtc = num_tcf;
+ 	vsi->all_enatc = ena_tc_qdisc;
+ 	ret = ice_vsi_cfg_tc(vsi, ena_tc_qdisc);
+ 	if (ret) {
+ 		netdev_err(netdev, "failed configuring TC for VSI id=%d\n",
+ 			   vsi->vsi_num);
+ 		goto exit;
+ 	}
+ 
+ 	if (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {
+ 		u64 max_tx_rate = vsi->mqprio_qopt.max_rate[0];
+ 		u64 min_tx_rate = vsi->mqprio_qopt.min_rate[0];
+ 
+ 		/* set TC0 rate limit if specified */
+ 		if (max_tx_rate || min_tx_rate) {
+ 			/* convert to Kbits/s */
+ 			if (max_tx_rate)
+ 				max_tx_rate = div_u64(max_tx_rate, ICE_BW_KBPS_DIVISOR);
+ 			if (min_tx_rate)
+ 				min_tx_rate = div_u64(min_tx_rate, ICE_BW_KBPS_DIVISOR);
+ 
+ 			ret = ice_set_bw_limit(vsi, max_tx_rate, min_tx_rate);
+ 			if (!ret) {
+ 				dev_dbg(dev, "set Tx rate max %llu min %llu for VSI(%u)\n",
+ 					max_tx_rate, min_tx_rate, vsi->vsi_num);
+ 			} else {
+ 				dev_err(dev, "failed to set Tx rate max %llu min %llu for VSI(%u)\n",
+ 					max_tx_rate, min_tx_rate, vsi->vsi_num);
+ 				goto exit;
+ 			}
+ 		}
+ 		ret = ice_create_q_channels(vsi);
+ 		if (ret) {
+ 			netdev_err(netdev, "failed configuring queue channels\n");
+ 			goto exit;
+ 		} else {
+ 			netdev_dbg(netdev, "successfully configured channels\n");
+ 		}
+ 	}
+ 
+ 	if (vsi->ch_rss_size)
+ 		ice_vsi_cfg_rss_lut_key(vsi);
+ 
+ exit:
+ 	/* if error, reset the all_numtc and all_enatc */
+ 	if (ret) {
+ 		vsi->all_numtc = 0;
+ 		vsi->all_enatc = 0;
+ 	}
+ 	/* resume VSI */
+ 	ice_ena_vsi(vsi, true);
+ 
+ 	return ret;
+ }
+ 
+ static LIST_HEAD(ice_block_cb_list);
+ 
+ static int
+ ice_setup_tc(struct net_device *netdev, enum tc_setup_type type,
+ 	     void *type_data)
+ {
+ 	struct ice_netdev_priv *np = netdev_priv(netdev);
+ 	struct ice_pf *pf = np->vsi->back;
+ 	int err;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_BLOCK:
+ 		return flow_block_cb_setup_simple(type_data,
+ 						  &ice_block_cb_list,
+ 						  ice_setup_tc_block_cb,
+ 						  np, np, true);
+ 	case TC_SETUP_QDISC_MQPRIO:
+ 		/* setup traffic classifier for receive side */
+ 		mutex_lock(&pf->tc_mutex);
+ 		err = ice_setup_tc_mqprio_qdisc(netdev, type_data);
+ 		mutex_unlock(&pf->tc_mutex);
+ 		return err;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static struct ice_indr_block_priv *
+ ice_indr_block_priv_lookup(struct ice_netdev_priv *np,
+ 			   struct net_device *netdev)
+ {
+ 	struct ice_indr_block_priv *cb_priv;
+ 
+ 	list_for_each_entry(cb_priv, &np->tc_indr_block_priv_list, list) {
+ 		if (!cb_priv->netdev)
+ 			return NULL;
+ 		if (cb_priv->netdev == netdev)
+ 			return cb_priv;
+ 	}
+ 	return NULL;
+ }
+ 
+ static int
+ ice_indr_setup_block_cb(enum tc_setup_type type, void *type_data,
+ 			void *indr_priv)
+ {
+ 	struct ice_indr_block_priv *priv = indr_priv;
+ 	struct ice_netdev_priv *np = priv->np;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_CLSFLOWER:
+ 		return ice_setup_tc_cls_flower(np, priv->netdev,
+ 					       (struct flow_cls_offload *)
+ 					       type_data);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ static int
+ ice_indr_setup_tc_block(struct net_device *netdev, struct Qdisc *sch,
+ 			struct ice_netdev_priv *np,
+ 			struct flow_block_offload *f, void *data,
+ 			void (*cleanup)(struct flow_block_cb *block_cb))
+ {
+ 	struct ice_indr_block_priv *indr_priv;
+ 	struct flow_block_cb *block_cb;
+ 
+ 	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (f->command) {
+ 	case FLOW_BLOCK_BIND:
+ 		indr_priv = ice_indr_block_priv_lookup(np, netdev);
+ 		if (indr_priv)
+ 			return -EEXIST;
+ 
+ 		indr_priv = kzalloc(sizeof(*indr_priv), GFP_KERNEL);
+ 		if (!indr_priv)
+ 			return -ENOMEM;
+ 
+ 		indr_priv->netdev = netdev;
+ 		indr_priv->np = np;
+ 		list_add(&indr_priv->list, &np->tc_indr_block_priv_list);
+ 
+ 		block_cb =
+ 			flow_indr_block_cb_alloc(ice_indr_setup_block_cb,
+ 						 indr_priv, indr_priv,
+ 						 ice_rep_indr_tc_block_unbind,
+ 						 f, netdev, sch, data, np,
+ 						 cleanup);
+ 
+ 		if (IS_ERR(block_cb)) {
+ 			list_del(&indr_priv->list);
+ 			kfree(indr_priv);
+ 			return PTR_ERR(block_cb);
+ 		}
+ 		flow_block_cb_add(block_cb, f);
+ 		list_add_tail(&block_cb->driver_list, &ice_block_cb_list);
+ 		break;
+ 	case FLOW_BLOCK_UNBIND:
+ 		indr_priv = ice_indr_block_priv_lookup(np, netdev);
+ 		if (!indr_priv)
+ 			return -ENOENT;
+ 
+ 		block_cb = flow_block_cb_lookup(f->block,
+ 						ice_indr_setup_block_cb,
+ 						indr_priv);
+ 		if (!block_cb)
+ 			return -ENOENT;
+ 
+ 		flow_indr_block_cb_remove(block_cb, f);
+ 
+ 		list_del(&block_cb->driver_list);
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ ice_indr_setup_tc_cb(struct net_device *netdev, struct Qdisc *sch,
+ 		     void *cb_priv, enum tc_setup_type type, void *type_data,
+ 		     void *data,
+ 		     void (*cleanup)(struct flow_block_cb *block_cb))
+ {
+ 	switch (type) {
+ 	case TC_SETUP_BLOCK:
+ 		return ice_indr_setup_tc_block(netdev, sch, cb_priv, type_data,
+ 					       data, cleanup);
+ 
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ /**
++>>>>>>> 195bb48fccde (ice: support for indirect notification)
   * ice_open - Called when a network interface becomes active
   * @netdev: network interface device structure
   *
* Unmerged path drivers/net/ethernet/intel/ice/ice_tc_lib.h
diff --git a/drivers/net/ethernet/intel/ice/ice.h b/drivers/net/ethernet/intel/ice/ice.h
index ffdeecf46b8e..e212d46ced62 100644
--- a/drivers/net/ethernet/intel/ice/ice.h
+++ b/drivers/net/ethernet/intel/ice/ice.h
@@ -34,6 +34,7 @@
 #include <linux/if_bridge.h>
 #include <linux/ctype.h>
 #include <linux/bpf.h>
+#include <linux/btf.h>
 #include <linux/auxiliary_bus.h>
 #include <linux/avf/virtchnl.h>
 #include <linux/cpu_rmap.h>
@@ -531,6 +532,13 @@ struct ice_pf {
 struct ice_netdev_priv {
 	struct ice_vsi *vsi;
 	struct ice_repr *repr;
+	/* indirect block callbacks on registered higher level devices
+	 * (e.g. tunnel devices)
+	 *
+	 * tc_indr_block_cb_priv_list is used to look up indirect callback
+	 * private data
+	 */
+	struct list_head tc_indr_block_priv_list;
 };
 
 /**
* Unmerged path drivers/net/ethernet/intel/ice/ice_main.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_tc_lib.h
