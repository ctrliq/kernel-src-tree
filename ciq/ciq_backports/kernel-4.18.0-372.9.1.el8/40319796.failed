ice: Add flow director support for channel mode

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Kiran Patil <kiran.patil@intel.com>
commit 40319796b7321e25d23671e525e01a1f4d85f6b6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/40319796.failed

Add support to enable flow-director filter when multiple TCs are
configured. Flow director filter can be configured using ethtool
(--config-ntuple option). When multiple TCs are configured, each
TC is mapped to an unique HW VSI. So VSI corresponding to queue
used in filter is identified and flow director context is updated
with correct VSI while configuring ntuple filter in HW.

	Signed-off-by: Kiran Patil <kiran.patil@intel.com>
	Signed-off-by: Amritha Nambiar <amritha.nambiar@intel.com>
	Signed-off-by: Sudheer Mogilappagari <sudheer.mogilappagari@intel.com>
	Tested-by: Bharathi Sreenivas <bharathi.sreenivas@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 40319796b7321e25d23671e525e01a1f4d85f6b6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice.h
#	drivers/net/ethernet/intel/ice/ice_main.c
diff --cc drivers/net/ethernet/intel/ice/ice.h
index 7f60707725ce,4e16d185077d..000000000000
--- a/drivers/net/ethernet/intel/ice/ice.h
+++ b/drivers/net/ethernet/intel/ice/ice.h
@@@ -102,6 -110,9 +102,12 @@@
  #define ICE_INVAL_VFID		256
  
  #define ICE_MAX_RXQS_PER_TC		256	/* Used when setting VSI context per TC Rx queues */
++<<<<<<< HEAD
++=======
+ 
+ #define ICE_CHNL_START_TC		1
+ 
++>>>>>>> 40319796b732 (ice: Add flow director support for channel mode)
  #define ICE_MAX_RESET_WAIT		20
  
  #define ICE_VSIQF_HKEY_ARRAY_SIZE	((VSIQF_HKEY_MAX_INDEX + 1) *	4)
@@@ -165,6 -186,24 +171,27 @@@ enum ice_feature 
  	ICE_F_MAX
  };
  
++<<<<<<< HEAD
++=======
+ DECLARE_STATIC_KEY_FALSE(ice_xdp_locking_key);
+ 
+ struct ice_channel {
+ 	struct list_head list;
+ 	u8 type;
+ 	u16 sw_id;
+ 	u16 base_q;
+ 	u16 num_rxq;
+ 	u16 num_txq;
+ 	u16 vsi_num;
+ 	u8 ena_tc;
+ 	struct ice_aqc_vsi_props info;
+ 	u64 max_tx_rate;
+ 	u64 min_tx_rate;
+ 	atomic_t num_sb_fltr;
+ 	struct ice_vsi *ch_vsi;
+ };
+ 
++>>>>>>> 40319796b732 (ice: Add flow director support for channel mode)
  struct ice_txq_meta {
  	u32 q_teid;	/* Tx-scheduler element identifier */
  	u16 q_id;	/* Entry in VSI's txq_map bitmap */
@@@ -683,7 -792,34 +710,10 @@@ static inline void ice_clear_sriov_cap(
  #define ICE_FD_STAT_PF_IDX(base_idx) \
  			((base_idx) * ICE_FD_STAT_CTR_BLOCK_COUNT)
  #define ICE_FD_SB_STAT_IDX(base_idx) ICE_FD_STAT_PF_IDX(base_idx)
+ #define ICE_FD_STAT_CH			1
+ #define ICE_FD_CH_STAT_IDX(base_idx) \
+ 			(ICE_FD_STAT_PF_IDX(base_idx) + ICE_FD_STAT_CH)
  
 -/**
 - * ice_is_adq_active - any active ADQs
 - * @pf: pointer to PF
 - *
 - * This function returns true if there are any ADQs configured (which is
 - * determined by looking at VSI type (which should be VSI_PF), numtc, and
 - * TC_MQPRIO flag) otherwise return false
 - */
 -static inline bool ice_is_adq_active(struct ice_pf *pf)
 -{
 -	struct ice_vsi *vsi;
 -
 -	vsi = ice_get_main_vsi(pf);
 -	if (!vsi)
 -		return false;
 -
 -	/* is ADQ configured */
 -	if (vsi->tc_cfg.numtc > ICE_CHNL_START_TC &&
 -	    test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))
 -		return true;
 -
 -	return false;
 -}
 -
  bool netif_is_ice(struct net_device *dev);
  int ice_vsi_setup_tx_rings(struct ice_vsi *vsi);
  int ice_vsi_setup_rx_rings(struct ice_vsi *vsi);
diff --cc drivers/net/ethernet/intel/ice/ice_main.c
index 7f5eec2f214d,e29176889c23..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_main.c
+++ b/drivers/net/ethernet/intel/ice/ice_main.c
@@@ -7069,6 -7254,1125 +7069,1128 @@@ static void ice_tx_timeout(struct net_d
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * ice_setup_tc_cls_flower - flower classifier offloads
+  * @np: net device to configure
+  * @filter_dev: device on which filter is added
+  * @cls_flower: offload data
+  */
+ static int
+ ice_setup_tc_cls_flower(struct ice_netdev_priv *np,
+ 			struct net_device *filter_dev,
+ 			struct flow_cls_offload *cls_flower)
+ {
+ 	struct ice_vsi *vsi = np->vsi;
+ 
+ 	if (cls_flower->common.chain_index)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (cls_flower->command) {
+ 	case FLOW_CLS_REPLACE:
+ 		return ice_add_cls_flower(filter_dev, vsi, cls_flower);
+ 	case FLOW_CLS_DESTROY:
+ 		return ice_del_cls_flower(vsi, cls_flower);
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
+ /**
+  * ice_setup_tc_block_cb - callback handler registered for TC block
+  * @type: TC SETUP type
+  * @type_data: TC flower offload data that contains user input
+  * @cb_priv: netdev private data
+  */
+ static int
+ ice_setup_tc_block_cb(enum tc_setup_type type, void *type_data, void *cb_priv)
+ {
+ 	struct ice_netdev_priv *np = cb_priv;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_CLSFLOWER:
+ 		return ice_setup_tc_cls_flower(np, np->vsi->netdev,
+ 					       type_data);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ /**
+  * ice_validate_mqprio_qopt - Validate TCF input parameters
+  * @vsi: Pointer to VSI
+  * @mqprio_qopt: input parameters for mqprio queue configuration
+  *
+  * This function validates MQPRIO params, such as qcount (power of 2 wherever
+  * needed), and make sure user doesn't specify qcount and BW rate limit
+  * for TCs, which are more than "num_tc"
+  */
+ static int
+ ice_validate_mqprio_qopt(struct ice_vsi *vsi,
+ 			 struct tc_mqprio_qopt_offload *mqprio_qopt)
+ {
+ 	u64 sum_max_rate = 0, sum_min_rate = 0;
+ 	int non_power_of_2_qcount = 0;
+ 	struct ice_pf *pf = vsi->back;
+ 	int max_rss_q_cnt = 0;
+ 	struct device *dev;
+ 	int i, speed;
+ 	u8 num_tc;
+ 
+ 	if (vsi->type != ICE_VSI_PF)
+ 		return -EINVAL;
+ 
+ 	if (mqprio_qopt->qopt.offset[0] != 0 ||
+ 	    mqprio_qopt->qopt.num_tc < 1 ||
+ 	    mqprio_qopt->qopt.num_tc > ICE_CHNL_MAX_TC)
+ 		return -EINVAL;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	vsi->ch_rss_size = 0;
+ 	num_tc = mqprio_qopt->qopt.num_tc;
+ 
+ 	for (i = 0; num_tc; i++) {
+ 		int qcount = mqprio_qopt->qopt.count[i];
+ 		u64 max_rate, min_rate, rem;
+ 
+ 		if (!qcount)
+ 			return -EINVAL;
+ 
+ 		if (is_power_of_2(qcount)) {
+ 			if (non_power_of_2_qcount &&
+ 			    qcount > non_power_of_2_qcount) {
+ 				dev_err(dev, "qcount[%d] cannot be greater than non power of 2 qcount[%d]\n",
+ 					qcount, non_power_of_2_qcount);
+ 				return -EINVAL;
+ 			}
+ 			if (qcount > max_rss_q_cnt)
+ 				max_rss_q_cnt = qcount;
+ 		} else {
+ 			if (non_power_of_2_qcount &&
+ 			    qcount != non_power_of_2_qcount) {
+ 				dev_err(dev, "Only one non power of 2 qcount allowed[%d,%d]\n",
+ 					qcount, non_power_of_2_qcount);
+ 				return -EINVAL;
+ 			}
+ 			if (qcount < max_rss_q_cnt) {
+ 				dev_err(dev, "non power of 2 qcount[%d] cannot be less than other qcount[%d]\n",
+ 					qcount, max_rss_q_cnt);
+ 				return -EINVAL;
+ 			}
+ 			max_rss_q_cnt = qcount;
+ 			non_power_of_2_qcount = qcount;
+ 		}
+ 
+ 		/* TC command takes input in K/N/Gbps or K/M/Gbit etc but
+ 		 * converts the bandwidth rate limit into Bytes/s when
+ 		 * passing it down to the driver. So convert input bandwidth
+ 		 * from Bytes/s to Kbps
+ 		 */
+ 		max_rate = mqprio_qopt->max_rate[i];
+ 		max_rate = div_u64(max_rate, ICE_BW_KBPS_DIVISOR);
+ 		sum_max_rate += max_rate;
+ 
+ 		/* min_rate is minimum guaranteed rate and it can't be zero */
+ 		min_rate = mqprio_qopt->min_rate[i];
+ 		min_rate = div_u64(min_rate, ICE_BW_KBPS_DIVISOR);
+ 		sum_min_rate += min_rate;
+ 
+ 		if (min_rate && min_rate < ICE_MIN_BW_LIMIT) {
+ 			dev_err(dev, "TC%d: min_rate(%llu Kbps) < %u Kbps\n", i,
+ 				min_rate, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		iter_div_u64_rem(min_rate, ICE_MIN_BW_LIMIT, &rem);
+ 		if (rem) {
+ 			dev_err(dev, "TC%d: Min Rate not multiple of %u Kbps",
+ 				i, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		iter_div_u64_rem(max_rate, ICE_MIN_BW_LIMIT, &rem);
+ 		if (rem) {
+ 			dev_err(dev, "TC%d: Max Rate not multiple of %u Kbps",
+ 				i, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		/* min_rate can't be more than max_rate, except when max_rate
+ 		 * is zero (implies max_rate sought is max line rate). In such
+ 		 * a case min_rate can be more than max.
+ 		 */
+ 		if (max_rate && min_rate > max_rate) {
+ 			dev_err(dev, "min_rate %llu Kbps can't be more than max_rate %llu Kbps\n",
+ 				min_rate, max_rate);
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (i >= mqprio_qopt->qopt.num_tc - 1)
+ 			break;
+ 		if (mqprio_qopt->qopt.offset[i + 1] !=
+ 		    (mqprio_qopt->qopt.offset[i] + qcount))
+ 			return -EINVAL;
+ 	}
+ 	if (vsi->num_rxq <
+ 	    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))
+ 		return -EINVAL;
+ 	if (vsi->num_txq <
+ 	    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))
+ 		return -EINVAL;
+ 
+ 	speed = ice_get_link_speed_kbps(vsi);
+ 	if (sum_max_rate && sum_max_rate > (u64)speed) {
+ 		dev_err(dev, "Invalid max Tx rate(%llu) Kbps > speed(%u) Kbps specified\n",
+ 			sum_max_rate, speed);
+ 		return -EINVAL;
+ 	}
+ 	if (sum_min_rate && sum_min_rate > (u64)speed) {
+ 		dev_err(dev, "Invalid min Tx rate(%llu) Kbps > speed (%u) Kbps specified\n",
+ 			sum_min_rate, speed);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* make sure vsi->ch_rss_size is set correctly based on TC's qcount */
+ 	vsi->ch_rss_size = max_rss_q_cnt;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_add_vsi_to_fdir - add a VSI to the flow director group for PF
+  * @pf: ptr to PF device
+  * @vsi: ptr to VSI
+  */
+ static int ice_add_vsi_to_fdir(struct ice_pf *pf, struct ice_vsi *vsi)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	bool added = false;
+ 	struct ice_hw *hw;
+ 	int flow;
+ 
+ 	if (!(vsi->num_gfltr || vsi->num_bfltr))
+ 		return -EINVAL;
+ 
+ 	hw = &pf->hw;
+ 	for (flow = 0; flow < ICE_FLTR_PTYPE_MAX; flow++) {
+ 		struct ice_fd_hw_prof *prof;
+ 		int tun, status;
+ 		u64 entry_h;
+ 
+ 		if (!(hw->fdir_prof && hw->fdir_prof[flow] &&
+ 		      hw->fdir_prof[flow]->cnt))
+ 			continue;
+ 
+ 		for (tun = 0; tun < ICE_FD_HW_SEG_MAX; tun++) {
+ 			enum ice_flow_priority prio;
+ 			u64 prof_id;
+ 
+ 			/* add this VSI to FDir profile for this flow */
+ 			prio = ICE_FLOW_PRIO_NORMAL;
+ 			prof = hw->fdir_prof[flow];
+ 			prof_id = flow + tun * ICE_FLTR_PTYPE_MAX;
+ 			status = ice_flow_add_entry(hw, ICE_BLK_FD, prof_id,
+ 						    prof->vsi_h[0], vsi->idx,
+ 						    prio, prof->fdir_seg[tun],
+ 						    &entry_h);
+ 			if (status) {
+ 				dev_err(dev, "channel VSI idx %d, not able to add to group %d\n",
+ 					vsi->idx, flow);
+ 				continue;
+ 			}
+ 
+ 			prof->entry_h[prof->cnt][tun] = entry_h;
+ 		}
+ 
+ 		/* store VSI for filter replay and delete */
+ 		prof->vsi_h[prof->cnt] = vsi->idx;
+ 		prof->cnt++;
+ 
+ 		added = true;
+ 		dev_dbg(dev, "VSI idx %d added to fdir group %d\n", vsi->idx,
+ 			flow);
+ 	}
+ 
+ 	if (!added)
+ 		dev_dbg(dev, "VSI idx %d not added to fdir groups\n", vsi->idx);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_add_channel - add a channel by adding VSI
+  * @pf: ptr to PF device
+  * @sw_id: underlying HW switching element ID
+  * @ch: ptr to channel structure
+  *
+  * Add a channel (VSI) using add_vsi and queue_map
+  */
+ static int ice_add_channel(struct ice_pf *pf, u16 sw_id, struct ice_channel *ch)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vsi *vsi;
+ 
+ 	if (ch->type != ICE_VSI_CHNL) {
+ 		dev_err(dev, "add new VSI failed, ch->type %d\n", ch->type);
+ 		return -EINVAL;
+ 	}
+ 
+ 	vsi = ice_chnl_vsi_setup(pf, pf->hw.port_info, ch);
+ 	if (!vsi || vsi->type != ICE_VSI_CHNL) {
+ 		dev_err(dev, "create chnl VSI failure\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	ice_add_vsi_to_fdir(pf, vsi);
+ 
+ 	ch->sw_id = sw_id;
+ 	ch->vsi_num = vsi->vsi_num;
+ 	ch->info.mapping_flags = vsi->info.mapping_flags;
+ 	ch->ch_vsi = vsi;
+ 	/* set the back pointer of channel for newly created VSI */
+ 	vsi->ch = ch;
+ 
+ 	memcpy(&ch->info.q_mapping, &vsi->info.q_mapping,
+ 	       sizeof(vsi->info.q_mapping));
+ 	memcpy(&ch->info.tc_mapping, vsi->info.tc_mapping,
+ 	       sizeof(vsi->info.tc_mapping));
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_chnl_cfg_res
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  *
+  * Configure channel specific resources such as rings, vector.
+  */
+ static void ice_chnl_cfg_res(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ch->num_txq; i++) {
+ 		struct ice_q_vector *tx_q_vector, *rx_q_vector;
+ 		struct ice_ring_container *rc;
+ 		struct ice_tx_ring *tx_ring;
+ 		struct ice_rx_ring *rx_ring;
+ 
+ 		tx_ring = vsi->tx_rings[ch->base_q + i];
+ 		rx_ring = vsi->rx_rings[ch->base_q + i];
+ 		if (!tx_ring || !rx_ring)
+ 			continue;
+ 
+ 		/* setup ring being channel enabled */
+ 		tx_ring->ch = ch;
+ 		rx_ring->ch = ch;
+ 
+ 		/* following code block sets up vector specific attributes */
+ 		tx_q_vector = tx_ring->q_vector;
+ 		rx_q_vector = rx_ring->q_vector;
+ 		if (!tx_q_vector && !rx_q_vector)
+ 			continue;
+ 
+ 		if (tx_q_vector) {
+ 			tx_q_vector->ch = ch;
+ 			/* setup Tx and Rx ITR setting if DIM is off */
+ 			rc = &tx_q_vector->tx;
+ 			if (!ITR_IS_DYNAMIC(rc))
+ 				ice_write_itr(rc, rc->itr_setting);
+ 		}
+ 		if (rx_q_vector) {
+ 			rx_q_vector->ch = ch;
+ 			/* setup Tx and Rx ITR setting if DIM is off */
+ 			rc = &rx_q_vector->rx;
+ 			if (!ITR_IS_DYNAMIC(rc))
+ 				ice_write_itr(rc, rc->itr_setting);
+ 		}
+ 	}
+ 
+ 	/* it is safe to assume that, if channel has non-zero num_t[r]xq, then
+ 	 * GLINT_ITR register would have written to perform in-context
+ 	 * update, hence perform flush
+ 	 */
+ 	if (ch->num_txq || ch->num_rxq)
+ 		ice_flush(&vsi->back->hw);
+ }
+ 
+ /**
+  * ice_cfg_chnl_all_res - configure channel resources
+  * @vsi: pte to main_vsi
+  * @ch: ptr to channel structure
+  *
+  * This function configures channel specific resources such as flow-director
+  * counter index, and other resources such as queues, vectors, ITR settings
+  */
+ static void
+ ice_cfg_chnl_all_res(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	/* configure channel (aka ADQ) resources such as queues, vectors,
+ 	 * ITR settings for channel specific vectors and anything else
+ 	 */
+ 	ice_chnl_cfg_res(vsi, ch);
+ }
+ 
+ /**
+  * ice_setup_hw_channel - setup new channel
+  * @pf: ptr to PF device
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  * @sw_id: underlying HW switching element ID
+  * @type: type of channel to be created (VMDq2/VF)
+  *
+  * Setup new channel (VSI) based on specified type (VMDq2/VF)
+  * and configures Tx rings accordingly
+  */
+ static int
+ ice_setup_hw_channel(struct ice_pf *pf, struct ice_vsi *vsi,
+ 		     struct ice_channel *ch, u16 sw_id, u8 type)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int ret;
+ 
+ 	ch->base_q = vsi->next_base_q;
+ 	ch->type = type;
+ 
+ 	ret = ice_add_channel(pf, sw_id, ch);
+ 	if (ret) {
+ 		dev_err(dev, "failed to add_channel using sw_id %u\n", sw_id);
+ 		return ret;
+ 	}
+ 
+ 	/* configure/setup ADQ specific resources */
+ 	ice_cfg_chnl_all_res(vsi, ch);
+ 
+ 	/* make sure to update the next_base_q so that subsequent channel's
+ 	 * (aka ADQ) VSI queue map is correct
+ 	 */
+ 	vsi->next_base_q = vsi->next_base_q + ch->num_rxq;
+ 	dev_dbg(dev, "added channel: vsi_num %u, num_rxq %u\n", ch->vsi_num,
+ 		ch->num_rxq);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_setup_channel - setup new channel using uplink element
+  * @pf: ptr to PF device
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  *
+  * Setup new channel (VSI) based on specified type (VMDq2/VF)
+  * and uplink switching element
+  */
+ static bool
+ ice_setup_channel(struct ice_pf *pf, struct ice_vsi *vsi,
+ 		  struct ice_channel *ch)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	u16 sw_id;
+ 	int ret;
+ 
+ 	if (vsi->type != ICE_VSI_PF) {
+ 		dev_err(dev, "unsupported parent VSI type(%d)\n", vsi->type);
+ 		return false;
+ 	}
+ 
+ 	sw_id = pf->first_sw->sw_id;
+ 
+ 	/* create channel (VSI) */
+ 	ret = ice_setup_hw_channel(pf, vsi, ch, sw_id, ICE_VSI_CHNL);
+ 	if (ret) {
+ 		dev_err(dev, "failed to setup hw_channel\n");
+ 		return false;
+ 	}
+ 	dev_dbg(dev, "successfully created channel()\n");
+ 
+ 	return ch->ch_vsi ? true : false;
+ }
+ 
+ /**
+  * ice_set_bw_limit - setup BW limit for Tx traffic based on max_tx_rate
+  * @vsi: VSI to be configured
+  * @max_tx_rate: max Tx rate in Kbps to be configured as maximum BW limit
+  * @min_tx_rate: min Tx rate in Kbps to be configured as minimum BW limit
+  */
+ static int
+ ice_set_bw_limit(struct ice_vsi *vsi, u64 max_tx_rate, u64 min_tx_rate)
+ {
+ 	int err;
+ 
+ 	err = ice_set_min_bw_limit(vsi, min_tx_rate);
+ 	if (err)
+ 		return err;
+ 
+ 	return ice_set_max_bw_limit(vsi, max_tx_rate);
+ }
+ 
+ /**
+  * ice_create_q_channel - function to create channel
+  * @vsi: VSI to be configured
+  * @ch: ptr to channel (it contains channel specific params)
+  *
+  * This function creates channel (VSI) using num_queues specified by user,
+  * reconfigs RSS if needed.
+  */
+ static int ice_create_q_channel(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct device *dev;
+ 
+ 	if (!ch)
+ 		return -EINVAL;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	if (!ch->num_txq || !ch->num_rxq) {
+ 		dev_err(dev, "Invalid num_queues requested: %d\n", ch->num_rxq);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!vsi->cnt_q_avail || vsi->cnt_q_avail < ch->num_txq) {
+ 		dev_err(dev, "cnt_q_avail (%u) less than num_queues %d\n",
+ 			vsi->cnt_q_avail, ch->num_txq);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!ice_setup_channel(pf, vsi, ch)) {
+ 		dev_info(dev, "Failed to setup channel\n");
+ 		return -EINVAL;
+ 	}
+ 	/* configure BW rate limit */
+ 	if (ch->ch_vsi && (ch->max_tx_rate || ch->min_tx_rate)) {
+ 		int ret;
+ 
+ 		ret = ice_set_bw_limit(ch->ch_vsi, ch->max_tx_rate,
+ 				       ch->min_tx_rate);
+ 		if (ret)
+ 			dev_err(dev, "failed to set Tx rate of %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->ch_vsi->vsi_num);
+ 		else
+ 			dev_dbg(dev, "set Tx rate of %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->ch_vsi->vsi_num);
+ 	}
+ 
+ 	vsi->cnt_q_avail -= ch->num_txq;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_rem_all_chnl_fltrs - removes all channel filters
+  * @pf: ptr to PF, TC-flower based filter are tracked at PF level
+  *
+  * Remove all advanced switch filters only if they are channel specific
+  * tc-flower based filter
+  */
+ static void ice_rem_all_chnl_fltrs(struct ice_pf *pf)
+ {
+ 	struct ice_tc_flower_fltr *fltr;
+ 	struct hlist_node *node;
+ 
+ 	/* to remove all channel filters, iterate an ordered list of filters */
+ 	hlist_for_each_entry_safe(fltr, node,
+ 				  &pf->tc_flower_fltr_list,
+ 				  tc_flower_node) {
+ 		struct ice_rule_query_data rule;
+ 		int status;
+ 
+ 		/* for now process only channel specific filters */
+ 		if (!ice_is_chnl_fltr(fltr))
+ 			continue;
+ 
+ 		rule.rid = fltr->rid;
+ 		rule.rule_id = fltr->rule_id;
+ 		rule.vsi_handle = fltr->dest_id;
+ 		status = ice_rem_adv_rule_by_id(&pf->hw, &rule);
+ 		if (status) {
+ 			if (status == -ENOENT)
+ 				dev_dbg(ice_pf_to_dev(pf), "TC flower filter (rule_id %u) does not exist\n",
+ 					rule.rule_id);
+ 			else
+ 				dev_err(ice_pf_to_dev(pf), "failed to delete TC flower filter, status %d\n",
+ 					status);
+ 		} else if (fltr->dest_vsi) {
+ 			/* update advanced switch filter count */
+ 			if (fltr->dest_vsi->type == ICE_VSI_CHNL) {
+ 				u32 flags = fltr->flags;
+ 
+ 				fltr->dest_vsi->num_chnl_fltr--;
+ 				if (flags & (ICE_TC_FLWR_FIELD_DST_MAC |
+ 					     ICE_TC_FLWR_FIELD_ENC_DST_MAC))
+ 					pf->num_dmac_chnl_fltrs--;
+ 			}
+ 		}
+ 
+ 		hlist_del(&fltr->tc_flower_node);
+ 		kfree(fltr);
+ 	}
+ }
+ 
+ /**
+  * ice_remove_q_channels - Remove queue channels for the TCs
+  * @vsi: VSI to be configured
+  * @rem_fltr: delete advanced switch filter or not
+  *
+  * Remove queue channels for the TCs
+  */
+ static void ice_remove_q_channels(struct ice_vsi *vsi, bool rem_fltr)
+ {
+ 	struct ice_channel *ch, *ch_tmp;
+ 	struct ice_pf *pf = vsi->back;
+ 	int i;
+ 
+ 	/* remove all tc-flower based filter if they are channel filters only */
+ 	if (rem_fltr)
+ 		ice_rem_all_chnl_fltrs(pf);
+ 
+ 	/* remove ntuple filters since queue configuration is being changed */
+ 	if  (vsi->netdev->features & NETIF_F_NTUPLE) {
+ 		struct ice_hw *hw = &pf->hw;
+ 
+ 		mutex_lock(&hw->fdir_fltr_lock);
+ 		ice_fdir_del_all_fltrs(vsi);
+ 		mutex_unlock(&hw->fdir_fltr_lock);
+ 	}
+ 
+ 	/* perform cleanup for channels if they exist */
+ 	list_for_each_entry_safe(ch, ch_tmp, &vsi->ch_list, list) {
+ 		struct ice_vsi *ch_vsi;
+ 
+ 		list_del(&ch->list);
+ 		ch_vsi = ch->ch_vsi;
+ 		if (!ch_vsi) {
+ 			kfree(ch);
+ 			continue;
+ 		}
+ 
+ 		/* Reset queue contexts */
+ 		for (i = 0; i < ch->num_rxq; i++) {
+ 			struct ice_tx_ring *tx_ring;
+ 			struct ice_rx_ring *rx_ring;
+ 
+ 			tx_ring = vsi->tx_rings[ch->base_q + i];
+ 			rx_ring = vsi->rx_rings[ch->base_q + i];
+ 			if (tx_ring) {
+ 				tx_ring->ch = NULL;
+ 				if (tx_ring->q_vector)
+ 					tx_ring->q_vector->ch = NULL;
+ 			}
+ 			if (rx_ring) {
+ 				rx_ring->ch = NULL;
+ 				if (rx_ring->q_vector)
+ 					rx_ring->q_vector->ch = NULL;
+ 			}
+ 		}
+ 
+ 		/* Release FD resources for the channel VSI */
+ 		ice_fdir_rem_adq_chnl(&pf->hw, ch->ch_vsi->idx);
+ 
+ 		/* clear the VSI from scheduler tree */
+ 		ice_rm_vsi_lan_cfg(ch->ch_vsi->port_info, ch->ch_vsi->idx);
+ 
+ 		/* Delete VSI from FW */
+ 		ice_vsi_delete(ch->ch_vsi);
+ 
+ 		/* Delete VSI from PF and HW VSI arrays */
+ 		ice_vsi_clear(ch->ch_vsi);
+ 
+ 		/* free the channel */
+ 		kfree(ch);
+ 	}
+ 
+ 	/* clear the channel VSI map which is stored in main VSI */
+ 	ice_for_each_chnl_tc(i)
+ 		vsi->tc_map_vsi[i] = NULL;
+ 
+ 	/* reset main VSI's all TC information */
+ 	vsi->all_enatc = 0;
+ 	vsi->all_numtc = 0;
+ }
+ 
+ /**
+  * ice_rebuild_channels - rebuild channel
+  * @pf: ptr to PF
+  *
+  * Recreate channel VSIs and replay filters
+  */
+ static int ice_rebuild_channels(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vsi *main_vsi;
+ 	bool rem_adv_fltr = true;
+ 	struct ice_channel *ch;
+ 	struct ice_vsi *vsi;
+ 	int tc_idx = 1;
+ 	int i, err;
+ 
+ 	main_vsi = ice_get_main_vsi(pf);
+ 	if (!main_vsi)
+ 		return 0;
+ 
+ 	if (!test_bit(ICE_FLAG_TC_MQPRIO, pf->flags) ||
+ 	    main_vsi->old_numtc == 1)
+ 		return 0; /* nothing to be done */
+ 
+ 	/* reconfigure main VSI based on old value of TC and cached values
+ 	 * for MQPRIO opts
+ 	 */
+ 	err = ice_vsi_cfg_tc(main_vsi, main_vsi->old_ena_tc);
+ 	if (err) {
+ 		dev_err(dev, "failed configuring TC(ena_tc:0x%02x) for HW VSI=%u\n",
+ 			main_vsi->old_ena_tc, main_vsi->vsi_num);
+ 		return err;
+ 	}
+ 
+ 	/* rebuild ADQ VSIs */
+ 	ice_for_each_vsi(pf, i) {
+ 		enum ice_vsi_type type;
+ 
+ 		vsi = pf->vsi[i];
+ 		if (!vsi || vsi->type != ICE_VSI_CHNL)
+ 			continue;
+ 
+ 		type = vsi->type;
+ 
+ 		/* rebuild ADQ VSI */
+ 		err = ice_vsi_rebuild(vsi, true);
+ 		if (err) {
+ 			dev_err(dev, "VSI (type:%s) at index %d rebuild failed, err %d\n",
+ 				ice_vsi_type_str(type), vsi->idx, err);
+ 			goto cleanup;
+ 		}
+ 
+ 		/* Re-map HW VSI number, using VSI handle that has been
+ 		 * previously validated in ice_replay_vsi() call above
+ 		 */
+ 		vsi->vsi_num = ice_get_hw_vsi_num(&pf->hw, vsi->idx);
+ 
+ 		/* replay filters for the VSI */
+ 		err = ice_replay_vsi(&pf->hw, vsi->idx);
+ 		if (err) {
+ 			dev_err(dev, "VSI (type:%s) replay failed, err %d, VSI index %d\n",
+ 				ice_vsi_type_str(type), err, vsi->idx);
+ 			rem_adv_fltr = false;
+ 			goto cleanup;
+ 		}
+ 		dev_info(dev, "VSI (type:%s) at index %d rebuilt successfully\n",
+ 			 ice_vsi_type_str(type), vsi->idx);
+ 
+ 		/* store ADQ VSI at correct TC index in main VSI's
+ 		 * map of TC to VSI
+ 		 */
+ 		main_vsi->tc_map_vsi[tc_idx++] = vsi;
+ 	}
+ 
+ 	/* ADQ VSI(s) has been rebuilt successfully, so setup
+ 	 * channel for main VSI's Tx and Rx rings
+ 	 */
+ 	list_for_each_entry(ch, &main_vsi->ch_list, list) {
+ 		struct ice_vsi *ch_vsi;
+ 
+ 		ch_vsi = ch->ch_vsi;
+ 		if (!ch_vsi)
+ 			continue;
+ 
+ 		/* reconfig channel resources */
+ 		ice_cfg_chnl_all_res(main_vsi, ch);
+ 
+ 		/* replay BW rate limit if it is non-zero */
+ 		if (!ch->max_tx_rate && !ch->min_tx_rate)
+ 			continue;
+ 
+ 		err = ice_set_bw_limit(ch_vsi, ch->max_tx_rate,
+ 				       ch->min_tx_rate);
+ 		if (err)
+ 			dev_err(dev, "failed (err:%d) to rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\n",
+ 				err, ch->max_tx_rate, ch->min_tx_rate,
+ 				ch_vsi->vsi_num);
+ 		else
+ 			dev_dbg(dev, "successfully rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->min_tx_rate,
+ 				ch_vsi->vsi_num);
+ 	}
+ 
+ 	/* reconfig RSS for main VSI */
+ 	if (main_vsi->ch_rss_size)
+ 		ice_vsi_cfg_rss_lut_key(main_vsi);
+ 
+ 	return 0;
+ 
+ cleanup:
+ 	ice_remove_q_channels(main_vsi, rem_adv_fltr);
+ 	return err;
+ }
+ 
+ /**
+  * ice_create_q_channels - Add queue channel for the given TCs
+  * @vsi: VSI to be configured
+  *
+  * Configures queue channel mapping to the given TCs
+  */
+ static int ice_create_q_channels(struct ice_vsi *vsi)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct ice_channel *ch;
+ 	int ret = 0, i;
+ 
+ 	ice_for_each_chnl_tc(i) {
+ 		if (!(vsi->all_enatc & BIT(i)))
+ 			continue;
+ 
+ 		ch = kzalloc(sizeof(*ch), GFP_KERNEL);
+ 		if (!ch) {
+ 			ret = -ENOMEM;
+ 			goto err_free;
+ 		}
+ 		INIT_LIST_HEAD(&ch->list);
+ 		ch->num_rxq = vsi->mqprio_qopt.qopt.count[i];
+ 		ch->num_txq = vsi->mqprio_qopt.qopt.count[i];
+ 		ch->base_q = vsi->mqprio_qopt.qopt.offset[i];
+ 		ch->max_tx_rate = vsi->mqprio_qopt.max_rate[i];
+ 		ch->min_tx_rate = vsi->mqprio_qopt.min_rate[i];
+ 
+ 		/* convert to Kbits/s */
+ 		if (ch->max_tx_rate)
+ 			ch->max_tx_rate = div_u64(ch->max_tx_rate,
+ 						  ICE_BW_KBPS_DIVISOR);
+ 		if (ch->min_tx_rate)
+ 			ch->min_tx_rate = div_u64(ch->min_tx_rate,
+ 						  ICE_BW_KBPS_DIVISOR);
+ 
+ 		ret = ice_create_q_channel(vsi, ch);
+ 		if (ret) {
+ 			dev_err(ice_pf_to_dev(pf),
+ 				"failed creating channel TC:%d\n", i);
+ 			kfree(ch);
+ 			goto err_free;
+ 		}
+ 		list_add_tail(&ch->list, &vsi->ch_list);
+ 		vsi->tc_map_vsi[i] = ch->ch_vsi;
+ 		dev_dbg(ice_pf_to_dev(pf),
+ 			"successfully created channel: VSI %pK\n", ch->ch_vsi);
+ 	}
+ 	return 0;
+ 
+ err_free:
+ 	ice_remove_q_channels(vsi, false);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * ice_setup_tc_mqprio_qdisc - configure multiple traffic classes
+  * @netdev: net device to configure
+  * @type_data: TC offload data
+  */
+ static int ice_setup_tc_mqprio_qdisc(struct net_device *netdev, void *type_data)
+ {
+ 	struct tc_mqprio_qopt_offload *mqprio_qopt = type_data;
+ 	struct ice_netdev_priv *np = netdev_priv(netdev);
+ 	struct ice_vsi *vsi = np->vsi;
+ 	struct ice_pf *pf = vsi->back;
+ 	u16 mode, ena_tc_qdisc = 0;
+ 	int cur_txq, cur_rxq;
+ 	u8 hw = 0, num_tcf;
+ 	struct device *dev;
+ 	int ret, i;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	num_tcf = mqprio_qopt->qopt.num_tc;
+ 	hw = mqprio_qopt->qopt.hw;
+ 	mode = mqprio_qopt->mode;
+ 	if (!hw) {
+ 		clear_bit(ICE_FLAG_TC_MQPRIO, pf->flags);
+ 		vsi->ch_rss_size = 0;
+ 		memcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));
+ 		goto config_tcf;
+ 	}
+ 
+ 	/* Generate queue region map for number of TCF requested */
+ 	for (i = 0; i < num_tcf; i++)
+ 		ena_tc_qdisc |= BIT(i);
+ 
+ 	switch (mode) {
+ 	case TC_MQPRIO_MODE_CHANNEL:
+ 
+ 		ret = ice_validate_mqprio_qopt(vsi, mqprio_qopt);
+ 		if (ret) {
+ 			netdev_err(netdev, "failed to validate_mqprio_qopt(), ret %d\n",
+ 				   ret);
+ 			return ret;
+ 		}
+ 		memcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));
+ 		set_bit(ICE_FLAG_TC_MQPRIO, pf->flags);
+ 		/* don't assume state of hw_tc_offload during driver load
+ 		 * and set the flag for TC flower filter if hw_tc_offload
+ 		 * already ON
+ 		 */
+ 		if (vsi->netdev->features & NETIF_F_HW_TC)
+ 			set_bit(ICE_FLAG_CLS_FLOWER, pf->flags);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ config_tcf:
+ 
+ 	/* Requesting same TCF configuration as already enabled */
+ 	if (ena_tc_qdisc == vsi->tc_cfg.ena_tc &&
+ 	    mode != TC_MQPRIO_MODE_CHANNEL)
+ 		return 0;
+ 
+ 	/* Pause VSI queues */
+ 	ice_dis_vsi(vsi, true);
+ 
+ 	if (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))
+ 		ice_remove_q_channels(vsi, true);
+ 
+ 	if (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {
+ 		vsi->req_txq = min_t(int, ice_get_avail_txq_count(pf),
+ 				     num_online_cpus());
+ 		vsi->req_rxq = min_t(int, ice_get_avail_rxq_count(pf),
+ 				     num_online_cpus());
+ 	} else {
+ 		/* logic to rebuild VSI, same like ethtool -L */
+ 		u16 offset = 0, qcount_tx = 0, qcount_rx = 0;
+ 
+ 		for (i = 0; i < num_tcf; i++) {
+ 			if (!(ena_tc_qdisc & BIT(i)))
+ 				continue;
+ 
+ 			offset = vsi->mqprio_qopt.qopt.offset[i];
+ 			qcount_rx = vsi->mqprio_qopt.qopt.count[i];
+ 			qcount_tx = vsi->mqprio_qopt.qopt.count[i];
+ 		}
+ 		vsi->req_txq = offset + qcount_tx;
+ 		vsi->req_rxq = offset + qcount_rx;
+ 
+ 		/* store away original rss_size info, so that it gets reused
+ 		 * form ice_vsi_rebuild during tc-qdisc delete stage - to
+ 		 * determine, what should be the rss_sizefor main VSI
+ 		 */
+ 		vsi->orig_rss_size = vsi->rss_size;
+ 	}
+ 
+ 	/* save current values of Tx and Rx queues before calling VSI rebuild
+ 	 * for fallback option
+ 	 */
+ 	cur_txq = vsi->num_txq;
+ 	cur_rxq = vsi->num_rxq;
+ 
+ 	/* proceed with rebuild main VSI using correct number of queues */
+ 	ret = ice_vsi_rebuild(vsi, false);
+ 	if (ret) {
+ 		/* fallback to current number of queues */
+ 		dev_info(dev, "Rebuild failed with new queues, try with current number of queues\n");
+ 		vsi->req_txq = cur_txq;
+ 		vsi->req_rxq = cur_rxq;
+ 		clear_bit(ICE_RESET_FAILED, pf->state);
+ 		if (ice_vsi_rebuild(vsi, false)) {
+ 			dev_err(dev, "Rebuild of main VSI failed again\n");
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	vsi->all_numtc = num_tcf;
+ 	vsi->all_enatc = ena_tc_qdisc;
+ 	ret = ice_vsi_cfg_tc(vsi, ena_tc_qdisc);
+ 	if (ret) {
+ 		netdev_err(netdev, "failed configuring TC for VSI id=%d\n",
+ 			   vsi->vsi_num);
+ 		goto exit;
+ 	}
+ 
+ 	if (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {
+ 		u64 max_tx_rate = vsi->mqprio_qopt.max_rate[0];
+ 		u64 min_tx_rate = vsi->mqprio_qopt.min_rate[0];
+ 
+ 		/* set TC0 rate limit if specified */
+ 		if (max_tx_rate || min_tx_rate) {
+ 			/* convert to Kbits/s */
+ 			if (max_tx_rate)
+ 				max_tx_rate = div_u64(max_tx_rate, ICE_BW_KBPS_DIVISOR);
+ 			if (min_tx_rate)
+ 				min_tx_rate = div_u64(min_tx_rate, ICE_BW_KBPS_DIVISOR);
+ 
+ 			ret = ice_set_bw_limit(vsi, max_tx_rate, min_tx_rate);
+ 			if (!ret) {
+ 				dev_dbg(dev, "set Tx rate max %llu min %llu for VSI(%u)\n",
+ 					max_tx_rate, min_tx_rate, vsi->vsi_num);
+ 			} else {
+ 				dev_err(dev, "failed to set Tx rate max %llu min %llu for VSI(%u)\n",
+ 					max_tx_rate, min_tx_rate, vsi->vsi_num);
+ 				goto exit;
+ 			}
+ 		}
+ 		ret = ice_create_q_channels(vsi);
+ 		if (ret) {
+ 			netdev_err(netdev, "failed configuring queue channels\n");
+ 			goto exit;
+ 		} else {
+ 			netdev_dbg(netdev, "successfully configured channels\n");
+ 		}
+ 	}
+ 
+ 	if (vsi->ch_rss_size)
+ 		ice_vsi_cfg_rss_lut_key(vsi);
+ 
+ exit:
+ 	/* if error, reset the all_numtc and all_enatc */
+ 	if (ret) {
+ 		vsi->all_numtc = 0;
+ 		vsi->all_enatc = 0;
+ 	}
+ 	/* resume VSI */
+ 	ice_ena_vsi(vsi, true);
+ 
+ 	return ret;
+ }
+ 
+ static LIST_HEAD(ice_block_cb_list);
+ 
+ static int
+ ice_setup_tc(struct net_device *netdev, enum tc_setup_type type,
+ 	     void *type_data)
+ {
+ 	struct ice_netdev_priv *np = netdev_priv(netdev);
+ 	struct ice_pf *pf = np->vsi->back;
+ 	int err;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_BLOCK:
+ 		return flow_block_cb_setup_simple(type_data,
+ 						  &ice_block_cb_list,
+ 						  ice_setup_tc_block_cb,
+ 						  np, np, true);
+ 	case TC_SETUP_QDISC_MQPRIO:
+ 		/* setup traffic classifier for receive side */
+ 		mutex_lock(&pf->tc_mutex);
+ 		err = ice_setup_tc_mqprio_qdisc(netdev, type_data);
+ 		mutex_unlock(&pf->tc_mutex);
+ 		return err;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static struct ice_indr_block_priv *
+ ice_indr_block_priv_lookup(struct ice_netdev_priv *np,
+ 			   struct net_device *netdev)
+ {
+ 	struct ice_indr_block_priv *cb_priv;
+ 
+ 	list_for_each_entry(cb_priv, &np->tc_indr_block_priv_list, list) {
+ 		if (!cb_priv->netdev)
+ 			return NULL;
+ 		if (cb_priv->netdev == netdev)
+ 			return cb_priv;
+ 	}
+ 	return NULL;
+ }
+ 
+ static int
+ ice_indr_setup_block_cb(enum tc_setup_type type, void *type_data,
+ 			void *indr_priv)
+ {
+ 	struct ice_indr_block_priv *priv = indr_priv;
+ 	struct ice_netdev_priv *np = priv->np;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_CLSFLOWER:
+ 		return ice_setup_tc_cls_flower(np, priv->netdev,
+ 					       (struct flow_cls_offload *)
+ 					       type_data);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ static int
+ ice_indr_setup_tc_block(struct net_device *netdev, struct Qdisc *sch,
+ 			struct ice_netdev_priv *np,
+ 			struct flow_block_offload *f, void *data,
+ 			void (*cleanup)(struct flow_block_cb *block_cb))
+ {
+ 	struct ice_indr_block_priv *indr_priv;
+ 	struct flow_block_cb *block_cb;
+ 
+ 	if (!ice_is_tunnel_supported(netdev) &&
+ 	    !(is_vlan_dev(netdev) &&
+ 	      vlan_dev_real_dev(netdev) == np->vsi->netdev))
+ 		return -EOPNOTSUPP;
+ 
+ 	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (f->command) {
+ 	case FLOW_BLOCK_BIND:
+ 		indr_priv = ice_indr_block_priv_lookup(np, netdev);
+ 		if (indr_priv)
+ 			return -EEXIST;
+ 
+ 		indr_priv = kzalloc(sizeof(*indr_priv), GFP_KERNEL);
+ 		if (!indr_priv)
+ 			return -ENOMEM;
+ 
+ 		indr_priv->netdev = netdev;
+ 		indr_priv->np = np;
+ 		list_add(&indr_priv->list, &np->tc_indr_block_priv_list);
+ 
+ 		block_cb =
+ 			flow_indr_block_cb_alloc(ice_indr_setup_block_cb,
+ 						 indr_priv, indr_priv,
+ 						 ice_rep_indr_tc_block_unbind,
+ 						 f, netdev, sch, data, np,
+ 						 cleanup);
+ 
+ 		if (IS_ERR(block_cb)) {
+ 			list_del(&indr_priv->list);
+ 			kfree(indr_priv);
+ 			return PTR_ERR(block_cb);
+ 		}
+ 		flow_block_cb_add(block_cb, f);
+ 		list_add_tail(&block_cb->driver_list, &ice_block_cb_list);
+ 		break;
+ 	case FLOW_BLOCK_UNBIND:
+ 		indr_priv = ice_indr_block_priv_lookup(np, netdev);
+ 		if (!indr_priv)
+ 			return -ENOENT;
+ 
+ 		block_cb = flow_block_cb_lookup(f->block,
+ 						ice_indr_setup_block_cb,
+ 						indr_priv);
+ 		if (!block_cb)
+ 			return -ENOENT;
+ 
+ 		flow_indr_block_cb_remove(block_cb, f);
+ 
+ 		list_del(&block_cb->driver_list);
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ ice_indr_setup_tc_cb(struct net_device *netdev, struct Qdisc *sch,
+ 		     void *cb_priv, enum tc_setup_type type, void *type_data,
+ 		     void *data,
+ 		     void (*cleanup)(struct flow_block_cb *block_cb))
+ {
+ 	switch (type) {
+ 	case TC_SETUP_BLOCK:
+ 		return ice_indr_setup_tc_block(netdev, sch, cb_priv, type_data,
+ 					       data, cleanup);
+ 
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ /**
++>>>>>>> 40319796b732 (ice: Add flow director support for channel mode)
   * ice_open - Called when a network interface becomes active
   * @netdev: network interface device structure
   *
* Unmerged path drivers/net/ethernet/intel/ice/ice.h
diff --git a/drivers/net/ethernet/intel/ice/ice_ethtool_fdir.c b/drivers/net/ethernet/intel/ice/ice_ethtool_fdir.c
index 38960bcc384c..5826394be986 100644
--- a/drivers/net/ethernet/intel/ice/ice_ethtool_fdir.c
+++ b/drivers/net/ethernet/intel/ice/ice_ethtool_fdir.c
@@ -5,6 +5,7 @@
 
 #include "ice.h"
 #include "ice_lib.h"
+#include "ice_fdir.h"
 #include "ice_flow.h"
 
 static struct in6_addr full_ipv6_addr_mask = {
@@ -205,7 +206,7 @@ int ice_get_ethtool_fdir_entry(struct ice_hw *hw, struct ethtool_rxnfc *cmd)
 	if (rule->dest_ctl == ICE_FLTR_PRGM_DESC_DEST_DROP_PKT)
 		fsp->ring_cookie = RX_CLS_FLOW_DISC;
 	else
-		fsp->ring_cookie = rule->q_index;
+		fsp->ring_cookie = rule->orig_q_index;
 
 	idx = ice_ethtool_flow_to_fltr(fsp->flow_type);
 	if (idx == ICE_FLTR_PTYPE_NONF_NONE) {
@@ -256,6 +257,80 @@ ice_get_fdir_fltr_ids(struct ice_hw *hw, struct ethtool_rxnfc *cmd,
 	return val;
 }
 
+/**
+ * ice_fdir_remap_entries - update the FDir entries in profile
+ * @prof: FDir structure pointer
+ * @tun: tunneled or non-tunneled packet
+ * @idx: FDir entry index
+ */
+static void
+ice_fdir_remap_entries(struct ice_fd_hw_prof *prof, int tun, int idx)
+{
+	if (idx != prof->cnt && tun < ICE_FD_HW_SEG_MAX) {
+		int i;
+
+		for (i = idx; i < (prof->cnt - 1); i++) {
+			u64 old_entry_h;
+
+			old_entry_h = prof->entry_h[i + 1][tun];
+			prof->entry_h[i][tun] = old_entry_h;
+			prof->vsi_h[i] = prof->vsi_h[i + 1];
+		}
+
+		prof->entry_h[i][tun] = 0;
+		prof->vsi_h[i] = 0;
+	}
+}
+
+/**
+ * ice_fdir_rem_adq_chnl - remove an ADQ channel from HW filter rules
+ * @hw: hardware structure containing filter list
+ * @vsi_idx: VSI handle
+ */
+void ice_fdir_rem_adq_chnl(struct ice_hw *hw, u16 vsi_idx)
+{
+	int status, flow;
+
+	if (!hw->fdir_prof)
+		return;
+
+	for (flow = 0; flow < ICE_FLTR_PTYPE_MAX; flow++) {
+		struct ice_fd_hw_prof *prof = hw->fdir_prof[flow];
+		int tun, i;
+
+		if (!prof || !prof->cnt)
+			continue;
+
+		for (tun = 0; tun < ICE_FD_HW_SEG_MAX; tun++) {
+			u64 prof_id;
+
+			prof_id = flow + tun * ICE_FLTR_PTYPE_MAX;
+
+			for (i = 0; i < prof->cnt; i++) {
+				if (prof->vsi_h[i] != vsi_idx)
+					continue;
+
+				prof->entry_h[i][tun] = 0;
+				prof->vsi_h[i] = 0;
+				break;
+			}
+
+			/* after clearing FDir entries update the remaining */
+			ice_fdir_remap_entries(prof, tun, i);
+
+			/* find flow profile corresponding to prof_id and clear
+			 * vsi_idx from bitmap.
+			 */
+			status = ice_flow_rem_vsi_prof(hw, vsi_idx, prof_id);
+			if (status) {
+				dev_err(ice_hw_to_dev(hw), "ice_flow_rem_vsi_prof() failed status=%d\n",
+					status);
+			}
+		}
+		prof->cnt--;
+	}
+}
+
 /**
  * ice_fdir_get_hw_prof - return the ice_fd_hw_proc associated with a flow
  * @hw: hardware structure containing the filter list
@@ -513,6 +588,28 @@ ice_fdir_alloc_flow_prof(struct ice_hw *hw, enum ice_fltr_ptype flow)
 	return 0;
 }
 
+/**
+ * ice_fdir_prof_vsi_idx - find or insert a vsi_idx in structure
+ * @prof: pointer to flow director HW profile
+ * @vsi_idx: vsi_idx to locate
+ *
+ * return the index of the vsi_idx. if vsi_idx is not found insert it
+ * into the vsi_h table.
+ */
+static u16
+ice_fdir_prof_vsi_idx(struct ice_fd_hw_prof *prof, int vsi_idx)
+{
+	u16 idx = 0;
+
+	for (idx = 0; idx < prof->cnt; idx++)
+		if (prof->vsi_h[idx] == vsi_idx)
+			return idx;
+
+	if (idx == prof->cnt)
+		prof->vsi_h[prof->cnt++] = vsi_idx;
+	return idx;
+}
+
 /**
  * ice_fdir_set_hw_fltr_rule - Configure HW tables to generate a FDir rule
  * @pf: pointer to the PF structure
@@ -533,8 +630,10 @@ ice_fdir_set_hw_fltr_rule(struct ice_pf *pf, struct ice_flow_seg_info *seg,
 	enum ice_status status;
 	u64 entry1_h = 0;
 	u64 entry2_h = 0;
+	bool del_last;
 	u64 prof_id;
 	int err;
+	int idx;
 
 	main_vsi = ice_get_main_vsi(pf);
 	if (!main_vsi)
@@ -608,8 +707,60 @@ ice_fdir_set_hw_fltr_rule(struct ice_pf *pf, struct ice_flow_seg_info *seg,
 	if (!hw_prof->cnt)
 		hw_prof->cnt = 2;
 
+	for (idx = 1; idx < ICE_CHNL_MAX_TC; idx++) {
+		u16 vsi_idx;
+		u16 vsi_h;
+
+		if (!ice_is_adq_active(pf) || !main_vsi->tc_map_vsi[idx])
+			continue;
+
+		entry1_h = 0;
+		vsi_h = main_vsi->tc_map_vsi[idx]->idx;
+		err = ice_flow_add_entry(hw, ICE_BLK_FD, prof_id,
+					 main_vsi->idx, vsi_h,
+					 ICE_FLOW_PRIO_NORMAL, seg,
+					 &entry1_h);
+		if (err) {
+			dev_err(dev, "Could not add Channel VSI %d to flow group\n",
+				idx);
+			goto err_unroll;
+		}
+
+		vsi_idx = ice_fdir_prof_vsi_idx(hw_prof,
+						main_vsi->tc_map_vsi[idx]->idx);
+		hw_prof->entry_h[vsi_idx][tun] = entry1_h;
+	}
+
 	return 0;
 
+err_unroll:
+	entry1_h = 0;
+	hw_prof->fdir_seg[tun] = NULL;
+
+	/* The variable del_last will be used to determine when to clean up
+	 * the VSI group data. The VSI data is not needed if there are no
+	 * segments.
+	 */
+	del_last = true;
+	for (idx = 0; idx < ICE_FD_HW_SEG_MAX; idx++)
+		if (hw_prof->fdir_seg[idx]) {
+			del_last = false;
+			break;
+		}
+
+	for (idx = 0; idx < hw_prof->cnt; idx++) {
+		u16 vsi_num = ice_get_hw_vsi_num(hw, hw_prof->vsi_h[idx]);
+
+		if (!hw_prof->entry_h[idx][tun])
+			continue;
+		ice_rem_prof_id_flow(hw, ICE_BLK_FD, vsi_num, prof_id);
+		ice_flow_rem_entry(hw, ICE_BLK_FD, hw_prof->entry_h[idx][tun]);
+		hw_prof->entry_h[idx][tun] = 0;
+		if (del_last)
+			hw_prof->vsi_h[idx] = 0;
+	}
+	if (del_last)
+		hw_prof->cnt = 0;
 err_entry:
 	ice_rem_prof_id_flow(hw, ICE_BLK_FD,
 			     ice_get_hw_vsi_num(hw, main_vsi->idx), prof_id);
@@ -1173,6 +1324,31 @@ ice_cfg_fdir_xtrct_seq(struct ice_pf *pf, struct ethtool_rx_flow_spec *fsp,
 	return -EOPNOTSUPP;
 }
 
+/**
+ * ice_update_per_q_fltr
+ * @vsi: ptr to VSI
+ * @q_index: queue index
+ * @inc: true to increment or false to decrement per queue filter count
+ *
+ * This function is used to keep track of per queue sideband filters
+ */
+static void ice_update_per_q_fltr(struct ice_vsi *vsi, u32 q_index, bool inc)
+{
+	struct ice_rx_ring *rx_ring;
+
+	if (!vsi->num_rxq || q_index >= vsi->num_rxq)
+		return;
+
+	rx_ring = vsi->rx_rings[q_index];
+	if (!rx_ring || !rx_ring->ch)
+		return;
+
+	if (inc)
+		atomic_inc(&rx_ring->ch->num_sb_fltr);
+	else
+		atomic_dec_if_positive(&rx_ring->ch->num_sb_fltr);
+}
+
 /**
  * ice_fdir_write_fltr - send a flow director filter to the hardware
  * @pf: PF data structure
@@ -1323,6 +1499,26 @@ int ice_fdir_create_dflt_rules(struct ice_pf *pf)
 	return err;
 }
 
+/**
+ * ice_fdir_del_all_fltrs - Delete all flow director filters
+ * @vsi: the VSI being changed
+ *
+ * This function needs to be called while holding hw->fdir_fltr_lock
+ */
+void ice_fdir_del_all_fltrs(struct ice_vsi *vsi)
+{
+	struct ice_fdir_fltr *f_rule, *tmp;
+	struct ice_pf *pf = vsi->back;
+	struct ice_hw *hw = &pf->hw;
+
+	list_for_each_entry_safe(f_rule, tmp, &hw->fdir_list_head, fltr_node) {
+		ice_fdir_write_all_fltr(pf, f_rule, false);
+		ice_fdir_update_cntrs(hw, f_rule->flow_type, false);
+		list_del(&f_rule->fltr_node);
+		devm_kfree(ice_pf_to_dev(pf), f_rule);
+	}
+}
+
 /**
  * ice_vsi_manage_fdir - turn on/off flow director
  * @vsi: the VSI being changed
@@ -1330,7 +1526,6 @@ int ice_fdir_create_dflt_rules(struct ice_pf *pf)
  */
 void ice_vsi_manage_fdir(struct ice_vsi *vsi, bool ena)
 {
-	struct ice_fdir_fltr *f_rule, *tmp;
 	struct ice_pf *pf = vsi->back;
 	struct ice_hw *hw = &pf->hw;
 	enum ice_fltr_ptype flow;
@@ -1344,13 +1539,8 @@ void ice_vsi_manage_fdir(struct ice_vsi *vsi, bool ena)
 	mutex_lock(&hw->fdir_fltr_lock);
 	if (!test_and_clear_bit(ICE_FLAG_FD_ENA, pf->flags))
 		goto release_lock;
-	list_for_each_entry_safe(f_rule, tmp, &hw->fdir_list_head, fltr_node) {
-		/* ignore return value */
-		ice_fdir_write_all_fltr(pf, f_rule, false);
-		ice_fdir_update_cntrs(hw, f_rule->flow_type, false);
-		list_del(&f_rule->fltr_node);
-		devm_kfree(ice_hw_to_dev(hw), f_rule);
-	}
+
+	ice_fdir_del_all_fltrs(vsi);
 
 	if (hw->fdir_prof)
 		for (flow = ICE_FLTR_PTYPE_NONF_NONE; flow < ICE_FLTR_PTYPE_MAX;
@@ -1401,18 +1591,25 @@ ice_fdir_update_list_entry(struct ice_pf *pf, struct ice_fdir_fltr *input,
 {
 	struct ice_fdir_fltr *old_fltr;
 	struct ice_hw *hw = &pf->hw;
+	struct ice_vsi *vsi;
 	int err = -ENOENT;
 
 	/* Do not update filters during reset */
 	if (ice_is_reset_in_progress(pf->state))
 		return -EBUSY;
 
+	vsi = ice_get_main_vsi(pf);
+	if (!vsi)
+		return -EINVAL;
+
 	old_fltr = ice_fdir_find_fltr_by_idx(hw, fltr_idx);
 	if (old_fltr) {
 		err = ice_fdir_write_all_fltr(pf, old_fltr, false);
 		if (err)
 			return err;
 		ice_fdir_update_cntrs(hw, old_fltr->flow_type, false);
+		/* update sb-filters count, specific to ring->channel */
+		ice_update_per_q_fltr(vsi, old_fltr->orig_q_index, false);
 		if (!input && !hw->fdir_fltr_cnt[old_fltr->flow_type])
 			/* we just deleted the last filter of flow_type so we
 			 * should also delete the HW filter info.
@@ -1424,6 +1621,8 @@ ice_fdir_update_list_entry(struct ice_pf *pf, struct ice_fdir_fltr *input,
 	if (!input)
 		return err;
 	ice_fdir_list_add_fltr(hw, input);
+	/* update sb-filters count, specific to ring->channel */
+	ice_update_per_q_fltr(vsi, input->orig_q_index, true);
 	ice_fdir_update_cntrs(hw, input->flow_type, true);
 	return 0;
 }
@@ -1462,6 +1661,39 @@ int ice_del_fdir_ethtool(struct ice_vsi *vsi, struct ethtool_rxnfc *cmd)
 	return val;
 }
 
+/**
+ * ice_update_ring_dest_vsi - update dest ring and dest VSI
+ * @vsi: pointer to target VSI
+ * @dest_vsi: ptr to dest VSI index
+ * @ring: ptr to dest ring
+ *
+ * This function updates destination VSI and queue if user specifies
+ * target queue which falls in channel's (aka ADQ) queue region
+ */
+static void
+ice_update_ring_dest_vsi(struct ice_vsi *vsi, u16 *dest_vsi, u32 *ring)
+{
+	struct ice_channel *ch;
+
+	list_for_each_entry(ch, &vsi->ch_list, list) {
+		if (!ch->ch_vsi)
+			continue;
+
+		/* make sure to locate corresponding channel based on "queue"
+		 * specified
+		 */
+		if ((*ring < ch->base_q) ||
+		    (*ring >= (ch->base_q + ch->num_rxq)))
+			continue;
+
+		/* update the dest_vsi based on channel */
+		*dest_vsi = ch->ch_vsi->idx;
+
+		/* update the "ring" to be correct based on channel */
+		*ring -= ch->base_q;
+	}
+}
+
 /**
  * ice_set_fdir_input_set - Set the input set for Flow Director
  * @vsi: pointer to target VSI
@@ -1473,6 +1705,7 @@ ice_set_fdir_input_set(struct ice_vsi *vsi, struct ethtool_rx_flow_spec *fsp,
 		       struct ice_fdir_fltr *input)
 {
 	u16 dest_vsi, q_index = 0;
+	u16 orig_q_index = 0;
 	struct ice_pf *pf;
 	struct ice_hw *hw;
 	int flow_type;
@@ -1499,6 +1732,8 @@ ice_set_fdir_input_set(struct ice_vsi *vsi, struct ethtool_rx_flow_spec *fsp,
 		if (ring >= vsi->num_rxq)
 			return -EINVAL;
 
+		orig_q_index = ring;
+		ice_update_ring_dest_vsi(vsi, &dest_vsi, &ring);
 		dest_ctl = ICE_FLTR_PRGM_DESC_DEST_DIRECT_PKT_QINDEX;
 		q_index = ring;
 	}
@@ -1507,6 +1742,11 @@ ice_set_fdir_input_set(struct ice_vsi *vsi, struct ethtool_rx_flow_spec *fsp,
 	input->q_index = q_index;
 	flow_type = fsp->flow_type & ~FLOW_EXT;
 
+	/* Record the original queue index as specified by user.
+	 * with channel configuration 'q_index' becomes relative
+	 * to TC (channel).
+	 */
+	input->orig_q_index = orig_q_index;
 	input->dest_vsi = dest_vsi;
 	input->dest_ctl = dest_ctl;
 	input->fltr_status = ICE_FLTR_PRGM_DESC_FD_STATUS_FD_ID;
@@ -1694,6 +1934,8 @@ int ice_add_fdir_ethtool(struct ice_vsi *vsi, struct ethtool_rxnfc *cmd)
 
 remove_sw_rule:
 	ice_fdir_update_cntrs(hw, input->flow_type, false);
+	/* update sb-filters count, specific to ring->channel */
+	ice_update_per_q_fltr(vsi, input->orig_q_index, false);
 	list_del(&input->fltr_node);
 release_lock:
 	mutex_unlock(&hw->fdir_fltr_lock);
diff --git a/drivers/net/ethernet/intel/ice/ice_fdir.h b/drivers/net/ethernet/intel/ice/ice_fdir.h
index da4163856f4c..53ca2ee2e459 100644
--- a/drivers/net/ethernet/intel/ice/ice_fdir.h
+++ b/drivers/net/ethernet/intel/ice/ice_fdir.h
@@ -182,6 +182,7 @@ struct ice_fdir_fltr {
 
 	/* filter control */
 	u16 q_index;
+	u16 orig_q_index;
 	u16 dest_vsi;
 	u8 dest_ctl;
 	u8 cnt_ena;
diff --git a/drivers/net/ethernet/intel/ice/ice_flow.c b/drivers/net/ethernet/intel/ice/ice_flow.c
index 3dd0092bed61..e89262ce1ef1 100644
--- a/drivers/net/ethernet/intel/ice/ice_flow.c
+++ b/drivers/net/ethernet/intel/ice/ice_flow.c
@@ -1810,6 +1810,57 @@ ice_flow_add_fld_raw(struct ice_flow_seg_info *seg, u16 off, u8 len,
 	seg->raws_cnt++;
 }
 
+/**
+ * ice_flow_rem_vsi_prof - remove VSI from flow profile
+ * @hw: pointer to the hardware structure
+ * @vsi_handle: software VSI handle
+ * @prof_id: unique ID to identify this flow profile
+ *
+ * This function removes the flow entries associated to the input
+ * VSI handle and disassociate the VSI from the flow profile.
+ */
+int ice_flow_rem_vsi_prof(struct ice_hw *hw, u16 vsi_handle, u64 prof_id)
+{
+	struct ice_flow_prof *prof;
+	int status = 0;
+
+	if (!ice_is_vsi_valid(hw, vsi_handle))
+		return -EINVAL;
+
+	/* find flow profile pointer with input package block and profile ID */
+	prof = ice_flow_find_prof_id(hw, ICE_BLK_FD, prof_id);
+	if (!prof) {
+		ice_debug(hw, ICE_DBG_PKG, "Cannot find flow profile id=%llu\n",
+			  prof_id);
+		return -ENOENT;
+	}
+
+	/* Remove all remaining flow entries before removing the flow profile */
+	if (!list_empty(&prof->entries)) {
+		struct ice_flow_entry *e, *t;
+
+		mutex_lock(&prof->entries_lock);
+		list_for_each_entry_safe(e, t, &prof->entries, l_entry) {
+			if (e->vsi_handle != vsi_handle)
+				continue;
+
+			status = ice_flow_rem_entry_sync(hw, ICE_BLK_FD, e);
+			if (status)
+				break;
+		}
+		mutex_unlock(&prof->entries_lock);
+	}
+	if (status)
+		return status;
+
+	/* disassociate the flow profile from sw VSI handle */
+	status = ice_flow_disassoc_prof(hw, ICE_BLK_FD, prof, vsi_handle);
+	if (status)
+		ice_debug(hw, ICE_DBG_PKG, "ice_flow_disassoc_prof() failed with status=%d\n",
+			  status);
+	return status;
+}
+
 #define ICE_FLOW_RSS_SEG_HDR_L2_MASKS \
 	(ICE_FLOW_SEG_HDR_ETH | ICE_FLOW_SEG_HDR_VLAN)
 
diff --git a/drivers/net/ethernet/intel/ice/ice_flow.h b/drivers/net/ethernet/intel/ice/ice_flow.h
index 2a2d8c1536cb..9a7bc4fdb64e 100644
--- a/drivers/net/ethernet/intel/ice/ice_flow.h
+++ b/drivers/net/ethernet/intel/ice/ice_flow.h
@@ -401,6 +401,7 @@ ice_flow_set_fld(struct ice_flow_seg_info *seg, enum ice_flow_field fld,
 void
 ice_flow_add_fld_raw(struct ice_flow_seg_info *seg, u16 off, u8 len,
 		     u16 val_loc, u16 mask_loc);
+int ice_flow_rem_vsi_prof(struct ice_hw *hw, u16 vsi_handle, u64 prof_id);
 void ice_rem_vsi_rss_list(struct ice_hw *hw, u16 vsi_handle);
 enum ice_status ice_replay_rss_cfg(struct ice_hw *hw, u16 vsi_handle);
 enum ice_status
diff --git a/drivers/net/ethernet/intel/ice/ice_lib.c b/drivers/net/ethernet/intel/ice/ice_lib.c
index 68fc7a57568e..44931a14a85c 100644
--- a/drivers/net/ethernet/intel/ice/ice_lib.c
+++ b/drivers/net/ethernet/intel/ice/ice_lib.c
@@ -520,10 +520,16 @@ static int ice_alloc_fd_res(struct ice_vsi *vsi)
 	struct ice_pf *pf = vsi->back;
 	u32 g_val, b_val;
 
-	/* Flow Director filters are only allocated/assigned to the PF VSI which
-	 * passes the traffic. The CTRL VSI is only used to add/delete filters
-	 * so we don't allocate resources to it
+	/* Flow Director filters are only allocated/assigned to the PF VSI or
+	 * CHNL VSI which passes the traffic. The CTRL VSI is only used to
+	 * add/delete filters so resources are not allocated to it
 	 */
+	if (!test_bit(ICE_FLAG_FD_ENA, pf->flags))
+		return -EPERM;
+
+	if (!(vsi->type == ICE_VSI_PF || vsi->type == ICE_VSI_VF ||
+	      vsi->type == ICE_VSI_CHNL))
+		return -EPERM;
 
 	/* FD filters from guaranteed pool per VSI */
 	g_val = pf->hw.func_caps.fd_fltr_guar;
@@ -535,19 +541,56 @@ static int ice_alloc_fd_res(struct ice_vsi *vsi)
 	if (!b_val)
 		return -EPERM;
 
-	if (!(vsi->type == ICE_VSI_PF || vsi->type == ICE_VSI_VF))
-		return -EPERM;
+	/* PF main VSI gets only 64 FD resources from guaranteed pool
+	 * when ADQ is configured.
+	 */
+#define ICE_PF_VSI_GFLTR	64
 
-	if (!test_bit(ICE_FLAG_FD_ENA, pf->flags))
-		return -EPERM;
+	/* determine FD filter resources per VSI from shared(best effort) and
+	 * dedicated pool
+	 */
+	if (vsi->type == ICE_VSI_PF) {
+		vsi->num_gfltr = g_val;
+		/* if MQPRIO is configured, main VSI doesn't get all FD
+		 * resources from guaranteed pool. PF VSI gets 64 FD resources
+		 */
+		if (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {
+			if (g_val < ICE_PF_VSI_GFLTR)
+				return -EPERM;
+			/* allow bare minimum entries for PF VSI */
+			vsi->num_gfltr = ICE_PF_VSI_GFLTR;
+		}
+
+		/* each VSI gets same "best_effort" quota */
+		vsi->num_bfltr = b_val;
+	} else if (vsi->type == ICE_VSI_VF) {
+		vsi->num_gfltr = 0;
 
-	vsi->num_gfltr = g_val / pf->num_alloc_vsi;
+		/* each VSI gets same "best_effort" quota */
+		vsi->num_bfltr = b_val;
+	} else {
+		struct ice_vsi *main_vsi;
+		int numtc;
 
-	/* each VSI gets same "best_effort" quota */
-	vsi->num_bfltr = b_val;
+		main_vsi = ice_get_main_vsi(pf);
+		if (!main_vsi)
+			return -EPERM;
 
-	if (vsi->type == ICE_VSI_VF) {
-		vsi->num_gfltr = 0;
+		if (!main_vsi->all_numtc)
+			return -EINVAL;
+
+		/* figure out ADQ numtc */
+		numtc = main_vsi->all_numtc - ICE_CHNL_START_TC;
+
+		/* only one TC but still asking resources for channels,
+		 * invalid config
+		 */
+		if (numtc < ICE_CHNL_START_TC)
+			return -EPERM;
+
+		g_val -= ICE_PF_VSI_GFLTR;
+		/* channel VSIs gets equal share from guaranteed pool */
+		vsi->num_gfltr = g_val / numtc;
 
 		/* each VSI gets same "best_effort" quota */
 		vsi->num_bfltr = b_val;
@@ -886,7 +929,7 @@ static void ice_set_fd_vsi_ctx(struct ice_vsi_ctx *ctxt, struct ice_vsi *vsi)
 	u16 dflt_q, report_q, val;
 
 	if (vsi->type != ICE_VSI_PF && vsi->type != ICE_VSI_CTRL &&
-	    vsi->type != ICE_VSI_VF)
+	    vsi->type != ICE_VSI_VF && vsi->type != ICE_VSI_CHNL)
 		return;
 
 	val = ICE_AQ_VSI_PROP_FLOW_DIR_VALID;
* Unmerged path drivers/net/ethernet/intel/ice/ice_main.c
diff --git a/drivers/net/ethernet/intel/ice/ice_type.h b/drivers/net/ethernet/intel/ice/ice_type.h
index 081f40fa697c..f011faf4ac79 100644
--- a/drivers/net/ethernet/intel/ice/ice_type.h
+++ b/drivers/net/ethernet/intel/ice/ice_type.h
@@ -6,6 +6,7 @@
 
 #define ICE_BYTES_PER_WORD	2
 #define ICE_BYTES_PER_DWORD	4
+#define ICE_CHNL_MAX_TC		16
 
 #include "ice_status.h"
 #include "ice_hw_autogen.h"
@@ -228,8 +229,8 @@ enum ice_fd_hw_seg {
 	ICE_FD_HW_SEG_MAX,
 };
 
-/* 2 VSI = 1 ICE_VSI_PF + 1 ICE_VSI_CTRL */
-#define ICE_MAX_FDIR_VSI_PER_FILTER	2
+/* 1 ICE_VSI_PF + 1 ICE_VSI_CTRL + ICE_CHNL_MAX_TC */
+#define ICE_MAX_FDIR_VSI_PER_FILTER	(2 + ICE_CHNL_MAX_TC)
 
 struct ice_fd_hw_prof {
 	struct ice_flow_seg_info *fdir_seg[ICE_FD_HW_SEG_MAX];
