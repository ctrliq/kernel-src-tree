net/mlx5: Bridge, support LAG

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Vlad Buslov <vladbu@nvidia.com>
commit ff9b7521468bc2909293c1cda66a245a49688f6f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/ff9b7521.failed

Allow adding bond net devices to mlx5 bridge with following changes:

- Modify bridge representor code to obtain uplink represetor that belongs
to eswitch that is registered for notification. Require representor to be
in shared FDB mode. If representor is the lag master, then consider its
port as local, otherwise treat it as peer.

- Use devcom to match on paired eswitch metadata in peer FDB entries. This
is necessary for shared FDB LAG to function since packets are always
received on active eswitch instance as opposed to parent eswitch of port.

- Support for deleting peer flows when receiving
SWITCHDEV_FDB_DEL_TO_BRIDGE notification was implemented in one of previous
patches in series. Now also implement support for handling
SWITCHDEV_FDB_ADD_TO_BRIDGE which can be generated on peer by bridge update
workqueue task in LAG configuration. Refresh the flow 'lastuse' timestamp
to current jiffies when receiving such notification on eswitch that manages
the local FDB entry. This allows peer entries to prevent ageing of the FDB.

	Signed-off-by: Vlad Buslov <vladbu@nvidia.com>
	Reviewed-by: Roi Dayan <roid@nvidia.com>
	Reviewed-by: Mark Bloch <mbloch@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit ff9b7521468bc2909293c1cda66a245a49688f6f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
#	drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
#	drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
index de7a68488a9d,0c38c2e319be..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
@@@ -8,6 -9,122 +8,125 @@@
  #include "esw/bridge.h"
  #include "en_rep.h"
  
++<<<<<<< HEAD
++=======
+ #define MLX5_ESW_BRIDGE_UPDATE_INTERVAL 1000
+ 
+ struct mlx5_bridge_switchdev_fdb_work {
+ 	struct work_struct work;
+ 	struct switchdev_notifier_fdb_info fdb_info;
+ 	struct net_device *dev;
+ 	struct mlx5_esw_bridge_offloads *br_offloads;
+ 	bool add;
+ };
+ 
+ static bool mlx5_esw_bridge_dev_same_esw(struct net_device *dev, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 
+ 	return esw == priv->mdev->priv.eswitch;
+ }
+ 
+ static bool mlx5_esw_bridge_dev_same_hw(struct net_device *dev, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	struct mlx5_core_dev *mdev, *esw_mdev;
+ 	u64 system_guid, esw_system_guid;
+ 
+ 	mdev = priv->mdev;
+ 	esw_mdev = esw->dev;
+ 
+ 	system_guid = mlx5_query_nic_system_image_guid(mdev);
+ 	esw_system_guid = mlx5_query_nic_system_image_guid(esw_mdev);
+ 
+ 	return system_guid == esw_system_guid;
+ }
+ 
+ static struct net_device *
+ mlx5_esw_bridge_lag_rep_get(struct net_device *dev, struct mlx5_eswitch *esw)
+ {
+ 	struct net_device *lower;
+ 	struct list_head *iter;
+ 
+ 	netdev_for_each_lower_dev(dev, lower, iter) {
+ 		struct mlx5_core_dev *mdev;
+ 		struct mlx5e_priv *priv;
+ 
+ 		if (!mlx5e_eswitch_rep(lower))
+ 			continue;
+ 
+ 		priv = netdev_priv(lower);
+ 		mdev = priv->mdev;
+ 		if (mlx5_lag_is_shared_fdb(mdev) && mlx5_esw_bridge_dev_same_esw(lower, esw))
+ 			return lower;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static struct net_device *
+ mlx5_esw_bridge_rep_vport_num_vhca_id_get(struct net_device *dev, struct mlx5_eswitch *esw,
+ 					  u16 *vport_num, u16 *esw_owner_vhca_id)
+ {
+ 	struct mlx5e_rep_priv *rpriv;
+ 	struct mlx5e_priv *priv;
+ 
+ 	if (netif_is_lag_master(dev))
+ 		dev = mlx5_esw_bridge_lag_rep_get(dev, esw);
+ 
+ 	if (!dev || !mlx5e_eswitch_rep(dev) || !mlx5_esw_bridge_dev_same_hw(dev, esw))
+ 		return NULL;
+ 
+ 	priv = netdev_priv(dev);
+ 	rpriv = priv->ppriv;
+ 	*vport_num = rpriv->rep->vport;
+ 	*esw_owner_vhca_id = MLX5_CAP_GEN(priv->mdev, vhca_id);
+ 	return dev;
+ }
+ 
+ static struct net_device *
+ mlx5_esw_bridge_lower_rep_vport_num_vhca_id_get(struct net_device *dev, struct mlx5_eswitch *esw,
+ 						u16 *vport_num, u16 *esw_owner_vhca_id)
+ {
+ 	struct net_device *lower_dev;
+ 	struct list_head *iter;
+ 
+ 	if (netif_is_lag_master(dev) || mlx5e_eswitch_rep(dev))
+ 		return mlx5_esw_bridge_rep_vport_num_vhca_id_get(dev, esw, vport_num,
+ 								 esw_owner_vhca_id);
+ 
+ 	netdev_for_each_lower_dev(dev, lower_dev, iter) {
+ 		struct net_device *rep;
+ 
+ 		if (netif_is_bridge_master(lower_dev))
+ 			continue;
+ 
+ 		rep = mlx5_esw_bridge_lower_rep_vport_num_vhca_id_get(lower_dev, esw, vport_num,
+ 								      esw_owner_vhca_id);
+ 		if (rep)
+ 			return rep;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+ static bool mlx5_esw_bridge_is_local(struct net_device *dev, struct net_device *rep,
+ 				     struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_core_dev *mdev;
+ 	struct mlx5e_priv *priv;
+ 
+ 	if (!mlx5_esw_bridge_dev_same_esw(rep, esw))
+ 		return false;
+ 
+ 	priv = netdev_priv(rep);
+ 	mdev = priv->mdev;
+ 	if (netif_is_lag_master(dev))
+ 		return mlx5_lag_is_shared_fdb(mdev) && mlx5_lag_is_master(mdev);
+ 	return true;
+ }
+ 
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  static int mlx5_esw_bridge_port_changeupper(struct notifier_block *nb, void *ptr)
  {
  	struct mlx5_esw_bridge_offloads *br_offloads = container_of(nb,
@@@ -15,37 -132,36 +134,65 @@@
  								    netdev_nb);
  	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
  	struct netdev_notifier_changeupper_info *info = ptr;
++<<<<<<< HEAD
++=======
+ 	struct net_device *upper = info->upper_dev, *rep;
+ 	struct mlx5_eswitch *esw = br_offloads->esw;
+ 	u16 vport_num, esw_owner_vhca_id;
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  	struct netlink_ext_ack *extack;
 -	int ifindex = upper->ifindex;
 -	int err;
 +	struct mlx5e_rep_priv *rpriv;
 +	struct mlx5_eswitch *esw;
 +	struct mlx5_vport *vport;
 +	struct net_device *upper;
 +	struct mlx5e_priv *priv;
 +	u16 vport_num;
 +
 +	if (!mlx5e_eswitch_rep(dev))
 +		return 0;
  
 +	upper = info->upper_dev;
  	if (!netif_is_bridge_master(upper))
  		return 0;
  
++<<<<<<< HEAD
 +	esw = br_offloads->esw;
 +	priv = netdev_priv(dev);
 +	if (esw != priv->mdev->priv.eswitch)
++=======
+ 	rep = mlx5_esw_bridge_rep_vport_num_vhca_id_get(dev, esw, &vport_num, &esw_owner_vhca_id);
+ 	if (!rep)
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  		return 0;
  
 +	rpriv = priv->ppriv;
 +	vport_num = rpriv->rep->vport;
 +	vport = mlx5_eswitch_get_vport(esw, vport_num);
 +	if (IS_ERR(vport))
 +		return PTR_ERR(vport);
 +
  	extack = netdev_notifier_info_to_extack(&info->info);
  
++<<<<<<< HEAD
 +	return info->linking ?
 +		mlx5_esw_bridge_vport_link(upper->ifindex, br_offloads, vport, extack) :
 +		mlx5_esw_bridge_vport_unlink(upper->ifindex, br_offloads, vport, extack);
++=======
+ 	if (mlx5_esw_bridge_is_local(dev, rep, esw))
+ 		err = info->linking ?
+ 			mlx5_esw_bridge_vport_link(ifindex, vport_num, esw_owner_vhca_id,
+ 						   br_offloads, extack) :
+ 			mlx5_esw_bridge_vport_unlink(ifindex, vport_num, esw_owner_vhca_id,
+ 						     br_offloads, extack);
+ 	else if (mlx5_esw_bridge_dev_same_hw(rep, esw))
+ 		err = info->linking ?
+ 			mlx5_esw_bridge_vport_peer_link(ifindex, vport_num, esw_owner_vhca_id,
+ 							br_offloads, extack) :
+ 			mlx5_esw_bridge_vport_peer_unlink(ifindex, vport_num, esw_owner_vhca_id,
+ 							  br_offloads, extack);
+ 
+ 	return err;
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  }
  
  static int mlx5_esw_bridge_switchdev_port_event(struct notifier_block *nb,
@@@ -65,6 -181,279 +212,282 @@@
  	return notifier_from_errno(err);
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ mlx5_esw_bridge_port_obj_add(struct net_device *dev,
+ 			     struct switchdev_notifier_port_obj_info *port_obj_info,
+ 			     struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct netlink_ext_ack *extack = switchdev_notifier_info_to_extack(&port_obj_info->info);
+ 	const struct switchdev_obj *obj = port_obj_info->obj;
+ 	const struct switchdev_obj_port_vlan *vlan;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 	int err;
+ 
+ 	if (!mlx5_esw_bridge_rep_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 						       &esw_owner_vhca_id))
+ 		return 0;
+ 
+ 	port_obj_info->handled = true;
+ 
+ 	switch (obj->id) {
+ 	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+ 		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+ 		err = mlx5_esw_bridge_port_vlan_add(vport_num, esw_owner_vhca_id, vlan->vid,
+ 						    vlan->flags, br_offloads, extack);
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return err;
+ }
+ 
+ static int
+ mlx5_esw_bridge_port_obj_del(struct net_device *dev,
+ 			     struct switchdev_notifier_port_obj_info *port_obj_info,
+ 			     struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	const struct switchdev_obj *obj = port_obj_info->obj;
+ 	const struct switchdev_obj_port_vlan *vlan;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 
+ 	if (!mlx5_esw_bridge_rep_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 						       &esw_owner_vhca_id))
+ 		return 0;
+ 
+ 	port_obj_info->handled = true;
+ 
+ 	switch (obj->id) {
+ 	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+ 		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+ 		mlx5_esw_bridge_port_vlan_del(vport_num, esw_owner_vhca_id, vlan->vid, br_offloads);
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ mlx5_esw_bridge_port_obj_attr_set(struct net_device *dev,
+ 				  struct switchdev_notifier_port_attr_info *port_attr_info,
+ 				  struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct netlink_ext_ack *extack = switchdev_notifier_info_to_extack(&port_attr_info->info);
+ 	const struct switchdev_attr *attr = port_attr_info->attr;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 	int err;
+ 
+ 	if (!mlx5_esw_bridge_lower_rep_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 							     &esw_owner_vhca_id))
+ 		return 0;
+ 
+ 	port_attr_info->handled = true;
+ 
+ 	switch (attr->id) {
+ 	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
+ 		if (attr->u.brport_flags.mask & ~(BR_LEARNING | BR_FLOOD | BR_MCAST_FLOOD)) {
+ 			NL_SET_ERR_MSG_MOD(extack, "Flag is not supported");
+ 			err = -EINVAL;
+ 		}
+ 		break;
+ 	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+ 		break;
+ 	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+ 		err = mlx5_esw_bridge_ageing_time_set(vport_num, esw_owner_vhca_id,
+ 						      attr->u.ageing_time, br_offloads);
+ 		break;
+ 	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+ 		err = mlx5_esw_bridge_vlan_filtering_set(vport_num, esw_owner_vhca_id,
+ 							 attr->u.vlan_filtering, br_offloads);
+ 		break;
+ 	default:
+ 		err = -EOPNOTSUPP;
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int mlx5_esw_bridge_event_blocking(struct notifier_block *nb,
+ 					  unsigned long event, void *ptr)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = container_of(nb,
+ 								    struct mlx5_esw_bridge_offloads,
+ 								    nb_blk);
+ 	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+ 	int err;
+ 
+ 	switch (event) {
+ 	case SWITCHDEV_PORT_OBJ_ADD:
+ 		err = mlx5_esw_bridge_port_obj_add(dev, ptr, br_offloads);
+ 		break;
+ 	case SWITCHDEV_PORT_OBJ_DEL:
+ 		err = mlx5_esw_bridge_port_obj_del(dev, ptr, br_offloads);
+ 		break;
+ 	case SWITCHDEV_PORT_ATTR_SET:
+ 		err = mlx5_esw_bridge_port_obj_attr_set(dev, ptr, br_offloads);
+ 		break;
+ 	default:
+ 		err = 0;
+ 	}
+ 
+ 	return notifier_from_errno(err);
+ }
+ 
+ static void
+ mlx5_esw_bridge_cleanup_switchdev_fdb_work(struct mlx5_bridge_switchdev_fdb_work *fdb_work)
+ {
+ 	dev_put(fdb_work->dev);
+ 	kfree(fdb_work->fdb_info.addr);
+ 	kfree(fdb_work);
+ }
+ 
+ static void mlx5_esw_bridge_switchdev_fdb_event_work(struct work_struct *work)
+ {
+ 	struct mlx5_bridge_switchdev_fdb_work *fdb_work =
+ 		container_of(work, struct mlx5_bridge_switchdev_fdb_work, work);
+ 	struct switchdev_notifier_fdb_info *fdb_info =
+ 		&fdb_work->fdb_info;
+ 	struct mlx5_esw_bridge_offloads *br_offloads =
+ 		fdb_work->br_offloads;
+ 	struct net_device *dev = fdb_work->dev;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 
+ 	rtnl_lock();
+ 
+ 	if (!mlx5_esw_bridge_rep_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 						       &esw_owner_vhca_id))
+ 		goto out;
+ 
+ 	if (fdb_work->add)
+ 		mlx5_esw_bridge_fdb_create(dev, vport_num, esw_owner_vhca_id, br_offloads,
+ 					   fdb_info);
+ 	else
+ 		mlx5_esw_bridge_fdb_remove(dev, vport_num, esw_owner_vhca_id, br_offloads,
+ 					   fdb_info);
+ 
+ out:
+ 	rtnl_unlock();
+ 	mlx5_esw_bridge_cleanup_switchdev_fdb_work(fdb_work);
+ }
+ 
+ static struct mlx5_bridge_switchdev_fdb_work *
+ mlx5_esw_bridge_init_switchdev_fdb_work(struct net_device *dev, bool add,
+ 					struct switchdev_notifier_fdb_info *fdb_info,
+ 					struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_bridge_switchdev_fdb_work *work;
+ 	u8 *addr;
+ 
+ 	work = kzalloc(sizeof(*work), GFP_ATOMIC);
+ 	if (!work)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	INIT_WORK(&work->work, mlx5_esw_bridge_switchdev_fdb_event_work);
+ 	memcpy(&work->fdb_info, fdb_info, sizeof(work->fdb_info));
+ 
+ 	addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+ 	if (!addr) {
+ 		kfree(work);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 	ether_addr_copy(addr, fdb_info->addr);
+ 	work->fdb_info.addr = addr;
+ 
+ 	dev_hold(dev);
+ 	work->dev = dev;
+ 	work->br_offloads = br_offloads;
+ 	work->add = add;
+ 	return work;
+ }
+ 
+ static int mlx5_esw_bridge_switchdev_event(struct notifier_block *nb,
+ 					   unsigned long event, void *ptr)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = container_of(nb,
+ 								    struct mlx5_esw_bridge_offloads,
+ 								    nb);
+ 	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+ 	struct switchdev_notifier_fdb_info *fdb_info;
+ 	struct mlx5_bridge_switchdev_fdb_work *work;
+ 	struct mlx5_eswitch *esw = br_offloads->esw;
+ 	struct switchdev_notifier_info *info = ptr;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 	struct net_device *upper, *rep;
+ 
+ 	if (event == SWITCHDEV_PORT_ATTR_SET) {
+ 		int err = mlx5_esw_bridge_port_obj_attr_set(dev, ptr, br_offloads);
+ 
+ 		return notifier_from_errno(err);
+ 	}
+ 
+ 	upper = netdev_master_upper_dev_get_rcu(dev);
+ 	if (!upper)
+ 		return NOTIFY_DONE;
+ 	if (!netif_is_bridge_master(upper))
+ 		return NOTIFY_DONE;
+ 
+ 	rep = mlx5_esw_bridge_rep_vport_num_vhca_id_get(dev, esw, &vport_num, &esw_owner_vhca_id);
+ 	if (!rep)
+ 		return NOTIFY_DONE;
+ 
+ 	switch (event) {
+ 	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+ 		/* only handle the event on native eswtich of representor */
+ 		if (!mlx5_esw_bridge_is_local(dev, rep, esw))
+ 			break;
+ 
+ 		fdb_info = container_of(info,
+ 					struct switchdev_notifier_fdb_info,
+ 					info);
+ 		mlx5_esw_bridge_fdb_update_used(dev, vport_num, esw_owner_vhca_id, br_offloads,
+ 						fdb_info);
+ 		break;
+ 	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+ 		/* only handle the event on peers */
+ 		if (mlx5_esw_bridge_is_local(dev, rep, esw))
+ 			break;
+ 		fallthrough;
+ 	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+ 	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+ 		fdb_info = container_of(info,
+ 					struct switchdev_notifier_fdb_info,
+ 					info);
+ 
+ 		work = mlx5_esw_bridge_init_switchdev_fdb_work(dev,
+ 							       event == SWITCHDEV_FDB_ADD_TO_DEVICE,
+ 							       fdb_info,
+ 							       br_offloads);
+ 		if (IS_ERR(work)) {
+ 			WARN_ONCE(1, "Failed to init switchdev work, err=%ld",
+ 				  PTR_ERR(work));
+ 			return notifier_from_errno(PTR_ERR(work));
+ 		}
+ 
+ 		queue_work(br_offloads->wq, &work->work);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	return NOTIFY_DONE;
+ }
+ 
+ static void mlx5_esw_bridge_update_work(struct work_struct *work)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = container_of(work,
+ 								    struct mlx5_esw_bridge_offloads,
+ 								    update_work.work);
+ 
+ 	rtnl_lock();
+ 	mlx5_esw_bridge_update(br_offloads);
+ 	rtnl_unlock();
+ 
+ 	queue_delayed_work(br_offloads->wq, &br_offloads->update_work,
+ 			   msecs_to_jiffies(MLX5_ESW_BRIDGE_UPDATE_INTERVAL));
+ }
+ 
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  void mlx5e_rep_bridge_init(struct mlx5e_priv *priv)
  {
  	struct mlx5_esw_bridge_offloads *br_offloads;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
index b503562f97d0,7e221038df8d..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
@@@ -1,15 -1,26 +1,16 @@@
  // SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB
  /* Copyright (c) 2021 Mellanox Technologies. */
  
 +#include <linux/netdevice.h>
  #include <linux/list.h>
 -#include <linux/notifier.h>
 -#include <net/netevent.h>
  #include <net/switchdev.h>
+ #include "lib/devcom.h"
  #include "bridge.h"
  #include "eswitch.h"
 -#include "bridge_priv.h"
 -#define CREATE_TRACE_POINTS
 -#include "diag/bridge_tracepoint.h"
 +#include "fs_core.h"
  
  #define MLX5_ESW_BRIDGE_INGRESS_TABLE_SIZE 64000
 -#define MLX5_ESW_BRIDGE_INGRESS_TABLE_VLAN_GRP_IDX_FROM 0
 -#define MLX5_ESW_BRIDGE_INGRESS_TABLE_VLAN_GRP_IDX_TO (MLX5_ESW_BRIDGE_INGRESS_TABLE_SIZE / 4 - 1)
 -#define MLX5_ESW_BRIDGE_INGRESS_TABLE_FILTER_GRP_IDX_FROM \
 -	(MLX5_ESW_BRIDGE_INGRESS_TABLE_VLAN_GRP_IDX_TO + 1)
 -#define MLX5_ESW_BRIDGE_INGRESS_TABLE_FILTER_GRP_IDX_TO \
 -	(MLX5_ESW_BRIDGE_INGRESS_TABLE_SIZE / 2 - 1)
 -#define MLX5_ESW_BRIDGE_INGRESS_TABLE_MAC_GRP_IDX_FROM \
 -	(MLX5_ESW_BRIDGE_INGRESS_TABLE_FILTER_GRP_IDX_TO + 1)
 +#define MLX5_ESW_BRIDGE_INGRESS_TABLE_MAC_GRP_IDX_FROM 0
  #define MLX5_ESW_BRIDGE_INGRESS_TABLE_MAC_GRP_IDX_TO (MLX5_ESW_BRIDGE_INGRESS_TABLE_SIZE - 1)
  
  #define MLX5_ESW_BRIDGE_EGRESS_TABLE_SIZE 64000
@@@ -194,6 -407,198 +195,200 @@@ mlx5_esw_bridge_egress_table_cleanup(st
  	mlx5_destroy_flow_table(bridge->egress_ft);
  }
  
++<<<<<<< HEAD
++=======
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_ingress_flow_with_esw_create(u16 vport_num, const unsigned char *addr,
+ 					     struct mlx5_esw_bridge_vlan *vlan, u32 counter_id,
+ 					     struct mlx5_esw_bridge *bridge,
+ 					     struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = bridge->br_offloads;
+ 	struct mlx5_flow_act flow_act = {
+ 		.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST | MLX5_FLOW_CONTEXT_ACTION_COUNT,
+ 		.flags = FLOW_ACT_NO_APPEND,
+ 	};
+ 	struct mlx5_flow_destination dests[2] = {};
+ 	struct mlx5_flow_spec *rule_spec;
+ 	struct mlx5_flow_handle *handle;
+ 	u8 *smac_v, *smac_c;
+ 
+ 	rule_spec = kvzalloc(sizeof(*rule_spec), GFP_KERNEL);
+ 	if (!rule_spec)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	rule_spec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS | MLX5_MATCH_MISC_PARAMETERS_2;
+ 
+ 	smac_v = MLX5_ADDR_OF(fte_match_param, rule_spec->match_value,
+ 			      outer_headers.smac_47_16);
+ 	ether_addr_copy(smac_v, addr);
+ 	smac_c = MLX5_ADDR_OF(fte_match_param, rule_spec->match_criteria,
+ 			      outer_headers.smac_47_16);
+ 	eth_broadcast_addr(smac_c);
+ 
+ 	MLX5_SET(fte_match_param, rule_spec->match_criteria,
+ 		 misc_parameters_2.metadata_reg_c_0, mlx5_eswitch_get_vport_metadata_mask());
+ 	MLX5_SET(fte_match_param, rule_spec->match_value, misc_parameters_2.metadata_reg_c_0,
+ 		 mlx5_eswitch_get_vport_metadata_for_match(esw, vport_num));
+ 
+ 	if (vlan && vlan->pkt_reformat_push) {
+ 		flow_act.action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;
+ 		flow_act.pkt_reformat = vlan->pkt_reformat_push;
+ 	} else if (vlan) {
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_value,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.first_vid);
+ 		MLX5_SET(fte_match_param, rule_spec->match_value, outer_headers.first_vid,
+ 			 vlan->vid);
+ 	}
+ 
+ 	dests[0].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
+ 	dests[0].ft = bridge->egress_ft;
+ 	dests[1].type = MLX5_FLOW_DESTINATION_TYPE_COUNTER;
+ 	dests[1].counter_id = counter_id;
+ 
+ 	handle = mlx5_add_flow_rules(br_offloads->ingress_ft, rule_spec, &flow_act, dests,
+ 				     ARRAY_SIZE(dests));
+ 
+ 	kvfree(rule_spec);
+ 	return handle;
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_ingress_flow_create(u16 vport_num, const unsigned char *addr,
+ 				    struct mlx5_esw_bridge_vlan *vlan, u32 counter_id,
+ 				    struct mlx5_esw_bridge *bridge)
+ {
+ 	return mlx5_esw_bridge_ingress_flow_with_esw_create(vport_num, addr, vlan, counter_id,
+ 							    bridge, bridge->br_offloads->esw);
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_ingress_flow_peer_create(u16 vport_num, const unsigned char *addr,
+ 					 struct mlx5_esw_bridge_vlan *vlan, u32 counter_id,
+ 					 struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_devcom *devcom = bridge->br_offloads->esw->dev->priv.devcom;
+ 	static struct mlx5_flow_handle *handle;
+ 	struct mlx5_eswitch *peer_esw;
+ 
+ 	peer_esw = mlx5_devcom_get_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+ 	if (!peer_esw)
+ 		return ERR_PTR(-ENODEV);
+ 
+ 	handle = mlx5_esw_bridge_ingress_flow_with_esw_create(vport_num, addr, vlan, counter_id,
+ 							      bridge, peer_esw);
+ 
+ 	mlx5_devcom_release_peer_data(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
+ 	return handle;
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_ingress_filter_flow_create(u16 vport_num, const unsigned char *addr,
+ 					   struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = bridge->br_offloads;
+ 	struct mlx5_flow_destination dest = {
+ 		.type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE,
+ 		.ft = br_offloads->skip_ft,
+ 	};
+ 	struct mlx5_flow_act flow_act = {
+ 		.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST,
+ 		.flags = FLOW_ACT_NO_APPEND,
+ 	};
+ 	struct mlx5_flow_spec *rule_spec;
+ 	struct mlx5_flow_handle *handle;
+ 	u8 *smac_v, *smac_c;
+ 
+ 	rule_spec = kvzalloc(sizeof(*rule_spec), GFP_KERNEL);
+ 	if (!rule_spec)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	rule_spec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS | MLX5_MATCH_MISC_PARAMETERS_2;
+ 
+ 	smac_v = MLX5_ADDR_OF(fte_match_param, rule_spec->match_value,
+ 			      outer_headers.smac_47_16);
+ 	ether_addr_copy(smac_v, addr);
+ 	smac_c = MLX5_ADDR_OF(fte_match_param, rule_spec->match_criteria,
+ 			      outer_headers.smac_47_16);
+ 	eth_broadcast_addr(smac_c);
+ 
+ 	MLX5_SET(fte_match_param, rule_spec->match_criteria,
+ 		 misc_parameters_2.metadata_reg_c_0, mlx5_eswitch_get_vport_metadata_mask());
+ 	MLX5_SET(fte_match_param, rule_spec->match_value, misc_parameters_2.metadata_reg_c_0,
+ 		 mlx5_eswitch_get_vport_metadata_for_match(br_offloads->esw, vport_num));
+ 
+ 	MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 			 outer_headers.cvlan_tag);
+ 	MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_value,
+ 			 outer_headers.cvlan_tag);
+ 
+ 	handle = mlx5_add_flow_rules(br_offloads->ingress_ft, rule_spec, &flow_act, &dest, 1);
+ 
+ 	kvfree(rule_spec);
+ 	return handle;
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_egress_flow_create(u16 vport_num, u16 esw_owner_vhca_id, const unsigned char *addr,
+ 				   struct mlx5_esw_bridge_vlan *vlan,
+ 				   struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_flow_destination dest = {
+ 		.type = MLX5_FLOW_DESTINATION_TYPE_VPORT,
+ 		.vport.num = vport_num,
+ 	};
+ 	struct mlx5_flow_act flow_act = {
+ 		.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST,
+ 		.flags = FLOW_ACT_NO_APPEND,
+ 	};
+ 	struct mlx5_flow_spec *rule_spec;
+ 	struct mlx5_flow_handle *handle;
+ 	u8 *dmac_v, *dmac_c;
+ 
+ 	rule_spec = kvzalloc(sizeof(*rule_spec), GFP_KERNEL);
+ 	if (!rule_spec)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	rule_spec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS;
+ 
+ 	dmac_v = MLX5_ADDR_OF(fte_match_param, rule_spec->match_value,
+ 			      outer_headers.dmac_47_16);
+ 	ether_addr_copy(dmac_v, addr);
+ 	dmac_c = MLX5_ADDR_OF(fte_match_param, rule_spec->match_criteria,
+ 			      outer_headers.dmac_47_16);
+ 	eth_broadcast_addr(dmac_c);
+ 
+ 	if (vlan) {
+ 		if (vlan->pkt_reformat_pop) {
+ 			flow_act.action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;
+ 			flow_act.pkt_reformat = vlan->pkt_reformat_pop;
+ 		}
+ 
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_value,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.first_vid);
+ 		MLX5_SET(fte_match_param, rule_spec->match_value, outer_headers.first_vid,
+ 			 vlan->vid);
+ 	}
+ 
+ 	if (MLX5_CAP_ESW(bridge->br_offloads->esw->dev, merged_eswitch)) {
+ 		dest.vport.flags = MLX5_FLOW_DEST_VPORT_VHCA_ID;
+ 		dest.vport.vhca_id = esw_owner_vhca_id;
+ 	}
+ 	handle = mlx5_add_flow_rules(bridge->egress_ft, rule_spec, &flow_act, &dest, 1);
+ 
+ 	kvfree(rule_spec);
+ 	return handle;
+ }
+ 
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  static struct mlx5_esw_bridge *mlx5_esw_bridge_create(int ifindex,
  						      struct mlx5_esw_bridge_offloads *br_offloads)
  {
@@@ -265,18 -680,464 +460,461 @@@ mlx5_esw_bridge_lookup(int ifindex, str
  	return bridge;
  }
  
 -static unsigned long mlx5_esw_bridge_port_key_from_data(u16 vport_num, u16 esw_owner_vhca_id)
 +static int mlx5_esw_bridge_vport_init(struct mlx5_esw_bridge *bridge,
 +				      struct mlx5_vport *vport)
  {
++<<<<<<< HEAD
 +	vport->bridge = bridge;
 +	return 0;
 +}
 +
++=======
+ 	return vport_num | (unsigned long)esw_owner_vhca_id << sizeof(vport_num) * BITS_PER_BYTE;
+ }
+ 
+ static unsigned long mlx5_esw_bridge_port_key(struct mlx5_esw_bridge_port *port)
+ {
+ 	return mlx5_esw_bridge_port_key_from_data(port->vport_num, port->esw_owner_vhca_id);
+ }
+ 
+ static int mlx5_esw_bridge_port_insert(struct mlx5_esw_bridge_port *port,
+ 				       struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	return xa_insert(&br_offloads->ports, mlx5_esw_bridge_port_key(port), port, GFP_KERNEL);
+ }
+ 
+ static struct mlx5_esw_bridge_port *
+ mlx5_esw_bridge_port_lookup(u16 vport_num, u16 esw_owner_vhca_id,
+ 			    struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	return xa_load(&br_offloads->ports, mlx5_esw_bridge_port_key_from_data(vport_num,
+ 									       esw_owner_vhca_id));
+ }
+ 
+ static void mlx5_esw_bridge_port_erase(struct mlx5_esw_bridge_port *port,
+ 				       struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	xa_erase(&br_offloads->ports, mlx5_esw_bridge_port_key(port));
+ }
+ 
+ static void mlx5_esw_bridge_fdb_entry_refresh(struct mlx5_esw_bridge_fdb_entry *entry)
+ {
+ 	trace_mlx5_esw_bridge_fdb_entry_refresh(entry);
+ 
+ 	mlx5_esw_bridge_fdb_offload_notify(entry->dev, entry->key.addr,
+ 					   entry->key.vid,
+ 					   SWITCHDEV_FDB_ADD_TO_BRIDGE);
+ }
+ 
+ static void
+ mlx5_esw_bridge_fdb_entry_cleanup(struct mlx5_esw_bridge_fdb_entry *entry,
+ 				  struct mlx5_esw_bridge *bridge)
+ {
+ 	trace_mlx5_esw_bridge_fdb_entry_cleanup(entry);
+ 
+ 	rhashtable_remove_fast(&bridge->fdb_ht, &entry->ht_node, fdb_ht_params);
+ 	mlx5_del_flow_rules(entry->egress_handle);
+ 	if (entry->filter_handle)
+ 		mlx5_del_flow_rules(entry->filter_handle);
+ 	mlx5_del_flow_rules(entry->ingress_handle);
+ 	mlx5_fc_destroy(bridge->br_offloads->esw->dev, entry->ingress_counter);
+ 	list_del(&entry->vlan_list);
+ 	list_del(&entry->list);
+ 	kvfree(entry);
+ }
+ 
+ static void mlx5_esw_bridge_fdb_flush(struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry, *tmp;
+ 
+ 	list_for_each_entry_safe(entry, tmp, &bridge->fdb_list, list) {
+ 		mlx5_esw_bridge_fdb_del_notify(entry);
+ 		mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
+ 	}
+ }
+ 
+ static struct mlx5_esw_bridge_vlan *
+ mlx5_esw_bridge_vlan_lookup(u16 vid, struct mlx5_esw_bridge_port *port)
+ {
+ 	return xa_load(&port->vlans, vid);
+ }
+ 
+ static int
+ mlx5_esw_bridge_vlan_push_create(struct mlx5_esw_bridge_vlan *vlan, struct mlx5_eswitch *esw)
+ {
+ 	struct {
+ 		__be16	h_vlan_proto;
+ 		__be16	h_vlan_TCI;
+ 	} vlan_hdr = { htons(ETH_P_8021Q), htons(vlan->vid) };
+ 	struct mlx5_pkt_reformat_params reformat_params = {};
+ 	struct mlx5_pkt_reformat *pkt_reformat;
+ 
+ 	if (!BIT(MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, reformat_insert)) ||
+ 	    MLX5_CAP_GEN_2(esw->dev, max_reformat_insert_size) < sizeof(vlan_hdr) ||
+ 	    MLX5_CAP_GEN_2(esw->dev, max_reformat_insert_offset) <
+ 	    offsetof(struct vlan_ethhdr, h_vlan_proto)) {
+ 		esw_warn(esw->dev, "Packet reformat INSERT_HEADER is not supported\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	reformat_params.type = MLX5_REFORMAT_TYPE_INSERT_HDR;
+ 	reformat_params.param_0 = MLX5_REFORMAT_CONTEXT_ANCHOR_MAC_START;
+ 	reformat_params.param_1 = offsetof(struct vlan_ethhdr, h_vlan_proto);
+ 	reformat_params.size = sizeof(vlan_hdr);
+ 	reformat_params.data = &vlan_hdr;
+ 	pkt_reformat = mlx5_packet_reformat_alloc(esw->dev,
+ 						  &reformat_params,
+ 						  MLX5_FLOW_NAMESPACE_FDB);
+ 	if (IS_ERR(pkt_reformat)) {
+ 		esw_warn(esw->dev, "Failed to alloc packet reformat INSERT_HEADER (err=%ld)\n",
+ 			 PTR_ERR(pkt_reformat));
+ 		return PTR_ERR(pkt_reformat);
+ 	}
+ 
+ 	vlan->pkt_reformat_push = pkt_reformat;
+ 	return 0;
+ }
+ 
+ static void
+ mlx5_esw_bridge_vlan_push_cleanup(struct mlx5_esw_bridge_vlan *vlan, struct mlx5_eswitch *esw)
+ {
+ 	mlx5_packet_reformat_dealloc(esw->dev, vlan->pkt_reformat_push);
+ 	vlan->pkt_reformat_push = NULL;
+ }
+ 
+ static int
+ mlx5_esw_bridge_vlan_pop_create(struct mlx5_esw_bridge_vlan *vlan, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_pkt_reformat_params reformat_params = {};
+ 	struct mlx5_pkt_reformat *pkt_reformat;
+ 
+ 	if (!BIT(MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, reformat_remove)) ||
+ 	    MLX5_CAP_GEN_2(esw->dev, max_reformat_remove_size) < sizeof(struct vlan_hdr) ||
+ 	    MLX5_CAP_GEN_2(esw->dev, max_reformat_remove_offset) <
+ 	    offsetof(struct vlan_ethhdr, h_vlan_proto)) {
+ 		esw_warn(esw->dev, "Packet reformat REMOVE_HEADER is not supported\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	reformat_params.type = MLX5_REFORMAT_TYPE_REMOVE_HDR;
+ 	reformat_params.param_0 = MLX5_REFORMAT_CONTEXT_ANCHOR_MAC_START;
+ 	reformat_params.param_1 = offsetof(struct vlan_ethhdr, h_vlan_proto);
+ 	reformat_params.size = sizeof(struct vlan_hdr);
+ 	pkt_reformat = mlx5_packet_reformat_alloc(esw->dev,
+ 						  &reformat_params,
+ 						  MLX5_FLOW_NAMESPACE_FDB);
+ 	if (IS_ERR(pkt_reformat)) {
+ 		esw_warn(esw->dev, "Failed to alloc packet reformat REMOVE_HEADER (err=%ld)\n",
+ 			 PTR_ERR(pkt_reformat));
+ 		return PTR_ERR(pkt_reformat);
+ 	}
+ 
+ 	vlan->pkt_reformat_pop = pkt_reformat;
+ 	return 0;
+ }
+ 
+ static void
+ mlx5_esw_bridge_vlan_pop_cleanup(struct mlx5_esw_bridge_vlan *vlan, struct mlx5_eswitch *esw)
+ {
+ 	mlx5_packet_reformat_dealloc(esw->dev, vlan->pkt_reformat_pop);
+ 	vlan->pkt_reformat_pop = NULL;
+ }
+ 
+ static struct mlx5_esw_bridge_vlan *
+ mlx5_esw_bridge_vlan_create(u16 vid, u16 flags, struct mlx5_esw_bridge_port *port,
+ 			    struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 	int err;
+ 
+ 	vlan = kvzalloc(sizeof(*vlan), GFP_KERNEL);
+ 	if (!vlan)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	vlan->vid = vid;
+ 	vlan->flags = flags;
+ 	INIT_LIST_HEAD(&vlan->fdb_list);
+ 
+ 	if (flags & BRIDGE_VLAN_INFO_PVID) {
+ 		err = mlx5_esw_bridge_vlan_push_create(vlan, esw);
+ 		if (err)
+ 			goto err_vlan_push;
+ 	}
+ 	if (flags & BRIDGE_VLAN_INFO_UNTAGGED) {
+ 		err = mlx5_esw_bridge_vlan_pop_create(vlan, esw);
+ 		if (err)
+ 			goto err_vlan_pop;
+ 	}
+ 
+ 	err = xa_insert(&port->vlans, vid, vlan, GFP_KERNEL);
+ 	if (err)
+ 		goto err_xa_insert;
+ 
+ 	trace_mlx5_esw_bridge_vlan_create(vlan);
+ 	return vlan;
+ 
+ err_xa_insert:
+ 	if (vlan->pkt_reformat_pop)
+ 		mlx5_esw_bridge_vlan_pop_cleanup(vlan, esw);
+ err_vlan_pop:
+ 	if (vlan->pkt_reformat_push)
+ 		mlx5_esw_bridge_vlan_push_cleanup(vlan, esw);
+ err_vlan_push:
+ 	kvfree(vlan);
+ 	return ERR_PTR(err);
+ }
+ 
+ static void mlx5_esw_bridge_vlan_erase(struct mlx5_esw_bridge_port *port,
+ 				       struct mlx5_esw_bridge_vlan *vlan)
+ {
+ 	xa_erase(&port->vlans, vlan->vid);
+ }
+ 
+ static void mlx5_esw_bridge_vlan_flush(struct mlx5_esw_bridge_vlan *vlan,
+ 				       struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry, *tmp;
+ 
+ 	list_for_each_entry_safe(entry, tmp, &vlan->fdb_list, vlan_list) {
+ 		mlx5_esw_bridge_fdb_del_notify(entry);
+ 		mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
+ 	}
+ 
+ 	if (vlan->pkt_reformat_pop)
+ 		mlx5_esw_bridge_vlan_pop_cleanup(vlan, bridge->br_offloads->esw);
+ 	if (vlan->pkt_reformat_push)
+ 		mlx5_esw_bridge_vlan_push_cleanup(vlan, bridge->br_offloads->esw);
+ }
+ 
+ static void mlx5_esw_bridge_vlan_cleanup(struct mlx5_esw_bridge_port *port,
+ 					 struct mlx5_esw_bridge_vlan *vlan,
+ 					 struct mlx5_esw_bridge *bridge)
+ {
+ 	trace_mlx5_esw_bridge_vlan_cleanup(vlan);
+ 	mlx5_esw_bridge_vlan_flush(vlan, bridge);
+ 	mlx5_esw_bridge_vlan_erase(port, vlan);
+ 	kvfree(vlan);
+ }
+ 
+ static void mlx5_esw_bridge_port_vlans_flush(struct mlx5_esw_bridge_port *port,
+ 					     struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 	unsigned long index;
+ 
+ 	xa_for_each(&port->vlans, index, vlan)
+ 		mlx5_esw_bridge_vlan_cleanup(port, vlan, bridge);
+ }
+ 
+ static struct mlx5_esw_bridge_vlan *
+ mlx5_esw_bridge_port_vlan_lookup(u16 vid, u16 vport_num, u16 esw_owner_vhca_id,
+ 				 struct mlx5_esw_bridge *bridge, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, bridge->br_offloads);
+ 	if (!port) {
+ 		/* FDB is added asynchronously on wq while port might have been deleted
+ 		 * concurrently. Report on 'info' logging level and skip the FDB offload.
+ 		 */
+ 		esw_info(esw->dev, "Failed to lookup bridge port (vport=%u)\n", vport_num);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	vlan = mlx5_esw_bridge_vlan_lookup(vid, port);
+ 	if (!vlan) {
+ 		/* FDB is added asynchronously on wq while vlan might have been deleted
+ 		 * concurrently. Report on 'info' logging level and skip the FDB offload.
+ 		 */
+ 		esw_info(esw->dev, "Failed to lookup bridge port vlan metadata (vport=%u)\n",
+ 			 vport_num);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	return vlan;
+ }
+ 
+ static struct mlx5_esw_bridge_fdb_entry *
+ mlx5_esw_bridge_fdb_entry_init(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 			       const unsigned char *addr, u16 vid, bool added_by_user, bool peer,
+ 			       struct mlx5_eswitch *esw, struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_vlan *vlan = NULL;
+ 	struct mlx5_esw_bridge_fdb_entry *entry;
+ 	struct mlx5_flow_handle *handle;
+ 	struct mlx5_fc *counter;
+ 	int err;
+ 
+ 	if (bridge->flags & MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG && vid) {
+ 		vlan = mlx5_esw_bridge_port_vlan_lookup(vid, vport_num, esw_owner_vhca_id, bridge,
+ 							esw);
+ 		if (IS_ERR(vlan))
+ 			return ERR_CAST(vlan);
+ 	}
+ 
+ 	entry = kvzalloc(sizeof(*entry), GFP_KERNEL);
+ 	if (!entry)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	ether_addr_copy(entry->key.addr, addr);
+ 	entry->key.vid = vid;
+ 	entry->dev = dev;
+ 	entry->vport_num = vport_num;
+ 	entry->esw_owner_vhca_id = esw_owner_vhca_id;
+ 	entry->lastuse = jiffies;
+ 	if (added_by_user)
+ 		entry->flags |= MLX5_ESW_BRIDGE_FLAG_ADDED_BY_USER;
+ 	if (peer)
+ 		entry->flags |= MLX5_ESW_BRIDGE_FLAG_PEER;
+ 
+ 	counter = mlx5_fc_create(esw->dev, true);
+ 	if (IS_ERR(counter)) {
+ 		err = PTR_ERR(counter);
+ 		goto err_ingress_fc_create;
+ 	}
+ 	entry->ingress_counter = counter;
+ 
+ 	handle = peer ?
+ 		mlx5_esw_bridge_ingress_flow_peer_create(vport_num, addr, vlan,
+ 							 mlx5_fc_id(counter), bridge) :
+ 		mlx5_esw_bridge_ingress_flow_create(vport_num, addr, vlan,
+ 						    mlx5_fc_id(counter), bridge);
+ 	if (IS_ERR(handle)) {
+ 		err = PTR_ERR(handle);
+ 		esw_warn(esw->dev, "Failed to create ingress flow(vport=%u,err=%d)\n",
+ 			 vport_num, err);
+ 		goto err_ingress_flow_create;
+ 	}
+ 	entry->ingress_handle = handle;
+ 
+ 	if (bridge->flags & MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG) {
+ 		handle = mlx5_esw_bridge_ingress_filter_flow_create(vport_num, addr, bridge);
+ 		if (IS_ERR(handle)) {
+ 			err = PTR_ERR(handle);
+ 			esw_warn(esw->dev, "Failed to create ingress filter(vport=%u,err=%d)\n",
+ 				 vport_num, err);
+ 			goto err_ingress_filter_flow_create;
+ 		}
+ 		entry->filter_handle = handle;
+ 	}
+ 
+ 	handle = mlx5_esw_bridge_egress_flow_create(vport_num, esw_owner_vhca_id, addr, vlan,
+ 						    bridge);
+ 	if (IS_ERR(handle)) {
+ 		err = PTR_ERR(handle);
+ 		esw_warn(esw->dev, "Failed to create egress flow(vport=%u,err=%d)\n",
+ 			 vport_num, err);
+ 		goto err_egress_flow_create;
+ 	}
+ 	entry->egress_handle = handle;
+ 
+ 	err = rhashtable_insert_fast(&bridge->fdb_ht, &entry->ht_node, fdb_ht_params);
+ 	if (err) {
+ 		esw_warn(esw->dev, "Failed to insert FDB flow(vport=%u,err=%d)\n", vport_num, err);
+ 		goto err_ht_init;
+ 	}
+ 
+ 	if (vlan)
+ 		list_add(&entry->vlan_list, &vlan->fdb_list);
+ 	else
+ 		INIT_LIST_HEAD(&entry->vlan_list);
+ 	list_add(&entry->list, &bridge->fdb_list);
+ 
+ 	trace_mlx5_esw_bridge_fdb_entry_init(entry);
+ 	return entry;
+ 
+ err_ht_init:
+ 	mlx5_del_flow_rules(entry->egress_handle);
+ err_egress_flow_create:
+ 	if (entry->filter_handle)
+ 		mlx5_del_flow_rules(entry->filter_handle);
+ err_ingress_filter_flow_create:
+ 	mlx5_del_flow_rules(entry->ingress_handle);
+ err_ingress_flow_create:
+ 	mlx5_fc_destroy(esw->dev, entry->ingress_counter);
+ err_ingress_fc_create:
+ 	kvfree(entry);
+ 	return ERR_PTR(err);
+ }
+ 
+ int mlx5_esw_bridge_ageing_time_set(u16 vport_num, u16 esw_owner_vhca_id, unsigned long ageing_time,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return -EINVAL;
+ 
+ 	port->bridge->ageing_time = clock_t_to_jiffies(ageing_time);
+ 	return 0;
+ }
+ 
+ int mlx5_esw_bridge_vlan_filtering_set(u16 vport_num, u16 esw_owner_vhca_id, bool enable,
+ 				       struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge *bridge;
+ 	bool filtering;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return -EINVAL;
+ 
+ 	bridge = port->bridge;
+ 	filtering = bridge->flags & MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG;
+ 	if (filtering == enable)
+ 		return 0;
+ 
+ 	mlx5_esw_bridge_fdb_flush(bridge);
+ 	if (enable)
+ 		bridge->flags |= MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG;
+ 	else
+ 		bridge->flags &= ~MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG;
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5_esw_bridge_vport_init(u16 vport_num, u16 esw_owner_vhca_id, u16 flags,
+ 				      struct mlx5_esw_bridge_offloads *br_offloads,
+ 				      struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_eswitch *esw = br_offloads->esw;
+ 	struct mlx5_esw_bridge_port *port;
+ 	int err;
+ 
+ 	port = kvzalloc(sizeof(*port), GFP_KERNEL);
+ 	if (!port)
+ 		return -ENOMEM;
+ 
+ 	port->vport_num = vport_num;
+ 	port->esw_owner_vhca_id = esw_owner_vhca_id;
+ 	port->bridge = bridge;
+ 	port->flags |= flags;
+ 	xa_init(&port->vlans);
+ 	err = mlx5_esw_bridge_port_insert(port, br_offloads);
+ 	if (err) {
+ 		esw_warn(esw->dev,
+ 			 "Failed to insert port metadata (vport=%u,esw_owner_vhca_id=%u,err=%d)\n",
+ 			 port->vport_num, port->esw_owner_vhca_id, err);
+ 		goto err_port_insert;
+ 	}
+ 	trace_mlx5_esw_bridge_vport_init(port);
+ 
+ 	return 0;
+ 
+ err_port_insert:
+ 	kvfree(port);
+ 	return err;
+ }
+ 
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  static int mlx5_esw_bridge_vport_cleanup(struct mlx5_esw_bridge_offloads *br_offloads,
 -					 struct mlx5_esw_bridge_port *port)
 +					 struct mlx5_vport *vport)
  {
 -	u16 vport_num = port->vport_num, esw_owner_vhca_id = port->esw_owner_vhca_id;
 -	struct mlx5_esw_bridge *bridge = port->bridge;
 -	struct mlx5_esw_bridge_fdb_entry *entry, *tmp;
 -
 -	list_for_each_entry_safe(entry, tmp, &bridge->fdb_list, list)
 -		if (entry->vport_num == vport_num && entry->esw_owner_vhca_id == esw_owner_vhca_id)
 -			mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
 -
 -	trace_mlx5_esw_bridge_vport_cleanup(port);
 -	mlx5_esw_bridge_port_vlans_flush(port, bridge);
 -	mlx5_esw_bridge_port_erase(port, br_offloads);
 -	kvfree(port);
 -	mlx5_esw_bridge_put(br_offloads, bridge);
 +	mlx5_esw_bridge_put(br_offloads, vport->bridge);
 +	vport->bridge = NULL;
  	return 0;
  }
  
@@@ -308,7 -1192,181 +946,185 @@@ int mlx5_esw_bridge_vport_unlink(int if
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
 +	return mlx5_esw_bridge_vport_cleanup(br_offloads, vport);
++=======
+ 	err = mlx5_esw_bridge_vport_cleanup(br_offloads, port);
+ 	if (err)
+ 		NL_SET_ERR_MSG_MOD(extack, "Port cleanup failed");
+ 	return err;
+ }
+ 
+ int mlx5_esw_bridge_vport_peer_link(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads,
+ 				    struct netlink_ext_ack *extack)
+ {
+ 	if (!MLX5_CAP_ESW(br_offloads->esw->dev, merged_eswitch))
+ 		return 0;
+ 
+ 	return mlx5_esw_bridge_vport_link_with_flags(ifindex, vport_num, esw_owner_vhca_id,
+ 						     MLX5_ESW_BRIDGE_PORT_FLAG_PEER,
+ 						     br_offloads, extack);
+ }
+ 
+ int mlx5_esw_bridge_vport_peer_unlink(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				      struct mlx5_esw_bridge_offloads *br_offloads,
+ 				      struct netlink_ext_ack *extack)
+ {
+ 	return mlx5_esw_bridge_vport_unlink(ifindex, vport_num, esw_owner_vhca_id, br_offloads,
+ 					    extack);
+ }
+ 
+ int mlx5_esw_bridge_port_vlan_add(u16 vport_num, u16 esw_owner_vhca_id, u16 vid, u16 flags,
+ 				  struct mlx5_esw_bridge_offloads *br_offloads,
+ 				  struct netlink_ext_ack *extack)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return -EINVAL;
+ 
+ 	vlan = mlx5_esw_bridge_vlan_lookup(vid, port);
+ 	if (vlan) {
+ 		if (vlan->flags == flags)
+ 			return 0;
+ 		mlx5_esw_bridge_vlan_cleanup(port, vlan, port->bridge);
+ 	}
+ 
+ 	vlan = mlx5_esw_bridge_vlan_create(vid, flags, port, br_offloads->esw);
+ 	if (IS_ERR(vlan)) {
+ 		NL_SET_ERR_MSG_MOD(extack, "Failed to create VLAN entry");
+ 		return PTR_ERR(vlan);
+ 	}
+ 	return 0;
+ }
+ 
+ void mlx5_esw_bridge_port_vlan_del(u16 vport_num, u16 esw_owner_vhca_id, u16 vid,
+ 				   struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return;
+ 
+ 	vlan = mlx5_esw_bridge_vlan_lookup(vid, port);
+ 	if (!vlan)
+ 		return;
+ 	mlx5_esw_bridge_vlan_cleanup(port, vlan, port->bridge);
+ }
+ 
+ void mlx5_esw_bridge_fdb_update_used(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				     struct mlx5_esw_bridge_offloads *br_offloads,
+ 				     struct switchdev_notifier_fdb_info *fdb_info)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry;
+ 	struct mlx5_esw_bridge_fdb_key key;
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge *bridge;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port || port->flags & MLX5_ESW_BRIDGE_PORT_FLAG_PEER)
+ 		return;
+ 
+ 	bridge = port->bridge;
+ 	ether_addr_copy(key.addr, fdb_info->addr);
+ 	key.vid = fdb_info->vid;
+ 	entry = rhashtable_lookup_fast(&bridge->fdb_ht, &key, fdb_ht_params);
+ 	if (!entry) {
+ 		esw_debug(br_offloads->esw->dev,
+ 			  "FDB entry with specified key not found (MAC=%pM,vid=%u,vport=%u)\n",
+ 			  key.addr, key.vid, vport_num);
+ 		return;
+ 	}
+ 
+ 	entry->lastuse = jiffies;
+ }
+ 
+ void mlx5_esw_bridge_fdb_create(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry;
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge *bridge;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return;
+ 
+ 	bridge = port->bridge;
+ 	entry = mlx5_esw_bridge_fdb_entry_init(dev, vport_num, esw_owner_vhca_id, fdb_info->addr,
+ 					       fdb_info->vid, fdb_info->added_by_user,
+ 					       port->flags & MLX5_ESW_BRIDGE_PORT_FLAG_PEER,
+ 					       br_offloads->esw, bridge);
+ 	if (IS_ERR(entry))
+ 		return;
+ 
+ 	if (entry->flags & MLX5_ESW_BRIDGE_FLAG_ADDED_BY_USER)
+ 		mlx5_esw_bridge_fdb_offload_notify(dev, entry->key.addr, entry->key.vid,
+ 						   SWITCHDEV_FDB_OFFLOADED);
+ 	else if (!(entry->flags & MLX5_ESW_BRIDGE_FLAG_PEER))
+ 		/* Take over dynamic entries to prevent kernel bridge from aging them out. */
+ 		mlx5_esw_bridge_fdb_offload_notify(dev, entry->key.addr, entry->key.vid,
+ 						   SWITCHDEV_FDB_ADD_TO_BRIDGE);
+ }
+ 
+ void mlx5_esw_bridge_fdb_remove(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info)
+ {
+ 	struct mlx5_eswitch *esw = br_offloads->esw;
+ 	struct mlx5_esw_bridge_fdb_entry *entry;
+ 	struct mlx5_esw_bridge_fdb_key key;
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge *bridge;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return;
+ 
+ 	bridge = port->bridge;
+ 	ether_addr_copy(key.addr, fdb_info->addr);
+ 	key.vid = fdb_info->vid;
+ 	entry = rhashtable_lookup_fast(&bridge->fdb_ht, &key, fdb_ht_params);
+ 	if (!entry) {
+ 		esw_warn(esw->dev,
+ 			 "FDB entry with specified key not found (MAC=%pM,vid=%u,vport=%u)\n",
+ 			 key.addr, key.vid, vport_num);
+ 		return;
+ 	}
+ 
+ 	mlx5_esw_bridge_fdb_del_notify(entry);
+ 	mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
+ }
+ 
+ void mlx5_esw_bridge_update(struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry, *tmp;
+ 	struct mlx5_esw_bridge *bridge;
+ 
+ 	list_for_each_entry(bridge, &br_offloads->bridges, list) {
+ 		list_for_each_entry_safe(entry, tmp, &bridge->fdb_list, list) {
+ 			unsigned long lastuse =
+ 				(unsigned long)mlx5_fc_query_lastuse(entry->ingress_counter);
+ 
+ 			if (entry->flags & MLX5_ESW_BRIDGE_FLAG_ADDED_BY_USER)
+ 				continue;
+ 
+ 			if (time_after(lastuse, entry->lastuse)) {
+ 				mlx5_esw_bridge_fdb_entry_refresh(entry);
+ 			} else if (!(entry->flags & MLX5_ESW_BRIDGE_FLAG_PEER) &&
+ 				   time_is_before_jiffies(entry->lastuse + bridge->ageing_time)) {
+ 				mlx5_esw_bridge_fdb_del_notify(entry);
+ 				mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
+ 			}
+ 		}
+ 	}
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  }
  
  static void mlx5_esw_bridge_flush(struct mlx5_esw_bridge_offloads *br_offloads)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
index 319b6f1db0ba,efc39975226e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
@@@ -22,9 -34,36 +22,43 @@@ struct mlx5_esw_bridge_offloads 
  
  struct mlx5_esw_bridge_offloads *mlx5_esw_bridge_init(struct mlx5_eswitch *esw);
  void mlx5_esw_bridge_cleanup(struct mlx5_eswitch *esw);
++<<<<<<< HEAD
 +int mlx5_esw_bridge_vport_link(int ifindex, struct mlx5_esw_bridge_offloads *br_offloads,
 +			       struct mlx5_vport *vport, struct netlink_ext_ack *extack);
 +int mlx5_esw_bridge_vport_unlink(int ifindex, struct mlx5_esw_bridge_offloads *br_offloads,
 +				 struct mlx5_vport *vport, struct netlink_ext_ack *extack);
++=======
+ int mlx5_esw_bridge_vport_link(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 			       struct mlx5_esw_bridge_offloads *br_offloads,
+ 			       struct netlink_ext_ack *extack);
+ int mlx5_esw_bridge_vport_unlink(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				 struct mlx5_esw_bridge_offloads *br_offloads,
+ 				 struct netlink_ext_ack *extack);
+ int mlx5_esw_bridge_vport_peer_link(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads,
+ 				    struct netlink_ext_ack *extack);
+ int mlx5_esw_bridge_vport_peer_unlink(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				      struct mlx5_esw_bridge_offloads *br_offloads,
+ 				      struct netlink_ext_ack *extack);
+ void mlx5_esw_bridge_fdb_update_used(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				     struct mlx5_esw_bridge_offloads *br_offloads,
+ 				     struct switchdev_notifier_fdb_info *fdb_info);
+ void mlx5_esw_bridge_fdb_create(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info);
+ void mlx5_esw_bridge_fdb_remove(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info);
+ void mlx5_esw_bridge_update(struct mlx5_esw_bridge_offloads *br_offloads);
+ int mlx5_esw_bridge_ageing_time_set(u16 vport_num, u16 esw_owner_vhca_id, unsigned long ageing_time,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads);
+ int mlx5_esw_bridge_vlan_filtering_set(u16 vport_num, u16 esw_owner_vhca_id, bool enable,
+ 				       struct mlx5_esw_bridge_offloads *br_offloads);
+ int mlx5_esw_bridge_port_vlan_add(u16 vport_num, u16 esw_owner_vhca_id, u16 vid, u16 flags,
+ 				  struct mlx5_esw_bridge_offloads *br_offloads,
+ 				  struct netlink_ext_ack *extack);
+ void mlx5_esw_bridge_port_vlan_del(u16 vport_num, u16 esw_owner_vhca_id, u16 vid,
+ 				   struct mlx5_esw_bridge_offloads *br_offloads);
++>>>>>>> ff9b7521468b (net/mlx5: Bridge, support LAG)
  
  #endif /* __MLX5_ESW_BRIDGE_H__ */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
