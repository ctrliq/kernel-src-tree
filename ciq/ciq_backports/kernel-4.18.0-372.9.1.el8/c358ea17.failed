net/mlx5: Bridge, allow merged eswitch connectivity

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Vlad Buslov <vladbu@nvidia.com>
commit c358ea1741bc5dda7032e2145805761119d81608
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/c358ea17.failed

Allow connectivity between representors of different eswitch instances that
are attached to same bridge when merged_eswitch capability is enabled. Add
ports of peer eswitch to bridge instance and mark them with
MLX5_ESW_BRIDGE_PORT_FLAG_PEER. Mark FDBs offloaded on peer ports with
MLX5_ESW_BRIDGE_FLAG_PEER flag. Such FDBs can only be aged out on their
local eswitch instance, which then sends SWITCHDEV_FDB_DEL_TO_BRIDGE event.
Listen to the event on mlx5 bridge implementation and delete peer FDBs in
event handler.

	Signed-off-by: Vlad Buslov <vladbu@nvidia.com>
	Reviewed-by: Roi Dayan <roid@nvidia.com>
	Reviewed-by: Mark Bloch <mbloch@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit c358ea1741bc5dda7032e2145805761119d81608)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
#	drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
#	drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
#	drivers/net/ethernet/mellanox/mlx5/core/esw/bridge_priv.h
#	drivers/net/ethernet/mellanox/mlx5/core/esw/diag/bridge_tracepoint.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
index de7a68488a9d,fdb9853bfe3f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
@@@ -8,6 -9,80 +8,83 @@@
  #include "esw/bridge.h"
  #include "en_rep.h"
  
++<<<<<<< HEAD
++=======
+ #define MLX5_ESW_BRIDGE_UPDATE_INTERVAL 1000
+ 
+ struct mlx5_bridge_switchdev_fdb_work {
+ 	struct work_struct work;
+ 	struct switchdev_notifier_fdb_info fdb_info;
+ 	struct net_device *dev;
+ 	struct mlx5_esw_bridge_offloads *br_offloads;
+ 	bool add;
+ };
+ 
+ static bool mlx5_esw_bridge_dev_same_esw(struct net_device *dev, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 
+ 	return esw == priv->mdev->priv.eswitch;
+ }
+ 
+ static bool mlx5_esw_bridge_dev_same_hw(struct net_device *dev, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	struct mlx5_core_dev *mdev, *esw_mdev;
+ 	u64 system_guid, esw_system_guid;
+ 
+ 	mdev = priv->mdev;
+ 	esw_mdev = esw->dev;
+ 
+ 	system_guid = mlx5_query_nic_system_image_guid(mdev);
+ 	esw_system_guid = mlx5_query_nic_system_image_guid(esw_mdev);
+ 
+ 	return system_guid == esw_system_guid;
+ }
+ 
+ static int mlx5_esw_bridge_vport_num_vhca_id_get(struct net_device *dev, struct mlx5_eswitch *esw,
+ 						 u16 *vport_num, u16 *esw_owner_vhca_id)
+ {
+ 	struct mlx5e_rep_priv *rpriv;
+ 	struct mlx5e_priv *priv;
+ 
+ 	if (!mlx5e_eswitch_rep(dev) || !mlx5_esw_bridge_dev_same_hw(dev, esw))
+ 		return -ENODEV;
+ 
+ 	priv = netdev_priv(dev);
+ 	rpriv = priv->ppriv;
+ 	*vport_num = rpriv->rep->vport;
+ 	*esw_owner_vhca_id = MLX5_CAP_GEN(priv->mdev, vhca_id);
+ 	return 0;
+ }
+ 
+ static int
+ mlx5_esw_bridge_lower_rep_vport_num_vhca_id_get(struct net_device *dev, struct mlx5_eswitch *esw,
+ 						u16 *vport_num, u16 *esw_owner_vhca_id)
+ {
+ 	struct net_device *lower_dev;
+ 	struct list_head *iter;
+ 
+ 	if (mlx5e_eswitch_rep(dev))
+ 		return mlx5_esw_bridge_vport_num_vhca_id_get(dev, esw, vport_num,
+ 							     esw_owner_vhca_id);
+ 
+ 	netdev_for_each_lower_dev(dev, lower_dev, iter) {
+ 		int err;
+ 
+ 		if (netif_is_bridge_master(lower_dev))
+ 			continue;
+ 
+ 		err = mlx5_esw_bridge_lower_rep_vport_num_vhca_id_get(lower_dev, esw, vport_num,
+ 								      esw_owner_vhca_id);
+ 		if (!err)
+ 			return 0;
+ 	}
+ 
+ 	return -ENODEV;
+ }
+ 
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  static int mlx5_esw_bridge_port_changeupper(struct notifier_block *nb, void *ptr)
  {
  	struct mlx5_esw_bridge_offloads *br_offloads = container_of(nb,
@@@ -15,37 -90,37 +92,60 @@@
  								    netdev_nb);
  	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
  	struct netdev_notifier_changeupper_info *info = ptr;
++<<<<<<< HEAD
++=======
+ 	struct mlx5_eswitch *esw = br_offloads->esw;
+ 	struct net_device *upper = info->upper_dev;
+ 	u16 vport_num, esw_owner_vhca_id;
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  	struct netlink_ext_ack *extack;
 -	int ifindex = upper->ifindex;
 -	int err;
 +	struct mlx5e_rep_priv *rpriv;
 +	struct mlx5_eswitch *esw;
 +	struct mlx5_vport *vport;
 +	struct net_device *upper;
 +	struct mlx5e_priv *priv;
 +	u16 vport_num;
 +
 +	if (!mlx5e_eswitch_rep(dev))
 +		return 0;
  
 +	upper = info->upper_dev;
  	if (!netif_is_bridge_master(upper))
  		return 0;
  
 -	err = mlx5_esw_bridge_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
 -						    &esw_owner_vhca_id);
 -	if (err)
 +	esw = br_offloads->esw;
 +	priv = netdev_priv(dev);
 +	if (esw != priv->mdev->priv.eswitch)
  		return 0;
  
 +	rpriv = priv->ppriv;
 +	vport_num = rpriv->rep->vport;
 +	vport = mlx5_eswitch_get_vport(esw, vport_num);
 +	if (IS_ERR(vport))
 +		return PTR_ERR(vport);
 +
  	extack = netdev_notifier_info_to_extack(&info->info);
  
++<<<<<<< HEAD
 +	return info->linking ?
 +		mlx5_esw_bridge_vport_link(upper->ifindex, br_offloads, vport, extack) :
 +		mlx5_esw_bridge_vport_unlink(upper->ifindex, br_offloads, vport, extack);
++=======
+ 	if (mlx5_esw_bridge_dev_same_esw(dev, esw))
+ 		err = info->linking ?
+ 			mlx5_esw_bridge_vport_link(ifindex, vport_num, esw_owner_vhca_id,
+ 						   br_offloads, extack) :
+ 			mlx5_esw_bridge_vport_unlink(ifindex, vport_num, esw_owner_vhca_id,
+ 						     br_offloads, extack);
+ 	else if (mlx5_esw_bridge_dev_same_hw(dev, esw))
+ 		err = info->linking ?
+ 			mlx5_esw_bridge_vport_peer_link(ifindex, vport_num, esw_owner_vhca_id,
+ 							br_offloads, extack) :
+ 			mlx5_esw_bridge_vport_peer_unlink(ifindex, vport_num, esw_owner_vhca_id,
+ 							  br_offloads, extack);
+ 
+ 	return err;
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  }
  
  static int mlx5_esw_bridge_switchdev_port_event(struct notifier_block *nb,
@@@ -65,6 -140,271 +165,274 @@@
  	return notifier_from_errno(err);
  }
  
++<<<<<<< HEAD
++=======
+ static int
+ mlx5_esw_bridge_port_obj_add(struct net_device *dev,
+ 			     struct switchdev_notifier_port_obj_info *port_obj_info,
+ 			     struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct netlink_ext_ack *extack = switchdev_notifier_info_to_extack(&port_obj_info->info);
+ 	const struct switchdev_obj *obj = port_obj_info->obj;
+ 	const struct switchdev_obj_port_vlan *vlan;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 	int err;
+ 
+ 	err = mlx5_esw_bridge_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 						    &esw_owner_vhca_id);
+ 	if (err)
+ 		return 0;
+ 
+ 	port_obj_info->handled = true;
+ 
+ 	switch (obj->id) {
+ 	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+ 		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+ 		err = mlx5_esw_bridge_port_vlan_add(vport_num, esw_owner_vhca_id, vlan->vid,
+ 						    vlan->flags, br_offloads, extack);
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return err;
+ }
+ 
+ static int
+ mlx5_esw_bridge_port_obj_del(struct net_device *dev,
+ 			     struct switchdev_notifier_port_obj_info *port_obj_info,
+ 			     struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	const struct switchdev_obj *obj = port_obj_info->obj;
+ 	const struct switchdev_obj_port_vlan *vlan;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 	int err;
+ 
+ 	err = mlx5_esw_bridge_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 						    &esw_owner_vhca_id);
+ 	if (err)
+ 		return 0;
+ 
+ 	port_obj_info->handled = true;
+ 
+ 	switch (obj->id) {
+ 	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+ 		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+ 		mlx5_esw_bridge_port_vlan_del(vport_num, esw_owner_vhca_id, vlan->vid, br_offloads);
+ 		break;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return 0;
+ }
+ 
+ static int
+ mlx5_esw_bridge_port_obj_attr_set(struct net_device *dev,
+ 				  struct switchdev_notifier_port_attr_info *port_attr_info,
+ 				  struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct netlink_ext_ack *extack = switchdev_notifier_info_to_extack(&port_attr_info->info);
+ 	const struct switchdev_attr *attr = port_attr_info->attr;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 	int err;
+ 
+ 	err = mlx5_esw_bridge_lower_rep_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 							      &esw_owner_vhca_id);
+ 	if (err)
+ 		return 0;
+ 
+ 	port_attr_info->handled = true;
+ 
+ 	switch (attr->id) {
+ 	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
+ 		if (attr->u.brport_flags.mask & ~(BR_LEARNING | BR_FLOOD | BR_MCAST_FLOOD)) {
+ 			NL_SET_ERR_MSG_MOD(extack, "Flag is not supported");
+ 			err = -EINVAL;
+ 		}
+ 		break;
+ 	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+ 		break;
+ 	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+ 		err = mlx5_esw_bridge_ageing_time_set(vport_num, esw_owner_vhca_id,
+ 						      attr->u.ageing_time, br_offloads);
+ 		break;
+ 	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+ 		err = mlx5_esw_bridge_vlan_filtering_set(vport_num, esw_owner_vhca_id,
+ 							 attr->u.vlan_filtering, br_offloads);
+ 		break;
+ 	default:
+ 		err = -EOPNOTSUPP;
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int mlx5_esw_bridge_event_blocking(struct notifier_block *nb,
+ 					  unsigned long event, void *ptr)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = container_of(nb,
+ 								    struct mlx5_esw_bridge_offloads,
+ 								    nb_blk);
+ 	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+ 	int err;
+ 
+ 	switch (event) {
+ 	case SWITCHDEV_PORT_OBJ_ADD:
+ 		err = mlx5_esw_bridge_port_obj_add(dev, ptr, br_offloads);
+ 		break;
+ 	case SWITCHDEV_PORT_OBJ_DEL:
+ 		err = mlx5_esw_bridge_port_obj_del(dev, ptr, br_offloads);
+ 		break;
+ 	case SWITCHDEV_PORT_ATTR_SET:
+ 		err = mlx5_esw_bridge_port_obj_attr_set(dev, ptr, br_offloads);
+ 		break;
+ 	default:
+ 		err = 0;
+ 	}
+ 
+ 	return notifier_from_errno(err);
+ }
+ 
+ static void
+ mlx5_esw_bridge_cleanup_switchdev_fdb_work(struct mlx5_bridge_switchdev_fdb_work *fdb_work)
+ {
+ 	dev_put(fdb_work->dev);
+ 	kfree(fdb_work->fdb_info.addr);
+ 	kfree(fdb_work);
+ }
+ 
+ static void mlx5_esw_bridge_switchdev_fdb_event_work(struct work_struct *work)
+ {
+ 	struct mlx5_bridge_switchdev_fdb_work *fdb_work =
+ 		container_of(work, struct mlx5_bridge_switchdev_fdb_work, work);
+ 	struct switchdev_notifier_fdb_info *fdb_info =
+ 		&fdb_work->fdb_info;
+ 	struct mlx5_esw_bridge_offloads *br_offloads =
+ 		fdb_work->br_offloads;
+ 	struct net_device *dev = fdb_work->dev;
+ 	u16 vport_num, esw_owner_vhca_id;
+ 	int err;
+ 
+ 	rtnl_lock();
+ 
+ 	err = mlx5_esw_bridge_vport_num_vhca_id_get(dev, br_offloads->esw, &vport_num,
+ 						    &esw_owner_vhca_id);
+ 	if (err)
+ 		goto out;
+ 
+ 	if (fdb_work->add)
+ 		mlx5_esw_bridge_fdb_create(dev, vport_num, esw_owner_vhca_id, br_offloads,
+ 					   fdb_info);
+ 	else
+ 		mlx5_esw_bridge_fdb_remove(dev, vport_num, esw_owner_vhca_id, br_offloads,
+ 					   fdb_info);
+ 
+ out:
+ 	rtnl_unlock();
+ 	mlx5_esw_bridge_cleanup_switchdev_fdb_work(fdb_work);
+ }
+ 
+ static struct mlx5_bridge_switchdev_fdb_work *
+ mlx5_esw_bridge_init_switchdev_fdb_work(struct net_device *dev, bool add,
+ 					struct switchdev_notifier_fdb_info *fdb_info,
+ 					struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_bridge_switchdev_fdb_work *work;
+ 	u8 *addr;
+ 
+ 	work = kzalloc(sizeof(*work), GFP_ATOMIC);
+ 	if (!work)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	INIT_WORK(&work->work, mlx5_esw_bridge_switchdev_fdb_event_work);
+ 	memcpy(&work->fdb_info, fdb_info, sizeof(work->fdb_info));
+ 
+ 	addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+ 	if (!addr) {
+ 		kfree(work);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 	ether_addr_copy(addr, fdb_info->addr);
+ 	work->fdb_info.addr = addr;
+ 
+ 	dev_hold(dev);
+ 	work->dev = dev;
+ 	work->br_offloads = br_offloads;
+ 	work->add = add;
+ 	return work;
+ }
+ 
+ static int mlx5_esw_bridge_switchdev_event(struct notifier_block *nb,
+ 					   unsigned long event, void *ptr)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = container_of(nb,
+ 								    struct mlx5_esw_bridge_offloads,
+ 								    nb);
+ 	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+ 	struct switchdev_notifier_fdb_info *fdb_info;
+ 	struct mlx5_bridge_switchdev_fdb_work *work;
+ 	struct switchdev_notifier_info *info = ptr;
+ 	struct net_device *upper;
+ 
+ 	if (event == SWITCHDEV_PORT_ATTR_SET) {
+ 		int err = mlx5_esw_bridge_port_obj_attr_set(dev, ptr, br_offloads);
+ 
+ 		return notifier_from_errno(err);
+ 	}
+ 
+ 	upper = netdev_master_upper_dev_get_rcu(dev);
+ 	if (!upper)
+ 		return NOTIFY_DONE;
+ 	if (!netif_is_bridge_master(upper))
+ 		return NOTIFY_DONE;
+ 
+ 	if (!mlx5e_eswitch_rep(dev))
+ 		return NOTIFY_DONE;
+ 
+ 	switch (event) {
+ 	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+ 		/* only handle the event when source is on another eswitch */
+ 		if (mlx5_esw_bridge_dev_same_esw(dev, br_offloads->esw))
+ 			break;
+ 		fallthrough;
+ 	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+ 	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+ 		fdb_info = container_of(info,
+ 					struct switchdev_notifier_fdb_info,
+ 					info);
+ 
+ 		work = mlx5_esw_bridge_init_switchdev_fdb_work(dev,
+ 							       event == SWITCHDEV_FDB_ADD_TO_DEVICE,
+ 							       fdb_info,
+ 							       br_offloads);
+ 		if (IS_ERR(work)) {
+ 			WARN_ONCE(1, "Failed to init switchdev work, err=%ld",
+ 				  PTR_ERR(work));
+ 			return notifier_from_errno(PTR_ERR(work));
+ 		}
+ 
+ 		queue_work(br_offloads->wq, &work->work);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 	return NOTIFY_DONE;
+ }
+ 
+ static void mlx5_esw_bridge_update_work(struct work_struct *work)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = container_of(work,
+ 								    struct mlx5_esw_bridge_offloads,
+ 								    update_work.work);
+ 
+ 	rtnl_lock();
+ 	mlx5_esw_bridge_update(br_offloads);
+ 	rtnl_unlock();
+ 
+ 	queue_delayed_work(br_offloads->wq, &br_offloads->update_work,
+ 			   msecs_to_jiffies(MLX5_ESW_BRIDGE_UPDATE_INTERVAL));
+ }
+ 
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  void mlx5e_rep_bridge_init(struct mlx5e_priv *priv)
  {
  	struct mlx5_esw_bridge_offloads *br_offloads;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
index b503562f97d0,20d44b0ae337..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
@@@ -25,11 -52,39 +25,35 @@@ struct mlx5_esw_bridge 
  	int ifindex;
  	int refcnt;
  	struct list_head list;
 -	struct mlx5_esw_bridge_offloads *br_offloads;
 -
 -	struct list_head fdb_list;
 -	struct rhashtable fdb_ht;
  
  	struct mlx5_flow_table *egress_ft;
 -	struct mlx5_flow_group *egress_vlan_fg;
  	struct mlx5_flow_group *egress_mac_fg;
 -	unsigned long ageing_time;
 -	u32 flags;
  };
  
++<<<<<<< HEAD
++=======
+ static void
+ mlx5_esw_bridge_fdb_offload_notify(struct net_device *dev, const unsigned char *addr, u16 vid,
+ 				   unsigned long val)
+ {
+ 	struct switchdev_notifier_fdb_info send_info = {};
+ 
+ 	send_info.addr = addr;
+ 	send_info.vid = vid;
+ 	send_info.offloaded = true;
+ 	call_switchdev_notifiers(val, dev, &send_info.info, NULL);
+ }
+ 
+ static void
+ mlx5_esw_bridge_fdb_del_notify(struct mlx5_esw_bridge_fdb_entry *entry)
+ {
+ 	if (!(entry->flags & (MLX5_ESW_BRIDGE_FLAG_ADDED_BY_USER | MLX5_ESW_BRIDGE_FLAG_PEER)))
+ 		mlx5_esw_bridge_fdb_offload_notify(entry->dev, entry->key.addr,
+ 						   entry->key.vid,
+ 						   SWITCHDEV_FDB_DEL_TO_BRIDGE);
+ }
+ 
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  static struct mlx5_flow_table *
  mlx5_esw_bridge_table_create(int max_fte, u32 level, struct mlx5_eswitch *esw)
  {
@@@ -194,6 -406,168 +218,170 @@@ mlx5_esw_bridge_egress_table_cleanup(st
  	mlx5_destroy_flow_table(bridge->egress_ft);
  }
  
++<<<<<<< HEAD
++=======
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_ingress_flow_create(u16 vport_num, const unsigned char *addr,
+ 				    struct mlx5_esw_bridge_vlan *vlan, u32 counter_id,
+ 				    struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = bridge->br_offloads;
+ 	struct mlx5_flow_act flow_act = {
+ 		.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST | MLX5_FLOW_CONTEXT_ACTION_COUNT,
+ 		.flags = FLOW_ACT_NO_APPEND,
+ 	};
+ 	struct mlx5_flow_destination dests[2] = {};
+ 	struct mlx5_flow_spec *rule_spec;
+ 	struct mlx5_flow_handle *handle;
+ 	u8 *smac_v, *smac_c;
+ 
+ 	rule_spec = kvzalloc(sizeof(*rule_spec), GFP_KERNEL);
+ 	if (!rule_spec)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	rule_spec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS | MLX5_MATCH_MISC_PARAMETERS_2;
+ 
+ 	smac_v = MLX5_ADDR_OF(fte_match_param, rule_spec->match_value,
+ 			      outer_headers.smac_47_16);
+ 	ether_addr_copy(smac_v, addr);
+ 	smac_c = MLX5_ADDR_OF(fte_match_param, rule_spec->match_criteria,
+ 			      outer_headers.smac_47_16);
+ 	eth_broadcast_addr(smac_c);
+ 
+ 	MLX5_SET(fte_match_param, rule_spec->match_criteria,
+ 		 misc_parameters_2.metadata_reg_c_0, mlx5_eswitch_get_vport_metadata_mask());
+ 	MLX5_SET(fte_match_param, rule_spec->match_value, misc_parameters_2.metadata_reg_c_0,
+ 		 mlx5_eswitch_get_vport_metadata_for_match(br_offloads->esw, vport_num));
+ 
+ 	if (vlan && vlan->pkt_reformat_push) {
+ 		flow_act.action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;
+ 		flow_act.pkt_reformat = vlan->pkt_reformat_push;
+ 	} else if (vlan) {
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_value,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.first_vid);
+ 		MLX5_SET(fte_match_param, rule_spec->match_value, outer_headers.first_vid,
+ 			 vlan->vid);
+ 	}
+ 
+ 	dests[0].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;
+ 	dests[0].ft = bridge->egress_ft;
+ 	dests[1].type = MLX5_FLOW_DESTINATION_TYPE_COUNTER;
+ 	dests[1].counter_id = counter_id;
+ 
+ 	handle = mlx5_add_flow_rules(br_offloads->ingress_ft, rule_spec, &flow_act, dests,
+ 				     ARRAY_SIZE(dests));
+ 
+ 	kvfree(rule_spec);
+ 	return handle;
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_ingress_filter_flow_create(u16 vport_num, const unsigned char *addr,
+ 					   struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_offloads *br_offloads = bridge->br_offloads;
+ 	struct mlx5_flow_destination dest = {
+ 		.type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE,
+ 		.ft = br_offloads->skip_ft,
+ 	};
+ 	struct mlx5_flow_act flow_act = {
+ 		.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST,
+ 		.flags = FLOW_ACT_NO_APPEND,
+ 	};
+ 	struct mlx5_flow_spec *rule_spec;
+ 	struct mlx5_flow_handle *handle;
+ 	u8 *smac_v, *smac_c;
+ 
+ 	rule_spec = kvzalloc(sizeof(*rule_spec), GFP_KERNEL);
+ 	if (!rule_spec)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	rule_spec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS | MLX5_MATCH_MISC_PARAMETERS_2;
+ 
+ 	smac_v = MLX5_ADDR_OF(fte_match_param, rule_spec->match_value,
+ 			      outer_headers.smac_47_16);
+ 	ether_addr_copy(smac_v, addr);
+ 	smac_c = MLX5_ADDR_OF(fte_match_param, rule_spec->match_criteria,
+ 			      outer_headers.smac_47_16);
+ 	eth_broadcast_addr(smac_c);
+ 
+ 	MLX5_SET(fte_match_param, rule_spec->match_criteria,
+ 		 misc_parameters_2.metadata_reg_c_0, mlx5_eswitch_get_vport_metadata_mask());
+ 	MLX5_SET(fte_match_param, rule_spec->match_value, misc_parameters_2.metadata_reg_c_0,
+ 		 mlx5_eswitch_get_vport_metadata_for_match(br_offloads->esw, vport_num));
+ 
+ 	MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 			 outer_headers.cvlan_tag);
+ 	MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_value,
+ 			 outer_headers.cvlan_tag);
+ 
+ 	handle = mlx5_add_flow_rules(br_offloads->ingress_ft, rule_spec, &flow_act, &dest, 1);
+ 
+ 	kvfree(rule_spec);
+ 	return handle;
+ }
+ 
+ static struct mlx5_flow_handle *
+ mlx5_esw_bridge_egress_flow_create(u16 vport_num, u16 esw_owner_vhca_id, const unsigned char *addr,
+ 				   struct mlx5_esw_bridge_vlan *vlan,
+ 				   struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_flow_destination dest = {
+ 		.type = MLX5_FLOW_DESTINATION_TYPE_VPORT,
+ 		.vport.num = vport_num,
+ 	};
+ 	struct mlx5_flow_act flow_act = {
+ 		.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST,
+ 		.flags = FLOW_ACT_NO_APPEND,
+ 	};
+ 	struct mlx5_flow_spec *rule_spec;
+ 	struct mlx5_flow_handle *handle;
+ 	u8 *dmac_v, *dmac_c;
+ 
+ 	rule_spec = kvzalloc(sizeof(*rule_spec), GFP_KERNEL);
+ 	if (!rule_spec)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	rule_spec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS;
+ 
+ 	dmac_v = MLX5_ADDR_OF(fte_match_param, rule_spec->match_value,
+ 			      outer_headers.dmac_47_16);
+ 	ether_addr_copy(dmac_v, addr);
+ 	dmac_c = MLX5_ADDR_OF(fte_match_param, rule_spec->match_criteria,
+ 			      outer_headers.dmac_47_16);
+ 	eth_broadcast_addr(dmac_c);
+ 
+ 	if (vlan) {
+ 		if (vlan->pkt_reformat_pop) {
+ 			flow_act.action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;
+ 			flow_act.pkt_reformat = vlan->pkt_reformat_pop;
+ 		}
+ 
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_value,
+ 				 outer_headers.cvlan_tag);
+ 		MLX5_SET_TO_ONES(fte_match_param, rule_spec->match_criteria,
+ 				 outer_headers.first_vid);
+ 		MLX5_SET(fte_match_param, rule_spec->match_value, outer_headers.first_vid,
+ 			 vlan->vid);
+ 	}
+ 
+ 	if (MLX5_CAP_ESW(bridge->br_offloads->esw->dev, merged_eswitch)) {
+ 		dest.vport.flags = MLX5_FLOW_DEST_VPORT_VHCA_ID;
+ 		dest.vport.vhca_id = esw_owner_vhca_id;
+ 	}
+ 	handle = mlx5_add_flow_rules(bridge->egress_ft, rule_spec, &flow_act, &dest, 1);
+ 
+ 	kvfree(rule_spec);
+ 	return handle;
+ }
+ 
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  static struct mlx5_esw_bridge *mlx5_esw_bridge_create(int ifindex,
  						      struct mlx5_esw_bridge_offloads *br_offloads)
  {
@@@ -272,20 -759,363 +460,361 @@@ static int mlx5_esw_bridge_vport_init(s
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static void
+ mlx5_esw_bridge_vlan_push_cleanup(struct mlx5_esw_bridge_vlan *vlan, struct mlx5_eswitch *esw)
+ {
+ 	mlx5_packet_reformat_dealloc(esw->dev, vlan->pkt_reformat_push);
+ 	vlan->pkt_reformat_push = NULL;
+ }
+ 
+ static int
+ mlx5_esw_bridge_vlan_pop_create(struct mlx5_esw_bridge_vlan *vlan, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_pkt_reformat_params reformat_params = {};
+ 	struct mlx5_pkt_reformat *pkt_reformat;
+ 
+ 	if (!BIT(MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, reformat_remove)) ||
+ 	    MLX5_CAP_GEN_2(esw->dev, max_reformat_remove_size) < sizeof(struct vlan_hdr) ||
+ 	    MLX5_CAP_GEN_2(esw->dev, max_reformat_remove_offset) <
+ 	    offsetof(struct vlan_ethhdr, h_vlan_proto)) {
+ 		esw_warn(esw->dev, "Packet reformat REMOVE_HEADER is not supported\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	reformat_params.type = MLX5_REFORMAT_TYPE_REMOVE_HDR;
+ 	reformat_params.param_0 = MLX5_REFORMAT_CONTEXT_ANCHOR_MAC_START;
+ 	reformat_params.param_1 = offsetof(struct vlan_ethhdr, h_vlan_proto);
+ 	reformat_params.size = sizeof(struct vlan_hdr);
+ 	pkt_reformat = mlx5_packet_reformat_alloc(esw->dev,
+ 						  &reformat_params,
+ 						  MLX5_FLOW_NAMESPACE_FDB);
+ 	if (IS_ERR(pkt_reformat)) {
+ 		esw_warn(esw->dev, "Failed to alloc packet reformat REMOVE_HEADER (err=%ld)\n",
+ 			 PTR_ERR(pkt_reformat));
+ 		return PTR_ERR(pkt_reformat);
+ 	}
+ 
+ 	vlan->pkt_reformat_pop = pkt_reformat;
+ 	return 0;
+ }
+ 
+ static void
+ mlx5_esw_bridge_vlan_pop_cleanup(struct mlx5_esw_bridge_vlan *vlan, struct mlx5_eswitch *esw)
+ {
+ 	mlx5_packet_reformat_dealloc(esw->dev, vlan->pkt_reformat_pop);
+ 	vlan->pkt_reformat_pop = NULL;
+ }
+ 
+ static struct mlx5_esw_bridge_vlan *
+ mlx5_esw_bridge_vlan_create(u16 vid, u16 flags, struct mlx5_esw_bridge_port *port,
+ 			    struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 	int err;
+ 
+ 	vlan = kvzalloc(sizeof(*vlan), GFP_KERNEL);
+ 	if (!vlan)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	vlan->vid = vid;
+ 	vlan->flags = flags;
+ 	INIT_LIST_HEAD(&vlan->fdb_list);
+ 
+ 	if (flags & BRIDGE_VLAN_INFO_PVID) {
+ 		err = mlx5_esw_bridge_vlan_push_create(vlan, esw);
+ 		if (err)
+ 			goto err_vlan_push;
+ 	}
+ 	if (flags & BRIDGE_VLAN_INFO_UNTAGGED) {
+ 		err = mlx5_esw_bridge_vlan_pop_create(vlan, esw);
+ 		if (err)
+ 			goto err_vlan_pop;
+ 	}
+ 
+ 	err = xa_insert(&port->vlans, vid, vlan, GFP_KERNEL);
+ 	if (err)
+ 		goto err_xa_insert;
+ 
+ 	trace_mlx5_esw_bridge_vlan_create(vlan);
+ 	return vlan;
+ 
+ err_xa_insert:
+ 	if (vlan->pkt_reformat_pop)
+ 		mlx5_esw_bridge_vlan_pop_cleanup(vlan, esw);
+ err_vlan_pop:
+ 	if (vlan->pkt_reformat_push)
+ 		mlx5_esw_bridge_vlan_push_cleanup(vlan, esw);
+ err_vlan_push:
+ 	kvfree(vlan);
+ 	return ERR_PTR(err);
+ }
+ 
+ static void mlx5_esw_bridge_vlan_erase(struct mlx5_esw_bridge_port *port,
+ 				       struct mlx5_esw_bridge_vlan *vlan)
+ {
+ 	xa_erase(&port->vlans, vlan->vid);
+ }
+ 
+ static void mlx5_esw_bridge_vlan_flush(struct mlx5_esw_bridge_vlan *vlan,
+ 				       struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry, *tmp;
+ 
+ 	list_for_each_entry_safe(entry, tmp, &vlan->fdb_list, vlan_list) {
+ 		mlx5_esw_bridge_fdb_del_notify(entry);
+ 		mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
+ 	}
+ 
+ 	if (vlan->pkt_reformat_pop)
+ 		mlx5_esw_bridge_vlan_pop_cleanup(vlan, bridge->br_offloads->esw);
+ 	if (vlan->pkt_reformat_push)
+ 		mlx5_esw_bridge_vlan_push_cleanup(vlan, bridge->br_offloads->esw);
+ }
+ 
+ static void mlx5_esw_bridge_vlan_cleanup(struct mlx5_esw_bridge_port *port,
+ 					 struct mlx5_esw_bridge_vlan *vlan,
+ 					 struct mlx5_esw_bridge *bridge)
+ {
+ 	trace_mlx5_esw_bridge_vlan_cleanup(vlan);
+ 	mlx5_esw_bridge_vlan_flush(vlan, bridge);
+ 	mlx5_esw_bridge_vlan_erase(port, vlan);
+ 	kvfree(vlan);
+ }
+ 
+ static void mlx5_esw_bridge_port_vlans_flush(struct mlx5_esw_bridge_port *port,
+ 					     struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 	unsigned long index;
+ 
+ 	xa_for_each(&port->vlans, index, vlan)
+ 		mlx5_esw_bridge_vlan_cleanup(port, vlan, bridge);
+ }
+ 
+ static struct mlx5_esw_bridge_vlan *
+ mlx5_esw_bridge_port_vlan_lookup(u16 vid, u16 vport_num, u16 esw_owner_vhca_id,
+ 				 struct mlx5_esw_bridge *bridge, struct mlx5_eswitch *esw)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, bridge->br_offloads);
+ 	if (!port) {
+ 		/* FDB is added asynchronously on wq while port might have been deleted
+ 		 * concurrently. Report on 'info' logging level and skip the FDB offload.
+ 		 */
+ 		esw_info(esw->dev, "Failed to lookup bridge port (vport=%u)\n", vport_num);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	vlan = mlx5_esw_bridge_vlan_lookup(vid, port);
+ 	if (!vlan) {
+ 		/* FDB is added asynchronously on wq while vlan might have been deleted
+ 		 * concurrently. Report on 'info' logging level and skip the FDB offload.
+ 		 */
+ 		esw_info(esw->dev, "Failed to lookup bridge port vlan metadata (vport=%u)\n",
+ 			 vport_num);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	return vlan;
+ }
+ 
+ static struct mlx5_esw_bridge_fdb_entry *
+ mlx5_esw_bridge_fdb_entry_init(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 			       const unsigned char *addr, u16 vid, bool added_by_user, bool peer,
+ 			       struct mlx5_eswitch *esw, struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_esw_bridge_vlan *vlan = NULL;
+ 	struct mlx5_esw_bridge_fdb_entry *entry;
+ 	struct mlx5_flow_handle *handle;
+ 	struct mlx5_fc *counter;
+ 	int err;
+ 
+ 	if (bridge->flags & MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG && vid) {
+ 		vlan = mlx5_esw_bridge_port_vlan_lookup(vid, vport_num, esw_owner_vhca_id, bridge,
+ 							esw);
+ 		if (IS_ERR(vlan))
+ 			return ERR_CAST(vlan);
+ 	}
+ 
+ 	entry = kvzalloc(sizeof(*entry), GFP_KERNEL);
+ 	if (!entry)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	ether_addr_copy(entry->key.addr, addr);
+ 	entry->key.vid = vid;
+ 	entry->dev = dev;
+ 	entry->vport_num = vport_num;
+ 	entry->esw_owner_vhca_id = esw_owner_vhca_id;
+ 	entry->lastuse = jiffies;
+ 	if (added_by_user)
+ 		entry->flags |= MLX5_ESW_BRIDGE_FLAG_ADDED_BY_USER;
+ 	if (peer)
+ 		entry->flags |= MLX5_ESW_BRIDGE_FLAG_PEER;
+ 
+ 	counter = mlx5_fc_create(esw->dev, true);
+ 	if (IS_ERR(counter)) {
+ 		err = PTR_ERR(counter);
+ 		goto err_ingress_fc_create;
+ 	}
+ 	entry->ingress_counter = counter;
+ 
+ 	handle = mlx5_esw_bridge_ingress_flow_create(vport_num, addr, vlan, mlx5_fc_id(counter),
+ 						     bridge);
+ 	if (IS_ERR(handle)) {
+ 		err = PTR_ERR(handle);
+ 		esw_warn(esw->dev, "Failed to create ingress flow(vport=%u,err=%d)\n",
+ 			 vport_num, err);
+ 		goto err_ingress_flow_create;
+ 	}
+ 	entry->ingress_handle = handle;
+ 
+ 	if (bridge->flags & MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG) {
+ 		handle = mlx5_esw_bridge_ingress_filter_flow_create(vport_num, addr, bridge);
+ 		if (IS_ERR(handle)) {
+ 			err = PTR_ERR(handle);
+ 			esw_warn(esw->dev, "Failed to create ingress filter(vport=%u,err=%d)\n",
+ 				 vport_num, err);
+ 			goto err_ingress_filter_flow_create;
+ 		}
+ 		entry->filter_handle = handle;
+ 	}
+ 
+ 	handle = mlx5_esw_bridge_egress_flow_create(vport_num, esw_owner_vhca_id, addr, vlan,
+ 						    bridge);
+ 	if (IS_ERR(handle)) {
+ 		err = PTR_ERR(handle);
+ 		esw_warn(esw->dev, "Failed to create egress flow(vport=%u,err=%d)\n",
+ 			 vport_num, err);
+ 		goto err_egress_flow_create;
+ 	}
+ 	entry->egress_handle = handle;
+ 
+ 	err = rhashtable_insert_fast(&bridge->fdb_ht, &entry->ht_node, fdb_ht_params);
+ 	if (err) {
+ 		esw_warn(esw->dev, "Failed to insert FDB flow(vport=%u,err=%d)\n", vport_num, err);
+ 		goto err_ht_init;
+ 	}
+ 
+ 	if (vlan)
+ 		list_add(&entry->vlan_list, &vlan->fdb_list);
+ 	else
+ 		INIT_LIST_HEAD(&entry->vlan_list);
+ 	list_add(&entry->list, &bridge->fdb_list);
+ 
+ 	trace_mlx5_esw_bridge_fdb_entry_init(entry);
+ 	return entry;
+ 
+ err_ht_init:
+ 	mlx5_del_flow_rules(entry->egress_handle);
+ err_egress_flow_create:
+ 	if (entry->filter_handle)
+ 		mlx5_del_flow_rules(entry->filter_handle);
+ err_ingress_filter_flow_create:
+ 	mlx5_del_flow_rules(entry->ingress_handle);
+ err_ingress_flow_create:
+ 	mlx5_fc_destroy(esw->dev, entry->ingress_counter);
+ err_ingress_fc_create:
+ 	kvfree(entry);
+ 	return ERR_PTR(err);
+ }
+ 
+ int mlx5_esw_bridge_ageing_time_set(u16 vport_num, u16 esw_owner_vhca_id, unsigned long ageing_time,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return -EINVAL;
+ 
+ 	port->bridge->ageing_time = clock_t_to_jiffies(ageing_time);
+ 	return 0;
+ }
+ 
+ int mlx5_esw_bridge_vlan_filtering_set(u16 vport_num, u16 esw_owner_vhca_id, bool enable,
+ 				       struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge *bridge;
+ 	bool filtering;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return -EINVAL;
+ 
+ 	bridge = port->bridge;
+ 	filtering = bridge->flags & MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG;
+ 	if (filtering == enable)
+ 		return 0;
+ 
+ 	mlx5_esw_bridge_fdb_flush(bridge);
+ 	if (enable)
+ 		bridge->flags |= MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG;
+ 	else
+ 		bridge->flags &= ~MLX5_ESW_BRIDGE_VLAN_FILTERING_FLAG;
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5_esw_bridge_vport_init(u16 vport_num, u16 esw_owner_vhca_id, u16 flags,
+ 				      struct mlx5_esw_bridge_offloads *br_offloads,
+ 				      struct mlx5_esw_bridge *bridge)
+ {
+ 	struct mlx5_eswitch *esw = br_offloads->esw;
+ 	struct mlx5_esw_bridge_port *port;
+ 	int err;
+ 
+ 	port = kvzalloc(sizeof(*port), GFP_KERNEL);
+ 	if (!port)
+ 		return -ENOMEM;
+ 
+ 	port->vport_num = vport_num;
+ 	port->esw_owner_vhca_id = esw_owner_vhca_id;
+ 	port->bridge = bridge;
+ 	port->flags |= flags;
+ 	xa_init(&port->vlans);
+ 	err = mlx5_esw_bridge_port_insert(port, br_offloads);
+ 	if (err) {
+ 		esw_warn(esw->dev,
+ 			 "Failed to insert port metadata (vport=%u,esw_owner_vhca_id=%u,err=%d)\n",
+ 			 port->vport_num, port->esw_owner_vhca_id, err);
+ 		goto err_port_insert;
+ 	}
+ 	trace_mlx5_esw_bridge_vport_init(port);
+ 
+ 	return 0;
+ 
+ err_port_insert:
+ 	kvfree(port);
+ 	return err;
+ }
+ 
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  static int mlx5_esw_bridge_vport_cleanup(struct mlx5_esw_bridge_offloads *br_offloads,
 -					 struct mlx5_esw_bridge_port *port)
 +					 struct mlx5_vport *vport)
  {
 -	u16 vport_num = port->vport_num, esw_owner_vhca_id = port->esw_owner_vhca_id;
 -	struct mlx5_esw_bridge *bridge = port->bridge;
 -	struct mlx5_esw_bridge_fdb_entry *entry, *tmp;
 -
 -	list_for_each_entry_safe(entry, tmp, &bridge->fdb_list, list)
 -		if (entry->vport_num == vport_num && entry->esw_owner_vhca_id == esw_owner_vhca_id)
 -			mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
 -
 -	trace_mlx5_esw_bridge_vport_cleanup(port);
 -	mlx5_esw_bridge_port_vlans_flush(port, bridge);
 -	mlx5_esw_bridge_port_erase(port, br_offloads);
 -	kvfree(port);
 -	mlx5_esw_bridge_put(br_offloads, bridge);
 +	mlx5_esw_bridge_put(br_offloads, vport->bridge);
 +	vport->bridge = NULL;
  	return 0;
  }
  
++<<<<<<< HEAD
 +int mlx5_esw_bridge_vport_link(int ifindex, struct mlx5_esw_bridge_offloads *br_offloads,
 +			       struct mlx5_vport *vport, struct netlink_ext_ack *extack)
++=======
+ static int mlx5_esw_bridge_vport_link_with_flags(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 						 u16 flags,
+ 						 struct mlx5_esw_bridge_offloads *br_offloads,
+ 						 struct netlink_ext_ack *extack)
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  {
  	struct mlx5_esw_bridge *bridge;
 -	int err;
 +
 +	WARN_ON(vport->bridge);
  
  	bridge = mlx5_esw_bridge_lookup(ifindex, br_offloads);
  	if (IS_ERR(bridge)) {
@@@ -293,13 -1123,35 +822,39 @@@
  		return PTR_ERR(bridge);
  	}
  
++<<<<<<< HEAD
 +	return mlx5_esw_bridge_vport_init(bridge, vport);
 +}
 +
 +int mlx5_esw_bridge_vport_unlink(int ifindex, struct mlx5_esw_bridge_offloads *br_offloads,
 +				 struct mlx5_vport *vport, struct netlink_ext_ack *extack)
++=======
+ 	err = mlx5_esw_bridge_vport_init(vport_num, esw_owner_vhca_id, flags, br_offloads, bridge);
+ 	if (err) {
+ 		NL_SET_ERR_MSG_MOD(extack, "Error initializing port");
+ 		goto err_vport;
+ 	}
+ 	return 0;
+ 
+ err_vport:
+ 	mlx5_esw_bridge_put(br_offloads, bridge);
+ 	return err;
+ }
+ 
+ int mlx5_esw_bridge_vport_link(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 			       struct mlx5_esw_bridge_offloads *br_offloads,
+ 			       struct netlink_ext_ack *extack)
+ {
+ 	return mlx5_esw_bridge_vport_link_with_flags(ifindex, vport_num, esw_owner_vhca_id, 0,
+ 						     br_offloads, extack);
+ }
+ 
+ int mlx5_esw_bridge_vport_unlink(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				 struct mlx5_esw_bridge_offloads *br_offloads,
+ 				 struct netlink_ext_ack *extack)
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  {
 -	struct mlx5_esw_bridge_port *port;
 -	int err;
 -
 -	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
 -	if (!port) {
 +	if (!vport->bridge) {
  		NL_SET_ERR_MSG_MOD(extack, "Port is not attached to any bridge");
  		return -EINVAL;
  	}
@@@ -308,7 -1160,154 +863,158 @@@
  		return -EINVAL;
  	}
  
++<<<<<<< HEAD
 +	return mlx5_esw_bridge_vport_cleanup(br_offloads, vport);
++=======
+ 	err = mlx5_esw_bridge_vport_cleanup(br_offloads, port);
+ 	if (err)
+ 		NL_SET_ERR_MSG_MOD(extack, "Port cleanup failed");
+ 	return err;
+ }
+ 
+ int mlx5_esw_bridge_vport_peer_link(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads,
+ 				    struct netlink_ext_ack *extack)
+ {
+ 	if (!MLX5_CAP_ESW(br_offloads->esw->dev, merged_eswitch))
+ 		return 0;
+ 
+ 	return mlx5_esw_bridge_vport_link_with_flags(ifindex, vport_num, esw_owner_vhca_id,
+ 						     MLX5_ESW_BRIDGE_PORT_FLAG_PEER,
+ 						     br_offloads, extack);
+ }
+ 
+ int mlx5_esw_bridge_vport_peer_unlink(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				      struct mlx5_esw_bridge_offloads *br_offloads,
+ 				      struct netlink_ext_ack *extack)
+ {
+ 	return mlx5_esw_bridge_vport_unlink(ifindex, vport_num, esw_owner_vhca_id, br_offloads,
+ 					    extack);
+ }
+ 
+ int mlx5_esw_bridge_port_vlan_add(u16 vport_num, u16 esw_owner_vhca_id, u16 vid, u16 flags,
+ 				  struct mlx5_esw_bridge_offloads *br_offloads,
+ 				  struct netlink_ext_ack *extack)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return -EINVAL;
+ 
+ 	vlan = mlx5_esw_bridge_vlan_lookup(vid, port);
+ 	if (vlan) {
+ 		if (vlan->flags == flags)
+ 			return 0;
+ 		mlx5_esw_bridge_vlan_cleanup(port, vlan, port->bridge);
+ 	}
+ 
+ 	vlan = mlx5_esw_bridge_vlan_create(vid, flags, port, br_offloads->esw);
+ 	if (IS_ERR(vlan)) {
+ 		NL_SET_ERR_MSG_MOD(extack, "Failed to create VLAN entry");
+ 		return PTR_ERR(vlan);
+ 	}
+ 	return 0;
+ }
+ 
+ void mlx5_esw_bridge_port_vlan_del(u16 vport_num, u16 esw_owner_vhca_id, u16 vid,
+ 				   struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge_vlan *vlan;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return;
+ 
+ 	vlan = mlx5_esw_bridge_vlan_lookup(vid, port);
+ 	if (!vlan)
+ 		return;
+ 	mlx5_esw_bridge_vlan_cleanup(port, vlan, port->bridge);
+ }
+ 
+ void mlx5_esw_bridge_fdb_create(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry;
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge *bridge;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return;
+ 
+ 	bridge = port->bridge;
+ 	entry = mlx5_esw_bridge_fdb_entry_init(dev, vport_num, esw_owner_vhca_id, fdb_info->addr,
+ 					       fdb_info->vid, fdb_info->added_by_user,
+ 					       port->flags & MLX5_ESW_BRIDGE_PORT_FLAG_PEER,
+ 					       br_offloads->esw, bridge);
+ 	if (IS_ERR(entry))
+ 		return;
+ 
+ 	if (entry->flags & MLX5_ESW_BRIDGE_FLAG_ADDED_BY_USER)
+ 		mlx5_esw_bridge_fdb_offload_notify(dev, entry->key.addr, entry->key.vid,
+ 						   SWITCHDEV_FDB_OFFLOADED);
+ 	else if (!(entry->flags & MLX5_ESW_BRIDGE_FLAG_PEER))
+ 		/* Take over dynamic entries to prevent kernel bridge from aging them out. */
+ 		mlx5_esw_bridge_fdb_offload_notify(dev, entry->key.addr, entry->key.vid,
+ 						   SWITCHDEV_FDB_ADD_TO_BRIDGE);
+ }
+ 
+ void mlx5_esw_bridge_fdb_remove(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info)
+ {
+ 	struct mlx5_eswitch *esw = br_offloads->esw;
+ 	struct mlx5_esw_bridge_fdb_entry *entry;
+ 	struct mlx5_esw_bridge_fdb_key key;
+ 	struct mlx5_esw_bridge_port *port;
+ 	struct mlx5_esw_bridge *bridge;
+ 
+ 	port = mlx5_esw_bridge_port_lookup(vport_num, esw_owner_vhca_id, br_offloads);
+ 	if (!port)
+ 		return;
+ 
+ 	bridge = port->bridge;
+ 	ether_addr_copy(key.addr, fdb_info->addr);
+ 	key.vid = fdb_info->vid;
+ 	entry = rhashtable_lookup_fast(&bridge->fdb_ht, &key, fdb_ht_params);
+ 	if (!entry) {
+ 		esw_warn(esw->dev,
+ 			 "FDB entry with specified key not found (MAC=%pM,vid=%u,vport=%u)\n",
+ 			 key.addr, key.vid, vport_num);
+ 		return;
+ 	}
+ 
+ 	mlx5_esw_bridge_fdb_del_notify(entry);
+ 	mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
+ }
+ 
+ void mlx5_esw_bridge_update(struct mlx5_esw_bridge_offloads *br_offloads)
+ {
+ 	struct mlx5_esw_bridge_fdb_entry *entry, *tmp;
+ 	struct mlx5_esw_bridge *bridge;
+ 
+ 	list_for_each_entry(bridge, &br_offloads->bridges, list) {
+ 		list_for_each_entry_safe(entry, tmp, &bridge->fdb_list, list) {
+ 			unsigned long lastuse =
+ 				(unsigned long)mlx5_fc_query_lastuse(entry->ingress_counter);
+ 
+ 			if (entry->flags & MLX5_ESW_BRIDGE_FLAG_ADDED_BY_USER)
+ 				continue;
+ 
+ 			if (time_after(lastuse, entry->lastuse)) {
+ 				mlx5_esw_bridge_fdb_entry_refresh(lastuse, entry);
+ 			} else if (!(entry->flags & MLX5_ESW_BRIDGE_FLAG_PEER) &&
+ 				   time_is_before_jiffies(entry->lastuse + bridge->ageing_time)) {
+ 				mlx5_esw_bridge_fdb_del_notify(entry);
+ 				mlx5_esw_bridge_fdb_entry_cleanup(entry, bridge);
+ 			}
+ 		}
+ 	}
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  }
  
  static void mlx5_esw_bridge_flush(struct mlx5_esw_bridge_offloads *br_offloads)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
index 319b6f1db0ba,a4f04f3f5b11..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
@@@ -22,9 -34,33 +22,40 @@@ struct mlx5_esw_bridge_offloads 
  
  struct mlx5_esw_bridge_offloads *mlx5_esw_bridge_init(struct mlx5_eswitch *esw);
  void mlx5_esw_bridge_cleanup(struct mlx5_eswitch *esw);
++<<<<<<< HEAD
 +int mlx5_esw_bridge_vport_link(int ifindex, struct mlx5_esw_bridge_offloads *br_offloads,
 +			       struct mlx5_vport *vport, struct netlink_ext_ack *extack);
 +int mlx5_esw_bridge_vport_unlink(int ifindex, struct mlx5_esw_bridge_offloads *br_offloads,
 +				 struct mlx5_vport *vport, struct netlink_ext_ack *extack);
++=======
+ int mlx5_esw_bridge_vport_link(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 			       struct mlx5_esw_bridge_offloads *br_offloads,
+ 			       struct netlink_ext_ack *extack);
+ int mlx5_esw_bridge_vport_unlink(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				 struct mlx5_esw_bridge_offloads *br_offloads,
+ 				 struct netlink_ext_ack *extack);
+ int mlx5_esw_bridge_vport_peer_link(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads,
+ 				    struct netlink_ext_ack *extack);
+ int mlx5_esw_bridge_vport_peer_unlink(int ifindex, u16 vport_num, u16 esw_owner_vhca_id,
+ 				      struct mlx5_esw_bridge_offloads *br_offloads,
+ 				      struct netlink_ext_ack *extack);
+ void mlx5_esw_bridge_fdb_create(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info);
+ void mlx5_esw_bridge_fdb_remove(struct net_device *dev, u16 vport_num, u16 esw_owner_vhca_id,
+ 				struct mlx5_esw_bridge_offloads *br_offloads,
+ 				struct switchdev_notifier_fdb_info *fdb_info);
+ void mlx5_esw_bridge_update(struct mlx5_esw_bridge_offloads *br_offloads);
+ int mlx5_esw_bridge_ageing_time_set(u16 vport_num, u16 esw_owner_vhca_id, unsigned long ageing_time,
+ 				    struct mlx5_esw_bridge_offloads *br_offloads);
+ int mlx5_esw_bridge_vlan_filtering_set(u16 vport_num, u16 esw_owner_vhca_id, bool enable,
+ 				       struct mlx5_esw_bridge_offloads *br_offloads);
+ int mlx5_esw_bridge_port_vlan_add(u16 vport_num, u16 esw_owner_vhca_id, u16 vid, u16 flags,
+ 				  struct mlx5_esw_bridge_offloads *br_offloads,
+ 				  struct netlink_ext_ack *extack);
+ void mlx5_esw_bridge_port_vlan_del(u16 vport_num, u16 esw_owner_vhca_id, u16 vid,
+ 				   struct mlx5_esw_bridge_offloads *br_offloads);
++>>>>>>> c358ea1741bc (net/mlx5: Bridge, allow merged eswitch connectivity)
  
  #endif /* __MLX5_ESW_BRIDGE_H__ */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/bridge_priv.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/diag/bridge_tracepoint.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/rep/bridge.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/bridge.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/bridge_priv.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/esw/diag/bridge_tracepoint.h
