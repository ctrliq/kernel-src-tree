mm/memremap_pages: convert to 'struct range'

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Dan Williams <dan.j.williams@intel.com>
commit a4574f63edc6f76fb46dcd65d3eb4d5a8e23ba38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/a4574f63.failed

The 'struct resource' in 'struct dev_pagemap' is only used for holding
resource span information.  The other fields, 'name', 'flags', 'desc',
'parent', 'sibling', and 'child' are all unused wasted space.

This is in preparation for introducing a multi-range extension of
devm_memremap_pages().

The bulk of this change is unwinding all the places internal to libnvdimm
that used 'struct resource' unnecessarily, and replacing instances of
'struct dev_pagemap'.res with 'struct dev_pagemap'.range.

P2PDMA had a minor usage of the resource flags field, but only to report
failures with "%pR".  That is replaced with an open coded print of the
range.

[dan.carpenter@oracle.com: mm/hmm/test: use after free in dmirror_allocate_chunk()]
  Link: https://lkml.kernel.org/r/20200926121402.GA7467@kadam

	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>	[xen]
	Cc: Paul Mackerras <paulus@ozlabs.org>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Vishal Verma <vishal.l.verma@intel.com>
	Cc: Vivek Goyal <vgoyal@redhat.com>
	Cc: Dave Jiang <dave.jiang@intel.com>
	Cc: Ben Skeggs <bskeggs@redhat.com>
	Cc: David Airlie <airlied@linux.ie>
	Cc: Daniel Vetter <daniel@ffwll.ch>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Bjorn Helgaas <bhelgaas@google.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: Stefano Stabellini <sstabellini@kernel.org>
	Cc: "Jérôme Glisse" <jglisse@redhat.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
	Cc: Ard Biesheuvel <ardb@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brice Goglin <Brice.Goglin@inria.fr>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Hulk Robot <hulkci@huawei.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jason Gunthorpe <jgg@mellanox.com>
	Cc: Jason Yan <yanaijie@huawei.com>
	Cc: Jeff Moyer <jmoyer@redhat.com>
	Cc: Jia He <justin.he@arm.com>
	Cc: Joao Martins <joao.m.martins@oracle.com>
	Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
	Cc: kernel test robot <lkp@intel.com>
	Cc: Mike Rapoport <rppt@linux.ibm.com>
	Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
	Cc: Randy Dunlap <rdunlap@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Wei Yang <richard.weiyang@linux.alibaba.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lkml.kernel.org/r/159643103173.4062302.768998885691711532.stgit@dwillia2-desk3.amr.corp.intel.com
Link: https://lkml.kernel.org/r/160106115761.30709.13539840236873663620.stgit@dwillia2-desk3.amr.corp.intel.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a4574f63edc6f76fb46dcd65d3eb4d5a8e23ba38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dax/bus.c
#	drivers/dax/bus.h
#	drivers/dax/device.c
#	drivers/dax/hmem/hmem.c
#	drivers/dax/pmem/core.c
#	drivers/gpu/drm/nouveau/nouveau_dmem.c
#	drivers/pci/p2pdma.c
#	drivers/xen/unpopulated-alloc.c
#	include/linux/memremap.h
#	lib/test_hmm.c
#	mm/memremap.c
diff --cc drivers/dax/bus.c
index dffa4655e128,00fa73a8dfb4..000000000000
--- a/drivers/dax/bus.c
+++ b/drivers/dax/bus.c
@@@ -226,7 -515,8 +226,12 @@@ static void dax_region_unregister(void 
  }
  
  struct dax_region *alloc_dax_region(struct device *parent, int region_id,
++<<<<<<< HEAD
 +		struct resource *res, int target_node, unsigned int align)
++=======
+ 		struct range *range, int target_node, unsigned int align,
+ 		unsigned long flags)
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  {
  	struct dax_region *dax_region;
  
@@@ -255,6 -544,13 +260,16 @@@
  	dax_region->align = align;
  	dax_region->dev = parent;
  	dax_region->target_node = target_node;
++<<<<<<< HEAD
++=======
+ 	ida_init(&dax_region->ida);
+ 	dax_region->res = (struct resource) {
+ 		.start = range->start,
+ 		.end = range->end,
+ 		.flags = IORESOURCE_MEM | flags,
+ 	};
+ 
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	if (sysfs_create_groups(&parent->kobj, dax_region_attribute_groups)) {
  		kfree(dax_region);
  		return NULL;
diff --cc drivers/dax/bus.h
index 299c2e7fac09,72b92f95509f..000000000000
--- a/drivers/dax/bus.h
+++ b/drivers/dax/bus.h
@@@ -9,8 -10,11 +9,13 @@@ struct resource
  struct dax_device;
  struct dax_region;
  void dax_region_put(struct dax_region *dax_region);
 -
 -#define IORESOURCE_DAX_STATIC (1UL << 0)
  struct dax_region *alloc_dax_region(struct device *parent, int region_id,
++<<<<<<< HEAD
 +		struct resource *res, int target_node, unsigned int align);
++=======
+ 		struct range *range, int target_node, unsigned int align,
+ 		unsigned long flags);
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  
  enum dev_dax_subsys {
  	DEV_DAX_BUS = 0, /* zeroed dev_dax_data picks this by default */
diff --cc drivers/dax/device.c
index eaa651ddeccd,a14448bca83d..000000000000
--- a/drivers/dax/device.c
+++ b/drivers/dax/device.c
@@@ -408,8 -411,15 +408,20 @@@ int dev_dax_probe(struct device *dev
  		return -EBUSY;
  	}
  
++<<<<<<< HEAD
 +	dev_dax->pgmap.type = MEMORY_DEVICE_DEVDAX;
 +	addr = devm_memremap_pages(dev, &dev_dax->pgmap);
++=======
+ 	pgmap = dev_dax->pgmap;
+ 	if (!pgmap) {
+ 		pgmap = devm_kzalloc(dev, sizeof(*pgmap), GFP_KERNEL);
+ 		if (!pgmap)
+ 			return -ENOMEM;
+ 		pgmap->range = *range;
+ 	}
+ 	pgmap->type = MEMORY_DEVICE_GENERIC;
+ 	addr = devm_memremap_pages(dev, pgmap);
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	if (IS_ERR(addr))
  		return PTR_ERR(addr);
  
diff --cc drivers/dax/hmem/hmem.c
index b84fe17178d8,1a3347bb6143..000000000000
--- a/drivers/dax/hmem/hmem.c
+++ b/drivers/dax/hmem/hmem.c
@@@ -20,10 -20,10 +21,17 @@@ static int dax_hmem_probe(struct platfo
  		return -ENOMEM;
  
  	mri = dev->platform_data;
++<<<<<<< HEAD
 +	memcpy(&pgmap.res, res, sizeof(*res));
 +
 +	dax_region = alloc_dax_region(dev, pdev->id, res, mri->target_node,
 +			PMD_SIZE);
++=======
+ 	range.start = res->start;
+ 	range.end = res->end;
+ 	dax_region = alloc_dax_region(dev, pdev->id, &range, mri->target_node,
+ 			PMD_SIZE, 0);
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	if (!dax_region)
  		return -ENOMEM;
  
diff --cc drivers/dax/pmem/core.c
index 615ebe8bfa98,62b26bfceab1..000000000000
--- a/drivers/dax/pmem/core.c
+++ b/drivers/dax/pmem/core.c
@@@ -50,11 -50,12 +50,20 @@@ struct dev_dax *__dax_pmem_probe(struc
  	if (rc != 2)
  		return ERR_PTR(-EINVAL);
  
++<<<<<<< HEAD
 +	/* adjust the dax_region resource to the start of data */
 +	memcpy(&res, &pgmap.res, sizeof(res));
 +	res.start += offset;
 +	dax_region = alloc_dax_region(dev, region_id, &res,
 +			nd_region->target_node, le32_to_cpu(pfn_sb->align));
++=======
+ 	/* adjust the dax_region range to the start of data */
+ 	range = pgmap.range;
+ 	range.start += offset,
+ 	dax_region = alloc_dax_region(dev, region_id, &range,
+ 			nd_region->target_node, le32_to_cpu(pfn_sb->align),
+ 			IORESOURCE_DAX_STATIC);
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	if (!dax_region)
  		return ERR_PTR(-ENOMEM);
  
@@@ -63,6 -64,7 +72,10 @@@
  		.id = id,
  		.pgmap = &pgmap,
  		.subsys = subsys,
++<<<<<<< HEAD
++=======
+ 		.size = range_len(&range),
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	};
  	dev_dax = devm_create_dev_dax(&data);
  
diff --cc drivers/gpu/drm/nouveau/nouveau_dmem.c
index 92987daa5e17,25811ed7e274..000000000000
--- a/drivers/gpu/drm/nouveau/nouveau_dmem.c
+++ b/drivers/gpu/drm/nouveau/nouveau_dmem.c
@@@ -251,7 -251,6 +251,10 @@@ nouveau_dmem_chunk_alloc(struct nouveau
  	chunk->pagemap.type = MEMORY_DEVICE_PRIVATE;
  	chunk->pagemap.range.start = res->start;
  	chunk->pagemap.range.end = res->end;
++<<<<<<< HEAD
 +	chunk->pagemap.nr_range = 1;
++=======
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	chunk->pagemap.ops = &nouveau_dmem_pagemap_ops;
  	chunk->pagemap.owner = drm->dev;
  
diff --cc drivers/pci/p2pdma.c
index 9736ba71ccbf,256850513813..000000000000
--- a/drivers/pci/p2pdma.c
+++ b/drivers/pci/p2pdma.c
@@@ -157,14 -180,17 +157,21 @@@ int pci_p2pdma_add_resource(struct pci_
  			return error;
  	}
  
 -	p2p_pgmap = devm_kzalloc(&pdev->dev, sizeof(*p2p_pgmap), GFP_KERNEL);
 -	if (!p2p_pgmap)
 +	pgmap = devm_kzalloc(&pdev->dev, sizeof(*pgmap), GFP_KERNEL);
 +	if (!pgmap)
  		return -ENOMEM;
++<<<<<<< HEAD
 +	pgmap->res.start = pci_resource_start(pdev, bar) + offset;
 +	pgmap->res.end = pgmap->res.start + size - 1;
 +	pgmap->res.flags = pci_resource_flags(pdev, bar);
++=======
+ 
+ 	pgmap = &p2p_pgmap->pgmap;
+ 	pgmap->range.start = pci_resource_start(pdev, bar) + offset;
+ 	pgmap->range.end = pgmap->range.start + size - 1;
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	pgmap->type = MEMORY_DEVICE_PCI_P2PDMA;
 -
 -	p2p_pgmap->provider = pdev;
 -	p2p_pgmap->bus_offset = pci_bus_address(pdev, bar) -
 +	pgmap->pci_p2pdma_bus_offset = pci_bus_address(pdev, bar) -
  		pci_resource_start(pdev, bar);
  
  	addr = devm_memremap_pages(&pdev->dev, pgmap);
diff --cc include/linux/memremap.h
index ecd38ce53a36,d0dd261d87c0..000000000000
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@@ -1,9 -1,9 +1,10 @@@
  /* SPDX-License-Identifier: GPL-2.0 */
  #ifndef _LINUX_MEMREMAP_H_
  #define _LINUX_MEMREMAP_H_
+ #include <linux/range.h>
  #include <linux/ioport.h>
  #include <linux/percpu-refcount.h>
 +#include <linux/mm_types.h>  /* needed in RHEL8 for dev_pagemap & vm_fault_t */
  
  struct resource;
  struct device;
@@@ -123,21 -106,15 +124,25 @@@ struct dev_pagemap_ops 
   *	foreign ZONE_DEVICE memory is accessed.
   */
  struct dev_pagemap {
 +	RH_KABI_DEPRECATE(dev_page_fault_t, page_fault)
 +	RH_KABI_DEPRECATE(dev_page_free_t, page_free)
  	struct vmem_altmap altmap;
++<<<<<<< HEAD
 +	RH_KABI_DEPRECATE(bool, altmap_valid)
 +	struct resource res;
++=======
+ 	struct range range;
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	struct percpu_ref *ref;
 -	struct percpu_ref internal_ref;
 -	struct completion done;
 +	RH_KABI_DEPRECATE(struct device *, dev)
 +	RH_KABI_DEPRECATE(void *, data)
  	enum memory_type type;
 -	unsigned int flags;
 -	const struct dev_pagemap_ops *ops;
 -	void *owner;
 +	u64 pci_p2pdma_bus_offset;
 +	RH_KABI_EXTEND(const struct dev_pagemap_ops *ops)
 +	RH_KABI_EXTEND(unsigned int flags)
 +	RH_KABI_EXTEND(struct percpu_ref internal_ref)
 +	RH_KABI_EXTEND(struct completion done)
 +	RH_KABI_EXTEND(void *owner)
  };
  
  static inline struct vmem_altmap *pgmap_altmap(struct dev_pagemap *pgmap)
diff --cc mm/memremap.c
index 6bc3fa8cccb8,d958d348b3ca..000000000000
--- a/mm/memremap.c
+++ b/mm/memremap.c
@@@ -321,9 -322,8 +322,14 @@@ void *memremap_pages(struct dev_pagema
  		struct zone *zone;
  
  		zone = &NODE_DATA(nid)->node_zones[ZONE_DEVICE];
++<<<<<<< HEAD
 +		move_pfn_range_to_zone(zone, PHYS_PFN(res->start),
 +				PHYS_PFN(resource_size(res)), params.altmap,
 +				MIGRATE_MOVABLE);
++=======
+ 		move_pfn_range_to_zone(zone, PHYS_PFN(range->start),
+ 				PHYS_PFN(range_len(range)), params.altmap);
++>>>>>>> a4574f63edc6 (mm/memremap_pages: convert to 'struct range')
  	}
  
  	mem_hotplug_done();
* Unmerged path drivers/xen/unpopulated-alloc.c
* Unmerged path lib/test_hmm.c
diff --git a/arch/powerpc/kvm/book3s_hv_uvmem.c b/arch/powerpc/kvm/book3s_hv_uvmem.c
index ccd2bc5c5973..2b8ef980caa1 100644
--- a/arch/powerpc/kvm/book3s_hv_uvmem.c
+++ b/arch/powerpc/kvm/book3s_hv_uvmem.c
@@ -687,9 +687,9 @@ static struct page *kvmppc_uvmem_get_page(unsigned long gpa, struct kvm *kvm)
 	struct kvmppc_uvmem_page_pvt *pvt;
 	unsigned long pfn_last, pfn_first;
 
-	pfn_first = kvmppc_uvmem_pgmap.res.start >> PAGE_SHIFT;
+	pfn_first = kvmppc_uvmem_pgmap.range.start >> PAGE_SHIFT;
 	pfn_last = pfn_first +
-		   (resource_size(&kvmppc_uvmem_pgmap.res) >> PAGE_SHIFT);
+		   (range_len(&kvmppc_uvmem_pgmap.range) >> PAGE_SHIFT);
 
 	spin_lock(&kvmppc_uvmem_bitmap_lock);
 	bit = find_first_zero_bit(kvmppc_uvmem_bitmap,
@@ -1007,7 +1007,7 @@ static vm_fault_t kvmppc_uvmem_migrate_to_ram(struct vm_fault *vmf)
 static void kvmppc_uvmem_page_free(struct page *page)
 {
 	unsigned long pfn = page_to_pfn(page) -
-			(kvmppc_uvmem_pgmap.res.start >> PAGE_SHIFT);
+			(kvmppc_uvmem_pgmap.range.start >> PAGE_SHIFT);
 	struct kvmppc_uvmem_page_pvt *pvt;
 
 	spin_lock(&kvmppc_uvmem_bitmap_lock);
@@ -1170,7 +1170,8 @@ int kvmppc_uvmem_init(void)
 	}
 
 	kvmppc_uvmem_pgmap.type = MEMORY_DEVICE_PRIVATE;
-	kvmppc_uvmem_pgmap.res = *res;
+	kvmppc_uvmem_pgmap.range.start = res->start;
+	kvmppc_uvmem_pgmap.range.end = res->end;
 	kvmppc_uvmem_pgmap.ops = &kvmppc_uvmem_ops;
 	/* just one global instance: */
 	kvmppc_uvmem_pgmap.owner = &kvmppc_uvmem_pgmap;
@@ -1205,7 +1206,7 @@ void kvmppc_uvmem_free(void)
 		return;
 
 	memunmap_pages(&kvmppc_uvmem_pgmap);
-	release_mem_region(kvmppc_uvmem_pgmap.res.start,
-			   resource_size(&kvmppc_uvmem_pgmap.res));
+	release_mem_region(kvmppc_uvmem_pgmap.range.start,
+			   range_len(&kvmppc_uvmem_pgmap.range));
 	kfree(kvmppc_uvmem_bitmap);
 }
* Unmerged path drivers/dax/bus.c
* Unmerged path drivers/dax/bus.h
* Unmerged path drivers/dax/device.c
* Unmerged path drivers/dax/hmem/hmem.c
* Unmerged path drivers/dax/pmem/core.c
* Unmerged path drivers/gpu/drm/nouveau/nouveau_dmem.c
diff --git a/drivers/nvdimm/badrange.c b/drivers/nvdimm/badrange.c
index e068d72b4357..8a09d5ed0486 100644
--- a/drivers/nvdimm/badrange.c
+++ b/drivers/nvdimm/badrange.c
@@ -219,7 +219,7 @@ static void __add_badblock_range(struct badblocks *bb, u64 ns_offset, u64 len)
 }
 
 static void badblocks_populate(struct badrange *badrange,
-		struct badblocks *bb, const struct resource *res)
+		struct badblocks *bb, const struct range *range)
 {
 	struct badrange_entry *bre;
 
@@ -230,34 +230,34 @@ static void badblocks_populate(struct badrange *badrange,
 		u64 bre_end = bre->start + bre->length - 1;
 
 		/* Discard intervals with no intersection */
-		if (bre_end < res->start)
+		if (bre_end < range->start)
 			continue;
-		if (bre->start >  res->end)
+		if (bre->start > range->end)
 			continue;
 		/* Deal with any overlap after start of the namespace */
-		if (bre->start >= res->start) {
+		if (bre->start >= range->start) {
 			u64 start = bre->start;
 			u64 len;
 
-			if (bre_end <= res->end)
+			if (bre_end <= range->end)
 				len = bre->length;
 			else
-				len = res->start + resource_size(res)
+				len = range->start + range_len(range)
 					- bre->start;
-			__add_badblock_range(bb, start - res->start, len);
+			__add_badblock_range(bb, start - range->start, len);
 			continue;
 		}
 		/*
 		 * Deal with overlap for badrange starting before
 		 * the namespace.
 		 */
-		if (bre->start < res->start) {
+		if (bre->start < range->start) {
 			u64 len;
 
-			if (bre_end < res->end)
-				len = bre->start + bre->length - res->start;
+			if (bre_end < range->end)
+				len = bre->start + bre->length - range->start;
 			else
-				len = resource_size(res);
+				len = range_len(range);
 			__add_badblock_range(bb, 0, len);
 		}
 	}
@@ -275,7 +275,7 @@ static void badblocks_populate(struct badrange *badrange,
  * and add badblocks entries for all matching sub-ranges
  */
 void nvdimm_badblocks_populate(struct nd_region *nd_region,
-		struct badblocks *bb, const struct resource *res)
+		struct badblocks *bb, const struct range *range)
 {
 	struct nvdimm_bus *nvdimm_bus;
 
@@ -287,7 +287,7 @@ void nvdimm_badblocks_populate(struct nd_region *nd_region,
 	nvdimm_bus = walk_to_nvdimm_bus(&nd_region->dev);
 
 	nvdimm_bus_lock(&nvdimm_bus->dev);
-	badblocks_populate(&nvdimm_bus->badrange, bb, res);
+	badblocks_populate(&nvdimm_bus->badrange, bb, range);
 	nvdimm_bus_unlock(&nvdimm_bus->dev);
 }
 EXPORT_SYMBOL_GPL(nvdimm_badblocks_populate);
diff --git a/drivers/nvdimm/claim.c b/drivers/nvdimm/claim.c
index 37a0ea954792..1a1468c555fa 100644
--- a/drivers/nvdimm/claim.c
+++ b/drivers/nvdimm/claim.c
@@ -311,13 +311,16 @@ static int nsio_rw_bytes(struct nd_namespace_common *ndns,
 int devm_nsio_enable(struct device *dev, struct nd_namespace_io *nsio,
 		resource_size_t size)
 {
-	struct resource *res = &nsio->res;
 	struct nd_namespace_common *ndns = &nsio->common;
+	struct range range = {
+		.start = nsio->res.start,
+		.end = nsio->res.end,
+	};
 
 	nsio->size = size;
-	if (!devm_request_mem_region(dev, res->start, size,
+	if (!devm_request_mem_region(dev, range.start, size,
 				dev_name(&ndns->dev))) {
-		dev_warn(dev, "could not reserve region %pR\n", res);
+		dev_warn(dev, "could not reserve region %pR\n", &nsio->res);
 		return -EBUSY;
 	}
 
@@ -325,9 +328,9 @@ int devm_nsio_enable(struct device *dev, struct nd_namespace_io *nsio,
 	if (devm_init_badblocks(dev, &nsio->bb))
 		return -ENOMEM;
 	nvdimm_badblocks_populate(to_nd_region(ndns->dev.parent), &nsio->bb,
-			&nsio->res);
+			&range);
 
-	nsio->addr = devm_memremap(dev, res->start, size, ARCH_MEMREMAP_PMEM);
+	nsio->addr = devm_memremap(dev, range.start, size, ARCH_MEMREMAP_PMEM);
 
 	return PTR_ERR_OR_ZERO(nsio->addr);
 }
diff --git a/drivers/nvdimm/nd.h b/drivers/nvdimm/nd.h
index 7db83ed3a791..0563bb60a9a5 100644
--- a/drivers/nvdimm/nd.h
+++ b/drivers/nvdimm/nd.h
@@ -389,8 +389,9 @@ int nvdimm_namespace_detach_btt(struct nd_btt *nd_btt);
 const char *nvdimm_namespace_disk_name(struct nd_namespace_common *ndns,
 		char *name);
 unsigned int pmem_sector_size(struct nd_namespace_common *ndns);
+struct range;
 void nvdimm_badblocks_populate(struct nd_region *nd_region,
-		struct badblocks *bb, const struct resource *res);
+		struct badblocks *bb, const struct range *range);
 int devm_namespace_enable(struct device *dev, struct nd_namespace_common *ndns,
 		resource_size_t size);
 void devm_namespace_disable(struct device *dev,
diff --git a/drivers/nvdimm/pfn_devs.c b/drivers/nvdimm/pfn_devs.c
index e6f29d1f118d..cda8a60b4c82 100644
--- a/drivers/nvdimm/pfn_devs.c
+++ b/drivers/nvdimm/pfn_devs.c
@@ -647,7 +647,7 @@ static unsigned long init_altmap_reserve(resource_size_t base)
 
 static int __nvdimm_setup_pfn(struct nd_pfn *nd_pfn, struct dev_pagemap *pgmap)
 {
-	struct resource *res = &pgmap->res;
+	struct range *range = &pgmap->range;
 	struct vmem_altmap *altmap = &pgmap->altmap;
 	struct nd_pfn_sb *pfn_sb = nd_pfn->pfn_sb;
 	u64 offset = le64_to_cpu(pfn_sb->dataoff);
@@ -664,16 +664,16 @@ static int __nvdimm_setup_pfn(struct nd_pfn *nd_pfn, struct dev_pagemap *pgmap)
 		.end_pfn = PHYS_PFN(end),
 	};
 
-	memcpy(res, &nsio->res, sizeof(*res));
-	res->start += start_pad;
-	res->end -= end_trunc;
-
+	*range = (struct range) {
+		.start = nsio->res.start + start_pad,
+		.end = nsio->res.end - end_trunc,
+	};
 	if (nd_pfn->mode == PFN_MODE_RAM) {
 		if (offset < reserve)
 			return -EINVAL;
 		nd_pfn->npfns = le64_to_cpu(pfn_sb->npfns);
 	} else if (nd_pfn->mode == PFN_MODE_PMEM) {
-		nd_pfn->npfns = PHYS_PFN((resource_size(res) - offset));
+		nd_pfn->npfns = PHYS_PFN((range_len(range) - offset));
 		if (le64_to_cpu(nd_pfn->pfn_sb->npfns) > nd_pfn->npfns)
 			dev_info(&nd_pfn->dev,
 					"number of pfns truncated from %lld to %ld\n",
diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c
index 974bf22c207b..eebdd85c43bd 100644
--- a/drivers/nvdimm/pmem.c
+++ b/drivers/nvdimm/pmem.c
@@ -384,7 +384,7 @@ static int pmem_attach_disk(struct device *dev,
 	struct nd_region *nd_region = to_nd_region(dev->parent);
 	int nid = dev_to_node(dev), fua;
 	struct resource *res = &nsio->res;
-	struct resource bb_res;
+	struct range bb_range;
 	struct nd_pfn *nd_pfn = NULL;
 	struct dax_device *dax_dev;
 	struct nd_pfn_sb *pfn_sb;
@@ -443,24 +443,26 @@ static int pmem_attach_disk(struct device *dev,
 		pfn_sb = nd_pfn->pfn_sb;
 		pmem->data_offset = le64_to_cpu(pfn_sb->dataoff);
 		pmem->pfn_pad = resource_size(res) -
-			resource_size(&pmem->pgmap.res);
+			range_len(&pmem->pgmap.range);
 		pmem->pfn_flags |= PFN_MAP;
-		memcpy(&bb_res, &pmem->pgmap.res, sizeof(bb_res));
-		bb_res.start += pmem->data_offset;
+		bb_range = pmem->pgmap.range;
+		bb_range.start += pmem->data_offset;
 	} else if (pmem_should_map_pages(dev)) {
-		memcpy(&pmem->pgmap.res, &nsio->res, sizeof(pmem->pgmap.res));
+		pmem->pgmap.range.start = res->start;
+		pmem->pgmap.range.end = res->end;
 		pmem->pgmap.type = MEMORY_DEVICE_FS_DAX;
 		pmem->pgmap.ops = &fsdax_pagemap_ops;
 		addr = devm_memremap_pages(dev, &pmem->pgmap);
 		pmem->pfn_flags |= PFN_MAP;
-		memcpy(&bb_res, &pmem->pgmap.res, sizeof(bb_res));
+		bb_range = pmem->pgmap.range;
 	} else {
 		if (devm_add_action_or_reset(dev, pmem_release_queue,
 					&pmem->pgmap))
 			return -ENOMEM;
 		addr = devm_memremap(dev, pmem->phys_addr,
 				pmem->size, ARCH_MEMREMAP_PMEM);
-		memcpy(&bb_res, &nsio->res, sizeof(bb_res));
+		bb_range.start =  res->start;
+		bb_range.end = res->end;
 	}
 
 	if (IS_ERR(addr))
@@ -489,7 +491,7 @@ static int pmem_attach_disk(struct device *dev,
 			/ 512);
 	if (devm_init_badblocks(dev, &pmem->bb))
 		return -ENOMEM;
-	nvdimm_badblocks_populate(nd_region, &pmem->bb, &bb_res);
+	nvdimm_badblocks_populate(nd_region, &pmem->bb, &bb_range);
 	disk->bb = &pmem->bb;
 
 	if (is_nvdimm_sync(nd_region))
@@ -600,8 +602,8 @@ static void nd_pmem_notify(struct device *dev, enum nvdimm_event event)
 	resource_size_t offset = 0, end_trunc = 0;
 	struct nd_namespace_common *ndns;
 	struct nd_namespace_io *nsio;
-	struct resource res;
 	struct badblocks *bb;
+	struct range range;
 	struct kernfs_node *bb_state;
 
 	if (event != NVDIMM_REVALIDATE_POISON)
@@ -637,9 +639,9 @@ static void nd_pmem_notify(struct device *dev, enum nvdimm_event event)
 		nsio = to_nd_namespace_io(&ndns->dev);
 	}
 
-	res.start = nsio->res.start + offset;
-	res.end = nsio->res.end - end_trunc;
-	nvdimm_badblocks_populate(nd_region, bb, &res);
+	range.start = nsio->res.start + offset;
+	range.end = nsio->res.end - end_trunc;
+	nvdimm_badblocks_populate(nd_region, bb, &range);
 	if (bb_state)
 		sysfs_notify_dirent(bb_state);
 }
diff --git a/drivers/nvdimm/region.c b/drivers/nvdimm/region.c
index 22224b21c34d..d0028c85b0ae 100644
--- a/drivers/nvdimm/region.c
+++ b/drivers/nvdimm/region.c
@@ -43,7 +43,10 @@ static int nd_region_probe(struct device *dev)
 		return rc;
 
 	if (is_memory(&nd_region->dev)) {
-		struct resource ndr_res;
+		struct range range = {
+			.start = nd_region->ndr_start,
+			.end = nd_region->ndr_start + nd_region->ndr_size - 1,
+		};
 
 		if (devm_init_badblocks(dev, &nd_region->bb))
 			return -ENODEV;
@@ -52,9 +55,7 @@ static int nd_region_probe(struct device *dev)
 		if (!nd_region->bb_state)
 			dev_warn(&nd_region->dev,
 					"'badblocks' notification disabled\n");
-		ndr_res.start = nd_region->ndr_start;
-		ndr_res.end = nd_region->ndr_start + nd_region->ndr_size - 1;
-		nvdimm_badblocks_populate(nd_region, &nd_region->bb, &ndr_res);
+		nvdimm_badblocks_populate(nd_region, &nd_region->bb, &range);
 	}
 
 	rc = nd_region_register_namespaces(nd_region, &err);
@@ -129,14 +130,16 @@ static void nd_region_notify(struct device *dev, enum nvdimm_event event)
 {
 	if (event == NVDIMM_REVALIDATE_POISON) {
 		struct nd_region *nd_region = to_nd_region(dev);
-		struct resource res;
 
 		if (is_memory(&nd_region->dev)) {
-			res.start = nd_region->ndr_start;
-			res.end = nd_region->ndr_start +
-				nd_region->ndr_size - 1;
+			struct range range = {
+				.start = nd_region->ndr_start,
+				.end = nd_region->ndr_start +
+					nd_region->ndr_size - 1,
+			};
+
 			nvdimm_badblocks_populate(nd_region,
-					&nd_region->bb, &res);
+					&nd_region->bb, &range);
 			if (nd_region->bb_state)
 				sysfs_notify_dirent(nd_region->bb_state);
 		}
* Unmerged path drivers/pci/p2pdma.c
* Unmerged path drivers/xen/unpopulated-alloc.c
* Unmerged path include/linux/memremap.h
diff --git a/include/linux/range.h b/include/linux/range.h
index d1fbeb664012..274681cc3154 100644
--- a/include/linux/range.h
+++ b/include/linux/range.h
@@ -1,12 +1,18 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _LINUX_RANGE_H
 #define _LINUX_RANGE_H
+#include <linux/types.h>
 
 struct range {
 	u64   start;
 	u64   end;
 };
 
+static inline u64 range_len(const struct range *range)
+{
+	return range->end - range->start + 1;
+}
+
 int add_range(struct range *range, int az, int nr_range,
 		u64 start, u64 end);
 
* Unmerged path lib/test_hmm.c
* Unmerged path mm/memremap.c
diff --git a/tools/testing/nvdimm/test/iomap.c b/tools/testing/nvdimm/test/iomap.c
index 033d1c39e5ef..016efd147e82 100644
--- a/tools/testing/nvdimm/test/iomap.c
+++ b/tools/testing/nvdimm/test/iomap.c
@@ -134,7 +134,7 @@ static void dev_pagemap_percpu_release(struct percpu_ref *ref)
 void *__wrap_devm_memremap_pages(struct device *dev, struct dev_pagemap *pgmap)
 {
 	int error;
-	resource_size_t offset = pgmap->res.start;
+	resource_size_t offset = pgmap->range.start;
 	struct nfit_test_resource *nfit_res = get_nfit_res(offset);
 
 	if (!nfit_res)
