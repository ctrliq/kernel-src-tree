net/mlx5e: Improve MQPRIO resiliency

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Tariq Toukan <tariqt@nvidia.com>
commit 7dbc849b2ab3b8ea8f767361c46f914bb2b7779d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/7dbc849b.failed

* Add netdev->tc_to_txq rollback in case of failure in
  mlx5e_update_netdev_queues().
* Fix broken transition between the two modes:
  MQPRIO DCB mode with tc==8, and MQPRIO channel mode.
* Disable MQPRIO channel mode if re-attaching with a different number
  of channels.
* Improve code sharing.

Fixes: ec60c4581bd9 ("net/mlx5e: Support MQPRIO channel mode")
	Signed-off-by: Tariq Toukan <tariqt@nvidia.com>
	Reviewed-by: Maxim Mikityanskiy <maximmi@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 7dbc849b2ab3b8ea8f767361c46f914bb2b7779d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en_main.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 73a162266789,03a7a4ce5cd5..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -252,11 -249,15 +252,19 @@@ struct mlx5e_params 
  	u8  rq_wq_type;
  	u8  log_rq_mtu_frames;
  	u16 num_channels;
++<<<<<<< HEAD
 +	u8  num_tc;
++=======
+ 	struct {
+ 		u16 mode;
+ 		u8 num_tc;
+ 		struct netdev_tc_txq tc_to_txq[TC_MAX_QUEUE];
+ 	} mqprio;
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  	bool rx_cqe_compress_def;
 -	bool tunneled_offload_en;
  	struct dim_cq_moder rx_cq_moderation;
  	struct dim_cq_moder tx_cq_moderation;
 +	bool tunneled_offload_en;
  	bool lro_en;
  	u8  tx_min_inline_mode;
  	bool vlan_strip_disable;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index c234e374c4b8,0390395f421f..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@@ -2635,7 -2263,8 +2635,12 @@@ void mlx5e_set_netdev_mtu_boundaries(st
  				ETH_MAX_MTU);
  }
  
++<<<<<<< HEAD
 +static int mlx5e_netdev_set_tcs(struct net_device *netdev, u16 nch, u8 ntc)
++=======
+ static int mlx5e_netdev_set_tcs(struct net_device *netdev, u16 nch, u8 ntc,
+ 				struct netdev_tc_txq *tc_to_txq)
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  {
  	int tc, err;
  
@@@ -2650,11 -2279,13 +2655,21 @@@
  		return err;
  	}
  
++<<<<<<< HEAD
 +	/* Map netdev TCs to offset 0
 +	 * We have our own UP to TXQ mapping for QoS
 +	 */
 +	for (tc = 0; tc < ntc; tc++)
 +		netdev_set_tc_queue(netdev, tc, nch, 0);
++=======
+ 	for (tc = 0; tc < ntc; tc++) {
+ 		u16 count, offset;
+ 
+ 		count = tc_to_txq[tc].count;
+ 		offset = tc_to_txq[tc].offset;
+ 		netdev_set_tc_queue(netdev, tc, count, offset);
+ 	}
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  
  	return 0;
  }
@@@ -2688,12 -2321,15 +2705,23 @@@ static int mlx5e_update_netdev_queues(s
  
  	old_num_txqs = netdev->real_num_tx_queues;
  	old_ntc = netdev->num_tc ? : 1;
+ 	for (i = 0; i < ARRAY_SIZE(old_tc_to_txq); i++)
+ 		old_tc_to_txq[i] = netdev->tc_to_txq[i];
  
  	nch = priv->channels.params.num_channels;
++<<<<<<< HEAD
 +	ntc = priv->channels.params.num_tc;
++=======
+ 	ntc = priv->channels.params.mqprio.num_tc;
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  	num_rxqs = nch * priv->profile->rq_groups;
+ 	tc_to_txq = priv->channels.params.mqprio.tc_to_txq;
  
++<<<<<<< HEAD
 +	err = mlx5e_netdev_set_tcs(netdev, nch, ntc);
++=======
+ 	err = mlx5e_netdev_set_tcs(netdev, nch, ntc, tc_to_txq);
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  	if (err)
  		goto err_out;
  	err = mlx5e_update_tx_netdev_queues(priv);
@@@ -2716,7 -2352,8 +2744,12 @@@ err_txqs
  	WARN_ON_ONCE(netif_set_real_num_tx_queues(netdev, old_num_txqs));
  
  err_tcs:
++<<<<<<< HEAD
 +	mlx5e_netdev_set_tcs(netdev, old_num_txqs / old_ntc, old_ntc);
++=======
+ 	WARN_ON_ONCE(mlx5e_netdev_set_tcs(netdev, old_num_txqs / old_ntc, old_ntc,
+ 					  old_tc_to_txq));
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  err_out:
  	return err;
  }
@@@ -3373,20 -2866,146 +3408,156 @@@ static int mlx5e_modify_channels_vsd(st
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int mlx5e_setup_tc_mqprio(struct mlx5e_priv *priv,
 +				 struct tc_mqprio_qopt *mqprio)
++=======
+ static void mlx5e_mqprio_build_default_tc_to_txq(struct netdev_tc_txq *tc_to_txq,
+ 						 int ntc, int nch)
+ {
+ 	int tc;
+ 
+ 	memset(tc_to_txq, 0, sizeof(*tc_to_txq) * TC_MAX_QUEUE);
+ 
+ 	/* Map netdev TCs to offset 0.
+ 	 * We have our own UP to TXQ mapping for DCB mode of QoS
+ 	 */
+ 	for (tc = 0; tc < ntc; tc++) {
+ 		tc_to_txq[tc] = (struct netdev_tc_txq) {
+ 			.count = nch,
+ 			.offset = 0,
+ 		};
+ 	}
+ }
+ 
+ static void mlx5e_mqprio_build_tc_to_txq(struct netdev_tc_txq *tc_to_txq,
+ 					 struct tc_mqprio_qopt *qopt)
+ {
+ 	int tc;
+ 
+ 	for (tc = 0; tc < TC_MAX_QUEUE; tc++) {
+ 		tc_to_txq[tc] = (struct netdev_tc_txq) {
+ 			.count = qopt->count[tc],
+ 			.offset = qopt->offset[tc],
+ 		};
+ 	}
+ }
+ 
+ static void mlx5e_params_mqprio_dcb_set(struct mlx5e_params *params, u8 num_tc)
+ {
+ 	params->mqprio.mode = TC_MQPRIO_MODE_DCB;
+ 	params->mqprio.num_tc = num_tc;
+ 	mlx5e_mqprio_build_default_tc_to_txq(params->mqprio.tc_to_txq, num_tc,
+ 					     params->num_channels);
+ }
+ 
+ static void mlx5e_params_mqprio_channel_set(struct mlx5e_params *params,
+ 					    struct tc_mqprio_qopt *qopt)
+ {
+ 	params->mqprio.mode = TC_MQPRIO_MODE_CHANNEL;
+ 	params->mqprio.num_tc = qopt->num_tc;
+ 	mlx5e_mqprio_build_tc_to_txq(params->mqprio.tc_to_txq, qopt);
+ }
+ 
+ static void mlx5e_params_mqprio_reset(struct mlx5e_params *params)
+ {
+ 	mlx5e_params_mqprio_dcb_set(params, 1);
+ }
+ 
+ static int mlx5e_setup_tc_mqprio_dcb(struct mlx5e_priv *priv,
+ 				     struct tc_mqprio_qopt *mqprio)
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  {
 -	struct mlx5e_params new_params;
 +	struct mlx5e_channels new_channels = {};
  	u8 tc = mqprio->num_tc;
 -	int err;
 +	int err = 0;
  
  	mqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;
  
  	if (tc && tc != MLX5E_MAX_NUM_TC)
  		return -EINVAL;
  
++<<<<<<< HEAD
 +	mutex_lock(&priv->state_lock);
 +
++=======
+ 	new_params = priv->channels.params;
+ 	mlx5e_params_mqprio_dcb_set(&new_params, tc ? tc : 1);
+ 
+ 	err = mlx5e_safe_switch_params(priv, &new_params,
+ 				       mlx5e_num_channels_changed_ctx, NULL, true);
+ 
+ 	priv->max_opened_tc = max_t(u8, priv->max_opened_tc,
+ 				    mlx5e_get_dcb_num_tc(&priv->channels.params));
+ 	return err;
+ }
+ 
+ static int mlx5e_mqprio_channel_validate(struct mlx5e_priv *priv,
+ 					 struct tc_mqprio_qopt_offload *mqprio)
+ {
+ 	struct net_device *netdev = priv->netdev;
+ 	int agg_count = 0;
+ 	int i;
+ 
+ 	if (mqprio->qopt.offset[0] != 0 || mqprio->qopt.num_tc < 1 ||
+ 	    mqprio->qopt.num_tc > MLX5E_MAX_NUM_MQPRIO_CH_TC)
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < mqprio->qopt.num_tc; i++) {
+ 		if (!mqprio->qopt.count[i]) {
+ 			netdev_err(netdev, "Zero size for queue-group (%d) is not supported\n", i);
+ 			return -EINVAL;
+ 		}
+ 		if (mqprio->min_rate[i]) {
+ 			netdev_err(netdev, "Min tx rate is not supported\n");
+ 			return -EINVAL;
+ 		}
+ 		if (mqprio->max_rate[i]) {
+ 			netdev_err(netdev, "Max tx rate is not supported\n");
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (mqprio->qopt.offset[i] != agg_count) {
+ 			netdev_err(netdev, "Discontinuous queues config is not supported\n");
+ 			return -EINVAL;
+ 		}
+ 		agg_count += mqprio->qopt.count[i];
+ 	}
+ 
+ 	if (priv->channels.params.num_channels < agg_count) {
+ 		netdev_err(netdev, "Num of queues (%d) exceeds available (%d)\n",
+ 			   agg_count, priv->channels.params.num_channels);
+ 		return -EINVAL;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int mlx5e_setup_tc_mqprio_channel(struct mlx5e_priv *priv,
+ 					 struct tc_mqprio_qopt_offload *mqprio)
+ {
+ 	mlx5e_fp_preactivate preactivate;
+ 	struct mlx5e_params new_params;
+ 	bool nch_changed;
+ 	int err;
+ 
+ 	err = mlx5e_mqprio_channel_validate(priv, mqprio);
+ 	if (err)
+ 		return err;
+ 
+ 	new_params = priv->channels.params;
+ 	mlx5e_params_mqprio_channel_set(&new_params, &mqprio->qopt);
+ 
+ 	nch_changed = mlx5e_get_dcb_num_tc(&priv->channels.params) > 1;
+ 	preactivate = nch_changed ? mlx5e_num_channels_changed_ctx :
+ 		mlx5e_update_netdev_queues_ctx;
+ 	return mlx5e_safe_switch_params(priv, &new_params, preactivate, NULL, true);
+ }
+ 
+ static int mlx5e_setup_tc_mqprio(struct mlx5e_priv *priv,
+ 				 struct tc_mqprio_qopt_offload *mqprio)
+ {
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  	/* MQPRIO is another toplevel qdisc that can't be attached
  	 * simultaneously with the offloaded HTB.
  	 */
@@@ -4689,7 -4233,7 +4860,11 @@@ void mlx5e_build_nic_params(struct mlx5
  	params->hard_mtu = MLX5E_ETH_HARD_MTU;
  	params->num_channels = min_t(unsigned int, MLX5E_MAX_NUM_CHANNELS / 2,
  				     priv->max_nch);
++<<<<<<< HEAD
 +	params->num_tc       = 1;
++=======
+ 	mlx5e_params_mqprio_reset(params);
++>>>>>>> 7dbc849b2ab3 (net/mlx5e: Improve MQPRIO resiliency)
  
  	/* Set an initial non-zero value, so that mlx5e_select_queue won't
  	 * divide by zero if called before first activating channels.
@@@ -5321,7 -4866,18 +5496,11 @@@ int mlx5e_attach_netdev(struct mlx5e_pr
  		 */
  		priv->netdev->priv_flags &= ~IFF_RXFH_CONFIGURED;
  		priv->channels.params.num_channels = max_nch;
+ 		if (priv->channels.params.mqprio.mode == TC_MQPRIO_MODE_CHANNEL) {
+ 			mlx5_core_warn(priv->mdev, "MLX5E: Disabling MQPRIO channel mode\n");
+ 			mlx5e_params_mqprio_reset(&priv->channels.params);
+ 		}
  	}
 -	if (max_nch != priv->max_nch) {
 -		mlx5_core_warn(priv->mdev,
 -			       "MLX5E: Updating max number of channels from %u to %u\n",
 -			       priv->max_nch, max_nch);
 -		priv->max_nch = max_nch;
 -	}
 -
  	/* 1. Set the real number of queues in the kernel the first time.
  	 * 2. Set our default XPS cpumask.
  	 * 3. Build the RQT.
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_main.c
