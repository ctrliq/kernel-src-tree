ice: introduce XDP_TX fallback path

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.9.1.el8
commit-author Maciej Fijalkowski <maciej.fijalkowski@intel.com>
commit 22bf877e528f683bffc2fc932d148e840f7cc27d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.9.1.el8/22bf877e.failed

Under rare circumstances there might be a situation where a requirement
of having XDP Tx queue per CPU could not be fulfilled and some of the Tx
resources have to be shared between CPUs. This yields a need for placing
accesses to xdp_ring inside a critical section protected by spinlock.
These accesses happen to be in the hot path, so let's introduce the
static branch that will be triggered from the control plane when driver
could not provide Tx queue dedicated for XDP on each CPU.

Currently, the design that has been picked is to allow any number of XDP
Tx queues that is at least half of a count of CPUs that platform has.
For lower number driver will bail out with a response to user that there
were not enough Tx resources that would allow configuring XDP. The
sharing of rings is signalled via static branch enablement which in turn
indicates that lock for xdp_ring accesses needs to be taken in hot path.

Approach based on static branch has no impact on performance of a
non-fallback path. One thing that is needed to be mentioned is a fact
that the static branch will act as a global driver switch, meaning that
if one PF got out of Tx resources, then other PFs that ice driver is
servicing will suffer. However, given the fact that HW that ice driver
is handling has 1024 Tx queues per each PF, this is currently an
unlikely scenario.

	Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Tested-by: George Kuruvinakunnel <george.kuruvinakunnel@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 22bf877e528f683bffc2fc932d148e840f7cc27d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_main.c
#	drivers/net/ethernet/intel/ice/ice_txrx.c
#	drivers/net/ethernet/intel/ice/ice_txrx_lib.c
diff --cc drivers/net/ethernet/intel/ice/ice_main.c
index 8b6a4047a03a,ccd9b9514001..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_main.c
+++ b/drivers/net/ethernet/intel/ice/ice_main.c
@@@ -2400,8 -2399,20 +2402,25 @@@ static int ice_xdp_alloc_setup_rings(st
  			goto free_xdp_rings;
  		ice_set_ring_xdp(xdp_ring);
  		xdp_ring->xsk_pool = ice_tx_xsk_pool(xdp_ring);
++<<<<<<< HEAD
 +	}
 +
++=======
+ 		spin_lock_init(&xdp_ring->tx_lock);
+ 		for (j = 0; j < xdp_ring->count; j++) {
+ 			tx_desc = ICE_TX_DESC(xdp_ring, j);
+ 			tx_desc->cmd_type_offset_bsz = cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE);
+ 		}
+ 	}
+ 
+ 	ice_for_each_rxq(vsi, i) {
+ 		if (static_key_enabled(&ice_xdp_locking_key))
+ 			vsi->rx_rings[i]->xdp_ring = vsi->xdp_rings[i % vsi->num_xdp_txq];
+ 		else
+ 			vsi->rx_rings[i]->xdp_ring = vsi->xdp_rings[i];
+ 	}
+ 
++>>>>>>> 22bf877e528f (ice: introduce XDP_TX fallback path)
  	return 0;
  
  free_xdp_rings:
diff --cc drivers/net/ethernet/intel/ice/ice_txrx.c
index 6dd00f973be2,01ae331927bd..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_txrx.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.c
@@@ -552,11 -547,14 +552,20 @@@ ice_run_xdp(struct ice_rx_ring *rx_ring
  	case XDP_PASS:
  		return ICE_XDP_PASS;
  	case XDP_TX:
++<<<<<<< HEAD
 +		xdp_ring = rx_ring->vsi->xdp_rings[smp_processor_id()];
 +		result = ice_xmit_xdp_ring(xdp->data, xdp->data_end - xdp->data, xdp_ring);
 +		if (result == ICE_XDP_CONSUMED)
++=======
+ 		if (static_branch_unlikely(&ice_xdp_locking_key))
+ 			spin_lock(&xdp_ring->tx_lock);
+ 		err = ice_xmit_xdp_ring(xdp->data, xdp->data_end - xdp->data, xdp_ring);
+ 		if (static_branch_unlikely(&ice_xdp_locking_key))
+ 			spin_unlock(&xdp_ring->tx_lock);
+ 		if (err == ICE_XDP_CONSUMED)
++>>>>>>> 22bf877e528f (ice: introduce XDP_TX fallback path)
  			goto out_failure;
 -		return err;
 +		return result;
  	case XDP_REDIRECT:
  		err = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);
  		if (err)
diff --cc drivers/net/ethernet/intel/ice/ice_txrx_lib.c
index 3065b5dbe3fc,1dd7e84f41f8..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_txrx_lib.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx_lib.c
@@@ -294,9 -351,10 +294,17 @@@ void ice_finalize_xdp_rx(struct ice_rx_
  		xdp_do_flush_map();
  
  	if (xdp_res & ICE_XDP_TX) {
++<<<<<<< HEAD
 +		struct ice_tx_ring *xdp_ring =
 +			rx_ring->vsi->xdp_rings[smp_processor_id()];
 +
 +		ice_xdp_ring_update_tail(xdp_ring);
++=======
+ 		if (static_branch_unlikely(&ice_xdp_locking_key))
+ 			spin_lock(&xdp_ring->tx_lock);
+ 		ice_xdp_ring_update_tail(xdp_ring);
+ 		if (static_branch_unlikely(&ice_xdp_locking_key))
+ 			spin_unlock(&xdp_ring->tx_lock);
++>>>>>>> 22bf877e528f (ice: introduce XDP_TX fallback path)
  	}
  }
diff --git a/drivers/net/ethernet/intel/ice/ice.h b/drivers/net/ethernet/intel/ice/ice.h
index ffdeecf46b8e..9ab81b135932 100644
--- a/drivers/net/ethernet/intel/ice/ice.h
+++ b/drivers/net/ethernet/intel/ice/ice.h
@@ -165,6 +165,8 @@ enum ice_feature {
 	ICE_F_MAX
 };
 
+DECLARE_STATIC_KEY_FALSE(ice_xdp_locking_key);
+
 struct ice_txq_meta {
 	u32 q_teid;	/* Tx-scheduler element identifier */
 	u16 q_id;	/* Entry in VSI's txq_map bitmap */
@@ -697,6 +699,7 @@ int ice_up(struct ice_vsi *vsi);
 int ice_down(struct ice_vsi *vsi);
 int ice_vsi_cfg(struct ice_vsi *vsi);
 struct ice_vsi *ice_lb_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi);
+int ice_vsi_determine_xdp_res(struct ice_vsi *vsi);
 int ice_prepare_xdp_rings(struct ice_vsi *vsi, struct bpf_prog *prog);
 int ice_destroy_xdp_rings(struct ice_vsi *vsi);
 int
diff --git a/drivers/net/ethernet/intel/ice/ice_lib.c b/drivers/net/ethernet/intel/ice/ice_lib.c
index d663b0838a72..4235472dc0c6 100644
--- a/drivers/net/ethernet/intel/ice/ice_lib.c
+++ b/drivers/net/ethernet/intel/ice/ice_lib.c
@@ -3200,7 +3200,9 @@ int ice_vsi_rebuild(struct ice_vsi *vsi, bool init_vsi)
 
 		ice_vsi_map_rings_to_vectors(vsi);
 		if (ice_is_xdp_ena_vsi(vsi)) {
-			vsi->num_xdp_txq = num_possible_cpus();
+			ret = ice_vsi_determine_xdp_res(vsi);
+			if (ret)
+				goto err_vectors;
 			ret = ice_prepare_xdp_rings(vsi, vsi->xdp_prog);
 			if (ret)
 				goto err_vectors;
* Unmerged path drivers/net/ethernet/intel/ice/ice_main.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_txrx.c
diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 65020bffafd4..f205cce1592e 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -330,6 +330,7 @@ struct ice_tx_ring {
 	struct rcu_head rcu;		/* to avoid race on free */
 	DECLARE_BITMAP(xps_state, ICE_TX_NBITS);	/* XPS Config State */
 	struct ice_ptp_tx *tx_tstamps;
+	spinlock_t tx_lock;
 	u32 txq_teid;			/* Added Tx queue TEID */
 	u16 q_handle;			/* Queue handle per TC */
 	u16 reg_idx;			/* HW register index of the ring */
* Unmerged path drivers/net/ethernet/intel/ice/ice_txrx_lib.c
