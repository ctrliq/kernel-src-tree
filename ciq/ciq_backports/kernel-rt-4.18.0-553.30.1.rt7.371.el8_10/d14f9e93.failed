locking/rtmutex: Use rt_mutex specific scheduler helpers

jira LE-3201
Rebuild_History Non-Buildable kernel-rt-4.18.0-553.30.1.rt7.371.el8_10
commit-author Sebastian Andrzej Siewior <bigeasy@linutronix.de>
commit d14f9e930b9073de264c106bf04968286ef9b3a4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-553.30.1.rt7.371.el8_10/d14f9e93.failed

Have rt_mutex use the rt_mutex specific scheduler helpers to avoid
recursion vs rtlock on the PI state.

[[ peterz: adapted to new names ]]

	Reported-by: Crystal Wood <swood@redhat.com>
	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20230908162254.999499-6-bigeasy@linutronix.de
(cherry picked from commit d14f9e930b9073de264c106bf04968286ef9b3a4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/futex/pi.c
#	kernel/locking/rwbase_rt.c
diff --cc kernel/locking/rwbase_rt.c
index a28148a05383,c7258cb32d91..000000000000
--- a/kernel/locking/rwbase_rt.c
+++ b/kernel/locking/rwbase_rt.c
@@@ -122,6 -124,9 +123,12 @@@ static int __sched __rwbase_read_lock(s
  	raw_spin_unlock_irq(&rtm->wait_lock);
  	if (!ret)
  		rwbase_rtmutex_unlock(rtm);
++<<<<<<< HEAD
++=======
+ 
+ 	trace_contention_end(rwb, ret);
+ 	rwbase_post_schedule();
++>>>>>>> d14f9e930b90 (locking/rtmutex: Use rt_mutex specific scheduler helpers)
  	return ret;
  }
  
@@@ -240,6 -252,8 +249,11 @@@ static int __sched rwbase_write_lock(st
  		if (rwbase_signal_pending_state(state, current)) {
  			rwbase_restore_current_state();
  			__rwbase_write_unlock(rwb, 0, flags);
++<<<<<<< HEAD
++=======
+ 			rwbase_post_schedule();
+ 			trace_contention_end(rwb, -EINTR);
++>>>>>>> d14f9e930b90 (locking/rtmutex: Use rt_mutex specific scheduler helpers)
  			return -EINTR;
  		}
  
* Unmerged path kernel/futex/pi.c
* Unmerged path kernel/futex/pi.c
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index 5fc59bb65ae1..bb799e9d3d24 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -1603,7 +1603,7 @@ static int __sched rt_mutex_slowlock_block(struct rt_mutex_base *lock,
 		raw_spin_unlock_irq(&lock->wait_lock);
 
 		if (!owner || !rtmutex_spin_on_owner(lock, waiter, owner))
-			schedule();
+			rt_mutex_schedule();
 
 		raw_spin_lock_irq(&lock->wait_lock);
 		set_current_state(state);
@@ -1628,7 +1628,7 @@ static void __sched rt_mutex_handle_deadlock(int res, int detect_deadlock,
 
 	while (1) {
 		set_current_state(TASK_INTERRUPTIBLE);
-		schedule();
+		rt_mutex_schedule();
 	}
 }
 
@@ -1719,6 +1719,15 @@ static int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,
 	unsigned long flags;
 	int ret;
 
+	/*
+	 * Do all pre-schedule work here, before we queue a waiter and invoke
+	 * PI -- any such work that trips on rtlock (PREEMPT_RT spinlock) would
+	 * otherwise recurse back into task_blocks_on_rt_mutex() through
+	 * rtlock_slowlock() and will then enqueue a second waiter for this
+	 * same task and things get really confusing real fast.
+	 */
+	rt_mutex_pre_schedule();
+
 	/*
 	 * Technically we could use raw_spin_[un]lock_irq() here, but this can
 	 * be called in early boot if the cmpxchg() fast path is disabled
@@ -1730,6 +1739,7 @@ static int __sched rt_mutex_slowlock(struct rt_mutex_base *lock,
 	raw_spin_lock_irqsave(&lock->wait_lock, flags);
 	ret = __rt_mutex_slowlock_locked(lock, ww_ctx, state);
 	raw_spin_unlock_irqrestore(&lock->wait_lock, flags);
+	rt_mutex_post_schedule();
 
 	return ret;
 }
* Unmerged path kernel/locking/rwbase_rt.c
diff --git a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
index 7aa7b734bf18..7cec5929f65c 100644
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@ -1429,8 +1429,14 @@ static inline void __downgrade_write(struct rw_semaphore *sem)
 #define rwbase_signal_pending_state(state, current)	\
 	signal_pending_state(state, current)
 
+#define rwbase_pre_schedule()				\
+	rt_mutex_pre_schedule()
+
 #define rwbase_schedule()				\
-	schedule()
+	rt_mutex_schedule()
+
+#define rwbase_post_schedule()				\
+	rt_mutex_post_schedule()
 
 #include "rwbase_rt.c"
 
diff --git a/kernel/locking/spinlock_rt.c b/kernel/locking/spinlock_rt.c
index 839041f8460f..7fabcea4b2de 100644
--- a/kernel/locking/spinlock_rt.c
+++ b/kernel/locking/spinlock_rt.c
@@ -173,9 +173,13 @@ static __always_inline int  rwbase_rtmutex_trylock(struct rt_mutex_base *rtm)
 
 #define rwbase_signal_pending_state(state, current)	(0)
 
+#define rwbase_pre_schedule()
+
 #define rwbase_schedule()				\
 	schedule_rtlock()
 
+#define rwbase_post_schedule()
+
 #include "rwbase_rt.c"
 /*
  * The common functions which get wrapped into the rwlock API.
