ALSA: memalloc: Workaround for Xen PV

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-499.el8
commit-author Takashi Iwai <tiwai@suse.de>
commit 53466ebdec614f915c691809b0861acecb941e30
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-499.el8/53466ebd.failed

We change recently the memalloc helper to use
dma_alloc_noncontiguous() and the fallback to get_pages().  Although
lots of issues with IOMMU (or non-IOMMU) have been addressed, but
there seems still a regression on Xen PV.  Interestingly, the only
proper way to work is use dma_alloc_coherent().  The use of
dma_alloc_coherent() for SG buffer was dropped as it's problematic on
IOMMU systems.  OTOH, Xen PV has a different way, and it's fine to use
the dma_alloc_coherent().

This patch is a workaround for Xen PV.  It consists of the following
changes:
- For Xen PV, use only the fallback allocation without
  dma_alloc_noncontiguous()
- In the fallback allocation, use dma_alloc_coherent();
  the DMA address from dma_alloc_coherent() is returned in get_addr
  ops
- The DMA addresses are stored in an array; the first entry stores the
  number of allocated pages in lower bits, which are referred at
  releasing pages again

	Reported-by: Marek Marczykowski-Górecki <marmarek@invisiblethingslab.com>
	Tested-by: Marek Marczykowski-Górecki <marmarek@invisiblethingslab.com>
Fixes: a8d302a0b770 ("ALSA: memalloc: Revive x86-specific WC page allocations again")
Fixes: 9736a325137b ("ALSA: memalloc: Don't fall back for SG-buffer with IOMMU")
Link: https://lore.kernel.org/r/87tu256lqs.wl-tiwai@suse.de
Link: https://lore.kernel.org/r/20230125153104.5527-1-tiwai@suse.de
	Signed-off-by: Takashi Iwai <tiwai@suse.de>
(cherry picked from commit 53466ebdec614f915c691809b0861acecb941e30)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	sound/core/memalloc.c
diff --cc sound/core/memalloc.c
index 01c95b0dffa9,f901504b5afc..000000000000
--- a/sound/core/memalloc.c
+++ b/sound/core/memalloc.c
@@@ -701,52 -714,38 +700,77 @@@ static const struct snd_malloc_ops snd_
  	.get_chunk_size = snd_dma_noncontig_get_chunk_size,
  };
  
 +/* manual page allocations with wc setup */
 +static void *do_alloc_fallback_pages(struct device *dev, size_t size,
 +				     dma_addr_t *addr, bool wc)
 +{
 +	gfp_t gfp = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;
 +	void *p;
 +
 + again:
 +	p = do_alloc_pages(size, addr, gfp);
 +	if (!p || (*addr + size - 1) & ~dev->coherent_dma_mask) {
 +		if (IS_ENABLED(CONFIG_ZONE_DMA32) && !(gfp & GFP_DMA32)) {
 +			gfp |= GFP_DMA32;
 +			goto again;
 +		}
 +		if (IS_ENABLED(CONFIG_ZONE_DMA) && !(gfp & GFP_DMA)) {
 +			gfp = (gfp & ~GFP_DMA32) | GFP_DMA;
 +			goto again;
 +		}
 +	}
 +	if (p && wc)
 +		set_memory_wc((unsigned long)(p), size >> PAGE_SHIFT);
 +	return p;
 +}
 +
 +static void do_free_fallback_pages(void *p, size_t size, bool wc)
 +{
 +	if (wc)
 +		set_memory_wb((unsigned long)(p), size >> PAGE_SHIFT);
 +	free_pages_exact(p, size);
 +}
 +
  /* Fallback SG-buffer allocations for x86 */
  struct snd_dma_sg_fallback {
+ 	bool use_dma_alloc_coherent;
  	size_t count;
  	struct page **pages;
++<<<<<<< HEAD
++=======
+ 	/* DMA address array; the first page contains #pages in ~PAGE_MASK */
++>>>>>>> 53466ebdec61 (ALSA: memalloc: Workaround for Xen PV)
  	dma_addr_t *addrs;
  };
  
  static void __snd_dma_sg_fallback_free(struct snd_dma_buffer *dmab,
  				       struct snd_dma_sg_fallback *sgbuf)
  {
- 	bool wc = dmab->dev.type == SNDRV_DMA_TYPE_DEV_WC_SG_FALLBACK;
- 	size_t i;
+ 	size_t i, size;
  
++<<<<<<< HEAD
 +	for (i = 0; i < sgbuf->count && sgbuf->pages[i]; i++)
 +		do_free_fallback_pages(page_address(sgbuf->pages[i]), PAGE_SIZE, wc);
++=======
+ 	if (sgbuf->pages && sgbuf->addrs) {
+ 		i = 0;
+ 		while (i < sgbuf->count) {
+ 			if (!sgbuf->pages[i] || !sgbuf->addrs[i])
+ 				break;
+ 			size = sgbuf->addrs[i] & ~PAGE_MASK;
+ 			if (WARN_ON(!size))
+ 				break;
+ 			if (sgbuf->use_dma_alloc_coherent)
+ 				dma_free_coherent(dmab->dev.dev, size << PAGE_SHIFT,
+ 						  page_address(sgbuf->pages[i]),
+ 						  sgbuf->addrs[i] & PAGE_MASK);
+ 			else
+ 				do_free_pages(page_address(sgbuf->pages[i]),
+ 					      size << PAGE_SHIFT, false);
+ 			i += size;
+ 		}
+ 	}
++>>>>>>> 53466ebdec61 (ALSA: memalloc: Workaround for Xen PV)
  	kvfree(sgbuf->pages);
  	kvfree(sgbuf->addrs);
  	kfree(sgbuf);
@@@ -755,37 -754,68 +779,91 @@@
  static void *snd_dma_sg_fallback_alloc(struct snd_dma_buffer *dmab, size_t size)
  {
  	struct snd_dma_sg_fallback *sgbuf;
++<<<<<<< HEAD
 +	struct page **pages;
 +	size_t i, count;
++=======
+ 	struct page **pagep, *curp;
+ 	size_t chunk, npages;
+ 	dma_addr_t *addrp;
+ 	dma_addr_t addr;
++>>>>>>> 53466ebdec61 (ALSA: memalloc: Workaround for Xen PV)
  	void *p;
- 	bool wc = dmab->dev.type == SNDRV_DMA_TYPE_DEV_WC_SG_FALLBACK;
+ 
+ 	/* correct the type */
+ 	if (dmab->dev.type == SNDRV_DMA_TYPE_DEV_SG)
+ 		dmab->dev.type = SNDRV_DMA_TYPE_DEV_SG_FALLBACK;
+ 	else if (dmab->dev.type == SNDRV_DMA_TYPE_DEV_WC_SG)
+ 		dmab->dev.type = SNDRV_DMA_TYPE_DEV_WC_SG_FALLBACK;
  
  	sgbuf = kzalloc(sizeof(*sgbuf), GFP_KERNEL);
  	if (!sgbuf)
  		return NULL;
++<<<<<<< HEAD
 +	count = PAGE_ALIGN(size) >> PAGE_SHIFT;
 +	pages = kvcalloc(count, sizeof(*pages), GFP_KERNEL);
 +	if (!pages)
 +		goto error;
 +	sgbuf->pages = pages;
 +	sgbuf->addrs = kvcalloc(count, sizeof(*sgbuf->addrs), GFP_KERNEL);
 +	if (!sgbuf->addrs)
 +		goto error;
 +
 +	for (i = 0; i < count; sgbuf->count++, i++) {
 +		p = do_alloc_fallback_pages(dmab->dev.dev, PAGE_SIZE,
 +					    &sgbuf->addrs[i], wc);
 +		if (!p)
 +			goto error;
 +		sgbuf->pages[i] = virt_to_page(p);
++=======
+ 	sgbuf->use_dma_alloc_coherent = cpu_feature_enabled(X86_FEATURE_XENPV);
+ 	size = PAGE_ALIGN(size);
+ 	sgbuf->count = size >> PAGE_SHIFT;
+ 	sgbuf->pages = kvcalloc(sgbuf->count, sizeof(*sgbuf->pages), GFP_KERNEL);
+ 	sgbuf->addrs = kvcalloc(sgbuf->count, sizeof(*sgbuf->addrs), GFP_KERNEL);
+ 	if (!sgbuf->pages || !sgbuf->addrs)
+ 		goto error;
+ 
+ 	pagep = sgbuf->pages;
+ 	addrp = sgbuf->addrs;
+ 	chunk = (PAGE_SIZE - 1) << PAGE_SHIFT; /* to fit in low bits in addrs */
+ 	while (size > 0) {
+ 		chunk = min(size, chunk);
+ 		if (sgbuf->use_dma_alloc_coherent)
+ 			p = dma_alloc_coherent(dmab->dev.dev, chunk, &addr, DEFAULT_GFP);
+ 		else
+ 			p = do_alloc_pages(dmab->dev.dev, chunk, &addr, false);
+ 		if (!p) {
+ 			if (chunk <= PAGE_SIZE)
+ 				goto error;
+ 			chunk >>= 1;
+ 			chunk = PAGE_SIZE << get_order(chunk);
+ 			continue;
+ 		}
+ 
+ 		size -= chunk;
+ 		/* fill pages */
+ 		npages = chunk >> PAGE_SHIFT;
+ 		*addrp = npages; /* store in lower bits */
+ 		curp = virt_to_page(p);
+ 		while (npages--) {
+ 			*pagep++ = curp++;
+ 			*addrp++ |= addr;
+ 			addr += PAGE_SIZE;
+ 		}
++>>>>>>> 53466ebdec61 (ALSA: memalloc: Workaround for Xen PV)
  	}
  
 -	p = vmap(sgbuf->pages, sgbuf->count, VM_MAP, PAGE_KERNEL);
 +	p = vmap(pages, count, VM_MAP, PAGE_KERNEL);
  	if (!p)
  		goto error;
+ 
+ 	if (dmab->dev.type == SNDRV_DMA_TYPE_DEV_WC_SG_FALLBACK)
+ 		set_pages_array_wc(sgbuf->pages, sgbuf->count);
+ 
  	dmab->private_data = sgbuf;
  	/* store the first page address for convenience */
- 	dmab->addr = snd_sgbuf_get_addr(dmab, 0);
+ 	dmab->addr = sgbuf->addrs[0] & PAGE_MASK;
  	return p;
  
   error:
* Unmerged path sound/core/memalloc.c
