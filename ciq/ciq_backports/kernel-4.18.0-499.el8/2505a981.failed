zsmalloc: fix races between asynchronous zspage free and page migration

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-499.el8
commit-author Sultan Alsawaf <sultan@kerneltoast.com>
commit 2505a981114dcb715f8977b8433f7540854851d8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-499.el8/2505a981.failed

The asynchronous zspage free worker tries to lock a zspage's entire page
list without defending against page migration.  Since pages which haven't
yet been locked can concurrently migrate off the zspage page list while
lock_zspage() churns away, lock_zspage() can suffer from a few different
lethal races.

It can lock a page which no longer belongs to the zspage and unsafely
dereference page_private(), it can unsafely dereference a torn pointer to
the next page (since there's a data race), and it can observe a spurious
NULL pointer to the next page and thus not lock all of the zspage's pages
(since a single page migration will reconstruct the entire page list, and
create_page_chain() unconditionally zeroes out each list pointer in the
process).

Fix the races by using migrate_read_lock() in lock_zspage() to synchronize
with page migration.

Link: https://lkml.kernel.org/r/20220509024703.243847-1-sultan@kerneltoast.com
Fixes: 77ff465799c602 ("zsmalloc: zs_page_migrate: skip unnecessary loops but not return -EBUSY if zspage is not inuse")
	Signed-off-by: Sultan Alsawaf <sultan@kerneltoast.com>
	Acked-by: Minchan Kim <minchan@kernel.org>
	Cc: Nitin Gupta <ngupta@vflare.org>
	Cc: Sergey Senozhatsky <senozhatsky@chromium.org>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 2505a981114dcb715f8977b8433f7540854851d8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/zsmalloc.c
diff --cc mm/zsmalloc.c
index a1beda480188,5d5fc04385b8..000000000000
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@@ -1824,6 -1712,48 +1824,51 @@@ static enum fullness_group putback_zspa
  }
  
  #ifdef CONFIG_COMPACTION
++<<<<<<< HEAD
++=======
+ /*
+  * To prevent zspage destroy during migration, zspage freeing should
+  * hold locks of all pages in the zspage.
+  */
+ static void lock_zspage(struct zspage *zspage)
+ {
+ 	struct page *curr_page, *page;
+ 
+ 	/*
+ 	 * Pages we haven't locked yet can be migrated off the list while we're
+ 	 * trying to lock them, so we need to be careful and only attempt to
+ 	 * lock each page under migrate_read_lock(). Otherwise, the page we lock
+ 	 * may no longer belong to the zspage. This means that we may wait for
+ 	 * the wrong page to unlock, so we must take a reference to the page
+ 	 * prior to waiting for it to unlock outside migrate_read_lock().
+ 	 */
+ 	while (1) {
+ 		migrate_read_lock(zspage);
+ 		page = get_first_page(zspage);
+ 		if (trylock_page(page))
+ 			break;
+ 		get_page(page);
+ 		migrate_read_unlock(zspage);
+ 		wait_on_page_locked(page);
+ 		put_page(page);
+ 	}
+ 
+ 	curr_page = page;
+ 	while ((page = get_next_page(curr_page))) {
+ 		if (trylock_page(page)) {
+ 			curr_page = page;
+ 		} else {
+ 			get_page(page);
+ 			migrate_read_unlock(zspage);
+ 			wait_on_page_locked(page);
+ 			put_page(page);
+ 			migrate_read_lock(zspage);
+ 		}
+ 	}
+ 	migrate_read_unlock(zspage);
+ }
+ 
++>>>>>>> 2505a981114d (zsmalloc: fix races between asynchronous zspage free and page migration)
  static int zs_init_fs_context(struct fs_context *fc)
  {
  	return init_pseudo(fc, ZSMALLOC_MAGIC) ? 0 : -ENOMEM;
* Unmerged path mm/zsmalloc.c
