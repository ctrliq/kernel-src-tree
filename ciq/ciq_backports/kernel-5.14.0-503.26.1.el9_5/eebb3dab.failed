mm: migrate: record the mlocked page status to remove unnecessary lru drain

jira LE-2424
Rebuild_History Non-Buildable kernel-5.14.0-503.26.1.el9_5
commit-author Baolin Wang <baolin.wang@linux.alibaba.com>
commit eebb3dabbb5cc590afe32880b5d3726d0fbf88db
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-503.26.1.el9_5/eebb3dab.failed

When doing compaction, I found the lru_add_drain() is an obvious hotspot
when migrating pages. The distribution of this hotspot is as follows:
   - 18.75% compact_zone
      - 17.39% migrate_pages
         - 13.79% migrate_pages_batch
            - 11.66% migrate_folio_move
               - 7.02% lru_add_drain
                  + 7.02% lru_add_drain_cpu
               + 3.00% move_to_new_folio
                 1.23% rmap_walk
            + 1.92% migrate_folio_unmap
         + 3.20% migrate_pages_sync
      + 0.90% isolate_migratepages

The lru_add_drain() was added by commit c3096e6782b7 ("mm/migrate:
__unmap_and_move() push good newpage to LRU") to drain the newpage to LRU
immediately, to help to build up the correct newpage->mlock_count in
remove_migration_ptes() for mlocked pages.  However, if there are no
mlocked pages are migrating, then we can avoid this lru drain operation,
especailly for the heavy concurrent scenarios.

So we can record the source pages' mlocked status in
migrate_folio_unmap(), and only drain the lru list when the mlocked status
is set in migrate_folio_move().

In addition, the page was already isolated from lru when migrating, so
checking the mlocked status is stable by folio_test_mlocked() in
migrate_folio_unmap().

After this patch, I can see the hotpot of the lru_add_drain() is gone:
   - 9.41% migrate_pages_batch
      - 6.15% migrate_folio_move
         - 3.64% move_to_new_folio
            + 1.80% migrate_folio_extra
            + 1.70% buffer_migrate_folio
         + 1.41% rmap_walk
         + 0.62% folio_add_lru
      + 3.07% migrate_folio_unmap

Meanwhile, the compaction latency shows some improvements when running
thpscale:
                            base                   patched
Amean     fault-both-1      1131.22 (   0.00%)     1112.55 *   1.65%*
Amean     fault-both-3      2489.75 (   0.00%)     2324.15 *   6.65%*
Amean     fault-both-5      3257.37 (   0.00%)     3183.18 *   2.28%*
Amean     fault-both-7      4257.99 (   0.00%)     4079.04 *   4.20%*
Amean     fault-both-12     6614.02 (   0.00%)     6075.60 *   8.14%*
Amean     fault-both-18    10607.78 (   0.00%)     8978.86 *  15.36%*
Amean     fault-both-24    14911.65 (   0.00%)    11619.55 *  22.08%*
Amean     fault-both-30    14954.67 (   0.00%)    14925.66 *   0.19%*
Amean     fault-both-32    16654.87 (   0.00%)    15580.31 *   6.45%*

Link: https://lkml.kernel.org/r/06e9153a7a4850352ec36602df3a3a844de45698.1697859741.git.baolin.wang@linux.alibaba.com
	Signed-off-by: Baolin Wang <baolin.wang@linux.alibaba.com>
	Reviewed-by: "Huang, Ying" <ying.huang@intel.com>
	Reviewed-by: Zi Yan <ziy@nvidia.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Yin Fengwei <fengwei.yin@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit eebb3dabbb5cc590afe32880b5d3726d0fbf88db)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/migrate.c
diff --cc mm/migrate.c
index 381ad99b8d75,35a88334bb3c..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -1111,10 -1109,9 +1117,14 @@@ static int migrate_folio_unmap(new_page
  {
  	struct folio *dst;
  	int rc = -EAGAIN;
++<<<<<<< HEAD
 +	struct page *newpage = NULL;
 +	int page_was_mapped = 0;
++=======
+ 	int old_page_state = 0;
++>>>>>>> eebb3dabbb5c (mm: migrate: record the mlocked page status to remove unnecessary lru drain)
  	struct anon_vma *anon_vma = NULL;
 -	bool is_lru = !__folio_test_movable(src);
 +	bool is_lru = !__PageMovable(&src->page);
  	bool locked = false;
  	bool dst_locked = false;
  
@@@ -1250,8 -1256,9 +1262,14 @@@ out
  	if (rc == -EAGAIN)
  		ret = NULL;
  
++<<<<<<< HEAD
 +	migrate_folio_undo_src(src, page_was_mapped, anon_vma, locked, ret);
 +	migrate_folio_undo_dst(dst, dst_locked, put_new_page, private);
++=======
+ 	migrate_folio_undo_src(src, old_page_state & PAGE_WAS_MAPPED,
+ 			       anon_vma, locked, ret);
+ 	migrate_folio_undo_dst(dst, dst_locked, put_new_folio, private);
++>>>>>>> eebb3dabbb5c (mm: migrate: record the mlocked page status to remove unnecessary lru drain)
  
  	return rc;
  }
@@@ -1263,12 -1270,12 +1281,12 @@@ static int migrate_folio_move(free_page
  			      struct list_head *ret)
  {
  	int rc;
- 	int page_was_mapped = 0;
+ 	int old_page_state = 0;
  	struct anon_vma *anon_vma = NULL;
 -	bool is_lru = !__folio_test_movable(src);
 +	bool is_lru = !__PageMovable(&src->page);
  	struct list_head *prev;
  
- 	__migrate_folio_extract(dst, &page_was_mapped, &anon_vma);
+ 	__migrate_folio_extract(dst, &old_page_state, &anon_vma);
  	prev = dst->lru.prev;
  	list_del(&dst->lru);
  
@@@ -1328,8 -1335,9 +1346,14 @@@ out
  		return rc;
  	}
  
++<<<<<<< HEAD
 +	migrate_folio_undo_src(src, page_was_mapped, anon_vma, true, ret);
 +	migrate_folio_undo_dst(dst, true, put_new_page, private);
++=======
+ 	migrate_folio_undo_src(src, old_page_state & PAGE_WAS_MAPPED,
+ 			       anon_vma, true, ret);
+ 	migrate_folio_undo_dst(dst, true, put_new_folio, private);
++>>>>>>> eebb3dabbb5c (mm: migrate: record the mlocked page status to remove unnecessary lru drain)
  
  	return rc;
  }
@@@ -1798,14 -1809,14 +1822,14 @@@ out
  	dst = list_first_entry(&dst_folios, struct folio, lru);
  	dst2 = list_next_entry(dst, lru);
  	list_for_each_entry_safe(folio, folio2, &unmap_folios, lru) {
- 		int page_was_mapped = 0;
+ 		int old_page_state = 0;
  		struct anon_vma *anon_vma = NULL;
  
- 		__migrate_folio_extract(dst, &page_was_mapped, &anon_vma);
- 		migrate_folio_undo_src(folio, page_was_mapped, anon_vma,
- 				       true, ret_folios);
+ 		__migrate_folio_extract(dst, &old_page_state, &anon_vma);
+ 		migrate_folio_undo_src(folio, old_page_state & PAGE_WAS_MAPPED,
+ 				       anon_vma, true, ret_folios);
  		list_del(&dst->lru);
 -		migrate_folio_undo_dst(dst, true, put_new_folio, private);
 +		migrate_folio_undo_dst(dst, true, put_new_page, private);
  		dst = dst2;
  		dst2 = list_next_entry(dst, lru);
  	}
* Unmerged path mm/migrate.c
