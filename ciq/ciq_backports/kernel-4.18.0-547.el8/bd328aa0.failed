x86/decompressor: Avoid the need for a stack in the 32-bit trampoline

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-547.el8
commit-author Ard Biesheuvel <ardb@kernel.org>
commit bd328aa01ff77a45aeffea5fc4521854291db11f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-547.el8/bd328aa0.failed

The 32-bit trampoline no longer uses the stack for anything except
performing a far return back to long mode, and preserving the caller's
stack pointer value. Currently, the trampoline stack is placed in the
same page that carries the trampoline code, which means this page must
be mapped writable and executable, and the stack is therefore executable
as well.

Replace the far return with a far jump, so that the return address can
be pre-calculated and patched into the code before it is called. This
removes the need for a 32-bit addressable stack entirely, and in a later
patch, this will be taken advantage of by removing writable permissions
from (and adding executable permissions to) the trampoline code page
when booting via the EFI stub.

Note that the value of RSP still needs to be preserved explicitly across
the switch into 32-bit mode, as the register may get truncated to 32
bits.

	Signed-off-by: Ard Biesheuvel <ardb@kernel.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
	Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Link: https://lore.kernel.org/r/20230807162720.545787-12-ardb@kernel.org
(cherry picked from commit bd328aa01ff77a45aeffea5fc4521854291db11f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/head_64.S
diff --cc arch/x86/boot/compressed/head_64.S
index c4f3be19a58f,37fd7b7d683d..000000000000
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@@ -561,25 -526,59 +561,79 @@@ SYM_FUNC_END(efi64_stub_entry
  /*
   * Jump to the decompressed kernel.
   */
 -	movq	%r15, %rsi
  	jmp	*%rax
++<<<<<<< HEAD
 +
 +	.code32
 +/*
 + * This is the 32-bit trampoline that will be copied over to low memory.
 + *
 + * RDI contains the return address (might be above 4G).
 + * ECX contains the base address of the trampoline memory.
 + * Non zero RDX means trampoline needs to enable 5-level paging.
 + */
 +SYM_CODE_START(trampoline_32bit_src)
 +	/* Set up data and stack segments */
 +	movl	$__KERNEL_DS, %eax
 +	movl	%eax, %ds
 +	movl	%eax, %ss
 +
 +	/* Set up new stack */
 +	leal	TRAMPOLINE_32BIT_STACK_END(%ecx), %esp
 +
++=======
+ SYM_FUNC_END(.Lrelocated)
+ 
+ /*
+  * This is the 32-bit trampoline that will be copied over to low memory. It
+  * will be called using the ordinary 64-bit calling convention from code
+  * running in 64-bit mode.
+  *
+  * Return address is at the top of the stack (might be above 4G).
+  * The first argument (EDI) contains the 32-bit addressable base of the
+  * trampoline memory. A non-zero second argument (ESI) means that the
+  * trampoline needs to enable 5-level paging.
+  */
+ 	.section ".rodata", "a", @progbits
+ SYM_CODE_START(trampoline_32bit_src)
+ 	/*
+ 	 * Preserve live 64-bit registers on the stack: this is necessary
+ 	 * because the architecture does not guarantee that GPRs will retain
+ 	 * their full 64-bit values across a 32-bit mode switch.
+ 	 */
+ 	pushq	%r15
+ 	pushq	%rbp
+ 	pushq	%rbx
+ 
+ 	/* Preserve top half of RSP in a legacy mode GPR to avoid truncation */
+ 	movq	%rsp, %rbx
+ 	shrq	$32, %rbx
+ 
+ 	/* Switch to compatibility mode (CS.L = 0 CS.D = 1) via far return */
+ 	pushq	$__KERNEL32_CS
+ 	leaq	0f(%rip), %rax
+ 	pushq	%rax
+ 	lretq
+ 
+ 	/*
+ 	 * The 32-bit code below will do a far jump back to long mode and end
+ 	 * up here after reconfiguring the number of paging levels. First, the
+ 	 * stack pointer needs to be restored to its full 64-bit value before
+ 	 * the callee save register contents can be popped from the stack.
+ 	 */
+ .Lret:
+ 	shlq	$32, %rbx
+ 	orq	%rbx, %rsp
+ 
+ 	/* Restore the preserved 64-bit registers */
+ 	popq	%rbx
+ 	popq	%rbp
+ 	popq	%r15
+ 	retq
+ 
+ 	.code32
+ 0:
++>>>>>>> bd328aa01ff7 (x86/decompressor: Avoid the need for a stack in the 32-bit trampoline)
  	/* Disable paging */
  	movl	%cr0, %eax
  	btrl	$X86_CR0_PG_BIT, %eax
@@@ -638,25 -633,25 +692,42 @@@
  1:
  	movl	%eax, %cr4
  
++<<<<<<< HEAD
 +	/* Calculate address of paging_enabled() once we are executing in the trampoline */
 +	leal	.Lpaging_enabled - trampoline_32bit_src + TRAMPOLINE_32BIT_CODE_OFFSET(%ecx), %eax
 +
 +	/* Prepare the stack for far return to Long Mode */
 +	pushl	$__KERNEL_CS
 +	pushl	%eax
 +
++=======
++>>>>>>> bd328aa01ff7 (x86/decompressor: Avoid the need for a stack in the 32-bit trampoline)
  	/* Enable paging again. */
  	movl	%cr0, %eax
  	btsl	$X86_CR0_PG_BIT, %eax
  	movl	%eax, %cr0
  
- 	lret
+ 	/*
+ 	 * Return to the 64-bit calling code using LJMP rather than LRET, to
+ 	 * avoid the need for a 32-bit addressable stack. The destination
+ 	 * address will be adjusted after the template code is copied into a
+ 	 * 32-bit addressable buffer.
+ 	 */
+ .Ljmp:	ljmpl	$__KERNEL_CS, $(.Lret - trampoline_32bit_src)
  SYM_CODE_END(trampoline_32bit_src)
  
++<<<<<<< HEAD
 +	.code64
 +.Lpaging_enabled:
 +	/* Return from the trampoline */
 +	jmp	*%rdi
++=======
+ /*
+  * This symbol is placed right after trampoline_32bit_src() so its address can
+  * be used to infer the size of the trampoline code.
+  */
+ SYM_DATA(trampoline_ljmp_imm_offset, .word  .Ljmp + 1 - trampoline_32bit_src)
++>>>>>>> bd328aa01ff7 (x86/decompressor: Avoid the need for a stack in the 32-bit trampoline)
  
  	/*
           * The trampoline code has a size limit.
@@@ -665,8 -660,8 +736,13 @@@
  	 */
  	.org	trampoline_32bit_src + TRAMPOLINE_32BIT_CODE_SIZE
  
++<<<<<<< HEAD
 +	.code32
 +.Lno_longmode:
++=======
+ 	.text
+ SYM_FUNC_START_LOCAL_NOALIGN(.Lno_longmode)
++>>>>>>> bd328aa01ff7 (x86/decompressor: Avoid the need for a stack in the 32-bit trampoline)
  	/* This isn't an x86-64 CPU, so hang intentionally, we cannot continue */
  1:
  	hlt
* Unmerged path arch/x86/boot/compressed/head_64.S
diff --git a/arch/x86/boot/compressed/pgtable.h b/arch/x86/boot/compressed/pgtable.h
index cc9b2529a086..fe9ff11abe83 100644
--- a/arch/x86/boot/compressed/pgtable.h
+++ b/arch/x86/boot/compressed/pgtable.h
@@ -8,13 +8,13 @@
 #define TRAMPOLINE_32BIT_CODE_OFFSET	PAGE_SIZE
 #define TRAMPOLINE_32BIT_CODE_SIZE	0x80
 
-#define TRAMPOLINE_32BIT_STACK_END	TRAMPOLINE_32BIT_SIZE
-
 #ifndef __ASSEMBLER__
 
 extern unsigned long *trampoline_32bit;
 
 extern void trampoline_32bit_src(void *return_ptr);
 
+extern const u16 trampoline_ljmp_imm_offset;
+
 #endif /* __ASSEMBLER__ */
 #endif /* BOOT_COMPRESSED_PAGETABLE_H */
diff --git a/arch/x86/boot/compressed/pgtable_64.c b/arch/x86/boot/compressed/pgtable_64.c
index af8b429eafb0..c67117c1f7e1 100644
--- a/arch/x86/boot/compressed/pgtable_64.c
+++ b/arch/x86/boot/compressed/pgtable_64.c
@@ -108,6 +108,7 @@ static unsigned long find_trampoline_placement(void)
 struct paging_config paging_prepare(void *rmode)
 {
 	struct paging_config paging_config = {};
+	void *tramp_code;
 
 	/* Initialize boot_params. Required for cmdline_find_option_bool(). */
 	boot_params = rmode;
@@ -147,9 +148,18 @@ struct paging_config paging_prepare(void *rmode)
 	memset(trampoline_32bit, 0, TRAMPOLINE_32BIT_SIZE);
 
 	/* Copy trampoline code in place */
-	memcpy(trampoline_32bit + TRAMPOLINE_32BIT_CODE_OFFSET / sizeof(unsigned long),
+	tramp_code = memcpy(trampoline_32bit +
+			TRAMPOLINE_32BIT_CODE_OFFSET / sizeof(unsigned long),
 			&trampoline_32bit_src, TRAMPOLINE_32BIT_CODE_SIZE);
 
+	/*
+	 * Avoid the need for a stack in the 32-bit trampoline code, by using
+	 * LJMP rather than LRET to return back to long mode. LJMP takes an
+	 * immediate absolute address, which needs to be adjusted based on the
+	 * placement of the trampoline.
+	 */
+	*(u32 *)(tramp_code + trampoline_ljmp_imm_offset) += (unsigned long)tramp_code;
+
 	/*
 	 * The code below prepares page table in trampoline memory.
 	 *
