x86/decompressor: Don't rely on upper 32 bits of GPRs being preserved

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-547.el8
commit-author Ard Biesheuvel <ardb@kernel.org>
commit 264b82fdb4989cf6a44a2bcd0c6ea05e8026b2ac
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-547.el8/264b82fd.failed

The 4-to-5 level mode switch trampoline disables long mode and paging in
order to be able to flick the LA57 bit. According to section 3.4.1.1 of
the x86 architecture manual [0], 64-bit GPRs might not retain the upper
32 bits of their contents across such a mode switch.

Given that RBP, RBX and RSI are live at this point, preserve them on the
stack, along with the return address that might be above 4G as well.

[0] Intel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 1: Basic Architecture

  "Because the upper 32 bits of 64-bit general-purpose registers are
   undefined in 32-bit modes, the upper 32 bits of any general-purpose
   register are not preserved when switching from 64-bit mode to a 32-bit
   mode (to protected mode or compatibility mode). Software must not
   depend on these bits to maintain a value after a 64-bit to 32-bit
   mode switch."

Fixes: 194a9749c73d650c ("x86/boot/compressed/64: Handle 5-level paging boot if kernel is above 4G")
	Signed-off-by: Ard Biesheuvel <ardb@kernel.org>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
Link: https://lore.kernel.org/r/20230807162720.545787-2-ardb@kernel.org
(cherry picked from commit 264b82fdb4989cf6a44a2bcd0c6ea05e8026b2ac)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/head_64.S
diff --cc arch/x86/boot/compressed/head_64.S
index 6cee4cfafe11,f732426d3b48..000000000000
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@@ -404,8 -485,13 +418,13 @@@ SYM_CODE_START(startup_64
  	pushq	%rax
  	lretq
  trampoline_return:
+ 	/* Restore live 64-bit registers */
+ 	popq	%rsi
+ 	popq	%rbx
+ 	popq	%rbp
+ 
  	/* Restore the stack, the 32-bit trampoline uses its own stack */
 -	leaq	rva(boot_stack_end)(%rbx), %rsp
 +	leaq	boot_stack_end(%rbx), %rsp
  
  	/*
  	 * cleanup_trampoline() would restore trampoline memory.
@@@ -654,9 -685,10 +670,14 @@@ SYM_CODE_START(trampoline_32bit_src
  SYM_CODE_END(trampoline_32bit_src)
  
  	.code64
 -SYM_FUNC_START_LOCAL_NOALIGN(.Lpaging_enabled)
 +.Lpaging_enabled:
  	/* Return from the trampoline */
++<<<<<<< HEAD
 +	jmp	*%rdi
++=======
+ 	retq
+ SYM_FUNC_END(.Lpaging_enabled)
++>>>>>>> 264b82fdb498 (x86/decompressor: Don't rely on upper 32 bits of GPRs being preserved)
  
  	/*
           * The trampoline code has a size limit.
* Unmerged path arch/x86/boot/compressed/head_64.S
