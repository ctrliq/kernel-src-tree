docs/core-api/mm: fix return value descriptions in mm/

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Mike Rapoport <rppt@linux.ibm.com>
commit a862f68a8b360086f248cbc3606029441b5f5197
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/a862f68a.failed

Many kernel-doc comments in mm/ have the return value descriptions
either misformatted or omitted at all which makes kernel-doc script
unhappy:

$ make V=1 htmldocs
...
./mm/util.c:36: info: Scanning doc for kstrdup
./mm/util.c:41: warning: No description found for return value of 'kstrdup'
./mm/util.c:57: info: Scanning doc for kstrdup_const
./mm/util.c:66: warning: No description found for return value of 'kstrdup_const'
./mm/util.c:75: info: Scanning doc for kstrndup
./mm/util.c:83: warning: No description found for return value of 'kstrndup'
...

Fixing the formatting and adding the missing return value descriptions
eliminates ~100 such warnings.

Link: http://lkml.kernel.org/r/1549549644-4903-4-git-send-email-rppt@linux.ibm.com
	Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit a862f68a8b360086f248cbc3606029441b5f5197)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/vmalloc.c
diff --cc mm/vmalloc.c
index c82a0db1aefc,e86ba6e74b50..000000000000
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@@ -1443,13 -1426,15 +1443,21 @@@ struct vm_struct *__get_vm_area_caller(
  }
  
  /**
 - * get_vm_area - reserve a contiguous kernel virtual area
 - * @size:	 size of the area
 - * @flags:	 %VM_IOREMAP for I/O mappings or VM_ALLOC
 - *
 + *	get_vm_area  -  reserve a contiguous kernel virtual area
 + *	@size:		size of the area
 + *	@flags:		%VM_IOREMAP for I/O mappings or VM_ALLOC
 + *
++<<<<<<< HEAD
 + *	Search an area of @size in the kernel virtual mapping area,
 + *	and reserved it for out purposes.  Returns the area descriptor
 + *	on success or %NULL on failure.
++=======
+  * Search an area of @size in the kernel virtual mapping area,
+  * and reserved it for out purposes.  Returns the area descriptor
+  * on success or %NULL on failure.
+  *
+  * Return: the area descriptor on success or %NULL on failure.
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  struct vm_struct *get_vm_area(unsigned long size, unsigned long flags)
  {
@@@ -1466,12 -1451,14 +1474,20 @@@ struct vm_struct *get_vm_area_caller(un
  }
  
  /**
 - * find_vm_area - find a continuous kernel virtual area
 - * @addr:	  base address
 - *
 + *	find_vm_area  -  find a continuous kernel virtual area
 + *	@addr:		base address
 + *
++<<<<<<< HEAD
 + *	Search for the kernel VM area starting at @addr, and return it.
 + *	It is up to the caller to do all required locking to keep the returned
 + *	pointer valid.
++=======
+  * Search for the kernel VM area starting at @addr, and return it.
+  * It is up to the caller to do all required locking to keep the returned
+  * pointer valid.
+  *
+  * Return: pointer to the found area or %NULL on faulure
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  struct vm_struct *find_vm_area(const void *addr)
  {
@@@ -1485,12 -1472,14 +1501,20 @@@
  }
  
  /**
 - * remove_vm_area - find and remove a continuous kernel virtual area
 - * @addr:	    base address
 - *
 + *	remove_vm_area  -  find and remove a continuous kernel virtual area
 + *	@addr:		base address
 + *
++<<<<<<< HEAD
 + *	Search for the kernel VM area starting at @addr, and remove it.
 + *	This function returns the found VM area, but using it is NOT safe
 + *	on SMP machines, except for its size or flags.
++=======
+  * Search for the kernel VM area starting at @addr, and remove it.
+  * This function returns the found VM area, but using it is NOT safe
+  * on SMP machines, except for its size or flags.
+  *
+  * Return: pointer to the found area or %NULL on faulure
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  struct vm_struct *remove_vm_area(const void *addr)
  {
@@@ -1693,17 -1634,19 +1717,24 @@@ void vunmap(const void *addr
  EXPORT_SYMBOL(vunmap);
  
  /**
 - * vmap - map an array of pages into virtually contiguous space
 - * @pages: array of page pointers
 - * @count: number of pages to map
 - * @flags: vm_area->flags
 - * @prot: page protection for the mapping
 - *
 + *	vmap  -  map an array of pages into virtually contiguous space
 + *	@pages:		array of page pointers
 + *	@count:		number of pages to map
 + *	@flags:		vm_area->flags
 + *	@prot:		page protection for the mapping
 + *
++<<<<<<< HEAD
 + *	Maps @count pages from @pages into contiguous kernel virtual
 + *	space.
++=======
+  * Maps @count pages from @pages into contiguous kernel virtual
+  * space.
+  *
+  * Return: the address of the area or %NULL on failure
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  void *vmap(struct page **pages, unsigned int count,
 -	   unsigned long flags, pgprot_t prot)
 +		unsigned long flags, pgprot_t prot)
  {
  	struct vm_struct *area;
  	unsigned long size;		/* In bytes */
@@@ -1790,20 -1733,22 +1821,28 @@@ fail
  }
  
  /**
 - * __vmalloc_node_range - allocate virtually contiguous memory
 - * @size:		  allocation size
 - * @align:		  desired alignment
 - * @start:		  vm area range start
 - * @end:		  vm area range end
 - * @gfp_mask:		  flags for the page level allocator
 - * @prot:		  protection mask for the allocated pages
 - * @vm_flags:		  additional vm area flags (e.g. %VM_NO_GUARD)
 - * @node:		  node to use for allocation or NUMA_NO_NODE
 - * @caller:		  caller's return address
 - *
 + *	__vmalloc_node_range  -  allocate virtually contiguous memory
 + *	@size:		allocation size
 + *	@align:		desired alignment
 + *	@start:		vm area range start
 + *	@end:		vm area range end
 + *	@gfp_mask:	flags for the page level allocator
 + *	@prot:		protection mask for the allocated pages
 + *	@vm_flags:	additional vm area flags (e.g. %VM_NO_GUARD)
 + *	@node:		node to use for allocation or NUMA_NO_NODE
 + *	@caller:	caller's return address
 + *
++<<<<<<< HEAD
 + *	Allocate enough pages to cover @size from the page level
 + *	allocator with @gfp_mask flags.  Map them into contiguous
 + *	kernel virtual space, using a pagetable protection of @prot.
++=======
+  * Allocate enough pages to cover @size from the page level
+  * allocator with @gfp_mask flags.  Map them into contiguous
+  * kernel virtual space, using a pagetable protection of @prot.
+  *
+  * Return: the address of the area or %NULL on failure
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  void *__vmalloc_node_range(unsigned long size, unsigned long align,
  			unsigned long start, unsigned long end, gfp_t gfp_mask,
@@@ -1844,25 -1789,35 +1883,32 @@@ fail
  	return NULL;
  }
  
 -/*
 - * This is only for performance analysis of vmalloc and stress purpose.
 - * It is required by vmalloc test module, therefore do not use it other
 - * than that.
 - */
 -#ifdef CONFIG_TEST_VMALLOC_MODULE
 -EXPORT_SYMBOL_GPL(__vmalloc_node_range);
 -#endif
 -
  /**
 - * __vmalloc_node - allocate virtually contiguous memory
 - * @size:	    allocation size
 - * @align:	    desired alignment
 - * @gfp_mask:	    flags for the page level allocator
 - * @prot:	    protection mask for the allocated pages
 - * @node:	    node to use for allocation or NUMA_NO_NODE
 - * @caller:	    caller's return address
 - *
 - * Allocate enough pages to cover @size from the page level
 - * allocator with @gfp_mask flags.  Map them into contiguous
 - * kernel virtual space, using a pagetable protection of @prot.
 - *
 - * Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL
 - * and __GFP_NOFAIL are not supported
 - *
 + *	__vmalloc_node  -  allocate virtually contiguous memory
 + *	@size:		allocation size
 + *	@align:		desired alignment
 + *	@gfp_mask:	flags for the page level allocator
 + *	@prot:		protection mask for the allocated pages
 + *	@node:		node to use for allocation or NUMA_NO_NODE
 + *	@caller:	caller's return address
 + *
 + *	Allocate enough pages to cover @size from the page level
 + *	allocator with @gfp_mask flags.  Map them into contiguous
 + *	kernel virtual space, using a pagetable protection of @prot.
 + *
 + *	Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL
 + *	and __GFP_NOFAIL are not supported
 + *
 + *	Any use of gfp flags outside of GFP_KERNEL should be consulted
 + *	with mm people.
 + *
++<<<<<<< HEAD
++=======
+  * Any use of gfp flags outside of GFP_KERNEL should be consulted
+  * with mm people.
+  *
+  * Return: pointer to the allocated memory or %NULL on error
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  static void *__vmalloc_node(unsigned long size, unsigned long align,
  			    gfp_t gfp_mask, pgprot_t prot,
@@@ -1894,13 -1849,16 +1940,23 @@@ void *__vmalloc_node_flags_caller(unsig
  }
  
  /**
 - * vmalloc - allocate virtually contiguous memory
 - * @size:    allocation size
 - *
 + *	vmalloc  -  allocate virtually contiguous memory
 + *	@size:		allocation size
 + *	Allocate enough pages to cover @size from the page level
 + *	allocator and map them into contiguous kernel virtual space.
 + *
++<<<<<<< HEAD
 + *	For tight control over page level allocator and protection flags
 + *	use __vmalloc() instead.
++=======
+  * Allocate enough pages to cover @size from the page level
+  * allocator and map them into contiguous kernel virtual space.
+  *
+  * For tight control over page level allocator and protection flags
+  * use __vmalloc() instead.
+  *
+  * Return: pointer to the allocated memory or %NULL on error
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  void *vmalloc(unsigned long size)
  {
@@@ -1910,14 -1868,17 +1966,25 @@@
  EXPORT_SYMBOL(vmalloc);
  
  /**
 - * vzalloc - allocate virtually contiguous memory with zero fill
 - * @size:    allocation size
 - *
 + *	vzalloc - allocate virtually contiguous memory with zero fill
 + *	@size:	allocation size
 + *	Allocate enough pages to cover @size from the page level
 + *	allocator and map them into contiguous kernel virtual space.
 + *	The memory allocated is set to zero.
 + *
++<<<<<<< HEAD
 + *	For tight control over page level allocator and protection flags
 + *	use __vmalloc() instead.
++=======
+  * Allocate enough pages to cover @size from the page level
+  * allocator and map them into contiguous kernel virtual space.
+  * The memory allocated is set to zero.
+  *
+  * For tight control over page level allocator and protection flags
+  * use __vmalloc() instead.
+  *
+  * Return: pointer to the allocated memory or %NULL on error
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  void *vzalloc(unsigned long size)
  {
@@@ -1951,15 -1906,17 +2020,22 @@@ void *vmalloc_user(unsigned long size
  EXPORT_SYMBOL(vmalloc_user);
  
  /**
 - * vmalloc_node - allocate memory on a specific node
 - * @size:	  allocation size
 - * @node:	  numa node
 + *	vmalloc_node  -  allocate memory on a specific node
 + *	@size:		allocation size
 + *	@node:		numa node
   *
 - * Allocate enough pages to cover @size from the page level
 - * allocator and map them into contiguous kernel virtual space.
 + *	Allocate enough pages to cover @size from the page level
 + *	allocator and map them into contiguous kernel virtual space.
   *
++<<<<<<< HEAD
 + *	For tight control over page level allocator and protection flags
 + *	use __vmalloc() instead.
++=======
+  * For tight control over page level allocator and protection flags
+  * use __vmalloc() instead.
+  *
+  * Return: pointer to the allocated memory or %NULL on error
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  void *vmalloc_node(unsigned long size, int node)
  {
@@@ -1988,46 -1947,22 +2066,52 @@@ void *vzalloc_node(unsigned long size, 
  EXPORT_SYMBOL(vzalloc_node);
  
  /**
 - * vmalloc_exec - allocate virtually contiguous, executable memory
 - * @size:	  allocation size
 + * vmalloc_user_node_flags - allocate memory for userspace on a specific node
 + * @size: allocation size
 + * @node: numa node
 + * @flags: flags for the page level allocator
   *
 - * Kernel-internal function to allocate enough pages to cover @size
 - * the page level allocator and map them into contiguous and
 - * executable kernel virtual space.
 + * The resulting memory area is zeroed so it can be mapped to userspace
 + * without leaking data.
   *
++<<<<<<< HEAD
++=======
+  * For tight control over page level allocator and protection flags
+  * use __vmalloc() instead.
+  *
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   * Return: pointer to the allocated memory or %NULL on error
   */
 +void *vmalloc_user_node_flags(unsigned long size, int node, gfp_t flags)
 +{
 +	return __vmalloc_node_range(size, SHMLBA,  VMALLOC_START, VMALLOC_END,
 +				    flags | __GFP_ZERO, PAGE_KERNEL,
 +				    VM_USERMAP, node,
 +				    __builtin_return_address(0));
 +}
 +EXPORT_SYMBOL(vmalloc_user_node_flags);
 +
 +#ifndef PAGE_KERNEL_EXEC
 +# define PAGE_KERNEL_EXEC PAGE_KERNEL
 +#endif
 +
 +/**
 + *	vmalloc_exec  -  allocate virtually contiguous, executable memory
 + *	@size:		allocation size
 + *
 + *	Kernel-internal function to allocate enough pages to cover @size
 + *	the page level allocator and map them into contiguous and
 + *	executable kernel virtual space.
 + *
 + *	For tight control over page level allocator and protection flags
 + *	use __vmalloc() instead.
 + */
 +
  void *vmalloc_exec(unsigned long size)
  {
 -	return __vmalloc_node(size, 1, GFP_KERNEL, PAGE_KERNEL_EXEC,
 -			      NUMA_NO_NODE, __builtin_return_address(0));
 +	return __vmalloc_node_range(size, 1, VMALLOC_START, VMALLOC_END,
 +			GFP_KERNEL, PAGE_KERNEL_EXEC, VM_FLUSH_RESET_PERMS,
 +			NUMA_NO_NODE, __builtin_return_address(0));
  }
  
  #if defined(CONFIG_64BIT) && defined(CONFIG_ZONE_DMA32)
@@@ -2043,11 -1978,13 +2127,18 @@@
  #endif
  
  /**
 - * vmalloc_32 - allocate virtually contiguous memory (32bit addressable)
 - * @size:	allocation size
 + *	vmalloc_32  -  allocate virtually contiguous memory (32bit addressable)
 + *	@size:		allocation size
   *
++<<<<<<< HEAD
 + *	Allocate enough 32bit PA addressable pages to cover @size from the
 + *	page level allocator and map them into contiguous kernel virtual space.
++=======
+  * Allocate enough 32bit PA addressable pages to cover @size from the
+  * page level allocator and map them into contiguous kernel virtual space.
+  *
+  * Return: pointer to the allocated memory or %NULL on error
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
  void *vmalloc_32(unsigned long size)
  {
@@@ -2160,31 -2093,29 +2253,50 @@@ static int aligned_vwrite(char *buf, ch
  }
  
  /**
 - * vread() - read vmalloc area in a safe way.
 - * @buf:     buffer for reading data
 - * @addr:    vm address.
 - * @count:   number of bytes to be read.
 - *
 + *	vread() -  read vmalloc area in a safe way.
 + *	@buf:		buffer for reading data
 + *	@addr:		vm address.
 + *	@count:		number of bytes to be read.
 + *
++<<<<<<< HEAD
 + *	Returns # of bytes which addr and buf should be increased.
 + *	(same number to @count). Returns 0 if [addr...addr+count) doesn't
 + *	includes any intersect with alive vmalloc area.
 + *
 + *	This function checks that addr is a valid vmalloc'ed area, and
 + *	copy data from that area to a given buffer. If the given memory range
 + *	of [addr...addr+count) includes some valid address, data is copied to
 + *	proper area of @buf. If there are memory holes, they'll be zero-filled.
 + *	IOREMAP area is treated as memory hole and no copy is done.
++=======
+  * This function checks that addr is a valid vmalloc'ed area, and
+  * copy data from that area to a given buffer. If the given memory range
+  * of [addr...addr+count) includes some valid address, data is copied to
+  * proper area of @buf. If there are memory holes, they'll be zero-filled.
+  * IOREMAP area is treated as memory hole and no copy is done.
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
 + *
 + *	If [addr...addr+count) doesn't includes any intersects with alive
 + *	vm_struct area, returns 0. @buf should be kernel's buffer.
   *
 - * If [addr...addr+count) doesn't includes any intersects with alive
 - * vm_struct area, returns 0. @buf should be kernel's buffer.
 + *	Note: In usual ops, vread() is never necessary because the caller
 + *	should know vmalloc() area is valid and can use memcpy().
 + *	This is for routines which have to access vmalloc area without
 + *	any informaion, as /dev/kmem.
   *
++<<<<<<< HEAD
++=======
+  * Note: In usual ops, vread() is never necessary because the caller
+  * should know vmalloc() area is valid and can use memcpy().
+  * This is for routines which have to access vmalloc area without
+  * any informaion, as /dev/kmem.
+  *
+  * Return: number of bytes for which addr and buf should be increased
+  * (same number as @count) or %0 if [addr...addr+count) doesn't
+  * include any intersection with valid vmalloc area
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
 +
  long vread(char *buf, char *addr, unsigned long count)
  {
  	struct vmap_area *va;
@@@ -2241,31 -2172,29 +2353,50 @@@ finished
  }
  
  /**
 - * vwrite() - write vmalloc area in a safe way.
 - * @buf:      buffer for source data
 - * @addr:     vm address.
 - * @count:    number of bytes to be read.
 - *
 + *	vwrite() -  write vmalloc area in a safe way.
 + *	@buf:		buffer for source data
 + *	@addr:		vm address.
 + *	@count:		number of bytes to be read.
 + *
++<<<<<<< HEAD
 + *	Returns # of bytes which addr and buf should be incresed.
 + *	(same number to @count).
 + *	If [addr...addr+count) doesn't includes any intersect with valid
 + *	vmalloc area, returns 0.
 + *
 + *	This function checks that addr is a valid vmalloc'ed area, and
 + *	copy data from a buffer to the given addr. If specified range of
 + *	[addr...addr+count) includes some valid address, data is copied from
 + *	proper area of @buf. If there are memory holes, no copy to hole.
 + *	IOREMAP area is treated as memory hole and no copy is done.
++=======
+  * This function checks that addr is a valid vmalloc'ed area, and
+  * copy data from a buffer to the given addr. If specified range of
+  * [addr...addr+count) includes some valid address, data is copied from
+  * proper area of @buf. If there are memory holes, no copy to hole.
+  * IOREMAP area is treated as memory hole and no copy is done.
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   *
 - * If [addr...addr+count) doesn't includes any intersects with alive
 - * vm_struct area, returns 0. @buf should be kernel's buffer.
 + *	If [addr...addr+count) doesn't includes any intersects with alive
 + *	vm_struct area, returns 0. @buf should be kernel's buffer.
   *
++<<<<<<< HEAD
 + *	Note: In usual ops, vwrite() is never necessary because the caller
 + *	should know vmalloc() area is valid and can use memcpy().
 + *	This is for routines which have to access vmalloc area without
 + *	any informaion, as /dev/kmem.
++=======
+  * Note: In usual ops, vwrite() is never necessary because the caller
+  * should know vmalloc() area is valid and can use memcpy().
+  * This is for routines which have to access vmalloc area without
+  * any informaion, as /dev/kmem.
+  *
+  * Return: number of bytes for which addr and buf should be
+  * increased (same number as @count) or %0 if [addr...addr+count)
+  * doesn't include any intersection with valid vmalloc area
++>>>>>>> a862f68a8b36 (docs/core-api/mm: fix return value descriptions in mm/)
   */
 +
  long vwrite(char *buf, char *addr, unsigned long count)
  {
  	struct vmap_area *va;
diff --git a/mm/dmapool.c b/mm/dmapool.c
index 6d4b97e7e9e9..76a160083506 100644
--- a/mm/dmapool.c
+++ b/mm/dmapool.c
@@ -114,10 +114,9 @@ static DEVICE_ATTR(pools, 0444, show_pools, NULL);
  * @size: size of the blocks in this pool.
  * @align: alignment requirement for blocks; must be a power of two
  * @boundary: returned blocks won't cross this power of two boundary
- * Context: !in_interrupt()
+ * Context: not in_interrupt()
  *
- * Returns a dma allocation pool with the requested characteristics, or
- * null if one can't be created.  Given one of these pools, dma_pool_alloc()
+ * Given one of these pools, dma_pool_alloc()
  * may be used to allocate memory.  Such memory will all have "consistent"
  * DMA mappings, accessible by the device and its driver without using
  * cache flushing primitives.  The actual size of blocks allocated may be
@@ -127,6 +126,9 @@ static DEVICE_ATTR(pools, 0444, show_pools, NULL);
  * cross that size boundary.  This is useful for devices which have
  * addressing restrictions on individual DMA transfers, such as not crossing
  * boundaries of 4KBytes.
+ *
+ * Return: a dma allocation pool with the requested characteristics, or
+ * %NULL if one can't be created.
  */
 struct dma_pool *dma_pool_create(const char *name, struct device *dev,
 				 size_t size, size_t align, size_t boundary)
@@ -313,7 +315,7 @@ EXPORT_SYMBOL(dma_pool_destroy);
  * @mem_flags: GFP_* bitmask
  * @handle: pointer to dma address of block
  *
- * This returns the kernel virtual address of a currently unused block,
+ * Return: the kernel virtual address of a currently unused block,
  * and reports its dma address through the handle.
  * If such a memory block can't be allocated, %NULL is returned.
  */
@@ -498,6 +500,9 @@ static int dmam_pool_match(struct device *dev, void *res, void *match_data)
  *
  * Managed dma_pool_create().  DMA pool created with this function is
  * automatically destroyed on driver detach.
+ *
+ * Return: a managed dma allocation pool with the requested
+ * characteristics, or %NULL if one can't be created.
  */
 struct dma_pool *dmam_pool_create(const char *name, struct device *dev,
 				  size_t size, size_t align, size_t allocation)
diff --git a/mm/filemap.c b/mm/filemap.c
index af446efa3a95..4d90669f3e76 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -394,6 +394,8 @@ static int filemap_check_and_keep_errors(struct address_space *mapping)
  * opposed to a regular memory cleansing writeback.  The difference between
  * these two operations is that if a dirty page/buffer is encountered, it must
  * be waited upon, and not just skipped over.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
 				loff_t end, int sync_mode)
@@ -440,6 +442,8 @@ EXPORT_SYMBOL(filemap_fdatawrite_range);
  *
  * This is a mostly non-blocking flush.  Not suitable for data-integrity
  * purposes - I/O may not be started against all dirty pages.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int filemap_flush(struct address_space *mapping)
 {
@@ -455,6 +459,9 @@ EXPORT_SYMBOL(filemap_flush);
  *
  * Find at least one page in the range supplied, usually used to check if
  * direct writing in this range will trigger a writeback.
+ *
+ * Return: %true if at least one page exists in the specified range,
+ * %false otherwise.
  */
 bool filemap_range_has_page(struct address_space *mapping,
 			   loff_t start_byte, loff_t end_byte)
@@ -531,6 +538,8 @@ static void __filemap_fdatawait_range(struct address_space *mapping,
  * Since the error status of the address space is cleared by this function,
  * callers are responsible for checking the return value and handling and/or
  * reporting the error.
+ *
+ * Return: error status of the address space.
  */
 int filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte,
 			    loff_t end_byte)
@@ -575,6 +584,8 @@ EXPORT_SYMBOL(filemap_fdatawait_range_keep_errors);
  * Since the error status of the file is advanced by this function,
  * callers are responsible for checking the return value and handling and/or
  * reporting the error.
+ *
+ * Return: error status of the address space vs. the file->f_wb_err cursor.
  */
 int file_fdatawait_range(struct file *file, loff_t start_byte, loff_t end_byte)
 {
@@ -596,6 +607,8 @@ EXPORT_SYMBOL(file_fdatawait_range);
  * Use this function if callers don't handle errors themselves.  Expected
  * call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),
  * fsfreeze(8)
+ *
+ * Return: error status of the address space.
  */
 int filemap_fdatawait_keep_errors(struct address_space *mapping)
 {
@@ -647,6 +660,8 @@ EXPORT_SYMBOL(filemap_write_and_wait);
  *
  * Note that @lend is inclusive (describes the last byte to be written) so
  * that this function can be used to write to the very end-of-file (end = -1).
+ *
+ * Return: error status of the address space.
  */
 int filemap_write_and_wait_range(struct address_space *mapping,
 				 loff_t lstart, loff_t lend)
@@ -702,6 +717,8 @@ EXPORT_SYMBOL(__filemap_set_wb_err);
  * While we handle mapping->wb_err with atomic operations, the f_wb_err
  * value is protected by the f_lock since we must ensure that it reflects
  * the latest value swapped in for this file descriptor.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int file_check_and_advance_wb_err(struct file *file)
 {
@@ -744,6 +761,8 @@ EXPORT_SYMBOL(file_check_and_advance_wb_err);
  *
  * After writing out and waiting on the data, we check and advance the
  * f_wb_err cursor to the latest value, and return any errors detected there.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int file_write_and_wait_range(struct file *file, loff_t lstart, loff_t lend)
 {
@@ -777,6 +796,8 @@ EXPORT_SYMBOL(file_write_and_wait_range);
  * caller must do that.
  *
  * The remove + add is atomic.  This function cannot fail.
+ *
+ * Return: %0
  */
 int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask)
 {
@@ -891,6 +912,8 @@ static int __add_to_page_cache_locked(struct page *page,
  *
  * This function is used to add a page to the pagecache. It must be locked.
  * This function does not add the page to the LRU.  The caller must do that.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
 		pgoff_t offset, gfp_t gfp_mask)
@@ -1484,7 +1507,7 @@ EXPORT_SYMBOL(page_cache_prev_miss);
  * If the slot holds a shadow entry of a previously evicted page, or a
  * swap entry from shmem/tmpfs, it is returned.
  *
- * Otherwise, %NULL is returned.
+ * Return: the found page or shadow entry, %NULL if nothing is found.
  */
 struct page *find_get_entry(struct address_space *mapping, pgoff_t offset)
 {
@@ -1542,9 +1565,9 @@ EXPORT_SYMBOL(find_get_entry);
  * If the slot holds a shadow entry of a previously evicted page, or a
  * swap entry from shmem/tmpfs, it is returned.
  *
- * Otherwise, %NULL is returned.
- *
  * find_lock_entry() may sleep.
+ *
+ * Return: the found page or shadow entry, %NULL if nothing is found.
  */
 struct page *find_lock_entry(struct address_space *mapping, pgoff_t offset)
 {
@@ -1584,12 +1607,14 @@ EXPORT_SYMBOL(find_lock_entry);
  * - FGP_CREAT: If page is not present then a new page is allocated using
  *   @gfp_mask and added to the page cache and the VM's LRU
  *   list. The page is returned locked and with an increased
- *   refcount. Otherwise, NULL is returned.
+ *   refcount.
  *
  * If FGP_LOCK or FGP_CREAT are specified then the function may sleep even
  * if the GFP flags specified for FGP_CREAT are atomic.
  *
  * If there is a page cache page, it is returned with an increased refcount.
+ *
+ * Return: the found page or %NULL otherwise.
  */
 struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 	int fgp_flags, gfp_t gfp_mask)
@@ -1677,8 +1702,7 @@ EXPORT_SYMBOL(pagecache_get_page);
  * Any shadow entries of evicted pages, or swap entries from
  * shmem/tmpfs, are included in the returned array.
  *
- * find_get_entries() returns the number of pages and shadow entries
- * which were found.
+ * Return: the number of pages and shadow entries which were found.
  */
 unsigned find_get_entries(struct address_space *mapping,
 			  pgoff_t start, unsigned int nr_entries,
@@ -1748,8 +1772,8 @@ unsigned find_get_entries(struct address_space *mapping,
  * indexes.  There may be holes in the indices due to not-present pages.
  * We also update @start to index the next page for the traversal.
  *
- * find_get_pages_range() returns the number of pages which were found. If this
- * number is smaller than @nr_pages, the end of specified range has been
+ * Return: the number of pages which were found. If this number is
+ * smaller than @nr_pages, the end of specified range has been
  * reached.
  */
 unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
@@ -1822,7 +1846,7 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
  * find_get_pages_contig() works exactly like find_get_pages(), except
  * that the returned number of pages are guaranteed to be contiguous.
  *
- * find_get_pages_contig() returns the number of pages which were found.
+ * Return: the number of pages which were found.
  */
 unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index,
 			       unsigned int nr_pages, struct page **pages)
@@ -1893,6 +1917,8 @@ EXPORT_SYMBOL(find_get_pages_contig);
  *
  * Like find_get_pages, except we only return pages which are tagged with
  * @tag.   We update @index to index the next page for the traversal.
+ *
+ * Return: the number of pages which were found.
  */
 unsigned find_get_pages_range_tag(struct address_space *mapping, pgoff_t *index,
 			pgoff_t end, xa_mark_t tag, unsigned int nr_pages,
@@ -1970,6 +1996,8 @@ EXPORT_SYMBOL(find_get_pages_range_tag);
  *
  * Like find_get_entries, except we only return entries which are tagged with
  * @tag.
+ *
+ * Return: the number of entries which were found.
  */
 unsigned find_get_entries_tag(struct address_space *mapping, pgoff_t start,
 			xa_mark_t tag, unsigned int nr_entries,
@@ -2055,6 +2083,10 @@ static void shrink_readahead_size_eio(struct file *filp,
  *
  * This is really ugly. But the goto's actually try to clarify some
  * of the logic when it comes to error handling etc.
+ *
+ * Return:
+ * * total number of bytes copied, including those the were already @written
+ * * negative error code if nothing was copied
  */
 static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		struct iov_iter *iter, ssize_t written)
@@ -2316,6 +2348,9 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
  *
  * This is the "read_iter()" routine for all filesystems
  * that can use the page cache directly.
+ * Return:
+ * * number of bytes copied, even for partial reads
+ * * negative error code if nothing was read
  */
 ssize_t
 generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
@@ -2383,6 +2418,8 @@ EXPORT_SYMBOL(generic_file_read_iter);
  *
  * This adds the requested page to the page cache if it isn't already there,
  * and schedules an I/O to read in its contents from disk.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 static int page_cache_read(struct file *file, pgoff_t offset, gfp_t gfp_mask)
 {
@@ -2497,6 +2534,8 @@ static void do_async_mmap_readahead(struct vm_area_struct *vma,
  * has not been released.
  *
  * We never return with VM_FAULT_RETRY and a bit from VM_FAULT_ERROR set.
+ *
+ * Return: bitwise-OR of %VM_FAULT_ codes.
  */
 vm_fault_t filemap_fault(struct vm_fault *vmf)
 {
@@ -2884,6 +2923,8 @@ static struct page *do_read_cache_page(struct address_space *mapping,
  * not set, try to fill the page and wait for it to become unlocked.
  *
  * If the page does not get brought uptodate, return -EIO.
+ *
+ * Return: up to date page on success, ERR_PTR() on failure.
  */
 struct page *read_cache_page(struct address_space *mapping,
 				pgoff_t index,
@@ -2904,6 +2945,8 @@ EXPORT_SYMBOL(read_cache_page);
  * any new page allocations done using the specified allocation flags.
  *
  * If the page does not get brought uptodate, return -EIO.
+ *
+ * Return: up to date page on success, ERR_PTR() on failure.
  */
 struct page *read_cache_page_gfp(struct address_space *mapping,
 				pgoff_t index,
@@ -3355,6 +3398,10 @@ EXPORT_SYMBOL(generic_perform_write);
  * This function does *not* take care of syncing data in case of O_SYNC write.
  * A caller has to handle it. This is mainly due to the fact that we want to
  * avoid syncing under i_mutex.
+ *
+ * Return:
+ * * number of bytes written, even for truncated writes
+ * * negative error code if no data has been written at all
  */
 ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 {
@@ -3439,6 +3486,10 @@ EXPORT_SYMBOL(__generic_file_write_iter);
  * This is a wrapper around __generic_file_write_iter() to be used by most
  * filesystems. It takes care of syncing the file in case of O_SYNC file
  * and acquires i_mutex as needed.
+ * Return:
+ * * negative error code if no data has been written at all of
+ *   vfs_fsync_range() failed for a synchronous write
+ * * number of bytes written, even for truncated writes
  */
 ssize_t generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 {
@@ -3465,8 +3516,7 @@ EXPORT_SYMBOL(generic_file_write_iter);
  * @gfp_mask: memory allocation flags (and I/O mode)
  *
  * The address_space is to try to release any data against the page
- * (presumably at page->private).  If the release was successful, return '1'.
- * Otherwise return zero.
+ * (presumably at page->private).
  *
  * This may also be called if PG_fscache is set on a page, indicating that the
  * page is known to the local caching routines.
@@ -3474,6 +3524,7 @@ EXPORT_SYMBOL(generic_file_write_iter);
  * The @gfp_mask argument specifies whether I/O may be performed to release
  * this page (__GFP_IO), and whether the call may block (__GFP_RECLAIM & __GFP_FS).
  *
+ * Return: %1 if the release was successful, otherwise return zero.
  */
 int try_to_release_page(struct page *page, gfp_t gfp_mask)
 {
diff --git a/mm/memory.c b/mm/memory.c
index 583eb7e0dd7f..24213877c0d6 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1718,6 +1718,8 @@ static int insert_page(struct vm_area_struct *vma, unsigned long addr,
  * under mm->mmap_sem write-lock, so it can change vma->vm_flags.
  * Caller must set VM_MIXEDMAP on vma if it wants to call this
  * function from other places, for example from page-fault handler.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,
 			struct page *page)
@@ -2177,7 +2179,9 @@ static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,
  * @size: size of map area
  * @prot: page protection flags for this mapping
  *
- *  Note: this is only safe if the mm semaphore is held when called.
+ * Note: this is only safe if the mm semaphore is held when called.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
 		    unsigned long pfn, unsigned long size, pgprot_t prot)
@@ -2250,6 +2254,8 @@ EXPORT_SYMBOL(remap_pfn_range);
  *
  * NOTE! Some drivers might want to tweak vma->vm_page_prot first to get
  * whatever write-combining details or similar.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)
 {
@@ -2724,12 +2730,13 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
  *
  * This function handles all that is needed to finish a write page fault in a
  * shared mapping due to PTE being read-only once the mapped page is prepared.
- * It handles locking of PTE and modifying it. The function returns
- * VM_FAULT_WRITE on success, 0 when PTE got changed before we acquired PTE
- * lock.
+ * It handles locking of PTE and modifying it.
  *
  * The function expects the page to be locked or other protection against
  * concurrent faults / writeback (such as DAX radix tree locks).
+ *
+ * Return: %VM_FAULT_WRITE on success, %0 when PTE got changed before
+ * we acquired PTE lock.
  */
 vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf)
 {
@@ -3539,6 +3546,8 @@ static vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)
  *
  * Target users are page handler itself and implementations of
  * vm_ops->map_pages.
+ *
+ * Return: %0 on success, %VM_FAULT_ code in case of error.
  */
 vm_fault_t alloc_set_pte(struct vm_fault *vmf, struct mem_cgroup *memcg,
 		struct page *page)
@@ -3599,11 +3608,12 @@ vm_fault_t alloc_set_pte(struct vm_fault *vmf, struct mem_cgroup *memcg,
  * This function handles all that is needed to finish a page fault once the
  * page to fault in is prepared. It handles locking of PTEs, inserts PTE for
  * given page, adds reverse page mapping, handles memcg charges and LRU
- * addition. The function returns 0 on success, VM_FAULT_ code in case of
- * error.
+ * addition.
  *
  * The function expects the page to be locked and on success it consumes a
  * reference of a page being mapped (for the PTE which maps it).
+ *
+ * Return: %0 on success, %VM_FAULT_ code in case of error.
  */
 vm_fault_t finish_fault(struct vm_fault *vmf)
 {
@@ -4472,7 +4482,7 @@ EXPORT_SYMBOL(follow_pte_pmd);
  *
  * Only IO mappings and raw PFN mappings are allowed.
  *
- * Returns zero and the pfn at @pfn on success, -ve otherwise.
+ * Return: zero and the pfn at @pfn on success, -ve otherwise.
  */
 int follow_pfn(struct vm_area_struct *vma, unsigned long address,
 	unsigned long *pfn)
@@ -4622,6 +4632,8 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
  * @gup_flags:	flags modifying lookup behaviour
  *
  * The caller must hold a reference on @mm.
+ *
+ * Return: number of bytes copied from source to destination.
  */
 int access_remote_vm(struct mm_struct *mm, unsigned long addr,
 		void *buf, int len, unsigned int gup_flags)
diff --git a/mm/mempool.c b/mm/mempool.c
index b54f2c20e5e0..6bfbd4d73ec5 100644
--- a/mm/mempool.c
+++ b/mm/mempool.c
@@ -221,6 +221,8 @@ EXPORT_SYMBOL(mempool_init_node);
  *
  * Like mempool_create(), but initializes the pool in (i.e. embedded in another
  * structure).
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int mempool_init(mempool_t *pool, int min_nr, mempool_alloc_t *alloc_fn,
 		 mempool_free_t *free_fn, void *pool_data)
@@ -244,6 +246,8 @@ EXPORT_SYMBOL(mempool_init);
  * functions. This function might sleep. Both the alloc_fn() and the free_fn()
  * functions might sleep - as long as the mempool_alloc() function is not called
  * from IRQ contexts.
+ *
+ * Return: pointer to the created memory pool object or %NULL on error.
  */
 mempool_t *mempool_create(int min_nr, mempool_alloc_t *alloc_fn,
 				mempool_free_t *free_fn, void *pool_data)
@@ -288,6 +292,8 @@ EXPORT_SYMBOL(mempool_create_node);
  * Note, the caller must guarantee that no mempool_destroy is called
  * while this function is running. mempool_alloc() & mempool_free()
  * might be called (eg. from IRQ contexts) while this function executes.
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 int mempool_resize(mempool_t *pool, int new_min_nr)
 {
@@ -362,6 +368,8 @@ EXPORT_SYMBOL(mempool_resize);
  * *never* fails when called from process contexts. (it might
  * fail if called from an IRQ context.)
  * Note: using __GFP_ZERO is not supported.
+ *
+ * Return: pointer to the allocated element or %NULL on error.
  */
 void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 {
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 77eee73a8fd5..5158ccc677a3 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -271,7 +271,7 @@ static void wb_min_max_ratio(struct bdi_writeback *wb,
  * node_dirtyable_memory - number of dirtyable pages in a node
  * @pgdat: the node
  *
- * Returns the node's number of pages potentially available for dirty
+ * Return: the node's number of pages potentially available for dirty
  * page cache.  This is the base value for the per-node dirty limits.
  */
 static unsigned long node_dirtyable_memory(struct pglist_data *pgdat)
@@ -356,7 +356,7 @@ static unsigned long highmem_dirtyable_memory(unsigned long total)
 /**
  * global_dirtyable_memory - number of globally dirtyable pages
  *
- * Returns the global number of pages potentially available for dirty
+ * Return: the global number of pages potentially available for dirty
  * page cache.  This is the base value for the global dirty limits.
  */
 static unsigned long global_dirtyable_memory(void)
@@ -470,7 +470,7 @@ void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty)
  * node_dirty_limit - maximum number of dirty pages allowed in a node
  * @pgdat: the node
  *
- * Returns the maximum number of dirty pages allowed in a node, based
+ * Return: the maximum number of dirty pages allowed in a node, based
  * on the node's dirtyable memory.
  */
 static unsigned long node_dirty_limit(struct pglist_data *pgdat)
@@ -495,7 +495,7 @@ static unsigned long node_dirty_limit(struct pglist_data *pgdat)
  * node_dirty_ok - tells whether a node is within its dirty limits
  * @pgdat: the node to check
  *
- * Returns %true when the dirty pages in @pgdat are within the node's
+ * Return: %true when the dirty pages in @pgdat are within the node's
  * dirty limit, %false if the limit is exceeded.
  */
 bool node_dirty_ok(struct pglist_data *pgdat)
@@ -743,9 +743,6 @@ static void mdtc_calc_avail(struct dirty_throttle_control *mdtc,
  * __wb_calc_thresh - @wb's share of dirty throttling threshold
  * @dtc: dirty_throttle_context of interest
  *
- * Returns @wb's dirty limit in pages. The term "dirty" in the context of
- * dirty balancing includes all PG_dirty, PG_writeback and NFS unstable pages.
- *
  * Note that balance_dirty_pages() will only seriously take it as a hard limit
  * when sleeping max_pause per page is not enough to keep the dirty pages under
  * control. For example, when the device is completely stalled due to some error
@@ -759,6 +756,9 @@ static void mdtc_calc_avail(struct dirty_throttle_control *mdtc,
  *
  * The wb's share of dirty limit will be adapting to its throughput and
  * bounded by the bdi->min_ratio and/or bdi->max_ratio parameters, if set.
+ *
+ * Return: @wb's dirty limit in pages. The term "dirty" in the context of
+ * dirty balancing includes all PG_dirty, PG_writeback and NFS unstable pages.
  */
 static unsigned long __wb_calc_thresh(struct dirty_throttle_control *dtc)
 {
@@ -1944,7 +1944,9 @@ EXPORT_SYMBOL(balance_dirty_pages_ratelimited);
  * @wb: bdi_writeback of interest
  *
  * Determines whether background writeback should keep writing @wb or it's
- * clean enough.  Returns %true if writeback should continue.
+ * clean enough.
+ *
+ * Return: %true if writeback should continue.
  */
 bool wb_over_bg_thresh(struct bdi_writeback *wb)
 {
@@ -2182,6 +2184,8 @@ EXPORT_SYMBOL(tag_pages_for_writeback);
  * lock/page writeback access order inversion - we should only ever lock
  * multiple pages in ascending page->index order, and looping back to the start
  * of the file violates that rule and causes deadlocks.
+ *
+ * Return: %0 on success, negative error code otherwise
  */
 int write_cache_pages(struct address_space *mapping,
 		      struct writeback_control *wbc, writepage_t writepage,
@@ -2340,6 +2344,8 @@ static int __writepage(struct page *page, struct writeback_control *wbc,
  *
  * This is a library function, which implements the writepages()
  * address_space_operation.
+ *
+ * Return: %0 on success, negative error code otherwise
  */
 int generic_writepages(struct address_space *mapping,
 		       struct writeback_control *wbc)
@@ -2386,6 +2392,8 @@ int do_writepages(struct address_space *mapping, struct writeback_control *wbc)
  *
  * Note that the mapping's AS_EIO/AS_ENOSPC flags will be cleared when this
  * function returns.
+ *
+ * Return: %0 on success, negative error code otherwise
  */
 int write_one_page(struct page *page)
 {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 0f3a8eeb4d26..3d3628bf9a05 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -4888,6 +4888,8 @@ static void *make_alloc_exact(unsigned long addr, unsigned int order,
  * This function is also limited by MAX_ORDER.
  *
  * Memory allocated by this function must be released by free_pages_exact().
+ *
+ * Return: pointer to the allocated area or %NULL in case of error.
  */
 void *alloc_pages_exact(size_t size, gfp_t gfp_mask)
 {
@@ -4908,6 +4910,8 @@ EXPORT_SYMBOL(alloc_pages_exact);
  *
  * Like alloc_pages_exact(), but try to allocate on node nid first before falling
  * back.
+ *
+ * Return: pointer to the allocated area or %NULL in case of error.
  */
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
 {
@@ -4941,11 +4945,13 @@ EXPORT_SYMBOL(free_pages_exact);
  * nr_free_zone_pages - count number of pages beyond high watermark
  * @offset: The zone index of the highest zone
  *
- * nr_free_zone_pages() counts the number of counts pages which are beyond the
+ * nr_free_zone_pages() counts the number of pages which are beyond the
  * high watermark within all zones at or below a given zone index.  For each
  * zone, the number of pages is calculated as:
  *
  *     nr_free_zone_pages = managed_pages - high_pages
+ *
+ * Return: number of pages beyond high watermark.
  */
 static unsigned long nr_free_zone_pages(int offset)
 {
@@ -4972,6 +4978,9 @@ static unsigned long nr_free_zone_pages(int offset)
  *
  * nr_free_buffer_pages() counts the number of pages which are beyond the high
  * watermark within ZONE_DMA and ZONE_NORMAL.
+ *
+ * Return: number of pages beyond high watermark within ZONE_DMA and
+ * ZONE_NORMAL.
  */
 unsigned long nr_free_buffer_pages(void)
 {
@@ -4984,6 +4993,8 @@ EXPORT_SYMBOL_GPL(nr_free_buffer_pages);
  *
  * nr_free_pagecache_pages() counts the number of pages which are beyond the
  * high watermark within all zones.
+ *
+ * Return: number of pages beyond high watermark within all zones.
  */
 unsigned long nr_free_pagecache_pages(void)
 {
@@ -5430,7 +5441,8 @@ static int node_load[MAX_NUMNODES];
  * from each node to each node in the system), and should also prefer nodes
  * with no CPUs, since presumably they'll have very little allocation pressure
  * on them otherwise.
- * It returns -1 if no node is found.
+ *
+ * Return: node id of the found node or %NUMA_NO_NODE if no node is found.
  */
 static int find_next_best_node(int node, nodemask_t *used_node_mask)
 {
@@ -6364,7 +6376,7 @@ unsigned long __init __absent_pages_in_range(int nid,
  * @start_pfn: The start PFN to start searching for holes
  * @end_pfn: The end PFN to stop searching for holes
  *
- * It returns the number of pages frames in memory holes within a range.
+ * Return: the number of pages frames in memory holes within a range.
  */
 unsigned long __init absent_pages_in_range(unsigned long start_pfn,
 							unsigned long end_pfn)
@@ -6886,7 +6898,7 @@ void __init setup_nr_node_ids(void)
  * model has fine enough granularity to avoid incorrect mapping for the
  * populated node map.
  *
- * Returns the determined alignment in pfn's.  0 if there is no alignment
+ * Return: the determined alignment in pfn's.  0 if there is no alignment
  * requirement (single node).
  */
 unsigned long __init node_map_pfn_alignment(void)
@@ -6923,7 +6935,7 @@ unsigned long __init node_map_pfn_alignment(void)
 /**
  * find_min_pfn_with_active_regions - Find the minimum PFN registered
  *
- * It returns the minimum PFN based on information provided via
+ * Return: the minimum PFN based on information provided via
  * memblock_set_node().
  */
 unsigned long __init find_min_pfn_with_active_regions(void)
@@ -8214,7 +8226,7 @@ static int __alloc_contig_migrate_range(struct compact_control *cc,
  * pageblocks in the range.  Once isolated, the pageblocks should not
  * be modified by others.
  *
- * Returns zero on success or negative error code.  On success all
+ * Return: zero on success or negative error code.  On success all
  * pages which PFN is in [start, end) are allocated for the caller and
  * need to be freed with free_contig_range().
  */
diff --git a/mm/readahead.c b/mm/readahead.c
index f3d6f9656a3c..6bdd54509ff7 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -81,6 +81,8 @@ static void read_cache_pages_invalidate_pages(struct address_space *mapping,
  * @data: private data for the callback routine.
  *
  * Hides the details of the LRU cache etc from the filesystems.
+ *
+ * Returns: %0 on success, error return by @filler otherwise
  */
 int read_cache_pages(struct address_space *mapping, struct list_head *pages,
 			int (*filler)(void *, struct page *), void *data)
diff --git a/mm/slab.c b/mm/slab.c
index 31552303d765..ec1fdbfa07e7 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -1713,6 +1713,8 @@ static void slabs_destroy(struct kmem_cache *cachep, struct list_head *list)
  * This could be made much more intelligent.  For now, try to avoid using
  * high order pages for slabs.  When the gfp() functions are more friendly
  * towards high-order requests, this should be changed.
+ *
+ * Return: number of left-over bytes in a slab
  */
 static size_t calculate_slab_order(struct kmem_cache *cachep,
 				size_t size, slab_flags_t flags)
@@ -1961,6 +1963,8 @@ static bool set_on_slab_cache(struct kmem_cache *cachep,
  * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
  * cacheline.  This can be beneficial if you're counting cycles as closely
  * as davem.
+ *
+ * Return: a pointer to the created cache or %NULL in case of error
  */
 int __kmem_cache_create(struct kmem_cache *cachep, slab_flags_t flags)
 {
@@ -3534,6 +3538,8 @@ void ___cache_free(struct kmem_cache *cachep, void *objp,
  *
  * Allocate an object from this cache.  The flags are only relevant
  * if the cache has no available objects.
+ *
+ * Return: pointer to the new object or %NULL in case of error
  */
 void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
 {
@@ -3623,6 +3629,8 @@ EXPORT_SYMBOL(kmem_cache_alloc_trace);
  * node, which can improve the performance for cpu bound structures.
  *
  * Fallback to other node is possible if __GFP_THISNODE is not set.
+ *
+ * Return: pointer to the new object or %NULL in case of error
  */
 void *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)
 {
@@ -3691,6 +3699,8 @@ EXPORT_SYMBOL(__kmalloc_node_track_caller);
  * @size: how many bytes of memory are required.
  * @flags: the type of memory to allocate (see kmalloc).
  * @caller: function caller for debug tracking of the caller
+ *
+ * Return: pointer to the allocated memory or %NULL in case of error
  */
 static __always_inline void *__do_kmalloc(size_t size, gfp_t flags,
 					  unsigned long caller)
@@ -4156,6 +4166,8 @@ void slabinfo_show_stats(struct seq_file *m, struct kmem_cache *cachep)
  * @buffer: user buffer
  * @count: data length
  * @ppos: unused
+ *
+ * Return: %0 on success, negative error code otherwise.
  */
 ssize_t slabinfo_write(struct file *file, const char __user *buffer,
 		       size_t count, loff_t *ppos)
@@ -4454,6 +4466,8 @@ void __check_heap_object(const void *ptr, unsigned long n, struct page *page,
  * The caller must guarantee that objp points to a valid object previously
  * allocated with either kmalloc() or kmem_cache_alloc(). The object
  * must not be freed during the duration of the call.
+ *
+ * Return: size of the actual memory used by @objp in bytes
  */
 size_t ksize(const void *objp)
 {
diff --git a/mm/slab_common.c b/mm/slab_common.c
index d38e88435224..9be473d1998c 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -977,6 +977,8 @@ EXPORT_SYMBOL(kmem_cache_destroy);
  *
  * Releases as many slabs as possible for a cache.
  * To help debugging, a zero exit status indicates all slabs were released.
+ *
+ * Return: %0 if all slabs were released, non-zero otherwise
  */
 int kmem_cache_shrink(struct kmem_cache *cachep)
 {
@@ -1633,6 +1635,8 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
  * This function is like krealloc() except it never frees the originally
  * allocated buffer. Use this if you don't want to free the buffer immediately
  * like, for example, with RCU.
+ *
+ * Return: pointer to the allocated memory or %NULL in case of error
  */
 void *__krealloc(const void *p, size_t new_size, gfp_t flags)
 {
@@ -1654,6 +1658,8 @@ EXPORT_SYMBOL(__krealloc);
  * lesser of the new and old sizes.  If @p is %NULL, krealloc()
  * behaves exactly like kmalloc().  If @new_size is 0 and @p is not a
  * %NULL pointer, the object pointed to is freed.
+ *
+ * Return: pointer to the allocated memory or %NULL in case of error
  */
 void *krealloc(const void *p, size_t new_size, gfp_t flags)
 {
diff --git a/mm/truncate.c b/mm/truncate.c
index f45fbcf6fe3c..84a2f4a3029c 100644
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@ -542,6 +542,8 @@ EXPORT_SYMBOL(truncate_inode_pages_final);
  * invalidate_mapping_pages() will not block on IO activity. It will not
  * invalidate pages which are dirty, locked, under writeback or mapped into
  * pagetables.
+ *
+ * Return: the number of the pages that were invalidated
  */
 unsigned long invalidate_mapping_pages(struct address_space *mapping,
 		pgoff_t start, pgoff_t end)
@@ -667,7 +669,7 @@ static int do_launder_page(struct address_space *mapping, struct page *page)
  * Any pages which are found to be mapped into pagetables are unmapped prior to
  * invalidation.
  *
- * Returns -EBUSY if any pages could not be invalidated.
+ * Return: -EBUSY if any pages could not be invalidated.
  */
 int invalidate_inode_pages2_range(struct address_space *mapping,
 				  pgoff_t start, pgoff_t end)
@@ -764,7 +766,7 @@ EXPORT_SYMBOL_GPL(invalidate_inode_pages2_range);
  * Any pages which are found to be mapped into pagetables are unmapped prior to
  * invalidation.
  *
- * Returns -EBUSY if any pages could not be invalidated.
+ * Return: -EBUSY if any pages could not be invalidated.
  */
 int invalidate_inode_pages2(struct address_space *mapping)
 {
diff --git a/mm/util.c b/mm/util.c
index e99de9d3c8ae..4e48aebdd86a 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -37,6 +37,8 @@ EXPORT_SYMBOL(kfree_const);
  * kstrdup - allocate space for and copy an existing string
  * @s: the string to duplicate
  * @gfp: the GFP mask used in the kmalloc() call when allocating memory
+ *
+ * Return: newly allocated copy of @s or %NULL in case of error
  */
 char *kstrdup(const char *s, gfp_t gfp)
 {
@@ -59,9 +61,10 @@ EXPORT_SYMBOL(kstrdup);
  * @s: the string to duplicate
  * @gfp: the GFP mask used in the kmalloc() call when allocating memory
  *
- * Function returns source string if it is in .rodata section otherwise it
- * fallbacks to kstrdup.
- * Strings allocated by kstrdup_const should be freed by kfree_const.
+ * Note: Strings allocated by kstrdup_const should be freed by kfree_const.
+ *
+ * Return: source string if it is in .rodata section otherwise
+ * fallback to kstrdup.
  */
 const char *kstrdup_const(const char *s, gfp_t gfp)
 {
@@ -79,6 +82,8 @@ EXPORT_SYMBOL(kstrdup_const);
  * @gfp: the GFP mask used in the kmalloc() call when allocating memory
  *
  * Note: Use kmemdup_nul() instead if the size is known exactly.
+ *
+ * Return: newly allocated copy of @s or %NULL in case of error
  */
 char *kstrndup(const char *s, size_t max, gfp_t gfp)
 {
@@ -104,6 +109,8 @@ EXPORT_SYMBOL(kstrndup);
  * @src: memory region to duplicate
  * @len: memory region length
  * @gfp: GFP mask to use
+ *
+ * Return: newly allocated copy of @src or %NULL in case of error
  */
 void *kmemdup(const void *src, size_t len, gfp_t gfp)
 {
@@ -121,6 +128,9 @@ EXPORT_SYMBOL(kmemdup);
  * @s: The data to stringify
  * @len: The size of the data
  * @gfp: the GFP mask used in the kmalloc() call when allocating memory
+ *
+ * Return: newly allocated copy of @s with NUL-termination or %NULL in
+ * case of error
  */
 char *kmemdup_nul(const char *s, size_t len, gfp_t gfp)
 {
@@ -144,7 +154,7 @@ EXPORT_SYMBOL(kmemdup_nul);
  * @src: source address in user space
  * @len: number of bytes to copy
  *
- * Returns an ERR_PTR() on failure.  Result is physically
+ * Return: an ERR_PTR() on failure.  Result is physically
  * contiguous, to be freed by kfree().
  */
 void *memdup_user(const void __user *src, size_t len)
@@ -170,7 +180,7 @@ EXPORT_SYMBOL(memdup_user);
  * @src: source address in user space
  * @len: number of bytes to copy
  *
- * Returns an ERR_PTR() on failure.  Result may be not
+ * Return: an ERR_PTR() on failure.  Result may be not
  * physically contiguous.  Use kvfree() to free.
  */
 void *vmemdup_user(const void __user *src, size_t len)
@@ -194,6 +204,8 @@ EXPORT_SYMBOL(vmemdup_user);
  * strndup_user - duplicate an existing string from user space
  * @s: The string to duplicate
  * @n: Maximum number of bytes to copy, including the trailing NUL.
+ *
+ * Return: newly allocated copy of @s or %NULL in case of error
  */
 char *strndup_user(const char __user *s, long n)
 {
@@ -225,7 +237,7 @@ EXPORT_SYMBOL(strndup_user);
  * @src: source address in user space
  * @len: number of bytes to copy
  *
- * Returns an ERR_PTR() on failure.
+ * Return: an ERR_PTR() on failure.
  */
 void *memdup_user_nul(const void __user *src, size_t len)
 {
@@ -311,10 +323,6 @@ EXPORT_SYMBOL_GPL(__get_user_pages_fast);
  * @pages:	array that receives pointers to the pages pinned.
  *		Should be at least nr_pages long.
  *
- * Returns number of pages pinned. This may be fewer than the number
- * requested. If nr_pages is 0 or negative, returns 0. If no pages
- * were pinned, returns -errno.
- *
  * get_user_pages_fast provides equivalent functionality to get_user_pages,
  * operating on current and current->mm, with force=0 and vma=NULL. However
  * unlike get_user_pages, it must be called without mmap_sem held.
@@ -326,6 +334,10 @@ EXPORT_SYMBOL_GPL(__get_user_pages_fast);
  * pages have to be faulted in, it may turn out to be slightly slower so
  * callers need to carefully consider what to use. On many architectures,
  * get_user_pages_fast simply falls back to get_user_pages.
+ *
+ * Return: number of pages pinned. This may be fewer than the number
+ * requested. If nr_pages is 0 or negative, returns 0. If no pages
+ * were pinned, returns -errno.
  */
 int __weak get_user_pages_fast(unsigned long start,
 				int nr_pages, unsigned int gup_flags,
@@ -461,6 +473,8 @@ EXPORT_SYMBOL(vm_mmap);
  *
  * Please note that any use of gfp flags outside of GFP_KERNEL is careful to not
  * fall back to vmalloc.
+ *
+ * Return: pointer to the allocated memory of %NULL in case of failure
  */
 void *kvmalloc_node(size_t size, gfp_t flags, int node)
 {
@@ -753,7 +767,8 @@ int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin)
  * @buffer:   the buffer to copy to.
  * @buflen:   the length of the buffer. Larger cmdline values are truncated
  *            to this length.
- * Returns the size of the cmdline field copied. Note that the copy does
+ *
+ * Return: the size of the cmdline field copied. Note that the copy does
  * not guarantee an ending NULL byte.
  */
 int get_cmdline(struct task_struct *task, char *buffer, int buflen)
* Unmerged path mm/vmalloc.c
