RDMA/core: Clean up cq pool mechanism

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-294.el8
commit-author Jack Morgenstein <jackm@dev.mellanox.co.il>
commit 286e1d3f9ba89c7db5eecd30f47f9e333843ea13
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-294.el8/286e1d3f.failed

The CQ pool mechanism had two problems:

1. The CQ pool lists were uninitialized in the device registration error
   flow.  As a result, all the list pointers remained NULL.  This caused
   the kernel to crash (in procedure ib_cq_pool_destroy) when that error
   flow was taken (and unregister called).  The stack trace snippet:

     BUG: kernel NULL pointer dereference, address: 0000000000000000
     #PF: supervisor read access in kernel mode
     #PF: error_code(0×0000) ? not-present page
     PGD 0 P4D 0
     Oops: 0000 [#1] SMP PTI
     . . .
     RIP: 0010:ib_cq_pool_destroy+0x1b/0×70 [ib_core]
     . . .
     Call Trace:
      disable_device+0x9f/0×130 [ib_core]
      __ib_unregister_device+0x35/0×90 [ib_core]
      ib_register_device+0x529/0×610 [ib_core]
      __mlx5_ib_add+0x3a/0×70 [mlx5_ib]
      mlx5_add_device+0x87/0×1c0 [mlx5_core]
      mlx5_register_interface+0x74/0xc0 [mlx5_core]
      do_one_initcall+0x4b/0×1f4
      do_init_module+0x5a/0×223
      load_module+0x1938/0×1d40

2. At device unregister, when cleaning up the cq pool, the cq's in the
   pool lists were freed, but the cq entries were left in the list.

The fix for the first issue is to initialize the cq pool lists when the
ib_device structure is allocated for a new device (in procedure
_ib_alloc_device).

The fix for the second problem is to delete cq entries from the pool lists
when cleaning up the cq pool.

In addition, procedure ib_cq_pool_destroy() is renamed to the more
appropriate name ib_cq_pool_cleanup().

Fixes: 4aa1615268a8 ("RDMA/core: Fix ordering of CQ pool destruction")
Link: https://lore.kernel.org/r/20201208073545.9723-2-leon@kernel.org
	Suggested-by: Jason Gunthorpe <jgg@nvidia.com>
	Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 286e1d3f9ba89c7db5eecd30f47f9e333843ea13)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/core_priv.h
#	drivers/infiniband/core/cq.c
#	drivers/infiniband/core/device.c
diff --cc drivers/infiniband/core/core_priv.h
index cf42acca4a3a,315f7a297eee..000000000000
--- a/drivers/infiniband/core/core_priv.h
+++ b/drivers/infiniband/core/core_priv.h
@@@ -414,4 -402,6 +414,9 @@@ void rdma_umap_priv_init(struct rdma_um
  			 struct vm_area_struct *vma,
  			 struct rdma_user_mmap_entry *entry);
  
++<<<<<<< HEAD
++=======
+ void ib_cq_pool_cleanup(struct ib_device *dev);
+ 
++>>>>>>> 286e1d3f9ba8 (RDMA/core: Clean up cq pool mechanism)
  #endif /* _CORE_PRIV_H */
diff --cc drivers/infiniband/core/cq.c
index 6bb62d04030a,433b426729d4..000000000000
--- a/drivers/infiniband/core/cq.c
+++ b/drivers/infiniband/core/cq.c
@@@ -341,4 -347,162 +341,166 @@@ void ib_free_cq_user(struct ib_cq *cq, 
  	kfree(cq->wc);
  	kfree(cq);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL(ib_free_cq_user);
++=======
+ EXPORT_SYMBOL(ib_free_cq);
+ 
+ void ib_cq_pool_cleanup(struct ib_device *dev)
+ {
+ 	struct ib_cq *cq, *n;
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < ARRAY_SIZE(dev->cq_pools); i++) {
+ 		list_for_each_entry_safe(cq, n, &dev->cq_pools[i],
+ 					 pool_entry) {
+ 			WARN_ON(cq->cqe_used);
+ 			list_del(&cq->pool_entry);
+ 			cq->shared = false;
+ 			ib_free_cq(cq);
+ 		}
+ 	}
+ }
+ 
+ static int ib_alloc_cqs(struct ib_device *dev, unsigned int nr_cqes,
+ 			enum ib_poll_context poll_ctx)
+ {
+ 	LIST_HEAD(tmp_list);
+ 	unsigned int nr_cqs, i;
+ 	struct ib_cq *cq, *n;
+ 	int ret;
+ 
+ 	if (poll_ctx > IB_POLL_LAST_POOL_TYPE) {
+ 		WARN_ON_ONCE(poll_ctx > IB_POLL_LAST_POOL_TYPE);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/*
+ 	 * Allocate at least as many CQEs as requested, and otherwise
+ 	 * a reasonable batch size so that we can share CQs between
+ 	 * multiple users instead of allocating a larger number of CQs.
+ 	 */
+ 	nr_cqes = min_t(unsigned int, dev->attrs.max_cqe,
+ 			max(nr_cqes, IB_MAX_SHARED_CQ_SZ));
+ 	nr_cqs = min_t(unsigned int, dev->num_comp_vectors, num_online_cpus());
+ 	for (i = 0; i < nr_cqs; i++) {
+ 		cq = ib_alloc_cq(dev, NULL, nr_cqes, i, poll_ctx);
+ 		if (IS_ERR(cq)) {
+ 			ret = PTR_ERR(cq);
+ 			goto out_free_cqs;
+ 		}
+ 		cq->shared = true;
+ 		list_add_tail(&cq->pool_entry, &tmp_list);
+ 	}
+ 
+ 	spin_lock_irq(&dev->cq_pools_lock);
+ 	list_splice(&tmp_list, &dev->cq_pools[poll_ctx]);
+ 	spin_unlock_irq(&dev->cq_pools_lock);
+ 
+ 	return 0;
+ 
+ out_free_cqs:
+ 	list_for_each_entry_safe(cq, n, &tmp_list, pool_entry) {
+ 		cq->shared = false;
+ 		ib_free_cq(cq);
+ 	}
+ 	return ret;
+ }
+ 
+ /**
+  * ib_cq_pool_get() - Find the least used completion queue that matches
+  *   a given cpu hint (or least used for wild card affinity) and fits
+  *   nr_cqe.
+  * @dev: rdma device
+  * @nr_cqe: number of needed cqe entries
+  * @comp_vector_hint: completion vector hint (-1) for the driver to assign
+  *   a comp vector based on internal counter
+  * @poll_ctx: cq polling context
+  *
+  * Finds a cq that satisfies @comp_vector_hint and @nr_cqe requirements and
+  * claim entries in it for us.  In case there is no available cq, allocate
+  * a new cq with the requirements and add it to the device pool.
+  * IB_POLL_DIRECT cannot be used for shared cqs so it is not a valid value
+  * for @poll_ctx.
+  */
+ struct ib_cq *ib_cq_pool_get(struct ib_device *dev, unsigned int nr_cqe,
+ 			     int comp_vector_hint,
+ 			     enum ib_poll_context poll_ctx)
+ {
+ 	static unsigned int default_comp_vector;
+ 	unsigned int vector, num_comp_vectors;
+ 	struct ib_cq *cq, *found = NULL;
+ 	int ret;
+ 
+ 	if (poll_ctx > IB_POLL_LAST_POOL_TYPE) {
+ 		WARN_ON_ONCE(poll_ctx > IB_POLL_LAST_POOL_TYPE);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	num_comp_vectors =
+ 		min_t(unsigned int, dev->num_comp_vectors, num_online_cpus());
+ 	/* Project the affinty to the device completion vector range */
+ 	if (comp_vector_hint < 0) {
+ 		comp_vector_hint =
+ 			(READ_ONCE(default_comp_vector) + 1) % num_comp_vectors;
+ 		WRITE_ONCE(default_comp_vector, comp_vector_hint);
+ 	}
+ 	vector = comp_vector_hint % num_comp_vectors;
+ 
+ 	/*
+ 	 * Find the least used CQ with correct affinity and
+ 	 * enough free CQ entries
+ 	 */
+ 	while (!found) {
+ 		spin_lock_irq(&dev->cq_pools_lock);
+ 		list_for_each_entry(cq, &dev->cq_pools[poll_ctx],
+ 				    pool_entry) {
+ 			/*
+ 			 * Check to see if we have found a CQ with the
+ 			 * correct completion vector
+ 			 */
+ 			if (vector != cq->comp_vector)
+ 				continue;
+ 			if (cq->cqe_used + nr_cqe > cq->cqe)
+ 				continue;
+ 			found = cq;
+ 			break;
+ 		}
+ 
+ 		if (found) {
+ 			found->cqe_used += nr_cqe;
+ 			spin_unlock_irq(&dev->cq_pools_lock);
+ 
+ 			return found;
+ 		}
+ 		spin_unlock_irq(&dev->cq_pools_lock);
+ 
+ 		/*
+ 		 * Didn't find a match or ran out of CQs in the device
+ 		 * pool, allocate a new array of CQs.
+ 		 */
+ 		ret = ib_alloc_cqs(dev, nr_cqe, poll_ctx);
+ 		if (ret)
+ 			return ERR_PTR(ret);
+ 	}
+ 
+ 	return found;
+ }
+ EXPORT_SYMBOL(ib_cq_pool_get);
+ 
+ /**
+  * ib_cq_pool_put - Return a CQ taken from a shared pool.
+  * @cq: The CQ to return.
+  * @nr_cqe: The max number of cqes that the user had requested.
+  */
+ void ib_cq_pool_put(struct ib_cq *cq, unsigned int nr_cqe)
+ {
+ 	if (WARN_ON_ONCE(nr_cqe > cq->cqe_used))
+ 		return;
+ 
+ 	spin_lock_irq(&cq->device->cq_pools_lock);
+ 	cq->cqe_used -= nr_cqe;
+ 	spin_unlock_irq(&cq->device->cq_pools_lock);
+ }
+ EXPORT_SYMBOL(ib_cq_pool_put);
++>>>>>>> 286e1d3f9ba8 (RDMA/core: Clean up cq pool mechanism)
diff --cc drivers/infiniband/core/device.c
index 1a03542a1ad8,11485b8748a2..000000000000
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@@ -600,6 -602,41 +601,44 @@@ struct ib_device *_ib_alloc_device(size
  	init_completion(&device->unreg_completion);
  	INIT_WORK(&device->unregistration_work, ib_unregister_work);
  
++<<<<<<< HEAD
++=======
+ 	spin_lock_init(&device->cq_pools_lock);
+ 	for (i = 0; i < ARRAY_SIZE(device->cq_pools); i++)
+ 		INIT_LIST_HEAD(&device->cq_pools[i]);
+ 
+ 	device->uverbs_cmd_mask =
+ 		BIT_ULL(IB_USER_VERBS_CMD_ALLOC_MW) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_ALLOC_PD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_ATTACH_MCAST) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CLOSE_XRCD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_AH) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_CQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_CREATE_XSRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DEALLOC_MW) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DEALLOC_PD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DEREG_MR) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DESTROY_AH) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DESTROY_CQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DESTROY_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DESTROY_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_DETACH_MCAST) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_GET_CONTEXT) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_MODIFY_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_MODIFY_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_OPEN_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_OPEN_XRCD) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_DEVICE) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_PORT) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_QP) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_QUERY_SRQ) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_REG_MR) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_REREG_MR) |
+ 		BIT_ULL(IB_USER_VERBS_CMD_RESIZE_CQ);
++>>>>>>> 286e1d3f9ba8 (RDMA/core: Clean up cq pool mechanism)
  	return device;
  }
  EXPORT_SYMBOL(_ib_alloc_device);
* Unmerged path drivers/infiniband/core/core_priv.h
* Unmerged path drivers/infiniband/core/cq.c
* Unmerged path drivers/infiniband/core/device.c
