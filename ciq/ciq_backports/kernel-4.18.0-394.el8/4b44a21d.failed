irq_work, smp: Allow irq_work on call_single_queue

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 4b44a21dd640b692d4e9b12d3e37c24825f90baa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/4b44a21d.failed

Currently irq_work_queue_on() will issue an unconditional
arch_send_call_function_single_ipi() and has the handler do
irq_work_run().

This is unfortunate in that it makes the IPI handler look at a second
cacheline and it misses the opportunity to avoid the IPI. Instead note
that struct irq_work and struct __call_single_data are very similar in
layout, so use a few bits in the flags word to encode a type and stick
the irq_work on the call_single_queue list.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20200526161908.011635912@infradead.org
(cherry picked from commit 4b44a21dd640b692d4e9b12d3e37c24825f90baa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/irq_work.h
#	kernel/smp.c
diff --cc include/linux/irq_work.h
index 52859a85ce23,f23a359c8f46..000000000000
--- a/include/linux/irq_work.h
+++ b/include/linux/irq_work.h
@@@ -24,9 -25,12 +26,16 @@@
  
  #define IRQ_WORK_CLAIMED	(IRQ_WORK_PENDING | IRQ_WORK_BUSY)
  
+ /*
+  * structure shares layout with single_call_data_t.
+  */
  struct irq_work {
++<<<<<<< HEAD
 +	RH_KABI_REPLACE(unsigned long flags, atomic_t flags)
++=======
++>>>>>>> 4b44a21dd640 (irq_work, smp: Allow irq_work on call_single_queue)
  	struct llist_node llnode;
+ 	atomic_t flags;
  	void (*func)(struct irq_work *);
  };
  
diff --cc kernel/smp.c
index db3e44b3aed3,856562b80794..000000000000
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@@ -134,6 -133,25 +132,28 @@@ static __always_inline void csd_unlock(
  
  static DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);
  
++<<<<<<< HEAD
++=======
+ extern void send_call_function_single_ipi(int cpu);
+ 
+ void __smp_call_single_queue(int cpu, struct llist_node *node)
+ {
+ 	/*
+ 	 * The list addition should be visible before sending the IPI
+ 	 * handler locks the list to pull the entry off it because of
+ 	 * normal cache coherency rules implied by spinlocks.
+ 	 *
+ 	 * If IPIs can go out of order to the cache coherency protocol
+ 	 * in an architecture, sufficient synchronisation should be added
+ 	 * to arch code to make it appear to obey cache coherency WRT
+ 	 * locking and barrier primitives. Generic code isn't really
+ 	 * equipped to do the right thing...
+ 	 */
+ 	if (llist_add(node, &per_cpu(call_single_queue, cpu)))
+ 		send_call_function_single_ipi(cpu);
+ }
+ 
++>>>>>>> 4b44a21dd640 (irq_work, smp: Allow irq_work on call_single_queue)
  /*
   * Insert a previously allocated call_single_data_t element
   * for execution on the given CPU. data must already have
@@@ -162,22 -180,7 +182,26 @@@ static int generic_exec_single(int cpu
  		return -ENXIO;
  	}
  
++<<<<<<< HEAD
 +	csd->func = func;
 +	csd->info = info;
 +
 +	/*
 +	 * The list addition should be visible before sending the IPI
 +	 * handler locks the list to pull the entry off it because of
 +	 * normal cache coherency rules implied by spinlocks.
 +	 *
 +	 * If IPIs can go out of order to the cache coherency protocol
 +	 * in an architecture, sufficient synchronisation should be added
 +	 * to arch code to make it appear to obey cache coherency WRT
 +	 * locking and barrier primitives. Generic code isn't really
 +	 * equipped to do the right thing...
 +	 */
 +	if (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))
 +		arch_send_call_function_single_ipi(cpu);
++=======
+ 	__smp_call_single_queue(cpu, &csd->llist);
++>>>>>>> 4b44a21dd640 (irq_work, smp: Allow irq_work on call_single_queue)
  
  	return 0;
  }
@@@ -602,8 -647,18 +642,19 @@@ void __init setup_nr_cpu_ids(void
  void __init smp_init(void)
  {
  	int num_nodes, num_cpus;
 +	unsigned int cpu;
  
+ 	/*
+ 	 * Ensure struct irq_work layout matches so that
+ 	 * flush_smp_call_function_queue() can do horrible things.
+ 	 */
+ 	BUILD_BUG_ON(offsetof(struct irq_work, llnode) !=
+ 		     offsetof(struct __call_single_data, llist));
+ 	BUILD_BUG_ON(offsetof(struct irq_work, func) !=
+ 		     offsetof(struct __call_single_data, func));
+ 	BUILD_BUG_ON(offsetof(struct irq_work, flags) !=
+ 		     offsetof(struct __call_single_data, flags));
+ 
  	idle_threads_init();
  	cpuhp_threads_init();
  
* Unmerged path include/linux/irq_work.h
diff --git a/include/linux/smp.h b/include/linux/smp.h
index f30fcf12cc27..dd7163fa97a2 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -16,17 +16,38 @@
 
 typedef void (*smp_call_func_t)(void *info);
 typedef bool (*smp_cond_func_t)(int cpu, void *info);
+
+enum {
+	CSD_FLAG_LOCK		= 0x01,
+
+	/* IRQ_WORK_flags */
+
+	CSD_TYPE_ASYNC		= 0x00,
+	CSD_TYPE_SYNC		= 0x10,
+	CSD_TYPE_IRQ_WORK	= 0x20,
+	CSD_FLAG_TYPE_MASK	= 0xF0,
+};
+
+/*
+ * structure shares (partial) layout with struct irq_work
+ */
 struct __call_single_data {
 	struct llist_node llist;
+	unsigned int flags;
 	smp_call_func_t func;
 	void *info;
-	unsigned int flags;
 };
 
 /* Use __aligned() to avoid to use 2 cache lines for 1 csd */
 typedef struct __call_single_data call_single_data_t
 	__aligned(sizeof(struct __call_single_data));
 
+/*
+ * Enqueue a llist_node on the call_single_queue; be very careful, read
+ * flush_smp_call_function_queue() in detail.
+ */
+extern void __smp_call_single_queue(int cpu, struct llist_node *node);
+
 /* total number of cpus in this system (may exceed NR_CPUS) */
 extern unsigned int total_cpus;
 
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index fc8d175a5d09..c61e5a4e3feb 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -30,7 +30,7 @@ static bool irq_work_claim(struct irq_work *work)
 {
 	int oflags;
 
-	oflags = atomic_fetch_or(IRQ_WORK_CLAIMED, &work->flags);
+	oflags = atomic_fetch_or(IRQ_WORK_CLAIMED | CSD_TYPE_IRQ_WORK, &work->flags);
 	/*
 	 * If the work is already pending, no need to raise the IPI.
 	 * The pairing atomic_fetch_andnot() in irq_work_run() makes sure
@@ -101,8 +101,7 @@ bool irq_work_queue_on(struct irq_work *work, int cpu)
 	if (cpu != smp_processor_id()) {
 		/* Arch remote IPI send/receive backend aren't NMI safe */
 		WARN_ON_ONCE(in_nmi());
-		if (llist_add(&work->llnode, &per_cpu(raised_list, cpu)))
-			arch_send_call_function_single_ipi(cpu);
+		__smp_call_single_queue(cpu, &work->llnode);
 	} else {
 		__irq_work_queue_local(work);
 	}
@@ -130,6 +129,31 @@ bool irq_work_needs_cpu(void)
 	return true;
 }
 
+void irq_work_single(void *arg)
+{
+	struct irq_work *work = arg;
+	int flags;
+
+	/*
+	 * Clear the PENDING bit, after this point the @work
+	 * can be re-used.
+	 * Make it immediately visible so that other CPUs trying
+	 * to claim that work don't rely on us to handle their data
+	 * while we are in the middle of the func.
+	 */
+	flags = atomic_fetch_andnot(IRQ_WORK_PENDING, &work->flags);
+
+	lockdep_irq_work_enter(work);
+	work->func(work);
+	lockdep_irq_work_exit(work);
+	/*
+	 * Clear the BUSY bit and return to the free state if
+	 * no-one else claimed it meanwhile.
+	 */
+	flags &= ~IRQ_WORK_PENDING;
+	(void)atomic_cmpxchg(&work->flags, flags, flags & ~IRQ_WORK_BUSY);
+}
+
 static void irq_work_run_list(struct llist_head *list)
 {
 	struct irq_work *work, *tmp;
@@ -141,27 +165,8 @@ static void irq_work_run_list(struct llist_head *list)
 		return;
 
 	llnode = llist_del_all(list);
-	llist_for_each_entry_safe(work, tmp, llnode, llnode) {
-		int flags;
-		/*
-		 * Clear the PENDING bit, after this point the @work
-		 * can be re-used.
-		 * Make it immediately visible so that other CPUs trying
-		 * to claim that work don't rely on us to handle their data
-		 * while we are in the middle of the func.
-		 */
-		flags = atomic_fetch_andnot(IRQ_WORK_PENDING, &work->flags);
-
-		lockdep_irq_work_enter(work);
-		work->func(work);
-		lockdep_irq_work_exit(work);
-		/*
-		 * Clear the BUSY bit and return to the free state if
-		 * no-one else claimed it meanwhile.
-		 */
-		flags &= ~IRQ_WORK_PENDING;
-		(void)atomic_cmpxchg(&work->flags, flags, flags & ~IRQ_WORK_BUSY);
-	}
+	llist_for_each_entry_safe(work, tmp, llnode, llnode)
+		irq_work_single(work);
 }
 
 /*
* Unmerged path kernel/smp.c
