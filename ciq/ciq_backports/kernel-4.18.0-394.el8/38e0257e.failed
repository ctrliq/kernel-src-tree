arm64: errata: Fix exec handling in erratum 1418040 workaround

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author D Scott Phillips <scott@os.amperecomputing.com>
commit 38e0257e0e6f4fef2aa2966b089b56a8b1cfb75c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/38e0257e.failed

The erratum 1418040 workaround enables CNTVCT_EL1 access trapping in EL0
when executing compat threads. The workaround is applied when switching
between tasks, but the need for the workaround could also change at an
exec(), when a non-compat task execs a compat binary or vice versa. Apply
the workaround in arch_setup_new_exec().

This leaves a small window of time between SET_PERSONALITY and
arch_setup_new_exec where preemption could occur and confuse the old
workaround logic that compares TIF_32BIT between prev and next. Instead, we
can just read cntkctl to make sure it's in the state that the next task
needs. I measured cntkctl read time to be about the same as a mov from a
general-purpose register on N1. Update the workaround logic to examine the
current value of cntkctl instead of the previous task's compat state.

Fixes: d49f7d7376d0 ("arm64: Move handling of erratum 1418040 into C code")
	Cc: <stable@vger.kernel.org> # 5.9.x
	Signed-off-by: D Scott Phillips <scott@os.amperecomputing.com>
	Reviewed-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20211220234114.3926-1-scott@os.amperecomputing.com
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit 38e0257e0e6f4fef2aa2966b089b56a8b1cfb75c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/process.c
diff --cc arch/arm64/kernel/process.c
index a22d98d7c74c,271d4bbf468e..000000000000
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@@ -513,36 -439,46 +513,28 @@@ static void entry_task_switch(struct ta
  
  /*
   * ARM erratum 1418040 handling, affecting the 32bit view of CNTVCT.
-  * Assuming the virtual counter is enabled at the beginning of times:
-  *
-  * - disable access when switching from a 64bit task to a 32bit task
-  * - enable access when switching from a 32bit task to a 64bit task
+  * Ensure access is disabled when switching to a 32bit task, ensure
+  * access is enabled when switching to a 64bit task.
   */
- static void erratum_1418040_thread_switch(struct task_struct *prev,
- 					  struct task_struct *next)
+ static void erratum_1418040_thread_switch(struct task_struct *next)
  {
- 	bool prev32, next32;
- 	u64 val;
- 
- 	if (!IS_ENABLED(CONFIG_ARM64_ERRATUM_1418040))
- 		return;
- 
- 	prev32 = is_compat_thread(task_thread_info(prev));
- 	next32 = is_compat_thread(task_thread_info(next));
- 
- 	if (prev32 == next32 || !this_cpu_has_cap(ARM64_WORKAROUND_1418040))
+ 	if (!IS_ENABLED(CONFIG_ARM64_ERRATUM_1418040) ||
+ 	    !this_cpu_has_cap(ARM64_WORKAROUND_1418040))
  		return;
  
- 	val = read_sysreg(cntkctl_el1);
- 
- 	if (!next32)
- 		val |= ARCH_TIMER_USR_VCT_ACCESS_EN;
+ 	if (is_compat_thread(task_thread_info(next)))
+ 		sysreg_clear_set(cntkctl_el1, ARCH_TIMER_USR_VCT_ACCESS_EN, 0);
  	else
- 		val &= ~ARCH_TIMER_USR_VCT_ACCESS_EN;
+ 		sysreg_clear_set(cntkctl_el1, 0, ARCH_TIMER_USR_VCT_ACCESS_EN);
+ }
  
- 	write_sysreg(val, cntkctl_el1);
+ static void erratum_1418040_new_exec(void)
+ {
+ 	preempt_disable();
+ 	erratum_1418040_thread_switch(current);
+ 	preempt_enable();
  }
  
 -/*
 - * __switch_to() checks current->thread.sctlr_user as an optimisation. Therefore
 - * this function must be called with preemption disabled and the update to
 - * sctlr_user must be made in the same preemption disabled block so that
 - * __switch_to() does not see the variable update before the SCTLR_EL1 one.
 - */
 -void update_sctlr_el1(u64 sctlr)
 -{
 -	/*
 -	 * EnIA must not be cleared while in the kernel as this is necessary for
 -	 * in-kernel PAC. It will be cleared on kernel exit if needed.
 -	 */
 -	sysreg_clear_set(sctlr_el1, SCTLR_USER_MASK & ~SCTLR_ELx_ENIA, sctlr);
 -
 -	/* ISB required for the kernel uaccess routines when setting TCF0. */
 -	isb();
 -}
 -
  /*
   * Thread switching.
   */
@@@ -556,10 -492,9 +548,15 @@@ __notrace_funcgraph struct task_struct 
  	hw_breakpoint_thread_switch(next);
  	contextidr_thread_switch(next);
  	entry_task_switch(next);
 +	uao_thread_switch(next);
 +	ptrauth_thread_switch(next);
  	ssbs_thread_switch(next);
++<<<<<<< HEAD
 +	erratum_1418040_thread_switch(prev, next);
++=======
+ 	erratum_1418040_thread_switch(next);
+ 	ptrauth_thread_switch_user(next);
++>>>>>>> 38e0257e0e6f (arm64: errata: Fix exec handling in erratum 1418040 workaround)
  
  	/*
  	 * Complete any pending TLB or cache maintenance on this CPU in case
@@@ -626,23 -580,135 +623,53 @@@ unsigned long arch_randomize_brk(struc
   */
  void arch_setup_new_exec(void)
  {
 -	unsigned long mmflags = 0;
 +	current->mm->context.flags = is_compat_task() ? MMCF_AARCH32 : 0;
  
++<<<<<<< HEAD
 +	ptrauth_thread_init_user(current);
++=======
+ 	if (is_compat_task()) {
+ 		mmflags = MMCF_AARCH32;
+ 
+ 		/*
+ 		 * Restrict the CPU affinity mask for a 32-bit task so that
+ 		 * it contains only 32-bit-capable CPUs.
+ 		 *
+ 		 * From the perspective of the task, this looks similar to
+ 		 * what would happen if the 64-bit-only CPUs were hot-unplugged
+ 		 * at the point of execve(), although we try a bit harder to
+ 		 * honour the cpuset hierarchy.
+ 		 */
+ 		if (static_branch_unlikely(&arm64_mismatched_32bit_el0))
+ 			force_compatible_cpus_allowed_ptr(current);
+ 	} else if (static_branch_unlikely(&arm64_mismatched_32bit_el0)) {
+ 		relax_compatible_cpus_allowed_ptr(current);
+ 	}
+ 
+ 	current->mm->context.flags = mmflags;
+ 	ptrauth_thread_init_user();
+ 	mte_thread_init_user();
+ 	erratum_1418040_new_exec();
+ 
+ 	if (task_spec_ssb_noexec(current)) {
+ 		arch_prctl_spec_ctrl_set(current, PR_SPEC_STORE_BYPASS,
+ 					 PR_SPEC_ENABLE);
+ 	}
++>>>>>>> 38e0257e0e6f (arm64: errata: Fix exec handling in erratum 1418040 workaround)
  }
  
 -#ifdef CONFIG_ARM64_TAGGED_ADDR_ABI
 -/*
 - * Control the relaxed ABI allowing tagged user addresses into the kernel.
 - */
 -static unsigned int tagged_addr_disabled;
 -
 -long set_tagged_addr_ctrl(struct task_struct *task, unsigned long arg)
 -{
 -	unsigned long valid_mask = PR_TAGGED_ADDR_ENABLE;
 -	struct thread_info *ti = task_thread_info(task);
 -
 -	if (is_compat_thread(ti))
 -		return -EINVAL;
 -
 -	if (system_supports_mte())
 -		valid_mask |= PR_MTE_TCF_MASK | PR_MTE_TAG_MASK;
 -
 -	if (arg & ~valid_mask)
 -		return -EINVAL;
 -
 -	/*
 -	 * Do not allow the enabling of the tagged address ABI if globally
 -	 * disabled via sysctl abi.tagged_addr_disabled.
 -	 */
 -	if (arg & PR_TAGGED_ADDR_ENABLE && tagged_addr_disabled)
 -		return -EINVAL;
 -
 -	if (set_mte_ctrl(task, arg) != 0)
 -		return -EINVAL;
 -
 -	update_ti_thread_flag(ti, TIF_TAGGED_ADDR, arg & PR_TAGGED_ADDR_ENABLE);
 -
 -	return 0;
 -}
 -
 -long get_tagged_addr_ctrl(struct task_struct *task)
 -{
 -	long ret = 0;
 -	struct thread_info *ti = task_thread_info(task);
 -
 -	if (is_compat_thread(ti))
 -		return -EINVAL;
 -
 -	if (test_ti_thread_flag(ti, TIF_TAGGED_ADDR))
 -		ret = PR_TAGGED_ADDR_ENABLE;
 -
 -	ret |= get_mte_ctrl(task);
 -
 -	return ret;
 -}
 -
 -/*
 - * Global sysctl to disable the tagged user addresses support. This control
 - * only prevents the tagged address ABI enabling via prctl() and does not
 - * disable it for tasks that already opted in to the relaxed ABI.
 - */
 -
 -static struct ctl_table tagged_addr_sysctl_table[] = {
 -	{
 -		.procname	= "tagged_addr_disabled",
 -		.mode		= 0644,
 -		.data		= &tagged_addr_disabled,
 -		.maxlen		= sizeof(int),
 -		.proc_handler	= proc_dointvec_minmax,
 -		.extra1		= SYSCTL_ZERO,
 -		.extra2		= SYSCTL_ONE,
 -	},
 -	{ }
 -};
 -
 -static int __init tagged_addr_init(void)
 +asmlinkage void __sched arm64_preempt_schedule_irq(void)
  {
 -	if (!register_sysctl("abi", tagged_addr_sysctl_table))
 -		return -EINVAL;
 -	return 0;
 -}
 -
 -core_initcall(tagged_addr_init);
 -#endif	/* CONFIG_ARM64_TAGGED_ADDR_ABI */
 +	lockdep_assert_irqs_disabled();
  
 -#ifdef CONFIG_BINFMT_ELF
 -int arch_elf_adjust_prot(int prot, const struct arch_elf_state *state,
 -			 bool has_interp, bool is_interp)
 -{
  	/*
 -	 * For dynamically linked executables the interpreter is
 -	 * responsible for setting PROT_BTI on everything except
 -	 * itself.
 +	 * Preempting a task from an IRQ means we leave copies of PSTATE
 +	 * on the stack. cpufeature's enable calls may modify PSTATE, but
 +	 * resuming one of these preempted tasks would undo those changes.
 +	 *
 +	 * Only allow a task to be preempted once cpufeatures have been
 +	 * enabled.
  	 */
 -	if (is_interp != has_interp)
 -		return prot;
 -
 -	if (!(state->flags & ARM64_ELF_BTI))
 -		return prot;
 -
 -	if (prot & PROT_EXEC)
 -		prot |= PROT_BTI;
 -
 -	return prot;
 +	if (system_capabilities_finalized())
 +		preempt_schedule_irq();
  }
 -#endif
* Unmerged path arch/arm64/kernel/process.c
