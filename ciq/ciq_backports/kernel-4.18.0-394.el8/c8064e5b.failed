bpf: Let bpf_warn_invalid_xdp_action() report more info

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Paolo Abeni <pabeni@redhat.com>
commit c8064e5b4adac5e1255cf4f3b374e75b5376e7ca
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/c8064e5b.failed

In non trivial scenarios, the action id alone is not sufficient to
identify the program causing the warning. Before the previous patch,
the generated stack-trace pointed out at least the involved device
driver.

Let's additionally include the program name and id, and the relevant
device name.

If the user needs additional infos, he can fetch them via a kernel
probe, leveraging the arguments added here.

	Signed-off-by: Paolo Abeni <pabeni@redhat.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
Link: https://lore.kernel.org/bpf/ddb96bb975cbfddb1546cf5da60e77d5100b533c.1638189075.git.pabeni@redhat.com
(cherry picked from commit c8064e5b4adac5e1255cf4f3b374e75b5376e7ca)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
#	drivers/net/ethernet/cavium/thunder/nicvf_main.c
#	drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
#	drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c
#	drivers/net/ethernet/freescale/enetc/enetc.c
#	drivers/net/ethernet/marvell/mvneta.c
#	drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
#	drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
#	drivers/net/ethernet/netronome/nfp/nfp_net_common.c
#	drivers/net/ethernet/socionext/netsec.c
#	drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
#	drivers/net/ethernet/ti/cpsw_priv.c
#	drivers/net/tun.c
#	drivers/net/veth.c
#	drivers/net/virtio_net.c
#	drivers/net/xen-netfront.c
#	kernel/bpf/cpumap.c
#	kernel/bpf/devmap.c
#	net/core/dev.c
diff --cc drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
index a46841280db4,52fad0fdeacf..000000000000
--- a/drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
@@@ -195,11 -195,11 +195,16 @@@ bool bnxt_rx_xdp(struct bnxt *bp, struc
  		*event |= BNXT_REDIRECT_EVENT;
  		break;
  	default:
++<<<<<<< HEAD
 +		bpf_warn_invalid_xdp_action(act);
 +		/* Fall thru */
++=======
+ 		bpf_warn_invalid_xdp_action(bp->dev, xdp_prog, act);
+ 		fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  	case XDP_ABORTED:
  		trace_xdp_exception(bp->dev, xdp_prog, act);
 -		fallthrough;
 +		/* Fall thru */
  	case XDP_DROP:
  		bnxt_reuse_rx_data(rxr, cons, page);
  		break;
diff --cc drivers/net/ethernet/cavium/thunder/nicvf_main.c
index a086087f42fa,30450efccad7..000000000000
--- a/drivers/net/ethernet/cavium/thunder/nicvf_main.c
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
@@@ -590,11 -590,11 +590,16 @@@ static inline bool nicvf_xdp_rx(struct 
  		nicvf_xdp_sq_append_pkt(nic, sq, (u64)xdp.data, dma_addr, len);
  		return true;
  	default:
++<<<<<<< HEAD
 +		bpf_warn_invalid_xdp_action(action);
 +		/* fall through */
++=======
+ 		bpf_warn_invalid_xdp_action(nic->netdev, prog, action);
+ 		fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  	case XDP_ABORTED:
  		trace_xdp_exception(nic->netdev, prog, action);
 -		fallthrough;
 +		/* fall through */
  	case XDP_DROP:
  		/* Check if it's a recycled page, if not
  		 * unmap the DMA mapping.
diff --cc drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
index 15fe3d780457,c78883c3a2c8..000000000000
--- a/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
+++ b/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
@@@ -2224,10 -2475,176 +2224,175 @@@ static enum qman_cb_dqrr_result rx_erro
  	return qman_cb_dqrr_consume;
  }
  
++<<<<<<< HEAD
++=======
+ static int dpaa_xdp_xmit_frame(struct net_device *net_dev,
+ 			       struct xdp_frame *xdpf)
+ {
+ 	struct dpaa_priv *priv = netdev_priv(net_dev);
+ 	struct rtnl_link_stats64 *percpu_stats;
+ 	struct dpaa_percpu_priv *percpu_priv;
+ 	struct dpaa_eth_swbp *swbp;
+ 	struct netdev_queue *txq;
+ 	void *buff_start;
+ 	struct qm_fd fd;
+ 	dma_addr_t addr;
+ 	int err;
+ 
+ 	percpu_priv = this_cpu_ptr(priv->percpu_priv);
+ 	percpu_stats = &percpu_priv->stats;
+ 
+ #ifdef CONFIG_DPAA_ERRATUM_A050385
+ 	if (unlikely(fman_has_errata_a050385())) {
+ 		if (dpaa_a050385_wa_xdpf(priv, &xdpf)) {
+ 			err = -ENOMEM;
+ 			goto out_error;
+ 		}
+ 	}
+ #endif
+ 
+ 	if (xdpf->headroom < DPAA_TX_PRIV_DATA_SIZE) {
+ 		err = -EINVAL;
+ 		goto out_error;
+ 	}
+ 
+ 	buff_start = xdpf->data - xdpf->headroom;
+ 
+ 	/* Leave empty the skb backpointer at the start of the buffer.
+ 	 * Save the XDP frame for easy cleanup on confirmation.
+ 	 */
+ 	swbp = (struct dpaa_eth_swbp *)buff_start;
+ 	swbp->skb = NULL;
+ 	swbp->xdpf = xdpf;
+ 
+ 	qm_fd_clear_fd(&fd);
+ 	fd.bpid = FSL_DPAA_BPID_INV;
+ 	fd.cmd |= cpu_to_be32(FM_FD_CMD_FCO);
+ 	qm_fd_set_contig(&fd, xdpf->headroom, xdpf->len);
+ 
+ 	addr = dma_map_single(priv->tx_dma_dev, buff_start,
+ 			      xdpf->headroom + xdpf->len,
+ 			      DMA_TO_DEVICE);
+ 	if (unlikely(dma_mapping_error(priv->tx_dma_dev, addr))) {
+ 		err = -EINVAL;
+ 		goto out_error;
+ 	}
+ 
+ 	qm_fd_addr_set64(&fd, addr);
+ 
+ 	/* Bump the trans_start */
+ 	txq = netdev_get_tx_queue(net_dev, smp_processor_id());
+ 	txq_trans_cond_update(txq);
+ 
+ 	err = dpaa_xmit(priv, percpu_stats, smp_processor_id(), &fd);
+ 	if (err) {
+ 		dma_unmap_single(priv->tx_dma_dev, addr,
+ 				 qm_fd_get_offset(&fd) + qm_fd_get_length(&fd),
+ 				 DMA_TO_DEVICE);
+ 		goto out_error;
+ 	}
+ 
+ 	return 0;
+ 
+ out_error:
+ 	percpu_stats->tx_errors++;
+ 	return err;
+ }
+ 
+ static u32 dpaa_run_xdp(struct dpaa_priv *priv, struct qm_fd *fd, void *vaddr,
+ 			struct dpaa_fq *dpaa_fq, unsigned int *xdp_meta_len)
+ {
+ 	ssize_t fd_off = qm_fd_get_offset(fd);
+ 	struct bpf_prog *xdp_prog;
+ 	struct xdp_frame *xdpf;
+ 	struct xdp_buff xdp;
+ 	u32 xdp_act;
+ 	int err;
+ 
+ 	xdp_prog = READ_ONCE(priv->xdp_prog);
+ 	if (!xdp_prog)
+ 		return XDP_PASS;
+ 
+ 	xdp_init_buff(&xdp, DPAA_BP_RAW_SIZE - DPAA_TX_PRIV_DATA_SIZE,
+ 		      &dpaa_fq->xdp_rxq);
+ 	xdp_prepare_buff(&xdp, vaddr + fd_off - XDP_PACKET_HEADROOM,
+ 			 XDP_PACKET_HEADROOM, qm_fd_get_length(fd), true);
+ 
+ 	/* We reserve a fixed headroom of 256 bytes under the erratum and we
+ 	 * offer it all to XDP programs to use. If no room is left for the
+ 	 * xdpf backpointer on TX, we will need to copy the data.
+ 	 * Disable metadata support since data realignments might be required
+ 	 * and the information can be lost.
+ 	 */
+ #ifdef CONFIG_DPAA_ERRATUM_A050385
+ 	if (unlikely(fman_has_errata_a050385())) {
+ 		xdp_set_data_meta_invalid(&xdp);
+ 		xdp.data_hard_start = vaddr;
+ 		xdp.frame_sz = DPAA_BP_RAW_SIZE;
+ 	}
+ #endif
+ 
+ 	xdp_act = bpf_prog_run_xdp(xdp_prog, &xdp);
+ 
+ 	/* Update the length and the offset of the FD */
+ 	qm_fd_set_contig(fd, xdp.data - vaddr, xdp.data_end - xdp.data);
+ 
+ 	switch (xdp_act) {
+ 	case XDP_PASS:
+ #ifdef CONFIG_DPAA_ERRATUM_A050385
+ 		*xdp_meta_len = xdp_data_meta_unsupported(&xdp) ? 0 :
+ 				xdp.data - xdp.data_meta;
+ #else
+ 		*xdp_meta_len = xdp.data - xdp.data_meta;
+ #endif
+ 		break;
+ 	case XDP_TX:
+ 		/* We can access the full headroom when sending the frame
+ 		 * back out
+ 		 */
+ 		xdp.data_hard_start = vaddr;
+ 		xdp.frame_sz = DPAA_BP_RAW_SIZE;
+ 		xdpf = xdp_convert_buff_to_frame(&xdp);
+ 		if (unlikely(!xdpf)) {
+ 			free_pages((unsigned long)vaddr, 0);
+ 			break;
+ 		}
+ 
+ 		if (dpaa_xdp_xmit_frame(priv->net_dev, xdpf))
+ 			xdp_return_frame_rx_napi(xdpf);
+ 
+ 		break;
+ 	case XDP_REDIRECT:
+ 		/* Allow redirect to use the full headroom */
+ 		xdp.data_hard_start = vaddr;
+ 		xdp.frame_sz = DPAA_BP_RAW_SIZE;
+ 
+ 		err = xdp_do_redirect(priv->net_dev, &xdp, xdp_prog);
+ 		if (err) {
+ 			trace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);
+ 			free_pages((unsigned long)vaddr, 0);
+ 		}
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(priv->net_dev, xdp_prog, xdp_act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		/* Free the buffer */
+ 		free_pages((unsigned long)vaddr, 0);
+ 		break;
+ 	}
+ 
+ 	return xdp_act;
+ }
+ 
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  static enum qman_cb_dqrr_result rx_default_dqrr(struct qman_portal *portal,
  						struct qman_fq *fq,
 -						const struct qm_dqrr_entry *dq,
 -						bool sched_napi)
 +						const struct qm_dqrr_entry *dq)
  {
 -	bool ts_valid = false, hash_valid = false;
 -	struct skb_shared_hwtstamps *shhwtstamps;
 -	unsigned int skb_len, xdp_meta_len = 0;
  	struct rtnl_link_stats64 *percpu_stats;
  	struct dpaa_percpu_priv *percpu_priv;
  	const struct qm_fd *fd = &dq->fd;
diff --cc drivers/net/ethernet/marvell/mvneta.c
index 25531a4f401f,23cc4b874285..000000000000
--- a/drivers/net/ethernet/marvell/mvneta.c
+++ b/drivers/net/ethernet/marvell/mvneta.c
@@@ -1894,11 -1999,376 +1894,379 @@@ static void mvneta_rxq_drop_pkts(struc
  	for (i = 0; i < rxq->size; i++) {
  		struct mvneta_rx_desc *rx_desc = rxq->descs + i;
  		void *data = rxq->buf_virt_addr[i];
 -		if (!data || !(rx_desc->buf_phys_addr))
 -			continue;
  
 -		page_pool_put_full_page(rxq->page_pool, data, false);
 +		dma_unmap_single(pp->dev->dev.parent, rx_desc->buf_phys_addr,
 +				 MVNETA_RX_BUF_SIZE(pp->pkt_size), DMA_FROM_DEVICE);
 +		mvneta_frag_free(pp->frag_size, data);
  	}
++<<<<<<< HEAD
++=======
+ 	if (xdp_rxq_info_is_reg(&rxq->xdp_rxq))
+ 		xdp_rxq_info_unreg(&rxq->xdp_rxq);
+ 	page_pool_destroy(rxq->page_pool);
+ 	rxq->page_pool = NULL;
+ }
+ 
+ static void
+ mvneta_update_stats(struct mvneta_port *pp,
+ 		    struct mvneta_stats *ps)
+ {
+ 	struct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);
+ 
+ 	u64_stats_update_begin(&stats->syncp);
+ 	stats->es.ps.rx_packets += ps->rx_packets;
+ 	stats->es.ps.rx_bytes += ps->rx_bytes;
+ 	/* xdp */
+ 	stats->es.ps.xdp_redirect += ps->xdp_redirect;
+ 	stats->es.ps.xdp_pass += ps->xdp_pass;
+ 	stats->es.ps.xdp_drop += ps->xdp_drop;
+ 	u64_stats_update_end(&stats->syncp);
+ }
+ 
+ static inline
+ int mvneta_rx_refill_queue(struct mvneta_port *pp, struct mvneta_rx_queue *rxq)
+ {
+ 	struct mvneta_rx_desc *rx_desc;
+ 	int curr_desc = rxq->first_to_refill;
+ 	int i;
+ 
+ 	for (i = 0; (i < rxq->refill_num) && (i < 64); i++) {
+ 		rx_desc = rxq->descs + curr_desc;
+ 		if (!(rx_desc->buf_phys_addr)) {
+ 			if (mvneta_rx_refill(pp, rx_desc, rxq, GFP_ATOMIC)) {
+ 				struct mvneta_pcpu_stats *stats;
+ 
+ 				pr_err("Can't refill queue %d. Done %d from %d\n",
+ 				       rxq->id, i, rxq->refill_num);
+ 
+ 				stats = this_cpu_ptr(pp->stats);
+ 				u64_stats_update_begin(&stats->syncp);
+ 				stats->es.refill_error++;
+ 				u64_stats_update_end(&stats->syncp);
+ 				break;
+ 			}
+ 		}
+ 		curr_desc = MVNETA_QUEUE_NEXT_DESC(rxq, curr_desc);
+ 	}
+ 	rxq->refill_num -= i;
+ 	rxq->first_to_refill = curr_desc;
+ 
+ 	return i;
+ }
+ 
+ static void
+ mvneta_xdp_put_buff(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,
+ 		    struct xdp_buff *xdp, struct skb_shared_info *sinfo,
+ 		    int sync_len)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < sinfo->nr_frags; i++)
+ 		page_pool_put_full_page(rxq->page_pool,
+ 					skb_frag_page(&sinfo->frags[i]), true);
+ 	page_pool_put_page(rxq->page_pool, virt_to_head_page(xdp->data),
+ 			   sync_len, true);
+ }
+ 
+ static int
+ mvneta_xdp_submit_frame(struct mvneta_port *pp, struct mvneta_tx_queue *txq,
+ 			struct xdp_frame *xdpf, bool dma_map)
+ {
+ 	struct mvneta_tx_desc *tx_desc;
+ 	struct mvneta_tx_buf *buf;
+ 	dma_addr_t dma_addr;
+ 
+ 	if (txq->count >= txq->tx_stop_threshold)
+ 		return MVNETA_XDP_DROPPED;
+ 
+ 	tx_desc = mvneta_txq_next_desc_get(txq);
+ 
+ 	buf = &txq->buf[txq->txq_put_index];
+ 	if (dma_map) {
+ 		/* ndo_xdp_xmit */
+ 		dma_addr = dma_map_single(pp->dev->dev.parent, xdpf->data,
+ 					  xdpf->len, DMA_TO_DEVICE);
+ 		if (dma_mapping_error(pp->dev->dev.parent, dma_addr)) {
+ 			mvneta_txq_desc_put(txq);
+ 			return MVNETA_XDP_DROPPED;
+ 		}
+ 		buf->type = MVNETA_TYPE_XDP_NDO;
+ 	} else {
+ 		struct page *page = virt_to_page(xdpf->data);
+ 
+ 		dma_addr = page_pool_get_dma_addr(page) +
+ 			   sizeof(*xdpf) + xdpf->headroom;
+ 		dma_sync_single_for_device(pp->dev->dev.parent, dma_addr,
+ 					   xdpf->len, DMA_BIDIRECTIONAL);
+ 		buf->type = MVNETA_TYPE_XDP_TX;
+ 	}
+ 	buf->xdpf = xdpf;
+ 
+ 	tx_desc->command = MVNETA_TXD_FLZ_DESC;
+ 	tx_desc->buf_phys_addr = dma_addr;
+ 	tx_desc->data_size = xdpf->len;
+ 
+ 	mvneta_txq_inc_put(txq);
+ 	txq->pending++;
+ 	txq->count++;
+ 
+ 	return MVNETA_XDP_TX;
+ }
+ 
+ static int
+ mvneta_xdp_xmit_back(struct mvneta_port *pp, struct xdp_buff *xdp)
+ {
+ 	struct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);
+ 	struct mvneta_tx_queue *txq;
+ 	struct netdev_queue *nq;
+ 	struct xdp_frame *xdpf;
+ 	int cpu;
+ 	u32 ret;
+ 
+ 	xdpf = xdp_convert_buff_to_frame(xdp);
+ 	if (unlikely(!xdpf))
+ 		return MVNETA_XDP_DROPPED;
+ 
+ 	cpu = smp_processor_id();
+ 	txq = &pp->txqs[cpu % txq_number];
+ 	nq = netdev_get_tx_queue(pp->dev, txq->id);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	ret = mvneta_xdp_submit_frame(pp, txq, xdpf, false);
+ 	if (ret == MVNETA_XDP_TX) {
+ 		u64_stats_update_begin(&stats->syncp);
+ 		stats->es.ps.tx_bytes += xdpf->len;
+ 		stats->es.ps.tx_packets++;
+ 		stats->es.ps.xdp_tx++;
+ 		u64_stats_update_end(&stats->syncp);
+ 
+ 		mvneta_txq_pend_desc_add(pp, txq, 0);
+ 	} else {
+ 		u64_stats_update_begin(&stats->syncp);
+ 		stats->es.ps.xdp_tx_err++;
+ 		u64_stats_update_end(&stats->syncp);
+ 	}
+ 	__netif_tx_unlock(nq);
+ 
+ 	return ret;
+ }
+ 
+ static int
+ mvneta_xdp_xmit(struct net_device *dev, int num_frame,
+ 		struct xdp_frame **frames, u32 flags)
+ {
+ 	struct mvneta_port *pp = netdev_priv(dev);
+ 	struct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);
+ 	int i, nxmit_byte = 0, nxmit = 0;
+ 	int cpu = smp_processor_id();
+ 	struct mvneta_tx_queue *txq;
+ 	struct netdev_queue *nq;
+ 	u32 ret;
+ 
+ 	if (unlikely(test_bit(__MVNETA_DOWN, &pp->state)))
+ 		return -ENETDOWN;
+ 
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
+ 
+ 	txq = &pp->txqs[cpu % txq_number];
+ 	nq = netdev_get_tx_queue(pp->dev, txq->id);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	for (i = 0; i < num_frame; i++) {
+ 		ret = mvneta_xdp_submit_frame(pp, txq, frames[i], true);
+ 		if (ret != MVNETA_XDP_TX)
+ 			break;
+ 
+ 		nxmit_byte += frames[i]->len;
+ 		nxmit++;
+ 	}
+ 
+ 	if (unlikely(flags & XDP_XMIT_FLUSH))
+ 		mvneta_txq_pend_desc_add(pp, txq, 0);
+ 	__netif_tx_unlock(nq);
+ 
+ 	u64_stats_update_begin(&stats->syncp);
+ 	stats->es.ps.tx_bytes += nxmit_byte;
+ 	stats->es.ps.tx_packets += nxmit;
+ 	stats->es.ps.xdp_xmit += nxmit;
+ 	stats->es.ps.xdp_xmit_err += num_frame - nxmit;
+ 	u64_stats_update_end(&stats->syncp);
+ 
+ 	return nxmit;
+ }
+ 
+ static int
+ mvneta_run_xdp(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,
+ 	       struct bpf_prog *prog, struct xdp_buff *xdp,
+ 	       u32 frame_sz, struct mvneta_stats *stats)
+ {
+ 	struct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);
+ 	unsigned int len, data_len, sync;
+ 	u32 ret, act;
+ 
+ 	len = xdp->data_end - xdp->data_hard_start - pp->rx_offset_correction;
+ 	data_len = xdp->data_end - xdp->data;
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 
+ 	/* Due xdp_adjust_tail: DMA sync for_device cover max len CPU touch */
+ 	sync = xdp->data_end - xdp->data_hard_start - pp->rx_offset_correction;
+ 	sync = max(sync, len);
+ 
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		stats->xdp_pass++;
+ 		return MVNETA_XDP_PASS;
+ 	case XDP_REDIRECT: {
+ 		int err;
+ 
+ 		err = xdp_do_redirect(pp->dev, xdp, prog);
+ 		if (unlikely(err)) {
+ 			mvneta_xdp_put_buff(pp, rxq, xdp, sinfo, sync);
+ 			ret = MVNETA_XDP_DROPPED;
+ 		} else {
+ 			ret = MVNETA_XDP_REDIR;
+ 			stats->xdp_redirect++;
+ 		}
+ 		break;
+ 	}
+ 	case XDP_TX:
+ 		ret = mvneta_xdp_xmit_back(pp, xdp);
+ 		if (ret != MVNETA_XDP_TX)
+ 			mvneta_xdp_put_buff(pp, rxq, xdp, sinfo, sync);
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(pp->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(pp->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		mvneta_xdp_put_buff(pp, rxq, xdp, sinfo, sync);
+ 		ret = MVNETA_XDP_DROPPED;
+ 		stats->xdp_drop++;
+ 		break;
+ 	}
+ 
+ 	stats->rx_bytes += frame_sz + xdp->data_end - xdp->data - data_len;
+ 	stats->rx_packets++;
+ 
+ 	return ret;
+ }
+ 
+ static void
+ mvneta_swbm_rx_frame(struct mvneta_port *pp,
+ 		     struct mvneta_rx_desc *rx_desc,
+ 		     struct mvneta_rx_queue *rxq,
+ 		     struct xdp_buff *xdp, int *size,
+ 		     struct page *page)
+ {
+ 	unsigned char *data = page_address(page);
+ 	int data_len = -MVNETA_MH_SIZE, len;
+ 	struct net_device *dev = pp->dev;
+ 	enum dma_data_direction dma_dir;
+ 	struct skb_shared_info *sinfo;
+ 
+ 	if (*size > MVNETA_MAX_RX_BUF_SIZE) {
+ 		len = MVNETA_MAX_RX_BUF_SIZE;
+ 		data_len += len;
+ 	} else {
+ 		len = *size;
+ 		data_len += len - ETH_FCS_LEN;
+ 	}
+ 	*size = *size - len;
+ 
+ 	dma_dir = page_pool_get_dma_dir(rxq->page_pool);
+ 	dma_sync_single_for_cpu(dev->dev.parent,
+ 				rx_desc->buf_phys_addr,
+ 				len, dma_dir);
+ 
+ 	rx_desc->buf_phys_addr = 0;
+ 
+ 	/* Prefetch header */
+ 	prefetch(data);
+ 	xdp_prepare_buff(xdp, data, pp->rx_offset_correction + MVNETA_MH_SIZE,
+ 			 data_len, false);
+ 
+ 	sinfo = xdp_get_shared_info_from_buff(xdp);
+ 	sinfo->nr_frags = 0;
+ }
+ 
+ static void
+ mvneta_swbm_add_rx_fragment(struct mvneta_port *pp,
+ 			    struct mvneta_rx_desc *rx_desc,
+ 			    struct mvneta_rx_queue *rxq,
+ 			    struct xdp_buff *xdp, int *size,
+ 			    struct skb_shared_info *xdp_sinfo,
+ 			    struct page *page)
+ {
+ 	struct net_device *dev = pp->dev;
+ 	enum dma_data_direction dma_dir;
+ 	int data_len, len;
+ 
+ 	if (*size > MVNETA_MAX_RX_BUF_SIZE) {
+ 		len = MVNETA_MAX_RX_BUF_SIZE;
+ 		data_len = len;
+ 	} else {
+ 		len = *size;
+ 		data_len = len - ETH_FCS_LEN;
+ 	}
+ 	dma_dir = page_pool_get_dma_dir(rxq->page_pool);
+ 	dma_sync_single_for_cpu(dev->dev.parent,
+ 				rx_desc->buf_phys_addr,
+ 				len, dma_dir);
+ 	rx_desc->buf_phys_addr = 0;
+ 
+ 	if (data_len > 0 && xdp_sinfo->nr_frags < MAX_SKB_FRAGS) {
+ 		skb_frag_t *frag = &xdp_sinfo->frags[xdp_sinfo->nr_frags++];
+ 
+ 		skb_frag_off_set(frag, pp->rx_offset_correction);
+ 		skb_frag_size_set(frag, data_len);
+ 		__skb_frag_set_page(frag, page);
+ 	} else {
+ 		page_pool_put_full_page(rxq->page_pool, page, true);
+ 	}
+ 
+ 	/* last fragment */
+ 	if (len == *size) {
+ 		struct skb_shared_info *sinfo;
+ 
+ 		sinfo = xdp_get_shared_info_from_buff(xdp);
+ 		sinfo->nr_frags = xdp_sinfo->nr_frags;
+ 		memcpy(sinfo->frags, xdp_sinfo->frags,
+ 		       sinfo->nr_frags * sizeof(skb_frag_t));
+ 	}
+ 	*size -= len;
+ }
+ 
+ static struct sk_buff *
+ mvneta_swbm_build_skb(struct mvneta_port *pp, struct page_pool *pool,
+ 		      struct xdp_buff *xdp, u32 desc_status)
+ {
+ 	struct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);
+ 	int i, num_frags = sinfo->nr_frags;
+ 	struct sk_buff *skb;
+ 
+ 	skb = build_skb(xdp->data_hard_start, PAGE_SIZE);
+ 	if (!skb)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	skb_mark_for_recycle(skb);
+ 
+ 	skb_reserve(skb, xdp->data - xdp->data_hard_start);
+ 	skb_put(skb, xdp->data_end - xdp->data);
+ 	skb->ip_summed = mvneta_rx_csum(pp, desc_status);
+ 
+ 	for (i = 0; i < num_frags; i++) {
+ 		skb_frag_t *frag = &sinfo->frags[i];
+ 
+ 		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+ 				skb_frag_page(frag), skb_frag_off(frag),
+ 				skb_frag_size(frag), PAGE_SIZE);
+ 	}
+ 
+ 	return skb;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  }
  
  /* Main rx processing when using software buffer management */
diff --cc drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
index 9638dce58ba2,7273a4c9dbb1..000000000000
--- a/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
+++ b/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
@@@ -2628,6 -3611,263 +2628,266 @@@ static u32 mvpp2_skb_tx_csum(struct mvp
  	return MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE;
  }
  
++<<<<<<< HEAD
++=======
+ static void mvpp2_xdp_finish_tx(struct mvpp2_port *port, u16 txq_id, int nxmit, int nxmit_byte)
+ {
+ 	unsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());
+ 	struct mvpp2_tx_queue *aggr_txq;
+ 	struct mvpp2_txq_pcpu *txq_pcpu;
+ 	struct mvpp2_tx_queue *txq;
+ 	struct netdev_queue *nq;
+ 
+ 	txq = port->txqs[txq_id];
+ 	txq_pcpu = per_cpu_ptr(txq->pcpu, thread);
+ 	nq = netdev_get_tx_queue(port->dev, txq_id);
+ 	aggr_txq = &port->priv->aggr_txqs[thread];
+ 
+ 	txq_pcpu->reserved_num -= nxmit;
+ 	txq_pcpu->count += nxmit;
+ 	aggr_txq->count += nxmit;
+ 
+ 	/* Enable transmit */
+ 	wmb();
+ 	mvpp2_aggr_txq_pend_desc_add(port, nxmit);
+ 
+ 	if (txq_pcpu->count >= txq_pcpu->stop_threshold)
+ 		netif_tx_stop_queue(nq);
+ 
+ 	/* Finalize TX processing */
+ 	if (!port->has_tx_irqs && txq_pcpu->count >= txq->done_pkts_coal)
+ 		mvpp2_txq_done(port, txq, txq_pcpu);
+ }
+ 
+ static int
+ mvpp2_xdp_submit_frame(struct mvpp2_port *port, u16 txq_id,
+ 		       struct xdp_frame *xdpf, bool dma_map)
+ {
+ 	unsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());
+ 	u32 tx_cmd = MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE |
+ 		     MVPP2_TXD_F_DESC | MVPP2_TXD_L_DESC;
+ 	enum mvpp2_tx_buf_type buf_type;
+ 	struct mvpp2_txq_pcpu *txq_pcpu;
+ 	struct mvpp2_tx_queue *aggr_txq;
+ 	struct mvpp2_tx_desc *tx_desc;
+ 	struct mvpp2_tx_queue *txq;
+ 	int ret = MVPP2_XDP_TX;
+ 	dma_addr_t dma_addr;
+ 
+ 	txq = port->txqs[txq_id];
+ 	txq_pcpu = per_cpu_ptr(txq->pcpu, thread);
+ 	aggr_txq = &port->priv->aggr_txqs[thread];
+ 
+ 	/* Check number of available descriptors */
+ 	if (mvpp2_aggr_desc_num_check(port, aggr_txq, 1) ||
+ 	    mvpp2_txq_reserved_desc_num_proc(port, txq, txq_pcpu, 1)) {
+ 		ret = MVPP2_XDP_DROPPED;
+ 		goto out;
+ 	}
+ 
+ 	/* Get a descriptor for the first part of the packet */
+ 	tx_desc = mvpp2_txq_next_desc_get(aggr_txq);
+ 	mvpp2_txdesc_txq_set(port, tx_desc, txq->id);
+ 	mvpp2_txdesc_size_set(port, tx_desc, xdpf->len);
+ 
+ 	if (dma_map) {
+ 		/* XDP_REDIRECT or AF_XDP */
+ 		dma_addr = dma_map_single(port->dev->dev.parent, xdpf->data,
+ 					  xdpf->len, DMA_TO_DEVICE);
+ 
+ 		if (unlikely(dma_mapping_error(port->dev->dev.parent, dma_addr))) {
+ 			mvpp2_txq_desc_put(txq);
+ 			ret = MVPP2_XDP_DROPPED;
+ 			goto out;
+ 		}
+ 
+ 		buf_type = MVPP2_TYPE_XDP_NDO;
+ 	} else {
+ 		/* XDP_TX */
+ 		struct page *page = virt_to_page(xdpf->data);
+ 
+ 		dma_addr = page_pool_get_dma_addr(page) +
+ 			   sizeof(*xdpf) + xdpf->headroom;
+ 		dma_sync_single_for_device(port->dev->dev.parent, dma_addr,
+ 					   xdpf->len, DMA_BIDIRECTIONAL);
+ 
+ 		buf_type = MVPP2_TYPE_XDP_TX;
+ 	}
+ 
+ 	mvpp2_txdesc_dma_addr_set(port, tx_desc, dma_addr);
+ 
+ 	mvpp2_txdesc_cmd_set(port, tx_desc, tx_cmd);
+ 	mvpp2_txq_inc_put(port, txq_pcpu, xdpf, tx_desc, buf_type);
+ 
+ out:
+ 	return ret;
+ }
+ 
+ static int
+ mvpp2_xdp_xmit_back(struct mvpp2_port *port, struct xdp_buff *xdp)
+ {
+ 	struct mvpp2_pcpu_stats *stats = this_cpu_ptr(port->stats);
+ 	struct xdp_frame *xdpf;
+ 	u16 txq_id;
+ 	int ret;
+ 
+ 	xdpf = xdp_convert_buff_to_frame(xdp);
+ 	if (unlikely(!xdpf))
+ 		return MVPP2_XDP_DROPPED;
+ 
+ 	/* The first of the TX queues are used for XPS,
+ 	 * the second half for XDP_TX
+ 	 */
+ 	txq_id = mvpp2_cpu_to_thread(port->priv, smp_processor_id()) + (port->ntxqs / 2);
+ 
+ 	ret = mvpp2_xdp_submit_frame(port, txq_id, xdpf, false);
+ 	if (ret == MVPP2_XDP_TX) {
+ 		u64_stats_update_begin(&stats->syncp);
+ 		stats->tx_bytes += xdpf->len;
+ 		stats->tx_packets++;
+ 		stats->xdp_tx++;
+ 		u64_stats_update_end(&stats->syncp);
+ 
+ 		mvpp2_xdp_finish_tx(port, txq_id, 1, xdpf->len);
+ 	} else {
+ 		u64_stats_update_begin(&stats->syncp);
+ 		stats->xdp_tx_err++;
+ 		u64_stats_update_end(&stats->syncp);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int
+ mvpp2_xdp_xmit(struct net_device *dev, int num_frame,
+ 	       struct xdp_frame **frames, u32 flags)
+ {
+ 	struct mvpp2_port *port = netdev_priv(dev);
+ 	int i, nxmit_byte = 0, nxmit = 0;
+ 	struct mvpp2_pcpu_stats *stats;
+ 	u16 txq_id;
+ 	u32 ret;
+ 
+ 	if (unlikely(test_bit(0, &port->state)))
+ 		return -ENETDOWN;
+ 
+ 	if (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))
+ 		return -EINVAL;
+ 
+ 	/* The first of the TX queues are used for XPS,
+ 	 * the second half for XDP_TX
+ 	 */
+ 	txq_id = mvpp2_cpu_to_thread(port->priv, smp_processor_id()) + (port->ntxqs / 2);
+ 
+ 	for (i = 0; i < num_frame; i++) {
+ 		ret = mvpp2_xdp_submit_frame(port, txq_id, frames[i], true);
+ 		if (ret != MVPP2_XDP_TX)
+ 			break;
+ 
+ 		nxmit_byte += frames[i]->len;
+ 		nxmit++;
+ 	}
+ 
+ 	if (likely(nxmit > 0))
+ 		mvpp2_xdp_finish_tx(port, txq_id, nxmit, nxmit_byte);
+ 
+ 	stats = this_cpu_ptr(port->stats);
+ 	u64_stats_update_begin(&stats->syncp);
+ 	stats->tx_bytes += nxmit_byte;
+ 	stats->tx_packets += nxmit;
+ 	stats->xdp_xmit += nxmit;
+ 	stats->xdp_xmit_err += num_frame - nxmit;
+ 	u64_stats_update_end(&stats->syncp);
+ 
+ 	return nxmit;
+ }
+ 
+ static int
+ mvpp2_run_xdp(struct mvpp2_port *port, struct bpf_prog *prog,
+ 	      struct xdp_buff *xdp, struct page_pool *pp,
+ 	      struct mvpp2_pcpu_stats *stats)
+ {
+ 	unsigned int len, sync, err;
+ 	struct page *page;
+ 	u32 ret, act;
+ 
+ 	len = xdp->data_end - xdp->data_hard_start - MVPP2_SKB_HEADROOM;
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 
+ 	/* Due xdp_adjust_tail: DMA sync for_device cover max len CPU touch */
+ 	sync = xdp->data_end - xdp->data_hard_start - MVPP2_SKB_HEADROOM;
+ 	sync = max(sync, len);
+ 
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		stats->xdp_pass++;
+ 		ret = MVPP2_XDP_PASS;
+ 		break;
+ 	case XDP_REDIRECT:
+ 		err = xdp_do_redirect(port->dev, xdp, prog);
+ 		if (unlikely(err)) {
+ 			ret = MVPP2_XDP_DROPPED;
+ 			page = virt_to_head_page(xdp->data);
+ 			page_pool_put_page(pp, page, sync, true);
+ 		} else {
+ 			ret = MVPP2_XDP_REDIR;
+ 			stats->xdp_redirect++;
+ 		}
+ 		break;
+ 	case XDP_TX:
+ 		ret = mvpp2_xdp_xmit_back(port, xdp);
+ 		if (ret != MVPP2_XDP_TX) {
+ 			page = virt_to_head_page(xdp->data);
+ 			page_pool_put_page(pp, page, sync, true);
+ 		}
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(port->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(port->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		page = virt_to_head_page(xdp->data);
+ 		page_pool_put_page(pp, page, sync, true);
+ 		ret = MVPP2_XDP_DROPPED;
+ 		stats->xdp_drop++;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static void mvpp2_buff_hdr_pool_put(struct mvpp2_port *port, struct mvpp2_rx_desc *rx_desc,
+ 				    int pool, u32 rx_status)
+ {
+ 	phys_addr_t phys_addr, phys_addr_next;
+ 	dma_addr_t dma_addr, dma_addr_next;
+ 	struct mvpp2_buff_hdr *buff_hdr;
+ 
+ 	phys_addr = mvpp2_rxdesc_dma_addr_get(port, rx_desc);
+ 	dma_addr = mvpp2_rxdesc_cookie_get(port, rx_desc);
+ 
+ 	do {
+ 		buff_hdr = (struct mvpp2_buff_hdr *)phys_to_virt(phys_addr);
+ 
+ 		phys_addr_next = le32_to_cpu(buff_hdr->next_phys_addr);
+ 		dma_addr_next = le32_to_cpu(buff_hdr->next_dma_addr);
+ 
+ 		if (port->priv->hw_version >= MVPP22) {
+ 			phys_addr_next |= ((u64)buff_hdr->next_phys_addr_high << 32);
+ 			dma_addr_next |= ((u64)buff_hdr->next_dma_addr_high << 32);
+ 		}
+ 
+ 		mvpp2_bm_pool_put(port, pool, dma_addr, phys_addr);
+ 
+ 		phys_addr = phys_addr_next;
+ 		dma_addr = dma_addr_next;
+ 
+ 	} while (!MVPP2_B_HDR_INFO_IS_LAST(le16_to_cpu(buff_hdr->info)));
+ }
+ 
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  /* Main rx processing */
  static int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,
  		    int rx_todo, struct mvpp2_rx_queue *rxq)
diff --cc drivers/net/ethernet/netronome/nfp/nfp_net_common.c
index 75f99cfc9e9f,79257ec41987..000000000000
--- a/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
+++ b/drivers/net/ethernet/netronome/nfp/nfp_net_common.c
@@@ -1939,11 -1944,11 +1939,16 @@@ static int nfp_net_rx(struct nfp_net_rx
  							    xdp_prog, act);
  				continue;
  			default:
++<<<<<<< HEAD
 +				bpf_warn_invalid_xdp_action(act);
 +				/* fall through */
++=======
+ 				bpf_warn_invalid_xdp_action(dp->netdev, xdp_prog, act);
+ 				fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  			case XDP_ABORTED:
  				trace_xdp_exception(dp->netdev, xdp_prog, act);
 -				fallthrough;
 +				/* fall through */
  			case XDP_DROP:
  				nfp_net_rx_give_one(dp, rx_ring, rxbuf->frag,
  						    rxbuf->dma_addr);
diff --cc drivers/net/ethernet/socionext/netsec.c
index 63c23f878c12,25dcd8eda5fc..000000000000
--- a/drivers/net/ethernet/socionext/netsec.c
+++ b/drivers/net/ethernet/socionext/netsec.c
@@@ -855,6 -824,315 +855,318 @@@ static void netsec_set_tx_de(struct net
  	dring->head = (dring->head + 1) % DESC_NUM;
  }
  
++<<<<<<< HEAD
++=======
+ /* The current driver only supports 1 Txq, this should run under spin_lock() */
+ static u32 netsec_xdp_queue_one(struct netsec_priv *priv,
+ 				struct xdp_frame *xdpf, bool is_ndo)
+ 
+ {
+ 	struct netsec_desc_ring *tx_ring = &priv->desc_ring[NETSEC_RING_TX];
+ 	struct page *page = virt_to_page(xdpf->data);
+ 	struct netsec_tx_pkt_ctrl tx_ctrl = {};
+ 	struct netsec_desc tx_desc;
+ 	dma_addr_t dma_handle;
+ 	u16 filled;
+ 
+ 	if (tx_ring->head >= tx_ring->tail)
+ 		filled = tx_ring->head - tx_ring->tail;
+ 	else
+ 		filled = tx_ring->head + DESC_NUM - tx_ring->tail;
+ 
+ 	if (DESC_NUM - filled <= 1)
+ 		return NETSEC_XDP_CONSUMED;
+ 
+ 	if (is_ndo) {
+ 		/* this is for ndo_xdp_xmit, the buffer needs mapping before
+ 		 * sending
+ 		 */
+ 		dma_handle = dma_map_single(priv->dev, xdpf->data, xdpf->len,
+ 					    DMA_TO_DEVICE);
+ 		if (dma_mapping_error(priv->dev, dma_handle))
+ 			return NETSEC_XDP_CONSUMED;
+ 		tx_desc.buf_type = TYPE_NETSEC_XDP_NDO;
+ 	} else {
+ 		/* This is the device Rx buffer from page_pool. No need to remap
+ 		 * just sync and send it
+ 		 */
+ 		struct netsec_desc_ring *rx_ring =
+ 			&priv->desc_ring[NETSEC_RING_RX];
+ 		enum dma_data_direction dma_dir =
+ 			page_pool_get_dma_dir(rx_ring->page_pool);
+ 
+ 		dma_handle = page_pool_get_dma_addr(page) + xdpf->headroom +
+ 			sizeof(*xdpf);
+ 		dma_sync_single_for_device(priv->dev, dma_handle, xdpf->len,
+ 					   dma_dir);
+ 		tx_desc.buf_type = TYPE_NETSEC_XDP_TX;
+ 	}
+ 
+ 	tx_desc.dma_addr = dma_handle;
+ 	tx_desc.addr = xdpf->data;
+ 	tx_desc.len = xdpf->len;
+ 
+ 	netdev_sent_queue(priv->ndev, xdpf->len);
+ 	netsec_set_tx_de(priv, tx_ring, &tx_ctrl, &tx_desc, xdpf);
+ 
+ 	return NETSEC_XDP_TX;
+ }
+ 
+ static u32 netsec_xdp_xmit_back(struct netsec_priv *priv, struct xdp_buff *xdp)
+ {
+ 	struct netsec_desc_ring *tx_ring = &priv->desc_ring[NETSEC_RING_TX];
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	u32 ret;
+ 
+ 	if (unlikely(!xdpf))
+ 		return NETSEC_XDP_CONSUMED;
+ 
+ 	spin_lock(&tx_ring->lock);
+ 	ret = netsec_xdp_queue_one(priv, xdpf, false);
+ 	spin_unlock(&tx_ring->lock);
+ 
+ 	return ret;
+ }
+ 
+ static u32 netsec_run_xdp(struct netsec_priv *priv, struct bpf_prog *prog,
+ 			  struct xdp_buff *xdp)
+ {
+ 	struct netsec_desc_ring *dring = &priv->desc_ring[NETSEC_RING_RX];
+ 	unsigned int sync, len = xdp->data_end - xdp->data;
+ 	u32 ret = NETSEC_XDP_PASS;
+ 	struct page *page;
+ 	int err;
+ 	u32 act;
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 
+ 	/* Due xdp_adjust_tail: DMA sync for_device cover max len CPU touch */
+ 	sync = xdp->data_end - xdp->data_hard_start - NETSEC_RXBUF_HEADROOM;
+ 	sync = max(sync, len);
+ 
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		ret = NETSEC_XDP_PASS;
+ 		break;
+ 	case XDP_TX:
+ 		ret = netsec_xdp_xmit_back(priv, xdp);
+ 		if (ret != NETSEC_XDP_TX) {
+ 			page = virt_to_head_page(xdp->data);
+ 			page_pool_put_page(dring->page_pool, page, sync, true);
+ 		}
+ 		break;
+ 	case XDP_REDIRECT:
+ 		err = xdp_do_redirect(priv->ndev, xdp, prog);
+ 		if (!err) {
+ 			ret = NETSEC_XDP_REDIR;
+ 		} else {
+ 			ret = NETSEC_XDP_CONSUMED;
+ 			page = virt_to_head_page(xdp->data);
+ 			page_pool_put_page(dring->page_pool, page, sync, true);
+ 		}
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(priv->ndev, prog, act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(priv->ndev, prog, act);
+ 		fallthrough;	/* handle aborts by dropping packet */
+ 	case XDP_DROP:
+ 		ret = NETSEC_XDP_CONSUMED;
+ 		page = virt_to_head_page(xdp->data);
+ 		page_pool_put_page(dring->page_pool, page, sync, true);
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int netsec_process_rx(struct netsec_priv *priv, int budget)
+ {
+ 	struct netsec_desc_ring *dring = &priv->desc_ring[NETSEC_RING_RX];
+ 	struct net_device *ndev = priv->ndev;
+ 	struct netsec_rx_pkt_info rx_info;
+ 	enum dma_data_direction dma_dir;
+ 	struct bpf_prog *xdp_prog;
+ 	struct xdp_buff xdp;
+ 	u16 xdp_xmit = 0;
+ 	u32 xdp_act = 0;
+ 	int done = 0;
+ 
+ 	xdp_init_buff(&xdp, PAGE_SIZE, &dring->xdp_rxq);
+ 
+ 	xdp_prog = READ_ONCE(priv->xdp_prog);
+ 	dma_dir = page_pool_get_dma_dir(dring->page_pool);
+ 
+ 	while (done < budget) {
+ 		u16 idx = dring->tail;
+ 		struct netsec_de *de = dring->vaddr + (DESC_SZ * idx);
+ 		struct netsec_desc *desc = &dring->desc[idx];
+ 		struct page *page = virt_to_page(desc->addr);
+ 		u32 xdp_result = NETSEC_XDP_PASS;
+ 		struct sk_buff *skb = NULL;
+ 		u16 pkt_len, desc_len;
+ 		dma_addr_t dma_handle;
+ 		void *buf_addr;
+ 
+ 		if (de->attr & (1U << NETSEC_RX_PKT_OWN_FIELD)) {
+ 			/* reading the register clears the irq */
+ 			netsec_read(priv, NETSEC_REG_NRM_RX_PKTCNT);
+ 			break;
+ 		}
+ 
+ 		/* This  barrier is needed to keep us from reading
+ 		 * any other fields out of the netsec_de until we have
+ 		 * verified the descriptor has been written back
+ 		 */
+ 		dma_rmb();
+ 		done++;
+ 
+ 		pkt_len = de->buf_len_info >> 16;
+ 		rx_info.err_code = (de->attr >> NETSEC_RX_PKT_ERR_FIELD) &
+ 			NETSEC_RX_PKT_ERR_MASK;
+ 		rx_info.err_flag = (de->attr >> NETSEC_RX_PKT_ER_FIELD) & 1;
+ 		if (rx_info.err_flag) {
+ 			netif_err(priv, drv, priv->ndev,
+ 				  "%s: rx fail err(%d)\n", __func__,
+ 				  rx_info.err_code);
+ 			ndev->stats.rx_dropped++;
+ 			dring->tail = (dring->tail + 1) % DESC_NUM;
+ 			/* reuse buffer page frag */
+ 			netsec_rx_fill(priv, idx, 1);
+ 			continue;
+ 		}
+ 		rx_info.rx_cksum_result =
+ 			(de->attr >> NETSEC_RX_PKT_CO_FIELD) & 3;
+ 
+ 		/* allocate a fresh buffer and map it to the hardware.
+ 		 * This will eventually replace the old buffer in the hardware
+ 		 */
+ 		buf_addr = netsec_alloc_rx_data(priv, &dma_handle, &desc_len);
+ 
+ 		if (unlikely(!buf_addr))
+ 			break;
+ 
+ 		dma_sync_single_for_cpu(priv->dev, desc->dma_addr, pkt_len,
+ 					dma_dir);
+ 		prefetch(desc->addr);
+ 
+ 		xdp_prepare_buff(&xdp, desc->addr, NETSEC_RXBUF_HEADROOM,
+ 				 pkt_len, false);
+ 
+ 		if (xdp_prog) {
+ 			xdp_result = netsec_run_xdp(priv, xdp_prog, &xdp);
+ 			if (xdp_result != NETSEC_XDP_PASS) {
+ 				xdp_act |= xdp_result;
+ 				if (xdp_result == NETSEC_XDP_TX)
+ 					xdp_xmit++;
+ 				goto next;
+ 			}
+ 		}
+ 		skb = build_skb(desc->addr, desc->len + NETSEC_RX_BUF_NON_DATA);
+ 
+ 		if (unlikely(!skb)) {
+ 			/* If skb fails recycle_direct will either unmap and
+ 			 * free the page or refill the cache depending on the
+ 			 * cache state. Since we paid the allocation cost if
+ 			 * building an skb fails try to put the page into cache
+ 			 */
+ 			page_pool_put_page(dring->page_pool, page, pkt_len,
+ 					   true);
+ 			netif_err(priv, drv, priv->ndev,
+ 				  "rx failed to build skb\n");
+ 			break;
+ 		}
+ 		page_pool_release_page(dring->page_pool, page);
+ 
+ 		skb_reserve(skb, xdp.data - xdp.data_hard_start);
+ 		skb_put(skb, xdp.data_end - xdp.data);
+ 		skb->protocol = eth_type_trans(skb, priv->ndev);
+ 
+ 		if (priv->rx_cksum_offload_flag &&
+ 		    rx_info.rx_cksum_result == NETSEC_RX_CKSUM_OK)
+ 			skb->ip_summed = CHECKSUM_UNNECESSARY;
+ 
+ next:
+ 		if (skb)
+ 			napi_gro_receive(&priv->napi, skb);
+ 		if (skb || xdp_result) {
+ 			ndev->stats.rx_packets++;
+ 			ndev->stats.rx_bytes += xdp.data_end - xdp.data;
+ 		}
+ 
+ 		/* Update the descriptor with fresh buffers */
+ 		desc->len = desc_len;
+ 		desc->dma_addr = dma_handle;
+ 		desc->addr = buf_addr;
+ 
+ 		netsec_rx_fill(priv, idx, 1);
+ 		dring->tail = (dring->tail + 1) % DESC_NUM;
+ 	}
+ 	netsec_finalize_xdp_rx(priv, xdp_act, xdp_xmit);
+ 
+ 	return done;
+ }
+ 
+ static int netsec_napi_poll(struct napi_struct *napi, int budget)
+ {
+ 	struct netsec_priv *priv;
+ 	int done;
+ 
+ 	priv = container_of(napi, struct netsec_priv, napi);
+ 
+ 	netsec_process_tx(priv);
+ 	done = netsec_process_rx(priv, budget);
+ 
+ 	if (done < budget && napi_complete_done(napi, done)) {
+ 		unsigned long flags;
+ 
+ 		spin_lock_irqsave(&priv->reglock, flags);
+ 		netsec_write(priv, NETSEC_REG_INTEN_SET,
+ 			     NETSEC_IRQ_RX | NETSEC_IRQ_TX);
+ 		spin_unlock_irqrestore(&priv->reglock, flags);
+ 	}
+ 
+ 	return done;
+ }
+ 
+ 
+ static int netsec_desc_used(struct netsec_desc_ring *dring)
+ {
+ 	int used;
+ 
+ 	if (dring->head >= dring->tail)
+ 		used = dring->head - dring->tail;
+ 	else
+ 		used = dring->head + DESC_NUM - dring->tail;
+ 
+ 	return used;
+ }
+ 
+ static int netsec_check_stop_tx(struct netsec_priv *priv, int used)
+ {
+ 	struct netsec_desc_ring *dring = &priv->desc_ring[NETSEC_RING_TX];
+ 
+ 	/* keep tail from touching the queue */
+ 	if (DESC_NUM - used < 2) {
+ 		netif_stop_queue(priv->ndev);
+ 
+ 		/* Make sure we read the updated value in case
+ 		 * descriptors got freed
+ 		 */
+ 		smp_rmb();
+ 
+ 		used = netsec_desc_used(dring);
+ 		if (DESC_NUM - used < 2)
+ 			return NETDEV_TX_BUSY;
+ 
+ 		netif_wake_queue(priv->ndev);
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  static netdev_tx_t netsec_netdev_start_xmit(struct sk_buff *skb,
  					    struct net_device *ndev)
  {
diff --cc drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 185a2e3b67eb,7f62ddc90088..000000000000
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@@ -4122,6 -4578,484 +4122,487 @@@ static unsigned int stmmac_rx_buf2_len(
  	return plen - len;
  }
  
++<<<<<<< HEAD
++=======
+ static int stmmac_xdp_xmit_xdpf(struct stmmac_priv *priv, int queue,
+ 				struct xdp_frame *xdpf, bool dma_map)
+ {
+ 	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
+ 	unsigned int entry = tx_q->cur_tx;
+ 	struct dma_desc *tx_desc;
+ 	dma_addr_t dma_addr;
+ 	bool set_ic;
+ 
+ 	if (stmmac_tx_avail(priv, queue) < STMMAC_TX_THRESH(priv))
+ 		return STMMAC_XDP_CONSUMED;
+ 
+ 	if (likely(priv->extend_desc))
+ 		tx_desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+ 	else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+ 		tx_desc = &tx_q->dma_entx[entry].basic;
+ 	else
+ 		tx_desc = tx_q->dma_tx + entry;
+ 
+ 	if (dma_map) {
+ 		dma_addr = dma_map_single(priv->device, xdpf->data,
+ 					  xdpf->len, DMA_TO_DEVICE);
+ 		if (dma_mapping_error(priv->device, dma_addr))
+ 			return STMMAC_XDP_CONSUMED;
+ 
+ 		tx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_XDP_NDO;
+ 	} else {
+ 		struct page *page = virt_to_page(xdpf->data);
+ 
+ 		dma_addr = page_pool_get_dma_addr(page) + sizeof(*xdpf) +
+ 			   xdpf->headroom;
+ 		dma_sync_single_for_device(priv->device, dma_addr,
+ 					   xdpf->len, DMA_BIDIRECTIONAL);
+ 
+ 		tx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_XDP_TX;
+ 	}
+ 
+ 	tx_q->tx_skbuff_dma[entry].buf = dma_addr;
+ 	tx_q->tx_skbuff_dma[entry].map_as_page = false;
+ 	tx_q->tx_skbuff_dma[entry].len = xdpf->len;
+ 	tx_q->tx_skbuff_dma[entry].last_segment = true;
+ 	tx_q->tx_skbuff_dma[entry].is_jumbo = false;
+ 
+ 	tx_q->xdpf[entry] = xdpf;
+ 
+ 	stmmac_set_desc_addr(priv, tx_desc, dma_addr);
+ 
+ 	stmmac_prepare_tx_desc(priv, tx_desc, 1, xdpf->len,
+ 			       true, priv->mode, true, true,
+ 			       xdpf->len);
+ 
+ 	tx_q->tx_count_frames++;
+ 
+ 	if (tx_q->tx_count_frames % priv->tx_coal_frames[queue] == 0)
+ 		set_ic = true;
+ 	else
+ 		set_ic = false;
+ 
+ 	if (set_ic) {
+ 		tx_q->tx_count_frames = 0;
+ 		stmmac_set_tx_ic(priv, tx_desc);
+ 		priv->xstats.tx_set_ic_bit++;
+ 	}
+ 
+ 	stmmac_enable_dma_transmission(priv, priv->ioaddr);
+ 
+ 	entry = STMMAC_GET_ENTRY(entry, priv->dma_tx_size);
+ 	tx_q->cur_tx = entry;
+ 
+ 	return STMMAC_XDP_TX;
+ }
+ 
+ static int stmmac_xdp_get_tx_queue(struct stmmac_priv *priv,
+ 				   int cpu)
+ {
+ 	int index = cpu;
+ 
+ 	if (unlikely(index < 0))
+ 		index = 0;
+ 
+ 	while (index >= priv->plat->tx_queues_to_use)
+ 		index -= priv->plat->tx_queues_to_use;
+ 
+ 	return index;
+ }
+ 
+ static int stmmac_xdp_xmit_back(struct stmmac_priv *priv,
+ 				struct xdp_buff *xdp)
+ {
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	int queue;
+ 	int res;
+ 
+ 	if (unlikely(!xdpf))
+ 		return STMMAC_XDP_CONSUMED;
+ 
+ 	queue = stmmac_xdp_get_tx_queue(priv, cpu);
+ 	nq = netdev_get_tx_queue(priv->dev, queue);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	/* Avoids TX time-out as we are sharing with slow path */
+ 	txq_trans_cond_update(nq);
+ 
+ 	res = stmmac_xdp_xmit_xdpf(priv, queue, xdpf, false);
+ 	if (res == STMMAC_XDP_TX)
+ 		stmmac_flush_tx_descriptors(priv, queue);
+ 
+ 	__netif_tx_unlock(nq);
+ 
+ 	return res;
+ }
+ 
+ static int __stmmac_xdp_run_prog(struct stmmac_priv *priv,
+ 				 struct bpf_prog *prog,
+ 				 struct xdp_buff *xdp)
+ {
+ 	u32 act;
+ 	int res;
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		res = STMMAC_XDP_PASS;
+ 		break;
+ 	case XDP_TX:
+ 		res = stmmac_xdp_xmit_back(priv, xdp);
+ 		break;
+ 	case XDP_REDIRECT:
+ 		if (xdp_do_redirect(priv->dev, xdp, prog) < 0)
+ 			res = STMMAC_XDP_CONSUMED;
+ 		else
+ 			res = STMMAC_XDP_REDIRECT;
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(priv->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(priv->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		res = STMMAC_XDP_CONSUMED;
+ 		break;
+ 	}
+ 
+ 	return res;
+ }
+ 
+ static struct sk_buff *stmmac_xdp_run_prog(struct stmmac_priv *priv,
+ 					   struct xdp_buff *xdp)
+ {
+ 	struct bpf_prog *prog;
+ 	int res;
+ 
+ 	prog = READ_ONCE(priv->xdp_prog);
+ 	if (!prog) {
+ 		res = STMMAC_XDP_PASS;
+ 		goto out;
+ 	}
+ 
+ 	res = __stmmac_xdp_run_prog(priv, prog, xdp);
+ out:
+ 	return ERR_PTR(-res);
+ }
+ 
+ static void stmmac_finalize_xdp_rx(struct stmmac_priv *priv,
+ 				   int xdp_status)
+ {
+ 	int cpu = smp_processor_id();
+ 	int queue;
+ 
+ 	queue = stmmac_xdp_get_tx_queue(priv, cpu);
+ 
+ 	if (xdp_status & STMMAC_XDP_TX)
+ 		stmmac_tx_timer_arm(priv, queue);
+ 
+ 	if (xdp_status & STMMAC_XDP_REDIRECT)
+ 		xdp_do_flush();
+ }
+ 
+ static struct sk_buff *stmmac_construct_skb_zc(struct stmmac_channel *ch,
+ 					       struct xdp_buff *xdp)
+ {
+ 	unsigned int metasize = xdp->data - xdp->data_meta;
+ 	unsigned int datasize = xdp->data_end - xdp->data;
+ 	struct sk_buff *skb;
+ 
+ 	skb = __napi_alloc_skb(&ch->rxtx_napi,
+ 			       xdp->data_end - xdp->data_hard_start,
+ 			       GFP_ATOMIC | __GFP_NOWARN);
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
+ 	skb_reserve(skb, xdp->data - xdp->data_hard_start);
+ 	memcpy(__skb_put(skb, datasize), xdp->data, datasize);
+ 	if (metasize)
+ 		skb_metadata_set(skb, metasize);
+ 
+ 	return skb;
+ }
+ 
+ static void stmmac_dispatch_skb_zc(struct stmmac_priv *priv, u32 queue,
+ 				   struct dma_desc *p, struct dma_desc *np,
+ 				   struct xdp_buff *xdp)
+ {
+ 	struct stmmac_channel *ch = &priv->channel[queue];
+ 	unsigned int len = xdp->data_end - xdp->data;
+ 	enum pkt_hash_types hash_type;
+ 	int coe = priv->hw->rx_csum;
+ 	struct sk_buff *skb;
+ 	u32 hash;
+ 
+ 	skb = stmmac_construct_skb_zc(ch, xdp);
+ 	if (!skb) {
+ 		priv->dev->stats.rx_dropped++;
+ 		return;
+ 	}
+ 
+ 	stmmac_get_rx_hwtstamp(priv, p, np, skb);
+ 	stmmac_rx_vlan(priv->dev, skb);
+ 	skb->protocol = eth_type_trans(skb, priv->dev);
+ 
+ 	if (unlikely(!coe))
+ 		skb_checksum_none_assert(skb);
+ 	else
+ 		skb->ip_summed = CHECKSUM_UNNECESSARY;
+ 
+ 	if (!stmmac_get_rx_hash(priv, p, &hash, &hash_type))
+ 		skb_set_hash(skb, hash, hash_type);
+ 
+ 	skb_record_rx_queue(skb, queue);
+ 	napi_gro_receive(&ch->rxtx_napi, skb);
+ 
+ 	priv->dev->stats.rx_packets++;
+ 	priv->dev->stats.rx_bytes += len;
+ }
+ 
+ static bool stmmac_rx_refill_zc(struct stmmac_priv *priv, u32 queue, u32 budget)
+ {
+ 	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+ 	unsigned int entry = rx_q->dirty_rx;
+ 	struct dma_desc *rx_desc = NULL;
+ 	bool ret = true;
+ 
+ 	budget = min(budget, stmmac_rx_dirty(priv, queue));
+ 
+ 	while (budget-- > 0 && entry != rx_q->cur_rx) {
+ 		struct stmmac_rx_buffer *buf = &rx_q->buf_pool[entry];
+ 		dma_addr_t dma_addr;
+ 		bool use_rx_wd;
+ 
+ 		if (!buf->xdp) {
+ 			buf->xdp = xsk_buff_alloc(rx_q->xsk_pool);
+ 			if (!buf->xdp) {
+ 				ret = false;
+ 				break;
+ 			}
+ 		}
+ 
+ 		if (priv->extend_desc)
+ 			rx_desc = (struct dma_desc *)(rx_q->dma_erx + entry);
+ 		else
+ 			rx_desc = rx_q->dma_rx + entry;
+ 
+ 		dma_addr = xsk_buff_xdp_get_dma(buf->xdp);
+ 		stmmac_set_desc_addr(priv, rx_desc, dma_addr);
+ 		stmmac_set_desc_sec_addr(priv, rx_desc, 0, false);
+ 		stmmac_refill_desc3(priv, rx_q, rx_desc);
+ 
+ 		rx_q->rx_count_frames++;
+ 		rx_q->rx_count_frames += priv->rx_coal_frames[queue];
+ 		if (rx_q->rx_count_frames > priv->rx_coal_frames[queue])
+ 			rx_q->rx_count_frames = 0;
+ 
+ 		use_rx_wd = !priv->rx_coal_frames[queue];
+ 		use_rx_wd |= rx_q->rx_count_frames > 0;
+ 		if (!priv->use_riwt)
+ 			use_rx_wd = false;
+ 
+ 		dma_wmb();
+ 		stmmac_set_rx_owner(priv, rx_desc, use_rx_wd);
+ 
+ 		entry = STMMAC_GET_ENTRY(entry, priv->dma_rx_size);
+ 	}
+ 
+ 	if (rx_desc) {
+ 		rx_q->dirty_rx = entry;
+ 		rx_q->rx_tail_addr = rx_q->dma_rx_phy +
+ 				     (rx_q->dirty_rx * sizeof(struct dma_desc));
+ 		stmmac_set_rx_tail_ptr(priv, priv->ioaddr, rx_q->rx_tail_addr, queue);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int stmmac_rx_zc(struct stmmac_priv *priv, int limit, u32 queue)
+ {
+ 	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+ 	unsigned int count = 0, error = 0, len = 0;
+ 	int dirty = stmmac_rx_dirty(priv, queue);
+ 	unsigned int next_entry = rx_q->cur_rx;
+ 	unsigned int desc_size;
+ 	struct bpf_prog *prog;
+ 	bool failure = false;
+ 	int xdp_status = 0;
+ 	int status = 0;
+ 
+ 	if (netif_msg_rx_status(priv)) {
+ 		void *rx_head;
+ 
+ 		netdev_dbg(priv->dev, "%s: descriptor ring:\n", __func__);
+ 		if (priv->extend_desc) {
+ 			rx_head = (void *)rx_q->dma_erx;
+ 			desc_size = sizeof(struct dma_extended_desc);
+ 		} else {
+ 			rx_head = (void *)rx_q->dma_rx;
+ 			desc_size = sizeof(struct dma_desc);
+ 		}
+ 
+ 		stmmac_display_ring(priv, rx_head, priv->dma_rx_size, true,
+ 				    rx_q->dma_rx_phy, desc_size);
+ 	}
+ 	while (count < limit) {
+ 		struct stmmac_rx_buffer *buf;
+ 		unsigned int buf1_len = 0;
+ 		struct dma_desc *np, *p;
+ 		int entry;
+ 		int res;
+ 
+ 		if (!count && rx_q->state_saved) {
+ 			error = rx_q->state.error;
+ 			len = rx_q->state.len;
+ 		} else {
+ 			rx_q->state_saved = false;
+ 			error = 0;
+ 			len = 0;
+ 		}
+ 
+ 		if (count >= limit)
+ 			break;
+ 
+ read_again:
+ 		buf1_len = 0;
+ 		entry = next_entry;
+ 		buf = &rx_q->buf_pool[entry];
+ 
+ 		if (dirty >= STMMAC_RX_FILL_BATCH) {
+ 			failure = failure ||
+ 				  !stmmac_rx_refill_zc(priv, queue, dirty);
+ 			dirty = 0;
+ 		}
+ 
+ 		if (priv->extend_desc)
+ 			p = (struct dma_desc *)(rx_q->dma_erx + entry);
+ 		else
+ 			p = rx_q->dma_rx + entry;
+ 
+ 		/* read the status of the incoming frame */
+ 		status = stmmac_rx_status(priv, &priv->dev->stats,
+ 					  &priv->xstats, p);
+ 		/* check if managed by the DMA otherwise go ahead */
+ 		if (unlikely(status & dma_own))
+ 			break;
+ 
+ 		/* Prefetch the next RX descriptor */
+ 		rx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx,
+ 						priv->dma_rx_size);
+ 		next_entry = rx_q->cur_rx;
+ 
+ 		if (priv->extend_desc)
+ 			np = (struct dma_desc *)(rx_q->dma_erx + next_entry);
+ 		else
+ 			np = rx_q->dma_rx + next_entry;
+ 
+ 		prefetch(np);
+ 
+ 		/* Ensure a valid XSK buffer before proceed */
+ 		if (!buf->xdp)
+ 			break;
+ 
+ 		if (priv->extend_desc)
+ 			stmmac_rx_extended_status(priv, &priv->dev->stats,
+ 						  &priv->xstats,
+ 						  rx_q->dma_erx + entry);
+ 		if (unlikely(status == discard_frame)) {
+ 			xsk_buff_free(buf->xdp);
+ 			buf->xdp = NULL;
+ 			dirty++;
+ 			error = 1;
+ 			if (!priv->hwts_rx_en)
+ 				priv->dev->stats.rx_errors++;
+ 		}
+ 
+ 		if (unlikely(error && (status & rx_not_ls)))
+ 			goto read_again;
+ 		if (unlikely(error)) {
+ 			count++;
+ 			continue;
+ 		}
+ 
+ 		/* XSK pool expects RX frame 1:1 mapped to XSK buffer */
+ 		if (likely(status & rx_not_ls)) {
+ 			xsk_buff_free(buf->xdp);
+ 			buf->xdp = NULL;
+ 			dirty++;
+ 			count++;
+ 			goto read_again;
+ 		}
+ 
+ 		/* XDP ZC Frame only support primary buffers for now */
+ 		buf1_len = stmmac_rx_buf1_len(priv, p, status, len);
+ 		len += buf1_len;
+ 
+ 		/* ACS is set; GMAC core strips PAD/FCS for IEEE 802.3
+ 		 * Type frames (LLC/LLC-SNAP)
+ 		 *
+ 		 * llc_snap is never checked in GMAC >= 4, so this ACS
+ 		 * feature is always disabled and packets need to be
+ 		 * stripped manually.
+ 		 */
+ 		if (likely(!(status & rx_not_ls)) &&
+ 		    (likely(priv->synopsys_id >= DWMAC_CORE_4_00) ||
+ 		     unlikely(status != llc_snap))) {
+ 			buf1_len -= ETH_FCS_LEN;
+ 			len -= ETH_FCS_LEN;
+ 		}
+ 
+ 		/* RX buffer is good and fit into a XSK pool buffer */
+ 		buf->xdp->data_end = buf->xdp->data + buf1_len;
+ 		xsk_buff_dma_sync_for_cpu(buf->xdp, rx_q->xsk_pool);
+ 
+ 		prog = READ_ONCE(priv->xdp_prog);
+ 		res = __stmmac_xdp_run_prog(priv, prog, buf->xdp);
+ 
+ 		switch (res) {
+ 		case STMMAC_XDP_PASS:
+ 			stmmac_dispatch_skb_zc(priv, queue, p, np, buf->xdp);
+ 			xsk_buff_free(buf->xdp);
+ 			break;
+ 		case STMMAC_XDP_CONSUMED:
+ 			xsk_buff_free(buf->xdp);
+ 			priv->dev->stats.rx_dropped++;
+ 			break;
+ 		case STMMAC_XDP_TX:
+ 		case STMMAC_XDP_REDIRECT:
+ 			xdp_status |= res;
+ 			break;
+ 		}
+ 
+ 		buf->xdp = NULL;
+ 		dirty++;
+ 		count++;
+ 	}
+ 
+ 	if (status & rx_not_ls) {
+ 		rx_q->state_saved = true;
+ 		rx_q->state.error = error;
+ 		rx_q->state.len = len;
+ 	}
+ 
+ 	stmmac_finalize_xdp_rx(priv, xdp_status);
+ 
+ 	priv->xstats.rx_pkt_n += count;
+ 	priv->xstats.rxq_stats[queue].rx_pkt_n += count;
+ 
+ 	if (xsk_uses_need_wakeup(rx_q->xsk_pool)) {
+ 		if (failure || stmmac_rx_dirty(priv, queue) > 0)
+ 			xsk_set_rx_need_wakeup(rx_q->xsk_pool);
+ 		else
+ 			xsk_clear_rx_need_wakeup(rx_q->xsk_pool);
+ 
+ 		return (int)count;
+ 	}
+ 
+ 	return failure ? limit : (int)count;
+ }
+ 
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  /**
   * stmmac_rx - manage the receive process
   * @priv: driver private structure
diff --cc drivers/net/tun.c
index 2fcb65f7e868,0e5d2272f63a..000000000000
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@@ -1663,13 -1551,13 +1663,18 @@@ static int tun_xdp_act(struct tun_struc
  	case XDP_PASS:
  		break;
  	default:
++<<<<<<< HEAD
 +		bpf_warn_invalid_xdp_action(act);
 +		/* fall through */
++=======
+ 		bpf_warn_invalid_xdp_action(tun->dev, xdp_prog, act);
+ 		fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  	case XDP_ABORTED:
  		trace_xdp_exception(tun->dev, xdp_prog, act);
 -		fallthrough;
 +		/* fall through */
  	case XDP_DROP:
 -		atomic_long_inc(&tun->dev->rx_dropped);
 +		this_cpu_inc(tun->pcpu_stats->rx_dropped);
  		break;
  	}
  
diff --cc drivers/net/veth.c
index 4d2628499b94,700cc9374f9c..000000000000
--- a/drivers/net/veth.c
+++ b/drivers/net/veth.c
@@@ -671,11 -644,11 +671,16 @@@ static struct xdp_frame *veth_xdp_rcv_o
  			rcu_read_unlock();
  			goto xdp_xmit;
  		default:
++<<<<<<< HEAD
 +			bpf_warn_invalid_xdp_action(act);
 +			/* fall through */
++=======
+ 			bpf_warn_invalid_xdp_action(rq->dev, xdp_prog, act);
+ 			fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  		case XDP_ABORTED:
  			trace_xdp_exception(rq->dev, xdp_prog, act);
 -			fallthrough;
 +			/* fall through */
  		case XDP_DROP:
  			stats->xdp_drops++;
  			goto err_xdp;
@@@ -821,11 -794,11 +826,16 @@@ static struct sk_buff *veth_xdp_rcv_skb
  		rcu_read_unlock();
  		goto xdp_xmit;
  	default:
++<<<<<<< HEAD
 +		bpf_warn_invalid_xdp_action(act);
 +		/* fall through */
++=======
+ 		bpf_warn_invalid_xdp_action(rq->dev, xdp_prog, act);
+ 		fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  	case XDP_ABORTED:
  		trace_xdp_exception(rq->dev, xdp_prog, act);
 -		fallthrough;
 +		/* fall through */
  	case XDP_DROP:
  		stats->xdp_drops++;
  		goto xdp_drop;
diff --cc drivers/net/virtio_net.c
index 286124211143,d3e0b3a533cd..000000000000
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@@ -735,9 -812,11 +735,14 @@@ static struct sk_buff *receive_small(st
  			rcu_read_unlock();
  			goto xdp_xmit;
  		default:
++<<<<<<< HEAD
 +			bpf_warn_invalid_xdp_action(act);
++=======
+ 			bpf_warn_invalid_xdp_action(vi->dev, xdp_prog, act);
+ 			fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  		case XDP_ABORTED:
  			trace_xdp_exception(vi->dev, xdp_prog, act);
 -			goto err_xdp;
  		case XDP_DROP:
  			goto err_xdp;
  		}
@@@ -931,9 -1025,11 +936,14 @@@ static struct sk_buff *receive_mergeabl
  			rcu_read_unlock();
  			goto xdp_xmit;
  		default:
++<<<<<<< HEAD
 +			bpf_warn_invalid_xdp_action(act);
++=======
+ 			bpf_warn_invalid_xdp_action(vi->dev, xdp_prog, act);
+ 			fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  		case XDP_ABORTED:
  			trace_xdp_exception(vi->dev, xdp_prog, act);
 -			fallthrough;
  		case XDP_DROP:
  			if (unlikely(xdp_page != page))
  				__free_pages(xdp_page, 0);
diff --cc drivers/net/xen-netfront.c
index c9ad8431e87a,7b7eb617051a..000000000000
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@@ -781,20 -889,71 +781,70 @@@ static int xennet_get_extras(struct net
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static u32 xennet_run_xdp(struct netfront_queue *queue, struct page *pdata,
+ 		   struct xen_netif_rx_response *rx, struct bpf_prog *prog,
+ 		   struct xdp_buff *xdp, bool *need_xdp_flush)
+ {
+ 	struct xdp_frame *xdpf;
+ 	u32 len = rx->status;
+ 	u32 act;
+ 	int err;
+ 
+ 	xdp_init_buff(xdp, XEN_PAGE_SIZE - XDP_PACKET_HEADROOM,
+ 		      &queue->xdp_rxq);
+ 	xdp_prepare_buff(xdp, page_address(pdata), XDP_PACKET_HEADROOM,
+ 			 len, false);
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 	switch (act) {
+ 	case XDP_TX:
+ 		get_page(pdata);
+ 		xdpf = xdp_convert_buff_to_frame(xdp);
+ 		err = xennet_xdp_xmit(queue->info->netdev, 1, &xdpf, 0);
+ 		if (unlikely(!err))
+ 			xdp_return_frame_rx_napi(xdpf);
+ 		else if (unlikely(err < 0))
+ 			trace_xdp_exception(queue->info->netdev, prog, act);
+ 		break;
+ 	case XDP_REDIRECT:
+ 		get_page(pdata);
+ 		err = xdp_do_redirect(queue->info->netdev, xdp, prog);
+ 		*need_xdp_flush = true;
+ 		if (unlikely(err))
+ 			trace_xdp_exception(queue->info->netdev, prog, act);
+ 		break;
+ 	case XDP_PASS:
+ 	case XDP_DROP:
+ 		break;
+ 
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(queue->info->netdev, prog, act);
+ 		break;
+ 
+ 	default:
+ 		bpf_warn_invalid_xdp_action(queue->info->netdev, prog, act);
+ 	}
+ 
+ 	return act;
+ }
+ 
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  static int xennet_get_responses(struct netfront_queue *queue,
  				struct netfront_rx_info *rinfo, RING_IDX rp,
 -				struct sk_buff_head *list,
 -				bool *need_xdp_flush)
 +				struct sk_buff_head *list)
  {
 -	struct xen_netif_rx_response *rx = &rinfo->rx, rx_local;
 -	int max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);
 +	struct xen_netif_rx_response *rx = &rinfo->rx;
 +	struct xen_netif_extra_info *extras = rinfo->extras;
 +	struct device *dev = &queue->info->netdev->dev;
  	RING_IDX cons = queue->rx.rsp_cons;
  	struct sk_buff *skb = xennet_get_rx_skb(queue, cons);
 -	struct xen_netif_extra_info *extras = rinfo->extras;
  	grant_ref_t ref = xennet_get_rx_ref(queue, cons);
 -	struct device *dev = &queue->info->netdev->dev;
 -	struct bpf_prog *xdp_prog;
 -	struct xdp_buff xdp;
 -	unsigned long ret;
 +	int max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);
  	int slots = 1;
  	int err = 0;
 -	u32 verdict;
 +	unsigned long ret;
  
  	if (rx->flags & XEN_NETRXF_extra_info) {
  		err = xennet_get_extras(queue, extras, rp);
diff --cc kernel/bpf/cpumap.c
index cabfd8f613a6,0421061d95f1..000000000000
--- a/kernel/bpf/cpumap.c
+++ b/kernel/bpf/cpumap.c
@@@ -168,6 -169,46 +168,49 @@@ static void put_cpu_map_entry(struct bp
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void cpu_map_bpf_prog_run_skb(struct bpf_cpu_map_entry *rcpu,
+ 				     struct list_head *listp,
+ 				     struct xdp_cpumap_stats *stats)
+ {
+ 	struct sk_buff *skb, *tmp;
+ 	struct xdp_buff xdp;
+ 	u32 act;
+ 	int err;
+ 
+ 	list_for_each_entry_safe(skb, tmp, listp, list) {
+ 		act = bpf_prog_run_generic_xdp(skb, &xdp, rcpu->prog);
+ 		switch (act) {
+ 		case XDP_PASS:
+ 			break;
+ 		case XDP_REDIRECT:
+ 			skb_list_del_init(skb);
+ 			err = xdp_do_generic_redirect(skb->dev, skb, &xdp,
+ 						      rcpu->prog);
+ 			if (unlikely(err)) {
+ 				kfree_skb(skb);
+ 				stats->drop++;
+ 			} else {
+ 				stats->redirect++;
+ 			}
+ 			return;
+ 		default:
+ 			bpf_warn_invalid_xdp_action(NULL, rcpu->prog, act);
+ 			fallthrough;
+ 		case XDP_ABORTED:
+ 			trace_xdp_exception(skb->dev, rcpu->prog, act);
+ 			fallthrough;
+ 		case XDP_DROP:
+ 			skb_list_del_init(skb);
+ 			kfree_skb(skb);
+ 			stats->drop++;
+ 			return;
+ 		}
+ 	}
+ }
+ 
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  static int cpu_map_bpf_prog_run_xdp(struct bpf_cpu_map_entry *rcpu,
  				    void **frames, int n,
  				    struct xdp_cpumap_stats *stats)
@@@ -218,8 -254,8 +261,13 @@@
  			}
  			break;
  		default:
++<<<<<<< HEAD
 +			bpf_warn_invalid_xdp_action(act);
 +			/* fallthrough */
++=======
+ 			bpf_warn_invalid_xdp_action(NULL, rcpu->prog, act);
+ 			fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  		case XDP_DROP:
  			xdp_return_frame(xdpf);
  			stats->drop++;
diff --cc kernel/bpf/devmap.c
index ef730460e324,6feea293ff10..000000000000
--- a/kernel/bpf/devmap.c
+++ b/kernel/bpf/devmap.c
@@@ -324,14 -322,43 +324,46 @@@ static int dev_map_hash_get_next_key(st
  	return -ENOENT;
  }
  
 -static int dev_map_bpf_prog_run(struct bpf_prog *xdp_prog,
 -				struct xdp_frame **frames, int n,
 -				struct net_device *dev)
 +bool dev_map_can_have_prog(struct bpf_map *map)
  {
 -	struct xdp_txq_info txq = { .dev = dev };
 -	struct xdp_buff xdp;
 -	int i, nframes = 0;
 +	if ((map->map_type == BPF_MAP_TYPE_DEVMAP ||
 +	     map->map_type == BPF_MAP_TYPE_DEVMAP_HASH) &&
 +	    map->value_size != offsetofend(struct bpf_devmap_val, ifindex))
 +		return true;
  
++<<<<<<< HEAD
 +	return false;
++=======
+ 	for (i = 0; i < n; i++) {
+ 		struct xdp_frame *xdpf = frames[i];
+ 		u32 act;
+ 		int err;
+ 
+ 		xdp_convert_frame_to_buff(xdpf, &xdp);
+ 		xdp.txq = &txq;
+ 
+ 		act = bpf_prog_run_xdp(xdp_prog, &xdp);
+ 		switch (act) {
+ 		case XDP_PASS:
+ 			err = xdp_update_frame_from_buff(&xdp, xdpf);
+ 			if (unlikely(err < 0))
+ 				xdp_return_frame_rx_napi(xdpf);
+ 			else
+ 				frames[nframes++] = xdpf;
+ 			break;
+ 		default:
+ 			bpf_warn_invalid_xdp_action(NULL, xdp_prog, act);
+ 			fallthrough;
+ 		case XDP_ABORTED:
+ 			trace_xdp_exception(dev, xdp_prog, act);
+ 			fallthrough;
+ 		case XDP_DROP:
+ 			xdp_return_frame_rx_napi(xdpf);
+ 			break;
+ 		}
+ 	}
+ 	return nframes; /* sent frames count */
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  }
  
  static void bq_xmit_all(struct xdp_dev_bulk_queue *bq, u32 flags)
@@@ -451,27 -489,31 +483,32 @@@ static inline int __xdp_enqueue(struct 
  	return 0;
  }
  
 -static u32 dev_map_bpf_prog_run_skb(struct sk_buff *skb, struct bpf_dtab_netdev *dst)
 +static struct xdp_buff *dev_map_run_prog(struct net_device *dev,
 +					 struct xdp_buff *xdp,
 +					 struct bpf_prog *xdp_prog)
  {
 -	struct xdp_txq_info txq = { .dev = dst->dev };
 -	struct xdp_buff xdp;
 +	struct xdp_txq_info txq = { .dev = dev };
  	u32 act;
  
 -	if (!dst->xdp_prog)
 -		return XDP_PASS;
 -
 -	__skb_pull(skb, skb->mac_len);
 -	xdp.txq = &txq;
 +	xdp_set_data_meta_invalid(xdp);
 +	xdp->txq = &txq;
  
 -	act = bpf_prog_run_generic_xdp(skb, &xdp, dst->xdp_prog);
 +	act = bpf_prog_run_xdp(xdp_prog, xdp);
  	switch (act) {
  	case XDP_PASS:
 -		__skb_push(skb, skb->mac_len);
 +		return xdp;
 +	case XDP_DROP:
  		break;
  	default:
++<<<<<<< HEAD
 +		bpf_warn_invalid_xdp_action(act);
 +		/* fallthrough */
++=======
+ 		bpf_warn_invalid_xdp_action(NULL, dst->xdp_prog, act);
+ 		fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  	case XDP_ABORTED:
 -		trace_xdp_exception(dst->dev, dst->xdp_prog, act);
 -		fallthrough;
 -	case XDP_DROP:
 -		kfree_skb(skb);
 +		trace_xdp_exception(dev, xdp_prog, act);
  		break;
  	}
  
diff --cc net/core/dev.c
index b713e3394123,c431c8925eed..000000000000
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@@ -4743,12 -4664,55 +4743,17 @@@ static u32 netif_receive_generic_xdp(st
  		if (metalen)
  			skb_metadata_set(skb, metalen);
  		break;
 -	}
 -
 -	return act;
 -}
 -
 -static u32 netif_receive_generic_xdp(struct sk_buff *skb,
 -				     struct xdp_buff *xdp,
 -				     struct bpf_prog *xdp_prog)
 -{
 -	u32 act = XDP_DROP;
 -
 -	/* Reinjected packets coming from act_mirred or similar should
 -	 * not get XDP generic processing.
 -	 */
 -	if (skb_is_redirected(skb))
 -		return XDP_PASS;
 -
 -	/* XDP packets must be linear and must have sufficient headroom
 -	 * of XDP_PACKET_HEADROOM bytes. This is the guarantee that also
 -	 * native XDP provides, thus we need to do it here as well.
 -	 */
 -	if (skb_cloned(skb) || skb_is_nonlinear(skb) ||
 -	    skb_headroom(skb) < XDP_PACKET_HEADROOM) {
 -		int hroom = XDP_PACKET_HEADROOM - skb_headroom(skb);
 -		int troom = skb->tail + skb->data_len - skb->end;
 -
 -		/* In case we have to go down the path and also linearize,
 -		 * then lets do the pskb_expand_head() work just once here.
 -		 */
 -		if (pskb_expand_head(skb,
 -				     hroom > 0 ? ALIGN(hroom, NET_SKB_PAD) : 0,
 -				     troom > 0 ? troom + 128 : 0, GFP_ATOMIC))
 -			goto do_drop;
 -		if (skb_linearize(skb))
 -			goto do_drop;
 -	}
 -
 -	act = bpf_prog_run_generic_xdp(skb, xdp, xdp_prog);
 -	switch (act) {
 -	case XDP_REDIRECT:
 -	case XDP_TX:
 -	case XDP_PASS:
 -		break;
  	default:
++<<<<<<< HEAD
 +		bpf_warn_invalid_xdp_action(act);
 +		/* fall through */
++=======
+ 		bpf_warn_invalid_xdp_action(skb->dev, xdp_prog, act);
+ 		fallthrough;
++>>>>>>> c8064e5b4ada (bpf: Let bpf_warn_invalid_xdp_action() report more info)
  	case XDP_ABORTED:
  		trace_xdp_exception(skb->dev, xdp_prog, act);
 -		fallthrough;
 +		/* fall through */
  	case XDP_DROP:
  	do_drop:
  		kfree_skb(skb);
* Unmerged path drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c
* Unmerged path drivers/net/ethernet/freescale/enetc/enetc.c
* Unmerged path drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
* Unmerged path drivers/net/ethernet/ti/cpsw_priv.c
diff --git a/drivers/net/ethernet/amazon/ena/ena_netdev.c b/drivers/net/ethernet/amazon/ena/ena_netdev.c
index 7a0887d0bfb3..7da8d11745f8 100644
--- a/drivers/net/ethernet/amazon/ena/ena_netdev.c
+++ b/drivers/net/ethernet/amazon/ena/ena_netdev.c
@@ -436,7 +436,7 @@ static int ena_xdp_execute(struct ena_ring *rx_ring, struct xdp_buff *xdp)
 		xdp_stat = &rx_ring->rx_stats.xdp_pass;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(verdict);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, verdict);
 		xdp_stat = &rx_ring->rx_stats.xdp_invalid;
 	}
 
* Unmerged path drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c
* Unmerged path drivers/net/ethernet/cavium/thunder/nicvf_main.c
* Unmerged path drivers/net/ethernet/freescale/dpaa/dpaa_eth.c
* Unmerged path drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c
* Unmerged path drivers/net/ethernet/freescale/enetc/enetc.c
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 596ff706992a..d26d578e2e3a 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -2322,7 +2322,7 @@ static int i40e_run_xdp(struct i40e_ring *rx_ring, struct xdp_buff *xdp)
 		result = I40E_XDP_REDIR;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/i40e/i40e_xsk.c b/drivers/net/ethernet/intel/i40e/i40e_xsk.c
index e7e778ca074c..3ab683c3084d 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_xsk.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_xsk.c
@@ -176,7 +176,7 @@ static int i40e_run_xdp_zc(struct i40e_ring *rx_ring, struct xdp_buff *xdp)
 			goto out_failure;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.c b/drivers/net/ethernet/intel/ice/ice_txrx.c
index 6c91de196a81..f4eed48352b8 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.c
@@ -576,7 +576,7 @@ ice_run_xdp(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp,
 			goto out_failure;
 		return ICE_XDP_REDIR;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/ice/ice_xsk.c b/drivers/net/ethernet/intel/ice/ice_xsk.c
index 490f8341668d..6577b9edf207 100644
--- a/drivers/net/ethernet/intel/ice/ice_xsk.c
+++ b/drivers/net/ethernet/intel/ice/ice_xsk.c
@@ -491,7 +491,7 @@ ice_run_xdp_zc(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp,
 			goto out_failure;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index 6faea2c3f4c9..f7b66ed69df7 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -8431,7 +8431,7 @@ static struct sk_buff *igb_run_xdp(struct igb_adapter *adapter,
 		result = IGB_XDP_REDIR;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(adapter->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/igc/igc_main.c b/drivers/net/ethernet/intel/igc/igc_main.c
index b12a935d0255..567bb0b651bb 100644
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@ -2236,7 +2236,7 @@ static int __igc_xdp_run_prog(struct igc_adapter *adapter,
 		return IGC_XDP_REDIRECT;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(adapter->netdev, prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
index fbca60e07375..0f2da036a51a 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c
@@ -2227,7 +2227,7 @@ static struct sk_buff *ixgbe_run_xdp(struct ixgbe_adapter *adapter,
 		result = IXGBE_XDP_REDIR;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
index b1d22e4d5ec9..70ed8b60b9a4 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
@@ -125,7 +125,7 @@ static int ixgbe_run_xdp_zc(struct ixgbe_adapter *adapter,
 			goto out_failure;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
diff --git a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
index 58207ffbbe6c..a8525db93996 100644
--- a/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
+++ b/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c
@@ -1073,7 +1073,7 @@ static struct sk_buff *ixgbevf_run_xdp(struct ixgbevf_adapter *adapter,
 			goto out_failure;
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 out_failure:
* Unmerged path drivers/net/ethernet/marvell/mvneta.c
* Unmerged path drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c
* Unmerged path drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c
diff --git a/drivers/net/ethernet/mellanox/mlx4/en_rx.c b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
index 03a54c87e25e..1bfb7ee95b85 100644
--- a/drivers/net/ethernet/mellanox/mlx4/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_rx.c
@@ -801,7 +801,7 @@ int mlx4_en_process_rx_cq(struct net_device *dev, struct mlx4_en_cq *cq, int bud
 				trace_xdp_exception(dev, xdp_prog, act);
 				goto xdp_drop_no_cnt; /* Drop on xmit failure */
 			default:
-				bpf_warn_invalid_xdp_action(act);
+				bpf_warn_invalid_xdp_action(dev, xdp_prog, act);
 				fallthrough;
 			case XDP_ABORTED:
 				trace_xdp_exception(dev, xdp_prog, act);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
index 15f2e3e50c28..4594bf1e83dd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
@@ -151,7 +151,7 @@ bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 		rq->stats->xdp_redirect++;
 		return true;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(rq->netdev, prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 xdp_abort:
diff --git a/drivers/net/ethernet/microsoft/mana/mana_bpf.c b/drivers/net/ethernet/microsoft/mana/mana_bpf.c
index 1bc8ff388341..1d2f948b5c00 100644
--- a/drivers/net/ethernet/microsoft/mana/mana_bpf.c
+++ b/drivers/net/ethernet/microsoft/mana/mana_bpf.c
@@ -60,7 +60,7 @@ u32 mana_run_xdp(struct net_device *ndev, struct mana_rxq *rxq,
 		break;
 
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(ndev, prog, act);
 	}
 
 out:
* Unmerged path drivers/net/ethernet/netronome/nfp/nfp_net_common.c
diff --git a/drivers/net/ethernet/qlogic/qede/qede_fp.c b/drivers/net/ethernet/qlogic/qede/qede_fp.c
index 8bc8578ade4d..bd882f8bf349 100644
--- a/drivers/net/ethernet/qlogic/qede/qede_fp.c
+++ b/drivers/net/ethernet/qlogic/qede/qede_fp.c
@@ -1152,7 +1152,7 @@ static bool qede_rx_xdp(struct qede_dev *edev,
 		qede_rx_bd_ring_consume(rxq);
 		break;
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(edev->ndev, prog, act);
 		fallthrough;
 	case XDP_ABORTED:
 		trace_xdp_exception(edev->ndev, prog, act);
diff --git a/drivers/net/ethernet/sfc/rx.c b/drivers/net/ethernet/sfc/rx.c
index 606750938b89..2375cef577e4 100644
--- a/drivers/net/ethernet/sfc/rx.c
+++ b/drivers/net/ethernet/sfc/rx.c
@@ -338,7 +338,7 @@ static bool efx_do_xdp(struct efx_nic *efx, struct efx_channel *channel,
 		break;
 
 	default:
-		bpf_warn_invalid_xdp_action(xdp_act);
+		bpf_warn_invalid_xdp_action(efx->net_dev, xdp_prog, xdp_act);
 		efx_free_rx_buffers(rx_queue, rx_buf, 1);
 		channel->n_rx_xdp_bad_drops++;
 		trace_xdp_exception(efx->net_dev, xdp_prog, xdp_act);
* Unmerged path drivers/net/ethernet/socionext/netsec.c
* Unmerged path drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
* Unmerged path drivers/net/ethernet/ti/cpsw_priv.c
diff --git a/drivers/net/hyperv/netvsc_bpf.c b/drivers/net/hyperv/netvsc_bpf.c
index aa877da113f8..7856905414eb 100644
--- a/drivers/net/hyperv/netvsc_bpf.c
+++ b/drivers/net/hyperv/netvsc_bpf.c
@@ -68,7 +68,7 @@ u32 netvsc_run_xdp(struct net_device *ndev, struct netvsc_channel *nvchan,
 		break;
 
 	default:
-		bpf_warn_invalid_xdp_action(act);
+		bpf_warn_invalid_xdp_action(ndev, prog, act);
 	}
 
 out:
* Unmerged path drivers/net/tun.c
* Unmerged path drivers/net/veth.c
* Unmerged path drivers/net/virtio_net.c
* Unmerged path drivers/net/xen-netfront.c
diff --git a/include/linux/filter.h b/include/linux/filter.h
index 526eb16adc71..7c305af916cf 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1014,7 +1014,7 @@ void xdp_do_flush(void);
  */
 #define xdp_do_flush_map xdp_do_flush
 
-void bpf_warn_invalid_xdp_action(u32 act);
+void bpf_warn_invalid_xdp_action(struct net_device *dev, struct bpf_prog *prog, u32 act);
 
 #ifdef CONFIG_INET
 struct sock *bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,
* Unmerged path kernel/bpf/cpumap.c
* Unmerged path kernel/bpf/devmap.c
* Unmerged path net/core/dev.c
diff --git a/net/core/filter.c b/net/core/filter.c
index c3f475c9065d..181866aa90df 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -8048,13 +8048,13 @@ static bool xdp_is_valid_access(int off, int size,
 	return __is_valid_xdp_access(off, size);
 }
 
-void bpf_warn_invalid_xdp_action(u32 act)
+void bpf_warn_invalid_xdp_action(struct net_device *dev, struct bpf_prog *prog, u32 act)
 {
 	const u32 act_max = XDP_REDIRECT;
 
-	pr_warn_once("%s XDP return value %u, expect packet loss!\n",
+	pr_warn_once("%s XDP return value %u on prog %s (id %d) dev %s, expect packet loss!\n",
 		     act > act_max ? "Illegal" : "Driver unsupported",
-		     act);
+		     act, prog->aux->name, prog->aux->id, dev ? dev->name : "N/A");
 }
 EXPORT_SYMBOL_GPL(bpf_warn_invalid_xdp_action);
 
