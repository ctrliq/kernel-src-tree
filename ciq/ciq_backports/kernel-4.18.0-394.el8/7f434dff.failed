sched/topology: Remove redundant variable and fix incorrect type in build_sched_domains

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author K Prateek Nayak <kprateek.nayak@amd.com>
commit 7f434dff76215af00c26ba6449eaa4738fe9e2ab
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/7f434dff.failed

While investigating the sparse warning reported by the LKP bot [1],
observed that we have a redundant variable "top" in the function
build_sched_domains that was introduced in the recent commit
e496132ebedd ("sched/fair: Adjust the allowed NUMA imbalance when
SD_NUMA spans multiple LLCs")

The existing variable "sd" suffices which allows us to remove the
redundant variable "top" while annotating the other variable "top_p"
with the "__rcu" annotation to silence the sparse warning.

[1] https://lore.kernel.org/lkml/202202170853.9vofgC3O-lkp@intel.com/

Fixes: e496132ebedd ("sched/fair: Adjust the allowed NUMA imbalance when SD_NUMA spans multiple LLCs")
	Reported-by: kernel test robot <lkp@intel.com>
	Signed-off-by: K Prateek Nayak <kprateek.nayak@amd.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
Link: https://lore.kernel.org/r/20220218162743.1134-1-kprateek.nayak@amd.com
(cherry picked from commit 7f434dff76215af00c26ba6449eaa4738fe9e2ab)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/topology.c
diff --cc kernel/sched/topology.c
index 6f4a7245aea7,43f2899fe0ef..000000000000
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@@ -2138,6 -2278,57 +2138,60 @@@ build_sched_domains(const struct cpumas
  		}
  	}
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Calculate an allowed NUMA imbalance such that LLCs do not get
+ 	 * imbalanced.
+ 	 */
+ 	for_each_cpu(i, cpu_map) {
+ 		unsigned int imb = 0;
+ 		unsigned int imb_span = 1;
+ 
+ 		for (sd = *per_cpu_ptr(d.sd, i); sd; sd = sd->parent) {
+ 			struct sched_domain *child = sd->child;
+ 
+ 			if (!(sd->flags & SD_SHARE_PKG_RESOURCES) && child &&
+ 			    (child->flags & SD_SHARE_PKG_RESOURCES)) {
+ 				struct sched_domain __rcu *top_p;
+ 				unsigned int nr_llcs;
+ 
+ 				/*
+ 				 * For a single LLC per node, allow an
+ 				 * imbalance up to 25% of the node. This is an
+ 				 * arbitrary cutoff based on SMT-2 to balance
+ 				 * between memory bandwidth and avoiding
+ 				 * premature sharing of HT resources and SMT-4
+ 				 * or SMT-8 *may* benefit from a different
+ 				 * cutoff.
+ 				 *
+ 				 * For multiple LLCs, allow an imbalance
+ 				 * until multiple tasks would share an LLC
+ 				 * on one node while LLCs on another node
+ 				 * remain idle.
+ 				 */
+ 				nr_llcs = sd->span_weight / child->span_weight;
+ 				if (nr_llcs == 1)
+ 					imb = sd->span_weight >> 2;
+ 				else
+ 					imb = nr_llcs;
+ 				sd->imb_numa_nr = imb;
+ 
+ 				/* Set span based on the first NUMA domain. */
+ 				top_p = sd->parent;
+ 				while (top_p && !(top_p->flags & SD_NUMA)) {
+ 					top_p = top_p->parent;
+ 				}
+ 				imb_span = top_p ? top_p->span_weight : sd->span_weight;
+ 			} else {
+ 				int factor = max(1U, (sd->span_weight / imb_span));
+ 
+ 				sd->imb_numa_nr = imb * factor;
+ 			}
+ 		}
+ 	}
+ 
++>>>>>>> 7f434dff7621 (sched/topology: Remove redundant variable and fix incorrect type in build_sched_domains)
  	/* Calculate CPU capacity for physical packages and nodes */
  	for (i = nr_cpumask_bits-1; i >= 0; i--) {
  		if (!cpumask_test_cpu(i, cpu_map))
* Unmerged path kernel/sched/topology.c
