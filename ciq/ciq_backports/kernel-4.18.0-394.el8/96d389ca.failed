arm64: Add workaround for Arm Cortex-A77 erratum 1508412

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Rob Herring <robh@kernel.org>
commit 96d389ca10110d7eefb46feb6af9a0c6832f78f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/96d389ca.failed

On Cortex-A77 r0p0 and r1p0, a sequence of a non-cacheable or device load
and a store exclusive or PAR_EL1 read can cause a deadlock.

The workaround requires a DMB SY before and after a PAR_EL1 register
read. In addition, it's possible an interrupt (doing a device read) or
KVM guest exit could be taken between the DMB and PAR read, so we
also need a DMB before returning from interrupt and before returning to
a guest.

A deadlock is still possible with the workaround as KVM guests must also
have the workaround. IOW, a malicious guest can deadlock an affected
systems.

This workaround also depends on a firmware counterpart to enable the h/w
to insert DMB SY after load and store exclusive instructions. See the
errata document SDEN-1152370 v10 [1] for more information.

[1] https://static.docs.arm.com/101992/0010/Arm_Cortex_A77_MP074_Software_Developer_Errata_Notice_v10.pdf

	Signed-off-by: Rob Herring <robh@kernel.org>
	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <maz@kernel.org>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: James Morse <james.morse@arm.com>
	Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
	Cc: Will Deacon <will@kernel.org>
	Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
	Cc: kvmarm@lists.cs.columbia.edu
Link: https://lore.kernel.org/r/20201028182839.166037-2-robh@kernel.org
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 96d389ca10110d7eefb46feb6af9a0c6832f78f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/arm64/silicon-errata.rst
#	arch/arm64/include/asm/cpucaps.h
#	arch/arm64/include/asm/sysreg.h
#	arch/arm64/kernel/cpu_errata.c
#	arch/arm64/kvm/hyp/nvhe/switch.c
diff --cc Documentation/arm64/silicon-errata.rst
index 646de9dd5b5d,719510247292..000000000000
--- a/Documentation/arm64/silicon-errata.rst
+++ b/Documentation/arm64/silicon-errata.rst
@@@ -85,9 -90,7 +85,13 @@@ stable kernels
  +----------------+-----------------+-----------------+-----------------------------+
  | ARM            | Cortex-A76      | #1463225        | ARM64_ERRATUM_1463225       |
  +----------------+-----------------+-----------------+-----------------------------+
++<<<<<<< HEAD
 +| ARM            | Cortex-A55      | #1530923        | ARM64_ERRATUM_1530923       |
 ++----------------+-----------------+-----------------+-----------------------------+
 +| ARM            | Neoverse-N1     | #1349291        | N/A                         |
++=======
+ | ARM            | Cortex-A77      | #1508412        | ARM64_ERRATUM_1508412       |
++>>>>>>> 96d389ca1011 (arm64: Add workaround for Arm Cortex-A77 erratum 1508412)
  +----------------+-----------------+-----------------+-----------------------------+
  | ARM            | Neoverse-N1     | #1188873,1418040| ARM64_ERRATUM_1418040       |
  +----------------+-----------------+-----------------+-----------------------------+
diff --cc arch/arm64/include/asm/cpucaps.h
index 7115f7a85078,e7d98997c09c..000000000000
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@@ -61,18 -50,23 +61,39 @@@
  #define ARM64_HAS_GENERIC_AUTH_ARCH		40
  #define ARM64_HAS_GENERIC_AUTH_IMP_DEF		41
  #define ARM64_HAS_IRQ_PRIO_MASKING		42
++<<<<<<< HEAD
 +#define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	43
 +#define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	44
 +#define ARM64_WORKAROUND_1463225		45
 +#define ARM64_WORKAROUND_1542419		46
 +#define ARM64_HAS_32BIT_EL1			47
 +#define ARM64_WORKAROUND_NVIDIA_CARMEL_CNP	48
 +#define ARM64_HAS_ARMv8_4_TTL			49
 +#define ARM64_HAS_TLB_RANGE			50
 +#define ARM64_HAS_DCPODP			51
 +#define ARM64_HAS_E0PD				52
 +#define ARM64_HAS_RNG				53
 +
 +#define ARM64_NCAPS				54
++=======
+ #define ARM64_HAS_DCPODP			43
+ #define ARM64_WORKAROUND_1463225		44
+ #define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	45
+ #define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	46
+ #define ARM64_WORKAROUND_1542419		47
+ #define ARM64_HAS_E0PD				48
+ #define ARM64_HAS_RNG				49
+ #define ARM64_HAS_AMU_EXTN			50
+ #define ARM64_HAS_ADDRESS_AUTH			51
+ #define ARM64_HAS_GENERIC_AUTH			52
+ #define ARM64_HAS_32BIT_EL1			53
+ #define ARM64_BTI				54
+ #define ARM64_HAS_ARMv8_4_TTL			55
+ #define ARM64_HAS_TLB_RANGE			56
+ #define ARM64_MTE				57
+ #define ARM64_WORKAROUND_1508412		58
+ 
+ #define ARM64_NCAPS				59
++>>>>>>> 96d389ca1011 (arm64: Add workaround for Arm Cortex-A77 erratum 1508412)
  
  #endif /* __ASM_CPUCAPS_H */
diff --cc arch/arm64/include/asm/sysreg.h
index 7b11f3be556e,174817ba119c..000000000000
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@@ -1156,6 -1089,21 +1157,24 @@@
  		write_sysreg(__scs_new, sysreg);			\
  } while (0)
  
++<<<<<<< HEAD
++=======
+ #define sysreg_clear_set_s(sysreg, clear, set) do {			\
+ 	u64 __scs_val = read_sysreg_s(sysreg);				\
+ 	u64 __scs_new = (__scs_val & ~(u64)(clear)) | (set);		\
+ 	if (__scs_new != __scs_val)					\
+ 		write_sysreg_s(__scs_new, sysreg);			\
+ } while (0)
+ 
+ #define read_sysreg_par() ({						\
+ 	u64 par;							\
+ 	asm(ALTERNATIVE("nop", "dmb sy", ARM64_WORKAROUND_1508412));	\
+ 	par = read_sysreg(par_el1);					\
+ 	asm(ALTERNATIVE("nop", "dmb sy", ARM64_WORKAROUND_1508412));	\
+ 	par;								\
+ })
+ 
++>>>>>>> 96d389ca1011 (arm64: Add workaround for Arm Cortex-A77 erratum 1508412)
  #endif
  
  #endif	/* __ASM_SYSREG_H */
diff --cc arch/arm64/kernel/cpu_errata.c
index 550bbacfeb31,61314fd70f13..000000000000
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@@ -950,12 -523,14 +950,23 @@@ const struct arm64_cpu_capabilities arm
  		.cpu_enable = cpu_enable_trap_ctr_access,
  	},
  #endif
++<<<<<<< HEAD
 +#ifdef CONFIG_NVIDIA_CARMEL_CNP_ERRATUM
 +	{
 +		/* NVIDIA Carmel */
 +		.desc = "NVIDIA Carmel CNP erratum",
 +		.capability = ARM64_WORKAROUND_NVIDIA_CARMEL_CNP,
 +		ERRATA_MIDR_ALL_VERSIONS(MIDR_NVIDIA_CARMEL),
++=======
+ #ifdef CONFIG_ARM64_ERRATUM_1508412
+ 	{
+ 		/* we depend on the firmware portion for correctness */
+ 		.desc = "ARM erratum 1508412 (kernel portion)",
+ 		.capability = ARM64_WORKAROUND_1508412,
+ 		ERRATA_MIDR_RANGE(MIDR_CORTEX_A77,
+ 				  0, 0,
+ 				  1, 0),
++>>>>>>> 96d389ca1011 (arm64: Add workaround for Arm Cortex-A77 erratum 1508412)
  	},
  #endif
  	{
diff --cc arch/arm64/kvm/hyp/nvhe/switch.c
index 34af17ec3932,8ae8160bc93a..000000000000
--- a/arch/arm64/kvm/hyp/nvhe/switch.c
+++ b/arch/arm64/kvm/hyp/nvhe/switch.c
@@@ -255,14 -250,18 +255,21 @@@ void __noreturn hyp_panic(struct kvm_cp
  {
  	u64 spsr = read_sysreg_el2(SYS_SPSR);
  	u64 elr = read_sysreg_el2(SYS_ELR);
++<<<<<<< HEAD
 +	u64 par = read_sysreg(par_el1);
 +	struct kvm_vcpu *vcpu = host_ctxt->__hyp_running_vcpu;
 +	unsigned long str_va;
++=======
+ 	u64 par = read_sysreg_par();
+ 	bool restore_host = true;
+ 	struct kvm_cpu_context *host_ctxt;
+ 	struct kvm_vcpu *vcpu;
++>>>>>>> 96d389ca1011 (arm64: Add workaround for Arm Cortex-A77 erratum 1508412)
  
 -	host_ctxt = &this_cpu_ptr(&kvm_host_data)->host_ctxt;
 -	vcpu = host_ctxt->__hyp_running_vcpu;
 -
 -	if (vcpu) {
 +	if (read_sysreg(vttbr_el2)) {
  		__timer_disable_traps(vcpu);
  		__deactivate_traps(vcpu);
 -		__load_host_stage2();
 +		__deactivate_vm(vcpu);
  		__sysreg_restore_state_nvhe(host_ctxt);
  	}
  
* Unmerged path Documentation/arm64/silicon-errata.rst
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 2416b2d4600e..86f6f9335b6c 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -625,6 +625,26 @@ config ARM64_ERRATUM_1542419
 
 	  If unsure, say Y.
 
+config ARM64_ERRATUM_1508412
+	bool "Cortex-A77: 1508412: workaround deadlock on sequence of NC/Device load and store exclusive or PAR read"
+	default y
+	help
+	  This option adds a workaround for Arm Cortex-A77 erratum 1508412.
+
+	  Affected Cortex-A77 cores (r0p0, r1p0) could deadlock on a sequence
+	  of a store-exclusive or read of PAR_EL1 and a load with device or
+	  non-cacheable memory attributes. The workaround depends on a firmware
+	  counterpart.
+
+	  KVM guests must also have the workaround implemented or they can
+	  deadlock the system.
+
+	  Work around the issue by inserting DMB SY barriers around PAR_EL1
+	  register reads and warning KVM users. The DMB barrier is sufficient
+	  to prevent a speculative PAR_EL1 read.
+
+	  If unsure, say Y.
+
 config CAVIUM_ERRATUM_22375
 	bool "Cavium erratum 22375, 24313"
 	default y
* Unmerged path arch/arm64/include/asm/cpucaps.h
* Unmerged path arch/arm64/include/asm/sysreg.h
* Unmerged path arch/arm64/kernel/cpu_errata.c
diff --git a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
index 0227e1cf5b0b..72a16f405c63 100644
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@ -393,6 +393,9 @@ alternative_insn eret, nop, ARM64_UNMAP_KERNEL_AT_EL0
 	br	x30
 #endif
 	.else
+	/* Ensure any device/NC reads complete */
+	alternative_insn nop, "dmb sy", ARM64_WORKAROUND_1508412
+
 	eret
 	.endif
 	sb
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index fd339b9ff6ea..f64f396edee2 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -1686,7 +1686,8 @@ int kvm_arch_init(void *opaque)
 		return -ENODEV;
 	}
 
-	if (cpus_have_final_cap(ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE))
+	if (cpus_have_final_cap(ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE) ||
+	    cpus_have_final_cap(ARM64_WORKAROUND_1508412))
 		kvm_info("Guests without required CPU erratum workarounds can deadlock system!\n" \
 			 "Only trusted guests should be used on this system.\n");
 
diff --git a/arch/arm64/kvm/hyp/include/hyp/switch.h b/arch/arm64/kvm/hyp/include/hyp/switch.h
index f56599d5be74..926ae0040f42 100644
--- a/arch/arm64/kvm/hyp/include/hyp/switch.h
+++ b/arch/arm64/kvm/hyp/include/hyp/switch.h
@@ -145,9 +145,9 @@ static inline bool __translate_far_to_hpfar(u64 far, u64 *hpfar)
 	 * We do need to save/restore PAR_EL1 though, as we haven't
 	 * saved the guest context yet, and we may return early...
 	 */
-	par = read_sysreg(par_el1);
+	par = read_sysreg_par();
 	if (!__kvm_at("s1e1r", far))
-		tmp = read_sysreg(par_el1);
+		tmp = read_sysreg_par();
 	else
 		tmp = SYS_PAR_EL1_F; /* back to the guest */
 	write_sysreg(par, par_el1);
@@ -424,7 +424,7 @@ static inline bool fixup_guest_exit(struct kvm_vcpu *vcpu, u64 *exit_code)
 	if (cpus_have_final_cap(ARM64_WORKAROUND_CAVIUM_TX2_219_TVM) &&
 	    kvm_vcpu_trap_get_class(vcpu) == ESR_ELx_EC_SYS64 &&
 	    handle_tx2_tvm(vcpu))
-		return true;
+		goto guest;
 
 	/*
 	 * We trap the first access to the FP/SIMD to save the host context
@@ -434,13 +434,13 @@ static inline bool fixup_guest_exit(struct kvm_vcpu *vcpu, u64 *exit_code)
 	 * Similarly for trapped SVE accesses.
 	 */
 	if (__hyp_handle_fpsimd(vcpu))
-		return true;
+		goto guest;
 
 	if (__hyp_handle_ptrauth(vcpu))
-		return true;
+		goto guest;
 
 	if (!__populate_fault_info(vcpu))
-		return true;
+		goto guest;
 
 	if (static_branch_unlikely(&vgic_v2_cpuif_trap)) {
 		bool valid;
@@ -455,7 +455,7 @@ static inline bool fixup_guest_exit(struct kvm_vcpu *vcpu, u64 *exit_code)
 			int ret = __vgic_v2_perform_cpuif_access(vcpu);
 
 			if (ret == 1)
-				return true;
+				goto guest;
 
 			/* Promote an illegal access to an SError.*/
 			if (ret == -1)
@@ -471,12 +471,17 @@ static inline bool fixup_guest_exit(struct kvm_vcpu *vcpu, u64 *exit_code)
 		int ret = __vgic_v3_perform_cpuif_access(vcpu);
 
 		if (ret == 1)
-			return true;
+			goto guest;
 	}
 
 exit:
 	/* Return to the host kernel and handle the exit */
 	return false;
+
+guest:
+	/* Re-enter the guest */
+	asm(ALTERNATIVE("nop", "dmb sy", ARM64_WORKAROUND_1508412));
+	return true;
 }
 
 static inline bool __needs_ssbd_off(struct kvm_vcpu *vcpu)
diff --git a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
index 418fcda09c82..67c3acd3bfed 100644
--- a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
+++ b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
@@ -54,7 +54,7 @@ static inline void __sysreg_save_el1_state(struct kvm_cpu_context *ctxt)
 	ctxt_sys_reg(ctxt, CONTEXTIDR_EL1) = read_sysreg_el1(SYS_CONTEXTIDR);
 	ctxt_sys_reg(ctxt, AMAIR_EL1)	= read_sysreg_el1(SYS_AMAIR);
 	ctxt_sys_reg(ctxt, CNTKCTL_EL1)	= read_sysreg_el1(SYS_CNTKCTL);
-	ctxt_sys_reg(ctxt, PAR_EL1)	= read_sysreg(par_el1);
+	ctxt_sys_reg(ctxt, PAR_EL1)	= read_sysreg_par();
 	ctxt_sys_reg(ctxt, TPIDR_EL1)	= read_sysreg(tpidr_el1);
 
 	ctxt_sys_reg(ctxt, SP_EL1)	= read_sysreg(sp_el1);
* Unmerged path arch/arm64/kvm/hyp/nvhe/switch.c
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 24e3726655a5..049655fcf582 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -212,7 +212,7 @@ void __noreturn hyp_panic(struct kvm_cpu_context *host_ctxt)
 {
 	u64 spsr = read_sysreg_el2(SYS_SPSR);
 	u64 elr = read_sysreg_el2(SYS_ELR);
-	u64 par = read_sysreg(par_el1);
+	u64 par = read_sysreg_par();
 
 	__hyp_call_panic(spsr, elr, par, host_ctxt);
 	unreachable();
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 0e16c0a416a6..b349ed430c86 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -106,7 +106,7 @@ static bool __vcpu_read_sys_reg_from_cpu(int reg, u64 *val)
 	case AMAIR_EL1:		*val = read_sysreg_s(SYS_AMAIR_EL12);	break;
 	case CNTKCTL_EL1:	*val = read_sysreg_s(SYS_CNTKCTL_EL12);	break;
 	case ELR_EL1:		*val = read_sysreg_s(SYS_ELR_EL12);	break;
-	case PAR_EL1:		*val = read_sysreg_s(SYS_PAR_EL1);	break;
+	case PAR_EL1:		*val = read_sysreg_par();		break;
 	case DACR32_EL2:	*val = read_sysreg_s(SYS_DACR32_EL2);	break;
 	case IFSR32_EL2:	*val = read_sysreg_s(SYS_IFSR32_EL2);	break;
 	case DBGVCR32_EL2:	*val = read_sysreg_s(SYS_DBGVCR32_EL2);	break;
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6d6083635932..1ea142b42196 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -298,7 +298,7 @@ static bool __kprobes is_spurious_el1_translation_fault(unsigned long addr,
 	local_irq_save(flags);
 	asm volatile("at s1e1r, %0" :: "r" (addr));
 	isb();
-	par = read_sysreg(par_el1);
+	par = read_sysreg_par();
 	local_irq_restore(flags);
 
 	/*
