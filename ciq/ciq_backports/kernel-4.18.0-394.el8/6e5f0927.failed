arm64: Remove Spectre-related CONFIG_* options

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Will Deacon <will@kernel.org>
commit 6e5f0927846adf39aebee450f13871e3cb4ab012
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/6e5f0927.failed

The spectre mitigations are too configurable for their own good, leading
to confusing logic trying to figure out when we should mitigate and when
we shouldn't. Although the plethora of command-line options need to stick
around for backwards compatibility, the default-on CONFIG options that
depend on EXPERT can be dropped, as the mitigations only do anything if
the system is vulnerable, a mitigation is available and the command-line
hasn't disabled it.

Remove CONFIG_HARDEN_BRANCH_PREDICTOR and CONFIG_ARM64_SSBD in favour of
enabling this code unconditionally.

	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 6e5f0927846adf39aebee450f13871e3cb4ab012)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/Kconfig
#	arch/arm64/kernel/entry.S
#	arch/arm64/kvm/Kconfig
diff --cc arch/arm64/Kconfig
index 2416b2d4600e,51259274a819..000000000000
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@@ -1142,61 -1165,105 +1142,64 @@@ config UNMAP_KERNEL_AT_EL
  
  	  If unsure, say Y.
  
 -config RODATA_FULL_DEFAULT_ENABLED
 -	bool "Apply r/o permissions of VM areas also to their linear aliases"
++<<<<<<< HEAD
 +config HARDEN_BRANCH_PREDICTOR
 +	bool "Harden the branch predictor against aliasing attacks" if EXPERT
  	default y
  	help
 -	  Apply read-only attributes of VM areas to the linear alias of
 -	  the backing pages as well. This prevents code or read-only data
 -	  from being modified (inadvertently or intentionally) via another
 -	  mapping of the same memory page. This additional enhancement can
 -	  be turned off at runtime by passing rodata=[off|on] (and turned on
 -	  with rodata=full if this option is set to 'n')
 +	  Speculation attacks against some high-performance processors rely on
 +	  being able to manipulate the branch predictor for a victim context by
 +	  executing aliasing branches in the attacker context.  Such attacks
 +	  can be partially mitigated against by clearing internal branch
 +	  predictor state and limiting the prediction logic in some situations.
  
 -	  This requires the linear region to be mapped down to pages,
 -	  which may adversely affect performance in some cases.
 +	  This config option will take CPU-specific actions to harden the
 +	  branch predictor against aliasing attacks and may rely on specific
 +	  instruction sequences or control bits being set by the system
 +	  firmware.
  
 -config ARM64_SW_TTBR0_PAN
 -	bool "Emulate Privileged Access Never using TTBR0_EL1 switching"
 -	help
 -	  Enabling this option prevents the kernel from accessing
 -	  user-space memory directly by pointing TTBR0_EL1 to a reserved
 -	  zeroed area and reserved ASID. The user access routines
 -	  restore the valid TTBR0_EL1 temporarily.
 +	  If unsure, say Y.
  
 -config ARM64_TAGGED_ADDR_ABI
 -	bool "Enable the tagged user addresses syscall ABI"
 +config HARDEN_EL2_VECTORS
 +	bool "Harden EL2 vector mapping against system register leak" if EXPERT
  	default y
  	help
 -	  When this option is enabled, user applications can opt in to a
 -	  relaxed ABI via prctl() allowing tagged addresses to be passed
 -	  to system calls as pointer arguments. For details, see
 -	  Documentation/arm64/tagged-address-abi.rst.
 -
 -menuconfig COMPAT
 -	bool "Kernel support for 32-bit EL0"
 -	depends on ARM64_4K_PAGES || EXPERT
 -	select COMPAT_BINFMT_ELF if BINFMT_ELF
 -	select HAVE_UID16
 -	select OLD_SIGSUSPEND3
 -	select COMPAT_OLD_SIGACTION
 -	help
 -	  This option enables support for a 32-bit EL0 running under a 64-bit
 -	  kernel at EL1. AArch32-specific components such as system calls,
 -	  the user helper functions, VFP support and the ptrace interface are
 -	  handled appropriately by the kernel.
 -
 -	  If you use a page size other than 4KB (i.e, 16KB or 64KB), please be aware
 -	  that you will only be able to execute AArch32 binaries that were compiled
 -	  with page size aligned segments.
 +	  Speculation attacks against some high-performance processors can
 +	  be used to leak privileged information such as the vector base
 +	  register, resulting in a potential defeat of the EL2 layout
 +	  randomization.
  
 -	  If you want to execute 32-bit userspace applications, say Y.
 +	  This config option will map the vectors to a fixed location,
 +	  independent of the EL2 code mapping, so that revealing VBAR_EL2
 +	  to an attacker does not give away any extra information. This
 +	  only gets enabled on affected CPUs.
  
 -if COMPAT
 +	  If unsure, say Y.
  
 -config KUSER_HELPERS
 -	bool "Enable kuser helpers page for 32-bit applications"
 +config ARM64_SSBD
 +	bool "Speculative Store Bypass Disable" if EXPERT
  	default y
  	help
 -	  Warning: disabling this option may break 32-bit user programs.
 -
 -	  Provide kuser helpers to compat tasks. The kernel provides
 -	  helper code to userspace in read only form at a fixed location
 -	  to allow userspace to be independent of the CPU type fitted to
 -	  the system. This permits binaries to be run on ARMv4 through
 -	  to ARMv8 without modification.
 +	  This enables mitigation of the bypassing of previous stores
 +	  by speculative loads.
  
 -	  See Documentation/arm/kernel_user_helpers.rst for details.
 -
 -	  However, the fixed address nature of these helpers can be used
 -	  by ROP (return orientated programming) authors when creating
 -	  exploits.
 -
 -	  If all of the binaries and libraries which run on your platform
 -	  are built specifically for your platform, and make no use of
 -	  these helpers, then you can turn this option off to hinder
 -	  such exploits. However, in that case, if a binary or library
 -	  relying on those helpers is run, it will not function correctly.
 -
 -	  Say N here only if you are absolutely certain that you do not
 -	  need these helpers; otherwise, the safe option is to say Y.
 +	  If unsure, say Y.
  
 -config COMPAT_VDSO
 -	bool "Enable vDSO for 32-bit applications"
 -	depends on !CPU_BIG_ENDIAN && "$(CROSS_COMPILE_COMPAT)" != ""
 -	select GENERIC_COMPAT_VDSO
++=======
++>>>>>>> 6e5f0927846a (arm64: Remove Spectre-related CONFIG_* options)
 +config RODATA_FULL_DEFAULT_ENABLED
 +	bool "Apply r/o permissions of VM areas also to their linear aliases"
  	default y
  	help
 -	  Place in the process address space of 32-bit applications an
 -	  ELF shared object providing fast implementations of gettimeofday
 -	  and clock_gettime.
 -
 -	  You must have a 32-bit build of glibc 2.22 or later for programs
 -	  to seamlessly take advantage of this.
 +	  Apply read-only attributes of VM areas to the linear alias of
 +	  the backing pages as well. This prevents code or read-only data
 +	  from being modified (inadvertently or intentionally) via another
 +	  mapping of the same memory page. This additional enhancement can
 +	  be turned off at runtime by passing rodata=[off|on] (and turned on
 +	  with rodata=full if this option is set to 'n')
  
 -config THUMB2_COMPAT_VDSO
 -	bool "Compile the 32-bit vDSO for Thumb-2 mode" if EXPERT
 -	depends on COMPAT_VDSO
 -	default y
 -	help
 -	  Compile the compat vDSO with '-mthumb -fomit-frame-pointer' if y,
 -	  otherwise with '-marm'.
 +	  This requires the linear region to be mapped down to pages,
 +	  which may adversely affect performance in some cases.
  
  menuconfig ARMV8_DEPRECATED
  	bool "Emulate deprecated/obsolete ARMv8 instructions"
diff --cc arch/arm64/kernel/entry.S
index 0227e1cf5b0b,81b709349d7b..000000000000
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@@ -135,23 -127,24 +135,35 @@@ alternative_else_nop_endi
  	add	\dst, \dst, #(\sym - .entry.tramp.text)
  	.endm
  
++<<<<<<< HEAD
 +	// This macro corrupts x0-x3. It is the caller's duty
 +	// to save/restore them if required.
 +	.macro	apply_ssbd, state, targ, tmp1, tmp2
 +#ifdef CONFIG_ARM64_SSBD
++=======
+ 	/*
+ 	 * This macro corrupts x0-x3. It is the caller's duty  to save/restore
+ 	 * them if required.
+ 	 */
+ 	.macro	apply_ssbd, state, tmp1, tmp2
++>>>>>>> 6e5f0927846a (arm64: Remove Spectre-related CONFIG_* options)
  alternative_cb	arm64_enable_wa2_handling
 -	b	.L__asm_ssbd_skip\@
 +	b	\targ
  alternative_cb_end
  	ldr_this_cpu	\tmp2, arm64_ssbd_callback_required, \tmp1
 -	cbz	\tmp2,	.L__asm_ssbd_skip\@
 +	cbz	\tmp2, \targ
  	ldr	\tmp2, [tsk, #TSK_TI_FLAGS]
 -	tbnz	\tmp2, #TIF_SSBD, .L__asm_ssbd_skip\@
 +	tbnz	\tmp2, #TIF_SSBD, \targ
  	mov	w0, #ARM_SMCCC_ARCH_WORKAROUND_2
  	mov	w1, #\state
  alternative_cb	arm64_update_smccc_conduit
  	nop					// Patched to SMC/HVC #0
  alternative_cb_end
++<<<<<<< HEAD
 +#endif
++=======
+ .L__asm_ssbd_skip\@:
++>>>>>>> 6e5f0927846a (arm64: Remove Spectre-related CONFIG_* options)
  	.endm
  
  	.macro	kernel_entry, el, regsize = 64
diff --cc arch/arm64/kvm/Kconfig
index f1c1f981482c,42e5895763b3..000000000000
--- a/arch/arm64/kvm/Kconfig
+++ b/arch/arm64/kvm/Kconfig
@@@ -58,7 -58,7 +58,11 @@@ config KVM_ARM_PM
  	  virtual machines.
  
  config KVM_INDIRECT_VECTORS
++<<<<<<< HEAD
 +	def_bool HARDEN_BRANCH_PREDICTOR || HARDEN_EL2_VECTORS
++=======
+ 	def_bool RANDOMIZE_BASE
++>>>>>>> 6e5f0927846a (arm64: Remove Spectre-related CONFIG_* options)
  
  endif # KVM
  
* Unmerged path arch/arm64/Kconfig
diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 5af39db67149..9f13bbe95cdc 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -703,12 +703,8 @@ int get_spectre_v2_workaround_state(void);
 
 static inline int arm64_get_ssbd_state(void)
 {
-#ifdef CONFIG_ARM64_SSBD
 	extern int ssbd_state;
 	return ssbd_state;
-#else
-	return ARM64_SSBD_UNKNOWN;
-#endif
 }
 
 void arm64_set_ssbd_mitigation(bool state);
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index cd6639c008da..ad7b144e10f1 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -532,7 +532,6 @@ static inline int kvm_map_vectors(void)
 }
 #endif
 
-#ifdef CONFIG_ARM64_SSBD
 DECLARE_PER_CPU_READ_MOSTLY(u64, arm64_ssbd_callback_required);
 
 static inline int hyp_map_aux_data(void)
@@ -549,12 +548,6 @@ static inline int hyp_map_aux_data(void)
 	}
 	return 0;
 }
-#else
-static inline int hyp_map_aux_data(void)
-{
-	return 0;
-}
-#endif
 
 #define kvm_phys_to_vttbr(addr)		phys_to_ttbr(addr)
 
diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index bbbb4f4dbe31..27a4768114ba 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -58,7 +58,6 @@ struct bp_hardening_data {
 	bp_hardening_cb_t	fn;
 };
 
-#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 DECLARE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 static inline struct bp_hardening_data *arm64_get_bp_hardening_data(void)
@@ -77,14 +76,6 @@ static inline void arm64_apply_bp_hardening(void)
 	if (d->fn)
 		d->fn();
 }
-#else
-static inline struct bp_hardening_data *arm64_get_bp_hardening_data(void)
-{
-	return NULL;
-}
-
-static inline void arm64_apply_bp_hardening(void)	{ }
-#endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 extern void arm64_memblock_init(void);
 extern void paging_init(void);
diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
index ebb6b1760e63..288e5791dc5a 100644
--- a/arch/arm64/kernel/Makefile
+++ b/arch/arm64/kernel/Makefile
@@ -19,7 +19,7 @@ obj-y			:= debug-monitors.o entry.o irq.o fpsimd.o		\
 			   return_address.o cpuinfo.o cpu_errata.o		\
 			   cpufeature.o alternative.o cacheinfo.o		\
 			   smp.o smp_spin_table.o topology.o smccc-call.o	\
-			   syscall.o
+			   ssbd.o syscall.o
 
 targets			+= efi-entry.o
 
@@ -58,7 +58,6 @@ arm64-reloc-test-y := reloc_test_core.o reloc_test_syms.o
 obj-$(CONFIG_CRASH_DUMP)		+= crash_dump.o
 obj-$(CONFIG_CRASH_CORE)		+= crash_core.o
 obj-$(CONFIG_ARM_SDE_INTERFACE)		+= sdei.o
-obj-$(CONFIG_ARM64_SSBD)		+= ssbd.o
 obj-$(CONFIG_ARM64_PTR_AUTH)		+= pointer_auth.o
 
 obj-y					+= vdso/ probes/
diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 550bbacfeb31..dd56b96a4b08 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -265,9 +265,7 @@ static int detect_harden_bp_fw(void)
 	    ((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR_V1))
 		cb = qcom_link_stack_sanitization;
 
-	if (IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR))
-		install_bp_hardening_cb(cb, smccc_start, smccc_end);
-
+	install_bp_hardening_cb(cb, smccc_start, smccc_end);
 	return 1;
 }
 
@@ -346,11 +344,6 @@ void arm64_set_ssbd_mitigation(bool state)
 {
 	int conduit;
 
-	if (!IS_ENABLED(CONFIG_ARM64_SSBD)) {
-		pr_info_once("SSBD disabled by kernel configuration\n");
-		return;
-	}
-
 	if (this_cpu_has_cap(ARM64_SSBS)) {
 		if (state)
 			asm volatile(SET_PSTATE_SSBS(0));
@@ -600,12 +593,6 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 
 	__spectrev2_safe = false;
 
-	if (!IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR)) {
-		pr_warn_once("spectrev2 mitigation disabled by kernel configuration\n");
-		__hardenbp_enab = false;
-		return false;
-	}
-
 	/* forced off */
 	if (__nospectre_v2 || cpu_mitigations_off()) {
 		pr_info_once("spectrev2 mitigation disabled by command line option\n");
@@ -991,9 +978,7 @@ ssize_t cpu_show_spec_store_bypass(struct device *dev,
 	switch (ssbd_state) {
 	case ARM64_SSBD_KERNEL:
 	case ARM64_SSBD_FORCE_ENABLE:
-		if (IS_ENABLED(CONFIG_ARM64_SSBD))
-			return sprintf(buf,
-			    "Mitigation: Speculative Store Bypass disabled via prctl\n");
+		return sprintf(buf, "Mitigation: Speculative Store Bypass disabled via prctl\n");
 	}
 
 	return sprintf(buf, "Vulnerable\n");
diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
index 976fb30e87c8..b58d4ff09ae5 100644
--- a/arch/arm64/kernel/cpufeature.c
+++ b/arch/arm64/kernel/cpufeature.c
@@ -1490,7 +1490,6 @@ static void cpu_has_fwb(const struct arm64_cpu_capabilities *__unused)
 	WARN_ON(val & (7 << 27 | 7 << 21));
 }
 
-#ifdef CONFIG_ARM64_SSBD
 static int ssbs_emulation_handler(struct pt_regs *regs, u32 instr)
 {
 	if (user_mode(regs))
@@ -1530,7 +1529,6 @@ static void cpu_enable_ssbs(const struct arm64_cpu_capabilities *__unused)
 		arm64_set_ssbd_mitigation(true);
 	}
 }
-#endif /* CONFIG_ARM64_SSBD */
 
 #ifdef CONFIG_ARM64_PAN
 static void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused)
@@ -1842,7 +1840,6 @@ static const struct arm64_cpu_capabilities arm64_features[] = {
 		.field_pos = ID_AA64ISAR0_CRC32_SHIFT,
 		.min_field_value = 1,
 	},
-#ifdef CONFIG_ARM64_SSBD
 	{
 		.desc = "Speculative Store Bypassing Safe (SSBS)",
 		.capability = ARM64_SSBS,
@@ -1854,7 +1851,6 @@ static const struct arm64_cpu_capabilities arm64_features[] = {
 		.min_field_value = ID_AA64PFR1_SSBS_PSTATE_ONLY,
 		.cpu_enable = cpu_enable_ssbs,
 	},
-#endif
 #ifdef CONFIG_ARM64_CNP
 	{
 		.desc = "Common not Private translations",
* Unmerged path arch/arm64/kernel/entry.S
* Unmerged path arch/arm64/kvm/Kconfig
diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
index 92fe932d9cea..678b9b6d6a8f 100644
--- a/arch/arm64/kvm/hyp/hyp-entry.S
+++ b/arch/arm64/kvm/hyp/hyp-entry.S
@@ -127,7 +127,6 @@ el1_hvc_guest:
 			  ARM_SMCCC_ARCH_WORKAROUND_2)
 	cbnz	w1, el1_trap
 
-#ifdef CONFIG_ARM64_SSBD
 alternative_cb	arm64_enable_wa2_handling
 	b	wa2_end
 alternative_cb_end
@@ -154,7 +153,6 @@ alternative_cb_end
 wa2_end:
 	mov	x2, xzr
 	mov	x1, xzr
-#endif
 
 wa_epilogue:
 	mov	x0, xzr
diff --git a/arch/arm64/kvm/hyp/include/hyp/switch.h b/arch/arm64/kvm/hyp/include/hyp/switch.h
index f56599d5be74..e6e61435b659 100644
--- a/arch/arm64/kvm/hyp/include/hyp/switch.h
+++ b/arch/arm64/kvm/hyp/include/hyp/switch.h
@@ -489,7 +489,6 @@ static inline bool __needs_ssbd_off(struct kvm_vcpu *vcpu)
 
 static inline void __set_guest_arch_workaround_state(struct kvm_vcpu *vcpu)
 {
-#ifdef CONFIG_ARM64_SSBD
 	/*
 	 * The host runs with the workaround always present. If the
 	 * guest wants it disabled, so be it...
@@ -497,19 +496,16 @@ static inline void __set_guest_arch_workaround_state(struct kvm_vcpu *vcpu)
 	if (__needs_ssbd_off(vcpu) &&
 	    __hyp_this_cpu_read(arm64_ssbd_callback_required))
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_WORKAROUND_2, 0, NULL);
-#endif
 }
 
 static inline void __set_host_arch_workaround_state(struct kvm_vcpu *vcpu)
 {
-#ifdef CONFIG_ARM64_SSBD
 	/*
 	 * If the guest has disabled the workaround, bring it back on.
 	 */
 	if (__needs_ssbd_off(vcpu) &&
 	    __hyp_this_cpu_read(arm64_ssbd_callback_required))
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_WORKAROUND_2, 1, NULL);
-#endif
 }
 
 static inline void __kvm_unexpected_el2_exception(void)
