RDMA/rxe: Move mcg_lock to rxe

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Bob Pearson <rpearsonhpe@gmail.com>
commit 9fd0eb7c3c73c80a7bbe28dc71ae8ec5698a7e84
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/9fd0eb7c.failed

Replace mcg->mcg_lock and mc_grp_pool->pool_lock by rxe->mcg_lock.  This
is the first step of several intended to decouple the mc_grp and mc_elem
objects from the rxe pool code.

Link: https://lore.kernel.org/r/20220208211644.123457-2-rpearsonhpe@gmail.com
	Signed-off-by: Bob Pearson <rpearsonhpe@gmail.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 9fd0eb7c3c73c80a7bbe28dc71ae8ec5698a7e84)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/sw/rxe/rxe_mcast.c
#	drivers/infiniband/sw/rxe/rxe_verbs.h
diff --cc drivers/infiniband/sw/rxe/rxe_mcast.c
index 5f1c72c1473c,fae04497cf2b..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_mcast.c
+++ b/drivers/infiniband/sw/rxe/rxe_mcast.c
@@@ -25,8 -25,8 +25,13 @@@ static int rxe_mcast_delete(struct rxe_
  	return dev_mc_del(rxe->ndev, ll_addr);
  }
  
++<<<<<<< HEAD
 +/* caller should hold mc_grp_pool->pool_lock */
 +static struct rxe_mc_grp *create_grp(struct rxe_dev *rxe,
++=======
+ /* caller should hold rxe->mcg_lock */
+ static struct rxe_mcg *create_grp(struct rxe_dev *rxe,
++>>>>>>> 9fd0eb7c3c73 (RDMA/rxe: Move mcg_lock to rxe)
  				     struct rxe_pool *pool,
  				     union ib_gid *mgid)
  {
@@@ -82,15 -81,15 +86,19 @@@ done
  	return 0;
  }
  
 -static int rxe_mcast_add_grp_elem(struct rxe_dev *rxe, struct rxe_qp *qp,
 -			   struct rxe_mcg *grp)
 +int rxe_mcast_add_grp_elem(struct rxe_dev *rxe, struct rxe_qp *qp,
 +			   struct rxe_mc_grp *grp)
  {
  	int err;
 -	struct rxe_mca *elem;
 -	unsigned long flags;
 +	struct rxe_mc_elem *elem;
  
  	/* check to see of the qp is already a member of the group */
++<<<<<<< HEAD
 +	spin_lock_bh(&qp->grp_lock);
 +	spin_lock_bh(&grp->mcg_lock);
++=======
+ 	spin_lock_irqsave(&rxe->mcg_lock, flags);
++>>>>>>> 9fd0eb7c3c73 (RDMA/rxe: Move mcg_lock to rxe)
  	list_for_each_entry(elem, &grp->qp_list, qp_list) {
  		if (elem->qp == qp) {
  			err = 0;
@@@ -121,8 -119,7 +129,12 @@@
  
  	err = 0;
  out:
++<<<<<<< HEAD
 +	spin_unlock_bh(&grp->mcg_lock);
 +	spin_unlock_bh(&qp->grp_lock);
++=======
+ 	spin_unlock_irqrestore(&rxe->mcg_lock, flags);
++>>>>>>> 9fd0eb7c3c73 (RDMA/rxe: Move mcg_lock to rxe)
  	return err;
  }
  
@@@ -136,17 -134,15 +148,25 @@@ int rxe_mcast_drop_grp_elem(struct rxe_
  	if (!grp)
  		goto err1;
  
++<<<<<<< HEAD
 +	spin_lock_bh(&qp->grp_lock);
 +	spin_lock_bh(&grp->mcg_lock);
++=======
+ 	spin_lock_irqsave(&rxe->mcg_lock, flags);
++>>>>>>> 9fd0eb7c3c73 (RDMA/rxe: Move mcg_lock to rxe)
  
  	list_for_each_entry_safe(elem, tmp, &grp->qp_list, qp_list) {
  		if (elem->qp == qp) {
  			list_del(&elem->qp_list);
 +			list_del(&elem->grp_list);
  			grp->num_qp--;
 -			atomic_dec(&qp->mcg_num);
  
++<<<<<<< HEAD
 +			spin_unlock_bh(&grp->mcg_lock);
 +			spin_unlock_bh(&qp->grp_lock);
++=======
+ 			spin_unlock_irqrestore(&rxe->mcg_lock, flags);
++>>>>>>> 9fd0eb7c3c73 (RDMA/rxe: Move mcg_lock to rxe)
  			rxe_drop_ref(elem);
  			rxe_drop_ref(grp);	/* ref held by QP */
  			rxe_drop_ref(grp);	/* ref from get_key */
@@@ -154,8 -150,7 +174,12 @@@
  		}
  	}
  
++<<<<<<< HEAD
 +	spin_unlock_bh(&grp->mcg_lock);
 +	spin_unlock_bh(&qp->grp_lock);
++=======
+ 	spin_unlock_irqrestore(&rxe->mcg_lock, flags);
++>>>>>>> 9fd0eb7c3c73 (RDMA/rxe: Move mcg_lock to rxe)
  	rxe_drop_ref(grp);			/* ref from get_key */
  err1:
  	return -EINVAL;
diff --cc drivers/infiniband/sw/rxe/rxe_verbs.h
index 2fd73c878e17,9940c69cbb63..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@@ -354,9 -351,8 +354,14 @@@ struct rxe_mw 
  	u64			length;
  };
  
++<<<<<<< HEAD
 +struct rxe_mc_grp {
 +	struct rxe_pool_entry	pelem;
 +	spinlock_t		mcg_lock; /* guard group */
++=======
+ struct rxe_mcg {
+ 	struct rxe_pool_elem	elem;
++>>>>>>> 9fd0eb7c3c73 (RDMA/rxe: Move mcg_lock to rxe)
  	struct rxe_dev		*rxe;
  	struct list_head	qp_list;
  	union ib_gid		mgid;
diff --git a/drivers/infiniband/sw/rxe/rxe.c b/drivers/infiniband/sw/rxe/rxe.c
index 1eb33c81b99d..a74ddbd5a841 100644
--- a/drivers/infiniband/sw/rxe/rxe.c
+++ b/drivers/infiniband/sw/rxe/rxe.c
@@ -213,6 +213,8 @@ static int rxe_init(struct rxe_dev *rxe)
 	spin_lock_init(&rxe->pending_lock);
 	INIT_LIST_HEAD(&rxe->pending_mmaps);
 
+	spin_lock_init(&rxe->mcg_lock);
+
 	mutex_init(&rxe->usdev_lock);
 
 	return 0;
* Unmerged path drivers/infiniband/sw/rxe/rxe_mcast.c
diff --git a/drivers/infiniband/sw/rxe/rxe_recv.c b/drivers/infiniband/sw/rxe/rxe_recv.c
index 6a6cc1fa90e4..101bd19fa6e8 100644
--- a/drivers/infiniband/sw/rxe/rxe_recv.c
+++ b/drivers/infiniband/sw/rxe/rxe_recv.c
@@ -250,7 +250,7 @@ static void rxe_rcv_mcast_pkt(struct rxe_dev *rxe, struct sk_buff *skb)
 	if (!mcg)
 		goto drop;	/* mcast group not registered */
 
-	spin_lock_bh(&mcg->mcg_lock);
+	spin_lock_bh(&rxe->mcg_lock);
 
 	/* this is unreliable datagram service so we let
 	 * failures to deliver a multicast packet to a
@@ -298,7 +298,7 @@ static void rxe_rcv_mcast_pkt(struct rxe_dev *rxe, struct sk_buff *skb)
 		}
 	}
 
-	spin_unlock_bh(&mcg->mcg_lock);
+	spin_unlock_bh(&rxe->mcg_lock);
 
 	rxe_drop_ref(mcg);	/* drop ref from rxe_pool_get_key. */
 
* Unmerged path drivers/infiniband/sw/rxe/rxe_verbs.h
