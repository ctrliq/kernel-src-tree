RDMA/rxe: Replace red-black trees by xarrays

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Bob Pearson <rpearsonhpe@gmail.com>
commit 3225717f6dfa29a6f03629b7a7f8492e1521d06d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/3225717f.failed

Currently the rxe driver uses red-black trees to add indices to the rxe
object pools. Linux xarrays provide a better way to implement the same
functionality for indices. This patch replaces red-black trees by xarrays
for pool objects. Since xarrays already have a spinlock use that in place
of the pool rwlock. Make sure that all changes in the xarray(index) and
kref(ref counnt) occur atomically.

Link: https://lore.kernel.org/r/20220304000808.225811-9-rpearsonhpe@gmail.com
	Signed-off-by: Bob Pearson <rpearsonhpe@gmail.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 3225717f6dfa29a6f03629b7a7f8492e1521d06d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/sw/rxe/rxe.c
#	drivers/infiniband/sw/rxe/rxe_mw.c
#	drivers/infiniband/sw/rxe/rxe_pool.c
#	drivers/infiniband/sw/rxe/rxe_pool.h
#	drivers/infiniband/sw/rxe/rxe_verbs.c
diff --cc drivers/infiniband/sw/rxe/rxe.c
index 71b5781d154b,2dae7538a2ea..000000000000
--- a/drivers/infiniband/sw/rxe/rxe.c
+++ b/drivers/infiniband/sw/rxe/rxe.c
@@@ -118,82 -114,16 +118,93 @@@ static void rxe_init_ports(struct rxe_d
  }
  
  /* init pools of managed objects */
- static int rxe_init_pools(struct rxe_dev *rxe)
+ static void rxe_init_pools(struct rxe_dev *rxe)
  {
++<<<<<<< HEAD
 +	int err;
 +
 +	err = rxe_pool_init(rxe, &rxe->uc_pool, RXE_TYPE_UC,
 +			    rxe->max_ucontext);
 +	if (err)
 +		goto err1;
 +
 +	err = rxe_pool_init(rxe, &rxe->pd_pool, RXE_TYPE_PD,
 +			    rxe->attr.max_pd);
 +	if (err)
 +		goto err2;
 +
 +	err = rxe_pool_init(rxe, &rxe->ah_pool, RXE_TYPE_AH,
 +			    rxe->attr.max_ah);
 +	if (err)
 +		goto err3;
 +
 +	err = rxe_pool_init(rxe, &rxe->srq_pool, RXE_TYPE_SRQ,
 +			    rxe->attr.max_srq);
 +	if (err)
 +		goto err4;
 +
 +	err = rxe_pool_init(rxe, &rxe->qp_pool, RXE_TYPE_QP,
 +			    rxe->attr.max_qp);
 +	if (err)
 +		goto err5;
 +
 +	err = rxe_pool_init(rxe, &rxe->cq_pool, RXE_TYPE_CQ,
 +			    rxe->attr.max_cq);
 +	if (err)
 +		goto err6;
 +
 +	err = rxe_pool_init(rxe, &rxe->mr_pool, RXE_TYPE_MR,
 +			    rxe->attr.max_mr);
 +	if (err)
 +		goto err7;
 +
 +	err = rxe_pool_init(rxe, &rxe->mw_pool, RXE_TYPE_MW,
 +			    rxe->attr.max_mw);
 +	if (err)
 +		goto err8;
 +
 +	err = rxe_pool_init(rxe, &rxe->mc_grp_pool, RXE_TYPE_MC_GRP,
 +			    rxe->attr.max_mcast_grp);
 +	if (err)
 +		goto err9;
 +
 +	err = rxe_pool_init(rxe, &rxe->mc_elem_pool, RXE_TYPE_MC_ELEM,
 +			    rxe->attr.max_total_mcast_qp_attach);
 +	if (err)
 +		goto err10;
 +
 +	return 0;
 +
 +err10:
 +	rxe_pool_cleanup(&rxe->mc_grp_pool);
 +err9:
 +	rxe_pool_cleanup(&rxe->mw_pool);
 +err8:
 +	rxe_pool_cleanup(&rxe->mr_pool);
 +err7:
 +	rxe_pool_cleanup(&rxe->cq_pool);
 +err6:
 +	rxe_pool_cleanup(&rxe->qp_pool);
 +err5:
 +	rxe_pool_cleanup(&rxe->srq_pool);
 +err4:
 +	rxe_pool_cleanup(&rxe->ah_pool);
 +err3:
 +	rxe_pool_cleanup(&rxe->pd_pool);
 +err2:
 +	rxe_pool_cleanup(&rxe->uc_pool);
 +err1:
 +	return err;
++=======
+ 	rxe_pool_init(rxe, &rxe->uc_pool, RXE_TYPE_UC);
+ 	rxe_pool_init(rxe, &rxe->pd_pool, RXE_TYPE_PD);
+ 	rxe_pool_init(rxe, &rxe->ah_pool, RXE_TYPE_AH);
+ 	rxe_pool_init(rxe, &rxe->srq_pool, RXE_TYPE_SRQ);
+ 	rxe_pool_init(rxe, &rxe->qp_pool, RXE_TYPE_QP);
+ 	rxe_pool_init(rxe, &rxe->cq_pool, RXE_TYPE_CQ);
+ 	rxe_pool_init(rxe, &rxe->mr_pool, RXE_TYPE_MR);
+ 	rxe_pool_init(rxe, &rxe->mw_pool, RXE_TYPE_MW);
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  }
  
  /* initialize rxe device state */
@@@ -215,9 -140,11 +221,7 @@@ static void rxe_init(struct rxe_dev *rx
  	spin_lock_init(&rxe->pending_lock);
  	INIT_LIST_HEAD(&rxe->pending_mmaps);
  
 -	/* init multicast support */
 -	spin_lock_init(&rxe->mcg_lock);
 -	rxe->mcg_tree = RB_ROOT;
 -
  	mutex_init(&rxe->usdev_lock);
- 
- 	return 0;
  }
  
  void rxe_set_mtu(struct rxe_dev *rxe, unsigned int ndev_mtu)
diff --cc drivers/infiniband/sw/rxe/rxe_mw.c
index 9534a7fe1a98,7df36c40eec2..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_mw.c
+++ b/drivers/infiniband/sw/rxe/rxe_mw.c
@@@ -20,8 -20,7 +20,12 @@@ int rxe_alloc_mw(struct ib_mw *ibmw, st
  		return ret;
  	}
  
++<<<<<<< HEAD
 +	rxe_add_index(mw);
 +	mw->rkey = ibmw->rkey = (mw->pelem.index << 8) | rxe_get_next_key(-1);
++=======
+ 	mw->rkey = ibmw->rkey = (mw->elem.index << 8) | rxe_get_next_key(-1);
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	mw->state = (mw->ibmw.type == IB_MW_TYPE_2) ?
  			RXE_MW_STATE_FREE : RXE_MW_STATE_VALID;
  	spin_lock_init(&mw->lock);
@@@ -332,10 -328,3 +336,13 @@@ struct rxe_mw *rxe_lookup_mw(struct rxe
  
  	return mw;
  }
++<<<<<<< HEAD
 +
 +void rxe_mw_cleanup(struct rxe_pool_entry *elem)
 +{
 +	struct rxe_mw *mw = container_of(elem, typeof(*mw), pelem);
 +
 +	rxe_drop_index(mw);
 +}
++=======
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
diff --cc drivers/infiniband/sw/rxe/rxe_pool.c
index cbac3d42c767,87066d04ed18..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_pool.c
+++ b/drivers/infiniband/sw/rxe/rxe_pool.c
@@@ -22,117 -21,77 +22,159 @@@ static const struct rxe_type_info 
  	[RXE_TYPE_UC] = {
  		.name		= "uc",
  		.size		= sizeof(struct rxe_ucontext),
++<<<<<<< HEAD
 +		.elem_offset	= offsetof(struct rxe_ucontext, pelem),
 +		.flags          = RXE_POOL_NO_ALLOC,
++=======
+ 		.elem_offset	= offsetof(struct rxe_ucontext, elem),
+ 		.min_index	= 1,
+ 		.max_index	= UINT_MAX,
+ 		.max_elem	= UINT_MAX,
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	},
  	[RXE_TYPE_PD] = {
  		.name		= "pd",
  		.size		= sizeof(struct rxe_pd),
++<<<<<<< HEAD
 +		.elem_offset	= offsetof(struct rxe_pd, pelem),
 +		.flags		= RXE_POOL_NO_ALLOC,
++=======
+ 		.elem_offset	= offsetof(struct rxe_pd, elem),
+ 		.min_index	= 1,
+ 		.max_index	= UINT_MAX,
+ 		.max_elem	= UINT_MAX,
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	},
  	[RXE_TYPE_AH] = {
  		.name		= "ah",
  		.size		= sizeof(struct rxe_ah),
++<<<<<<< HEAD
 +		.elem_offset	= offsetof(struct rxe_ah, pelem),
 +		.flags		= RXE_POOL_INDEX | RXE_POOL_NO_ALLOC,
++=======
+ 		.elem_offset	= offsetof(struct rxe_ah, elem),
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  		.min_index	= RXE_MIN_AH_INDEX,
  		.max_index	= RXE_MAX_AH_INDEX,
 -		.max_elem	= RXE_MAX_AH_INDEX - RXE_MIN_AH_INDEX + 1,
  	},
  	[RXE_TYPE_SRQ] = {
  		.name		= "srq",
  		.size		= sizeof(struct rxe_srq),
++<<<<<<< HEAD
 +		.elem_offset	= offsetof(struct rxe_srq, pelem),
 +		.flags		= RXE_POOL_INDEX | RXE_POOL_NO_ALLOC,
++=======
+ 		.elem_offset	= offsetof(struct rxe_srq, elem),
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  		.min_index	= RXE_MIN_SRQ_INDEX,
  		.max_index	= RXE_MAX_SRQ_INDEX,
 -		.max_elem	= RXE_MAX_SRQ_INDEX - RXE_MIN_SRQ_INDEX + 1,
  	},
  	[RXE_TYPE_QP] = {
  		.name		= "qp",
  		.size		= sizeof(struct rxe_qp),
 -		.elem_offset	= offsetof(struct rxe_qp, elem),
 +		.elem_offset	= offsetof(struct rxe_qp, pelem),
  		.cleanup	= rxe_qp_cleanup,
- 		.flags		= RXE_POOL_INDEX,
  		.min_index	= RXE_MIN_QP_INDEX,
  		.max_index	= RXE_MAX_QP_INDEX,
 -		.max_elem	= RXE_MAX_QP_INDEX - RXE_MIN_QP_INDEX + 1,
  	},
  	[RXE_TYPE_CQ] = {
  		.name		= "cq",
  		.size		= sizeof(struct rxe_cq),
 -		.elem_offset	= offsetof(struct rxe_cq, elem),
 +		.elem_offset	= offsetof(struct rxe_cq, pelem),
 +		.flags          = RXE_POOL_NO_ALLOC,
  		.cleanup	= rxe_cq_cleanup,
++<<<<<<< HEAD
++=======
+ 		.min_index	= 1,
+ 		.max_index	= UINT_MAX,
+ 		.max_elem	= UINT_MAX,
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	},
  	[RXE_TYPE_MR] = {
  		.name		= "mr",
  		.size		= sizeof(struct rxe_mr),
 -		.elem_offset	= offsetof(struct rxe_mr, elem),
 +		.elem_offset	= offsetof(struct rxe_mr, pelem),
  		.cleanup	= rxe_mr_cleanup,
++<<<<<<< HEAD
 +		.flags		= RXE_POOL_INDEX,
++=======
+ 		.flags		= RXE_POOL_ALLOC,
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  		.min_index	= RXE_MIN_MR_INDEX,
  		.max_index	= RXE_MAX_MR_INDEX,
 -		.max_elem	= RXE_MAX_MR_INDEX - RXE_MIN_MR_INDEX + 1,
  	},
  	[RXE_TYPE_MW] = {
  		.name		= "mw",
  		.size		= sizeof(struct rxe_mw),
++<<<<<<< HEAD
 +		.elem_offset	= offsetof(struct rxe_mw, pelem),
 +		.cleanup	= rxe_mw_cleanup,
 +		.flags		= RXE_POOL_INDEX | RXE_POOL_NO_ALLOC,
++=======
+ 		.elem_offset	= offsetof(struct rxe_mw, elem),
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  		.min_index	= RXE_MIN_MW_INDEX,
  		.max_index	= RXE_MAX_MW_INDEX,
 -		.max_elem	= RXE_MAX_MW_INDEX - RXE_MIN_MW_INDEX + 1,
 +	},
 +	[RXE_TYPE_MC_GRP] = {
 +		.name		= "rxe-mc_grp",
 +		.size		= sizeof(struct rxe_mc_grp),
 +		.elem_offset	= offsetof(struct rxe_mc_grp, pelem),
 +		.cleanup	= rxe_mc_cleanup,
 +		.flags		= RXE_POOL_KEY,
 +		.key_offset	= offsetof(struct rxe_mc_grp, mgid),
 +		.key_size	= sizeof(union ib_gid),
 +	},
 +	[RXE_TYPE_MC_ELEM] = {
 +		.name		= "rxe-mc_elem",
 +		.size		= sizeof(struct rxe_mc_elem),
 +		.elem_offset	= offsetof(struct rxe_mc_elem, pelem),
  	},
  };
  
++<<<<<<< HEAD
 +static inline const char *pool_name(struct rxe_pool *pool)
 +{
 +	return rxe_type_info[pool->type].name;
 +}
 +
 +static int rxe_pool_init_index(struct rxe_pool *pool, u32 max, u32 min)
 +{
 +	int err = 0;
 +
 +	if ((max - min + 1) < pool->max_elem) {
 +		pr_warn("not enough indices for max_elem\n");
 +		err = -EINVAL;
 +		goto out;
 +	}
 +
 +	pool->index.max_index = max;
 +	pool->index.min_index = min;
 +
 +	pool->index.table = bitmap_zalloc(max - min + 1, GFP_KERNEL);
 +	if (!pool->index.table) {
 +		err = -ENOMEM;
 +		goto out;
 +	}
 +
 +out:
 +	return err;
 +}
 +
 +int rxe_pool_init(
 +	struct rxe_dev		*rxe,
 +	struct rxe_pool		*pool,
 +	enum rxe_elem_type	type,
 +	unsigned int		max_elem)
 +{
 +	int			err = 0;
 +	size_t			size = rxe_type_info[type].size;
++=======
+ void rxe_pool_init(struct rxe_dev *rxe, struct rxe_pool *pool,
+ 		   enum rxe_elem_type type)
+ {
+ 	const struct rxe_type_info *info = &rxe_type_info[type];
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  
  	memset(pool, 0, sizeof(*pool));
  
@@@ -147,358 -106,92 +189,409 @@@
  
  	atomic_set(&pool->num_elem, 0);
  
++<<<<<<< HEAD
 +	rwlock_init(&pool->pool_lock);
 +
 +	if (rxe_type_info[type].flags & RXE_POOL_INDEX) {
 +		err = rxe_pool_init_index(pool,
 +					  rxe_type_info[type].max_index,
 +					  rxe_type_info[type].min_index);
 +		if (err)
 +			goto out;
 +	}
 +
 +	if (rxe_type_info[type].flags & RXE_POOL_KEY) {
 +		pool->key.key_offset = rxe_type_info[type].key_offset;
 +		pool->key.key_size = rxe_type_info[type].key_size;
 +	}
 +
 +out:
 +	return err;
++=======
+ 	xa_init_flags(&pool->xa, XA_FLAGS_ALLOC);
+ 	pool->limit.min = info->min_index;
+ 	pool->limit.max = info->max_index;
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  }
  
  void rxe_pool_cleanup(struct rxe_pool *pool)
  {
++<<<<<<< HEAD
 +	if (atomic_read(&pool->num_elem) > 0)
 +		pr_warn("%s pool destroyed with unfree'd elem\n",
 +			pool_name(pool));
 +
 +	bitmap_free(pool->index.table);
 +}
 +
 +static u32 alloc_index(struct rxe_pool *pool)
 +{
 +	u32 index;
 +	u32 range = pool->index.max_index - pool->index.min_index + 1;
 +
 +	index = find_next_zero_bit(pool->index.table, range, pool->index.last);
 +	if (index >= range)
 +		index = find_first_zero_bit(pool->index.table, range);
 +
 +	WARN_ON_ONCE(index >= range);
 +	set_bit(index, pool->index.table);
 +	pool->index.last = index;
 +	return index + pool->index.min_index;
 +}
 +
 +static int rxe_insert_index(struct rxe_pool *pool, struct rxe_pool_entry *new)
 +{
 +	struct rb_node **link = &pool->index.tree.rb_node;
 +	struct rb_node *parent = NULL;
 +	struct rxe_pool_entry *elem;
 +
 +	while (*link) {
 +		parent = *link;
 +		elem = rb_entry(parent, struct rxe_pool_entry, index_node);
 +
 +		if (elem->index == new->index) {
 +			pr_warn("element already exists!\n");
 +			return -EINVAL;
 +		}
 +
 +		if (elem->index > new->index)
 +			link = &(*link)->rb_left;
 +		else
 +			link = &(*link)->rb_right;
 +	}
 +
 +	rb_link_node(&new->index_node, parent, link);
 +	rb_insert_color(&new->index_node, &pool->index.tree);
 +
 +	return 0;
 +}
 +
 +static int rxe_insert_key(struct rxe_pool *pool, struct rxe_pool_entry *new)
 +{
 +	struct rb_node **link = &pool->key.tree.rb_node;
 +	struct rb_node *parent = NULL;
 +	struct rxe_pool_entry *elem;
 +	int cmp;
 +
 +	while (*link) {
 +		parent = *link;
 +		elem = rb_entry(parent, struct rxe_pool_entry, key_node);
 +
 +		cmp = memcmp((u8 *)elem + pool->key.key_offset,
 +			     (u8 *)new + pool->key.key_offset, pool->key.key_size);
 +
 +		if (cmp == 0) {
 +			pr_warn("key already exists!\n");
 +			return -EINVAL;
 +		}
 +
 +		if (cmp > 0)
 +			link = &(*link)->rb_left;
 +		else
 +			link = &(*link)->rb_right;
 +	}
 +
 +	rb_link_node(&new->key_node, parent, link);
 +	rb_insert_color(&new->key_node, &pool->key.tree);
 +
 +	return 0;
 +}
 +
 +int __rxe_add_key_locked(struct rxe_pool_entry *elem, void *key)
 +{
 +	struct rxe_pool *pool = elem->pool;
 +	int err;
 +
 +	memcpy((u8 *)elem + pool->key.key_offset, key, pool->key.key_size);
 +	err = rxe_insert_key(pool, elem);
 +
 +	return err;
 +}
 +
 +int __rxe_add_key(struct rxe_pool_entry *elem, void *key)
 +{
 +	struct rxe_pool *pool = elem->pool;
 +	unsigned long flags;
 +	int err;
 +
 +	write_lock_irqsave(&pool->pool_lock, flags);
 +	err = __rxe_add_key_locked(elem, key);
 +	write_unlock_irqrestore(&pool->pool_lock, flags);
 +
 +	return err;
 +}
 +
 +void __rxe_drop_key_locked(struct rxe_pool_entry *elem)
 +{
 +	struct rxe_pool *pool = elem->pool;
 +
 +	rb_erase(&elem->key_node, &pool->key.tree);
 +}
 +
 +void __rxe_drop_key(struct rxe_pool_entry *elem)
 +{
 +	struct rxe_pool *pool = elem->pool;
 +	unsigned long flags;
 +
 +	write_lock_irqsave(&pool->pool_lock, flags);
 +	__rxe_drop_key_locked(elem);
 +	write_unlock_irqrestore(&pool->pool_lock, flags);
++=======
+ 	WARN_ON(!xa_empty(&pool->xa));
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  }
  
 -void *rxe_alloc(struct rxe_pool *pool)
 +int __rxe_add_index_locked(struct rxe_pool_entry *elem)
  {
++<<<<<<< HEAD
 +	struct rxe_pool *pool = elem->pool;
++=======
+ 	struct rxe_pool_elem *elem;
+ 	void *obj;
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
 +	int err;
 +
 +	elem->index = alloc_index(pool);
 +	err = rxe_insert_index(pool, elem);
 +
 +	return err;
 +}
 +
 +int __rxe_add_index(struct rxe_pool_entry *elem)
 +{
 +	struct rxe_pool *pool = elem->pool;
 +	unsigned long flags;
  	int err;
  
 -	if (WARN_ON(!(pool->flags & RXE_POOL_ALLOC)))
 -		return NULL;
 +	write_lock_irqsave(&pool->pool_lock, flags);
 +	err = __rxe_add_index_locked(elem);
 +	write_unlock_irqrestore(&pool->pool_lock, flags);
 +
 +	return err;
 +}
 +
 +void __rxe_drop_index_locked(struct rxe_pool_entry *elem)
 +{
 +	struct rxe_pool *pool = elem->pool;
 +
 +	clear_bit(elem->index - pool->index.min_index, pool->index.table);
 +	rb_erase(&elem->index_node, &pool->index.tree);
 +}
 +
 +void __rxe_drop_index(struct rxe_pool_entry *elem)
 +{
 +	struct rxe_pool *pool = elem->pool;
 +	unsigned long flags;
 +
 +	write_lock_irqsave(&pool->pool_lock, flags);
 +	__rxe_drop_index_locked(elem);
 +	write_unlock_irqrestore(&pool->pool_lock, flags);
 +}
 +
 +void *rxe_alloc_locked(struct rxe_pool *pool)
 +{
 +	const struct rxe_type_info *info = &rxe_type_info[pool->type];
 +	struct rxe_pool_entry *elem;
 +	u8 *obj;
  
  	if (atomic_inc_return(&pool->num_elem) > pool->max_elem)
- 		goto out_cnt;
+ 		goto err_cnt;
  
 -	obj = kzalloc(pool->elem_size, GFP_KERNEL);
 +	obj = kzalloc(info->size, GFP_ATOMIC);
  	if (!obj)
- 		goto out_cnt;
+ 		goto err_cnt;
  
 -	elem = (struct rxe_pool_elem *)((u8 *)obj + pool->elem_offset);
 +	elem = (struct rxe_pool_entry *)(obj + info->elem_offset);
  
  	elem->pool = pool;
 -	elem->obj = obj;
  	kref_init(&elem->ref_cnt);
  
+ 	err = xa_alloc_cyclic(&pool->xa, &elem->index, elem, pool->limit,
+ 			      &pool->next, GFP_KERNEL);
+ 	if (err)
+ 		goto err_free;
+ 
  	return obj;
  
- out_cnt:
+ err_free:
+ 	kfree(obj);
+ err_cnt:
  	atomic_dec(&pool->num_elem);
  	return NULL;
  }
  
 -int __rxe_add_to_pool(struct rxe_pool *pool, struct rxe_pool_elem *elem)
 +void *rxe_alloc(struct rxe_pool *pool)
  {
++<<<<<<< HEAD
 +	const struct rxe_type_info *info = &rxe_type_info[pool->type];
 +	struct rxe_pool_entry *elem;
 +	u8 *obj;
++=======
+ 	int err;
+ 
+ 	if (WARN_ON(pool->flags & RXE_POOL_ALLOC))
+ 		return -EINVAL;
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  
  	if (atomic_inc_return(&pool->num_elem) > pool->max_elem)
- 		goto out_cnt;
+ 		goto err_cnt;
  
 +	obj = kzalloc(info->size, GFP_KERNEL);
 +	if (!obj)
 +		goto out_cnt;
 +
 +	elem = (struct rxe_pool_entry *)(obj + info->elem_offset);
 +
 +	elem->pool = pool;
 +	kref_init(&elem->ref_cnt);
 +
 +	return obj;
 +
 +out_cnt:
 +	atomic_dec(&pool->num_elem);
 +	return NULL;
 +}
 +
 +int __rxe_add_to_pool(struct rxe_pool *pool, struct rxe_pool_entry *elem)
 +{
 +	if (atomic_inc_return(&pool->num_elem) > pool->max_elem)
 +		goto out_cnt;
 +
  	elem->pool = pool;
 -	elem->obj = (u8 *)elem - pool->elem_offset;
  	kref_init(&elem->ref_cnt);
  
+ 	err = xa_alloc_cyclic(&pool->xa, &elem->index, elem, pool->limit,
+ 			      &pool->next, GFP_KERNEL);
+ 	if (err)
+ 		goto err_cnt;
+ 
  	return 0;
  
- out_cnt:
+ err_cnt:
  	atomic_dec(&pool->num_elem);
  	return -EINVAL;
  }
  
++<<<<<<< HEAD
 +void rxe_elem_release(struct kref *kref)
 +{
 +	struct rxe_pool_entry *elem =
 +		container_of(kref, struct rxe_pool_entry, ref_cnt);
 +	struct rxe_pool *pool = elem->pool;
 +	const struct rxe_type_info *info = &rxe_type_info[pool->type];
 +	u8 *obj;
 +
 +	if (pool->cleanup)
 +		pool->cleanup(elem);
 +
 +	if (!(pool->flags & RXE_POOL_NO_ALLOC)) {
 +		obj = (u8 *)elem - info->elem_offset;
 +		kfree(obj);
 +	}
 +
 +	atomic_dec(&pool->num_elem);
 +}
 +
 +void *rxe_pool_get_index_locked(struct rxe_pool *pool, u32 index)
 +{
 +	const struct rxe_type_info *info = &rxe_type_info[pool->type];
 +	struct rb_node *node;
 +	struct rxe_pool_entry *elem;
 +	u8 *obj;
 +
 +	node = pool->index.tree.rb_node;
 +
 +	while (node) {
 +		elem = rb_entry(node, struct rxe_pool_entry, index_node);
 +
 +		if (elem->index > index)
 +			node = node->rb_left;
 +		else if (elem->index < index)
 +			node = node->rb_right;
 +		else
 +			break;
 +	}
 +
 +	if (node) {
 +		kref_get(&elem->ref_cnt);
 +		obj = (u8 *)elem - info->elem_offset;
 +	} else {
 +		obj = NULL;
 +	}
 +
 +	return obj;
 +}
 +
 +void *rxe_pool_get_index(struct rxe_pool *pool, u32 index)
 +{
 +	u8 *obj;
 +	unsigned long flags;
 +
 +	read_lock_irqsave(&pool->pool_lock, flags);
 +	obj = rxe_pool_get_index_locked(pool, index);
 +	read_unlock_irqrestore(&pool->pool_lock, flags);
 +
 +	return obj;
 +}
 +
 +void *rxe_pool_get_key_locked(struct rxe_pool *pool, void *key)
 +{
 +	const struct rxe_type_info *info = &rxe_type_info[pool->type];
 +	struct rb_node *node;
 +	struct rxe_pool_entry *elem;
 +	u8 *obj;
 +	int cmp;
 +
 +	node = pool->key.tree.rb_node;
 +
 +	while (node) {
 +		elem = rb_entry(node, struct rxe_pool_entry, key_node);
 +
 +		cmp = memcmp((u8 *)elem + pool->key.key_offset,
 +			     key, pool->key.key_size);
 +
 +		if (cmp > 0)
 +			node = node->rb_left;
 +		else if (cmp < 0)
 +			node = node->rb_right;
 +		else
 +			break;
 +	}
 +
 +	if (node) {
 +		kref_get(&elem->ref_cnt);
 +		obj = (u8 *)elem - info->elem_offset;
 +	} else {
 +		obj = NULL;
 +	}
 +
 +	return obj;
 +}
 +
 +void *rxe_pool_get_key(struct rxe_pool *pool, void *key)
 +{
 +	u8 *obj;
 +	unsigned long flags;
 +
 +	read_lock_irqsave(&pool->pool_lock, flags);
 +	obj = rxe_pool_get_key_locked(pool, key);
 +	read_unlock_irqrestore(&pool->pool_lock, flags);
++=======
+ void *rxe_pool_get_index(struct rxe_pool *pool, u32 index)
+ {
+ 	struct rxe_pool_elem *elem;
+ 	struct xarray *xa = &pool->xa;
+ 	unsigned long flags;
+ 	void *obj;
+ 
+ 	xa_lock_irqsave(xa, flags);
+ 	elem = xa_load(xa, index);
+ 	if (elem && kref_get_unless_zero(&elem->ref_cnt))
+ 		obj = elem->obj;
+ 	else
+ 		obj = NULL;
+ 	xa_unlock_irqrestore(xa, flags);
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  
  	return obj;
  }
diff --cc drivers/infiniband/sw/rxe/rxe_pool.h
index 837585fdbc34,d1e05d384b2c..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_pool.h
+++ b/drivers/infiniband/sw/rxe/rxe_pool.h
@@@ -8,9 -8,7 +8,13 @@@
  #define RXE_POOL_H
  
  enum rxe_pool_flags {
++<<<<<<< HEAD
 +	RXE_POOL_INDEX		= BIT(1),
 +	RXE_POOL_KEY		= BIT(2),
 +	RXE_POOL_NO_ALLOC	= BIT(4),
++=======
+ 	RXE_POOL_ALLOC		= BIT(1),
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  };
  
  enum rxe_elem_type {
@@@ -27,55 -23,37 +31,74 @@@
  	RXE_NUM_TYPES,		/* keep me last */
  };
  
 -struct rxe_pool_elem {
 +struct rxe_pool_entry;
 +
 +struct rxe_pool_entry {
  	struct rxe_pool		*pool;
 -	void			*obj;
  	struct kref		ref_cnt;
  	struct list_head	list;
++<<<<<<< HEAD
 +
 +	/* only used if keyed */
 +	struct rb_node		key_node;
 +
 +	/* only used if indexed */
 +	struct rb_node		index_node;
++=======
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	u32			index;
  };
  
  struct rxe_pool {
  	struct rxe_dev		*rxe;
++<<<<<<< HEAD
 +	rwlock_t		pool_lock; /* protects pool add/del/search */
 +	size_t			elem_size;
 +	void			(*cleanup)(struct rxe_pool_entry *obj);
++=======
+ 	const char		*name;
+ 	void			(*cleanup)(struct rxe_pool_elem *elem);
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	enum rxe_pool_flags	flags;
  	enum rxe_elem_type	type;
  
  	unsigned int		max_elem;
  	atomic_t		num_elem;
 -	size_t			elem_size;
 -	size_t			elem_offset;
  
++<<<<<<< HEAD
 +	/* only used if indexed */
 +	struct {
 +		struct rb_root		tree;
 +		unsigned long		*table;
 +		u32			last;
 +		u32			max_index;
 +		u32			min_index;
 +	} index;
 +
 +	/* only used if keyed */
 +	struct {
 +		struct rb_root		tree;
 +		size_t			key_offset;
 +		size_t			key_size;
 +	} key;
++=======
+ 	struct xarray		xa;
+ 	struct xa_limit		limit;
+ 	u32			next;
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  };
  
  /* initialize a pool of objects with given limit on
   * number of elements. gets parameters from rxe_type_info
   * pool elements will be allocated out of a slab cache
   */
++<<<<<<< HEAD
 +int rxe_pool_init(struct rxe_dev *rxe, struct rxe_pool *pool,
 +		  enum rxe_elem_type type, u32 max_elem);
++=======
+ void rxe_pool_init(struct rxe_dev *rxe, struct rxe_pool *pool,
+ 		  enum rxe_elem_type type);
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  
  /* free resources from object pool */
  void rxe_pool_cleanup(struct rxe_pool *pool);
@@@ -86,73 -62,23 +109,90 @@@ void *rxe_alloc_locked(struct rxe_pool 
  void *rxe_alloc(struct rxe_pool *pool);
  
  /* connect already allocated object to pool */
 -int __rxe_add_to_pool(struct rxe_pool *pool, struct rxe_pool_elem *elem);
 +int __rxe_add_to_pool(struct rxe_pool *pool, struct rxe_pool_entry *elem);
 +
 +#define rxe_add_to_pool(pool, obj) __rxe_add_to_pool(pool, &(obj)->pelem)
 +
++<<<<<<< HEAD
 +/* assign an index to an indexed object and insert object into
 + *  pool's rb tree holding and not holding the pool_lock
 + */
 +int __rxe_add_index_locked(struct rxe_pool_entry *elem);
 +
 +#define rxe_add_index_locked(obj) __rxe_add_index_locked(&(obj)->pelem)
 +
 +int __rxe_add_index(struct rxe_pool_entry *elem);
 +
 +#define rxe_add_index(obj) __rxe_add_index(&(obj)->pelem)
 +
 +/* drop an index and remove object from rb tree
 + * holding and not holding the pool_lock
 + */
 +void __rxe_drop_index_locked(struct rxe_pool_entry *elem);
 +
 +#define rxe_drop_index_locked(obj) __rxe_drop_index_locked(&(obj)->pelem)
 +
 +void __rxe_drop_index(struct rxe_pool_entry *elem);
 +
 +#define rxe_drop_index(obj) __rxe_drop_index(&(obj)->pelem)
 +
 +/* assign a key to a keyed object and insert object into
 + * pool's rb tree holding and not holding pool_lock
 + */
 +int __rxe_add_key_locked(struct rxe_pool_entry *elem, void *key);
 +
 +#define rxe_add_key_locked(obj, key) __rxe_add_key_locked(&(obj)->pelem, key)
 +
 +int __rxe_add_key(struct rxe_pool_entry *elem, void *key);
 +
 +#define rxe_add_key(obj, key) __rxe_add_key(&(obj)->pelem, key)
 +
 +/* remove elem from rb tree holding and not holding the pool_lock */
 +void __rxe_drop_key_locked(struct rxe_pool_entry *elem);
 +
 +#define rxe_drop_key_locked(obj) __rxe_drop_key_locked(&(obj)->pelem)
 +
 +void __rxe_drop_key(struct rxe_pool_entry *elem);
 +
 +#define rxe_drop_key(obj) __rxe_drop_key(&(obj)->pelem)
 +
 +/* lookup an indexed object from index holding and not holding the pool_lock.
 + * takes a reference on object
 + */
 +void *rxe_pool_get_index_locked(struct rxe_pool *pool, u32 index);
 +
 +void *rxe_pool_get_index(struct rxe_pool *pool, u32 index);
 +
 +/* lookup keyed object from key holding and not holding the pool_lock.
 + * takes a reference on the objecti
 + */
 +void *rxe_pool_get_key_locked(struct rxe_pool *pool, void *key);
  
 -#define rxe_add_to_pool(pool, obj) __rxe_add_to_pool(pool, &(obj)->elem)
 +void *rxe_pool_get_key(struct rxe_pool *pool, void *key);
  
 +/* cleanup an object when all references are dropped */
 +void rxe_elem_release(struct kref *kref);
 +
 +/* take a reference on an object */
 +#define rxe_add_ref(elem) kref_get(&(elem)->pelem.ref_cnt)
 +
 +/* drop a reference on an object */
 +#define rxe_drop_ref(elem) kref_put(&(elem)->pelem.ref_cnt, rxe_elem_release)
++=======
+ /* lookup an indexed object from index. takes a reference on object */
+ void *rxe_pool_get_index(struct rxe_pool *pool, u32 index);
+ 
+ /* take a reference on an object */
+ int __rxe_get(struct rxe_pool_elem *elem);
+ 
+ #define rxe_add_ref(obj) __rxe_get(&(obj)->elem)
+ 
+ /* drop a reference on an object */
+ int __rxe_put(struct rxe_pool_elem *elem);
+ 
+ #define rxe_drop_ref(obj) __rxe_put(&(obj)->elem)
+ 
+ #define rxe_read_ref(obj) kref_read(&(obj)->elem.ref_cnt)
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  
  #endif /* RXE_POOL_H */
diff --cc drivers/infiniband/sw/rxe/rxe_verbs.c
index e2250e9584ce,f0c5715ac500..000000000000
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@@ -181,8 -181,7 +181,12 @@@ static int rxe_create_ah(struct ib_ah *
  		return err;
  
  	/* create index > 0 */
++<<<<<<< HEAD
 +	rxe_add_index(ah);
 +	ah->ah_num = ah->pelem.index;
++=======
+ 	ah->ah_num = ah->elem.index;
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  
  	if (uresp) {
  		/* only if new user provider */
@@@ -453,20 -431,19 +455,32 @@@ static struct ib_qp *rxe_create_qp(stru
  		qp->is_user = false;
  	}
  
++<<<<<<< HEAD
 +	rxe_add_index(qp);
 +
 +	err = rxe_qp_from_init(rxe, qp, pd, init, uresp, ibpd, udata);
++=======
+ 	err = rxe_add_to_pool(&rxe->qp_pool, qp);
+ 	if (err)
+ 		return err;
+ 
+ 	err = rxe_qp_from_init(rxe, qp, pd, init, uresp, ibqp->pd, udata);
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	if (err)
 -		goto qp_init;
 +		goto err3;
  
 -	return 0;
 +	return &qp->ibqp;
  
++<<<<<<< HEAD
 +err3:
 +	rxe_drop_index(qp);
 +err2:
++=======
+ qp_init:
++>>>>>>> 3225717f6dfa (RDMA/rxe: Replace red-black trees by xarrays)
  	rxe_drop_ref(qp);
 -	return err;
 +err1:
 +	return ERR_PTR(err);
  }
  
  static int rxe_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
@@@ -512,9 -489,13 +526,8 @@@ static int rxe_query_qp(struct ib_qp *i
  static int rxe_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata)
  {
  	struct rxe_qp *qp = to_rqp(ibqp);
 -	int ret;
 -
 -	ret = rxe_qp_chk_destroy(qp);
 -	if (ret)
 -		return ret;
  
  	rxe_qp_destroy(qp);
- 	rxe_drop_index(qp);
  	rxe_drop_ref(qp);
  	return 0;
  }
* Unmerged path drivers/infiniband/sw/rxe/rxe.c
diff --git a/drivers/infiniband/sw/rxe/rxe_mr.c b/drivers/infiniband/sw/rxe/rxe_mr.c
index 8d20900dbfc6..ee2cdd40cfbb 100644
--- a/drivers/infiniband/sw/rxe/rxe_mr.c
+++ b/drivers/infiniband/sw/rxe/rxe_mr.c
@@ -691,7 +691,6 @@ int rxe_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
 
 	mr->state = RXE_MR_STATE_INVALID;
 	rxe_drop_ref(mr_pd(mr));
-	rxe_drop_index(mr);
 	rxe_drop_ref(mr);
 
 	return 0;
* Unmerged path drivers/infiniband/sw/rxe/rxe_mw.c
* Unmerged path drivers/infiniband/sw/rxe/rxe_pool.c
* Unmerged path drivers/infiniband/sw/rxe/rxe_pool.h
* Unmerged path drivers/infiniband/sw/rxe/rxe_verbs.c
