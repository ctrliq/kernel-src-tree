sched: Make scheduler_ipi inline

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 2a0a24ebb499c9d499eea948d3fc108f936e36d4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/2a0a24eb.failed

Now that the scheduler IPI is trivial and simple again there is no point to
have the little function out of line. This simplifies the effort of
constraining the instrumentation nicely.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
	Acked-by: Peter Zijlstra <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20200505134058.453581595@linutronix.de
(cherry picked from commit 2a0a24ebb499c9d499eea948d3fc108f936e36d4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched.h
#	kernel/sched/core.c
diff --cc include/linux/sched.h
index 6d61d76e88d8,d4ea4407cd6d..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -1853,11 -1715,19 +1853,24 @@@ extern char *__get_task_comm(char *to, 
  })
  
  #ifdef CONFIG_SMP
++<<<<<<< HEAD
 +void scheduler_ipi(void);
 +extern unsigned long wait_task_inactive(struct task_struct *, unsigned int match_state);
++=======
+ static __always_inline void scheduler_ipi(void)
+ {
+ 	/*
+ 	 * Fold TIF_NEED_RESCHED into the preempt_count; anybody setting
+ 	 * TIF_NEED_RESCHED remotely (for the first time) will also send
+ 	 * this IPI.
+ 	 */
+ 	preempt_fold_need_resched();
+ }
+ extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
++>>>>>>> 2a0a24ebb499 (sched: Make scheduler_ipi inline)
  #else
  static inline void scheduler_ipi(void) { }
 -static inline unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 +static inline unsigned long wait_task_inactive(struct task_struct *p, unsigned int match_state)
  {
  	return 1;
  }
diff --cc kernel/sched/core.c
index 2305d5f0d4b2,74fb89b5ce3e..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -2412,23 -2312,7 +2412,27 @@@ static void wake_csd_func(void *info
  	sched_ttwu_pending();
  }
  
++<<<<<<< HEAD
 +void scheduler_ipi(void)
 +{
 +	/*
 +	 * Fold TIF_NEED_RESCHED into the preempt_count; anybody setting
 +	 * TIF_NEED_RESCHED remotely (for the first time) will also send
 +	 * this IPI.
 +	 */
 +	preempt_fold_need_resched();
 +}
 +
 +/*
 + * Queue a task on the target CPUs wake_list and wake the CPU via IPI if
 + * necessary. The wakee CPU on receipt of the IPI will queue the task
 + * via sched_ttwu_wakeup() for activation so the wakee incurs the cost
 + * of the wakeup instead of the waker.
 + */
 +static void __ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
++=======
+ static void ttwu_queue_remote(struct task_struct *p, int cpu, int wake_flags)
++>>>>>>> 2a0a24ebb499 (sched: Make scheduler_ipi inline)
  {
  	struct rq *rq = cpu_rq(cpu);
  
* Unmerged path include/linux/sched.h
* Unmerged path kernel/sched/core.c
