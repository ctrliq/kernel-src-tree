PM: clk: remove kernel-doc warning

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Pierre-Louis Bossart <pierre-louis.bossart@linux.intel.com>
commit 33b688e3854d6ad76d0acbeebc601ce2ddb8513d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/33b688e3.failed

Remove make W=1 warning:

drivers/base/power/clock_ops.c:148: warning: expecting prototype for
pm_clk_enable(). Prototype was for __pm_clk_enable() instead

	Signed-off-by: Pierre-Louis Bossart <pierre-louis.bossart@linux.intel.com>
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 33b688e3854d6ad76d0acbeebc601ce2ddb8513d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/base/power/clock_ops.c
diff --cc drivers/base/power/clock_ops.c
index 1a90708a0ffb,0251f3e6e61d..000000000000
--- a/drivers/base/power/clock_ops.c
+++ b/drivers/base/power/clock_ops.c
@@@ -36,7 -36,111 +36,114 @@@ struct pm_clock_entry 
  };
  
  /**
++<<<<<<< HEAD
 + * pm_clk_enable - Enable a clock, reporting any errors
++=======
+  * pm_clk_list_lock - ensure exclusive access for modifying the PM clock
+  *		      entry list.
+  * @psd: pm_subsys_data instance corresponding to the PM clock entry list
+  *	 and clk_op_might_sleep count to be modified.
+  *
+  * Get exclusive access before modifying the PM clock entry list and the
+  * clock_op_might_sleep count to guard against concurrent modifications.
+  * This also protects against a concurrent clock_op_might_sleep and PM clock
+  * entry list usage in pm_clk_suspend()/pm_clk_resume() that may or may not
+  * happen in atomic context, hence both the mutex and the spinlock must be
+  * taken here.
+  */
+ static void pm_clk_list_lock(struct pm_subsys_data *psd)
+ 	__acquires(&psd->lock)
+ {
+ 	mutex_lock(&psd->clock_mutex);
+ 	spin_lock_irq(&psd->lock);
+ }
+ 
+ /**
+  * pm_clk_list_unlock - counterpart to pm_clk_list_lock().
+  * @psd: the same pm_subsys_data instance previously passed to
+  *	 pm_clk_list_lock().
+  */
+ static void pm_clk_list_unlock(struct pm_subsys_data *psd)
+ 	__releases(&psd->lock)
+ {
+ 	spin_unlock_irq(&psd->lock);
+ 	mutex_unlock(&psd->clock_mutex);
+ }
+ 
+ /**
+  * pm_clk_op_lock - ensure exclusive access for performing clock operations.
+  * @psd: pm_subsys_data instance corresponding to the PM clock entry list
+  *	 and clk_op_might_sleep count being used.
+  * @flags: stored irq flags.
+  * @fn: string for the caller function's name.
+  *
+  * This is used by pm_clk_suspend() and pm_clk_resume() to guard
+  * against concurrent modifications to the clock entry list and the
+  * clock_op_might_sleep count. If clock_op_might_sleep is != 0 then
+  * only the mutex can be locked and those functions can only be used in
+  * non atomic context. If clock_op_might_sleep == 0 then these functions
+  * may be used in any context and only the spinlock can be locked.
+  * Returns -EINVAL if called in atomic context when clock ops might sleep.
+  */
+ static int pm_clk_op_lock(struct pm_subsys_data *psd, unsigned long *flags,
+ 			  const char *fn)
+ 	/* sparse annotations don't work here as exit state isn't static */
+ {
+ 	bool atomic_context = in_atomic() || irqs_disabled();
+ 
+ try_again:
+ 	spin_lock_irqsave(&psd->lock, *flags);
+ 	if (!psd->clock_op_might_sleep) {
+ 		/* the __release is there to work around sparse limitations */
+ 		__release(&psd->lock);
+ 		return 0;
+ 	}
+ 
+ 	/* bail out if in atomic context */
+ 	if (atomic_context) {
+ 		pr_err("%s: atomic context with clock_ops_might_sleep = %d",
+ 		       fn, psd->clock_op_might_sleep);
+ 		spin_unlock_irqrestore(&psd->lock, *flags);
+ 		might_sleep();
+ 		return -EPERM;
+ 	}
+ 
+ 	/* we must switch to the mutex */
+ 	spin_unlock_irqrestore(&psd->lock, *flags);
+ 	mutex_lock(&psd->clock_mutex);
+ 
+ 	/*
+ 	 * There was a possibility for psd->clock_op_might_sleep
+ 	 * to become 0 above. Keep the mutex only if not the case.
+ 	 */
+ 	if (likely(psd->clock_op_might_sleep))
+ 		return 0;
+ 
+ 	mutex_unlock(&psd->clock_mutex);
+ 	goto try_again;
+ }
+ 
+ /**
+  * pm_clk_op_unlock - counterpart to pm_clk_op_lock().
+  * @psd: the same pm_subsys_data instance previously passed to
+  *	 pm_clk_op_lock().
+  * @flags: irq flags provided by pm_clk_op_lock().
+  */
+ static void pm_clk_op_unlock(struct pm_subsys_data *psd, unsigned long *flags)
+ 	/* sparse annotations don't work here as entry state isn't static */
+ {
+ 	if (psd->clock_op_might_sleep) {
+ 		mutex_unlock(&psd->clock_mutex);
+ 	} else {
+ 		/* the __acquire is there to work around sparse limitations */
+ 		__acquire(&psd->lock);
+ 		spin_unlock_irqrestore(&psd->lock, *flags);
+ 	}
+ }
+ 
+ /**
+  * __pm_clk_enable - Enable a clock, reporting any errors
++>>>>>>> 33b688e3854d (PM: clk: remove kernel-doc warning)
   * @dev: The device for the given clock
   * @ce: PM clock entry corresponding to the clock.
   */
* Unmerged path drivers/base/power/clock_ops.c
