bpf, cgroup: Assign cgroup in cgroup_sk_alloc when called from interrupt

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Daniel Borkmann <daniel@iogearbox.net>
commit 78cc316e9583067884eb8bd154301dc1e9ee945c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/78cc316e.failed

If cgroup_sk_alloc() is called from interrupt context, then just assign the
root cgroup to skcd->cgroup. Prior to commit 8520e224f547 ("bpf, cgroups:
Fix cgroup v2 fallback on v1/v2 mixed mode") we would just return, and later
on in sock_cgroup_ptr(), we were NULL-testing the cgroup in fast-path, and
iff indeed NULL returning the root cgroup (v ?: &cgrp_dfl_root.cgrp). Rather
than re-adding the NULL-test to the fast-path we can just assign it once from
cgroup_sk_alloc() given v1/v2 handling has been simplified. The migration from
NULL test with returning &cgrp_dfl_root.cgrp to assigning &cgrp_dfl_root.cgrp
directly does /not/ change behavior for callers of sock_cgroup_ptr().

syzkaller was able to trigger a splat in the legacy netrom code base, where
the RX handler in nr_rx_frame() calls nr_make_new() which calls sk_alloc()
and therefore cgroup_sk_alloc() with in_interrupt() condition. Thus the NULL
skcd->cgroup, where it trips over on cgroup_sk_free() side given it expects
a non-NULL object. There are a few other candidates aside from netrom which
have similar pattern where in their accept-like implementation, they just call
to sk_alloc() and thus cgroup_sk_alloc() instead of sk_clone_lock() with the
corresponding cgroup_sk_clone() which then inherits the cgroup from the parent
socket. None of them are related to core protocols where BPF cgroup programs
are running from. However, in future, they should follow to implement a similar
inheritance mechanism.

Additionally, with a !CONFIG_CGROUP_NET_PRIO and !CONFIG_CGROUP_NET_CLASSID
configuration, the same issue was exposed also prior to 8520e224f547 due to
commit e876ecc67db8 ("cgroup: memcg: net: do not associate sock with unrelated
cgroup") which added the early in_interrupt() return back then.

Fixes: 8520e224f547 ("bpf, cgroups: Fix cgroup v2 fallback on v1/v2 mixed mode")
Fixes: e876ecc67db8 ("cgroup: memcg: net: do not associate sock with unrelated cgroup")
	Reported-by: syzbot+df709157a4ecaf192b03@syzkaller.appspotmail.com
	Reported-by: syzbot+533f389d4026d86a2a95@syzkaller.appspotmail.com
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
	Tested-by: syzbot+df709157a4ecaf192b03@syzkaller.appspotmail.com
	Tested-by: syzbot+533f389d4026d86a2a95@syzkaller.appspotmail.com
	Acked-by: Tejun Heo <tj@kernel.org>
Link: https://lore.kernel.org/bpf/20210927123921.21535-1-daniel@iogearbox.net
(cherry picked from commit 78cc316e9583067884eb8bd154301dc1e9ee945c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cgroup/cgroup.c
diff --cc kernel/cgroup/cgroup.c
index 81aa9d14a344,570b0c97392a..000000000000
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@@ -6348,50 -6572,31 +6348,72 @@@ int cgroup_parse_float(const char *inpu
   */
  #ifdef CONFIG_SOCK_CGROUP_DATA
  
 +#if defined(CONFIG_CGROUP_NET_PRIO) || defined(CONFIG_CGROUP_NET_CLASSID)
 +
 +DEFINE_SPINLOCK(cgroup_sk_update_lock);
 +static bool cgroup_sk_alloc_disabled __read_mostly;
 +
 +void cgroup_sk_alloc_disable(void)
 +{
 +	if (cgroup_sk_alloc_disabled)
 +		return;
 +	pr_info("cgroup: disabling cgroup2 socket matching due to net_prio or net_cls activation\n");
 +	cgroup_sk_alloc_disabled = true;
 +}
 +
 +#else
 +
 +#define cgroup_sk_alloc_disabled	false
 +
 +#endif
 +
  void cgroup_sk_alloc(struct sock_cgroup_data *skcd)
  {
++<<<<<<< HEAD
 +	if (cgroup_sk_alloc_disabled) {
 +		skcd->no_refcnt = 1;
 +		return;
 +	}
 +
 +	/* Don't associate the sock with unrelated interrupted task's cgroup. */
 +	if (in_interrupt())
 +		return;
 +
 +	rcu_read_lock();
++=======
+ 	struct cgroup *cgroup;
+ 
+ 	rcu_read_lock();
+ 	/* Don't associate the sock with unrelated interrupted task's cgroup. */
+ 	if (in_interrupt()) {
+ 		cgroup = &cgrp_dfl_root.cgrp;
+ 		cgroup_get(cgroup);
+ 		goto out;
+ 	}
++>>>>>>> 78cc316e9583 (bpf, cgroup: Assign cgroup in cgroup_sk_alloc when called from interrupt)
  
  	while (true) {
  		struct css_set *cset;
  
  		cset = task_css_set(current);
  		if (likely(cgroup_tryget(cset->dfl_cgrp))) {
++<<<<<<< HEAD
 +			skcd->val = (unsigned long)cset->dfl_cgrp;
 +			cgroup_bpf_get(cset->dfl_cgrp);
++=======
+ 			cgroup = cset->dfl_cgrp;
++>>>>>>> 78cc316e9583 (bpf, cgroup: Assign cgroup in cgroup_sk_alloc when called from interrupt)
  			break;
  		}
  		cpu_relax();
  	}
++<<<<<<< HEAD
 +
++=======
+ out:
+ 	skcd->cgroup = cgroup;
+ 	cgroup_bpf_get(cgroup);
++>>>>>>> 78cc316e9583 (bpf, cgroup: Assign cgroup in cgroup_sk_alloc when called from interrupt)
  	rcu_read_unlock();
  }
  
* Unmerged path kernel/cgroup/cgroup.c
