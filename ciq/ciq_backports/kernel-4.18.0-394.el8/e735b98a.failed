arm64: Add tlbi_user_level TLB invalidation helper

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Zhenyu Ye <yezhenyu2@huawei.com>
commit e735b98a5fe08c0f50f9fdc3e3a844e3638e6649
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/e735b98a.failed

Add a level-hinted parameter to __tlbi_user, which only gets used
if ARMv8.4-TTL gets detected.

ARMv8.4-TTL provides the TTL field in tlbi instruction to indicate
the level of translation table walk holding the leaf entry for the
address that is being invalidated.

This patch set the default level value of flush_tlb_range() to 0,
which will be updated in future patches.  And set the ttl value of
flush_tlb_page_nosync() to 3 because it is only called to flush a
single pte page.

	Signed-off-by: Zhenyu Ye <yezhenyu2@huawei.com>
Link: https://lore.kernel.org/r/20200625080314.230-4-yezhenyu2@huawei.com
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit e735b98a5fe08c0f50f9fdc3e3a844e3638e6649)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/tlbflush.h
diff --cc arch/arm64/include/asm/tlbflush.h
index 60446bc6b633,e1d07612e147..000000000000
--- a/arch/arm64/include/asm/tlbflush.h
+++ b/arch/arm64/include/asm/tlbflush.h
@@@ -390,68 -243,23 +396,79 @@@ static inline void __flush_tlb_range(st
  		return;
  	}
  
 -	/* Convert the stride into units of 4k */
 -	stride >>= 12;
 -
 -	start = __TLBI_VADDR(start, asid);
 -	end = __TLBI_VADDR(end, asid);
 -
 +	flush = tlb_flush_check(mm, get_cpu());
 +	switch (flush) {
 +	case TLB_FLUSH_LOCAL:
 +		stride >>= PAGE_SHIFT;
 +
 +		start = __TLBI_VADDR(start, asid);
 +		end = __TLBI_VADDR(end, asid);
 +
++<<<<<<< HEAD
 +		dsb(nshst);
 +		for (addr = start; addr < end; addr += stride) {
 +			if (last_level) {
 +				__tlbi(vale1, addr);
 +				__tlbi_user(vale1, addr);
 +			} else {
 +				__tlbi(vae1, addr);
 +				__tlbi_user(vae1, addr);
 +			}
++=======
+ 	dsb(ishst);
+ 	for (addr = start; addr < end; addr += stride) {
+ 		if (last_level) {
+ 			__tlbi_level(vale1is, addr, 0);
+ 			__tlbi_user_level(vale1is, addr, 0);
+ 		} else {
+ 			__tlbi_level(vae1is, addr, 0);
+ 			__tlbi_user_level(vae1is, addr, 0);
++>>>>>>> e735b98a5fe0 (arm64: Add tlbi_user_level TLB invalidation helper)
 +		}
 +		dsb(nsh);
 +
 +		/* fall through */
 +	case TLB_FLUSH_NO:
 +		put_cpu();
 +		break;
 +	case TLB_FLUSH_BROADCAST:
 +		put_cpu();
 +
 +		dsb(ishst);
 +		while (pages > 0) {
 +			if (!system_supports_tlb_range() ||
 +			    pages % 2 == 1) {
 +			        addr = __TLBI_VADDR(start, asid);
 +				if (last_level) {
 +					__tlbi(vale1is, addr);
 +					__tlbi_user(vale1is, addr);
 +				} else {
 +					__tlbi(vae1is, addr);
 +					__tlbi_user(vae1is, addr);
 +				}
 +				start += stride;
 +				pages -= stride >> PAGE_SHIFT;
 +			} else {
 +				num = __TLBI_RANGE_NUM(pages, scale);
 +				if (num >= 0) {
 +					addr = __TLBI_VADDR_RANGE(start, asid, scale,
 +								  num, 0);
 +					if (last_level) {
 +						 __tlbi(rvale1is, addr);
 +						 __tlbi_user(rvale1is, addr);
 +					} else {
 +						__tlbi(rvae1is, addr);
 +						__tlbi_user(rvae1is, addr);
 +					}
 +					start += __TLBI_RANGE_PAGES(num, scale) << PAGE_SHIFT;
 +					pages -= __TLBI_RANGE_PAGES(num, scale);
 +				}
 +				scale++;
 +			}
  		}
 +		dsb(ish);
 +		break;
  	}
 -	dsb(ish);
  }
  
  static inline void flush_tlb_range(struct vm_area_struct *vma,
* Unmerged path arch/arm64/include/asm/tlbflush.h
