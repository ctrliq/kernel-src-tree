arm64: remove CONFIG_DEBUG_ALIGN_RODATA feature

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Ard Biesheuvel <ardb@kernel.org>
commit e16e65a02913d29a7b27c4e3a415ceec967b0629
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/e16e65a0.failed

When CONFIG_DEBUG_ALIGN_RODATA is enabled, kernel segments mapped with
different permissions (r-x for .text, r-- for .rodata, rw- for .data,
etc) are rounded up to 2 MiB so they can be mapped more efficiently.
In particular, it permits the segments to be mapped using level 2
block entries when using 4k pages, which is expected to result in less
TLB pressure.

However, the mappings for the bulk of the kernel will use level 2
entries anyway, and the misaligned fringes are organized such that they
can take advantage of the contiguous bit, and use far fewer level 3
entries than would be needed otherwise.

This makes the value of this feature dubious at best, and since it is not
enabled in defconfig or in the distro configs, it does not appear to be
in wide use either. So let's just remove it.

	Signed-off-by: Ard Biesheuvel <ardb@kernel.org>
	Acked-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Will Deacon <will@kernel.org>
	Acked-by: Laura Abbott <labbott@kernel.org>
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit e16e65a02913d29a7b27c4e3a415ceec967b0629)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/memory.h
diff --cc arch/arm64/include/asm/memory.h
index 6d1182b5d477,a1871bb32bb1..000000000000
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@@ -139,22 -120,12 +139,30 @@@
  
  /*
   * Alignment of kernel segments (e.g. .text, .data).
++<<<<<<< HEAD
 + */
 +#if defined(CONFIG_DEBUG_ALIGN_RODATA)
 +/*
 + *  4 KB granule:   1 level 2 entry
 + * 16 KB granule: 128 level 3 entries, with contiguous bit
 + * 64 KB granule:  32 level 3 entries, with contiguous bit
 + */
 +#define SEGMENT_ALIGN			SZ_2M
 +#else
 +/*
++=======
+  *
++>>>>>>> e16e65a02913 (arm64: remove CONFIG_DEBUG_ALIGN_RODATA feature)
   *  4 KB granule:  16 level 3 entries, with contiguous bit
   * 16 KB granule:   4 level 3 entries, without contiguous bit
   * 64 KB granule:   1 level 3 entry
   */
++<<<<<<< HEAD
 +#define SEGMENT_ALIGN			SZ_64K
 +#endif
++=======
+ #define SEGMENT_ALIGN		SZ_64K
++>>>>>>> e16e65a02913 (arm64: remove CONFIG_DEBUG_ALIGN_RODATA feature)
  
  /*
   * Memory types available.
diff --git a/arch/arm64/Kconfig.debug b/arch/arm64/Kconfig.debug
index 29d6bb9da9ec..488f30eafd15 100644
--- a/arch/arm64/Kconfig.debug
+++ b/arch/arm64/Kconfig.debug
@@ -54,19 +54,6 @@ config DEBUG_WX
 
 	  If in doubt, say "Y".
 
-config DEBUG_ALIGN_RODATA
-	depends on STRICT_KERNEL_RWX
-	bool "Align linker sections up to SECTION_SIZE"
-	help
-	  If this option is enabled, sections that may potentially be marked as
-	  read only or non-executable will be aligned up to the section size of
-	  the kernel. This prevents sections from being split into pages and
-	  avoids a potential TLB penalty. The downside is an increase in
-	  alignment and potentially wasted space. Turn on this option if
-	  performance is more important than memory pressure.
-
-	  If in doubt, say N.
-
 config DEBUG_EFI
 	depends on EFI && DEBUG_INFO
 	bool "UEFI debugging"
* Unmerged path arch/arm64/include/asm/memory.h
diff --git a/drivers/firmware/efi/libstub/arm64-stub.c b/drivers/firmware/efi/libstub/arm64-stub.c
index 1be62596b1c7..80927ef51bdc 100644
--- a/drivers/firmware/efi/libstub/arm64-stub.c
+++ b/drivers/firmware/efi/libstub/arm64-stub.c
@@ -89,14 +89,12 @@ efi_status_t handle_kernel_image(efi_system_table_t *sys_table_arg,
 
 	if (IS_ENABLED(CONFIG_RANDOMIZE_BASE) && phys_seed != 0) {
 		/*
-		 * If CONFIG_DEBUG_ALIGN_RODATA is not set, produce a
-		 * displacement in the interval [0, MIN_KIMG_ALIGN) that
-		 * doesn't violate this kernel's de-facto alignment
+		 * Produce a displacement in the interval [0, MIN_KIMG_ALIGN)
+		 * that doesn't violate this kernel's de-facto alignment
 		 * constraints.
 		 */
 		u32 mask = (MIN_KIMG_ALIGN - 1) & ~(EFI_KIMG_ALIGN - 1);
-		u32 offset = !IS_ENABLED(CONFIG_DEBUG_ALIGN_RODATA) ?
-			     (phys_seed >> 32) & mask : TEXT_OFFSET;
+		u32 offset = (phys_seed >> 32) & mask;
 
 		/*
 		 * With CONFIG_RANDOMIZE_TEXT_OFFSET=y, TEXT_OFFSET may not
