arm64: Basic Branch Target Identification support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Dave Martin <Dave.Martin@arm.com>
commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/8ef8f360.failed

This patch adds the bare minimum required to expose the ARMv8.5
Branch Target Identification feature to userspace.

By itself, this does _not_ automatically enable BTI for any initial
executable pages mapped by execve().  This will come later, but for
now it should be possible to enable BTI manually on those pages by
using mprotect() from within the target process.

Other arches already using the generic mman.h are already using
0x10 for arch-specific prot flags, so we use that for PROT_BTI
here.

For consistency, signal handler entry points in BTI guarded pages
are required to be annotated as such, just like any other function.
This blocks a relatively minor attack vector, but comforming
userspace will have the annotations anyway, so we may as well
enforce them.

	Signed-off-by: Mark Brown <broonie@kernel.org>
	Signed-off-by: Dave Martin <Dave.Martin@arm.com>
	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/cpucaps.h
#	arch/arm64/include/asm/cpufeature.h
#	arch/arm64/include/asm/exception.h
#	arch/arm64/include/asm/ptrace.h
#	arch/arm64/include/asm/sysreg.h
#	arch/arm64/kernel/cpufeature.c
#	arch/arm64/kernel/entry-common.c
#	arch/arm64/kernel/ptrace.c
diff --cc arch/arm64/include/asm/cpucaps.h
index 7115f7a85078,58e776c22aab..000000000000
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@@ -61,18 -50,16 +61,32 @@@
  #define ARM64_HAS_GENERIC_AUTH_ARCH		40
  #define ARM64_HAS_GENERIC_AUTH_IMP_DEF		41
  #define ARM64_HAS_IRQ_PRIO_MASKING		42
++<<<<<<< HEAD
 +#define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	43
 +#define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	44
 +#define ARM64_WORKAROUND_1463225		45
 +#define ARM64_WORKAROUND_1542419		46
 +#define ARM64_HAS_32BIT_EL1			47
 +#define ARM64_WORKAROUND_NVIDIA_CARMEL_CNP	48
 +#define ARM64_HAS_ARMv8_4_TTL			49
 +#define ARM64_HAS_TLB_RANGE			50
 +#define ARM64_HAS_DCPODP			51
 +#define ARM64_HAS_E0PD				52
 +#define ARM64_HAS_RNG				53
 +
 +#define ARM64_NCAPS				54
++=======
+ #define ARM64_HAS_DCPODP			43
+ #define ARM64_WORKAROUND_1463225		44
+ #define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	45
+ #define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	46
+ #define ARM64_WORKAROUND_1542419		47
+ #define ARM64_WORKAROUND_SPECULATIVE_AT_NVHE	48
+ #define ARM64_HAS_E0PD				49
+ #define ARM64_HAS_RNG				50
+ #define ARM64_BTI				51
+ 
+ #define ARM64_NCAPS				52
++>>>>>>> 8ef8f360cf30 (arm64: Basic Branch Target Identification support)
  
  #endif /* __ASM_CPUCAPS_H */
diff --cc arch/arm64/include/asm/cpufeature.h
index 41fe5b1ea3d0,e3ebcc59e83b..000000000000
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@@ -682,6 -613,17 +682,20 @@@ static inline bool system_has_prio_mask
  	       system_uses_irq_prio_masking();
  }
  
++<<<<<<< HEAD
++=======
+ static inline bool system_supports_bti(void)
+ {
+ 	return IS_ENABLED(CONFIG_ARM64_BTI) &&
+ 		cpus_have_const_cap(ARM64_BTI);
+ }
+ 
+ static inline bool system_capabilities_finalized(void)
+ {
+ 	return static_branch_likely(&arm64_const_caps_ready);
+ }
+ 
++>>>>>>> 8ef8f360cf30 (arm64: Basic Branch Target Identification support)
  #define ARM64_BP_HARDEN_UNKNOWN		-1
  #define ARM64_BP_HARDEN_WA_NEEDED	0
  #define ARM64_BP_HARDEN_NOT_REQUIRED	1
diff --cc arch/arm64/include/asm/exception.h
index 55a461d97487,7577a754d443..000000000000
--- a/arch/arm64/include/asm/exception.h
+++ b/arch/arm64/include/asm/exception.h
@@@ -42,5 -32,19 +42,23 @@@ static inline u32 disr_to_esr(u64 disr
  }
  
  asmlinkage void enter_from_user_mode(void);
++<<<<<<< HEAD
 +
++=======
+ void do_mem_abort(unsigned long addr, unsigned int esr, struct pt_regs *regs);
+ void do_undefinstr(struct pt_regs *regs);
+ void do_bti(struct pt_regs *regs);
+ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr);
+ void do_debug_exception(unsigned long addr_if_watchpoint, unsigned int esr,
+ 			struct pt_regs *regs);
+ void do_fpsimd_acc(unsigned int esr, struct pt_regs *regs);
+ void do_sve_acc(unsigned int esr, struct pt_regs *regs);
+ void do_fpsimd_exc(unsigned int esr, struct pt_regs *regs);
+ void do_sysinstr(unsigned int esr, struct pt_regs *regs);
+ void do_sp_pc_abort(unsigned long addr, unsigned int esr, struct pt_regs *regs);
+ void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr);
+ void do_cp15instr(unsigned int esr, struct pt_regs *regs);
+ void do_el0_svc(struct pt_regs *regs);
+ void do_el0_svc_compat(struct pt_regs *regs);
++>>>>>>> 8ef8f360cf30 (arm64: Basic Branch Target Identification support)
  #endif	/* __ASM_EXCEPTION_H */
diff --cc arch/arm64/include/asm/ptrace.h
index c5faabef5314,2172ec7594ba..000000000000
--- a/arch/arm64/include/asm/ptrace.h
+++ b/arch/arm64/include/asm/ptrace.h
@@@ -43,23 -31,11 +43,27 @@@
   * interrupt disabling temporarily does not rely on IRQ priorities.
   */
  #define GIC_PRIO_IRQON			0xe0
 -#define GIC_PRIO_IRQOFF			(GIC_PRIO_IRQON & ~0x80)
 +#define __GIC_PRIO_IRQOFF		(GIC_PRIO_IRQON & ~0x80)
 +#define __GIC_PRIO_IRQOFF_NS		0xa0
  #define GIC_PRIO_PSR_I_SET		(1 << 4)
  
 +#define GIC_PRIO_IRQOFF							\
 +	({								\
 +		extern struct static_key_false gic_nonsecure_priorities;\
 +		u8 __prio = __GIC_PRIO_IRQOFF;				\
 +									\
 +		if (static_branch_unlikely(&gic_nonsecure_priorities))	\
 +			__prio = __GIC_PRIO_IRQOFF_NS;			\
 +									\
 +		__prio;							\
 +	})
 +
  /* Additional SPSR bits not exposed in the UABI */
++<<<<<<< HEAD
 +#define PSR_MODE_THREAD_BIT	(1 << 0)
++=======
+ 
++>>>>>>> 8ef8f360cf30 (arm64: Basic Branch Target Identification support)
  #define PSR_IL_BIT		(1 << 20)
  
  /* AArch32-specific ptrace requests */
diff --cc arch/arm64/include/asm/sysreg.h
index de824e1dcbd7,db08ceb4cc9a..000000000000
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@@ -609,32 -514,23 +609,49 @@@
  #endif
  
  /* SCTLR_EL1 specific flags. */
++<<<<<<< HEAD
 +#define SCTLR_EL1_ATA0		(BIT(42))
++=======
+ #define SCTLR_EL1_BT1		(BIT(36))
+ #define SCTLR_EL1_BT0		(BIT(35))
+ #define SCTLR_EL1_UCI		(BIT(26))
+ #define SCTLR_EL1_E0E		(BIT(24))
+ #define SCTLR_EL1_SPAN		(BIT(23))
+ #define SCTLR_EL1_NTWE		(BIT(18))
+ #define SCTLR_EL1_NTWI		(BIT(16))
+ #define SCTLR_EL1_UCT		(BIT(15))
+ #define SCTLR_EL1_DZE		(BIT(14))
+ #define SCTLR_EL1_UMA		(BIT(9))
+ #define SCTLR_EL1_SED		(BIT(8))
+ #define SCTLR_EL1_ITD		(BIT(7))
+ #define SCTLR_EL1_CP15BEN	(BIT(5))
+ #define SCTLR_EL1_SA0		(BIT(4))
 -
 -#define SCTLR_EL1_RES1	((BIT(11)) | (BIT(20)) | (BIT(22)) | (BIT(28)) | \
 -			 (BIT(29)))
++>>>>>>> 8ef8f360cf30 (arm64: Basic Branch Target Identification support)
 +
 +#define SCTLR_EL1_TCF0_SHIFT	38
 +#define SCTLR_EL1_TCF0_NONE	(UL(0x0) << SCTLR_EL1_TCF0_SHIFT)
 +#define SCTLR_EL1_TCF0_SYNC	(UL(0x1) << SCTLR_EL1_TCF0_SHIFT)
 +#define SCTLR_EL1_TCF0_ASYNC	(UL(0x2) << SCTLR_EL1_TCF0_SHIFT)
 +#define SCTLR_EL1_TCF0_MASK	(UL(0x3) << SCTLR_EL1_TCF0_SHIFT)
 +
 +#define SCTLR_EL1_UCI		(1 << 26)
 +#define SCTLR_EL1_E0E		(1 << 24)
 +#define SCTLR_EL1_SPAN		(1 << 23)
 +#define SCTLR_EL1_NTWE		(1 << 18)
 +#define SCTLR_EL1_NTWI		(1 << 16)
 +#define SCTLR_EL1_UCT		(1 << 15)
 +#define SCTLR_EL1_DZE		(1 << 14)
 +#define SCTLR_EL1_UMA		(1 << 9)
 +#define SCTLR_EL1_SED		(1 << 8)
 +#define SCTLR_EL1_ITD		(1 << 7)
 +#define SCTLR_EL1_CP15BEN	(1 << 5)
 +#define SCTLR_EL1_SA0		(1 << 4)
 +
 +#define SCTLR_EL1_RES1	((1 << 11) | (1 << 20) | (1 << 22) | (1 << 28) | \
 +			 (1 << 29))
 +#define SCTLR_EL1_RES0  ((1 << 6)  | (1 << 10) | (1 << 13) | (1 << 17) | \
 +			 (1 << 27) | (1 << 30) | (1 << 31) | \
 +			 (0xffffefffUL << 32))
  
  #ifdef CONFIG_CPU_BIG_ENDIAN
  #define ENDIAN_SET_EL1		(SCTLR_EL1_E0E | SCTLR_ELx_EE)
@@@ -750,11 -627,8 +767,12 @@@
  #define ID_AA64PFR1_SSBS_PSTATE_NI	0
  #define ID_AA64PFR1_SSBS_PSTATE_ONLY	1
  #define ID_AA64PFR1_SSBS_PSTATE_INSNS	2
+ #define ID_AA64PFR1_BT_BTI		0x1
  
 +#define ID_AA64PFR1_MTE_NI		0x0
 +#define ID_AA64PFR1_MTE_EL0		0x1
 +#define ID_AA64PFR1_MTE			0x2
 +
  /* id_aa64zfr0 */
  #define ID_AA64ZFR0_F64MM_SHIFT		56
  #define ID_AA64ZFR0_F32MM_SHIFT		52
diff --cc arch/arm64/kernel/cpufeature.c
index 23e372cfd69d,e6d31776e49b..000000000000
--- a/arch/arm64/kernel/cpufeature.c
+++ b/arch/arm64/kernel/cpufeature.c
@@@ -192,9 -178,9 +192,11 @@@ static const struct arm64_ftr_bits ftr_
  };
  
  static const struct arm64_ftr_bits ftr_id_aa64pfr1[] = {
 +	ARM64_FTR_BITS(FTR_HIDDEN, FTR_STRICT, FTR_LOWER_SAFE, ID_AA64PFR1_MPAMFRAC_SHIFT, 4, 0),
 +	ARM64_FTR_BITS(FTR_HIDDEN, FTR_STRICT, FTR_LOWER_SAFE, ID_AA64PFR1_RASFRAC_SHIFT, 4, 0),
  	ARM64_FTR_BITS(FTR_VISIBLE, FTR_STRICT, FTR_LOWER_SAFE, ID_AA64PFR1_SSBS_SHIFT, 4, ID_AA64PFR1_SSBS_PSTATE_NI),
+ 	ARM64_FTR_BITS(FTR_VISIBLE_IF_IS_ENABLED(CONFIG_ARM64_BTI),
+ 				    FTR_STRICT, FTR_LOWER_SAFE, ID_AA64PFR1_BT_SHIFT, 4, 0),
  	ARM64_FTR_END,
  };
  
@@@ -1384,24 -1349,20 +1386,41 @@@ static bool can_use_gic_priorities(cons
  }
  #endif
  
++<<<<<<< HEAD
 +/* Internal helper functions to match cpu capability type */
 +static bool
 +cpucap_late_cpu_optional(const struct arm64_cpu_capabilities *cap)
 +{
 +	return !!(cap->type & ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU);
 +}
 +
 +static bool
 +cpucap_late_cpu_permitted(const struct arm64_cpu_capabilities *cap)
 +{
 +	return !!(cap->type & ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU);
 +}
 +
 +static bool
 +cpucap_panic_on_conflict(const struct arm64_cpu_capabilities *cap)
 +{
 +	return !!(cap->type & ARM64_CPUCAP_PANIC_ON_CONFLICT);
 +}
++=======
+ #ifdef CONFIG_ARM64_BTI
+ static void bti_enable(const struct arm64_cpu_capabilities *__unused)
+ {
+ 	/*
+ 	 * Use of X16/X17 for tail-calls and trampolines that jump to
+ 	 * function entry points using BR is a requirement for
+ 	 * marking binaries with GNU_PROPERTY_AARCH64_FEATURE_1_BTI.
+ 	 * So, be strict and forbid other BRs using other registers to
+ 	 * jump onto a PACIxSP instruction:
+ 	 */
+ 	sysreg_clear_set(sctlr_el1, 0, SCTLR_EL1_BT0 | SCTLR_EL1_BT1);
+ 	isb();
+ }
+ #endif /* CONFIG_ARM64_BTI */
++>>>>>>> 8ef8f360cf30 (arm64: Basic Branch Target Identification support)
  
  static const struct arm64_cpu_capabilities arm64_features[] = {
  	{
diff --cc arch/arm64/kernel/ptrace.c
index e7125cb2c489,fd8ac7cf68e7..000000000000
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@@ -1757,8 -1873,8 +1757,13 @@@ void syscall_trace_exit(struct pt_regs 
   * We also reserve IL for the kernel; SS is handled dynamically.
   */
  #define SPSR_EL1_AARCH64_RES0_BITS \
++<<<<<<< HEAD
 +	(GENMASK_ULL(63, 32) | GENMASK_ULL(27, 26) | GENMASK_ULL(23, 22) | \
 +	 GENMASK_ULL(20, 13) | GENMASK_ULL(11, 10) | GENMASK_ULL(5, 5))
++=======
+ 	(GENMASK_ULL(63, 32) | GENMASK_ULL(27, 25) | GENMASK_ULL(23, 22) | \
+ 	 GENMASK_ULL(20, 13) | GENMASK_ULL(5, 5))
++>>>>>>> 8ef8f360cf30 (arm64: Basic Branch Target Identification support)
  #define SPSR_EL1_AARCH32_RES0_BITS \
  	(GENMASK_ULL(63, 32) | GENMASK_ULL(22, 22) | GENMASK_ULL(20, 20))
  
* Unmerged path arch/arm64/kernel/entry-common.c
diff --git a/Documentation/arm64/cpu-feature-registers.rst b/Documentation/arm64/cpu-feature-registers.rst
index 41937a8091aa..314fa5bc2655 100644
--- a/Documentation/arm64/cpu-feature-registers.rst
+++ b/Documentation/arm64/cpu-feature-registers.rst
@@ -176,6 +176,8 @@ infrastructure:
      +------------------------------+---------+---------+
      | SSBS                         | [7-4]   |    y    |
      +------------------------------+---------+---------+
+     | BT                           | [3-0]   |    y    |
+     +------------------------------+---------+---------+
 
 
   4) MIDR_EL1 - Main ID Register
diff --git a/Documentation/arm64/elf_hwcaps.rst b/Documentation/arm64/elf_hwcaps.rst
index 7dfb97dfe416..84a9fd2d41b4 100644
--- a/Documentation/arm64/elf_hwcaps.rst
+++ b/Documentation/arm64/elf_hwcaps.rst
@@ -236,6 +236,11 @@ HWCAP2_RNG
 
     Functionality implied by ID_AA64ISAR0_EL1.RNDR == 0b0001.
 
+HWCAP2_BTI
+
+    Functionality implied by ID_AA64PFR0_EL1.BT == 0b0001.
+
+
 4. Unused AT_HWCAP bits
 -----------------------
 
* Unmerged path arch/arm64/include/asm/cpucaps.h
* Unmerged path arch/arm64/include/asm/cpufeature.h
diff --git a/arch/arm64/include/asm/esr.h b/arch/arm64/include/asm/esr.h
index 15652b9b3461..8f60392eb8fa 100644
--- a/arch/arm64/include/asm/esr.h
+++ b/arch/arm64/include/asm/esr.h
@@ -33,7 +33,7 @@
 #define ESR_ELx_EC_PAC		(0x09)	/* EL2 and above */
 /* Unallocated EC: 0x0A - 0x0B */
 #define ESR_ELx_EC_CP14_64	(0x0C)
-/* Unallocated EC: 0x0d */
+#define ESR_ELx_EC_BTI		(0x0D)
 #define ESR_ELx_EC_ILL		(0x0E)
 /* Unallocated EC: 0x0F - 0x10 */
 #define ESR_ELx_EC_SVC32	(0x11)
* Unmerged path arch/arm64/include/asm/exception.h
diff --git a/arch/arm64/include/asm/hwcap.h b/arch/arm64/include/asm/hwcap.h
index 8793fb43c406..eeb9dae4a232 100644
--- a/arch/arm64/include/asm/hwcap.h
+++ b/arch/arm64/include/asm/hwcap.h
@@ -105,6 +105,7 @@
 #define KERNEL_HWCAP_BF16		__khwcap2_feature(BF16)
 #define KERNEL_HWCAP_DGH		__khwcap2_feature(DGH)
 #define KERNEL_HWCAP_RNG		__khwcap2_feature(RNG)
+#define KERNEL_HWCAP_BTI		__khwcap2_feature(BTI)
 
 /*
  * This yields a mask that user programs can use to figure out what
diff --git a/arch/arm64/include/asm/mman.h b/arch/arm64/include/asm/mman.h
new file mode 100644
index 000000000000..081ec8de9ea6
--- /dev/null
+++ b/arch/arm64/include/asm/mman.h
@@ -0,0 +1,37 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __ASM_MMAN_H__
+#define __ASM_MMAN_H__
+
+#include <linux/compiler.h>
+#include <linux/types.h>
+#include <uapi/asm/mman.h>
+
+static inline unsigned long arch_calc_vm_prot_bits(unsigned long prot,
+	unsigned long pkey __always_unused)
+{
+	if (system_supports_bti() && (prot & PROT_BTI))
+		return VM_ARM64_BTI;
+
+	return 0;
+}
+#define arch_calc_vm_prot_bits(prot, pkey) arch_calc_vm_prot_bits(prot, pkey)
+
+static inline pgprot_t arch_vm_get_page_prot(unsigned long vm_flags)
+{
+	return (vm_flags & VM_ARM64_BTI) ? __pgprot(PTE_GP) : __pgprot(0);
+}
+#define arch_vm_get_page_prot(vm_flags) arch_vm_get_page_prot(vm_flags)
+
+static inline bool arch_validate_prot(unsigned long prot,
+	unsigned long addr __always_unused)
+{
+	unsigned long supported = PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM;
+
+	if (system_supports_bti())
+		supported |= PROT_BTI;
+
+	return (prot & ~supported) == 0;
+}
+#define arch_validate_prot(prot, addr) arch_validate_prot(prot, addr)
+
+#endif /* ! __ASM_MMAN_H__ */
diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
index e91bf1f0c580..38a0282d7d11 100644
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@ -161,6 +161,7 @@
 #define PTE_SHARED		(_AT(pteval_t, 3) << 8)		/* SH[1:0], inner shareable */
 #define PTE_AF			(_AT(pteval_t, 1) << 10)	/* Access Flag */
 #define PTE_NG			(_AT(pteval_t, 1) << 11)	/* nG */
+#define PTE_GP			(_AT(pteval_t, 1) << 50)	/* BTI guarded */
 #define PTE_DBM			(_AT(pteval_t, 1) << 51)	/* Dirty Bit Management */
 #define PTE_CONT		(_AT(pteval_t, 1) << 52)	/* Contiguous range */
 #define PTE_PXN			(_AT(pteval_t, 1) << 53)	/* Privileged XN */
diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 4cf77be27a70..4214f9921da4 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -649,7 +649,7 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
-			      PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
+			      PTE_PROT_NONE | PTE_VALID | PTE_WRITE | PTE_GP;
 	/* preserve the hardware dirty information */
 	if (pte_hw_dirty(pte))
 		pte = pte_mkdirty(pte);
* Unmerged path arch/arm64/include/asm/ptrace.h
* Unmerged path arch/arm64/include/asm/sysreg.h
diff --git a/arch/arm64/include/uapi/asm/hwcap.h b/arch/arm64/include/uapi/asm/hwcap.h
index 7752d93bb50f..2d6ba1c2592e 100644
--- a/arch/arm64/include/uapi/asm/hwcap.h
+++ b/arch/arm64/include/uapi/asm/hwcap.h
@@ -73,5 +73,6 @@
 #define HWCAP2_BF16		(1 << 14)
 #define HWCAP2_DGH		(1 << 15)
 #define HWCAP2_RNG		(1 << 16)
+#define HWCAP2_BTI		(1 << 17)
 
 #endif /* _UAPI__ASM_HWCAP_H */
diff --git a/arch/arm64/include/uapi/asm/mman.h b/arch/arm64/include/uapi/asm/mman.h
new file mode 100644
index 000000000000..6fdd71eb644f
--- /dev/null
+++ b/arch/arm64/include/uapi/asm/mman.h
@@ -0,0 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+#ifndef _UAPI__ASM_MMAN_H
+#define _UAPI__ASM_MMAN_H
+
+#include <asm-generic/mman.h>
+
+#define PROT_BTI	0x10		/* BTI guarded page */
+
+#endif /* ! _UAPI__ASM_MMAN_H */
diff --git a/arch/arm64/include/uapi/asm/ptrace.h b/arch/arm64/include/uapi/asm/ptrace.h
index f02a3b1877b0..06413d9f2341 100644
--- a/arch/arm64/include/uapi/asm/ptrace.h
+++ b/arch/arm64/include/uapi/asm/ptrace.h
@@ -57,6 +57,8 @@
 #define PSR_Z_BIT	0x40000000
 #define PSR_N_BIT	0x80000000
 
+#define PSR_BTYPE_SHIFT		10
+
 /*
  * Groups of PSR bits
  */
@@ -65,6 +67,12 @@
 #define PSR_x		0x0000ff00	/* Extension		*/
 #define PSR_c		0x000000ff	/* Control		*/
 
+/* Convenience names for the values of PSTATE.BTYPE */
+#define PSR_BTYPE_NONE		(0b00 << PSR_BTYPE_SHIFT)
+#define PSR_BTYPE_JC		(0b01 << PSR_BTYPE_SHIFT)
+#define PSR_BTYPE_C		(0b10 << PSR_BTYPE_SHIFT)
+#define PSR_BTYPE_J		(0b11 << PSR_BTYPE_SHIFT)
+
 /* syscall emulation path in ptrace */
 #define PTRACE_SYSEMU		  31
 #define PTRACE_SYSEMU_SINGLESTEP  32
* Unmerged path arch/arm64/kernel/cpufeature.c
diff --git a/arch/arm64/kernel/cpuinfo.c b/arch/arm64/kernel/cpuinfo.c
index 91acc4fe92e1..24c32dc2e24e 100644
--- a/arch/arm64/kernel/cpuinfo.c
+++ b/arch/arm64/kernel/cpuinfo.c
@@ -102,6 +102,7 @@ static const char *const hwcap_str[] = {
 	"bf16",
 	"dgh",
 	"rng",
+	"bti",
 	NULL
 };
 
* Unmerged path arch/arm64/kernel/entry-common.c
* Unmerged path arch/arm64/kernel/ptrace.c
diff --git a/arch/arm64/kernel/signal.c b/arch/arm64/kernel/signal.c
index 1683335966b3..3afbcf92aa4f 100644
--- a/arch/arm64/kernel/signal.c
+++ b/arch/arm64/kernel/signal.c
@@ -743,6 +743,22 @@ static void setup_return(struct pt_regs *regs, struct k_sigaction *ka,
 	regs->regs[29] = (unsigned long)&user->next_frame->fp;
 	regs->pc = (unsigned long)ka->sa.sa_handler;
 
+	/*
+	 * Signal delivery is a (wacky) indirect function call in
+	 * userspace, so simulate the same setting of BTYPE as a BLR
+	 * <register containing the signal handler entry point>.
+	 * Signal delivery to a location in a PROT_BTI guarded page
+	 * that is not a function entry point will now trigger a
+	 * SIGILL in userspace.
+	 *
+	 * If the signal handler entry point is not in a PROT_BTI
+	 * guarded page, this is harmless.
+	 */
+	if (system_supports_bti()) {
+		regs->pstate &= ~PSR_BTYPE_MASK;
+		regs->pstate |= PSR_BTYPE_C;
+	}
+
 	if (ka->sa.sa_flags & SA_RESTORER)
 		sigtramp = ka->sa.sa_restorer;
 	else
diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 871c739f060a..8962e704d4c8 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -98,6 +98,24 @@ static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 	regs->orig_x0 = regs->regs[0];
 	regs->syscallno = scno;
 
+	/*
+	 * BTI note:
+	 * The architecture does not guarantee that SPSR.BTYPE is zero
+	 * on taking an SVC, so we could return to userspace with a
+	 * non-zero BTYPE after the syscall.
+	 *
+	 * This shouldn't matter except when userspace is explicitly
+	 * doing something stupid, such as setting PROT_BTI on a page
+	 * that lacks conforming BTI/PACIxSP instructions, falling
+	 * through from one executable page to another with differing
+	 * PROT_BTI, or messing with BTYPE via ptrace: in such cases,
+	 * userspace should not be surprised if a SIGILL occurs on
+	 * syscall return.
+	 *
+	 * So, don't touch regs->pstate & PSR_BTYPE_MASK here.
+	 * (Similarly for HVC and SMC elsewhere.)
+	 */
+
 	cortex_a76_erratum_1463225_svc_handler();
 	local_daif_restore(DAIF_PROCCTX);
 	user_exit();
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index ccfd884c601f..48c669fc0bec 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -415,6 +415,13 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	BUG_ON(!user_mode(regs));
 }
 
+void do_bti(struct pt_regs *regs)
+{
+	BUG_ON(!user_mode(regs));
+	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
+}
+NOKPROBE_SYMBOL(do_bti);
+
 #define __user_cache_maint(insn, address, res)			\
 	if (address >= user_addr_max()) {			\
 		res = -EFAULT;					\
@@ -755,6 +762,7 @@ static const char *esr_class_str[] = {
 	[ESR_ELx_EC_CP10_ID]		= "CP10 MRC/VMRS",
 	[ESR_ELx_EC_PAC]		= "PAC",
 	[ESR_ELx_EC_CP14_64]		= "CP14 MCRR/MRRC",
+	[ESR_ELx_EC_BTI]		= "BTI",
 	[ESR_ELx_EC_ILL]		= "PSTATE.IL",
 	[ESR_ELx_EC_SVC32]		= "SVC (AArch32)",
 	[ESR_ELx_EC_HVC32]		= "HVC (AArch32)",
diff --git a/include/linux/mm.h b/include/linux/mm.h
index c6cb083cabb0..e40f448ad105 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -292,6 +292,9 @@ extern unsigned int kobjsize(const void *objp);
 #elif defined(CONFIG_SPARC64)
 # define VM_SPARC_ADI	VM_ARCH_1	/* Uses ADI tag for access control */
 # define VM_ARCH_CLEAR	VM_SPARC_ADI
+#elif defined(CONFIG_ARM64)
+# define VM_ARM64_BTI	VM_ARCH_1	/* BTI guarded page, a.k.a. GP bit */
+# define VM_ARCH_CLEAR	VM_ARM64_BTI
 #elif !defined(CONFIG_MMU)
 # define VM_MAPPED_COPY	VM_ARCH_1	/* T if mapped copy of data (nommu mmap) */
 #endif
