RDMA: Constify netdev->dev_addr accesses

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Jakub Kicinski <kuba@kernel.org>
commit fd92213e9af3b8e5cb3b4d3bf925c9baafb46c9e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/fd92213e.failed

netdev->dev_addr will become const soon, make sure drivers propagate the
qualifier.

Link: https://lore.kernel.org/r/20211019182604.1441387-4-kuba@kernel.org
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
	Reviewed-by: Leon Romanovsky <leonro@nvidia.com>
	Acked-by: Dennis Dalessandro <dennis.dalessandro@cornelisnetworks.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit fd92213e9af3b8e5cb3b4d3bf925c9baafb46c9e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hns/hns_roce_hw_v1.c
#	drivers/infiniband/hw/hns/hns_roce_hw_v2.c
#	drivers/infiniband/hw/hns/hns_roce_main.c
diff --cc drivers/infiniband/hw/hns/hns_roce_hw_v1.c
index 8d4e1d9dfa0c,f4af3992ba95..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_hw_v1.c
+++ b/drivers/infiniband/hw/hns/hns_roce_hw_v1.c
@@@ -69,16 -85,16 +69,25 @@@ static int hns_roce_v1_post_send(struc
  	struct hns_roce_wqe_data_seg *dseg = NULL;
  	struct hns_roce_qp *qp = to_hr_qp(ibqp);
  	struct device *dev = &hr_dev->pdev->dev;
 -	struct hns_roce_sq_db sq_db = {};
 -	int ps_opcode, i;
 +	struct hns_roce_sq_db sq_db;
 +	int ps_opcode = 0, i = 0;
  	unsigned long flags = 0;
  	void *wqe = NULL;
++<<<<<<< HEAD
 +	u32 doorbell[2];
 +	int nreq = 0;
 +	u32 ind = 0;
 +	int ret = 0;
 +	u8 *smac;
 +	int loopback;
++=======
+ 	__le32 doorbell[2];
+ 	const u8 *smac;
+ 	int ret = 0;
+ 	int loopback;
+ 	u32 wqe_idx;
+ 	int nreq;
++>>>>>>> fd92213e9af3 (RDMA: Constify netdev->dev_addr accesses)
  
  	if (unlikely(ibqp->qp_type != IB_QPT_GSI &&
  		ibqp->qp_type != IB_QPT_RC)) {
@@@ -2723,15 -2741,22 +2732,22 @@@ static int hns_roce_v1_m_qp(struct ib_q
  	dma_addr_t dma_handle_2 = 0;
  	dma_addr_t dma_handle = 0;
  	__le32 doorbell[2] = {0};
 +	int rq_pa_start = 0;
  	u64 *mtts_2 = NULL;
  	int ret = -EINVAL;
++<<<<<<< HEAD
 +	u64 *mtts = NULL;
 +	int port;
 +	u8 port_num;
++=======
+ 	const u8 *smac;
+ 	u64 sq_ba = 0;
+ 	u64 rq_ba = 0;
+ 	u32 port;
+ 	u32 port_num;
++>>>>>>> fd92213e9af3 (RDMA: Constify netdev->dev_addr accesses)
  	u8 *dmac;
- 	u8 *smac;
  
 -	if (!check_qp_state(cur_state, new_state)) {
 -		ibdev_err(ibqp->device,
 -			  "not support QP(%u) status from %d to %d\n",
 -			  ibqp->qp_num, cur_state, new_state);
 -		return -EINVAL;
 -	}
 -
  	context = kzalloc(sizeof(*context), GFP_KERNEL);
  	if (!context)
  		return -ENOMEM;
diff --cc drivers/infiniband/hw/hns/hns_roce_hw_v2.c
index b61a836a2647,c2916b170a41..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_hw_v2.c
+++ b/drivers/infiniband/hw/hns/hns_roce_hw_v2.c
@@@ -1605,159 -1314,244 +1605,159 @@@ static int hns_roce_v2_post_mbox(struc
  	return ret;
  }
  
 -static int hns_roce_cmq_send(struct hns_roce_dev *hr_dev,
 -			     struct hns_roce_cmq_desc *desc, int num)
 +static int hns_roce_v2_chk_mbox(struct hns_roce_dev *hr_dev,
 +				unsigned long timeout)
  {
 -	bool busy;
 -	int ret;
 +	struct device *dev = hr_dev->dev;
 +	unsigned long end = 0;
 +	u32 status;
  
 -	if (!v2_chk_mbox_is_avail(hr_dev, &busy))
 -		return busy ? -EBUSY : 0;
 +	end = msecs_to_jiffies(timeout) + jiffies;
 +	while (hns_roce_v2_cmd_pending(hr_dev) && time_before(jiffies, end))
 +		cond_resched();
  
 -	ret = __hns_roce_cmq_send(hr_dev, desc, num);
 -	if (ret) {
 -		if (!v2_chk_mbox_is_avail(hr_dev, &busy))
 -			return busy ? -EBUSY : 0;
 +	if (hns_roce_v2_cmd_pending(hr_dev)) {
 +		dev_err(dev, "[cmd_poll]hw run cmd TIMEDOUT!\n");
 +		return -ETIMEDOUT;
  	}
  
 -	return ret;
 -}
 -
 -static int config_hem_ba_to_hw(struct hns_roce_dev *hr_dev, unsigned long obj,
 -			       dma_addr_t base_addr, u16 op)
 -{
 -	struct hns_roce_cmd_mailbox *mbox = hns_roce_alloc_cmd_mailbox(hr_dev);
 -	int ret;
 -
 -	if (IS_ERR(mbox))
 -		return PTR_ERR(mbox);
 +	status = hns_roce_v2_cmd_complete(hr_dev);
 +	if (status != 0x1) {
 +		dev_err(dev, "mailbox status 0x%x!\n", status);
 +		return -EBUSY;
 +	}
  
 -	ret = hns_roce_cmd_mbox(hr_dev, base_addr, mbox->dma, obj, 0, op,
 -				HNS_ROCE_CMD_TIMEOUT_MSECS);
 -	hns_roce_free_cmd_mailbox(hr_dev, mbox);
 -	return ret;
 +	return 0;
  }
  
 -static int hns_roce_cmq_query_hw_info(struct hns_roce_dev *hr_dev)
 +static int hns_roce_config_sgid_table(struct hns_roce_dev *hr_dev,
 +				      int gid_index, const union ib_gid *gid,
 +				      enum hns_roce_sgid_type sgid_type)
  {
 -	struct hns_roce_query_version *resp;
  	struct hns_roce_cmq_desc desc;
 -	int ret;
 +	struct hns_roce_cfg_sgid_tb *sgid_tb =
 +				    (struct hns_roce_cfg_sgid_tb *)desc.data;
 +	u32 *p;
  
 -	hns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_QUERY_HW_VER, true);
 -	ret = hns_roce_cmq_send(hr_dev, &desc, 1);
 -	if (ret)
 -		return ret;
 +	hns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_SGID_TB, false);
  
 -	resp = (struct hns_roce_query_version *)desc.data;
 -	hr_dev->hw_rev = le16_to_cpu(resp->rocee_hw_version);
 -	hr_dev->vendor_id = hr_dev->pci_dev->vendor;
 -
 -	return 0;
 -}
 -
 -static void func_clr_hw_resetting_state(struct hns_roce_dev *hr_dev,
 -					struct hnae3_handle *handle)
 -{
 -	const struct hnae3_ae_ops *ops = handle->ae_algo->ops;
 -	unsigned long end;
 -
 -	hr_dev->dis_db = true;
 +	roce_set_field(sgid_tb->table_idx_rsv,
 +		       CFG_SGID_TB_TABLE_IDX_M,
 +		       CFG_SGID_TB_TABLE_IDX_S, gid_index);
 +	roce_set_field(sgid_tb->vf_sgid_type_rsv,
 +		       CFG_SGID_TB_VF_SGID_TYPE_M,
 +		       CFG_SGID_TB_VF_SGID_TYPE_S, sgid_type);
  
 -	dev_warn(hr_dev->dev,
 -		 "Func clear is pending, device in resetting state.\n");
 -	end = HNS_ROCE_V2_HW_RST_TIMEOUT;
 -	while (end) {
 -		if (!ops->get_hw_reset_stat(handle)) {
 -			hr_dev->is_reset = true;
 -			dev_info(hr_dev->dev,
 -				 "Func clear success after reset.\n");
 -			return;
 -		}
 -		msleep(HNS_ROCE_V2_HW_RST_COMPLETION_WAIT);
 -		end -= HNS_ROCE_V2_HW_RST_COMPLETION_WAIT;
 -	}
 +	p = (u32 *)&gid->raw[0];
 +	sgid_tb->vf_sgid_l = cpu_to_le32(*p);
  
 -	dev_warn(hr_dev->dev, "Func clear failed.\n");
 -}
 +	p = (u32 *)&gid->raw[4];
 +	sgid_tb->vf_sgid_ml = cpu_to_le32(*p);
  
 -static void func_clr_sw_resetting_state(struct hns_roce_dev *hr_dev,
 -					struct hnae3_handle *handle)
 -{
 -	const struct hnae3_ae_ops *ops = handle->ae_algo->ops;
 -	unsigned long end;
 +	p = (u32 *)&gid->raw[8];
 +	sgid_tb->vf_sgid_mh = cpu_to_le32(*p);
  
 -	hr_dev->dis_db = true;
 -
 -	dev_warn(hr_dev->dev,
 -		 "Func clear is pending, device in resetting state.\n");
 -	end = HNS_ROCE_V2_HW_RST_TIMEOUT;
 -	while (end) {
 -		if (ops->ae_dev_reset_cnt(handle) !=
 -		    hr_dev->reset_cnt) {
 -			hr_dev->is_reset = true;
 -			dev_info(hr_dev->dev,
 -				 "Func clear success after sw reset\n");
 -			return;
 -		}
 -		msleep(HNS_ROCE_V2_HW_RST_COMPLETION_WAIT);
 -		end -= HNS_ROCE_V2_HW_RST_COMPLETION_WAIT;
 -	}
 +	p = (u32 *)&gid->raw[0xc];
 +	sgid_tb->vf_sgid_h = cpu_to_le32(*p);
  
 -	dev_warn(hr_dev->dev, "Func clear failed because of unfinished sw reset\n");
 +	return hns_roce_cmq_send(hr_dev, &desc, 1);
  }
  
 -static void hns_roce_func_clr_rst_proc(struct hns_roce_dev *hr_dev, int retval,
 -				       int flag)
 +static int hns_roce_v2_set_gid(struct hns_roce_dev *hr_dev, u8 port,
 +			       int gid_index, const union ib_gid *gid,
 +			       const struct ib_gid_attr *attr)
  {
 -	struct hns_roce_v2_priv *priv = hr_dev->priv;
 -	struct hnae3_handle *handle = priv->handle;
 -	const struct hnae3_ae_ops *ops = handle->ae_algo->ops;
 +	enum hns_roce_sgid_type sgid_type = GID_TYPE_FLAG_ROCE_V1;
 +	int ret;
  
 -	if (ops->ae_dev_reset_cnt(handle) != hr_dev->reset_cnt) {
 -		hr_dev->dis_db = true;
 -		hr_dev->is_reset = true;
 -		dev_info(hr_dev->dev, "Func clear success after reset.\n");
 -		return;
 -	}
 +	if (!gid || !attr)
 +		return -EINVAL;
  
 -	if (ops->get_hw_reset_stat(handle)) {
 -		func_clr_hw_resetting_state(hr_dev, handle);
 -		return;
 -	}
 +	if (attr->gid_type == IB_GID_TYPE_ROCE)
 +		sgid_type = GID_TYPE_FLAG_ROCE_V1;
  
 -	if (ops->ae_dev_resetting(handle) &&
 -	    handle->rinfo.instance_state == HNS_ROCE_STATE_INIT) {
 -		func_clr_sw_resetting_state(hr_dev, handle);
 -		return;
 +	if (attr->gid_type == IB_GID_TYPE_ROCE_UDP_ENCAP) {
 +		if (ipv6_addr_v4mapped((void *)gid))
 +			sgid_type = GID_TYPE_FLAG_ROCE_V2_IPV4;
 +		else
 +			sgid_type = GID_TYPE_FLAG_ROCE_V2_IPV6;
  	}
  
 -	if (retval && !flag)
 -		dev_warn(hr_dev->dev,
 -			 "Func clear read failed, ret = %d.\n", retval);
 +	ret = hns_roce_config_sgid_table(hr_dev, gid_index, gid, sgid_type);
 +	if (ret)
 +		dev_err(hr_dev->dev, "Configure sgid table failed(%d)!\n", ret);
  
 -	dev_warn(hr_dev->dev, "Func clear failed.\n");
 +	return ret;
  }
  
 -static void __hns_roce_function_clear(struct hns_roce_dev *hr_dev, int vf_id)
 +static int hns_roce_v2_set_mac(struct hns_roce_dev *hr_dev, u8 phy_port,
- 			       u8 *addr)
++			       const u8 *addr)
  {
 -	bool fclr_write_fail_flag = false;
 -	struct hns_roce_func_clear *resp;
  	struct hns_roce_cmq_desc desc;
 -	unsigned long end;
 -	int ret = 0;
 -
 -	if (check_device_is_in_reset(hr_dev))
 -		goto out;
 -
 -	hns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_FUNC_CLEAR, false);
 -	resp = (struct hns_roce_func_clear *)desc.data;
 -	resp->rst_funcid_en = cpu_to_le32(vf_id);
 -
 -	ret = hns_roce_cmq_send(hr_dev, &desc, 1);
 -	if (ret) {
 -		fclr_write_fail_flag = true;
 -		dev_err(hr_dev->dev, "Func clear write failed, ret = %d.\n",
 -			 ret);
 -		goto out;
 -	}
 -
 -	msleep(HNS_ROCE_V2_READ_FUNC_CLEAR_FLAG_INTERVAL);
 -	end = HNS_ROCE_V2_FUNC_CLEAR_TIMEOUT_MSECS;
 -	while (end) {
 -		if (check_device_is_in_reset(hr_dev))
 -			goto out;
 -		msleep(HNS_ROCE_V2_READ_FUNC_CLEAR_FLAG_FAIL_WAIT);
 -		end -= HNS_ROCE_V2_READ_FUNC_CLEAR_FLAG_FAIL_WAIT;
 -
 -		hns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_FUNC_CLEAR,
 -					      true);
 +	struct hns_roce_cfg_smac_tb *smac_tb =
 +				    (struct hns_roce_cfg_smac_tb *)desc.data;
 +	u16 reg_smac_h;
 +	u32 reg_smac_l;
  
 -		resp->rst_funcid_en = cpu_to_le32(vf_id);
 -		ret = hns_roce_cmq_send(hr_dev, &desc, 1);
 -		if (ret)
 -			continue;
 +	hns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CFG_SMAC_TB, false);
  
 -		if (roce_get_bit(resp->func_done, FUNC_CLEAR_RST_FUN_DONE_S)) {
 -			if (vf_id == 0)
 -				hr_dev->is_reset = true;
 -			return;
 -		}
 -	}
 +	reg_smac_l = *(u32 *)(&addr[0]);
 +	reg_smac_h = *(u16 *)(&addr[4]);
  
 -out:
 -	hns_roce_func_clr_rst_proc(hr_dev, ret, fclr_write_fail_flag);
 -}
 +	memset(smac_tb, 0, sizeof(*smac_tb));
 +	roce_set_field(smac_tb->tb_idx_rsv,
 +		       CFG_SMAC_TB_IDX_M,
 +		       CFG_SMAC_TB_IDX_S, phy_port);
 +	roce_set_field(smac_tb->vf_smac_h_rsv,
 +		       CFG_SMAC_TB_VF_SMAC_H_M,
 +		       CFG_SMAC_TB_VF_SMAC_H_S, reg_smac_h);
 +	smac_tb->vf_smac_l = reg_smac_l;
  
 -static void hns_roce_free_vf_resource(struct hns_roce_dev *hr_dev, int vf_id)
 -{
 -	enum hns_roce_opcode_type opcode = HNS_ROCE_OPC_ALLOC_VF_RES;
 -	struct hns_roce_cmq_desc desc[2];
 -	struct hns_roce_cmq_req *req_a;
 -
 -	req_a = (struct hns_roce_cmq_req *)desc[0].data;
 -	hns_roce_cmq_setup_basic_desc(&desc[0], opcode, false);
 -	desc[0].flag |= cpu_to_le16(HNS_ROCE_CMD_FLAG_NEXT);
 -	hns_roce_cmq_setup_basic_desc(&desc[1], opcode, false);
 -	hr_reg_write(req_a, FUNC_RES_A_VF_ID, vf_id);
 -	hns_roce_cmq_send(hr_dev, desc, 2);
 +	return hns_roce_cmq_send(hr_dev, &desc, 1);
  }
  
 -static void hns_roce_function_clear(struct hns_roce_dev *hr_dev)
 +static int set_mtpt_pbl(struct hns_roce_v2_mpt_entry *mpt_entry,
 +			struct hns_roce_mr *mr)
  {
 +	struct sg_dma_page_iter sg_iter;
 +	u64 page_addr;
 +	u64 *pages;
  	int i;
  
 -	for (i = hr_dev->func_num - 1; i >= 0; i--) {
 -		__hns_roce_function_clear(hr_dev, i);
 -		if (i != 0)
 -			hns_roce_free_vf_resource(hr_dev, i);
 -	}
 -}
 -
 -static int hns_roce_clear_extdb_list_info(struct hns_roce_dev *hr_dev)
 -{
 -	struct hns_roce_cmq_desc desc;
 -	int ret;
 +	mpt_entry->pbl_size = cpu_to_le32(mr->pbl_size);
 +	mpt_entry->pbl_ba_l = cpu_to_le32(lower_32_bits(mr->pbl_ba >> 3));
 +	roce_set_field(mpt_entry->byte_48_mode_ba,
 +		       V2_MPT_BYTE_48_PBL_BA_H_M, V2_MPT_BYTE_48_PBL_BA_H_S,
 +		       upper_32_bits(mr->pbl_ba >> 3));
  
 -	hns_roce_cmq_setup_basic_desc(&desc, HNS_ROCE_OPC_CLEAR_EXTDB_LIST_INFO,
 -				      false);
 -	ret = hns_roce_cmq_send(hr_dev, &desc, 1);
 -	if (ret)
 -		ibdev_err(&hr_dev->ib_dev,
 -			  "failed to clear extended doorbell info, ret = %d.\n",
 -			  ret);
 +	pages = (u64 *)__get_free_page(GFP_KERNEL);
 +	if (!pages)
 +		return -ENOMEM;
  
 -	return ret;
 -}
 +	i = 0;
 +	for_each_sg_dma_page(mr->umem->sg_head.sgl, &sg_iter, mr->umem->nmap, 0) {
 +		page_addr = sg_page_iter_dma_address(&sg_iter);
 +		pages[i] = page_addr >> 6;
  
 -static int hns_roce_query_fw_ver(struct hns_roce_dev *hr_dev)
 -{
 -	struct hns_roce_query_fw_info *resp;
 -	struct hns_roce_cmq_desc desc;
 -	int ret;
 +		/* Record the first 2 entry directly to MTPT table */
 +		if (i >= HNS_ROCE_V2_MAX_INNER_MTPT_NUM - 1)
 +			goto found;
 +		i++;
 +	}
 +found:
 +	mpt_entry->pa0_l = cpu_to_le32(lower_32_bits(pages[0]));
 +	roce_set_field(mpt_entry->byte_56_pa0_h, V2_MPT_BYTE_56_PA0_H_M,
 +		       V2_MPT_BYTE_56_PA0_H_S, upper_32_bits(pages[0]));
  
 -	hns_roce_cmq_setup_basic_desc(&desc, HNS_QUERY_FW_VER, true);
 -	ret = hns_roce_cmq_send(hr_dev, &desc, 1);
 -	if (ret)
 -		return ret;
 +	mpt_entry->pa1_l = cpu_to_le32(lower_32_bits(pages[1]));
 +	roce_set_field(mpt_entry->byte_64_buf_pa1, V2_MPT_BYTE_64_PA1_H_M,
 +		       V2_MPT_BYTE_64_PA1_H_S, upper_32_bits(pages[1]));
 +	roce_set_field(mpt_entry->byte_64_buf_pa1,
 +		       V2_MPT_BYTE_64_PBL_BUF_PG_SZ_M,
 +		       V2_MPT_BYTE_64_PBL_BUF_PG_SZ_S,
 +		       mr->pbl_buf_pg_sz + PG_SHIFT_OFFSET);
  
 -	resp = (struct hns_roce_query_fw_info *)desc.data;
 -	hr_dev->caps.fw_ver = (u64)(le32_to_cpu(resp->fw_ver));
 +	free_page((unsigned long)pages);
  
  	return 0;
  }
@@@ -3049,139 -4695,165 +3049,264 @@@ static int modify_qp_init_to_rtr(struc
  		return -EINVAL;
  	}
  
 -	hr_reg_write(context, QPC_UDPSPN,
 -		     is_udp ? get_udp_sport(grh->flow_label, ibqp->qp_num,
 -					    attr->dest_qp_num) : 0);
 -
 -	hr_reg_clear(qpc_mask, QPC_UDPSPN);
 -
 -	hr_reg_write(context, QPC_GMV_IDX, grh->sgid_index);
 -
 -	hr_reg_clear(qpc_mask, QPC_GMV_IDX);
 -
 -	hr_reg_write(context, QPC_HOPLIMIT, grh->hop_limit);
 -	hr_reg_clear(qpc_mask, QPC_HOPLIMIT);
 +	dmac = (u8 *)attr->ah_attr.roce.dmac;
 +	context->wqe_sge_ba = (u32)(dma_handle >> 3);
 +	qpc_mask->wqe_sge_ba = 0;
  
 -	ret = fill_cong_field(ibqp, attr, context, qpc_mask);
 -	if (ret)
 -		return ret;
 +	/*
 +	 * In v2 engine, software pass context and context mask to hardware
 +	 * when modifying qp. If software need modify some fields in context,
 +	 * we should set all bits of the relevant fields in context mask to
 +	 * 0 at the same time, else set them to 0x1.
 +	 */
 +	roce_set_field(context->byte_12_sq_hop, V2_QPC_BYTE_12_WQE_SGE_BA_M,
 +		       V2_QPC_BYTE_12_WQE_SGE_BA_S, dma_handle >> (32 + 3));
 +	roce_set_field(qpc_mask->byte_12_sq_hop, V2_QPC_BYTE_12_WQE_SGE_BA_M,
 +		       V2_QPC_BYTE_12_WQE_SGE_BA_S, 0);
 +
 +	roce_set_field(context->byte_12_sq_hop, V2_QPC_BYTE_12_SQ_HOP_NUM_M,
 +		       V2_QPC_BYTE_12_SQ_HOP_NUM_S,
 +		       hr_dev->caps.mtt_hop_num == HNS_ROCE_HOP_NUM_0 ?
 +		       0 : hr_dev->caps.mtt_hop_num);
 +	roce_set_field(qpc_mask->byte_12_sq_hop, V2_QPC_BYTE_12_SQ_HOP_NUM_M,
 +		       V2_QPC_BYTE_12_SQ_HOP_NUM_S, 0);
 +
 +	roce_set_field(context->byte_20_smac_sgid_idx,
 +		       V2_QPC_BYTE_20_SGE_HOP_NUM_M,
 +		       V2_QPC_BYTE_20_SGE_HOP_NUM_S,
 +		       ((ibqp->qp_type == IB_QPT_GSI) || hr_qp->sq.max_gs > 2) ?
 +		       hr_dev->caps.mtt_hop_num : 0);
 +	roce_set_field(qpc_mask->byte_20_smac_sgid_idx,
 +		       V2_QPC_BYTE_20_SGE_HOP_NUM_M,
 +		       V2_QPC_BYTE_20_SGE_HOP_NUM_S, 0);
 +
 +	roce_set_field(context->byte_20_smac_sgid_idx,
 +		       V2_QPC_BYTE_20_RQ_HOP_NUM_M,
 +		       V2_QPC_BYTE_20_RQ_HOP_NUM_S,
 +		       hr_dev->caps.mtt_hop_num == HNS_ROCE_HOP_NUM_0 ?
 +		       0 : hr_dev->caps.mtt_hop_num);
 +	roce_set_field(qpc_mask->byte_20_smac_sgid_idx,
 +		       V2_QPC_BYTE_20_RQ_HOP_NUM_M,
 +		       V2_QPC_BYTE_20_RQ_HOP_NUM_S, 0);
 +
 +	roce_set_field(context->byte_16_buf_ba_pg_sz,
 +		       V2_QPC_BYTE_16_WQE_SGE_BA_PG_SZ_M,
 +		       V2_QPC_BYTE_16_WQE_SGE_BA_PG_SZ_S,
 +		       hr_dev->caps.mtt_ba_pg_sz + PG_SHIFT_OFFSET);
 +	roce_set_field(qpc_mask->byte_16_buf_ba_pg_sz,
 +		       V2_QPC_BYTE_16_WQE_SGE_BA_PG_SZ_M,
 +		       V2_QPC_BYTE_16_WQE_SGE_BA_PG_SZ_S, 0);
 +
 +	roce_set_field(context->byte_16_buf_ba_pg_sz,
 +		       V2_QPC_BYTE_16_WQE_SGE_BUF_PG_SZ_M,
 +		       V2_QPC_BYTE_16_WQE_SGE_BUF_PG_SZ_S,
 +		       hr_dev->caps.mtt_buf_pg_sz + PG_SHIFT_OFFSET);
 +	roce_set_field(qpc_mask->byte_16_buf_ba_pg_sz,
 +		       V2_QPC_BYTE_16_WQE_SGE_BUF_PG_SZ_M,
 +		       V2_QPC_BYTE_16_WQE_SGE_BUF_PG_SZ_S, 0);
 +
 +	roce_set_field(context->byte_80_rnr_rx_cqn,
 +		       V2_QPC_BYTE_80_MIN_RNR_TIME_M,
 +		       V2_QPC_BYTE_80_MIN_RNR_TIME_S, attr->min_rnr_timer);
 +	roce_set_field(qpc_mask->byte_80_rnr_rx_cqn,
 +		       V2_QPC_BYTE_80_MIN_RNR_TIME_M,
 +		       V2_QPC_BYTE_80_MIN_RNR_TIME_S, 0);
 +
 +	page_size = 1 << (hr_dev->caps.mtt_buf_pg_sz + PAGE_SHIFT);
 +	context->rq_cur_blk_addr = (u32)(mtts[hr_qp->rq.offset / page_size]
 +				    >> PAGE_ADDR_SHIFT);
 +	qpc_mask->rq_cur_blk_addr = 0;
  
 -	hr_reg_write(context, QPC_TC, get_tclass(&attr->ah_attr.grh));
 -	hr_reg_clear(qpc_mask, QPC_TC);
 +	roce_set_field(context->byte_92_srq_info,
 +		       V2_QPC_BYTE_92_RQ_CUR_BLK_ADDR_M,
 +		       V2_QPC_BYTE_92_RQ_CUR_BLK_ADDR_S,
 +		       mtts[hr_qp->rq.offset / page_size]
 +		       >> (32 + PAGE_ADDR_SHIFT));
 +	roce_set_field(qpc_mask->byte_92_srq_info,
 +		       V2_QPC_BYTE_92_RQ_CUR_BLK_ADDR_M,
 +		       V2_QPC_BYTE_92_RQ_CUR_BLK_ADDR_S, 0);
 +
 +	context->rq_nxt_blk_addr = (u32)(mtts[hr_qp->rq.offset / page_size + 1]
 +				    >> PAGE_ADDR_SHIFT);
 +	qpc_mask->rq_nxt_blk_addr = 0;
  
 -	hr_reg_write(context, QPC_FL, grh->flow_label);
 -	hr_reg_clear(qpc_mask, QPC_FL);
 -	memcpy(context->dgid, grh->dgid.raw, sizeof(grh->dgid.raw));
 -	memset(qpc_mask->dgid, 0, sizeof(grh->dgid.raw));
 +	roce_set_field(context->byte_104_rq_sge,
 +		       V2_QPC_BYTE_104_RQ_NXT_BLK_ADDR_M,
 +		       V2_QPC_BYTE_104_RQ_NXT_BLK_ADDR_S,
 +		       mtts[hr_qp->rq.offset / page_size + 1]
 +		       >> (32 + PAGE_ADDR_SHIFT));
 +	roce_set_field(qpc_mask->byte_104_rq_sge,
 +		       V2_QPC_BYTE_104_RQ_NXT_BLK_ADDR_M,
 +		       V2_QPC_BYTE_104_RQ_NXT_BLK_ADDR_S, 0);
 +
 +	roce_set_field(context->byte_108_rx_reqepsn,
 +		       V2_QPC_BYTE_108_RX_REQ_EPSN_M,
 +		       V2_QPC_BYTE_108_RX_REQ_EPSN_S, attr->rq_psn);
 +	roce_set_field(qpc_mask->byte_108_rx_reqepsn,
 +		       V2_QPC_BYTE_108_RX_REQ_EPSN_M,
 +		       V2_QPC_BYTE_108_RX_REQ_EPSN_S, 0);
 +
++<<<<<<< HEAD
 +	roce_set_field(context->byte_132_trrl, V2_QPC_BYTE_132_TRRL_BA_M,
 +		       V2_QPC_BYTE_132_TRRL_BA_S, dma_handle_3 >> 4);
 +	roce_set_field(qpc_mask->byte_132_trrl, V2_QPC_BYTE_132_TRRL_BA_M,
 +		       V2_QPC_BYTE_132_TRRL_BA_S, 0);
 +	context->trrl_ba = (u32)(dma_handle_3 >> (16 + 4));
++=======
++static int config_qp_sq_buf(struct hns_roce_dev *hr_dev,
++			    struct hns_roce_qp *hr_qp,
++			    struct hns_roce_v2_qp_context *context,
++			    struct hns_roce_v2_qp_context *qpc_mask)
++{
++	struct ib_device *ibdev = &hr_dev->ib_dev;
++	u64 sge_cur_blk = 0;
++	u64 sq_cur_blk = 0;
++	int count;
+ 
 -	hr_qp->sl = rdma_ah_get_sl(&attr->ah_attr);
 -	if (unlikely(hr_qp->sl > MAX_SERVICE_LEVEL)) {
 -		ibdev_err(ibdev,
 -			  "failed to fill QPC, sl (%d) shouldn't be larger than %d.\n",
 -			  hr_qp->sl, MAX_SERVICE_LEVEL);
++	/* search qp buf's mtts */
++	count = hns_roce_mtr_find(hr_dev, &hr_qp->mtr, 0, &sq_cur_blk, 1, NULL);
++	if (count < 1) {
++		ibdev_err(ibdev, "failed to find QP(0x%lx) SQ buf.\n",
++			  hr_qp->qpn);
+ 		return -EINVAL;
+ 	}
++	if (hr_qp->sge.sge_cnt > 0) {
++		count = hns_roce_mtr_find(hr_dev, &hr_qp->mtr,
++					  hr_qp->sge.offset,
++					  &sge_cur_blk, 1, NULL);
++		if (count < 1) {
++			ibdev_err(ibdev, "failed to find QP(0x%lx) SGE buf.\n",
++				  hr_qp->qpn);
++			return -EINVAL;
++		}
++	}
+ 
 -	hr_reg_write(context, QPC_SL, hr_qp->sl);
 -	hr_reg_clear(qpc_mask, QPC_SL);
++	/*
++	 * In v2 engine, software pass context and context mask to hardware
++	 * when modifying qp. If software need modify some fields in context,
++	 * we should set all bits of the relevant fields in context mask to
++	 * 0 at the same time, else set them to 0x1.
++	 */
++	hr_reg_write(context, QPC_SQ_CUR_BLK_ADDR_L,
++		     lower_32_bits(to_hr_hw_page_addr(sq_cur_blk)));
++	hr_reg_write(context, QPC_SQ_CUR_BLK_ADDR_H,
++		     upper_32_bits(to_hr_hw_page_addr(sq_cur_blk)));
++	hr_reg_clear(qpc_mask, QPC_SQ_CUR_BLK_ADDR_L);
++	hr_reg_clear(qpc_mask, QPC_SQ_CUR_BLK_ADDR_H);
+ 
 -	return 0;
 -}
++	hr_reg_write(context, QPC_SQ_CUR_SGE_BLK_ADDR_L,
++		     lower_32_bits(to_hr_hw_page_addr(sge_cur_blk)));
++	hr_reg_write(context, QPC_SQ_CUR_SGE_BLK_ADDR_H,
++		     upper_32_bits(to_hr_hw_page_addr(sge_cur_blk)));
++	hr_reg_clear(qpc_mask, QPC_SQ_CUR_SGE_BLK_ADDR_L);
++	hr_reg_clear(qpc_mask, QPC_SQ_CUR_SGE_BLK_ADDR_H);
+ 
 -static bool check_qp_state(enum ib_qp_state cur_state,
 -			   enum ib_qp_state new_state)
 -{
 -	static const bool sm[][IB_QPS_ERR + 1] = {
 -		[IB_QPS_RESET] = { [IB_QPS_RESET] = true,
 -				   [IB_QPS_INIT] = true },
 -		[IB_QPS_INIT] = { [IB_QPS_RESET] = true,
 -				  [IB_QPS_INIT] = true,
 -				  [IB_QPS_RTR] = true,
 -				  [IB_QPS_ERR] = true },
 -		[IB_QPS_RTR] = { [IB_QPS_RESET] = true,
 -				 [IB_QPS_RTS] = true,
 -				 [IB_QPS_ERR] = true },
 -		[IB_QPS_RTS] = { [IB_QPS_RESET] = true,
 -				 [IB_QPS_RTS] = true,
 -				 [IB_QPS_ERR] = true },
 -		[IB_QPS_SQD] = {},
 -		[IB_QPS_SQE] = {},
 -		[IB_QPS_ERR] = { [IB_QPS_RESET] = true, [IB_QPS_ERR] = true }
 -	};
 -
 -	return sm[cur_state][new_state];
++	hr_reg_write(context, QPC_RX_SQ_CUR_BLK_ADDR_L,
++		     lower_32_bits(to_hr_hw_page_addr(sq_cur_blk)));
++	hr_reg_write(context, QPC_RX_SQ_CUR_BLK_ADDR_H,
++		     upper_32_bits(to_hr_hw_page_addr(sq_cur_blk)));
++	hr_reg_clear(qpc_mask, QPC_RX_SQ_CUR_BLK_ADDR_L);
++	hr_reg_clear(qpc_mask, QPC_RX_SQ_CUR_BLK_ADDR_H);
++
++	return 0;
+ }
+ 
 -static int hns_roce_v2_set_abs_fields(struct ib_qp *ibqp,
 -				      const struct ib_qp_attr *attr,
 -				      int attr_mask,
 -				      enum ib_qp_state cur_state,
 -				      enum ib_qp_state new_state,
 -				      struct hns_roce_v2_qp_context *context,
 -				      struct hns_roce_v2_qp_context *qpc_mask)
++static inline enum ib_mtu get_mtu(struct ib_qp *ibqp,
++				  const struct ib_qp_attr *attr)
+ {
 -	struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);
 -	int ret = 0;
 -
 -	if (!check_qp_state(cur_state, new_state)) {
 -		ibdev_err(&hr_dev->ib_dev, "Illegal state for QP!\n");
 -		return -EINVAL;
 -	}
 -
 -	if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT) {
 -		memset(qpc_mask, 0, hr_dev->caps.qpc_sz);
 -		modify_qp_reset_to_init(ibqp, attr, attr_mask, context,
 -					qpc_mask);
 -	} else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_INIT) {
 -		modify_qp_init_to_init(ibqp, attr, attr_mask, context,
 -				       qpc_mask);
 -	} else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_RTR) {
 -		ret = modify_qp_init_to_rtr(ibqp, attr, attr_mask, context,
 -					    qpc_mask);
 -	} else if (cur_state == IB_QPS_RTR && new_state == IB_QPS_RTS) {
 -		ret = modify_qp_rtr_to_rts(ibqp, attr, attr_mask, context,
 -					   qpc_mask);
 -	}
++	if (ibqp->qp_type == IB_QPT_GSI || ibqp->qp_type == IB_QPT_UD)
++		return IB_MTU_4096;
+ 
 -	return ret;
++	return attr->path_mtu;
+ }
+ 
 -static int hns_roce_v2_set_opt_fields(struct ib_qp *ibqp,
 -				      const struct ib_qp_attr *attr,
 -				      int attr_mask,
 -				      struct hns_roce_v2_qp_context *context,
 -				      struct hns_roce_v2_qp_context *qpc_mask)
++static int modify_qp_init_to_rtr(struct ib_qp *ibqp,
++				 const struct ib_qp_attr *attr, int attr_mask,
++				 struct hns_roce_v2_qp_context *context,
++				 struct hns_roce_v2_qp_context *qpc_mask)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device);
+ 	struct hns_roce_qp *hr_qp = to_hr_qp(ibqp);
 -	int ret = 0;
++	struct ib_device *ibdev = &hr_dev->ib_dev;
++	dma_addr_t trrl_ba;
++	dma_addr_t irrl_ba;
++	enum ib_mtu ib_mtu;
++	const u8 *smac;
++	u8 lp_pktn_ini;
++	u64 *mtts;
++	u8 *dmac;
++	u32 port;
++	int mtu;
++	int ret;
+ 
 -	if (attr_mask & IB_QP_AV) {
 -		ret = hns_roce_v2_set_path(ibqp, attr, attr_mask, context,
 -					   qpc_mask);
 -		if (ret)
 -			return ret;
++	ret = config_qp_rq_buf(hr_dev, hr_qp, context, qpc_mask);
++	if (ret) {
++		ibdev_err(ibdev, "failed to config rq buf, ret = %d.\n", ret);
++		return ret;
+ 	}
+ 
 -	if (attr_mask & IB_QP_TIMEOUT) {
 -		if (attr->timeout < 31) {
 -			hr_reg_write(context, QPC_AT, attr->timeout);
 -			hr_reg_clear(qpc_mask, QPC_AT);
 -		} else {
 -			ibdev_warn(&hr_dev->ib_dev,
 -				   "Local ACK timeout shall be 0 to 30.\n");
 -		}
++	/* Search IRRL's mtts */
++	mtts = hns_roce_table_find(hr_dev, &hr_dev->qp_table.irrl_table,
++				   hr_qp->qpn, &irrl_ba);
++	if (!mtts) {
++		ibdev_err(ibdev, "failed to find qp irrl_table.\n");
++		return -EINVAL;
+ 	}
+ 
 -	if (attr_mask & IB_QP_RETRY_CNT) {
 -		hr_reg_write(context, QPC_RETRY_NUM_INIT, attr->retry_cnt);
 -		hr_reg_clear(qpc_mask, QPC_RETRY_NUM_INIT);
 -
 -		hr_reg_write(context, QPC_RETRY_CNT, attr->retry_cnt);
 -		hr_reg_clear(qpc_mask, QPC_RETRY_CNT);
++	/* Search TRRL's mtts */
++	mtts = hns_roce_table_find(hr_dev, &hr_dev->qp_table.trrl_table,
++				   hr_qp->qpn, &trrl_ba);
++	if (!mtts) {
++		ibdev_err(ibdev, "failed to find qp trrl_table.\n");
++		return -EINVAL;
+ 	}
+ 
 -	if (attr_mask & IB_QP_RNR_RETRY) {
 -		hr_reg_write(context, QPC_RNR_NUM_INIT, attr->rnr_retry);
 -		hr_reg_clear(qpc_mask, QPC_RNR_NUM_INIT);
 -
 -		hr_reg_write(context, QPC_RNR_CNT, attr->rnr_retry);
 -		hr_reg_clear(qpc_mask, QPC_RNR_CNT);
++	if (attr_mask & IB_QP_ALT_PATH) {
++		ibdev_err(ibdev, "INIT2RTR attr_mask (0x%x) error.\n",
++			  attr_mask);
++		return -EINVAL;
+ 	}
+ 
 -	if (attr_mask & IB_QP_SQ_PSN) {
 -		hr_reg_write(context, QPC_SQ_CUR_PSN, attr->sq_psn);
 -		hr_reg_clear(qpc_mask, QPC_SQ_CUR_PSN);
++	hr_reg_write(context, QPC_TRRL_BA_L, trrl_ba >> 4);
++	hr_reg_clear(qpc_mask, QPC_TRRL_BA_L);
++	context->trrl_ba = cpu_to_le32(trrl_ba >> (16 + 4));
++>>>>>>> fd92213e9af3 (RDMA: Constify netdev->dev_addr accesses)
 +	qpc_mask->trrl_ba = 0;
 +	roce_set_field(context->byte_140_raq, V2_QPC_BYTE_140_TRRL_BA_M,
 +		       V2_QPC_BYTE_140_TRRL_BA_S,
 +		       (u32)(dma_handle_3 >> (32 + 16 + 4)));
 +	roce_set_field(qpc_mask->byte_140_raq, V2_QPC_BYTE_140_TRRL_BA_M,
 +		       V2_QPC_BYTE_140_TRRL_BA_S, 0);
  
 -		hr_reg_write(context, QPC_SQ_MAX_PSN, attr->sq_psn);
 -		hr_reg_clear(qpc_mask, QPC_SQ_MAX_PSN);
 +	context->irrl_ba = (u32)(dma_handle_2 >> 6);
 +	qpc_mask->irrl_ba = 0;
 +	roce_set_field(context->byte_208_irrl, V2_QPC_BYTE_208_IRRL_BA_M,
 +		       V2_QPC_BYTE_208_IRRL_BA_S,
 +		       dma_handle_2 >> (32 + 6));
 +	roce_set_field(qpc_mask->byte_208_irrl, V2_QPC_BYTE_208_IRRL_BA_M,
 +		       V2_QPC_BYTE_208_IRRL_BA_S, 0);
  
 -		hr_reg_write(context, QPC_RETRY_MSG_PSN_L, attr->sq_psn);
 -		hr_reg_clear(qpc_mask, QPC_RETRY_MSG_PSN_L);
 +	roce_set_bit(context->byte_208_irrl, V2_QPC_BYTE_208_RMT_E2E_S, 1);
 +	roce_set_bit(qpc_mask->byte_208_irrl, V2_QPC_BYTE_208_RMT_E2E_S, 0);
  
 -		hr_reg_write(context, QPC_RETRY_MSG_PSN_H,
 -			     attr->sq_psn >> RETRY_MSG_PSN_SHIFT);
 -		hr_reg_clear(qpc_mask, QPC_RETRY_MSG_PSN_H);
 +	roce_set_bit(context->byte_252_err_txcqn, V2_QPC_BYTE_252_SIG_TYPE_S,
 +		     hr_qp->sq_signal_bits);
 +	roce_set_bit(qpc_mask->byte_252_err_txcqn, V2_QPC_BYTE_252_SIG_TYPE_S,
 +		     0);
  
 -		hr_reg_write(context, QPC_RETRY_MSG_FPKT_PSN, attr->sq_psn);
 -		hr_reg_clear(qpc_mask, QPC_RETRY_MSG_FPKT_PSN);
 +	port = (attr_mask & IB_QP_PORT) ? (attr->port_num - 1) : hr_qp->port;
  
 -		hr_reg_write(context, QPC_RX_ACK_EPSN, attr->sq_psn);
 -		hr_reg_clear(qpc_mask, QPC_RX_ACK_EPSN);
++<<<<<<< HEAD
 +	smac = (u8 *)hr_dev->dev_addr[port];
++=======
++	smac = (const u8 *)hr_dev->dev_addr[port];
++	dmac = (u8 *)attr->ah_attr.roce.dmac;
++>>>>>>> fd92213e9af3 (RDMA: Constify netdev->dev_addr accesses)
 +	/* when dmac equals smac or loop_idc is 1, it should loopback */
 +	if (ether_addr_equal_unaligned(dmac, smac) ||
 +	    hr_dev->loop_idc == 0x1) {
 +		roce_set_bit(context->byte_28_at_fl, V2_QPC_BYTE_28_LBI_S, 1);
 +		roce_set_bit(qpc_mask->byte_28_at_fl, V2_QPC_BYTE_28_LBI_S, 0);
  	}
  
  	if ((attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) &&
diff --cc drivers/infiniband/hw/hns/hns_roce_main.c
index e13c3bab1a01,b3595b6079b5..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_main.c
+++ b/drivers/infiniband/hw/hns/hns_roce_main.c
@@@ -39,35 -40,21 +39,40 @@@
  #include <rdma/ib_cache.h>
  #include "hns_roce_common.h"
  #include "hns_roce_device.h"
 +#include <rdma/hns-abi.h>
  #include "hns_roce_hem.h"
  
++<<<<<<< HEAD
 +/**
 + * hns_get_gid_index - Get gid index.
 + * @hr_dev: pointer to structure hns_roce_dev.
 + * @port:  port, value range: 0 ~ MAX
 + * @gid_index:  gid_index, value range: 0 ~ MAX
 + * Description:
 + *    N ports shared gids, allocation method as follow:
 + *		GID[0][0], GID[1][0],.....GID[N - 1][0],
 + *		GID[0][0], GID[1][0],.....GID[N - 1][0],
 + *		And so on
 + */
 +int hns_get_gid_index(struct hns_roce_dev *hr_dev, u8 port, int gid_index)
 +{
 +	return gid_index * hr_dev->caps.num_ports + port;
 +}
 +EXPORT_SYMBOL_GPL(hns_get_gid_index);
 +
 +static int hns_roce_set_mac(struct hns_roce_dev *hr_dev, u8 port, u8 *addr)
++=======
+ static int hns_roce_set_mac(struct hns_roce_dev *hr_dev, u32 port,
+ 			    const u8 *addr)
++>>>>>>> fd92213e9af3 (RDMA: Constify netdev->dev_addr accesses)
  {
  	u8 phy_port;
 -	u32 i;
 +	u32 i = 0;
  
 -	if (hr_dev->pci_dev->revision >= PCI_REVISION_ID_HIP09)
 +	if (!memcmp(hr_dev->dev_addr[port], addr, MAC_ADDR_OCTET_NUM))
  		return 0;
  
 -	if (!memcmp(hr_dev->dev_addr[port], addr, ETH_ALEN))
 -		return 0;
 -
 -	for (i = 0; i < ETH_ALEN; i++)
 +	for (i = 0; i < MAC_ADDR_OCTET_NUM; i++)
  		hr_dev->dev_addr[port][i] = addr[i];
  
  	phy_port = hr_dev->iboe.phy_port[port];
diff --git a/drivers/infiniband/hw/bnxt_re/qplib_res.c b/drivers/infiniband/hw/bnxt_re/qplib_res.c
index 44282a8cdd4f..78e4fa280e56 100644
--- a/drivers/infiniband/hw/bnxt_re/qplib_res.c
+++ b/drivers/infiniband/hw/bnxt_re/qplib_res.c
@@ -572,7 +572,7 @@ int bnxt_qplib_alloc_ctx(struct bnxt_qplib_res *res,
 }
 
 /* GUID */
-void bnxt_qplib_get_guid(u8 *dev_addr, u8 *guid)
+void bnxt_qplib_get_guid(const u8 *dev_addr, u8 *guid)
 {
 	u8 mac[ETH_ALEN];
 
diff --git a/drivers/infiniband/hw/bnxt_re/qplib_res.h b/drivers/infiniband/hw/bnxt_re/qplib_res.h
index c39b20236f16..d2951a713cc8 100644
--- a/drivers/infiniband/hw/bnxt_re/qplib_res.h
+++ b/drivers/infiniband/hw/bnxt_re/qplib_res.h
@@ -346,7 +346,7 @@ void bnxt_qplib_free_hwq(struct bnxt_qplib_res *res,
 			 struct bnxt_qplib_hwq *hwq);
 int bnxt_qplib_alloc_init_hwq(struct bnxt_qplib_hwq *hwq,
 			      struct bnxt_qplib_hwq_attr *hwq_attr);
-void bnxt_qplib_get_guid(u8 *dev_addr, u8 *guid);
+void bnxt_qplib_get_guid(const u8 *dev_addr, u8 *guid);
 int bnxt_qplib_alloc_pd(struct bnxt_qplib_pd_tbl *pd_tbl,
 			struct bnxt_qplib_pd *pd);
 int bnxt_qplib_dealloc_pd(struct bnxt_qplib_res *res,
diff --git a/drivers/infiniband/hw/bnxt_re/qplib_sp.c b/drivers/infiniband/hw/bnxt_re/qplib_sp.c
index cbe83e9bce5c..379e715ebd30 100644
--- a/drivers/infiniband/hw/bnxt_re/qplib_sp.c
+++ b/drivers/infiniband/hw/bnxt_re/qplib_sp.c
@@ -287,8 +287,8 @@ int bnxt_qplib_del_sgid(struct bnxt_qplib_sgid_tbl *sgid_tbl,
 }
 
 int bnxt_qplib_add_sgid(struct bnxt_qplib_sgid_tbl *sgid_tbl,
-			struct bnxt_qplib_gid *gid, u8 *smac, u16 vlan_id,
-			bool update, u32 *index)
+			struct bnxt_qplib_gid *gid, const u8 *smac,
+			u16 vlan_id, bool update, u32 *index)
 {
 	struct bnxt_qplib_res *res = to_bnxt_qplib(sgid_tbl,
 						   struct bnxt_qplib_res,
@@ -379,7 +379,7 @@ int bnxt_qplib_add_sgid(struct bnxt_qplib_sgid_tbl *sgid_tbl,
 
 int bnxt_qplib_update_sgid(struct bnxt_qplib_sgid_tbl *sgid_tbl,
 			   struct bnxt_qplib_gid *gid, u16 gid_idx,
-			   u8 *smac)
+			   const u8 *smac)
 {
 	struct bnxt_qplib_res *res = to_bnxt_qplib(sgid_tbl,
 						   struct bnxt_qplib_res,
diff --git a/drivers/infiniband/hw/bnxt_re/qplib_sp.h b/drivers/infiniband/hw/bnxt_re/qplib_sp.h
index 3d5c41841668..a18f568cb23e 100644
--- a/drivers/infiniband/hw/bnxt_re/qplib_sp.h
+++ b/drivers/infiniband/hw/bnxt_re/qplib_sp.h
@@ -250,10 +250,11 @@ int bnxt_qplib_get_sgid(struct bnxt_qplib_res *res,
 int bnxt_qplib_del_sgid(struct bnxt_qplib_sgid_tbl *sgid_tbl,
 			struct bnxt_qplib_gid *gid, u16 vlan_id, bool update);
 int bnxt_qplib_add_sgid(struct bnxt_qplib_sgid_tbl *sgid_tbl,
-			struct bnxt_qplib_gid *gid, u8 *mac, u16 vlan_id,
+			struct bnxt_qplib_gid *gid, const u8 *mac, u16 vlan_id,
 			bool update, u32 *index);
 int bnxt_qplib_update_sgid(struct bnxt_qplib_sgid_tbl *sgid_tbl,
-			   struct bnxt_qplib_gid *gid, u16 gid_idx, u8 *smac);
+			   struct bnxt_qplib_gid *gid, u16 gid_idx,
+			   const u8 *smac);
 int bnxt_qplib_get_pkey(struct bnxt_qplib_res *res,
 			struct bnxt_qplib_pkey_tbl *pkey_tbl, u16 index,
 			u16 *pkey);
diff --git a/drivers/infiniband/hw/hfi1/ipoib_main.c b/drivers/infiniband/hw/hfi1/ipoib_main.c
index e594a961f513..e1a2b02bbd91 100644
--- a/drivers/infiniband/hw/hfi1/ipoib_main.c
+++ b/drivers/infiniband/hw/hfi1/ipoib_main.c
@@ -11,7 +11,7 @@
 #include "ipoib.h"
 #include "hfi.h"
 
-static u32 qpn_from_mac(u8 *mac_arr)
+static u32 qpn_from_mac(const u8 *mac_arr)
 {
 	return (u32)mac_arr[1] << 16 | mac_arr[2] << 8 | mac_arr[3];
 }
diff --git a/drivers/infiniband/hw/hns/hns_roce_device.h b/drivers/infiniband/hw/hns/hns_roce_device.h
index 3d4d4e585465..2195684857ae 100644
--- a/drivers/infiniband/hw/hns/hns_roce_device.h
+++ b/drivers/infiniband/hw/hns/hns_roce_device.h
@@ -752,7 +752,8 @@ struct hns_roce_hw {
 	int (*chk_mbox)(struct hns_roce_dev *hr_dev, unsigned long timeout);
 	int (*set_gid)(struct hns_roce_dev *hr_dev, u8 port, int gid_index,
 		       const union ib_gid *gid, const struct ib_gid_attr *attr);
-	int (*set_mac)(struct hns_roce_dev *hr_dev, u8 phy_port, u8 *addr);
+	int (*set_mac)(struct hns_roce_dev *hr_dev, u8 phy_port,
+		       const u8 *addr);
 	void (*set_mtu)(struct hns_roce_dev *hr_dev, u8 phy_port,
 			enum ib_mtu mtu);
 	int (*write_mtpt)(void *mb_buf, struct hns_roce_mr *mr,
* Unmerged path drivers/infiniband/hw/hns/hns_roce_hw_v1.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_hw_v2.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_main.c
diff --git a/drivers/infiniband/hw/irdma/cm.h b/drivers/infiniband/hw/irdma/cm.h
index 1fbe72e18625..3bf42728e9b7 100644
--- a/drivers/infiniband/hw/irdma/cm.h
+++ b/drivers/infiniband/hw/irdma/cm.h
@@ -389,7 +389,7 @@ int irdma_reject(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len);
 int irdma_connect(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param);
 int irdma_create_listen(struct iw_cm_id *cm_id, int backlog);
 int irdma_destroy_listen(struct iw_cm_id *cm_id);
-int irdma_add_arp(struct irdma_pci_f *rf, u32 *ip, bool ipv4, u8 *mac);
+int irdma_add_arp(struct irdma_pci_f *rf, u32 *ip, bool ipv4, const u8 *mac);
 void irdma_cm_teardown_connections(struct irdma_device *iwdev, u32 *ipaddr,
 				   struct irdma_cm_info *nfo,
 				   bool disconnect_all);
@@ -398,7 +398,7 @@ int irdma_cm_stop(struct irdma_device *dev);
 bool irdma_ipv4_is_lpb(u32 loc_addr, u32 rem_addr);
 bool irdma_ipv6_is_lpb(u32 *loc_addr, u32 *rem_addr);
 int irdma_arp_table(struct irdma_pci_f *rf, u32 *ip_addr, bool ipv4,
-		    u8 *mac_addr, u32 action);
+		    const u8 *mac_addr, u32 action);
 void irdma_if_notify(struct irdma_device *iwdev, struct net_device *netdev,
 		     u32 *ipaddr, bool ipv4, bool ifup);
 bool irdma_port_in_use(struct irdma_cm_core *cm_core, u16 port);
diff --git a/drivers/infiniband/hw/irdma/hw.c b/drivers/infiniband/hw/irdma/hw.c
index d077eb399556..56b263396aa8 100644
--- a/drivers/infiniband/hw/irdma/hw.c
+++ b/drivers/infiniband/hw/irdma/hw.c
@@ -1062,7 +1062,7 @@ static enum irdma_status_code irdma_alloc_set_mac(struct irdma_device *iwdev)
 					     &iwdev->mac_ip_table_idx);
 	if (!status) {
 		status = irdma_add_local_mac_entry(iwdev->rf,
-						   (u8 *)iwdev->netdev->dev_addr,
+						   (const u8 *)iwdev->netdev->dev_addr,
 						   (u8)iwdev->mac_ip_table_idx);
 		if (status)
 			irdma_del_local_mac_entry(iwdev->rf,
@@ -2196,7 +2196,7 @@ void irdma_del_local_mac_entry(struct irdma_pci_f *rf, u16 idx)
  * @mac_addr: pointer to mac address
  * @idx: the index of the mac ip address to add
  */
-int irdma_add_local_mac_entry(struct irdma_pci_f *rf, u8 *mac_addr, u16 idx)
+int irdma_add_local_mac_entry(struct irdma_pci_f *rf, const u8 *mac_addr, u16 idx)
 {
 	struct irdma_local_mac_entry_info *info;
 	struct irdma_cqp *iwcqp = &rf->cqp;
@@ -2367,7 +2367,8 @@ void irdma_del_apbvt(struct irdma_device *iwdev,
  * @ipv4: flag inicating IPv4
  * @action: add, delete or modify
  */
-void irdma_manage_arp_cache(struct irdma_pci_f *rf, unsigned char *mac_addr,
+void irdma_manage_arp_cache(struct irdma_pci_f *rf,
+			    const unsigned char *mac_addr,
 			    u32 *ip_addr, bool ipv4, u32 action)
 {
 	struct irdma_add_arp_cache_entry_info *info;
diff --git a/drivers/infiniband/hw/irdma/main.h b/drivers/infiniband/hw/irdma/main.h
index 8b215f3cee89..cb218cab79ac 100644
--- a/drivers/infiniband/hw/irdma/main.h
+++ b/drivers/infiniband/hw/irdma/main.h
@@ -467,7 +467,8 @@ void irdma_qp_rem_ref(struct ib_qp *ibqp);
 void irdma_free_lsmm_rsrc(struct irdma_qp *iwqp);
 struct ib_qp *irdma_get_qp(struct ib_device *ibdev, int qpn);
 void irdma_flush_wqes(struct irdma_qp *iwqp, u32 flush_mask);
-void irdma_manage_arp_cache(struct irdma_pci_f *rf, unsigned char *mac_addr,
+void irdma_manage_arp_cache(struct irdma_pci_f *rf,
+			    const unsigned char *mac_addr,
 			    u32 *ip_addr, bool ipv4, u32 action);
 struct irdma_apbvt_entry *irdma_add_apbvt(struct irdma_device *iwdev, u16 port);
 void irdma_del_apbvt(struct irdma_device *iwdev,
@@ -479,7 +480,7 @@ void irdma_free_cqp_request(struct irdma_cqp *cqp,
 void irdma_put_cqp_request(struct irdma_cqp *cqp,
 			   struct irdma_cqp_request *cqp_request);
 int irdma_alloc_local_mac_entry(struct irdma_pci_f *rf, u16 *mac_tbl_idx);
-int irdma_add_local_mac_entry(struct irdma_pci_f *rf, u8 *mac_addr, u16 idx);
+int irdma_add_local_mac_entry(struct irdma_pci_f *rf, const u8 *mac_addr, u16 idx);
 void irdma_del_local_mac_entry(struct irdma_pci_f *rf, u16 idx);
 
 u32 irdma_initialize_hw_rsrc(struct irdma_pci_f *rf);
diff --git a/drivers/infiniband/hw/irdma/trace_cm.h b/drivers/infiniband/hw/irdma/trace_cm.h
index bcf10ec427d6..f633fb343328 100644
--- a/drivers/infiniband/hw/irdma/trace_cm.h
+++ b/drivers/infiniband/hw/irdma/trace_cm.h
@@ -144,7 +144,7 @@ DEFINE_EVENT(tos_template, irdma_dcb_tos,
 DECLARE_EVENT_CLASS(qhash_template,
 		    TP_PROTO(struct irdma_device *iwdev,
 			     struct irdma_cm_listener *listener,
-			     char *dev_addr),
+			     const char *dev_addr),
 		    TP_ARGS(iwdev, listener, dev_addr),
 		    TP_STRUCT__entry(__field(struct irdma_device *, iwdev)
 				     __field(u16, lport)
@@ -173,12 +173,14 @@ DECLARE_EVENT_CLASS(qhash_template,
 
 DEFINE_EVENT(qhash_template, irdma_add_mqh_6,
 	     TP_PROTO(struct irdma_device *iwdev,
-		      struct irdma_cm_listener *listener, char *dev_addr),
+		      struct irdma_cm_listener *listener,
+		      const char *dev_addr),
 	     TP_ARGS(iwdev, listener, dev_addr));
 
 DEFINE_EVENT(qhash_template, irdma_add_mqh_4,
 	     TP_PROTO(struct irdma_device *iwdev,
-		      struct irdma_cm_listener *listener, char *dev_addr),
+		      struct irdma_cm_listener *listener,
+		      const char *dev_addr),
 	     TP_ARGS(iwdev, listener, dev_addr));
 
 TRACE_EVENT(irdma_addr_resolve,
diff --git a/drivers/infiniband/hw/irdma/utils.c b/drivers/infiniband/hw/irdma/utils.c
index b5e6b977eebb..563107e958f4 100644
--- a/drivers/infiniband/hw/irdma/utils.c
+++ b/drivers/infiniband/hw/irdma/utils.c
@@ -11,7 +11,7 @@
  * @action: modify, delete or add
  */
 int irdma_arp_table(struct irdma_pci_f *rf, u32 *ip_addr, bool ipv4,
-		    u8 *mac_addr, u32 action)
+		    const u8 *mac_addr, u32 action)
 {
 	unsigned long flags;
 	int arp_index;
@@ -77,7 +77,7 @@ int irdma_arp_table(struct irdma_pci_f *rf, u32 *ip_addr, bool ipv4,
  * @ipv4: IPv4 flag
  * @mac: MAC address
  */
-int irdma_add_arp(struct irdma_pci_f *rf, u32 *ip, bool ipv4, u8 *mac)
+int irdma_add_arp(struct irdma_pci_f *rf, u32 *ip, bool ipv4, const u8 *mac)
 {
 	int arpidx;
 
diff --git a/drivers/infiniband/hw/irdma/verbs.c b/drivers/infiniband/hw/irdma/verbs.c
index bf5f7f8b7ba4..1c406a8aef71 100644
--- a/drivers/infiniband/hw/irdma/verbs.c
+++ b/drivers/infiniband/hw/irdma/verbs.c
@@ -4349,7 +4349,7 @@ static enum rdma_link_layer irdma_get_link_layer(struct ib_device *ibdev,
 
 static __be64 irdma_mac_to_guid(struct net_device *ndev)
 {
-	unsigned char *mac = ndev->dev_addr;
+	const unsigned char *mac = ndev->dev_addr;
 	__be64 guid;
 	unsigned char *dst = (unsigned char *)&guid;
 
diff --git a/drivers/infiniband/hw/usnic/usnic_fwd.c b/drivers/infiniband/hw/usnic/usnic_fwd.c
index 398c4c00b932..18a70850b738 100644
--- a/drivers/infiniband/hw/usnic/usnic_fwd.c
+++ b/drivers/infiniband/hw/usnic/usnic_fwd.c
@@ -103,7 +103,7 @@ void usnic_fwd_dev_free(struct usnic_fwd_dev *ufdev)
 	kfree(ufdev);
 }
 
-void usnic_fwd_set_mac(struct usnic_fwd_dev *ufdev, char mac[ETH_ALEN])
+void usnic_fwd_set_mac(struct usnic_fwd_dev *ufdev, const char mac[ETH_ALEN])
 {
 	spin_lock(&ufdev->lock);
 	memcpy(&ufdev->mac, mac, sizeof(ufdev->mac));
diff --git a/drivers/infiniband/hw/usnic/usnic_fwd.h b/drivers/infiniband/hw/usnic/usnic_fwd.h
index f0b71d593da5..a91200886922 100644
--- a/drivers/infiniband/hw/usnic/usnic_fwd.h
+++ b/drivers/infiniband/hw/usnic/usnic_fwd.h
@@ -74,7 +74,7 @@ struct usnic_filter_action {
 struct usnic_fwd_dev *usnic_fwd_dev_alloc(struct pci_dev *pdev);
 void usnic_fwd_dev_free(struct usnic_fwd_dev *ufdev);
 
-void usnic_fwd_set_mac(struct usnic_fwd_dev *ufdev, char mac[ETH_ALEN]);
+void usnic_fwd_set_mac(struct usnic_fwd_dev *ufdev, const char mac[ETH_ALEN]);
 void usnic_fwd_add_ipaddr(struct usnic_fwd_dev *ufdev, __be32 inaddr);
 void usnic_fwd_del_ipaddr(struct usnic_fwd_dev *ufdev);
 void usnic_fwd_carrier_up(struct usnic_fwd_dev *ufdev);
