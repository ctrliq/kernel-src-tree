arm64: kernel: Correct annotation of end of el0_sync

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-394.el8
commit-author Mark Brown <broonie@kernel.org>
commit 73d6890fe8ff40e357039b626537ac82d8782aeb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-394.el8/73d6890f.failed

Commit 582f95835a8fc812c ("arm64: entry: convert el0_sync to C") caused
the ENDPROC() annotating the end of el0_sync to be placed after the code
for el0_sync_compat. This replaced the previous annotation where it was
located after all the cases that are now converted to C, including after
the currently unannotated el0_irq_compat and el0_error_compat. Move the
annotation to the end of the function and add separate annotations for
the _compat ones.

Fixes: 582f95835a8fc812c (arm64: entry: convert el0_sync to C)
	Signed-off-by: Mark Brown <broonie@kernel.org>
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 73d6890fe8ff40e357039b626537ac82d8782aeb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry.S
diff --cc arch/arm64/kernel/entry.S
index 3d4784da5e03,d54d165b286a..000000000000
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@@ -734,71 -650,19 +734,82 @@@ ENDPROC(el1_irq
  	.align	6
  el0_sync:
  	kernel_entry 0
++<<<<<<< HEAD
 +	mrs	x25, esr_el1			// read the syndrome register
 +	lsr	x24, x25, #ESR_ELx_EC_SHIFT	// exception class
 +	cmp	x24, #ESR_ELx_EC_SVC64		// SVC in 64-bit state
 +	b.eq	el0_svc
 +	cmp	x24, #ESR_ELx_EC_DABT_LOW	// data abort in EL0
 +	b.eq	el0_da
 +	cmp	x24, #ESR_ELx_EC_IABT_LOW	// instruction abort in EL0
 +	b.eq	el0_ia
 +	cmp	x24, #ESR_ELx_EC_FP_ASIMD	// FP/ASIMD access
 +	b.eq	el0_fpsimd_acc
 +	cmp	x24, #ESR_ELx_EC_SVE		// SVE access
 +	b.eq	el0_sve_acc
 +	cmp	x24, #ESR_ELx_EC_FP_EXC64	// FP/ASIMD exception
 +	b.eq	el0_fpsimd_exc
 +	cmp	x24, #ESR_ELx_EC_SYS64		// configurable trap
 +	ccmp	x24, #ESR_ELx_EC_WFx, #4, ne
 +	b.eq	el0_sys
 +	cmp	x24, #ESR_ELx_EC_SP_ALIGN	// stack alignment exception
 +	b.eq	el0_sp
 +	cmp	x24, #ESR_ELx_EC_PC_ALIGN	// pc alignment exception
 +	b.eq	el0_pc
 +	cmp	x24, #ESR_ELx_EC_UNKNOWN	// unknown exception in EL0
 +	b.eq	el0_undef
 +	cmp	x24, #ESR_ELx_EC_BREAKPT_LOW	// debug exception in EL0
 +	b.ge	el0_dbg
 +	b	el0_inv
++=======
+ 	mov	x0, sp
+ 	bl	el0_sync_handler
+ 	b	ret_to_user
+ ENDPROC(el0_sync)
++>>>>>>> 73d6890fe8ff (arm64: kernel: Correct annotation of end of el0_sync)
  
  #ifdef CONFIG_COMPAT
  	.align	6
  el0_sync_compat:
  	kernel_entry 0, 32
 +	mrs	x25, esr_el1			// read the syndrome register
 +	lsr	x24, x25, #ESR_ELx_EC_SHIFT	// exception class
 +	cmp	x24, #ESR_ELx_EC_SVC32		// SVC in 32-bit state
 +	b.eq	el0_svc_compat
 +	cmp	x24, #ESR_ELx_EC_DABT_LOW	// data abort in EL0
 +	b.eq	el0_da
 +	cmp	x24, #ESR_ELx_EC_IABT_LOW	// instruction abort in EL0
 +	b.eq	el0_ia
 +	cmp	x24, #ESR_ELx_EC_FP_ASIMD	// FP/ASIMD access
 +	b.eq	el0_fpsimd_acc
 +	cmp	x24, #ESR_ELx_EC_FP_EXC32	// FP/ASIMD exception
 +	b.eq	el0_fpsimd_exc
 +	cmp	x24, #ESR_ELx_EC_PC_ALIGN	// pc alignment exception
 +	b.eq	el0_pc
 +	cmp	x24, #ESR_ELx_EC_UNKNOWN	// unknown exception in EL0
 +	b.eq	el0_undef
 +	cmp	x24, #ESR_ELx_EC_CP15_32	// CP15 MRC/MCR trap
 +	b.eq	el0_cp15
 +	cmp	x24, #ESR_ELx_EC_CP15_64	// CP15 MRRC/MCRR trap
 +	b.eq	el0_cp15
 +	cmp	x24, #ESR_ELx_EC_CP14_MR	// CP14 MRC/MCR trap
 +	b.eq	el0_undef
 +	cmp	x24, #ESR_ELx_EC_CP14_LS	// CP14 LDC/STC trap
 +	b.eq	el0_undef
 +	cmp	x24, #ESR_ELx_EC_CP14_64	// CP14 MRRC/MCRR trap
 +	b.eq	el0_undef
 +	cmp	x24, #ESR_ELx_EC_BREAKPT_LOW	// debug exception in EL0
 +	b.ge	el0_dbg
 +	b	el0_inv
 +el0_svc_compat:
 +	gic_prio_kentry_setup tmp=x1
  	mov	x0, sp
 -	bl	el0_sync_compat_handler
 +	bl	el0_svc_compat_handler
  	b	ret_to_user
++<<<<<<< HEAD
++=======
+ ENDPROC(el0_sync_compat)
++>>>>>>> 73d6890fe8ff (arm64: kernel: Correct annotation of end of el0_sync)
  
  	.align	6
  el0_irq_compat:
@@@ -808,139 -673,8 +820,143 @@@ ENDPROC(el0_irq_compat
  el0_error_compat:
  	kernel_entry 0, 32
  	b	el0_error_naked
++<<<<<<< HEAD
 +
 +el0_cp15:
 +	/*
 +	 * Trapped CP15 (MRC, MCR, MRRC, MCRR) instructions
 +	 */
 +	ct_user_exit_irqoff
 +	enable_daif
 +	mov	x0, x25
 +	mov	x1, sp
 +	bl	do_cp15instr
 +	b	ret_to_user
++=======
+ ENDPROC(el0_error_compat)
++>>>>>>> 73d6890fe8ff (arm64: kernel: Correct annotation of end of el0_sync)
 +#endif
 +
 +el0_da:
 +	/*
 +	 * Data abort handling
 +	 */
 +	mrs	x26, far_el1
 +	ct_user_exit_irqoff
 +	enable_daif
 +	untagged_addr	x0, x26
 +	mov	x1, x25
 +	mov	x2, sp
 +	bl	do_mem_abort
 +	b	ret_to_user
 +el0_ia:
 +	/*
 +	 * Instruction abort handling
 +	 */
 +	mrs	x26, far_el1
 +	gic_prio_kentry_setup tmp=x0
 +	ct_user_exit_irqoff
 +	enable_da_f
 +#ifdef CONFIG_TRACE_IRQFLAGS
 +	bl	trace_hardirqs_off
 +#endif
 +	mov	x0, x26
 +	mov	x1, x25
 +	mov	x2, sp
 +	bl	do_el0_ia_bp_hardening
 +	b	ret_to_user
 +el0_fpsimd_acc:
 +	/*
 +	 * Floating Point or Advanced SIMD access
 +	 */
 +	ct_user_exit_irqoff
 +	enable_daif
 +	mov	x0, x25
 +	mov	x1, sp
 +	bl	do_fpsimd_acc
 +	b	ret_to_user
 +el0_sve_acc:
 +	/*
 +	 * Scalable Vector Extension access
 +	 */
 +	ct_user_exit_irqoff
 +	enable_daif
 +	mov	x0, x25
 +	mov	x1, sp
 +	bl	do_sve_acc
 +	b	ret_to_user
 +el0_fpsimd_exc:
 +	/*
 +	 * Floating Point, Advanced SIMD or SVE exception
 +	 */
 +	ct_user_exit_irqoff
 +	enable_daif
 +	mov	x0, x25
 +	mov	x1, sp
 +	bl	do_fpsimd_exc
 +	b	ret_to_user
 +el0_sp:
 +	ldr	x26, [sp, #S_SP]
 +	b	el0_sp_pc
 +el0_pc:
 +	mrs	x26, far_el1
 +el0_sp_pc:
 +	/*
 +	 * Stack or PC alignment exception handling
 +	 */
 +	gic_prio_kentry_setup tmp=x0
 +	ct_user_exit_irqoff
 +	enable_da_f
 +#ifdef CONFIG_TRACE_IRQFLAGS
 +	bl	trace_hardirqs_off
  #endif
 +	mov	x0, x26
 +	mov	x1, x25
 +	mov	x2, sp
 +	bl	do_sp_pc_abort
 +	b	ret_to_user
 +el0_undef:
 +	/*
 +	 * Undefined instruction
 +	 */
 +	ct_user_exit_irqoff
 +	enable_daif
 +	mov	x0, sp
 +	bl	do_undefinstr
 +	b	ret_to_user
 +el0_sys:
 +	/*
 +	 * System instructions, for trapped cache maintenance instructions
 +	 */
 +	ct_user_exit_irqoff
 +	enable_daif
 +	mov	x0, x25
 +	mov	x1, sp
 +	bl	do_sysinstr
 +	b	ret_to_user
 +el0_dbg:
 +	/*
 +	 * Debug exception handling
 +	 */
 +	tbnz	x24, #0, el0_inv		// EL0 only
 +	mrs	x24, far_el1
 +	gic_prio_kentry_setup tmp=x3
 +	ct_user_exit_irqoff
 +	mov	x0, x24
 +	mov	x1, x25
 +	mov	x2, sp
 +	bl	do_debug_exception
 +	enable_da_f
 +	b	ret_to_user
 +el0_inv:
 +	ct_user_exit_irqoff
 +	enable_daif
 +	mov	x0, sp
 +	mov	x1, #BAD_SYNC
 +	mov	x2, x25
 +	bl	bad_el0_sync
 +	b	ret_to_user
 +ENDPROC(el0_sync)
  
  	.align	6
  el0_irq:
* Unmerged path arch/arm64/kernel/entry.S
