blk-mq: enforce op-specific segment limits in blk_insert_cloned_request

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-520.el8
commit-author Uday Shankar <ushankar@purestorage.com>
commit 49d24398327e32265eccdeec4baeb5a6a609c0bd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-520.el8/49d24398.failed

The block layer might merge together discard requests up until the
max_discard_segments limit is hit, but blk_insert_cloned_request checks
the segment count against max_segments regardless of the req op. This
can result in errors like the following when discards are issued through
a DM device and max_discard_segments exceeds max_segments for the queue
of the chosen underlying device.

blk_insert_cloned_request: over max segments limit. (256 > 129)

Fix this by looking at the req_op and enforcing the appropriate segment
limit - max_discard_segments for REQ_OP_DISCARDs and max_segments for
everything else.

	Signed-off-by: Uday Shankar <ushankar@purestorage.com>
	Reviewed-by: Keith Busch <kbusch@kernel.org>
	Reviewed-by: Ming Lei <ming.lei@redhat.com>
Link: https://lore.kernel.org/r/20230301000655.48112-1-ushankar@purestorage.com
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 49d24398327e32265eccdeec4baeb5a6a609c0bd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-merge.c
#	block/blk-mq.c
#	block/blk.h
diff --cc block/blk-merge.c
index cd7e1bab1937,ff72edd7ee03..000000000000
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@@ -519,20 -586,31 +519,25 @@@ int __blk_rq_map_sg(struct request_queu
  }
  EXPORT_SYMBOL(__blk_rq_map_sg);
  
 -static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
 -						  sector_t offset)
++<<<<<<< HEAD
 +static inline unsigned int blk_rq_get_max_segments(struct request *rq)
  {
 -	struct request_queue *q = rq->q;
 -	unsigned int max_sectors;
 -
 -	if (blk_rq_is_passthrough(rq))
 -		return q->limits.max_hw_sectors;
 -
 -	max_sectors = blk_queue_get_max_sectors(q, req_op(rq));
 -	if (!q->limits.chunk_sectors ||
 -	    req_op(rq) == REQ_OP_DISCARD ||
 -	    req_op(rq) == REQ_OP_SECURE_ERASE)
 -		return max_sectors;
 -	return min(max_sectors,
 -		   blk_chunk_sectors_left(offset, q->limits.chunk_sectors));
 +	if (req_op(rq) == REQ_OP_DISCARD)
 +		return queue_max_discard_segments(rq->q);
 +	return queue_max_segments(rq->q);
  }
  
 -static inline int ll_new_hw_segment(struct request *req, struct bio *bio,
 -		unsigned int nr_phys_segs)
 +static inline int ll_new_hw_segment(struct request_queue *q,
 +				    struct request *req,
 +				    struct bio *bio)
++=======
++static inline unsigned int blk_rq_get_max_sectors(struct request *rq,
++						  sector_t offset)
++>>>>>>> 49d24398327e (blk-mq: enforce op-specific segment limits in blk_insert_cloned_request)
  {
 -	if (!blk_cgroup_mergeable(req, bio))
 -		goto no_merge;
 +	int nr_phys_segs = bio_phys_segments(q, bio);
  
 -	if (blk_integrity_merge_bio(req->q, req, bio) == false)
 +	if (blk_integrity_merge_bio(q, req, bio) == false)
  		goto no_merge;
  
  	/* discard request merge won't add new segment */
diff --cc block/blk-mq.c
index 4ccb2724b338,7c6f812323af..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -2315,86 -2965,204 +2315,254 @@@ blk_qc_t blk_mq_make_request(struct req
  
  	rq_qos_track(q, rq, bio);
  
 -	blk_mq_bio_to_request(rq, bio, nr_segs);
 +	cookie = request_to_qc_t(data.hctx, rq);
  
 -	ret = blk_crypto_init_request(rq);
 -	if (ret != BLK_STS_OK) {
 -		bio->bi_status = ret;
 -		bio_endio(bio);
 -		blk_mq_free_request(rq);
 -		return;
 -	}
 +	blk_mq_bio_to_request(rq, bio);
  
 -	if (op_is_flush(bio->bi_opf)) {
 +	plug = blk_mq_plug(q, bio);
 +	if (unlikely(is_flush_fua)) {
 +		/* Bypass scheduler for flush requests */
  		blk_insert_flush(rq);
++<<<<<<< HEAD
 +		blk_mq_run_hw_queue(data.hctx, true);
 +	} else if (plug && (q->nr_hw_queues == 1 ||
 +		   blk_mq_is_sbitmap_shared(rq->mq_hctx->flags) ||
 +		   q->mq_ops->commit_rqs || !blk_queue_nonrot(q))) {
++=======
+ 		return;
+ 	}
+ 
+ 	if (plug)
+ 		blk_add_rq_to_plug(plug, rq);
+ 	else if ((rq->rq_flags & RQF_ELV) ||
+ 		 (rq->mq_hctx->dispatch_busy &&
+ 		  (q->nr_hw_queues == 1 || !is_sync)))
+ 		blk_mq_sched_insert_request(rq, false, true, true);
+ 	else
+ 		blk_mq_run_dispatch_ops(rq->q,
+ 				blk_mq_try_issue_directly(rq->mq_hctx, rq));
+ }
+ 
+ #ifdef CONFIG_BLK_MQ_STACKING
+ /**
+  * blk_insert_cloned_request - Helper for stacking drivers to submit a request
+  * @rq: the request being queued
+  */
+ blk_status_t blk_insert_cloned_request(struct request *rq)
+ {
+ 	struct request_queue *q = rq->q;
+ 	unsigned int max_sectors = blk_queue_get_max_sectors(q, req_op(rq));
+ 	unsigned int max_segments = blk_rq_get_max_segments(rq);
+ 	blk_status_t ret;
+ 
+ 	if (blk_rq_sectors(rq) > max_sectors) {
++>>>>>>> 49d24398327e (blk-mq: enforce op-specific segment limits in blk_insert_cloned_request)
  		/*
 -		 * SCSI device does not have a good way to return if
 -		 * Write Same/Zero is actually supported. If a device rejects
 -		 * a non-read/write command (discard, write same,etc.) the
 -		 * low-level device driver will set the relevant queue limit to
 -		 * 0 to prevent blk-lib from issuing more of the offending
 -		 * operations. Commands queued prior to the queue limit being
 -		 * reset need to be completed with BLK_STS_NOTSUPP to avoid I/O
 -		 * errors being propagated to upper layers.
 +		 * Use plugging if we have a ->commit_rqs() hook as well, as
 +		 * we know the driver uses bd->last in a smart fashion.
 +		 *
 +		 * Use normal plugging if this disk is slow HDD, as sequential
 +		 * IO may benefit a lot from plug merging.
  		 */
 -		if (max_sectors == 0)
 -			return BLK_STS_NOTSUPP;
 +		unsigned int request_count = plug->rq_count;
 +		struct request *last = NULL;
  
++<<<<<<< HEAD
 +		if (!request_count)
 +			trace_block_plug(q);
++=======
+ 		printk(KERN_ERR "%s: over max size limit. (%u > %u)\n",
+ 			__func__, blk_rq_sectors(rq), max_sectors);
+ 		return BLK_STS_IOERR;
+ 	}
+ 
+ 	/*
+ 	 * The queue settings related to segment counting may differ from the
+ 	 * original queue.
+ 	 */
+ 	rq->nr_phys_segments = blk_recalc_rq_segments(rq);
+ 	if (rq->nr_phys_segments > max_segments) {
+ 		printk(KERN_ERR "%s: over max segments limit. (%u > %u)\n",
+ 			__func__, rq->nr_phys_segments, max_segments);
+ 		return BLK_STS_IOERR;
+ 	}
+ 
+ 	if (q->disk && should_fail_request(q->disk->part0, blk_rq_bytes(rq)))
+ 		return BLK_STS_IOERR;
+ 
+ 	if (blk_crypto_insert_cloned_request(rq))
+ 		return BLK_STS_IOERR;
+ 
+ 	blk_account_io_start(rq);
+ 
+ 	/*
+ 	 * Since we have a scheduler attached on the top device,
+ 	 * bypass a potential scheduler on the bottom device for
+ 	 * insert.
+ 	 */
+ 	blk_mq_run_dispatch_ops(q,
+ 			ret = blk_mq_request_issue_directly(rq, true));
+ 	if (ret)
+ 		blk_account_io_done(rq, ktime_get_ns());
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(blk_insert_cloned_request);
+ 
+ /**
+  * blk_rq_unprep_clone - Helper function to free all bios in a cloned request
+  * @rq: the clone request to be cleaned up
+  *
+  * Description:
+  *     Free all bios in @rq for a cloned request.
+  */
+ void blk_rq_unprep_clone(struct request *rq)
+ {
+ 	struct bio *bio;
+ 
+ 	while ((bio = rq->bio) != NULL) {
+ 		rq->bio = bio->bi_next;
+ 
+ 		bio_put(bio);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(blk_rq_unprep_clone);
+ 
+ /**
+  * blk_rq_prep_clone - Helper function to setup clone request
+  * @rq: the request to be setup
+  * @rq_src: original request to be cloned
+  * @bs: bio_set that bios for clone are allocated from
+  * @gfp_mask: memory allocation mask for bio
+  * @bio_ctr: setup function to be called for each clone bio.
+  *           Returns %0 for success, non %0 for failure.
+  * @data: private data to be passed to @bio_ctr
+  *
+  * Description:
+  *     Clones bios in @rq_src to @rq, and copies attributes of @rq_src to @rq.
+  *     Also, pages which the original bios are pointing to are not copied
+  *     and the cloned bios just point same pages.
+  *     So cloned bios must be completed before original bios, which means
+  *     the caller must complete @rq before @rq_src.
+  */
+ int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
+ 		      struct bio_set *bs, gfp_t gfp_mask,
+ 		      int (*bio_ctr)(struct bio *, struct bio *, void *),
+ 		      void *data)
+ {
+ 	struct bio *bio, *bio_src;
+ 
+ 	if (!bs)
+ 		bs = &fs_bio_set;
+ 
+ 	__rq_for_each_bio(bio_src, rq_src) {
+ 		bio = bio_alloc_clone(rq->q->disk->part0, bio_src, gfp_mask,
+ 				      bs);
+ 		if (!bio)
+ 			goto free_and_out;
+ 
+ 		if (bio_ctr && bio_ctr(bio, bio_src, data))
+ 			goto free_and_out;
+ 
+ 		if (rq->bio) {
+ 			rq->biotail->bi_next = bio;
+ 			rq->biotail = bio;
+ 		} else {
+ 			rq->bio = rq->biotail = bio;
+ 		}
+ 		bio = NULL;
+ 	}
+ 
+ 	/* Copy attributes of the original request to the clone request. */
+ 	rq->__sector = blk_rq_pos(rq_src);
+ 	rq->__data_len = blk_rq_bytes(rq_src);
+ 	if (rq_src->rq_flags & RQF_SPECIAL_PAYLOAD) {
+ 		rq->rq_flags |= RQF_SPECIAL_PAYLOAD;
+ 		rq->special_vec = rq_src->special_vec;
+ 	}
+ 	rq->nr_phys_segments = rq_src->nr_phys_segments;
+ 	rq->ioprio = rq_src->ioprio;
+ 
+ 	if (rq->bio && blk_crypto_rq_bio_prep(rq, rq->bio, gfp_mask) < 0)
+ 		goto free_and_out;
+ 
+ 	return 0;
+ 
+ free_and_out:
+ 	if (bio)
+ 		bio_put(bio);
+ 	blk_rq_unprep_clone(rq);
+ 
+ 	return -ENOMEM;
+ }
+ EXPORT_SYMBOL_GPL(blk_rq_prep_clone);
+ #endif /* CONFIG_BLK_MQ_STACKING */
+ 
+ /*
+  * Steal bios from a request and add them to a bio list.
+  * The request must not have been partially completed before.
+  */
+ void blk_steal_bios(struct bio_list *list, struct request *rq)
+ {
+ 	if (rq->bio) {
+ 		if (list->tail)
+ 			list->tail->bi_next = rq->bio;
++>>>>>>> 49d24398327e (blk-mq: enforce op-specific segment limits in blk_insert_cloned_request)
  		else
 -			list->head = rq->bio;
 -		list->tail = rq->biotail;
 +			last = list_entry_rq(plug->mq_list.prev);
 +
 +		if (request_count >= blk_plug_max_rq_count(plug) || (last &&
 +		    blk_rq_bytes(last) >= BLK_PLUG_FLUSH_SIZE)) {
 +			blk_flush_plug_list(plug, false);
 +			trace_block_plug(q);
 +		}
 +
 +		blk_add_rq_to_plug(plug, rq);
 +	} else if (q->elevator) {
 +		/* Insert the request at the IO scheduler queue */
 +		blk_mq_sched_insert_request(rq, false, true, true);
 +	} else if (plug && !blk_queue_nomerges(q)) {
 +		/*
 +		 * We do limited plugging. If the bio can be merged, do that.
 +		 * Otherwise the existing request in the plug list will be
 +		 * issued. So the plug list will have one request at most
 +		 * The plug list might get flushed before this. If that happens,
 +		 * the plug list is empty, and same_queue_rq is invalid.
 +		 */
 +		if (list_empty(&plug->mq_list))
 +			same_queue_rq = NULL;
 +		if (same_queue_rq) {
 +			list_del_init(&same_queue_rq->queuelist);
 +			plug->rq_count--;
 +		}
 +		blk_add_rq_to_plug(plug, rq);
 +		trace_block_plug(q);
  
 -		rq->bio = NULL;
 -		rq->biotail = NULL;
 +		if (same_queue_rq) {
 +			data.hctx = same_queue_rq->mq_hctx;
 +			trace_block_unplug(q, 1, true);
 +			blk_mq_try_issue_directly(data.hctx, same_queue_rq,
 +					&cookie);
 +		}
 +	} else if ((q->nr_hw_queues > 1 && is_sync) ||
 +			!data.hctx->dispatch_busy) {
 +		/*
 +		 * There is no scheduler and we can try to send directly
 +		 * to the hardware.
 +		 */
 +		blk_mq_try_issue_directly(data.hctx, rq, &cookie);
 +	} else {
 +		/* Default case. */
 +		blk_mq_sched_insert_request(rq, false, true, true);
  	}
  
 -	rq->__data_len = 0;
 +	if (!hipri)
 +		return BLK_QC_T_NONE;
 +	return cookie;
 +queue_exit:
 +	blk_queue_exit(q);
 +	return BLK_QC_T_NONE;
  }
 -EXPORT_SYMBOL_GPL(blk_steal_bios);
 +EXPORT_SYMBOL_GPL(blk_mq_make_request);
  
  static size_t order_to_size(unsigned int order)
  {
diff --cc block/blk.h
index e7208bc9c7e3,cc4e8873dfde..000000000000
--- a/block/blk.h
+++ b/block/blk.h
@@@ -91,12 -110,70 +91,74 @@@ static inline bool __bvec_gap_to_prev(s
   * Check if adding a bio_vec after bprv with offset would create a gap in
   * the SG list. Most drivers don't care about this, but some do.
   */
 -static inline bool bvec_gap_to_prev(const struct queue_limits *lim,
 +static inline bool bvec_gap_to_prev(struct request_queue *q,
  		struct bio_vec *bprv, unsigned int offset)
  {
 -	if (!lim->virt_boundary_mask)
 +	if (!queue_virt_boundary(q))
  		return false;
++<<<<<<< HEAD
 +	return __bvec_gap_to_prev(q, bprv, offset);
++=======
+ 	return __bvec_gap_to_prev(lim, bprv, offset);
+ }
+ 
+ static inline bool rq_mergeable(struct request *rq)
+ {
+ 	if (blk_rq_is_passthrough(rq))
+ 		return false;
+ 
+ 	if (req_op(rq) == REQ_OP_FLUSH)
+ 		return false;
+ 
+ 	if (req_op(rq) == REQ_OP_WRITE_ZEROES)
+ 		return false;
+ 
+ 	if (req_op(rq) == REQ_OP_ZONE_APPEND)
+ 		return false;
+ 
+ 	if (rq->cmd_flags & REQ_NOMERGE_FLAGS)
+ 		return false;
+ 	if (rq->rq_flags & RQF_NOMERGE_FLAGS)
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ /*
+  * There are two different ways to handle DISCARD merges:
+  *  1) If max_discard_segments > 1, the driver treats every bio as a range and
+  *     send the bios to controller together. The ranges don't need to be
+  *     contiguous.
+  *  2) Otherwise, the request will be normal read/write requests.  The ranges
+  *     need to be contiguous.
+  */
+ static inline bool blk_discard_mergable(struct request *req)
+ {
+ 	if (req_op(req) == REQ_OP_DISCARD &&
+ 	    queue_max_discard_segments(req->q) > 1)
+ 		return true;
+ 	return false;
+ }
+ 
+ static inline unsigned int blk_rq_get_max_segments(struct request *rq)
+ {
+ 	if (req_op(rq) == REQ_OP_DISCARD)
+ 		return queue_max_discard_segments(rq->q);
+ 	return queue_max_segments(rq->q);
+ }
+ 
+ static inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,
+ 						     enum req_op op)
+ {
+ 	if (unlikely(op == REQ_OP_DISCARD || op == REQ_OP_SECURE_ERASE))
+ 		return min(q->limits.max_discard_sectors,
+ 			   UINT_MAX >> SECTOR_SHIFT);
+ 
+ 	if (unlikely(op == REQ_OP_WRITE_ZEROES))
+ 		return q->limits.max_write_zeroes_sectors;
+ 
+ 	return q->limits.max_sectors;
++>>>>>>> 49d24398327e (blk-mq: enforce op-specific segment limits in blk_insert_cloned_request)
  }
  
  #ifdef CONFIG_BLK_DEV_INTEGRITY
* Unmerged path block/blk-merge.c
* Unmerged path block/blk-mq.c
* Unmerged path block/blk.h
