IB/rdmavt: Fix concurrency panics in QP post_send and modify to error

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Michael J. Ruhl <michael.j.ruhl@intel.com>
commit d757c60eca9b22f4d108929a24401e0fdecda0b1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/d757c60e.failed

The RC/UC code path can go through a software loopback. In this code path
the receive side QP is manipulated.

If two threads are working on the QP receive side (i.e. post_send, and
modify_qp to an error state), QP information can be corrupted.

(post_send via loopback)
  set r_sge
  loop
     update r_sge
(modify_qp)
     take r_lock
     update r_sge <---- r_sge is now incorrect
(post_send)
     update r_sge <---- crash, etc.
     ...

This can lead to one of the two following crashes:

 BUG: unable to handle kernel NULL pointer dereference at (null)
  IP:  hfi1_copy_sge+0xf1/0x2e0 [hfi1]
  PGD 8000001fe6a57067 PUD 1fd9e0c067 PMD 0
 Call Trace:
  ruc_loopback+0x49b/0xbc0 [hfi1]
  hfi1_do_send+0x38e/0x3e0 [hfi1]
  _hfi1_do_send+0x1e/0x20 [hfi1]
  process_one_work+0x17f/0x440
  worker_thread+0x126/0x3c0
  kthread+0xd1/0xe0
  ret_from_fork_nospec_begin+0x21/0x21

or:

 BUG: unable to handle kernel NULL pointer dereference at 0000000000000048
  IP:  rvt_clear_mr_refs+0x45/0x370 [rdmavt]
  PGD 80000006ae5eb067 PUD ef15d0067 PMD 0
 Call Trace:
  rvt_error_qp+0xaa/0x240 [rdmavt]
  rvt_modify_qp+0x47f/0xaa0 [rdmavt]
  ib_security_modify_qp+0x8f/0x400 [ib_core]
  ib_modify_qp_with_udata+0x44/0x70 [ib_core]
  modify_qp.isra.23+0x1eb/0x2b0 [ib_uverbs]
  ib_uverbs_modify_qp+0xaa/0xf0 [ib_uverbs]
  ib_uverbs_write+0x272/0x430 [ib_uverbs]
  vfs_write+0xc0/0x1f0
  SyS_write+0x7f/0xf0
  system_call_fastpath+0x1c/0x21

Fix by using the appropriate locking on the receiving QP.

Fixes: 15703461533a ("IB/{hfi1, qib, rdmavt}: Move ruc_loopback to rdmavt")
	Cc: <stable@vger.kernel.org> #v4.9+
	Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
	Signed-off-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
	Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
	Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
(cherry picked from commit d757c60eca9b22f4d108929a24401e0fdecda0b1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/sw/rdmavt/qp.c
diff --cc drivers/infiniband/sw/rdmavt/qp.c
index 7b45580bb62e,a34b9a2a32b6..000000000000
--- a/drivers/infiniband/sw/rdmavt/qp.c
+++ b/drivers/infiniband/sw/rdmavt/qp.c
@@@ -2476,3 -2669,456 +2476,459 @@@ void rvt_qp_iter(struct rvt_dev_info *r
  	rcu_read_unlock();
  }
  EXPORT_SYMBOL(rvt_qp_iter);
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * This should be called with s_lock held.
+  */
+ void rvt_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,
+ 		       enum ib_wc_status status)
+ {
+ 	u32 old_last, last;
+ 	struct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);
+ 
+ 	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_OR_FLUSH_SEND))
+ 		return;
+ 
+ 	last = qp->s_last;
+ 	old_last = last;
+ 	trace_rvt_qp_send_completion(qp, wqe, last);
+ 	if (++last >= qp->s_size)
+ 		last = 0;
+ 	trace_rvt_qp_send_completion(qp, wqe, last);
+ 	qp->s_last = last;
+ 	/* See post_send() */
+ 	barrier();
+ 	rvt_put_swqe(wqe);
+ 	if (qp->ibqp.qp_type == IB_QPT_UD ||
+ 	    qp->ibqp.qp_type == IB_QPT_SMI ||
+ 	    qp->ibqp.qp_type == IB_QPT_GSI)
+ 		atomic_dec(&ibah_to_rvtah(wqe->ud_wr.ah)->refcount);
+ 
+ 	rvt_qp_swqe_complete(qp,
+ 			     wqe,
+ 			     rdi->wc_opcode[wqe->wr.opcode],
+ 			     status);
+ 
+ 	if (qp->s_acked == old_last)
+ 		qp->s_acked = last;
+ 	if (qp->s_cur == old_last)
+ 		qp->s_cur = last;
+ 	if (qp->s_tail == old_last)
+ 		qp->s_tail = last;
+ 	if (qp->state == IB_QPS_SQD && last == qp->s_cur)
+ 		qp->s_draining = 0;
+ }
+ EXPORT_SYMBOL(rvt_send_complete);
+ 
+ /**
+  * rvt_copy_sge - copy data to SGE memory
+  * @qp: associated QP
+  * @ss: the SGE state
+  * @data: the data to copy
+  * @length: the length of the data
+  * @release: boolean to release MR
+  * @copy_last: do a separate copy of the last 8 bytes
+  */
+ void rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,
+ 		  void *data, u32 length,
+ 		  bool release, bool copy_last)
+ {
+ 	struct rvt_sge *sge = &ss->sge;
+ 	int i;
+ 	bool in_last = false;
+ 	bool cacheless_copy = false;
+ 	struct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);
+ 	struct rvt_wss *wss = rdi->wss;
+ 	unsigned int sge_copy_mode = rdi->dparms.sge_copy_mode;
+ 
+ 	if (sge_copy_mode == RVT_SGE_COPY_CACHELESS) {
+ 		cacheless_copy = length >= PAGE_SIZE;
+ 	} else if (sge_copy_mode == RVT_SGE_COPY_ADAPTIVE) {
+ 		if (length >= PAGE_SIZE) {
+ 			/*
+ 			 * NOTE: this *assumes*:
+ 			 * o The first vaddr is the dest.
+ 			 * o If multiple pages, then vaddr is sequential.
+ 			 */
+ 			wss_insert(wss, sge->vaddr);
+ 			if (length >= (2 * PAGE_SIZE))
+ 				wss_insert(wss, (sge->vaddr + PAGE_SIZE));
+ 
+ 			cacheless_copy = wss_exceeds_threshold(wss);
+ 		} else {
+ 			wss_advance_clean_counter(wss);
+ 		}
+ 	}
+ 
+ 	if (copy_last) {
+ 		if (length > 8) {
+ 			length -= 8;
+ 		} else {
+ 			copy_last = false;
+ 			in_last = true;
+ 		}
+ 	}
+ 
+ again:
+ 	while (length) {
+ 		u32 len = rvt_get_sge_length(sge, length);
+ 
+ 		WARN_ON_ONCE(len == 0);
+ 		if (unlikely(in_last)) {
+ 			/* enforce byte transfer ordering */
+ 			for (i = 0; i < len; i++)
+ 				((u8 *)sge->vaddr)[i] = ((u8 *)data)[i];
+ 		} else if (cacheless_copy) {
+ 			cacheless_memcpy(sge->vaddr, data, len);
+ 		} else {
+ 			memcpy(sge->vaddr, data, len);
+ 		}
+ 		rvt_update_sge(ss, len, release);
+ 		data += len;
+ 		length -= len;
+ 	}
+ 
+ 	if (copy_last) {
+ 		copy_last = false;
+ 		in_last = true;
+ 		length = 8;
+ 		goto again;
+ 	}
+ }
+ EXPORT_SYMBOL(rvt_copy_sge);
+ 
+ static enum ib_wc_status loopback_qp_drop(struct rvt_ibport *rvp,
+ 					  struct rvt_qp *sqp)
+ {
+ 	rvp->n_pkt_drops++;
+ 	/*
+ 	 * For RC, the requester would timeout and retry so
+ 	 * shortcut the timeouts and just signal too many retries.
+ 	 */
+ 	return sqp->ibqp.qp_type == IB_QPT_RC ?
+ 		IB_WC_RETRY_EXC_ERR : IB_WC_SUCCESS;
+ }
+ 
+ /**
+  * ruc_loopback - handle UC and RC loopback requests
+  * @sqp: the sending QP
+  *
+  * This is called from rvt_do_send() to forward a WQE addressed to the same HFI
+  * Note that although we are single threaded due to the send engine, we still
+  * have to protect against post_send().  We don't have to worry about
+  * receive interrupts since this is a connected protocol and all packets
+  * will pass through here.
+  */
+ void rvt_ruc_loopback(struct rvt_qp *sqp)
+ {
+ 	struct rvt_ibport *rvp =  NULL;
+ 	struct rvt_dev_info *rdi = ib_to_rvt(sqp->ibqp.device);
+ 	struct rvt_qp *qp;
+ 	struct rvt_swqe *wqe;
+ 	struct rvt_sge *sge;
+ 	unsigned long flags;
+ 	struct ib_wc wc;
+ 	u64 sdata;
+ 	atomic64_t *maddr;
+ 	enum ib_wc_status send_status;
+ 	bool release;
+ 	int ret;
+ 	bool copy_last = false;
+ 	int local_ops = 0;
+ 
+ 	rcu_read_lock();
+ 	rvp = rdi->ports[sqp->port_num - 1];
+ 
+ 	/*
+ 	 * Note that we check the responder QP state after
+ 	 * checking the requester's state.
+ 	 */
+ 
+ 	qp = rvt_lookup_qpn(ib_to_rvt(sqp->ibqp.device), rvp,
+ 			    sqp->remote_qpn);
+ 
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 
+ 	/* Return if we are already busy processing a work request. */
+ 	if ((sqp->s_flags & (RVT_S_BUSY | RVT_S_ANY_WAIT)) ||
+ 	    !(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_OR_FLUSH_SEND))
+ 		goto unlock;
+ 
+ 	sqp->s_flags |= RVT_S_BUSY;
+ 
+ again:
+ 	if (sqp->s_last == READ_ONCE(sqp->s_head))
+ 		goto clr_busy;
+ 	wqe = rvt_get_swqe_ptr(sqp, sqp->s_last);
+ 
+ 	/* Return if it is not OK to start a new work request. */
+ 	if (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_NEXT_SEND_OK)) {
+ 		if (!(ib_rvt_state_ops[sqp->state] & RVT_FLUSH_SEND))
+ 			goto clr_busy;
+ 		/* We are in the error state, flush the work request. */
+ 		send_status = IB_WC_WR_FLUSH_ERR;
+ 		goto flush_send;
+ 	}
+ 
+ 	/*
+ 	 * We can rely on the entry not changing without the s_lock
+ 	 * being held until we update s_last.
+ 	 * We increment s_cur to indicate s_last is in progress.
+ 	 */
+ 	if (sqp->s_last == sqp->s_cur) {
+ 		if (++sqp->s_cur >= sqp->s_size)
+ 			sqp->s_cur = 0;
+ 	}
+ 	spin_unlock_irqrestore(&sqp->s_lock, flags);
+ 
+ 	if (!qp) {
+ 		send_status = loopback_qp_drop(rvp, sqp);
+ 		goto serr_no_r_lock;
+ 	}
+ 	spin_lock_irqsave(&qp->r_lock, flags);
+ 	if (!(ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) ||
+ 	    qp->ibqp.qp_type != sqp->ibqp.qp_type) {
+ 		send_status = loopback_qp_drop(rvp, sqp);
+ 		goto serr;
+ 	}
+ 
+ 	memset(&wc, 0, sizeof(wc));
+ 	send_status = IB_WC_SUCCESS;
+ 
+ 	release = true;
+ 	sqp->s_sge.sge = wqe->sg_list[0];
+ 	sqp->s_sge.sg_list = wqe->sg_list + 1;
+ 	sqp->s_sge.num_sge = wqe->wr.num_sge;
+ 	sqp->s_len = wqe->length;
+ 	switch (wqe->wr.opcode) {
+ 	case IB_WR_REG_MR:
+ 		goto send_comp;
+ 
+ 	case IB_WR_LOCAL_INV:
+ 		if (!(wqe->wr.send_flags & RVT_SEND_COMPLETION_ONLY)) {
+ 			if (rvt_invalidate_rkey(sqp,
+ 						wqe->wr.ex.invalidate_rkey))
+ 				send_status = IB_WC_LOC_PROT_ERR;
+ 			local_ops = 1;
+ 		}
+ 		goto send_comp;
+ 
+ 	case IB_WR_SEND_WITH_INV:
+ 	case IB_WR_SEND_WITH_IMM:
+ 	case IB_WR_SEND:
+ 		ret = rvt_get_rwqe(qp, false);
+ 		if (ret < 0)
+ 			goto op_err;
+ 		if (!ret)
+ 			goto rnr_nak;
+ 		if (wqe->length > qp->r_len)
+ 			goto inv_err;
+ 		switch (wqe->wr.opcode) {
+ 		case IB_WR_SEND_WITH_INV:
+ 			if (!rvt_invalidate_rkey(qp,
+ 						 wqe->wr.ex.invalidate_rkey)) {
+ 				wc.wc_flags = IB_WC_WITH_INVALIDATE;
+ 				wc.ex.invalidate_rkey =
+ 					wqe->wr.ex.invalidate_rkey;
+ 			}
+ 			break;
+ 		case IB_WR_SEND_WITH_IMM:
+ 			wc.wc_flags = IB_WC_WITH_IMM;
+ 			wc.ex.imm_data = wqe->wr.ex.imm_data;
+ 			break;
+ 		default:
+ 			break;
+ 		}
+ 		break;
+ 
+ 	case IB_WR_RDMA_WRITE_WITH_IMM:
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))
+ 			goto inv_err;
+ 		wc.wc_flags = IB_WC_WITH_IMM;
+ 		wc.ex.imm_data = wqe->wr.ex.imm_data;
+ 		ret = rvt_get_rwqe(qp, true);
+ 		if (ret < 0)
+ 			goto op_err;
+ 		if (!ret)
+ 			goto rnr_nak;
+ 		/* skip copy_last set and qp_access_flags recheck */
+ 		goto do_write;
+ 	case IB_WR_RDMA_WRITE:
+ 		copy_last = rvt_is_user_qp(qp);
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))
+ 			goto inv_err;
+ do_write:
+ 		if (wqe->length == 0)
+ 			break;
+ 		if (unlikely(!rvt_rkey_ok(qp, &qp->r_sge.sge, wqe->length,
+ 					  wqe->rdma_wr.remote_addr,
+ 					  wqe->rdma_wr.rkey,
+ 					  IB_ACCESS_REMOTE_WRITE)))
+ 			goto acc_err;
+ 		qp->r_sge.sg_list = NULL;
+ 		qp->r_sge.num_sge = 1;
+ 		qp->r_sge.total_len = wqe->length;
+ 		break;
+ 
+ 	case IB_WR_RDMA_READ:
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_READ)))
+ 			goto inv_err;
+ 		if (unlikely(!rvt_rkey_ok(qp, &sqp->s_sge.sge, wqe->length,
+ 					  wqe->rdma_wr.remote_addr,
+ 					  wqe->rdma_wr.rkey,
+ 					  IB_ACCESS_REMOTE_READ)))
+ 			goto acc_err;
+ 		release = false;
+ 		sqp->s_sge.sg_list = NULL;
+ 		sqp->s_sge.num_sge = 1;
+ 		qp->r_sge.sge = wqe->sg_list[0];
+ 		qp->r_sge.sg_list = wqe->sg_list + 1;
+ 		qp->r_sge.num_sge = wqe->wr.num_sge;
+ 		qp->r_sge.total_len = wqe->length;
+ 		break;
+ 
+ 	case IB_WR_ATOMIC_CMP_AND_SWP:
+ 	case IB_WR_ATOMIC_FETCH_AND_ADD:
+ 		if (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_ATOMIC)))
+ 			goto inv_err;
+ 		if (unlikely(!rvt_rkey_ok(qp, &qp->r_sge.sge, sizeof(u64),
+ 					  wqe->atomic_wr.remote_addr,
+ 					  wqe->atomic_wr.rkey,
+ 					  IB_ACCESS_REMOTE_ATOMIC)))
+ 			goto acc_err;
+ 		/* Perform atomic OP and save result. */
+ 		maddr = (atomic64_t *)qp->r_sge.sge.vaddr;
+ 		sdata = wqe->atomic_wr.compare_add;
+ 		*(u64 *)sqp->s_sge.sge.vaddr =
+ 			(wqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD) ?
+ 			(u64)atomic64_add_return(sdata, maddr) - sdata :
+ 			(u64)cmpxchg((u64 *)qp->r_sge.sge.vaddr,
+ 				      sdata, wqe->atomic_wr.swap);
+ 		rvt_put_mr(qp->r_sge.sge.mr);
+ 		qp->r_sge.num_sge = 0;
+ 		goto send_comp;
+ 
+ 	default:
+ 		send_status = IB_WC_LOC_QP_OP_ERR;
+ 		goto serr;
+ 	}
+ 
+ 	sge = &sqp->s_sge.sge;
+ 	while (sqp->s_len) {
+ 		u32 len = rvt_get_sge_length(sge, sqp->s_len);
+ 
+ 		WARN_ON_ONCE(len == 0);
+ 		rvt_copy_sge(qp, &qp->r_sge, sge->vaddr,
+ 			     len, release, copy_last);
+ 		rvt_update_sge(&sqp->s_sge, len, !release);
+ 		sqp->s_len -= len;
+ 	}
+ 	if (release)
+ 		rvt_put_ss(&qp->r_sge);
+ 
+ 	if (!test_and_clear_bit(RVT_R_WRID_VALID, &qp->r_aflags))
+ 		goto send_comp;
+ 
+ 	if (wqe->wr.opcode == IB_WR_RDMA_WRITE_WITH_IMM)
+ 		wc.opcode = IB_WC_RECV_RDMA_WITH_IMM;
+ 	else
+ 		wc.opcode = IB_WC_RECV;
+ 	wc.wr_id = qp->r_wr_id;
+ 	wc.status = IB_WC_SUCCESS;
+ 	wc.byte_len = wqe->length;
+ 	wc.qp = &qp->ibqp;
+ 	wc.src_qp = qp->remote_qpn;
+ 	wc.slid = rdma_ah_get_dlid(&qp->remote_ah_attr) & U16_MAX;
+ 	wc.sl = rdma_ah_get_sl(&qp->remote_ah_attr);
+ 	wc.port_num = 1;
+ 	/* Signal completion event if the solicited bit is set. */
+ 	rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.recv_cq), &wc,
+ 		     wqe->wr.send_flags & IB_SEND_SOLICITED);
+ 
+ send_comp:
+ 	spin_unlock_irqrestore(&qp->r_lock, flags);
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 	rvp->n_loop_pkts++;
+ flush_send:
+ 	sqp->s_rnr_retry = sqp->s_rnr_retry_cnt;
+ 	rvt_send_complete(sqp, wqe, send_status);
+ 	if (local_ops) {
+ 		atomic_dec(&sqp->local_ops_pending);
+ 		local_ops = 0;
+ 	}
+ 	goto again;
+ 
+ rnr_nak:
+ 	/* Handle RNR NAK */
+ 	if (qp->ibqp.qp_type == IB_QPT_UC)
+ 		goto send_comp;
+ 	rvp->n_rnr_naks++;
+ 	/*
+ 	 * Note: we don't need the s_lock held since the BUSY flag
+ 	 * makes this single threaded.
+ 	 */
+ 	if (sqp->s_rnr_retry == 0) {
+ 		send_status = IB_WC_RNR_RETRY_EXC_ERR;
+ 		goto serr;
+ 	}
+ 	if (sqp->s_rnr_retry_cnt < 7)
+ 		sqp->s_rnr_retry--;
+ 	spin_unlock_irqrestore(&qp->r_lock, flags);
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 	if (!(ib_rvt_state_ops[sqp->state] & RVT_PROCESS_RECV_OK))
+ 		goto clr_busy;
+ 	rvt_add_rnr_timer(sqp, qp->r_min_rnr_timer <<
+ 				IB_AETH_CREDIT_SHIFT);
+ 	goto clr_busy;
+ 
+ op_err:
+ 	send_status = IB_WC_REM_OP_ERR;
+ 	wc.status = IB_WC_LOC_QP_OP_ERR;
+ 	goto err;
+ 
+ inv_err:
+ 	send_status =
+ 		sqp->ibqp.qp_type == IB_QPT_RC ?
+ 			IB_WC_REM_INV_REQ_ERR :
+ 			IB_WC_SUCCESS;
+ 	wc.status = IB_WC_LOC_QP_OP_ERR;
+ 	goto err;
+ 
+ acc_err:
+ 	send_status = IB_WC_REM_ACCESS_ERR;
+ 	wc.status = IB_WC_LOC_PROT_ERR;
+ err:
+ 	/* responder goes to error state */
+ 	rvt_rc_error(qp, wc.status);
+ 
+ serr:
+ 	spin_unlock_irqrestore(&qp->r_lock, flags);
+ serr_no_r_lock:
+ 	spin_lock_irqsave(&sqp->s_lock, flags);
+ 	rvt_send_complete(sqp, wqe, send_status);
+ 	if (sqp->ibqp.qp_type == IB_QPT_RC) {
+ 		int lastwqe = rvt_error_qp(sqp, IB_WC_WR_FLUSH_ERR);
+ 
+ 		sqp->s_flags &= ~RVT_S_BUSY;
+ 		spin_unlock_irqrestore(&sqp->s_lock, flags);
+ 		if (lastwqe) {
+ 			struct ib_event ev;
+ 
+ 			ev.device = sqp->ibqp.device;
+ 			ev.element.qp = &sqp->ibqp;
+ 			ev.event = IB_EVENT_QP_LAST_WQE_REACHED;
+ 			sqp->ibqp.event_handler(&ev, sqp->ibqp.qp_context);
+ 		}
+ 		goto done;
+ 	}
+ clr_busy:
+ 	sqp->s_flags &= ~RVT_S_BUSY;
+ unlock:
+ 	spin_unlock_irqrestore(&sqp->s_lock, flags);
+ done:
+ 	rcu_read_unlock();
+ }
+ EXPORT_SYMBOL(rvt_ruc_loopback);
++>>>>>>> d757c60eca9b (IB/rdmavt: Fix concurrency panics in QP post_send and modify to error)
* Unmerged path drivers/infiniband/sw/rdmavt/qp.c
