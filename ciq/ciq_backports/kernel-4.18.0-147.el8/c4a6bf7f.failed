xfs: don't ever put nlink > 0 inodes on the unlinked list

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Darrick J. Wong <darrick.wong@oracle.com>
commit c4a6bf7f6cc7eb4cce120fb7eb1e1fb8b2d65e09
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/c4a6bf7f.failed

When XFS creates an O_TMPFILE file, the inode is created with nlink = 1,
put on the unlinked list, and then the VFS sets nlink = 0 in d_tmpfile.
If we crash before anything logs the inode (it's dirty incore but the
vfs doesn't tell us it's dirty so we never log that change), the iunlink
processing part of recovery will then explode with a pile of:

XFS: Assertion failed: VFS_I(ip)->i_nlink == 0, file:
fs/xfs/xfs_log_recover.c, line: 5072

Worse yet, since nlink is nonzero, the inodes also don't get cleaned up
and they just leak until the next xfs_repair run.

Therefore, change xfs_iunlink to require that inodes being put on the
unlinked list have nlink == 0, change the tmpfile callers to instantiate
nodes that way, and set the nlink to 1 just prior to calling d_tmpfile.
Fix the comment for xfs_iunlink while we're at it.

	Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit c4a6bf7f6cc7eb4cce120fb7eb1e1fb8b2d65e09)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/xfs/xfs_inode.c
diff --cc fs/xfs/xfs_inode.c
index a7acf67ba1b8,f643a9295179..000000000000
--- a/fs/xfs/xfs_inode.c
+++ b/fs/xfs/xfs_inode.c
@@@ -1907,37 -1907,356 +1907,367 @@@ xfs_inactive
  }
  
  /*
++<<<<<<< HEAD
 + * This is called when the inode's link count goes to 0 or we are creating a
 + * tmpfile via O_TMPFILE. In the case of a tmpfile, @ignore_linkcount will be
 + * set to true as the link count is dropped to zero by the VFS after we've
 + * created the file successfully, so we have to add it to the unlinked list
 + * while the link count is non-zero.
++=======
+  * In-Core Unlinked List Lookups
+  * =============================
+  *
+  * Every inode is supposed to be reachable from some other piece of metadata
+  * with the exception of the root directory.  Inodes with a connection to a
+  * file descriptor but not linked from anywhere in the on-disk directory tree
+  * are collectively known as unlinked inodes, though the filesystem itself
+  * maintains links to these inodes so that on-disk metadata are consistent.
+  *
+  * XFS implements a per-AG on-disk hash table of unlinked inodes.  The AGI
+  * header contains a number of buckets that point to an inode, and each inode
+  * record has a pointer to the next inode in the hash chain.  This
+  * singly-linked list causes scaling problems in the iunlink remove function
+  * because we must walk that list to find the inode that points to the inode
+  * being removed from the unlinked hash bucket list.
+  *
+  * What if we modelled the unlinked list as a collection of records capturing
+  * "X.next_unlinked = Y" relations?  If we indexed those records on Y, we'd
+  * have a fast way to look up unlinked list predecessors, which avoids the
+  * slow list walk.  That's exactly what we do here (in-core) with a per-AG
+  * rhashtable.
+  *
+  * Because this is a backref cache, we ignore operational failures since the
+  * iunlink code can fall back to the slow bucket walk.  The only errors that
+  * should bubble out are for obviously incorrect situations.
+  *
+  * All users of the backref cache MUST hold the AGI buffer lock to serialize
+  * access or have otherwise provided for concurrency control.
+  */
+ 
+ /* Capture a "X.next_unlinked = Y" relationship. */
+ struct xfs_iunlink {
+ 	struct rhash_head	iu_rhash_head;
+ 	xfs_agino_t		iu_agino;		/* X */
+ 	xfs_agino_t		iu_next_unlinked;	/* Y */
+ };
+ 
+ /* Unlinked list predecessor lookup hashtable construction */
+ static int
+ xfs_iunlink_obj_cmpfn(
+ 	struct rhashtable_compare_arg	*arg,
+ 	const void			*obj)
+ {
+ 	const xfs_agino_t		*key = arg->key;
+ 	const struct xfs_iunlink	*iu = obj;
+ 
+ 	if (iu->iu_next_unlinked != *key)
+ 		return 1;
+ 	return 0;
+ }
+ 
+ static const struct rhashtable_params xfs_iunlink_hash_params = {
+ 	.min_size		= XFS_AGI_UNLINKED_BUCKETS,
+ 	.key_len		= sizeof(xfs_agino_t),
+ 	.key_offset		= offsetof(struct xfs_iunlink,
+ 					   iu_next_unlinked),
+ 	.head_offset		= offsetof(struct xfs_iunlink, iu_rhash_head),
+ 	.automatic_shrinking	= true,
+ 	.obj_cmpfn		= xfs_iunlink_obj_cmpfn,
+ };
+ 
+ /*
+  * Return X, where X.next_unlinked == @agino.  Returns NULLAGINO if no such
+  * relation is found.
+  */
+ static xfs_agino_t
+ xfs_iunlink_lookup_backref(
+ 	struct xfs_perag	*pag,
+ 	xfs_agino_t		agino)
+ {
+ 	struct xfs_iunlink	*iu;
+ 
+ 	iu = rhashtable_lookup_fast(&pag->pagi_unlinked_hash, &agino,
+ 			xfs_iunlink_hash_params);
+ 	return iu ? iu->iu_agino : NULLAGINO;
+ }
+ 
+ /*
+  * Take ownership of an iunlink cache entry and insert it into the hash table.
+  * If successful, the entry will be owned by the cache; if not, it is freed.
+  * Either way, the caller does not own @iu after this call.
+  */
+ static int
+ xfs_iunlink_insert_backref(
+ 	struct xfs_perag	*pag,
+ 	struct xfs_iunlink	*iu)
+ {
+ 	int			error;
+ 
+ 	error = rhashtable_insert_fast(&pag->pagi_unlinked_hash,
+ 			&iu->iu_rhash_head, xfs_iunlink_hash_params);
+ 	/*
+ 	 * Fail loudly if there already was an entry because that's a sign of
+ 	 * corruption of in-memory data.  Also fail loudly if we see an error
+ 	 * code we didn't anticipate from the rhashtable code.  Currently we
+ 	 * only anticipate ENOMEM.
+ 	 */
+ 	if (error) {
+ 		WARN(error != -ENOMEM, "iunlink cache insert error %d", error);
+ 		kmem_free(iu);
+ 	}
+ 	/*
+ 	 * Absorb any runtime errors that aren't a result of corruption because
+ 	 * this is a cache and we can always fall back to bucket list scanning.
+ 	 */
+ 	if (error != 0 && error != -EEXIST)
+ 		error = 0;
+ 	return error;
+ }
+ 
+ /* Remember that @prev_agino.next_unlinked = @this_agino. */
+ static int
+ xfs_iunlink_add_backref(
+ 	struct xfs_perag	*pag,
+ 	xfs_agino_t		prev_agino,
+ 	xfs_agino_t		this_agino)
+ {
+ 	struct xfs_iunlink	*iu;
+ 
+ 	if (XFS_TEST_ERROR(false, pag->pag_mount, XFS_ERRTAG_IUNLINK_FALLBACK))
+ 		return 0;
+ 
+ 	iu = kmem_zalloc(sizeof(*iu), KM_SLEEP | KM_NOFS);
+ 	iu->iu_agino = prev_agino;
+ 	iu->iu_next_unlinked = this_agino;
+ 
+ 	return xfs_iunlink_insert_backref(pag, iu);
+ }
+ 
+ /*
+  * Replace X.next_unlinked = @agino with X.next_unlinked = @next_unlinked.
+  * If @next_unlinked is NULLAGINO, we drop the backref and exit.  If there
+  * wasn't any such entry then we don't bother.
+  */
+ static int
+ xfs_iunlink_change_backref(
+ 	struct xfs_perag	*pag,
+ 	xfs_agino_t		agino,
+ 	xfs_agino_t		next_unlinked)
+ {
+ 	struct xfs_iunlink	*iu;
+ 	int			error;
+ 
+ 	/* Look up the old entry; if there wasn't one then exit. */
+ 	iu = rhashtable_lookup_fast(&pag->pagi_unlinked_hash, &agino,
+ 			xfs_iunlink_hash_params);
+ 	if (!iu)
+ 		return 0;
+ 
+ 	/*
+ 	 * Remove the entry.  This shouldn't ever return an error, but if we
+ 	 * couldn't remove the old entry we don't want to add it again to the
+ 	 * hash table, and if the entry disappeared on us then someone's
+ 	 * violated the locking rules and we need to fail loudly.  Either way
+ 	 * we cannot remove the inode because internal state is or would have
+ 	 * been corrupt.
+ 	 */
+ 	error = rhashtable_remove_fast(&pag->pagi_unlinked_hash,
+ 			&iu->iu_rhash_head, xfs_iunlink_hash_params);
+ 	if (error)
+ 		return error;
+ 
+ 	/* If there is no new next entry just free our item and return. */
+ 	if (next_unlinked == NULLAGINO) {
+ 		kmem_free(iu);
+ 		return 0;
+ 	}
+ 
+ 	/* Update the entry and re-add it to the hash table. */
+ 	iu->iu_next_unlinked = next_unlinked;
+ 	return xfs_iunlink_insert_backref(pag, iu);
+ }
+ 
+ /* Set up the in-core predecessor structures. */
+ int
+ xfs_iunlink_init(
+ 	struct xfs_perag	*pag)
+ {
+ 	return rhashtable_init(&pag->pagi_unlinked_hash,
+ 			&xfs_iunlink_hash_params);
+ }
+ 
+ /* Free the in-core predecessor structures. */
+ static void
+ xfs_iunlink_free_item(
+ 	void			*ptr,
+ 	void			*arg)
+ {
+ 	struct xfs_iunlink	*iu = ptr;
+ 	bool			*freed_anything = arg;
+ 
+ 	*freed_anything = true;
+ 	kmem_free(iu);
+ }
+ 
+ void
+ xfs_iunlink_destroy(
+ 	struct xfs_perag	*pag)
+ {
+ 	bool			freed_anything = false;
+ 
+ 	rhashtable_free_and_destroy(&pag->pagi_unlinked_hash,
+ 			xfs_iunlink_free_item, &freed_anything);
+ 
+ 	ASSERT(freed_anything == false || XFS_FORCED_SHUTDOWN(pag->pag_mount));
+ }
+ 
+ /*
+  * Point the AGI unlinked bucket at an inode and log the results.  The caller
+  * is responsible for validating the old value.
+  */
+ STATIC int
+ xfs_iunlink_update_bucket(
+ 	struct xfs_trans	*tp,
+ 	xfs_agnumber_t		agno,
+ 	struct xfs_buf		*agibp,
+ 	unsigned int		bucket_index,
+ 	xfs_agino_t		new_agino)
+ {
+ 	struct xfs_agi		*agi = XFS_BUF_TO_AGI(agibp);
+ 	xfs_agino_t		old_value;
+ 	int			offset;
+ 
+ 	ASSERT(xfs_verify_agino_or_null(tp->t_mountp, agno, new_agino));
+ 
+ 	old_value = be32_to_cpu(agi->agi_unlinked[bucket_index]);
+ 	trace_xfs_iunlink_update_bucket(tp->t_mountp, agno, bucket_index,
+ 			old_value, new_agino);
+ 
+ 	/*
+ 	 * We should never find the head of the list already set to the value
+ 	 * passed in because either we're adding or removing ourselves from the
+ 	 * head of the list.
+ 	 */
+ 	if (old_value == new_agino)
+ 		return -EFSCORRUPTED;
+ 
+ 	agi->agi_unlinked[bucket_index] = cpu_to_be32(new_agino);
+ 	offset = offsetof(struct xfs_agi, agi_unlinked) +
+ 			(sizeof(xfs_agino_t) * bucket_index);
+ 	xfs_trans_log_buf(tp, agibp, offset, offset + sizeof(xfs_agino_t) - 1);
+ 	return 0;
+ }
+ 
+ /* Set an on-disk inode's next_unlinked pointer. */
+ STATIC void
+ xfs_iunlink_update_dinode(
+ 	struct xfs_trans	*tp,
+ 	xfs_agnumber_t		agno,
+ 	xfs_agino_t		agino,
+ 	struct xfs_buf		*ibp,
+ 	struct xfs_dinode	*dip,
+ 	struct xfs_imap		*imap,
+ 	xfs_agino_t		next_agino)
+ {
+ 	struct xfs_mount	*mp = tp->t_mountp;
+ 	int			offset;
+ 
+ 	ASSERT(xfs_verify_agino_or_null(mp, agno, next_agino));
+ 
+ 	trace_xfs_iunlink_update_dinode(mp, agno, agino,
+ 			be32_to_cpu(dip->di_next_unlinked), next_agino);
+ 
+ 	dip->di_next_unlinked = cpu_to_be32(next_agino);
+ 	offset = imap->im_boffset +
+ 			offsetof(struct xfs_dinode, di_next_unlinked);
+ 
+ 	/* need to recalc the inode CRC if appropriate */
+ 	xfs_dinode_calc_crc(mp, dip);
+ 	xfs_trans_inode_buf(tp, ibp);
+ 	xfs_trans_log_buf(tp, ibp, offset, offset + sizeof(xfs_agino_t) - 1);
+ 	xfs_inobp_check(mp, ibp);
+ }
+ 
+ /* Set an in-core inode's unlinked pointer and return the old value. */
+ STATIC int
+ xfs_iunlink_update_inode(
+ 	struct xfs_trans	*tp,
+ 	struct xfs_inode	*ip,
+ 	xfs_agnumber_t		agno,
+ 	xfs_agino_t		next_agino,
+ 	xfs_agino_t		*old_next_agino)
+ {
+ 	struct xfs_mount	*mp = tp->t_mountp;
+ 	struct xfs_dinode	*dip;
+ 	struct xfs_buf		*ibp;
+ 	xfs_agino_t		old_value;
+ 	int			error;
+ 
+ 	ASSERT(xfs_verify_agino_or_null(mp, agno, next_agino));
+ 
+ 	error = xfs_imap_to_bp(mp, tp, &ip->i_imap, &dip, &ibp, 0, 0);
+ 	if (error)
+ 		return error;
+ 
+ 	/* Make sure the old pointer isn't garbage. */
+ 	old_value = be32_to_cpu(dip->di_next_unlinked);
+ 	if (!xfs_verify_agino_or_null(mp, agno, old_value)) {
+ 		error = -EFSCORRUPTED;
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * Since we're updating a linked list, we should never find that the
+ 	 * current pointer is the same as the new value, unless we're
+ 	 * terminating the list.
+ 	 */
+ 	*old_next_agino = old_value;
+ 	if (old_value == next_agino) {
+ 		if (next_agino != NULLAGINO)
+ 			error = -EFSCORRUPTED;
+ 		goto out;
+ 	}
+ 
+ 	/* Ok, update the new pointer. */
+ 	xfs_iunlink_update_dinode(tp, agno, XFS_INO_TO_AGINO(mp, ip->i_ino),
+ 			ibp, dip, &ip->i_imap, next_agino);
+ 	return 0;
+ out:
+ 	xfs_trans_brelse(tp, ibp);
+ 	return error;
+ }
+ 
+ /*
+  * This is called when the inode's link count has gone to 0 or we are creating
+  * a tmpfile via O_TMPFILE.  The inode @ip must have nlink == 0.
++>>>>>>> c4a6bf7f6cc7 (xfs: don't ever put nlink > 0 inodes on the unlinked list)
   *
   * We place the on-disk inode on a list in the AGI.  It will be pulled from this
   * list when the inode is freed.
   */
  STATIC int
  xfs_iunlink(
 -	struct xfs_trans	*tp,
 -	struct xfs_inode	*ip)
 +	struct xfs_trans *tp,
 +	struct xfs_inode *ip)
  {
 -	struct xfs_mount	*mp = tp->t_mountp;
 -	struct xfs_agi		*agi;
 -	struct xfs_buf		*agibp;
 -	xfs_agino_t		next_agino;
 -	xfs_agnumber_t		agno = XFS_INO_TO_AGNO(mp, ip->i_ino);
 -	xfs_agino_t		agino = XFS_INO_TO_AGINO(mp, ip->i_ino);
 -	short			bucket_index = agino % XFS_AGI_UNLINKED_BUCKETS;
 -	int			error;
 +	xfs_mount_t	*mp = tp->t_mountp;
 +	xfs_agi_t	*agi;
 +	xfs_dinode_t	*dip;
 +	xfs_buf_t	*agibp;
 +	xfs_buf_t	*ibp;
 +	xfs_agino_t	agino;
 +	short		bucket_index;
 +	int		offset;
 +	int		error;
  
+ 	ASSERT(VFS_I(ip)->i_nlink == 0);
  	ASSERT(VFS_I(ip)->i_mode != 0);
 -	trace_xfs_iunlink(ip);
  
 -	/* Get the agi buffer first.  It ensures lock ordering on the list. */
 -	error = xfs_read_agi(mp, tp, agno, &agibp);
 +	/*
 +	 * Get the agi buffer first.  It ensures lock ordering
 +	 * on the list.
 +	 */
 +	error = xfs_read_agi(mp, tp, XFS_INO_TO_AGNO(mp, ip->i_ino), &agibp);
  	if (error)
  		return error;
  	agi = XFS_BUF_TO_AGI(agibp);
* Unmerged path fs/xfs/xfs_inode.c
diff --git a/fs/xfs/xfs_iops.c b/fs/xfs/xfs_iops.c
index 505337c99fc2..b2d9523f7398 100644
--- a/fs/xfs/xfs_iops.c
+++ b/fs/xfs/xfs_iops.c
@@ -191,9 +191,18 @@ xfs_generic_create(
 
 	xfs_setup_iops(ip);
 
-	if (tmpfile)
+	if (tmpfile) {
+		/*
+		 * The VFS requires that any inode fed to d_tmpfile must have
+		 * nlink == 1 so that it can decrement the nlink in d_tmpfile.
+		 * However, we created the temp file with nlink == 0 because
+		 * we're not allowed to put an inode with nlink > 0 on the
+		 * unlinked list.  Therefore we have to set nlink to 1 so that
+		 * d_tmpfile can immediately set it back to zero.
+		 */
+		set_nlink(inode, 1);
 		d_tmpfile(dentry, inode);
-	else
+	} else
 		d_instantiate(dentry, inode);
 
 	xfs_finish_inode_setup(ip);
