net/mlx5: EQ, Generic EQ

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-147.el8
commit-author Saeed Mahameed <saeedm@mellanox.com>
commit 7701707cb94ed4d1e63ae4fa5ef62a2345ef9db7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-147.el8/7701707c.failed

Add mlx5_eq_{create/destroy}_generic APIs and EQE access methods, for
mlx5 core consumers generic EQs.

This API will be used in downstream patch to move page fault (RDMA ODP)
EQ logic into mlx5_ib rdma driver, hence it will use a generic EQ.

Current mlx5 EQ allocation scheme:
On load mlx5 allocates 4 (for async) + #cores (for data completions)
MSIX vectors, mlx5 core will assign 3 MSIX vectors for internal async
EQs and will use all of the #cores MSIX vectors for completion EQs,
(One vector is going to be reserved for a generic EQ).

After this patch an external user (e.g mlx5_ib) of mlx5_core
can use this new API to create new generic EQs with the reserved msix
vector index for that eq.

	Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
	Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
	Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
	Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
(cherry picked from commit 7701707cb94ed4d1e63ae4fa5ef62a2345ef9db7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index bb2de668b530,ec1f5018546e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -64,6 -67,29 +65,32 @@@ enum 
  	MLX5_EQ_DOORBEL_OFFSET	= 0x40,
  };
  
++<<<<<<< HEAD
++=======
+ struct mlx5_irq_info {
+ 	cpumask_var_t mask;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	void *context; /* dev_id provided to request_irq */
+ };
+ 
+ struct mlx5_eq_table {
+ 	struct list_head        comp_eqs_list;
+ 	struct mlx5_eq          pages_eq;
+ 	struct mlx5_eq          async_eq;
+ 	struct mlx5_eq	        cmd_eq;
+ 
+ #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+ 	struct mlx5_eq_pagefault pfault_eq;
+ #endif
+ 	struct mutex            lock; /* sync async eqs creations */
+ 	int			num_comp_vectors;
+ 	struct mlx5_irq_info	*irq_info;
+ #ifdef CONFIG_RFS_ACCEL
+ 	struct cpu_rmap         *rmap;
+ #endif
+ };
+ 
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  #define MLX5_ASYNC_EVENT_MASK ((1ull << MLX5_EVENT_TYPE_PATH_MIG)	    | \
  			       (1ull << MLX5_EVENT_TYPE_COMM_EST)	    | \
  			       (1ull << MLX5_EVENT_TYPE_SQ_DRAINED)	    | \
@@@ -200,24 -228,24 +227,38 @@@ static void eqe_pf_action(struct work_s
  	struct mlx5_pagefault *pfault = container_of(work,
  						     struct mlx5_pagefault,
  						     work);
 -	struct mlx5_eq_pagefault *eq = pfault->eq;
 +	struct mlx5_eq *eq = pfault->eq;
  
++<<<<<<< HEAD
 +	mlx5_core_page_fault(eq->dev, pfault);
 +	mempool_free(pfault, eq->pf_ctx.pool);
++=======
+ 	mlx5_core_page_fault(eq->core->dev, pfault);
+ 	mempool_free(pfault, eq->pool);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  }
  
 -static void eq_pf_process(struct mlx5_eq_pagefault *eq)
 +static void eq_pf_process(struct mlx5_eq *eq)
  {
++<<<<<<< HEAD
 +	struct mlx5_core_dev *dev = eq->dev;
++=======
+ 	struct mlx5_core_dev *dev = eq->core->dev;
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	struct mlx5_eqe_page_fault *pf_eqe;
  	struct mlx5_pagefault *pfault;
  	struct mlx5_eqe *eqe;
  	int set_ci = 0;
  
++<<<<<<< HEAD
 +	while ((eqe = next_eqe_sw(eq))) {
 +		pfault = mempool_alloc(eq->pf_ctx.pool, GFP_ATOMIC);
++=======
+ 	while ((eqe = next_eqe_sw(eq->core))) {
+ 		pfault = mempool_alloc(eq->pool, GFP_ATOMIC);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  		if (!pfault) {
 -			schedule_work(&eq->work);
 +			schedule_work(&eq->pf_ctx.work);
  			break;
  		}
  
@@@ -287,18 -315,18 +328,30 @@@
  
  		pfault->eq = eq;
  		INIT_WORK(&pfault->work, eqe_pf_action);
 -		queue_work(eq->wq, &pfault->work);
 +		queue_work(eq->pf_ctx.wq, &pfault->work);
 +
++<<<<<<< HEAD
 +		++eq->cons_index;
 +		++set_ci;
  
 +		if (unlikely(set_ci >= MLX5_NUM_SPARE_EQE)) {
 +			eq_update_ci(eq, 0);
++=======
+ 		++eq->core->cons_index;
+ 		++set_ci;
+ 
+ 		if (unlikely(set_ci >= MLX5_NUM_SPARE_EQE)) {
+ 			eq_update_ci(eq->core, 0);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  			set_ci = 0;
  		}
  	}
  
++<<<<<<< HEAD
 +	eq_update_ci(eq, 1);
++=======
+ 	eq_update_ci(eq->core, 1);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  }
  
  static irqreturn_t mlx5_eq_pf_int(int irq, void *eq_ptr)
@@@ -328,35 -356,70 +381,82 @@@ static void mempool_refill(mempool_t *p
  
  static void eq_pf_action(struct work_struct *work)
  {
 -	struct mlx5_eq_pagefault *eq =
 -		container_of(work, struct mlx5_eq_pagefault, work);
 +	struct mlx5_eq *eq = container_of(work, struct mlx5_eq, pf_ctx.work);
  
 -	mempool_refill(eq->pool);
 +	mempool_refill(eq->pf_ctx.pool);
  
 -	spin_lock_irq(&eq->lock);
 +	spin_lock_irq(&eq->pf_ctx.lock);
  	eq_pf_process(eq);
 -	spin_unlock_irq(&eq->lock);
 +	spin_unlock_irq(&eq->pf_ctx.lock);
  }
  
 -static int
 -create_pf_eq(struct mlx5_core_dev *dev, struct mlx5_eq_pagefault *eq)
 +static int init_pf_ctx(struct mlx5_eq_pagefault *pf_ctx, const char *name)
  {
++<<<<<<< HEAD
 +	spin_lock_init(&pf_ctx->lock);
 +	INIT_WORK(&pf_ctx->work, eq_pf_action);
++=======
+ 	struct mlx5_eq_param param = {};
+ 	int err;
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  
 -	spin_lock_init(&eq->lock);
 -	INIT_WORK(&eq->work, eq_pf_action);
 -
 -	eq->pool = mempool_create_kmalloc_pool(MLX5_NUM_PF_DRAIN,
 -					       sizeof(struct mlx5_pagefault));
 -	if (!eq->pool)
 +	pf_ctx->wq = alloc_workqueue(name,
 +				     WQ_HIGHPRI | WQ_UNBOUND | WQ_MEM_RECLAIM,
 +				     MLX5_NUM_CMD_EQE);
 +	if (!pf_ctx->wq)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	pf_ctx->pool = mempool_create_kmalloc_pool
 +		(MLX5_NUM_PF_DRAIN, sizeof(struct mlx5_pagefault));
 +	if (!pf_ctx->pool)
++=======
+ 	eq->wq = alloc_workqueue("mlx5_page_fault",
+ 				 WQ_HIGHPRI | WQ_UNBOUND | WQ_MEM_RECLAIM,
+ 				 MLX5_NUM_CMD_EQE);
+ 	if (!eq->wq) {
+ 		err = -ENOMEM;
+ 		goto err_mempool;
+ 	}
+ 
+ 	param = (struct mlx5_eq_param) {
+ 		.index = MLX5_EQ_PFAULT_IDX,
+ 		.mask = 1 << MLX5_EVENT_TYPE_PAGE_FAULT,
+ 		.nent = MLX5_NUM_ASYNC_EQE,
+ 		.context = eq,
+ 		.handler = mlx5_eq_pf_int
+ 	};
+ 
+ 	eq->core = mlx5_eq_create_generic(dev, "mlx5_page_fault_eq", &param);
+ 	if (IS_ERR(eq->core)) {
+ 		err = PTR_ERR(eq->core);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  		goto err_wq;
+ 	}
  
  	return 0;
  err_wq:
++<<<<<<< HEAD
 +	destroy_workqueue(pf_ctx->wq);
 +	return -ENOMEM;
++=======
+ 	destroy_workqueue(eq->wq);
+ err_mempool:
+ 	mempool_destroy(eq->pool);
+ 	return err;
+ }
+ 
+ static int destroy_pf_eq(struct mlx5_core_dev *dev, struct mlx5_eq_pagefault *eq)
+ {
+ 	int err;
+ 
+ 	err = mlx5_eq_destroy_generic(dev, eq->core);
+ 	cancel_work_sync(&eq->work);
+ 	destroy_workqueue(eq->wq);
+ 	mempool_destroy(eq->pool);
+ 
+ 	return err;
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  }
  
  int mlx5_core_page_fault_resume(struct mlx5_core_dev *dev, u32 token,
@@@ -625,14 -719,15 +725,24 @@@ static void init_eq_buf(struct mlx5_eq 
  	}
  }
  
++<<<<<<< HEAD
 +int mlx5_create_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq, u8 vecidx,
 +		       int nent, u64 mask, const char *name,
 +		       enum mlx5_eq_type type)
++=======
+ static int
+ create_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq, const char *name,
+ 	      struct mlx5_eq_param *param)
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  {
 -	struct mlx5_eq_table *eq_table = dev->priv.eq_table;
  	struct mlx5_cq_table *cq_table = &eq->cq_table;
  	u32 out[MLX5_ST_SZ_DW(create_eq_out)] = {0};
  	struct mlx5_priv *priv = &dev->priv;
++<<<<<<< HEAD
 +	irq_handler_t handler;
++=======
+ 	u8 vecidx = param->index;
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	__be64 *pas;
  	void *eqc;
  	int inlen;
@@@ -644,8 -742,7 +757,12 @@@
  	spin_lock_init(&cq_table->lock);
  	INIT_RADIX_TREE(&cq_table->tree, GFP_ATOMIC);
  
++<<<<<<< HEAD
 +	eq->type = type;
 +	eq->nent = roundup_pow_of_two(nent + MLX5_NUM_SPARE_EQE);
++=======
+ 	eq->nent = roundup_pow_of_two(param->nent + MLX5_NUM_SPARE_EQE);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	eq->cons_index = 0;
  	err = mlx5_buf_alloc(dev, eq->nent * MLX5_EQE_SIZE, &eq->buf);
  	if (err)
@@@ -686,15 -776,17 +803,22 @@@
  	if (err)
  		goto err_in;
  
 -	snprintf(eq_table->irq_info[vecidx].name, MLX5_MAX_IRQ_NAME, "%s@pci:%s",
 +	snprintf(priv->irq_info[vecidx].name, MLX5_MAX_IRQ_NAME, "%s@pci:%s",
  		 name, pci_name(dev->pdev));
+ 	eq_table->irq_info[vecidx].context = param->context;
  
+ 	eq->vecidx = vecidx;
  	eq->eqn = MLX5_GET(create_eq_out, out, eq_number);
  	eq->irqn = pci_irq_vector(dev->pdev, vecidx);
  	eq->dev = dev;
  	eq->doorbell = priv->uar->map + MLX5_EQ_DOORBEL_OFFSET;
++<<<<<<< HEAD
 +	err = request_irq(eq->irqn, handler, 0,
 +			  priv->irq_info[vecidx].name, eq);
++=======
+ 	err = request_irq(eq->irqn, param->handler, 0,
+ 			  eq_table->irq_info[vecidx].name, param->context);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	if (err)
  		goto err_eq;
  
@@@ -738,12 -815,19 +862,23 @@@ err_buf
  	return err;
  }
  
++<<<<<<< HEAD
 +int mlx5_destroy_unmap_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq)
++=======
+ static int destroy_unmap_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq)
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  {
+ 	struct mlx5_eq_table *eq_table = dev->priv.eq_table;
+ 	struct mlx5_irq_info *irq_info;
  	int err;
  
+ 	irq_info = &eq_table->irq_info[eq->vecidx];
+ 
  	mlx5_debug_eq_remove(dev, eq);
- 	free_irq(eq->irqn, eq);
+ 
+ 	free_irq(eq->irqn, irq_info->context);
+ 	irq_info->context = NULL;
+ 
  	err = mlx5_cmd_destroy_eq(dev, eq->eqn);
  	if (err)
  		mlx5_core_warn(dev, "failed to destroy a previously created eq: eqn %d\n",
@@@ -807,17 -895,50 +942,52 @@@ int mlx5_eq_init(struct mlx5_core_dev *
  	return err;
  }
  
 -void mlx5_eq_table_cleanup(struct mlx5_core_dev *dev)
 +void mlx5_eq_cleanup(struct mlx5_core_dev *dev)
  {
  	mlx5_eq_debugfs_cleanup(dev);
 -	kvfree(dev->priv.eq_table);
  }
  
++<<<<<<< HEAD
 +int mlx5_start_eqs(struct mlx5_core_dev *dev)
 +{
 +	struct mlx5_eq_table *table = &dev->priv.eq_table;
- 	u64 async_event_mask = MLX5_ASYNC_EVENT_MASK;
++=======
+ /* Async EQs */
+ 
+ static int create_async_eq(struct mlx5_core_dev *dev, const char *name,
+ 			   struct mlx5_eq *eq, struct mlx5_eq_param *param)
+ {
+ 	struct mlx5_eq_table *eq_table = dev->priv.eq_table;
  	int err;
  
+ 	mutex_lock(&eq_table->lock);
+ 	if (param->index >= MLX5_EQ_MAX_ASYNC_EQS) {
+ 		err = -ENOSPC;
+ 		goto unlock;
+ 	}
+ 
+ 	err = create_map_eq(dev, eq, name, param);
+ unlock:
+ 	mutex_unlock(&eq_table->lock);
+ 	return err;
+ }
+ 
+ static int destroy_async_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq)
+ {
+ 	struct mlx5_eq_table *eq_table = dev->priv.eq_table;
+ 	int err;
+ 
+ 	mutex_lock(&eq_table->lock);
+ 	err = destroy_unmap_eq(dev, eq);
+ 	mutex_unlock(&eq_table->lock);
+ 	return err;
+ }
+ 
+ static u64 gather_async_events_mask(struct mlx5_core_dev *dev)
+ {
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
+ 	u64 async_event_mask = MLX5_ASYNC_EVENT_MASK;
+ 
  	if (MLX5_VPORT_MANAGER(dev))
  		async_event_mask |= (1ull << MLX5_EVENT_TYPE_NIC_VPORT_CHANGE);
  
@@@ -842,9 -963,26 +1012,32 @@@
  	if (MLX5_CAP_GEN(dev, temp_warn_event))
  		async_event_mask |= (1ull << MLX5_EVENT_TYPE_TEMP_WARN_EVENT);
  
++<<<<<<< HEAD
 +	err = mlx5_create_map_eq(dev, &table->cmd_eq, MLX5_EQ_VEC_CMD,
 +				 MLX5_NUM_CMD_EQE, 1ull << MLX5_EVENT_TYPE_CMD,
 +				 "mlx5_cmd_eq", MLX5_EQ_TYPE_ASYNC);
++=======
+ 	if (MLX5_CAP_MCAM_REG(dev, tracer_registers))
+ 		async_event_mask |= (1ull << MLX5_EVENT_TYPE_DEVICE_TRACER);
+ 
+ 	return async_event_mask;
+ }
+ 
+ static int create_async_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_param param = {};
+ 	int err;
+ 
+ 	param = (struct mlx5_eq_param) {
+ 		.index = MLX5_EQ_CMD_IDX,
+ 		.mask = 1ull << MLX5_EVENT_TYPE_CMD,
+ 		.nent = MLX5_NUM_CMD_EQE,
+ 		.context = &table->cmd_eq,
+ 		.handler = mlx5_eq_async_int,
+ 	};
+ 	err = create_async_eq(dev, "mlx5_cmd_eq", &table->cmd_eq, &param);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	if (err) {
  		mlx5_core_warn(dev, "failed to create cmd EQ %d\n", err);
  		return err;
@@@ -852,19 -990,27 +1045,41 @@@
  
  	mlx5_cmd_use_events(dev);
  
++<<<<<<< HEAD
 +	err = mlx5_create_map_eq(dev, &table->async_eq, MLX5_EQ_VEC_ASYNC,
 +				 MLX5_NUM_ASYNC_EQE, async_event_mask,
 +				 "mlx5_async_eq", MLX5_EQ_TYPE_ASYNC);
++=======
+ 	param = (struct mlx5_eq_param) {
+ 		.index = MLX5_EQ_ASYNC_IDX,
+ 		.mask = gather_async_events_mask(dev),
+ 		.nent = MLX5_NUM_ASYNC_EQE,
+ 		.context = &table->async_eq,
+ 		.handler = mlx5_eq_async_int,
+ 	};
+ 	err = create_async_eq(dev, "mlx5_async_eq", &table->async_eq, &param);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	if (err) {
  		mlx5_core_warn(dev, "failed to create async EQ %d\n", err);
  		goto err1;
  	}
  
++<<<<<<< HEAD
 +	err = mlx5_create_map_eq(dev, &table->pages_eq,
 +				 MLX5_EQ_VEC_PAGES,
 +				 /* TODO: sriov max_vf + */ 1,
 +				 1 << MLX5_EVENT_TYPE_PAGE_REQUEST, "mlx5_pages_eq",
 +				 MLX5_EQ_TYPE_ASYNC);
++=======
+ 	param = (struct mlx5_eq_param) {
+ 		.index = MLX5_EQ_PAGEREQ_IDX,
+ 		.mask =  1 << MLX5_EVENT_TYPE_PAGE_REQUEST,
+ 		.nent = /* TODO: sriov max_vf + */ 1,
+ 		.context = &table->pages_eq,
+ 		.handler = mlx5_eq_async_int,
+ 	};
+ 	err = create_async_eq(dev, "mlx5_pages_eq", &table->pages_eq, &param);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	if (err) {
  		mlx5_core_warn(dev, "failed to create pages EQ %d\n", err);
  		goto err2;
@@@ -887,17 -1028,17 +1102,29 @@@
  
  	return err;
  err3:
++<<<<<<< HEAD
 +	mlx5_destroy_unmap_eq(dev, &table->pages_eq);
++=======
+ 	destroy_async_eq(dev, &table->pages_eq);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  #else
  	return err;
  #endif
  
  err2:
++<<<<<<< HEAD
 +	mlx5_destroy_unmap_eq(dev, &table->async_eq);
 +
 +err1:
 +	mlx5_cmd_use_polling(dev);
 +	mlx5_destroy_unmap_eq(dev, &table->cmd_eq);
++=======
+ 	destroy_async_eq(dev, &table->async_eq);
+ 
+ err1:
+ 	mlx5_cmd_use_polling(dev);
+ 	destroy_async_eq(dev, &table->cmd_eq);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	return err;
  }
  
@@@ -915,54 -1056,424 +1142,379 @@@ void mlx5_stop_eqs(struct mlx5_core_de
  	}
  #endif
  
++<<<<<<< HEAD
 +	err = mlx5_destroy_unmap_eq(dev, &table->pages_eq);
++=======
+ 	err = destroy_async_eq(dev, &table->pages_eq);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	if (err)
  		mlx5_core_err(dev, "failed to destroy pages eq, err(%d)\n",
  			      err);
  
++<<<<<<< HEAD
 +	err = mlx5_destroy_unmap_eq(dev, &table->async_eq);
++=======
+ 	err = destroy_async_eq(dev, &table->async_eq);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	if (err)
  		mlx5_core_err(dev, "failed to destroy async eq, err(%d)\n",
  			      err);
  	mlx5_cmd_use_polling(dev);
  
++<<<<<<< HEAD
 +	err = mlx5_destroy_unmap_eq(dev, &table->cmd_eq);
++=======
+ 	err = destroy_async_eq(dev, &table->cmd_eq);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	if (err)
  		mlx5_core_err(dev, "failed to destroy command eq, err(%d)\n",
  			      err);
  }
  
 -struct mlx5_eq *mlx5_get_async_eq(struct mlx5_core_dev *dev)
 +int mlx5_core_eq_query(struct mlx5_core_dev *dev, struct mlx5_eq *eq,
 +		       u32 *out, int outlen)
  {
 -	return &dev->priv.eq_table->async_eq;
 -}
 +	u32 in[MLX5_ST_SZ_DW(query_eq_in)] = {0};
  
++<<<<<<< HEAD
 +	MLX5_SET(query_eq_in, in, opcode, MLX5_CMD_OP_QUERY_EQ);
 +	MLX5_SET(query_eq_in, in, eq_number, eq->eqn);
 +	return mlx5_cmd_exec(dev, in, sizeof(in), out, outlen);
++=======
+ void mlx5_eq_synchronize_async_irq(struct mlx5_core_dev *dev)
+ {
+ 	synchronize_irq(dev->priv.eq_table->async_eq.irqn);
+ }
+ 
+ void mlx5_eq_synchronize_cmd_irq(struct mlx5_core_dev *dev)
+ {
+ 	synchronize_irq(dev->priv.eq_table->cmd_eq.irqn);
+ }
+ 
+ /* Generic EQ API for mlx5_core consumers
+  * Needed For RDMA ODP EQ for now
+  */
+ struct mlx5_eq *
+ mlx5_eq_create_generic(struct mlx5_core_dev *dev, const char *name,
+ 		       struct mlx5_eq_param *param)
+ {
+ 	struct mlx5_eq *eq = kvzalloc(sizeof(*eq), GFP_KERNEL);
+ 	int err;
+ 
+ 	if (!eq)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	err = create_async_eq(dev, name, eq, param);
+ 	if (err) {
+ 		kvfree(eq);
+ 		eq = ERR_PTR(err);
+ 	}
+ 
+ 	return eq;
+ }
+ EXPORT_SYMBOL(mlx5_eq_create_generic);
+ 
+ int mlx5_eq_destroy_generic(struct mlx5_core_dev *dev, struct mlx5_eq *eq)
+ {
+ 	int err;
+ 
+ 	if (IS_ERR(eq))
+ 		return -EINVAL;
+ 
+ 	err = destroy_async_eq(dev, eq);
+ 	if (err)
+ 		goto out;
+ 
+ 	kvfree(eq);
+ out:
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_eq_destroy_generic);
+ 
+ struct mlx5_eqe *mlx5_eq_get_eqe(struct mlx5_eq *eq, u32 cc)
+ {
+ 	u32 ci = eq->cons_index + cc;
+ 	struct mlx5_eqe *eqe;
+ 
+ 	eqe = get_eqe(eq, ci & (eq->nent - 1));
+ 	eqe = ((eqe->owner & 1) ^ !!(ci & eq->nent)) ? NULL : eqe;
+ 	/* Make sure we read EQ entry contents after we've
+ 	 * checked the ownership bit.
+ 	 */
+ 	if (eqe)
+ 		dma_rmb();
+ 
+ 	return eqe;
+ }
+ EXPORT_SYMBOL(mlx5_eq_get_eqe);
+ 
+ void mlx5_eq_update_ci(struct mlx5_eq *eq, u32 cc, bool arm)
+ {
+ 	__be32 __iomem *addr = eq->doorbell + (arm ? 0 : 2);
+ 	u32 val;
+ 
+ 	eq->cons_index += cc;
+ 	val = (eq->cons_index & 0xffffff) | (eq->eqn << 24);
+ 
+ 	__raw_writel((__force u32)cpu_to_be32(val), addr);
+ 	/* We still want ordering, just not swabbing, so add a barrier */
+ 	mb();
+ }
+ EXPORT_SYMBOL(mlx5_eq_update_ci);
+ 
+ /* Completion EQs */
+ 
+ static int set_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	if (!zalloc_cpumask_var(&irq_info->mask, GFP_KERNEL)) {
+ 		mlx5_core_warn(mdev, "zalloc_cpumask_var failed");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	cpumask_set_cpu(cpumask_local_spread(i, priv->numa_node),
+ 			irq_info->mask);
+ 
+ 	if (IS_ENABLED(CONFIG_SMP) &&
+ 	    irq_set_affinity_hint(irq, irq_info->mask))
+ 		mlx5_core_warn(mdev, "irq_set_affinity_hint failed, irq 0x%.4x", irq);
+ 
+ 	return 0;
+ }
+ 
+ static void clear_comp_irq_affinity_hint(struct mlx5_core_dev *mdev, int i)
+ {
+ 	int vecidx = MLX5_EQ_VEC_COMP_BASE + i;
+ 	struct mlx5_priv *priv  = &mdev->priv;
+ 	int irq = pci_irq_vector(mdev->pdev, vecidx);
+ 	struct mlx5_irq_info *irq_info = &priv->eq_table->irq_info[vecidx];
+ 
+ 	irq_set_affinity_hint(irq, NULL);
+ 	free_cpumask_var(irq_info->mask);
+ }
+ 
+ static int set_comp_irq_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int err;
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++) {
+ 		err = set_comp_irq_affinity_hint(mdev, i);
+ 		if (err)
+ 			goto err_out;
+ 	}
+ 
+ 	return 0;
+ 
+ err_out:
+ 	for (i--; i >= 0; i--)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ 
+ 	return err;
+ }
+ 
+ static void clear_comp_irqs_affinity_hints(struct mlx5_core_dev *mdev)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < mdev->priv.eq_table->num_comp_vectors; i++)
+ 		clear_comp_irq_affinity_hint(mdev, i);
+ }
+ 
+ static void destroy_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq, *n;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 	if (table->rmap) {
+ 		free_irq_cpu_rmap(table->rmap);
+ 		table->rmap = NULL;
+ 	}
+ #endif
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		list_del(&eq->list);
+ 		if (destroy_unmap_eq(dev, &eq->core))
+ 			mlx5_core_warn(dev, "failed to destroy comp EQ 0x%x\n",
+ 				       eq->core.eqn);
+ 		tasklet_disable(&eq->tasklet_ctx.task);
+ 		kfree(eq);
+ 	}
+ }
+ 
+ static int create_comp_eqs(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	char name[MLX5_MAX_IRQ_NAME];
+ 	struct mlx5_eq_comp *eq;
+ 	int ncomp_vec;
+ 	int nent;
+ 	int err;
+ 	int i;
+ 
+ 	INIT_LIST_HEAD(&table->comp_eqs_list);
+ 	ncomp_vec = table->num_comp_vectors;
+ 	nent = MLX5_COMP_EQ_SIZE;
+ #ifdef CONFIG_RFS_ACCEL
+ 	table->rmap = alloc_irq_cpu_rmap(ncomp_vec);
+ 	if (!table->rmap)
+ 		return -ENOMEM;
+ #endif
+ 	for (i = 0; i < ncomp_vec; i++) {
+ 		int vecidx = i + MLX5_EQ_VEC_COMP_BASE;
+ 		struct mlx5_eq_param param = {};
+ 
+ 		eq = kzalloc(sizeof(*eq), GFP_KERNEL);
+ 		if (!eq) {
+ 			err = -ENOMEM;
+ 			goto clean;
+ 		}
+ 
+ 		INIT_LIST_HEAD(&eq->tasklet_ctx.list);
+ 		INIT_LIST_HEAD(&eq->tasklet_ctx.process_list);
+ 		spin_lock_init(&eq->tasklet_ctx.lock);
+ 		tasklet_init(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb,
+ 			     (unsigned long)&eq->tasklet_ctx);
+ 
+ #ifdef CONFIG_RFS_ACCEL
+ 		irq_cpu_rmap_add(table->rmap, pci_irq_vector(dev->pdev, vecidx));
+ #endif
+ 		snprintf(name, MLX5_MAX_IRQ_NAME, "mlx5_comp%d", i);
+ 		param = (struct mlx5_eq_param) {
+ 			.index = vecidx,
+ 			.mask = 0,
+ 			.nent = nent,
+ 			.context = &eq->core,
+ 			.handler = mlx5_eq_comp_int
+ 		};
+ 		err = create_map_eq(dev, &eq->core, name, &param);
+ 		if (err) {
+ 			kfree(eq);
+ 			goto clean;
+ 		}
+ 		mlx5_core_dbg(dev, "allocated completion EQN %d\n", eq->core.eqn);
+ 		/* add tail, to keep the list ordered, for mlx5_vector2eqn to work */
+ 		list_add_tail(&eq->list, &table->comp_eqs_list);
+ 	}
+ 
+ 	err = set_comp_irq_affinity_hints(dev);
+ 	if (err) {
+ 		mlx5_core_err(dev, "Failed to alloc affinity hint cpumask\n");
+ 		goto clean;
+ 	}
+ 
+ 	return 0;
+ 
+ clean:
+ 	destroy_comp_eqs(dev);
+ 	return err;
+ }
+ 
+ int mlx5_vector2eqn(struct mlx5_core_dev *dev, int vector, int *eqn,
+ 		    unsigned int *irqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq, *n;
+ 	int err = -ENOENT;
+ 	int i = 0;
+ 
+ 	list_for_each_entry_safe(eq, n, &table->comp_eqs_list, list) {
+ 		if (i++ == vector) {
+ 			*eqn = eq->core.eqn;
+ 			*irqn = eq->core.irqn;
+ 			err = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(mlx5_vector2eqn);
+ 
+ unsigned int mlx5_comp_vectors_count(struct mlx5_core_dev *dev)
+ {
+ 	return dev->priv.eq_table->num_comp_vectors;
+ }
+ EXPORT_SYMBOL(mlx5_comp_vectors_count);
+ 
+ struct cpumask *
+ mlx5_comp_irq_get_affinity_mask(struct mlx5_core_dev *dev, int vector)
+ {
+ 	/* TODO: consider irq_get_affinity_mask(irq) */
+ 	return dev->priv.eq_table->irq_info[vector + MLX5_EQ_VEC_COMP_BASE].mask;
+ }
+ EXPORT_SYMBOL(mlx5_comp_irq_get_affinity_mask);
+ 
+ struct cpu_rmap *mlx5_eq_table_get_rmap(struct mlx5_core_dev *dev)
+ {
+ #ifdef CONFIG_RFS_ACCEL
+ 	return dev->priv.eq_table->rmap;
+ #else
+ 	return NULL;
+ #endif
+ }
+ 
+ struct mlx5_eq_comp *mlx5_eqn2comp_eq(struct mlx5_core_dev *dev, int eqn)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	struct mlx5_eq_comp *eq;
+ 
+ 	list_for_each_entry(eq, &table->comp_eqs_list, list) {
+ 		if (eq->core.eqn == eqn)
+ 			return eq;
+ 	}
+ 
+ 	return ERR_PTR(-ENOENT);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  }
  
  /* This function should only be called after mlx5_cmd_force_teardown_hca */
  void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev)
  {
++<<<<<<< HEAD
 +	struct mlx5_eq_table *table = &dev->priv.eq_table;
 +	struct mlx5_eq *eq;
++=======
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	int i, max_eqs;
+ 
+ 	clear_comp_irqs_affinity_hints(dev);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  
  #ifdef CONFIG_RFS_ACCEL
 -	if (table->rmap) {
 -		free_irq_cpu_rmap(table->rmap);
 -		table->rmap = NULL;
 +	if (dev->rmap) {
 +		free_irq_cpu_rmap(dev->rmap);
 +		dev->rmap = NULL;
  	}
  #endif
++<<<<<<< HEAD
 +	list_for_each_entry(eq, &table->comp_eqs_list, list)
 +		free_irq(eq->irqn, eq);
 +
 +	free_irq(table->pages_eq.irqn, &table->pages_eq);
 +	free_irq(table->async_eq.irqn, &table->async_eq);
 +	free_irq(table->cmd_eq.irqn, &table->cmd_eq);
 +#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 +	if (MLX5_CAP_GEN(dev, pg))
 +		free_irq(table->pfault_eq.irqn, &table->pfault_eq);
 +#endif
++=======
+ 
+ 	mutex_lock(&table->lock); /* sync with create/destroy_async_eq */
+ 	max_eqs = table->num_comp_vectors + MLX5_EQ_VEC_COMP_BASE;
+ 	for (i = max_eqs - 1; i >= 0; i--) {
+ 		if (!table->irq_info[i].context)
+ 			continue;
+ 		free_irq(pci_irq_vector(dev->pdev, i), table->irq_info[i].context);
+ 		table->irq_info[i].context = NULL;
+ 	}
+ 	mutex_unlock(&table->lock);
++>>>>>>> 7701707cb94e (net/mlx5: EQ, Generic EQ)
  	pci_free_irq_vectors(dev->pdev);
  }
 -
 -static int alloc_irq_vectors(struct mlx5_core_dev *dev)
 -{
 -	struct mlx5_priv *priv = &dev->priv;
 -	struct mlx5_eq_table *table = priv->eq_table;
 -	int num_eqs = MLX5_CAP_GEN(dev, max_num_eqs) ?
 -		      MLX5_CAP_GEN(dev, max_num_eqs) :
 -		      1 << MLX5_CAP_GEN(dev, log_max_eq);
 -	int nvec;
 -	int err;
 -
 -	nvec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() +
 -	       MLX5_EQ_VEC_COMP_BASE;
 -	nvec = min_t(int, nvec, num_eqs);
 -	if (nvec <= MLX5_EQ_VEC_COMP_BASE)
 -		return -ENOMEM;
 -
 -	table->irq_info = kcalloc(nvec, sizeof(*table->irq_info), GFP_KERNEL);
 -	if (!table->irq_info)
 -		return -ENOMEM;
 -
 -	nvec = pci_alloc_irq_vectors(dev->pdev, MLX5_EQ_VEC_COMP_BASE + 1,
 -				     nvec, PCI_IRQ_MSIX);
 -	if (nvec < 0) {
 -		err = nvec;
 -		goto err_free_irq_info;
 -	}
 -
 -	table->num_comp_vectors = nvec - MLX5_EQ_VEC_COMP_BASE;
 -
 -	return 0;
 -
 -err_free_irq_info:
 -	kfree(table->irq_info);
 -	return err;
 -}
 -
 -static void free_irq_vectors(struct mlx5_core_dev *dev)
 -{
 -	struct mlx5_priv *priv = &dev->priv;
 -
 -	pci_free_irq_vectors(dev->pdev);
 -	kfree(priv->eq_table->irq_info);
 -}
 -
 -int mlx5_eq_table_create(struct mlx5_core_dev *dev)
 -{
 -	int err;
 -
 -	err = alloc_irq_vectors(dev);
 -	if (err) {
 -		mlx5_core_err(dev, "alloc irq vectors failed\n");
 -		return err;
 -	}
 -
 -	err = create_async_eqs(dev);
 -	if (err) {
 -		mlx5_core_err(dev, "Failed to create async EQs\n");
 -		goto err_async_eqs;
 -	}
 -
 -	err = create_comp_eqs(dev);
 -	if (err) {
 -		mlx5_core_err(dev, "Failed to create completion EQs\n");
 -		goto err_comp_eqs;
 -	}
 -
 -	return 0;
 -err_comp_eqs:
 -	destroy_async_eqs(dev);
 -err_async_eqs:
 -	free_irq_vectors(dev);
 -	return err;
 -}
 -
 -void mlx5_eq_table_destroy(struct mlx5_core_dev *dev)
 -{
 -	destroy_comp_eqs(dev);
 -	destroy_async_eqs(dev);
 -	free_irq_vectors(dev);
 -}
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
diff --git a/include/linux/mlx5/eq.h b/include/linux/mlx5/eq.h
new file mode 100644
index 000000000000..c733673ba5f6
--- /dev/null
+++ b/include/linux/mlx5/eq.h
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB */
+/* Copyright (c) 2018 Mellanox Technologies. */
+
+#ifndef MLX5_CORE_EQ_H
+#define MLX5_CORE_EQ_H
+
+#include <linux/mlx5/driver.h>
+
+enum {
+	MLX5_EQ_PAGEREQ_IDX        = 0,
+	MLX5_EQ_CMD_IDX            = 1,
+	MLX5_EQ_ASYNC_IDX          = 2,
+	/* reserved to be used by mlx5_core ulps (mlx5e/mlx5_ib) */
+	MLX5_EQ_PFAULT_IDX         = 3,
+	MLX5_EQ_MAX_ASYNC_EQS,
+	/* completion eqs vector indices start here */
+	MLX5_EQ_VEC_COMP_BASE = MLX5_EQ_MAX_ASYNC_EQS,
+};
+
+struct mlx5_eq;
+
+struct mlx5_eq_param {
+	u8             index;
+	int            nent;
+	u64            mask;
+	void          *context;
+	irq_handler_t  handler;
+};
+
+struct mlx5_eq *
+mlx5_eq_create_generic(struct mlx5_core_dev *dev, const char *name,
+		       struct mlx5_eq_param *param);
+int
+mlx5_eq_destroy_generic(struct mlx5_core_dev *dev, struct mlx5_eq *eq);
+
+struct mlx5_eqe *mlx5_eq_get_eqe(struct mlx5_eq *eq, u32 cc);
+void mlx5_eq_update_ci(struct mlx5_eq *eq, u32 cc, bool arm);
+
+#endif /* MLX5_CORE_EQ_H */
