locking/rwsem: Conditionally wake waiters in reader/writer slowpaths

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.19.2.el8_7
commit-author Waiman Long <longman@redhat.com>
commit 54c1ee4d614d52844cf24c46d8415bf1392021d0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.19.2.el8_7/54c1ee4d.failed

In an analysis of a recent vmcore, a reader-owned rwsem was found with
385 readers but no writer in the wait queue. That is kind of unusual
but it may be caused by some race conditions that we have not fully
understood yet. In such a case, all the readers in the wait queue should
join the other reader-owners and acquire the read lock.

In rwsem_down_write_slowpath(), an incoming writer will try to
wake up the front readers under such circumstance. That is not
the case for rwsem_down_read_slowpath(), add a new helper function
rwsem_cond_wake_waiter() to do wakeup and use it in both reader and
writer slowpaths to have a consistent and correct behavior.

	Signed-off-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20220322152059.2182333-3-longman@redhat.com
(cherry picked from commit 54c1ee4d614d52844cf24c46d8415bf1392021d0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/rwsem.c
diff --cc kernel/locking/rwsem.c
index 8a9b81cbaffb,03cb97a8e4cd..000000000000
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@@ -1027,13 -1060,10 +1040,16 @@@ out_nolock
  /*
   * Wait until we successfully acquire the write lock
   */
 -static struct rw_semaphore __sched *
 +static struct rw_semaphore *
  rwsem_down_write_slowpath(struct rw_semaphore *sem, int state)
  {
++<<<<<<< HEAD
 +	long count;
 +	enum writer_wait_state wstate;
++=======
++>>>>>>> 54c1ee4d614d (locking/rwsem: Conditionally wake waiters in reader/writer slowpaths)
  	struct rwsem_waiter waiter;
 +	struct rw_semaphore *ret = sem;
  	DEFINE_WAKE_Q(wake_q);
  
  	/* do optimistic spinning and steal lock if possible */
@@@ -1049,33 -1079,15 +1065,39 @@@
  	waiter.task = current;
  	waiter.type = RWSEM_WAITING_FOR_WRITE;
  	waiter.timeout = jiffies + RWSEM_WAIT_TIMEOUT;
 -	waiter.handoff_set = false;
  
  	raw_spin_lock_irq(&sem->wait_lock);
 -	rwsem_add_waiter(sem, &waiter);
 +
 +	/* account for this before adding a new element to the list */
 +	wstate = list_empty(&sem->wait_list) ? WRITER_FIRST : WRITER_NOT_FIRST;
 +
 +	list_add_tail(&waiter.list, &sem->wait_list);
  
  	/* we're now waiting on the lock */
++<<<<<<< HEAD
 +	if (wstate == WRITER_NOT_FIRST) {
 +		count = atomic_long_read(&sem->count);
 +
 +		/*
 +		 * If there were already threads queued before us and:
 +		 *  1) there are no active locks, wake the front
 +		 *     queued process(es) as the handoff bit might be set.
 +		 *  2) there are no active writers and some readers, the lock
 +		 *     must be read owned; so we try to wake any read lock
 +		 *     waiters that were queued ahead of us.
 +		 */
 +		if (count & RWSEM_WRITER_MASK)
 +			goto wait;
 +
 +		rwsem_mark_wake(sem, (count & RWSEM_READER_MASK)
 +					? RWSEM_WAKE_READERS
 +					: RWSEM_WAKE_ANY, &wake_q);
 +
++=======
+ 	if (rwsem_first_waiter(sem) != &waiter) {
+ 		rwsem_cond_wake_waiter(sem, atomic_long_read(&sem->count),
+ 				       &wake_q);
++>>>>>>> 54c1ee4d614d (locking/rwsem: Conditionally wake waiters in reader/writer slowpaths)
  		if (!wake_q_empty(&wake_q)) {
  			/*
  			 * We want to minimize wait_lock hold time especially
* Unmerged path kernel/locking/rwsem.c
