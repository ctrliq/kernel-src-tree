KVM: x86: smm: preserve interrupt shadow in SMRAM

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.19.2.el8_7
commit-author Maxim Levitsky <mlevitsk@redhat.com>
commit fb28875fd7da184079150295da7ee8d80a70917e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.19.2.el8_7/fb28875f.failed

When #SMI is asserted, the CPU can be in interrupt shadow due to sti or
mov ss.

It is not mandatory in  Intel/AMD prm to have the #SMI blocked during the
shadow, and on top of that, since neither SVM nor VMX has true support
for SMI window, waiting for one instruction would mean single stepping
the guest.

Instead, allow #SMI in this case, but both reset the interrupt window and
stash its value in SMRAM to restore it on exit from SMM.

This fixes rare failures seen mostly on windows guests on VMX, when #SMI
falls on the sti instruction which mainfest in VM entry failure due
to EFLAGS.IF not being set, but STI interrupt window still being set
in the VMCS.

	Signed-off-by: Maxim Levitsky <mlevitsk@redhat.com>
Message-Id: <20221025124741.228045-24-mlevitsk@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit fb28875fd7da184079150295da7ee8d80a70917e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/smm.c
#	arch/x86/kvm/smm.h
diff --cc arch/x86/kvm/smm.c
index e4ce2d06521c,a9c1c2af8d94..000000000000
--- a/arch/x86/kvm/smm.c
+++ b/arch/x86/kvm/smm.c
@@@ -8,8 -8,110 +8,111 @@@
  #include "cpuid.h"
  #include "trace.h"
  
++<<<<<<< HEAD
++=======
+ #define CHECK_SMRAM32_OFFSET(field, offset) \
+ 	ASSERT_STRUCT_OFFSET(struct kvm_smram_state_32, field, offset - 0xFE00)
+ 
+ #define CHECK_SMRAM64_OFFSET(field, offset) \
+ 	ASSERT_STRUCT_OFFSET(struct kvm_smram_state_64, field, offset - 0xFE00)
+ 
+ static void check_smram_offsets(void)
+ {
+ 	/* 32 bit SMRAM image */
+ 	CHECK_SMRAM32_OFFSET(reserved1,			0xFE00);
+ 	CHECK_SMRAM32_OFFSET(smbase,			0xFEF8);
+ 	CHECK_SMRAM32_OFFSET(smm_revision,		0xFEFC);
+ 	CHECK_SMRAM32_OFFSET(io_inst_restart,		0xFF00);
+ 	CHECK_SMRAM32_OFFSET(auto_hlt_restart,		0xFF02);
+ 	CHECK_SMRAM32_OFFSET(io_restart_rdi,		0xFF04);
+ 	CHECK_SMRAM32_OFFSET(io_restart_rcx,		0xFF08);
+ 	CHECK_SMRAM32_OFFSET(io_restart_rsi,		0xFF0C);
+ 	CHECK_SMRAM32_OFFSET(io_restart_rip,		0xFF10);
+ 	CHECK_SMRAM32_OFFSET(cr4,			0xFF14);
+ 	CHECK_SMRAM32_OFFSET(reserved2,			0xFF18);
+ 	CHECK_SMRAM32_OFFSET(int_shadow,		0xFF1A);
+ 	CHECK_SMRAM32_OFFSET(reserved3,			0xFF1B);
+ 	CHECK_SMRAM32_OFFSET(ds,			0xFF2C);
+ 	CHECK_SMRAM32_OFFSET(fs,			0xFF38);
+ 	CHECK_SMRAM32_OFFSET(gs,			0xFF44);
+ 	CHECK_SMRAM32_OFFSET(idtr,			0xFF50);
+ 	CHECK_SMRAM32_OFFSET(tr,			0xFF5C);
+ 	CHECK_SMRAM32_OFFSET(gdtr,			0xFF6C);
+ 	CHECK_SMRAM32_OFFSET(ldtr,			0xFF78);
+ 	CHECK_SMRAM32_OFFSET(es,			0xFF84);
+ 	CHECK_SMRAM32_OFFSET(cs,			0xFF90);
+ 	CHECK_SMRAM32_OFFSET(ss,			0xFF9C);
+ 	CHECK_SMRAM32_OFFSET(es_sel,			0xFFA8);
+ 	CHECK_SMRAM32_OFFSET(cs_sel,			0xFFAC);
+ 	CHECK_SMRAM32_OFFSET(ss_sel,			0xFFB0);
+ 	CHECK_SMRAM32_OFFSET(ds_sel,			0xFFB4);
+ 	CHECK_SMRAM32_OFFSET(fs_sel,			0xFFB8);
+ 	CHECK_SMRAM32_OFFSET(gs_sel,			0xFFBC);
+ 	CHECK_SMRAM32_OFFSET(ldtr_sel,			0xFFC0);
+ 	CHECK_SMRAM32_OFFSET(tr_sel,			0xFFC4);
+ 	CHECK_SMRAM32_OFFSET(dr7,			0xFFC8);
+ 	CHECK_SMRAM32_OFFSET(dr6,			0xFFCC);
+ 	CHECK_SMRAM32_OFFSET(gprs,			0xFFD0);
+ 	CHECK_SMRAM32_OFFSET(eip,			0xFFF0);
+ 	CHECK_SMRAM32_OFFSET(eflags,			0xFFF4);
+ 	CHECK_SMRAM32_OFFSET(cr3,			0xFFF8);
+ 	CHECK_SMRAM32_OFFSET(cr0,			0xFFFC);
+ 
+ 	/* 64 bit SMRAM image */
+ 	CHECK_SMRAM64_OFFSET(es,			0xFE00);
+ 	CHECK_SMRAM64_OFFSET(cs,			0xFE10);
+ 	CHECK_SMRAM64_OFFSET(ss,			0xFE20);
+ 	CHECK_SMRAM64_OFFSET(ds,			0xFE30);
+ 	CHECK_SMRAM64_OFFSET(fs,			0xFE40);
+ 	CHECK_SMRAM64_OFFSET(gs,			0xFE50);
+ 	CHECK_SMRAM64_OFFSET(gdtr,			0xFE60);
+ 	CHECK_SMRAM64_OFFSET(ldtr,			0xFE70);
+ 	CHECK_SMRAM64_OFFSET(idtr,			0xFE80);
+ 	CHECK_SMRAM64_OFFSET(tr,			0xFE90);
+ 	CHECK_SMRAM64_OFFSET(io_restart_rip,		0xFEA0);
+ 	CHECK_SMRAM64_OFFSET(io_restart_rcx,		0xFEA8);
+ 	CHECK_SMRAM64_OFFSET(io_restart_rsi,		0xFEB0);
+ 	CHECK_SMRAM64_OFFSET(io_restart_rdi,		0xFEB8);
+ 	CHECK_SMRAM64_OFFSET(io_restart_dword,		0xFEC0);
+ 	CHECK_SMRAM64_OFFSET(reserved1,			0xFEC4);
+ 	CHECK_SMRAM64_OFFSET(io_inst_restart,		0xFEC8);
+ 	CHECK_SMRAM64_OFFSET(auto_hlt_restart,		0xFEC9);
+ 	CHECK_SMRAM64_OFFSET(amd_nmi_mask,		0xFECA);
+ 	CHECK_SMRAM64_OFFSET(int_shadow,		0xFECB);
+ 	CHECK_SMRAM64_OFFSET(reserved2,			0xFECC);
+ 	CHECK_SMRAM64_OFFSET(efer,			0xFED0);
+ 	CHECK_SMRAM64_OFFSET(svm_guest_flag,		0xFED8);
+ 	CHECK_SMRAM64_OFFSET(svm_guest_vmcb_gpa,	0xFEE0);
+ 	CHECK_SMRAM64_OFFSET(svm_guest_virtual_int,	0xFEE8);
+ 	CHECK_SMRAM64_OFFSET(reserved3,			0xFEF0);
+ 	CHECK_SMRAM64_OFFSET(smm_revison,		0xFEFC);
+ 	CHECK_SMRAM64_OFFSET(smbase,			0xFF00);
+ 	CHECK_SMRAM64_OFFSET(reserved4,			0xFF04);
+ 	CHECK_SMRAM64_OFFSET(ssp,			0xFF18);
+ 	CHECK_SMRAM64_OFFSET(svm_guest_pat,		0xFF20);
+ 	CHECK_SMRAM64_OFFSET(svm_host_efer,		0xFF28);
+ 	CHECK_SMRAM64_OFFSET(svm_host_cr4,		0xFF30);
+ 	CHECK_SMRAM64_OFFSET(svm_host_cr3,		0xFF38);
+ 	CHECK_SMRAM64_OFFSET(svm_host_cr0,		0xFF40);
+ 	CHECK_SMRAM64_OFFSET(cr4,			0xFF48);
+ 	CHECK_SMRAM64_OFFSET(cr3,			0xFF50);
+ 	CHECK_SMRAM64_OFFSET(cr0,			0xFF58);
+ 	CHECK_SMRAM64_OFFSET(dr7,			0xFF60);
+ 	CHECK_SMRAM64_OFFSET(dr6,			0xFF68);
+ 	CHECK_SMRAM64_OFFSET(rflags,			0xFF70);
+ 	CHECK_SMRAM64_OFFSET(rip,			0xFF78);
+ 	CHECK_SMRAM64_OFFSET(gprs,			0xFF80);
+ 
+ 	BUILD_BUG_ON(sizeof(union kvm_smram) != 512);
+ }
+ 
+ #undef CHECK_SMRAM64_OFFSET
+ #undef CHECK_SMRAM32_OFFSET
+ 
+ 
++>>>>>>> fb28875fd7da (KVM: x86: smm: preserve interrupt shadow in SMRAM)
  void kvm_smm_changed(struct kvm_vcpu *vcpu, bool entering_smm)
  {
 -	BUILD_BUG_ON(HF_SMM_MASK != X86EMUL_SMM_MASK);
 -
  	trace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);
  
  	if (entering_smm) {
@@@ -94,47 -188,43 +197,55 @@@ static void enter_smm_save_state_32(str
  	unsigned long val;
  	int i;
  
 -	smram->cr0     = kvm_read_cr0(vcpu);
 -	smram->cr3     = kvm_read_cr3(vcpu);
 -	smram->eflags  = kvm_get_rflags(vcpu);
 -	smram->eip     = kvm_rip_read(vcpu);
 +	PUT_SMSTATE(u32, buf, 0x7ffc, kvm_read_cr0(vcpu));
 +	PUT_SMSTATE(u32, buf, 0x7ff8, kvm_read_cr3(vcpu));
 +	PUT_SMSTATE(u32, buf, 0x7ff4, kvm_get_rflags(vcpu));
 +	PUT_SMSTATE(u32, buf, 0x7ff0, kvm_rip_read(vcpu));
  
  	for (i = 0; i < 8; i++)
 -		smram->gprs[i] = kvm_register_read_raw(vcpu, i);
 +		PUT_SMSTATE(u32, buf, 0x7fd0 + i * 4, kvm_register_read_raw(vcpu, i));
  
  	kvm_get_dr(vcpu, 6, &val);
 -	smram->dr6     = (u32)val;
 +	PUT_SMSTATE(u32, buf, 0x7fcc, (u32)val);
  	kvm_get_dr(vcpu, 7, &val);
 -	smram->dr7     = (u32)val;
 +	PUT_SMSTATE(u32, buf, 0x7fc8, (u32)val);
  
 -	enter_smm_save_seg_32(vcpu, &smram->tr, &smram->tr_sel, VCPU_SREG_TR);
 -	enter_smm_save_seg_32(vcpu, &smram->ldtr, &smram->ldtr_sel, VCPU_SREG_LDTR);
 +	kvm_get_segment(vcpu, &seg, VCPU_SREG_TR);
 +	PUT_SMSTATE(u32, buf, 0x7fc4, seg.selector);
 +	PUT_SMSTATE(u32, buf, 0x7f64, seg.base);
 +	PUT_SMSTATE(u32, buf, 0x7f60, seg.limit);
 +	PUT_SMSTATE(u32, buf, 0x7f5c, enter_smm_get_segment_flags(&seg));
 +
 +	kvm_get_segment(vcpu, &seg, VCPU_SREG_LDTR);
 +	PUT_SMSTATE(u32, buf, 0x7fc0, seg.selector);
 +	PUT_SMSTATE(u32, buf, 0x7f80, seg.base);
 +	PUT_SMSTATE(u32, buf, 0x7f7c, seg.limit);
 +	PUT_SMSTATE(u32, buf, 0x7f78, enter_smm_get_segment_flags(&seg));
  
  	static_call(kvm_x86_get_gdt)(vcpu, &dt);
 -	smram->gdtr.base = dt.address;
 -	smram->gdtr.limit = dt.size;
 +	PUT_SMSTATE(u32, buf, 0x7f74, dt.address);
 +	PUT_SMSTATE(u32, buf, 0x7f70, dt.size);
  
  	static_call(kvm_x86_get_idt)(vcpu, &dt);
 -	smram->idtr.base = dt.address;
 -	smram->idtr.limit = dt.size;
 +	PUT_SMSTATE(u32, buf, 0x7f58, dt.address);
 +	PUT_SMSTATE(u32, buf, 0x7f54, dt.size);
  
 -	enter_smm_save_seg_32(vcpu, &smram->es, &smram->es_sel, VCPU_SREG_ES);
 -	enter_smm_save_seg_32(vcpu, &smram->cs, &smram->cs_sel, VCPU_SREG_CS);
 -	enter_smm_save_seg_32(vcpu, &smram->ss, &smram->ss_sel, VCPU_SREG_SS);
 +	for (i = 0; i < 6; i++)
 +		enter_smm_save_seg_32(vcpu, buf, i);
  
 -	enter_smm_save_seg_32(vcpu, &smram->ds, &smram->ds_sel, VCPU_SREG_DS);
 -	enter_smm_save_seg_32(vcpu, &smram->fs, &smram->fs_sel, VCPU_SREG_FS);
 -	enter_smm_save_seg_32(vcpu, &smram->gs, &smram->gs_sel, VCPU_SREG_GS);
 +	PUT_SMSTATE(u32, buf, 0x7f14, kvm_read_cr4(vcpu));
  
++<<<<<<< HEAD
 +	/* revision id */
 +	PUT_SMSTATE(u32, buf, 0x7efc, 0x00020000);
 +	PUT_SMSTATE(u32, buf, 0x7ef8, vcpu->arch.smbase);
++=======
+ 	smram->cr4 = kvm_read_cr4(vcpu);
+ 	smram->smm_revision = 0x00020000;
+ 	smram->smbase = vcpu->arch.smbase;
+ 
+ 	smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
++>>>>>>> fb28875fd7da (KVM: x86: smm: preserve interrupt shadow in SMRAM)
  }
  
  #ifdef CONFIG_X86_64
@@@ -146,49 -236,46 +257,60 @@@ static void enter_smm_save_state_64(str
  	int i;
  
  	for (i = 0; i < 16; i++)
 -		smram->gprs[15 - i] = kvm_register_read_raw(vcpu, i);
 -
 -	smram->rip    = kvm_rip_read(vcpu);
 -	smram->rflags = kvm_get_rflags(vcpu);
 +		PUT_SMSTATE(u64, buf, 0x7ff8 - i * 8, kvm_register_read_raw(vcpu, i));
  
 +	PUT_SMSTATE(u64, buf, 0x7f78, kvm_rip_read(vcpu));
 +	PUT_SMSTATE(u32, buf, 0x7f70, kvm_get_rflags(vcpu));
  
  	kvm_get_dr(vcpu, 6, &val);
 -	smram->dr6 = val;
 +	PUT_SMSTATE(u64, buf, 0x7f68, val);
  	kvm_get_dr(vcpu, 7, &val);
 -	smram->dr7 = val;
 +	PUT_SMSTATE(u64, buf, 0x7f60, val);
  
 -	smram->cr0 = kvm_read_cr0(vcpu);
 -	smram->cr3 = kvm_read_cr3(vcpu);
 -	smram->cr4 = kvm_read_cr4(vcpu);
 +	PUT_SMSTATE(u64, buf, 0x7f58, kvm_read_cr0(vcpu));
 +	PUT_SMSTATE(u64, buf, 0x7f50, kvm_read_cr3(vcpu));
 +	PUT_SMSTATE(u64, buf, 0x7f48, kvm_read_cr4(vcpu));
  
 -	smram->smbase = vcpu->arch.smbase;
 -	smram->smm_revison = 0x00020064;
 +	PUT_SMSTATE(u32, buf, 0x7f00, vcpu->arch.smbase);
 +
 +	/* revision id */
 +	PUT_SMSTATE(u32, buf, 0x7efc, 0x00020064);
  
 -	smram->efer = vcpu->arch.efer;
 +	PUT_SMSTATE(u64, buf, 0x7ed0, vcpu->arch.efer);
  
 -	enter_smm_save_seg_64(vcpu, &smram->tr, VCPU_SREG_TR);
 +	kvm_get_segment(vcpu, &seg, VCPU_SREG_TR);
 +	PUT_SMSTATE(u16, buf, 0x7e90, seg.selector);
 +	PUT_SMSTATE(u16, buf, 0x7e92, enter_smm_get_segment_flags(&seg) >> 8);
 +	PUT_SMSTATE(u32, buf, 0x7e94, seg.limit);
 +	PUT_SMSTATE(u64, buf, 0x7e98, seg.base);
  
  	static_call(kvm_x86_get_idt)(vcpu, &dt);
 -	smram->idtr.limit = dt.size;
 -	smram->idtr.base = dt.address;
 +	PUT_SMSTATE(u32, buf, 0x7e84, dt.size);
 +	PUT_SMSTATE(u64, buf, 0x7e88, dt.address);
  
 -	enter_smm_save_seg_64(vcpu, &smram->ldtr, VCPU_SREG_LDTR);
 +	kvm_get_segment(vcpu, &seg, VCPU_SREG_LDTR);
 +	PUT_SMSTATE(u16, buf, 0x7e70, seg.selector);
 +	PUT_SMSTATE(u16, buf, 0x7e72, enter_smm_get_segment_flags(&seg) >> 8);
 +	PUT_SMSTATE(u32, buf, 0x7e74, seg.limit);
 +	PUT_SMSTATE(u64, buf, 0x7e78, seg.base);
  
  	static_call(kvm_x86_get_gdt)(vcpu, &dt);
 -	smram->gdtr.limit = dt.size;
 -	smram->gdtr.base = dt.address;
 +	PUT_SMSTATE(u32, buf, 0x7e64, dt.size);
 +	PUT_SMSTATE(u64, buf, 0x7e68, dt.address);
  
++<<<<<<< HEAD
 +	for (i = 0; i < 6; i++)
 +		enter_smm_save_seg_64(vcpu, buf, i);
++=======
+ 	enter_smm_save_seg_64(vcpu, &smram->es, VCPU_SREG_ES);
+ 	enter_smm_save_seg_64(vcpu, &smram->cs, VCPU_SREG_CS);
+ 	enter_smm_save_seg_64(vcpu, &smram->ss, VCPU_SREG_SS);
+ 	enter_smm_save_seg_64(vcpu, &smram->ds, VCPU_SREG_DS);
+ 	enter_smm_save_seg_64(vcpu, &smram->fs, VCPU_SREG_FS);
+ 	enter_smm_save_seg_64(vcpu, &smram->gs, VCPU_SREG_GS);
+ 
+ 	smram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);
++>>>>>>> fb28875fd7da (KVM: x86: smm: preserve interrupt shadow in SMRAM)
  }
  #endif
  
@@@ -380,63 -466,54 +504,80 @@@ static int rsm_enter_protected_mode(str
  }
  
  static int rsm_load_state_32(struct x86_emulate_ctxt *ctxt,
 -			     const struct kvm_smram_state_32 *smstate)
 +			     const char *smstate)
  {
  	struct kvm_vcpu *vcpu = ctxt->vcpu;
 +	struct kvm_segment desc;
  	struct desc_ptr dt;
++<<<<<<< HEAD
 +	u32 val, cr0, cr3, cr4;
 +	int i;
++=======
+ 	int i, r;
++>>>>>>> fb28875fd7da (KVM: x86: smm: preserve interrupt shadow in SMRAM)
  
 -	ctxt->eflags =  smstate->eflags | X86_EFLAGS_FIXED;
 -	ctxt->_eip =  smstate->eip;
 +	cr0 =                      GET_SMSTATE(u32, smstate, 0x7ffc);
 +	cr3 =                      GET_SMSTATE(u32, smstate, 0x7ff8);
 +	ctxt->eflags =             GET_SMSTATE(u32, smstate, 0x7ff4) | X86_EFLAGS_FIXED;
 +	ctxt->_eip =               GET_SMSTATE(u32, smstate, 0x7ff0);
  
  	for (i = 0; i < 8; i++)
 -		*reg_write(ctxt, i) = smstate->gprs[i];
 +		*reg_write(ctxt, i) = GET_SMSTATE(u32, smstate, 0x7fd0 + i * 4);
 +
 +	val = GET_SMSTATE(u32, smstate, 0x7fcc);
  
 -	if (kvm_set_dr(vcpu, 6, smstate->dr6))
 +	if (kvm_set_dr(vcpu, 6, val))
  		return X86EMUL_UNHANDLEABLE;
 -	if (kvm_set_dr(vcpu, 7, smstate->dr7))
 +
 +	val = GET_SMSTATE(u32, smstate, 0x7fc8);
 +
 +	if (kvm_set_dr(vcpu, 7, val))
  		return X86EMUL_UNHANDLEABLE;
  
 -	rsm_load_seg_32(vcpu, &smstate->tr, smstate->tr_sel, VCPU_SREG_TR);
 -	rsm_load_seg_32(vcpu, &smstate->ldtr, smstate->ldtr_sel, VCPU_SREG_LDTR);
 +	desc.selector =            GET_SMSTATE(u32, smstate, 0x7fc4);
 +	desc.base =                GET_SMSTATE(u32, smstate, 0x7f64);
 +	desc.limit =               GET_SMSTATE(u32, smstate, 0x7f60);
 +	rsm_set_desc_flags(&desc,  GET_SMSTATE(u32, smstate, 0x7f5c));
 +	kvm_set_segment(vcpu, &desc, VCPU_SREG_TR);
  
 -	dt.address =               smstate->gdtr.base;
 -	dt.size =                  smstate->gdtr.limit;
 +	desc.selector =            GET_SMSTATE(u32, smstate, 0x7fc0);
 +	desc.base =                GET_SMSTATE(u32, smstate, 0x7f80);
 +	desc.limit =               GET_SMSTATE(u32, smstate, 0x7f7c);
 +	rsm_set_desc_flags(&desc,  GET_SMSTATE(u32, smstate, 0x7f78));
 +	kvm_set_segment(vcpu, &desc, VCPU_SREG_LDTR);
 +
 +	dt.address =               GET_SMSTATE(u32, smstate, 0x7f74);
 +	dt.size =                  GET_SMSTATE(u32, smstate, 0x7f70);
  	static_call(kvm_x86_set_gdt)(vcpu, &dt);
  
 -	dt.address =               smstate->idtr.base;
 -	dt.size =                  smstate->idtr.limit;
 +	dt.address =               GET_SMSTATE(u32, smstate, 0x7f58);
 +	dt.size =                  GET_SMSTATE(u32, smstate, 0x7f54);
  	static_call(kvm_x86_set_idt)(vcpu, &dt);
  
 -	rsm_load_seg_32(vcpu, &smstate->es, smstate->es_sel, VCPU_SREG_ES);
 -	rsm_load_seg_32(vcpu, &smstate->cs, smstate->cs_sel, VCPU_SREG_CS);
 -	rsm_load_seg_32(vcpu, &smstate->ss, smstate->ss_sel, VCPU_SREG_SS);
 +	for (i = 0; i < 6; i++) {
 +		int r = rsm_load_seg_32(vcpu, smstate, i);
 +		if (r != X86EMUL_CONTINUE)
 +			return r;
 +	}
  
 -	rsm_load_seg_32(vcpu, &smstate->ds, smstate->ds_sel, VCPU_SREG_DS);
 -	rsm_load_seg_32(vcpu, &smstate->fs, smstate->fs_sel, VCPU_SREG_FS);
 -	rsm_load_seg_32(vcpu, &smstate->gs, smstate->gs_sel, VCPU_SREG_GS);
 +	cr4 = GET_SMSTATE(u32, smstate, 0x7f14);
  
 -	vcpu->arch.smbase = smstate->smbase;
 +	vcpu->arch.smbase = GET_SMSTATE(u32, smstate, 0x7ef8);
  
++<<<<<<< HEAD
 +	return rsm_enter_protected_mode(vcpu, cr0, cr3, cr4);
++=======
+ 	r = rsm_enter_protected_mode(vcpu, smstate->cr0,
+ 					smstate->cr3, smstate->cr4);
+ 
+ 	if (r != X86EMUL_CONTINUE)
+ 		return r;
+ 
+ 	static_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);
+ 	ctxt->interruptibility = (u8)smstate->int_shadow;
+ 
+ 	return r;
++>>>>>>> fb28875fd7da (KVM: x86: smm: preserve interrupt shadow in SMRAM)
  }
  
  #ifdef CONFIG_X86_64
@@@ -498,12 -556,16 +639,15 @@@ static int rsm_load_state_64(struct x86
  	if (r != X86EMUL_CONTINUE)
  		return r;
  
 -	rsm_load_seg_64(vcpu, &smstate->es, VCPU_SREG_ES);
 -	rsm_load_seg_64(vcpu, &smstate->cs, VCPU_SREG_CS);
 -	rsm_load_seg_64(vcpu, &smstate->ss, VCPU_SREG_SS);
 -	rsm_load_seg_64(vcpu, &smstate->ds, VCPU_SREG_DS);
 -	rsm_load_seg_64(vcpu, &smstate->fs, VCPU_SREG_FS);
 -	rsm_load_seg_64(vcpu, &smstate->gs, VCPU_SREG_GS);
 +	for (i = 0; i < 6; i++) {
 +		r = rsm_load_seg_64(vcpu, smstate, i);
 +		if (r != X86EMUL_CONTINUE)
 +			return r;
 +	}
  
+ 	static_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);
+ 	ctxt->interruptibility = (u8)smstate->int_shadow;
+ 
  	return X86EMUL_CONTINUE;
  }
  #endif
diff --cc arch/x86/kvm/smm.h
index b0602a92e511,a1cf2ac5bd78..000000000000
--- a/arch/x86/kvm/smm.h
+++ b/arch/x86/kvm/smm.h
@@@ -2,11 -2,143 +2,149 @@@
  #ifndef ASM_KVM_SMM_H
  #define ASM_KVM_SMM_H
  
 -#include <linux/build_bug.h>
 +#define GET_SMSTATE(type, buf, offset)		\
 +	(*(type *)((buf) + (offset) - 0x7e00))
  
++<<<<<<< HEAD
 +#define PUT_SMSTATE(type, buf, offset, val)                      \
 +	*(type *)((buf) + (offset) - 0x7e00) = val
++=======
+ #ifdef CONFIG_KVM_SMM
+ 
+ 
+ /*
+  * 32 bit KVM's emulated SMM layout. Based on Intel P6 layout
+  * (https://www.sandpile.org/x86/smm.htm).
+  */
+ 
+ struct kvm_smm_seg_state_32 {
+ 	u32 flags;
+ 	u32 limit;
+ 	u32 base;
+ } __packed;
+ 
+ struct kvm_smram_state_32 {
+ 	u32 reserved1[62];
+ 	u32 smbase;
+ 	u32 smm_revision;
+ 	u16 io_inst_restart;
+ 	u16 auto_hlt_restart;
+ 	u32 io_restart_rdi;
+ 	u32 io_restart_rcx;
+ 	u32 io_restart_rsi;
+ 	u32 io_restart_rip;
+ 	u32 cr4;
+ 
+ 	/* A20M#, CPL, shutdown and other reserved/undocumented fields */
+ 	u16 reserved2;
+ 	u8 int_shadow; /* KVM extension */
+ 	u8 reserved3[17];
+ 
+ 	struct kvm_smm_seg_state_32 ds;
+ 	struct kvm_smm_seg_state_32 fs;
+ 	struct kvm_smm_seg_state_32 gs;
+ 	struct kvm_smm_seg_state_32 idtr; /* IDTR has only base and limit */
+ 	struct kvm_smm_seg_state_32 tr;
+ 	u32 reserved;
+ 	struct kvm_smm_seg_state_32 gdtr; /* GDTR has only base and limit */
+ 	struct kvm_smm_seg_state_32 ldtr;
+ 	struct kvm_smm_seg_state_32 es;
+ 	struct kvm_smm_seg_state_32 cs;
+ 	struct kvm_smm_seg_state_32 ss;
+ 
+ 	u32 es_sel;
+ 	u32 cs_sel;
+ 	u32 ss_sel;
+ 	u32 ds_sel;
+ 	u32 fs_sel;
+ 	u32 gs_sel;
+ 	u32 ldtr_sel;
+ 	u32 tr_sel;
+ 
+ 	u32 dr7;
+ 	u32 dr6;
+ 	u32 gprs[8]; /* GPRS in the "natural" X86 order (EAX/ECX/EDX.../EDI) */
+ 	u32 eip;
+ 	u32 eflags;
+ 	u32 cr3;
+ 	u32 cr0;
+ } __packed;
+ 
+ 
+ /* 64 bit KVM's emulated SMM layout. Based on AMD64 layout */
+ 
+ struct kvm_smm_seg_state_64 {
+ 	u16 selector;
+ 	u16 attributes;
+ 	u32 limit;
+ 	u64 base;
+ };
+ 
+ struct kvm_smram_state_64 {
+ 
+ 	struct kvm_smm_seg_state_64 es;
+ 	struct kvm_smm_seg_state_64 cs;
+ 	struct kvm_smm_seg_state_64 ss;
+ 	struct kvm_smm_seg_state_64 ds;
+ 	struct kvm_smm_seg_state_64 fs;
+ 	struct kvm_smm_seg_state_64 gs;
+ 	struct kvm_smm_seg_state_64 gdtr; /* GDTR has only base and limit*/
+ 	struct kvm_smm_seg_state_64 ldtr;
+ 	struct kvm_smm_seg_state_64 idtr; /* IDTR has only base and limit*/
+ 	struct kvm_smm_seg_state_64 tr;
+ 
+ 	/* I/O restart and auto halt restart are not implemented by KVM */
+ 	u64 io_restart_rip;
+ 	u64 io_restart_rcx;
+ 	u64 io_restart_rsi;
+ 	u64 io_restart_rdi;
+ 	u32 io_restart_dword;
+ 	u32 reserved1;
+ 	u8 io_inst_restart;
+ 	u8 auto_hlt_restart;
+ 	u8 amd_nmi_mask; /* Documented in AMD BKDG as NMI mask, not used by KVM */
+ 	u8 int_shadow;
+ 	u32 reserved2;
+ 
+ 	u64 efer;
+ 
+ 	/*
+ 	 * Two fields below are implemented on AMD only, to store
+ 	 * SVM guest vmcb address if the #SMI was received while in the guest mode.
+ 	 */
+ 	u64 svm_guest_flag;
+ 	u64 svm_guest_vmcb_gpa;
+ 	u64 svm_guest_virtual_int; /* unknown purpose, not implemented */
+ 
+ 	u32 reserved3[3];
+ 	u32 smm_revison;
+ 	u32 smbase;
+ 	u32 reserved4[5];
+ 
+ 	/* ssp and svm_* fields below are not implemented by KVM */
+ 	u64 ssp;
+ 	u64 svm_guest_pat;
+ 	u64 svm_host_efer;
+ 	u64 svm_host_cr4;
+ 	u64 svm_host_cr3;
+ 	u64 svm_host_cr0;
+ 
+ 	u64 cr4;
+ 	u64 cr3;
+ 	u64 cr0;
+ 	u64 dr7;
+ 	u64 dr6;
+ 	u64 rflags;
+ 	u64 rip;
+ 	u64 gprs[16]; /* GPRS in a reversed "natural" X86 order (R15/R14/../RCX/RAX.) */
+ };
+ 
+ union kvm_smram {
+ 	struct kvm_smram_state_64 smram64;
+ 	struct kvm_smram_state_32 smram32;
+ 	u8 bytes[512];
+ };
++>>>>>>> fb28875fd7da (KVM: x86: smm: preserve interrupt shadow in SMRAM)
  
  static inline int kvm_inject_smi(struct kvm_vcpu *vcpu)
  {
* Unmerged path arch/x86/kvm/smm.c
* Unmerged path arch/x86/kvm/smm.h
