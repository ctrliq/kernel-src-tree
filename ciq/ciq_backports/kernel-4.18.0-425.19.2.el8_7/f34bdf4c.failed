KVM: x86: smm: use smram struct for 32 bit smram load/restore

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.19.2.el8_7
commit-author Maxim Levitsky <mlevitsk@redhat.com>
commit f34bdf4c1707cdc687db87965d08bb5a51300c58
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.19.2.el8_7/f34bdf4c.failed

Use kvm_smram_state_32 struct to save/restore 32 bit SMM state
(used when X86_FEATURE_LM is not present in the guest CPUID).

	Signed-off-by: Maxim Levitsky <mlevitsk@redhat.com>
Message-Id: <20221025124741.228045-19-mlevitsk@redhat.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f34bdf4c1707cdc687db87965d08bb5a51300c58)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/smm.c
diff --cc arch/x86/kvm/smm.c
index e4ce2d06521c,2c808d0a8e92..000000000000
--- a/arch/x86/kvm/smm.c
+++ b/arch/x86/kvm/smm.c
@@@ -197,15 -284,18 +186,19 @@@ void enter_smm(struct kvm_vcpu *vcpu
  	struct kvm_segment cs, ds;
  	struct desc_ptr dt;
  	unsigned long cr0;
 -	union kvm_smram smram;
 -
 -	check_smram_offsets();
 -
 -	memset(smram.bytes, 0, sizeof(smram.bytes));
 +	char buf[512];
  
 +	memset(buf, 0, 512);
  #ifdef CONFIG_X86_64
  	if (guest_cpuid_has(vcpu, X86_FEATURE_LM))
 -		enter_smm_save_state_64(vcpu, smram.bytes);
 +		enter_smm_save_state_64(vcpu, buf);
  	else
  #endif
++<<<<<<< HEAD
 +		enter_smm_save_state_32(vcpu, buf);
++=======
+ 		enter_smm_save_state_32(vcpu, &smram.smram32);
++>>>>>>> f34bdf4c1707 (KVM: x86: smm: use smram struct for 32 bit smram load/restore)
  
  	/*
  	 * Give enter_smm() a chance to make ISA-specific changes to the vCPU
@@@ -380,63 -465,46 +368,50 @@@ static int rsm_enter_protected_mode(str
  }
  
  static int rsm_load_state_32(struct x86_emulate_ctxt *ctxt,
++<<<<<<< HEAD
 +			     const char *smstate)
++=======
+ 			     const struct kvm_smram_state_32 *smstate)
++>>>>>>> f34bdf4c1707 (KVM: x86: smm: use smram struct for 32 bit smram load/restore)
  {
  	struct kvm_vcpu *vcpu = ctxt->vcpu;
- 	struct kvm_segment desc;
  	struct desc_ptr dt;
- 	u32 val, cr0, cr3, cr4;
  	int i;
  
- 	cr0 =                      GET_SMSTATE(u32, smstate, 0x7ffc);
- 	cr3 =                      GET_SMSTATE(u32, smstate, 0x7ff8);
- 	ctxt->eflags =             GET_SMSTATE(u32, smstate, 0x7ff4) | X86_EFLAGS_FIXED;
- 	ctxt->_eip =               GET_SMSTATE(u32, smstate, 0x7ff0);
+ 	ctxt->eflags =  smstate->eflags | X86_EFLAGS_FIXED;
+ 	ctxt->_eip =  smstate->eip;
  
  	for (i = 0; i < 8; i++)
- 		*reg_write(ctxt, i) = GET_SMSTATE(u32, smstate, 0x7fd0 + i * 4);
+ 		*reg_write(ctxt, i) = smstate->gprs[i];
  
- 	val = GET_SMSTATE(u32, smstate, 0x7fcc);
- 
- 	if (kvm_set_dr(vcpu, 6, val))
+ 	if (kvm_set_dr(vcpu, 6, smstate->dr6))
  		return X86EMUL_UNHANDLEABLE;
- 
- 	val = GET_SMSTATE(u32, smstate, 0x7fc8);
- 
- 	if (kvm_set_dr(vcpu, 7, val))
+ 	if (kvm_set_dr(vcpu, 7, smstate->dr7))
  		return X86EMUL_UNHANDLEABLE;
  
- 	desc.selector =            GET_SMSTATE(u32, smstate, 0x7fc4);
- 	desc.base =                GET_SMSTATE(u32, smstate, 0x7f64);
- 	desc.limit =               GET_SMSTATE(u32, smstate, 0x7f60);
- 	rsm_set_desc_flags(&desc,  GET_SMSTATE(u32, smstate, 0x7f5c));
- 	kvm_set_segment(vcpu, &desc, VCPU_SREG_TR);
- 
- 	desc.selector =            GET_SMSTATE(u32, smstate, 0x7fc0);
- 	desc.base =                GET_SMSTATE(u32, smstate, 0x7f80);
- 	desc.limit =               GET_SMSTATE(u32, smstate, 0x7f7c);
- 	rsm_set_desc_flags(&desc,  GET_SMSTATE(u32, smstate, 0x7f78));
- 	kvm_set_segment(vcpu, &desc, VCPU_SREG_LDTR);
+ 	rsm_load_seg_32(vcpu, &smstate->tr, smstate->tr_sel, VCPU_SREG_TR);
+ 	rsm_load_seg_32(vcpu, &smstate->ldtr, smstate->ldtr_sel, VCPU_SREG_LDTR);
  
- 	dt.address =               GET_SMSTATE(u32, smstate, 0x7f74);
- 	dt.size =                  GET_SMSTATE(u32, smstate, 0x7f70);
+ 	dt.address =               smstate->gdtr.base;
+ 	dt.size =                  smstate->gdtr.limit;
  	static_call(kvm_x86_set_gdt)(vcpu, &dt);
  
- 	dt.address =               GET_SMSTATE(u32, smstate, 0x7f58);
- 	dt.size =                  GET_SMSTATE(u32, smstate, 0x7f54);
+ 	dt.address =               smstate->idtr.base;
+ 	dt.size =                  smstate->idtr.limit;
  	static_call(kvm_x86_set_idt)(vcpu, &dt);
  
- 	for (i = 0; i < 6; i++) {
- 		int r = rsm_load_seg_32(vcpu, smstate, i);
- 		if (r != X86EMUL_CONTINUE)
- 			return r;
- 	}
+ 	rsm_load_seg_32(vcpu, &smstate->es, smstate->es_sel, VCPU_SREG_ES);
+ 	rsm_load_seg_32(vcpu, &smstate->cs, smstate->cs_sel, VCPU_SREG_CS);
+ 	rsm_load_seg_32(vcpu, &smstate->ss, smstate->ss_sel, VCPU_SREG_SS);
  
- 	cr4 = GET_SMSTATE(u32, smstate, 0x7f14);
+ 	rsm_load_seg_32(vcpu, &smstate->ds, smstate->ds_sel, VCPU_SREG_DS);
+ 	rsm_load_seg_32(vcpu, &smstate->fs, smstate->fs_sel, VCPU_SREG_FS);
+ 	rsm_load_seg_32(vcpu, &smstate->gs, smstate->gs_sel, VCPU_SREG_GS);
  
- 	vcpu->arch.smbase = GET_SMSTATE(u32, smstate, 0x7ef8);
+ 	vcpu->arch.smbase = smstate->smbase;
  
- 	return rsm_enter_protected_mode(vcpu, cr0, cr3, cr4);
+ 	return rsm_enter_protected_mode(vcpu, smstate->cr0,
+ 					smstate->cr3, smstate->cr4);
  }
  
  #ifdef CONFIG_X86_64
@@@ -580,8 -648,8 +555,12 @@@ int emulator_leave_smm(struct x86_emula
  
  #ifdef CONFIG_X86_64
  	if (guest_cpuid_has(vcpu, X86_FEATURE_LM))
 -		return rsm_load_state_64(ctxt, smram.bytes);
 +		return rsm_load_state_64(ctxt, buf);
  	else
  #endif
++<<<<<<< HEAD
 +		return rsm_load_state_32(ctxt, buf);
++=======
+ 		return rsm_load_state_32(ctxt, &smram.smram32);
++>>>>>>> f34bdf4c1707 (KVM: x86: smm: use smram struct for 32 bit smram load/restore)
  }
* Unmerged path arch/x86/kvm/smm.c
