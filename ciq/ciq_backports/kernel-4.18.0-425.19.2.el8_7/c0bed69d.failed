locking: Make owner_on_cpu() into <linux/sched.h>

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.19.2.el8_7
commit-author Kefeng Wang <wangkefeng.wang@huawei.com>
commit c0bed69daf4b67809b58cc7cd81a8fa4f45bc161
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.19.2.el8_7/c0bed69d.failed

Move the owner_on_cpu() from kernel/locking/rwsem.c into
include/linux/sched.h with under CONFIG_SMP, then use it
in the mutex/rwsem/rtmutex to simplify the code.

	Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lore.kernel.org/r/20211203075935.136808-2-wangkefeng.wang@huawei.com
(cherry picked from commit c0bed69daf4b67809b58cc7cd81a8fa4f45bc161)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched.h
diff --cc include/linux/sched.h
index 9bc6a48c435c,ff609d9c2f21..000000000000
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@@ -2082,6 -2170,20 +2082,23 @@@ extern long sched_getaffinity(pid_t pid
  #define TASK_SIZE_OF(tsk)	TASK_SIZE
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_SMP
+ static inline bool owner_on_cpu(struct task_struct *owner)
+ {
+ 	/*
+ 	 * As lock holder preemption issue, we both skip spinning if
+ 	 * task is not on cpu or its cpu is preempted
+ 	 */
+ 	return owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));
+ }
+ 
+ /* Returns effective CPU energy utilization, as seen by the scheduler */
+ unsigned long sched_cpu_util(int cpu, unsigned long max);
+ #endif /* CONFIG_SMP */
+ 
++>>>>>>> c0bed69daf4b (locking: Make owner_on_cpu() into <linux/sched.h>)
  #ifdef CONFIG_RSEQ
  
  /*
* Unmerged path include/linux/sched.h
diff --git a/kernel/locking/mutex.c b/kernel/locking/mutex.c
index 40fb2be920a5..98cc6f8566bd 100644
--- a/kernel/locking/mutex.c
+++ b/kernel/locking/mutex.c
@@ -376,8 +376,7 @@ bool mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner,
 		/*
 		 * Use vcpu_is_preempted to detect lock holder preemption issue.
 		 */
-		if (!owner->on_cpu || need_resched() ||
-				vcpu_is_preempted(task_cpu(owner))) {
+		if (!owner_on_cpu(owner) || need_resched()) {
 			ret = false;
 			break;
 		}
@@ -412,14 +411,8 @@ static inline int mutex_can_spin_on_owner(struct mutex *lock)
 	 * structure won't go away during the spinning period.
 	 */
 	owner = __mutex_owner(lock);
-
-	/*
-	 * As lock holder preemption issue, we both skip spinning if task is not
-	 * on cpu or its cpu is preempted
-	 */
-
 	if (owner)
-		retval = owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));
+		retval = owner_on_cpu(owner);
 
 	/*
 	 * If lock->owner is not set, the mutex has been released. Return true
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index f41036480ce6..0bca38d1110b 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -1371,9 +1371,8 @@ static bool rtmutex_spin_on_owner(struct rt_mutex_base *lock,
 		 *    for CONFIG_PREEMPT_RCU=y)
 		 *  - the VCPU on which owner runs is preempted
 		 */
-		if (!owner->on_cpu || need_resched() ||
-		    rt_mutex_waiter_is_top_waiter(lock, waiter) ||
-		    vcpu_is_preempted(task_cpu(owner))) {
+		if (!owner_on_cpu(owner) || need_resched() ||
+		    rt_mutex_waiter_is_top_waiter(lock, waiter)) {
 			res = false;
 			break;
 		}
diff --git a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
index 8a9b81cbaffb..b4c7ebc03070 100644
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@ -624,15 +624,6 @@ static inline bool rwsem_try_write_lock_unqueued(struct rw_semaphore *sem)
 	return false;
 }
 
-static inline bool owner_on_cpu(struct task_struct *owner)
-{
-	/*
-	 * As lock holder preemption issue, we both skip spinning if
-	 * task is not on cpu or its cpu is preempted
-	 */
-	return owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));
-}
-
 static inline bool rwsem_can_spin_on_owner(struct rw_semaphore *sem)
 {
 	struct task_struct *owner;
