ice: convert ice_reset_vf to take flags

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-372.26.1.rt7.183.el8_6
commit-author Jacob Keller <jacob.e.keller@intel.com>
commit 7eb517e434c653a4afa16ec3d0a750c2f46b3560
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-372.26.1.rt7.183.el8_6/7eb517e4.failed

The ice_reset_vf function takes a boolean parameter which indicates
whether or not the reset is due to a VFLR event.

This is somewhat confusing to read because readers must interpret what
"true" and "false" mean when seeing a line of code like
"ice_reset_vf(vf, false)".

We will want to add another toggle to the ice_reset_vf in a following
change. To avoid proliferating many arguments, convert this function to
take flags instead. ICE_VF_RESET_VFLR will indicate if this is a VFLR
reset. A value of 0 indicates no flags.

One could argue that "ice_reset_vf(vf, 0)" is no more readable than
"ice_reset_vf(vf, false)".. However, this type of flags interface is
somewhat common and using 0 to mean "no flags" makes sense in this
context. We could bother to add a define for "ICE_VF_RESET_PLAIN" or
something similar, but this can be confusing since its not an actual bit
flag.

This paves the way to add another flag to the function in a following
change.

	Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
	Tested-by: Konrad Jankowski <konrad0.jankowski@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 7eb517e434c653a4afa16ec3d0a750c2f46b3560)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_main.c
#	drivers/net/ethernet/intel/ice/ice_sriov.c
#	drivers/net/ethernet/intel/ice/ice_vf_lib.c
#	drivers/net/ethernet/intel/ice/ice_vf_lib.h
diff --cc drivers/net/ethernet/intel/ice/ice_main.c
index 1e2157690b86,2e23fdc099e0..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_main.c
+++ b/drivers/net/ethernet/intel/ice/ice_main.c
@@@ -1800,9 -1823,9 +1800,15 @@@ static void ice_handle_mdd_event(struc
  				 * reset, so print the event prior to reset.
  				 */
  				ice_print_vf_rx_mdd_event(vf);
++<<<<<<< HEAD
 +				mutex_lock(&pf->vf[i].cfg_lock);
 +				ice_reset_vf(&pf->vf[i], false);
 +				mutex_unlock(&pf->vf[i].cfg_lock);
++=======
+ 				mutex_lock(&vf->cfg_lock);
+ 				ice_reset_vf(vf, 0);
+ 				mutex_unlock(&vf->cfg_lock);
++>>>>>>> 7eb517e434c6 (ice: convert ice_reset_vf to take flags)
  			}
  		}
  	}
diff --cc drivers/net/ethernet/intel/ice/ice_sriov.c
index 52c6bac41bf7,c234e4edc7f0..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_sriov.c
+++ b/drivers/net/ethernet/intel/ice/ice_sriov.c
@@@ -1,220 -1,1953 +1,1751 @@@
  // SPDX-License-Identifier: GPL-2.0
  /* Copyright (c) 2018, Intel Corporation. */
  
 -#include "ice.h"
 -#include "ice_vf_lib_private.h"
 -#include "ice_base.h"
 -#include "ice_lib.h"
 -#include "ice_fltr.h"
 -#include "ice_dcb_lib.h"
 -#include "ice_flow.h"
 -#include "ice_eswitch.h"
 -#include "ice_virtchnl_allowlist.h"
 -#include "ice_flex_pipe.h"
 -#include "ice_vf_vsi_vlan_ops.h"
 -#include "ice_vlan.h"
 -
 -#define FIELD_SELECTOR(proto_hdr_field) \
 -		BIT((proto_hdr_field) & PROTO_HDR_FIELD_MASK)
 -
 -struct ice_vc_hdr_match_type {
 -	u32 vc_hdr;	/* virtchnl headers (VIRTCHNL_PROTO_HDR_XXX) */
 -	u32 ice_hdr;	/* ice headers (ICE_FLOW_SEG_HDR_XXX) */
 -};
 -
 -static const struct ice_vc_hdr_match_type ice_vc_hdr_list[] = {
 -	{VIRTCHNL_PROTO_HDR_NONE,	ICE_FLOW_SEG_HDR_NONE},
 -	{VIRTCHNL_PROTO_HDR_ETH,	ICE_FLOW_SEG_HDR_ETH},
 -	{VIRTCHNL_PROTO_HDR_S_VLAN,	ICE_FLOW_SEG_HDR_VLAN},
 -	{VIRTCHNL_PROTO_HDR_C_VLAN,	ICE_FLOW_SEG_HDR_VLAN},
 -	{VIRTCHNL_PROTO_HDR_IPV4,	ICE_FLOW_SEG_HDR_IPV4 |
 -					ICE_FLOW_SEG_HDR_IPV_OTHER},
 -	{VIRTCHNL_PROTO_HDR_IPV6,	ICE_FLOW_SEG_HDR_IPV6 |
 -					ICE_FLOW_SEG_HDR_IPV_OTHER},
 -	{VIRTCHNL_PROTO_HDR_TCP,	ICE_FLOW_SEG_HDR_TCP},
 -	{VIRTCHNL_PROTO_HDR_UDP,	ICE_FLOW_SEG_HDR_UDP},
 -	{VIRTCHNL_PROTO_HDR_SCTP,	ICE_FLOW_SEG_HDR_SCTP},
 -	{VIRTCHNL_PROTO_HDR_PPPOE,	ICE_FLOW_SEG_HDR_PPPOE},
 -	{VIRTCHNL_PROTO_HDR_GTPU_IP,	ICE_FLOW_SEG_HDR_GTPU_IP},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH,	ICE_FLOW_SEG_HDR_GTPU_EH},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_DWN,
 -					ICE_FLOW_SEG_HDR_GTPU_DWN},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_UP,
 -					ICE_FLOW_SEG_HDR_GTPU_UP},
 -	{VIRTCHNL_PROTO_HDR_L2TPV3,	ICE_FLOW_SEG_HDR_L2TPV3},
 -	{VIRTCHNL_PROTO_HDR_ESP,	ICE_FLOW_SEG_HDR_ESP},
 -	{VIRTCHNL_PROTO_HDR_AH,		ICE_FLOW_SEG_HDR_AH},
 -	{VIRTCHNL_PROTO_HDR_PFCP,	ICE_FLOW_SEG_HDR_PFCP_SESSION},
 -};
 -
 -struct ice_vc_hash_field_match_type {
 -	u32 vc_hdr;		/* virtchnl headers
 -				 * (VIRTCHNL_PROTO_HDR_XXX)
 -				 */
 -	u32 vc_hash_field;	/* virtchnl hash fields selector
 -				 * FIELD_SELECTOR((VIRTCHNL_PROTO_HDR_ETH_XXX))
 -				 */
 -	u64 ice_hash_field;	/* ice hash fields
 -				 * (BIT_ULL(ICE_FLOW_FIELD_IDX_XXX))
 -				 */
 -};
 -
 -static const struct
 -ice_vc_hash_field_match_type ice_vc_hash_field_list[] = {
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_SA)},
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_DA)},
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_DST),
 -		ICE_FLOW_HASH_ETH},
 -	{VIRTCHNL_PROTO_HDR_ETH,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_ETHERTYPE),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_TYPE)},
 -	{VIRTCHNL_PROTO_HDR_S_VLAN,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_S_VLAN_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_S_VLAN)},
 -	{VIRTCHNL_PROTO_HDR_C_VLAN,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_C_VLAN_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_C_VLAN)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_SA)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_DA)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST),
 -		ICE_FLOW_HASH_IPV4},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_SA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_DA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		ICE_FLOW_HASH_IPV4 | BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_SA)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_DA)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST),
 -		ICE_FLOW_HASH_IPV6},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_SA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_DA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		ICE_FLOW_HASH_IPV6 | BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_TCP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_TCP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_DST_PORT),
 -		ICE_FLOW_HASH_TCP_PORT},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_UDP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_UDP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_DST_PORT),
 -		ICE_FLOW_HASH_UDP_PORT},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_SCTP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_SCTP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_DST_PORT),
 -		ICE_FLOW_HASH_SCTP_PORT},
 -	{VIRTCHNL_PROTO_HDR_PPPOE,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_PPPOE_SESS_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_PPPOE_SESS_ID)},
 -	{VIRTCHNL_PROTO_HDR_GTPU_IP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_GTPU_IP_TEID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_GTPU_IP_TEID)},
 -	{VIRTCHNL_PROTO_HDR_L2TPV3,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_L2TPV3_SESS_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_L2TPV3_SESS_ID)},
 -	{VIRTCHNL_PROTO_HDR_ESP, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ESP_SPI),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ESP_SPI)},
 -	{VIRTCHNL_PROTO_HDR_AH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_AH_SPI),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_AH_SPI)},
 -	{VIRTCHNL_PROTO_HDR_PFCP, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_PFCP_SEID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_PFCP_SEID)},
 -};
 -
 -/**
 - * ice_free_vf_entries - Free all VF entries from the hash table
 - * @pf: pointer to the PF structure
 - *
 - * Iterate over the VF hash table, removing and releasing all VF entries.
 - * Called during VF teardown or as cleanup during failed VF initialization.
 - */
 -static void ice_free_vf_entries(struct ice_pf *pf)
 -{
 -	struct ice_vfs *vfs = &pf->vfs;
 -	struct hlist_node *tmp;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	/* Remove all VFs from the hash table and release their main
 -	 * reference. Once all references to the VF are dropped, ice_put_vf()
 -	 * will call ice_release_vf which will remove the VF memory.
 -	 */
 -	lockdep_assert_held(&vfs->table_lock);
 -
 -	hash_for_each_safe(vfs->table, bkt, tmp, vf, entry) {
 -		hash_del_rcu(&vf->entry);
 -		ice_put_vf(vf);
 -	}
 -}
 +#include "ice_common.h"
 +#include "ice_sriov.h"
  
  /**
 - * ice_vc_vf_broadcast - Broadcast a message to all VFs on PF
 - * @pf: pointer to the PF structure
 - * @v_opcode: operation code
 - * @v_retval: return value
 + * ice_aq_send_msg_to_vf
 + * @hw: pointer to the hardware structure
 + * @vfid: VF ID to send msg
 + * @v_opcode: opcodes for VF-PF communication
 + * @v_retval: return error code
   * @msg: pointer to the msg buffer
   * @msglen: msg length
 - */
 -static void
 -ice_vc_vf_broadcast(struct ice_pf *pf, enum virtchnl_ops v_opcode,
 -		    enum virtchnl_status_code v_retval, u8 *msg, u16 msglen)
 -{
 -	struct ice_hw *hw = &pf->hw;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	mutex_lock(&pf->vfs.table_lock);
 -	ice_for_each_vf(pf, bkt, vf) {
 -		/* Not all vfs are enabled so skip the ones that are not */
 -		if (!test_bit(ICE_VF_STATE_INIT, vf->vf_states) &&
 -		    !test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states))
 -			continue;
 -
 -		/* Ignore return value on purpose - a given VF may fail, but
 -		 * we need to keep going and send to all of them
 -		 */
 -		ice_aq_send_msg_to_vf(hw, vf->vf_id, v_opcode, v_retval, msg,
 -				      msglen, NULL);
 -	}
 -	mutex_unlock(&pf->vfs.table_lock);
 -}
 -
 -/**
 - * ice_set_pfe_link - Set the link speed/status of the virtchnl_pf_event
 - * @vf: pointer to the VF structure
 - * @pfe: pointer to the virtchnl_pf_event to set link speed/status for
 - * @ice_link_speed: link speed specified by ICE_AQ_LINK_SPEED_*
 - * @link_up: whether or not to set the link up/down
 - */
 -static void
 -ice_set_pfe_link(struct ice_vf *vf, struct virtchnl_pf_event *pfe,
 -		 int ice_link_speed, bool link_up)
 -{
 -	if (vf->driver_caps & VIRTCHNL_VF_CAP_ADV_LINK_SPEED) {
 -		pfe->event_data.link_event_adv.link_status = link_up;
 -		/* Speed in Mbps */
 -		pfe->event_data.link_event_adv.link_speed =
 -			ice_conv_link_speed_to_virtchnl(true, ice_link_speed);
 -	} else {
 -		pfe->event_data.link_event.link_status = link_up;
 -		/* Legacy method for virtchnl link speeds */
 -		pfe->event_data.link_event.link_speed =
 -			(enum virtchnl_link_speed)
 -			ice_conv_link_speed_to_virtchnl(false, ice_link_speed);
 -	}
 -}
 -
 -/**
 - * ice_vc_notify_vf_link_state - Inform a VF of link status
 - * @vf: pointer to the VF structure
 + * @cd: pointer to command details
   *
++<<<<<<< HEAD
 + * Send message to VF driver (0x0802) using mailbox
 + * queue and asynchronously sending message via
 + * ice_sq_send_cmd() function
++=======
+  * send a link status message to a single VF
+  */
+ void ice_vc_notify_vf_link_state(struct ice_vf *vf)
+ {
+ 	struct virtchnl_pf_event pfe = { 0 };
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_LINK_CHANGE;
+ 	pfe.severity = PF_EVENT_SEVERITY_INFO;
+ 
+ 	if (ice_is_vf_link_up(vf))
+ 		ice_set_pfe_link(vf, &pfe,
+ 				 hw->port_info->phy.link_info.link_speed, true);
+ 	else
+ 		ice_set_pfe_link(vf, &pfe, ICE_AQ_LINK_SPEED_UNKNOWN, false);
+ 
+ 	ice_aq_send_msg_to_vf(hw, vf->vf_id, VIRTCHNL_OP_EVENT,
+ 			      VIRTCHNL_STATUS_SUCCESS, (u8 *)&pfe,
+ 			      sizeof(pfe), NULL);
+ }
+ 
+ /**
+  * ice_vf_vsi_release - invalidate the VF's VSI after freeing it
+  * @vf: invalidate this VF's VSI after freeing it
+  */
+ static void ice_vf_vsi_release(struct ice_vf *vf)
+ {
+ 	ice_vsi_release(ice_get_vf_vsi(vf));
+ 	ice_vf_invalidate_vsi(vf);
+ }
+ 
+ /**
+  * ice_free_vf_res - Free a VF's resources
+  * @vf: pointer to the VF info
+  */
+ static void ice_free_vf_res(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	int i, last_vector_idx;
+ 
+ 	/* First, disable VF's configuration API to prevent OS from
+ 	 * accessing the VF's VSI after it's freed or invalidated.
+ 	 */
+ 	clear_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 	ice_vf_fdir_exit(vf);
+ 	/* free VF control VSI */
+ 	if (vf->ctrl_vsi_idx != ICE_NO_VSI)
+ 		ice_vf_ctrl_vsi_release(vf);
+ 
+ 	/* free VSI and disconnect it from the parent uplink */
+ 	if (vf->lan_vsi_idx != ICE_NO_VSI) {
+ 		ice_vf_vsi_release(vf);
+ 		vf->num_mac = 0;
+ 	}
+ 
+ 	last_vector_idx = vf->first_vector_idx + pf->vfs.num_msix_per - 1;
+ 
+ 	/* clear VF MDD event information */
+ 	memset(&vf->mdd_tx_events, 0, sizeof(vf->mdd_tx_events));
+ 	memset(&vf->mdd_rx_events, 0, sizeof(vf->mdd_rx_events));
+ 
+ 	/* Disable interrupts so that VF starts in a known state */
+ 	for (i = vf->first_vector_idx; i <= last_vector_idx; i++) {
+ 		wr32(&pf->hw, GLINT_DYN_CTL(i), GLINT_DYN_CTL_CLEARPBA_M);
+ 		ice_flush(&pf->hw);
+ 	}
+ 	/* reset some of the state variables keeping track of the resources */
+ 	clear_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states);
+ 	clear_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states);
+ }
+ 
+ /**
+  * ice_dis_vf_mappings
+  * @vf: pointer to the VF structure
+  */
+ static void ice_dis_vf_mappings(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	int first, last, v;
+ 	struct ice_hw *hw;
+ 
+ 	hw = &pf->hw;
+ 	vsi = ice_get_vf_vsi(vf);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	wr32(hw, VPINT_ALLOC(vf->vf_id), 0);
+ 	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), 0);
+ 
+ 	first = vf->first_vector_idx;
+ 	last = first + pf->vfs.num_msix_per - 1;
+ 	for (v = first; v <= last; v++) {
+ 		u32 reg;
+ 
+ 		reg = (((1 << GLINT_VECT2FUNC_IS_PF_S) &
+ 			GLINT_VECT2FUNC_IS_PF_M) |
+ 		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
+ 			GLINT_VECT2FUNC_PF_NUM_M));
+ 		wr32(hw, GLINT_VECT2FUNC(v), reg);
+ 	}
+ 
+ 	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG)
+ 		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), 0);
+ 	else
+ 		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
+ 
+ 	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG)
+ 		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), 0);
+ 	else
+ 		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
+ }
+ 
+ /**
+  * ice_sriov_free_msix_res - Reset/free any used MSIX resources
+  * @pf: pointer to the PF structure
+  *
+  * Since no MSIX entries are taken from the pf->irq_tracker then just clear
+  * the pf->sriov_base_vector.
+  *
+  * Returns 0 on success, and -EINVAL on error.
+  */
+ static int ice_sriov_free_msix_res(struct ice_pf *pf)
+ {
+ 	struct ice_res_tracker *res;
+ 
+ 	if (!pf)
+ 		return -EINVAL;
+ 
+ 	res = pf->irq_tracker;
+ 	if (!res)
+ 		return -EINVAL;
+ 
+ 	/* give back irq_tracker resources used */
+ 	WARN_ON(pf->sriov_base_vector < res->num_entries);
+ 
+ 	pf->sriov_base_vector = 0;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_free_vfs - Free all VFs
+  * @pf: pointer to the PF structure
+  */
+ void ice_free_vfs(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vfs *vfs = &pf->vfs;
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	if (!ice_has_vfs(pf))
+ 		return;
+ 
+ 	while (test_and_set_bit(ICE_VF_DIS, pf->state))
+ 		usleep_range(1000, 2000);
+ 
+ 	/* Disable IOV before freeing resources. This lets any VF drivers
+ 	 * running in the host get themselves cleaned up before we yank
+ 	 * the carpet out from underneath their feet.
+ 	 */
+ 	if (!pci_vfs_assigned(pf->pdev))
+ 		pci_disable_sriov(pf->pdev);
+ 	else
+ 		dev_warn(dev, "VFs are assigned - not disabling SR-IOV\n");
+ 
+ 	mutex_lock(&vfs->table_lock);
+ 
+ 	ice_eswitch_release(pf);
+ 
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		mutex_lock(&vf->cfg_lock);
+ 
+ 		ice_dis_vf_qs(vf);
+ 
+ 		if (test_bit(ICE_VF_STATE_INIT, vf->vf_states)) {
+ 			/* disable VF qp mappings and set VF disable state */
+ 			ice_dis_vf_mappings(vf);
+ 			set_bit(ICE_VF_STATE_DIS, vf->vf_states);
+ 			ice_free_vf_res(vf);
+ 		}
+ 
+ 		if (!pci_vfs_assigned(pf->pdev)) {
+ 			u32 reg_idx, bit_idx;
+ 
+ 			reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
+ 			bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
+ 			wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
+ 		}
+ 
+ 		/* clear malicious info since the VF is getting released */
+ 		if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
+ 					ICE_MAX_SRIOV_VFS, vf->vf_id))
+ 			dev_dbg(dev, "failed to clear malicious VF state for VF %u\n",
+ 				vf->vf_id);
+ 
+ 		mutex_unlock(&vf->cfg_lock);
+ 	}
+ 
+ 	if (ice_sriov_free_msix_res(pf))
+ 		dev_err(dev, "Failed to free MSIX resources used by SR-IOV\n");
+ 
+ 	vfs->num_qps_per = 0;
+ 	ice_free_vf_entries(pf);
+ 
+ 	mutex_unlock(&vfs->table_lock);
+ 
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 	clear_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
+ }
+ 
+ /**
+  * ice_vf_vsi_setup - Set up a VF VSI
+  * @vf: VF to setup VSI for
+  *
+  * Returns pointer to the successfully allocated VSI struct on success,
+  * otherwise returns NULL on failure.
+  */
+ static struct ice_vsi *ice_vf_vsi_setup(struct ice_vf *vf)
+ {
+ 	struct ice_port_info *pi = ice_vf_get_port_info(vf);
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 
+ 	vsi = ice_vsi_setup(pf, pi, ICE_VSI_VF, vf, NULL);
+ 
+ 	if (!vsi) {
+ 		dev_err(ice_pf_to_dev(pf), "Failed to create VF VSI\n");
+ 		ice_vf_invalidate_vsi(vf);
+ 		return NULL;
+ 	}
+ 
+ 	vf->lan_vsi_idx = vsi->idx;
+ 	vf->lan_vsi_num = vsi->vsi_num;
+ 
+ 	return vsi;
+ }
+ 
+ /**
+  * ice_calc_vf_first_vector_idx - Calculate MSIX vector index in the PF space
+  * @pf: pointer to PF structure
+  * @vf: pointer to VF that the first MSIX vector index is being calculated for
+  *
+  * This returns the first MSIX vector index in PF space that is used by this VF.
+  * This index is used when accessing PF relative registers such as
+  * GLINT_VECT2FUNC and GLINT_DYN_CTL.
+  * This will always be the OICR index in the AVF driver so any functionality
+  * using vf->first_vector_idx for queue configuration will have to increment by
+  * 1 to avoid meddling with the OICR index.
+  */
+ static int ice_calc_vf_first_vector_idx(struct ice_pf *pf, struct ice_vf *vf)
+ {
+ 	return pf->sriov_base_vector + vf->vf_id * pf->vfs.num_msix_per;
+ }
+ 
+ /**
+  * ice_ena_vf_msix_mappings - enable VF MSIX mappings in hardware
+  * @vf: VF to enable MSIX mappings for
+  *
+  * Some of the registers need to be indexed/configured using hardware global
+  * device values and other registers need 0-based values, which represent PF
+  * based values.
+  */
+ static void ice_ena_vf_msix_mappings(struct ice_vf *vf)
+ {
+ 	int device_based_first_msix, device_based_last_msix;
+ 	int pf_based_first_msix, pf_based_last_msix, v;
+ 	struct ice_pf *pf = vf->pf;
+ 	int device_based_vf_id;
+ 	struct ice_hw *hw;
+ 	u32 reg;
+ 
+ 	hw = &pf->hw;
+ 	pf_based_first_msix = vf->first_vector_idx;
+ 	pf_based_last_msix = (pf_based_first_msix + pf->vfs.num_msix_per) - 1;
+ 
+ 	device_based_first_msix = pf_based_first_msix +
+ 		pf->hw.func_caps.common_cap.msix_vector_first_id;
+ 	device_based_last_msix =
+ 		(device_based_first_msix + pf->vfs.num_msix_per) - 1;
+ 	device_based_vf_id = vf->vf_id + hw->func_caps.vf_base_id;
+ 
+ 	reg = (((device_based_first_msix << VPINT_ALLOC_FIRST_S) &
+ 		VPINT_ALLOC_FIRST_M) |
+ 	       ((device_based_last_msix << VPINT_ALLOC_LAST_S) &
+ 		VPINT_ALLOC_LAST_M) | VPINT_ALLOC_VALID_M);
+ 	wr32(hw, VPINT_ALLOC(vf->vf_id), reg);
+ 
+ 	reg = (((device_based_first_msix << VPINT_ALLOC_PCI_FIRST_S)
+ 		 & VPINT_ALLOC_PCI_FIRST_M) |
+ 	       ((device_based_last_msix << VPINT_ALLOC_PCI_LAST_S) &
+ 		VPINT_ALLOC_PCI_LAST_M) | VPINT_ALLOC_PCI_VALID_M);
+ 	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), reg);
+ 
+ 	/* map the interrupts to its functions */
+ 	for (v = pf_based_first_msix; v <= pf_based_last_msix; v++) {
+ 		reg = (((device_based_vf_id << GLINT_VECT2FUNC_VF_NUM_S) &
+ 			GLINT_VECT2FUNC_VF_NUM_M) |
+ 		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
+ 			GLINT_VECT2FUNC_PF_NUM_M));
+ 		wr32(hw, GLINT_VECT2FUNC(v), reg);
+ 	}
+ 
+ 	/* Map mailbox interrupt to VF MSI-X vector 0 */
+ 	wr32(hw, VPINT_MBX_CTL(device_based_vf_id), VPINT_MBX_CTL_CAUSE_ENA_M);
+ }
+ 
+ /**
+  * ice_ena_vf_q_mappings - enable Rx/Tx queue mappings for a VF
+  * @vf: VF to enable the mappings for
+  * @max_txq: max Tx queues allowed on the VF's VSI
+  * @max_rxq: max Rx queues allowed on the VF's VSI
+  */
+ static void ice_ena_vf_q_mappings(struct ice_vf *vf, u16 max_txq, u16 max_rxq)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 	u32 reg;
+ 
+ 	/* set regardless of mapping mode */
+ 	wr32(hw, VPLAN_TXQ_MAPENA(vf->vf_id), VPLAN_TXQ_MAPENA_TX_ENA_M);
+ 
+ 	/* VF Tx queues allocation */
+ 	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG) {
+ 		/* set the VF PF Tx queue range
+ 		 * VFNUMQ value should be set to (number of queues - 1). A value
+ 		 * of 0 means 1 queue and a value of 255 means 256 queues
+ 		 */
+ 		reg = (((vsi->txq_map[0] << VPLAN_TX_QBASE_VFFIRSTQ_S) &
+ 			VPLAN_TX_QBASE_VFFIRSTQ_M) |
+ 		       (((max_txq - 1) << VPLAN_TX_QBASE_VFNUMQ_S) &
+ 			VPLAN_TX_QBASE_VFNUMQ_M));
+ 		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), reg);
+ 	} else {
+ 		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
+ 	}
+ 
+ 	/* set regardless of mapping mode */
+ 	wr32(hw, VPLAN_RXQ_MAPENA(vf->vf_id), VPLAN_RXQ_MAPENA_RX_ENA_M);
+ 
+ 	/* VF Rx queues allocation */
+ 	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG) {
+ 		/* set the VF PF Rx queue range
+ 		 * VFNUMQ value should be set to (number of queues - 1). A value
+ 		 * of 0 means 1 queue and a value of 255 means 256 queues
+ 		 */
+ 		reg = (((vsi->rxq_map[0] << VPLAN_RX_QBASE_VFFIRSTQ_S) &
+ 			VPLAN_RX_QBASE_VFFIRSTQ_M) |
+ 		       (((max_rxq - 1) << VPLAN_RX_QBASE_VFNUMQ_S) &
+ 			VPLAN_RX_QBASE_VFNUMQ_M));
+ 		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), reg);
+ 	} else {
+ 		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
+ 	}
+ }
+ 
+ /**
+  * ice_ena_vf_mappings - enable VF MSIX and queue mapping
+  * @vf: pointer to the VF structure
+  */
+ static void ice_ena_vf_mappings(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_ena_vf_msix_mappings(vf);
+ 	ice_ena_vf_q_mappings(vf, vsi->alloc_txq, vsi->alloc_rxq);
+ }
+ 
+ /**
+  * ice_calc_vf_reg_idx - Calculate the VF's register index in the PF space
+  * @vf: VF to calculate the register index for
+  * @q_vector: a q_vector associated to the VF
+  */
+ int ice_calc_vf_reg_idx(struct ice_vf *vf, struct ice_q_vector *q_vector)
+ {
+ 	struct ice_pf *pf;
+ 
+ 	if (!vf || !q_vector)
+ 		return -EINVAL;
+ 
+ 	pf = vf->pf;
+ 
+ 	/* always add one to account for the OICR being the first MSIX */
+ 	return pf->sriov_base_vector + pf->vfs.num_msix_per * vf->vf_id +
+ 		q_vector->v_idx + 1;
+ }
+ 
+ /**
+  * ice_get_max_valid_res_idx - Get the max valid resource index
+  * @res: pointer to the resource to find the max valid index for
+  *
+  * Start from the end of the ice_res_tracker and return right when we find the
+  * first res->list entry with the ICE_RES_VALID_BIT set. This function is only
+  * valid for SR-IOV because it is the only consumer that manipulates the
+  * res->end and this is always called when res->end is set to res->num_entries.
+  */
+ static int ice_get_max_valid_res_idx(struct ice_res_tracker *res)
+ {
+ 	int i;
+ 
+ 	if (!res)
+ 		return -EINVAL;
+ 
+ 	for (i = res->num_entries - 1; i >= 0; i--)
+ 		if (res->list[i] & ICE_RES_VALID_BIT)
+ 			return i;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_set_msix_res - Set any used MSIX resources
+  * @pf: pointer to PF structure
+  * @num_msix_needed: number of MSIX vectors needed for all SR-IOV VFs
+  *
+  * This function allows SR-IOV resources to be taken from the end of the PF's
+  * allowed HW MSIX vectors so that the irq_tracker will not be affected. We
+  * just set the pf->sriov_base_vector and return success.
+  *
+  * If there are not enough resources available, return an error. This should
+  * always be caught by ice_set_per_vf_res().
+  *
+  * Return 0 on success, and -EINVAL when there are not enough MSIX vectors
+  * in the PF's space available for SR-IOV.
+  */
+ static int ice_sriov_set_msix_res(struct ice_pf *pf, u16 num_msix_needed)
+ {
+ 	u16 total_vectors = pf->hw.func_caps.common_cap.num_msix_vectors;
+ 	int vectors_used = pf->irq_tracker->num_entries;
+ 	int sriov_base_vector;
+ 
+ 	sriov_base_vector = total_vectors - num_msix_needed;
+ 
+ 	/* make sure we only grab irq_tracker entries from the list end and
+ 	 * that we have enough available MSIX vectors
+ 	 */
+ 	if (sriov_base_vector < vectors_used)
+ 		return -EINVAL;
+ 
+ 	pf->sriov_base_vector = sriov_base_vector;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_set_per_vf_res - check if vectors and queues are available
+  * @pf: pointer to the PF structure
+  * @num_vfs: the number of SR-IOV VFs being configured
+  *
+  * First, determine HW interrupts from common pool. If we allocate fewer VFs, we
+  * get more vectors and can enable more queues per VF. Note that this does not
+  * grab any vectors from the SW pool already allocated. Also note, that all
+  * vector counts include one for each VF's miscellaneous interrupt vector
+  * (i.e. OICR).
+  *
+  * Minimum VFs - 2 vectors, 1 queue pair
+  * Small VFs - 5 vectors, 4 queue pairs
+  * Medium VFs - 17 vectors, 16 queue pairs
+  *
+  * Second, determine number of queue pairs per VF by starting with a pre-defined
+  * maximum each VF supports. If this is not possible, then we adjust based on
+  * queue pairs available on the device.
+  *
+  * Lastly, set queue and MSI-X VF variables tracked by the PF so it can be used
+  * by each VF during VF initialization and reset.
+  */
+ static int ice_set_per_vf_res(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	int max_valid_res_idx = ice_get_max_valid_res_idx(pf->irq_tracker);
+ 	u16 num_msix_per_vf, num_txq, num_rxq, avail_qs;
+ 	int msix_avail_per_vf, msix_avail_for_sriov;
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	lockdep_assert_held(&pf->vfs.table_lock);
+ 
+ 	if (!num_vfs)
+ 		return -EINVAL;
+ 
+ 	if (max_valid_res_idx < 0)
+ 		return -ENOSPC;
+ 
+ 	/* determine MSI-X resources per VF */
+ 	msix_avail_for_sriov = pf->hw.func_caps.common_cap.num_msix_vectors -
+ 		pf->irq_tracker->num_entries;
+ 	msix_avail_per_vf = msix_avail_for_sriov / num_vfs;
+ 	if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MED) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_MED;
+ 	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_SMALL) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_SMALL;
+ 	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MULTIQ_MIN) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_MULTIQ_MIN;
+ 	} else if (msix_avail_per_vf >= ICE_MIN_INTR_PER_VF) {
+ 		num_msix_per_vf = ICE_MIN_INTR_PER_VF;
+ 	} else {
+ 		dev_err(dev, "Only %d MSI-X interrupts available for SR-IOV. Not enough to support minimum of %d MSI-X interrupts per VF for %d VFs\n",
+ 			msix_avail_for_sriov, ICE_MIN_INTR_PER_VF,
+ 			num_vfs);
+ 		return -ENOSPC;
+ 	}
+ 
+ 	num_txq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
+ 			ICE_MAX_RSS_QS_PER_VF);
+ 	avail_qs = ice_get_avail_txq_count(pf) / num_vfs;
+ 	if (!avail_qs)
+ 		num_txq = 0;
+ 	else if (num_txq > avail_qs)
+ 		num_txq = rounddown_pow_of_two(avail_qs);
+ 
+ 	num_rxq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
+ 			ICE_MAX_RSS_QS_PER_VF);
+ 	avail_qs = ice_get_avail_rxq_count(pf) / num_vfs;
+ 	if (!avail_qs)
+ 		num_rxq = 0;
+ 	else if (num_rxq > avail_qs)
+ 		num_rxq = rounddown_pow_of_two(avail_qs);
+ 
+ 	if (num_txq < ICE_MIN_QS_PER_VF || num_rxq < ICE_MIN_QS_PER_VF) {
+ 		dev_err(dev, "Not enough queues to support minimum of %d queue pairs per VF for %d VFs\n",
+ 			ICE_MIN_QS_PER_VF, num_vfs);
+ 		return -ENOSPC;
+ 	}
+ 
+ 	err = ice_sriov_set_msix_res(pf, num_msix_per_vf * num_vfs);
+ 	if (err) {
+ 		dev_err(dev, "Unable to set MSI-X resources for %d VFs, err %d\n",
+ 			num_vfs, err);
+ 		return err;
+ 	}
+ 
+ 	/* only allow equal Tx/Rx queue count (i.e. queue pairs) */
+ 	pf->vfs.num_qps_per = min_t(int, num_txq, num_rxq);
+ 	pf->vfs.num_msix_per = num_msix_per_vf;
+ 	dev_info(dev, "Enabling %d VFs with %d vectors and %d queues per VF\n",
+ 		 num_vfs, pf->vfs.num_msix_per, pf->vfs.num_qps_per);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_vc_notify_link_state - Inform all VFs on a PF of link status
+  * @pf: pointer to the PF structure
+  */
+ void ice_vc_notify_link_state(struct ice_pf *pf)
+ {
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 	ice_for_each_vf(pf, bkt, vf)
+ 		ice_vc_notify_vf_link_state(vf);
+ 	mutex_unlock(&pf->vfs.table_lock);
+ }
+ 
+ /**
+  * ice_vc_notify_reset - Send pending reset message to all VFs
+  * @pf: pointer to the PF structure
+  *
+  * indicate a pending reset to all VFs on a given PF
+  */
+ void ice_vc_notify_reset(struct ice_pf *pf)
+ {
+ 	struct virtchnl_pf_event pfe;
+ 
+ 	if (!ice_has_vfs(pf))
+ 		return;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
+ 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
+ 	ice_vc_vf_broadcast(pf, VIRTCHNL_OP_EVENT, VIRTCHNL_STATUS_SUCCESS,
+ 			    (u8 *)&pfe, sizeof(struct virtchnl_pf_event));
+ }
+ 
+ /**
+  * ice_vc_notify_vf_reset - Notify VF of a reset event
+  * @vf: pointer to the VF structure
+  */
+ static void ice_vc_notify_vf_reset(struct ice_vf *vf)
+ {
+ 	struct virtchnl_pf_event pfe;
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	/* Bail out if VF is in disabled state, neither initialized, nor active
+ 	 * state - otherwise proceed with notifications
+ 	 */
+ 	if ((!test_bit(ICE_VF_STATE_INIT, vf->vf_states) &&
+ 	     !test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) ||
+ 	    test_bit(ICE_VF_STATE_DIS, vf->vf_states))
+ 		return;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
+ 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
+ 	ice_aq_send_msg_to_vf(&pf->hw, vf->vf_id, VIRTCHNL_OP_EVENT,
+ 			      VIRTCHNL_STATUS_SUCCESS, (u8 *)&pfe, sizeof(pfe),
+ 			      NULL);
+ }
+ 
+ /**
+  * ice_init_vf_vsi_res - initialize/setup VF VSI resources
+  * @vf: VF to initialize/setup the VSI for
+  *
+  * This function creates a VSI for the VF, adds a VLAN 0 filter, and sets up the
+  * VF VSI's broadcast filter and is only used during initial VF creation.
+  */
+ static int ice_init_vf_vsi_res(struct ice_vf *vf)
+ {
+ 	struct ice_vsi_vlan_ops *vlan_ops;
+ 	struct ice_pf *pf = vf->pf;
+ 	u8 broadcast[ETH_ALEN];
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	int err;
+ 
+ 	vf->first_vector_idx = ice_calc_vf_first_vector_idx(pf, vf);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	vsi = ice_vf_vsi_setup(vf);
+ 	if (!vsi)
+ 		return -ENOMEM;
+ 
+ 	err = ice_vsi_add_vlan_zero(vsi);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to add VLAN 0 filter for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
+ 	err = vlan_ops->ena_rx_filtering(vsi);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to enable Rx VLAN filtering for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	eth_broadcast_addr(broadcast);
+ 	err = ice_fltr_add_mac(vsi, broadcast, ICE_FWD_TO_VSI);
+ 	if (err) {
+ 		dev_err(dev, "Failed to add broadcast MAC filter for VF %d, error %d\n",
+ 			vf->vf_id, err);
+ 		goto release_vsi;
+ 	}
+ 
+ 	err = ice_vsi_apply_spoofchk(vsi, vf->spoofchk);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to initialize spoofchk setting for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	vf->num_mac = 1;
+ 
+ 	return 0;
+ 
+ release_vsi:
+ 	ice_vf_vsi_release(vf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_start_vfs - start VFs so they are ready to be used by SR-IOV
+  * @pf: PF the VFs are associated with
+  */
+ static int ice_start_vfs(struct ice_pf *pf)
+ {
+ 	struct ice_hw *hw = &pf->hw;
+ 	unsigned int bkt, it_cnt;
+ 	struct ice_vf *vf;
+ 	int retval;
+ 
+ 	lockdep_assert_held(&pf->vfs.table_lock);
+ 
+ 	it_cnt = 0;
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		vf->vf_ops->clear_reset_trigger(vf);
+ 
+ 		retval = ice_init_vf_vsi_res(vf);
+ 		if (retval) {
+ 			dev_err(ice_pf_to_dev(pf), "Failed to initialize VSI resources for VF %d, error %d\n",
+ 				vf->vf_id, retval);
+ 			goto teardown;
+ 		}
+ 
+ 		set_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 		ice_ena_vf_mappings(vf);
+ 		wr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
+ 		it_cnt++;
+ 	}
+ 
+ 	ice_flush(hw);
+ 	return 0;
+ 
+ teardown:
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		if (it_cnt == 0)
+ 			break;
+ 
+ 		ice_dis_vf_mappings(vf);
+ 		ice_vf_vsi_release(vf);
+ 		it_cnt--;
+ 	}
+ 
+ 	return retval;
+ }
+ 
+ /**
+  * ice_sriov_free_vf - Free VF memory after all references are dropped
+  * @vf: pointer to VF to free
+  *
+  * Called by ice_put_vf through ice_release_vf once the last reference to a VF
+  * structure has been dropped.
+  */
+ static void ice_sriov_free_vf(struct ice_vf *vf)
+ {
+ 	mutex_destroy(&vf->cfg_lock);
+ 
+ 	kfree_rcu(vf, rcu);
+ }
+ 
+ /**
+  * ice_sriov_clear_mbx_register - clears SRIOV VF's mailbox registers
+  * @vf: the vf to configure
+  */
+ static void ice_sriov_clear_mbx_register(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	wr32(&pf->hw, VF_MBX_ARQLEN(vf->vf_id), 0);
+ 	wr32(&pf->hw, VF_MBX_ATQLEN(vf->vf_id), 0);
+ }
+ 
+ /**
+  * ice_sriov_trigger_reset_register - trigger VF reset for SRIOV VF
+  * @vf: pointer to VF structure
+  * @is_vflr: true if reset occurred due to VFLR
+  *
+  * Trigger and cleanup after a VF reset for a SR-IOV VF.
+  */
+ static void ice_sriov_trigger_reset_register(struct ice_vf *vf, bool is_vflr)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	u32 reg, reg_idx, bit_idx;
+ 	unsigned int vf_abs_id, i;
+ 	struct device *dev;
+ 	struct ice_hw *hw;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	hw = &pf->hw;
+ 	vf_abs_id = vf->vf_id + hw->func_caps.vf_base_id;
+ 
+ 	/* In the case of a VFLR, HW has already reset the VF and we just need
+ 	 * to clean up. Otherwise we must first trigger the reset using the
+ 	 * VFRTRIG register.
+ 	 */
+ 	if (!is_vflr) {
+ 		reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
+ 		reg |= VPGEN_VFRTRIG_VFSWR_M;
+ 		wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
+ 	}
+ 
+ 	/* clear the VFLR bit in GLGEN_VFLRSTAT */
+ 	reg_idx = (vf_abs_id) / 32;
+ 	bit_idx = (vf_abs_id) % 32;
+ 	wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
+ 	ice_flush(hw);
+ 
+ 	wr32(hw, PF_PCI_CIAA,
+ 	     VF_DEVICE_STATUS | (vf_abs_id << PF_PCI_CIAA_VF_NUM_S));
+ 	for (i = 0; i < ICE_PCI_CIAD_WAIT_COUNT; i++) {
+ 		reg = rd32(hw, PF_PCI_CIAD);
+ 		/* no transactions pending so stop polling */
+ 		if ((reg & VF_TRANS_PENDING_M) == 0)
+ 			break;
+ 
+ 		dev_err(dev, "VF %u PCI transactions stuck\n", vf->vf_id);
+ 		udelay(ICE_PCI_CIAD_WAIT_DELAY_US);
+ 	}
+ }
+ 
+ /**
+  * ice_sriov_poll_reset_status - poll SRIOV VF reset status
+  * @vf: pointer to VF structure
+  *
+  * Returns true when reset is successful, else returns false
+  */
+ static bool ice_sriov_poll_reset_status(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	unsigned int i;
+ 	u32 reg;
+ 
+ 	for (i = 0; i < 10; i++) {
+ 		/* VF reset requires driver to first reset the VF and then
+ 		 * poll the status register to make sure that the reset
+ 		 * completed successfully.
+ 		 */
+ 		reg = rd32(&pf->hw, VPGEN_VFRSTAT(vf->vf_id));
+ 		if (reg & VPGEN_VFRSTAT_VFRD_M)
+ 			return true;
+ 
+ 		/* only sleep if the reset is not done */
+ 		usleep_range(10, 20);
+ 	}
+ 	return false;
+ }
+ 
+ /**
+  * ice_sriov_clear_reset_trigger - enable VF to access hardware
+  * @vf: VF to enabled hardware access for
+  */
+ static void ice_sriov_clear_reset_trigger(struct ice_vf *vf)
+ {
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 	u32 reg;
+ 
+ 	reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
+ 	reg &= ~VPGEN_VFRTRIG_VFSWR_M;
+ 	wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
+ 	ice_flush(hw);
+ }
+ 
+ /**
+  * ice_sriov_vsi_rebuild - release and rebuild VF's VSI
+  * @vf: VF to release and setup the VSI for
+  *
+  * This is only called when a single VF is being reset (i.e. VFR, VFLR, host VF
+  * configuration change, etc.).
+  */
+ static int ice_sriov_vsi_rebuild(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	ice_vf_vsi_release(vf);
+ 	if (!ice_vf_vsi_setup(vf)) {
+ 		dev_err(ice_pf_to_dev(pf),
+ 			"Failed to release and setup the VF%u's VSI\n",
+ 			vf->vf_id);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_post_vsi_rebuild - tasks to do after the VF's VSI have been rebuilt
+  * @vf: VF to perform tasks on
+  */
+ static void ice_sriov_post_vsi_rebuild(struct ice_vf *vf)
+ {
+ 	ice_vf_rebuild_host_cfg(vf);
+ 	ice_vf_set_initialized(vf);
+ 	ice_ena_vf_mappings(vf);
+ 	wr32(&vf->pf->hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
+ }
+ 
+ static const struct ice_vf_ops ice_sriov_vf_ops = {
+ 	.reset_type = ICE_VF_RESET,
+ 	.free = ice_sriov_free_vf,
+ 	.clear_mbx_register = ice_sriov_clear_mbx_register,
+ 	.trigger_reset_register = ice_sriov_trigger_reset_register,
+ 	.poll_reset_status = ice_sriov_poll_reset_status,
+ 	.clear_reset_trigger = ice_sriov_clear_reset_trigger,
+ 	.vsi_rebuild = ice_sriov_vsi_rebuild,
+ 	.post_vsi_rebuild = ice_sriov_post_vsi_rebuild,
+ };
+ 
+ /**
+  * ice_create_vf_entries - Allocate and insert VF entries
+  * @pf: pointer to the PF structure
+  * @num_vfs: the number of VFs to allocate
+  *
+  * Allocate new VF entries and insert them into the hash table. Set some
+  * basic default fields for initializing the new VFs.
+  *
+  * After this function exits, the hash table will have num_vfs entries
+  * inserted.
+  *
+  * Returns 0 on success or an integer error code on failure.
+  */
+ static int ice_create_vf_entries(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	struct ice_vfs *vfs = &pf->vfs;
+ 	struct ice_vf *vf;
+ 	u16 vf_id;
+ 	int err;
+ 
+ 	lockdep_assert_held(&vfs->table_lock);
+ 
+ 	for (vf_id = 0; vf_id < num_vfs; vf_id++) {
+ 		vf = kzalloc(sizeof(*vf), GFP_KERNEL);
+ 		if (!vf) {
+ 			err = -ENOMEM;
+ 			goto err_free_entries;
+ 		}
+ 		kref_init(&vf->refcnt);
+ 
+ 		vf->pf = pf;
+ 		vf->vf_id = vf_id;
+ 
+ 		/* set sriov vf ops for VFs created during SRIOV flow */
+ 		vf->vf_ops = &ice_sriov_vf_ops;
+ 
+ 		vf->vf_sw_id = pf->first_sw;
+ 		/* assign default capabilities */
+ 		vf->spoofchk = true;
+ 		vf->num_vf_qs = pf->vfs.num_qps_per;
+ 		ice_vc_set_default_allowlist(vf);
+ 
+ 		/* ctrl_vsi_idx will be set to a valid value only when VF
+ 		 * creates its first fdir rule.
+ 		 */
+ 		ice_vf_ctrl_invalidate_vsi(vf);
+ 		ice_vf_fdir_init(vf);
+ 
+ 		ice_virtchnl_set_dflt_ops(vf);
+ 
+ 		mutex_init(&vf->cfg_lock);
+ 
+ 		hash_add_rcu(vfs->table, &vf->entry, vf_id);
+ 	}
+ 
+ 	return 0;
+ 
+ err_free_entries:
+ 	ice_free_vf_entries(pf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_ena_vfs - enable VFs so they are ready to be used
+  * @pf: pointer to the PF structure
+  * @num_vfs: number of VFs to enable
+  */
+ static int ice_ena_vfs(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_hw *hw = &pf->hw;
+ 	int ret;
+ 
+ 	/* Disable global interrupt 0 so we don't try to handle the VFLR. */
+ 	wr32(hw, GLINT_DYN_CTL(pf->oicr_idx),
+ 	     ICE_ITR_NONE << GLINT_DYN_CTL_ITR_INDX_S);
+ 	set_bit(ICE_OICR_INTR_DIS, pf->state);
+ 	ice_flush(hw);
+ 
+ 	ret = pci_enable_sriov(pf->pdev, num_vfs);
+ 	if (ret)
+ 		goto err_unroll_intr;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 
+ 	ret = ice_set_per_vf_res(pf, num_vfs);
+ 	if (ret) {
+ 		dev_err(dev, "Not enough resources for %d VFs, err %d. Try with fewer number of VFs\n",
+ 			num_vfs, ret);
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	ret = ice_create_vf_entries(pf, num_vfs);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to allocate VF entries for %d VFs\n",
+ 			num_vfs);
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	ret = ice_start_vfs(pf);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to start %d VFs, err %d\n", num_vfs, ret);
+ 		ret = -EAGAIN;
+ 		goto err_unroll_vf_entries;
+ 	}
+ 
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 
+ 	ret = ice_eswitch_configure(pf);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to configure eswitch, err %d\n", ret);
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	/* rearm global interrupts */
+ 	if (test_and_clear_bit(ICE_OICR_INTR_DIS, pf->state))
+ 		ice_irq_dynamic_ena(hw, NULL, NULL);
+ 
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 
+ 	return 0;
+ 
+ err_unroll_vf_entries:
+ 	ice_free_vf_entries(pf);
+ err_unroll_sriov:
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 	pci_disable_sriov(pf->pdev);
+ err_unroll_intr:
+ 	/* rearm interrupts here */
+ 	ice_irq_dynamic_ena(hw, NULL, NULL);
+ 	clear_bit(ICE_OICR_INTR_DIS, pf->state);
+ 	return ret;
+ }
+ 
+ /**
+  * ice_pci_sriov_ena - Enable or change number of VFs
+  * @pf: pointer to the PF structure
+  * @num_vfs: number of VFs to allocate
+  *
+  * Returns 0 on success and negative on failure
+  */
+ static int ice_pci_sriov_ena(struct ice_pf *pf, int num_vfs)
+ {
+ 	int pre_existing_vfs = pci_num_vf(pf->pdev);
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	if (pre_existing_vfs && pre_existing_vfs != num_vfs)
+ 		ice_free_vfs(pf);
+ 	else if (pre_existing_vfs && pre_existing_vfs == num_vfs)
+ 		return 0;
+ 
+ 	if (num_vfs > pf->vfs.num_supported) {
+ 		dev_err(dev, "Can't enable %d VFs, max VFs supported is %d\n",
+ 			num_vfs, pf->vfs.num_supported);
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	dev_info(dev, "Enabling %d VFs\n", num_vfs);
+ 	err = ice_ena_vfs(pf, num_vfs);
+ 	if (err) {
+ 		dev_err(dev, "Failed to enable SR-IOV: %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	set_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
+ 	return 0;
+ }
+ 
+ /**
+  * ice_check_sriov_allowed - check if SR-IOV is allowed based on various checks
+  * @pf: PF to enabled SR-IOV on
+  */
+ static int ice_check_sriov_allowed(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	if (!test_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags)) {
+ 		dev_err(dev, "This device is not capable of SR-IOV\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (ice_is_safe_mode(pf)) {
+ 		dev_err(dev, "SR-IOV cannot be configured - Device is in Safe Mode\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (!ice_pf_state_is_nominal(pf)) {
+ 		dev_err(dev, "Cannot enable SR-IOV, device not ready\n");
+ 		return -EBUSY;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_configure - Enable or change number of VFs via sysfs
+  * @pdev: pointer to a pci_dev structure
+  * @num_vfs: number of VFs to allocate or 0 to free VFs
+  *
+  * This function is called when the user updates the number of VFs in sysfs. On
+  * success return whatever num_vfs was set to by the caller. Return negative on
+  * failure.
+  */
+ int ice_sriov_configure(struct pci_dev *pdev, int num_vfs)
+ {
+ 	struct ice_pf *pf = pci_get_drvdata(pdev);
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	err = ice_check_sriov_allowed(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	if (!num_vfs) {
+ 		if (!pci_vfs_assigned(pdev)) {
+ 			ice_mbx_deinit_snapshot(&pf->hw);
+ 			ice_free_vfs(pf);
+ 			if (pf->lag)
+ 				ice_enable_lag(pf->lag);
+ 			return 0;
+ 		}
+ 
+ 		dev_err(dev, "can't free VFs because some are assigned to VMs.\n");
+ 		return -EBUSY;
+ 	}
+ 
+ 	err = ice_mbx_init_snapshot(&pf->hw, num_vfs);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ice_pci_sriov_ena(pf, num_vfs);
+ 	if (err) {
+ 		ice_mbx_deinit_snapshot(&pf->hw);
+ 		return err;
+ 	}
+ 
+ 	if (pf->lag)
+ 		ice_disable_lag(pf->lag);
+ 	return num_vfs;
+ }
+ 
+ /**
+  * ice_process_vflr_event - Free VF resources via IRQ calls
+  * @pf: pointer to the PF structure
+  *
+  * called from the VFLR IRQ handler to
+  * free up VF resources and state variables
+  */
+ void ice_process_vflr_event(struct ice_pf *pf)
+ {
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 	u32 reg;
+ 
+ 	if (!test_and_clear_bit(ICE_VFLR_EVENT_PENDING, pf->state) ||
+ 	    !ice_has_vfs(pf))
+ 		return;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		u32 reg_idx, bit_idx;
+ 
+ 		reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
+ 		bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
+ 		/* read GLGEN_VFLRSTAT register to find out the flr VFs */
+ 		reg = rd32(hw, GLGEN_VFLRSTAT(reg_idx));
+ 		if (reg & BIT(bit_idx)) {
+ 			/* GLGEN_VFLRSTAT bit will be cleared in ice_reset_vf */
+ 			mutex_lock(&vf->cfg_lock);
+ 			ice_reset_vf(vf, ICE_VF_RESET_VFLR);
+ 			mutex_unlock(&vf->cfg_lock);
+ 		}
+ 	}
+ 	mutex_unlock(&pf->vfs.table_lock);
+ }
+ 
+ /**
+  * ice_vc_reset_vf - Perform software reset on the VF after informing the AVF
+  * @vf: pointer to the VF info
+  */
+ static void ice_vc_reset_vf(struct ice_vf *vf)
+ {
+ 	ice_vc_notify_vf_reset(vf);
+ 	ice_reset_vf(vf, 0);
+ }
+ 
+ /**
+  * ice_get_vf_from_pfq - get the VF who owns the PF space queue passed in
+  * @pf: PF used to index all VFs
+  * @pfq: queue index relative to the PF's function space
+  *
+  * If no VF is found who owns the pfq then return NULL, otherwise return a
+  * pointer to the VF who owns the pfq
+  *
+  * If this function returns non-NULL, it acquires a reference count of the VF
+  * structure. The caller is responsible for calling ice_put_vf() to drop this
+  * reference.
+  */
+ static struct ice_vf *ice_get_vf_from_pfq(struct ice_pf *pf, u16 pfq)
+ {
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	rcu_read_lock();
+ 	ice_for_each_vf_rcu(pf, bkt, vf) {
+ 		struct ice_vsi *vsi;
+ 		u16 rxq_idx;
+ 
+ 		vsi = ice_get_vf_vsi(vf);
+ 
+ 		ice_for_each_rxq(vsi, rxq_idx)
+ 			if (vsi->rxq_map[rxq_idx] == pfq) {
+ 				struct ice_vf *found;
+ 
+ 				if (kref_get_unless_zero(&vf->refcnt))
+ 					found = vf;
+ 				else
+ 					found = NULL;
+ 				rcu_read_unlock();
+ 				return found;
+ 			}
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	return NULL;
+ }
+ 
+ /**
+  * ice_globalq_to_pfq - convert from global queue index to PF space queue index
+  * @pf: PF used for conversion
+  * @globalq: global queue index used to convert to PF space queue index
+  */
+ static u32 ice_globalq_to_pfq(struct ice_pf *pf, u32 globalq)
+ {
+ 	return globalq - pf->hw.func_caps.common_cap.rxq_first_id;
+ }
+ 
+ /**
+  * ice_vf_lan_overflow_event - handle LAN overflow event for a VF
+  * @pf: PF that the LAN overflow event happened on
+  * @event: structure holding the event information for the LAN overflow event
+  *
+  * Determine if the LAN overflow event was caused by a VF queue. If it was not
+  * caused by a VF, do nothing. If a VF caused this LAN overflow event trigger a
+  * reset on the offending VF.
+  */
+ void
+ ice_vf_lan_overflow_event(struct ice_pf *pf, struct ice_rq_event_info *event)
+ {
+ 	u32 gldcb_rtctq, queue;
+ 	struct ice_vf *vf;
+ 
+ 	gldcb_rtctq = le32_to_cpu(event->desc.params.lan_overflow.prtdcb_ruptq);
+ 	dev_dbg(ice_pf_to_dev(pf), "GLDCB_RTCTQ: 0x%08x\n", gldcb_rtctq);
+ 
+ 	/* event returns device global Rx queue number */
+ 	queue = (gldcb_rtctq & GLDCB_RTCTQ_RXQNUM_M) >>
+ 		GLDCB_RTCTQ_RXQNUM_S;
+ 
+ 	vf = ice_get_vf_from_pfq(pf, ice_globalq_to_pfq(pf, queue));
+ 	if (!vf)
+ 		return;
+ 
+ 	mutex_lock(&vf->cfg_lock);
+ 	ice_vc_reset_vf(vf);
+ 	mutex_unlock(&vf->cfg_lock);
+ 
+ 	ice_put_vf(vf);
+ }
+ 
+ /**
+  * ice_vc_send_msg_to_vf - Send message to VF
+  * @vf: pointer to the VF info
+  * @v_opcode: virtual channel opcode
+  * @v_retval: virtual channel return value
+  * @msg: pointer to the msg buffer
+  * @msglen: msg length
+  *
+  * send msg to VF
++>>>>>>> 7eb517e434c6 (ice: convert ice_reset_vf to take flags)
   */
  int
 -ice_vc_send_msg_to_vf(struct ice_vf *vf, u32 v_opcode,
 -		      enum virtchnl_status_code v_retval, u8 *msg, u16 msglen)
 +ice_aq_send_msg_to_vf(struct ice_hw *hw, u16 vfid, u32 v_opcode, u32 v_retval,
 +		      u8 *msg, u16 msglen, struct ice_sq_cd *cd)
  {
 -	struct device *dev;
 -	struct ice_pf *pf;
 -	int aq_ret;
 +	struct ice_aqc_pf_vf_msg *cmd;
 +	struct ice_aq_desc desc;
  
 -	pf = vf->pf;
 -	dev = ice_pf_to_dev(pf);
 +	ice_fill_dflt_direct_cmd_desc(&desc, ice_mbx_opc_send_msg_to_vf);
  
 -	aq_ret = ice_aq_send_msg_to_vf(&pf->hw, vf->vf_id, v_opcode, v_retval,
 -				       msg, msglen, NULL);
 -	if (aq_ret && pf->hw.mailboxq.sq_last_status != ICE_AQ_RC_ENOSYS) {
 -		dev_info(dev, "Unable to send the message to VF %d ret %d aq_err %s\n",
 -			 vf->vf_id, aq_ret,
 -			 ice_aq_str(pf->hw.mailboxq.sq_last_status));
 -		return -EIO;
 -	}
 +	cmd = &desc.params.virt;
 +	cmd->id = cpu_to_le32(vfid);
  
 -	return 0;
 +	desc.cookie_high = cpu_to_le32(v_opcode);
 +	desc.cookie_low = cpu_to_le32(v_retval);
 +
 +	if (msglen)
 +		desc.flags |= cpu_to_le16(ICE_AQ_FLAG_RD);
 +
 +	return ice_sq_send_cmd(hw, &hw->mailboxq, &desc, msg, msglen, cd);
  }
  
  /**
 - * ice_vc_get_ver_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 + * ice_conv_link_speed_to_virtchnl
 + * @adv_link_support: determines the format of the returned link speed
 + * @link_speed: variable containing the link_speed to be converted
   *
 - * called from the VF to request the API version used by the PF
 + * Convert link speed supported by HW to link speed supported by virtchnl.
 + * If adv_link_support is true, then return link speed in Mbps. Else return
 + * link speed as a VIRTCHNL_LINK_SPEED_* casted to a u32. Note that the caller
 + * needs to cast back to an enum virtchnl_link_speed in the case where
 + * adv_link_support is false, but when adv_link_support is true the caller can
 + * expect the speed in Mbps.
   */
 -static int ice_vc_get_ver_msg(struct ice_vf *vf, u8 *msg)
 +u32 ice_conv_link_speed_to_virtchnl(bool adv_link_support, u16 link_speed)
  {
 -	struct virtchnl_version_info info = {
 -		VIRTCHNL_VERSION_MAJOR, VIRTCHNL_VERSION_MINOR
 -	};
 +	u32 speed;
  
++<<<<<<< HEAD
 +	if (adv_link_support)
 +		switch (link_speed) {
 +		case ICE_AQ_LINK_SPEED_10MB:
 +			speed = ICE_LINK_SPEED_10MBPS;
++=======
+ 	vf->vf_ver = *(struct virtchnl_version_info *)msg;
+ 	/* VFs running the 1.0 API expect to get 1.0 back or they will cry. */
+ 	if (VF_IS_V10(&vf->vf_ver))
+ 		info.minor = VIRTCHNL_VERSION_MINOR_NO_VF_CAPS;
+ 
+ 	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_VERSION,
+ 				     VIRTCHNL_STATUS_SUCCESS, (u8 *)&info,
+ 				     sizeof(struct virtchnl_version_info));
+ }
+ 
+ /**
+  * ice_vc_get_max_frame_size - get max frame size allowed for VF
+  * @vf: VF used to determine max frame size
+  *
+  * Max frame size is determined based on the current port's max frame size and
+  * whether a port VLAN is configured on this VF. The VF is not aware whether
+  * it's in a port VLAN so the PF needs to account for this in max frame size
+  * checks and sending the max frame size to the VF.
+  */
+ static u16 ice_vc_get_max_frame_size(struct ice_vf *vf)
+ {
+ 	struct ice_port_info *pi = ice_vf_get_port_info(vf);
+ 	u16 max_frame_size;
+ 
+ 	max_frame_size = pi->phy.link_info.max_frame_size;
+ 
+ 	if (ice_vf_is_port_vlan_ena(vf))
+ 		max_frame_size -= VLAN_HLEN;
+ 
+ 	return max_frame_size;
+ }
+ 
+ /**
+  * ice_vc_get_vf_res_msg
+  * @vf: pointer to the VF info
+  * @msg: pointer to the msg buffer
+  *
+  * called from the VF to request its resources
+  */
+ static int ice_vc_get_vf_res_msg(struct ice_vf *vf, u8 *msg)
+ {
+ 	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
+ 	struct virtchnl_vf_resource *vfres = NULL;
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 	int len = 0;
+ 	int ret;
+ 
+ 	if (ice_check_vf_init(pf, vf)) {
+ 		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
+ 		goto err;
+ 	}
+ 
+ 	len = sizeof(struct virtchnl_vf_resource);
+ 
+ 	vfres = kzalloc(len, GFP_KERNEL);
+ 	if (!vfres) {
+ 		v_ret = VIRTCHNL_STATUS_ERR_NO_MEMORY;
+ 		len = 0;
+ 		goto err;
+ 	}
+ 	if (VF_IS_V11(&vf->vf_ver))
+ 		vf->driver_caps = *(u32 *)msg;
+ 	else
+ 		vf->driver_caps = VIRTCHNL_VF_OFFLOAD_L2 |
+ 				  VIRTCHNL_VF_OFFLOAD_RSS_REG |
+ 				  VIRTCHNL_VF_OFFLOAD_VLAN;
+ 
+ 	vfres->vf_cap_flags = VIRTCHNL_VF_OFFLOAD_L2;
+ 	vsi = ice_get_vf_vsi(vf);
+ 	if (!vsi) {
+ 		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
+ 		goto err;
+ 	}
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_VLAN_V2) {
+ 		/* VLAN offloads based on current device configuration */
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_VLAN_V2;
+ 	} else if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_VLAN) {
+ 		/* allow VF to negotiate VIRTCHNL_VF_OFFLOAD explicitly for
+ 		 * these two conditions, which amounts to guest VLAN filtering
+ 		 * and offloads being based on the inner VLAN or the
+ 		 * inner/single VLAN respectively and don't allow VF to
+ 		 * negotiate VIRTCHNL_VF_OFFLOAD in any other cases
+ 		 */
+ 		if (ice_is_dvm_ena(&pf->hw) && ice_vf_is_port_vlan_ena(vf)) {
+ 			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_VLAN;
+ 		} else if (!ice_is_dvm_ena(&pf->hw) &&
+ 			   !ice_vf_is_port_vlan_ena(vf)) {
+ 			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_VLAN;
+ 			/* configure backward compatible support for VFs that
+ 			 * only support VIRTCHNL_VF_OFFLOAD_VLAN, the PF is
+ 			 * configured in SVM, and no port VLAN is configured
+ 			 */
+ 			ice_vf_vsi_cfg_svm_legacy_vlan_mode(vsi);
+ 		} else if (ice_is_dvm_ena(&pf->hw)) {
+ 			/* configure software offloaded VLAN support when DVM
+ 			 * is enabled, but no port VLAN is enabled
+ 			 */
+ 			ice_vf_vsi_cfg_dvm_legacy_vlan_mode(vsi);
+ 		}
+ 	}
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_PF) {
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_PF;
+ 	} else {
+ 		if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_AQ)
+ 			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_AQ;
+ 		else
+ 			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_REG;
+ 	}
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_FDIR_PF)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_FDIR_PF;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_PCTYPE_V2)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_PCTYPE_V2;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_ENCAP)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_ENCAP;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_ENCAP_CSUM)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_ENCAP_CSUM;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RX_POLLING)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RX_POLLING;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_WB_ON_ITR)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_WB_ON_ITR;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_REQ_QUEUES)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_REQ_QUEUES;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_CAP_ADV_LINK_SPEED)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_CAP_ADV_LINK_SPEED;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_ADV_RSS_PF)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_ADV_RSS_PF;
+ 
+ 	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_USO)
+ 		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_USO;
+ 
+ 	vfres->num_vsis = 1;
+ 	/* Tx and Rx queue are equal for VF */
+ 	vfres->num_queue_pairs = vsi->num_txq;
+ 	vfres->max_vectors = pf->vfs.num_msix_per;
+ 	vfres->rss_key_size = ICE_VSIQF_HKEY_ARRAY_SIZE;
+ 	vfres->rss_lut_size = ICE_VSIQF_HLUT_ARRAY_SIZE;
+ 	vfres->max_mtu = ice_vc_get_max_frame_size(vf);
+ 
+ 	vfres->vsi_res[0].vsi_id = vf->lan_vsi_num;
+ 	vfres->vsi_res[0].vsi_type = VIRTCHNL_VSI_SRIOV;
+ 	vfres->vsi_res[0].num_queue_pairs = vsi->num_txq;
+ 	ether_addr_copy(vfres->vsi_res[0].default_mac_addr,
+ 			vf->hw_lan_addr.addr);
+ 
+ 	/* match guest capabilities */
+ 	vf->driver_caps = vfres->vf_cap_flags;
+ 
+ 	ice_vc_set_caps_allowlist(vf);
+ 	ice_vc_set_working_allowlist(vf);
+ 
+ 	set_bit(ICE_VF_STATE_ACTIVE, vf->vf_states);
+ 
+ err:
+ 	/* send the response back to the VF */
+ 	ret = ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_GET_VF_RESOURCES, v_ret,
+ 				    (u8 *)vfres, len);
+ 
+ 	kfree(vfres);
+ 	return ret;
+ }
+ 
+ /**
+  * ice_vc_reset_vf_msg
+  * @vf: pointer to the VF info
+  *
+  * called from the VF to reset itself,
+  * unlike other virtchnl messages, PF driver
+  * doesn't send the response back to the VF
+  */
+ static void ice_vc_reset_vf_msg(struct ice_vf *vf)
+ {
+ 	if (test_bit(ICE_VF_STATE_INIT, vf->vf_states))
+ 		ice_reset_vf(vf, 0);
+ }
+ 
+ /**
+  * ice_find_vsi_from_id
+  * @pf: the PF structure to search for the VSI
+  * @id: ID of the VSI it is searching for
+  *
+  * searches for the VSI with the given ID
+  */
+ static struct ice_vsi *ice_find_vsi_from_id(struct ice_pf *pf, u16 id)
+ {
+ 	int i;
+ 
+ 	ice_for_each_vsi(pf, i)
+ 		if (pf->vsi[i] && pf->vsi[i]->vsi_num == id)
+ 			return pf->vsi[i];
+ 
+ 	return NULL;
+ }
+ 
+ /**
+  * ice_vc_isvalid_vsi_id
+  * @vf: pointer to the VF info
+  * @vsi_id: VF relative VSI ID
+  *
+  * check for the valid VSI ID
+  */
+ bool ice_vc_isvalid_vsi_id(struct ice_vf *vf, u16 vsi_id)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 
+ 	vsi = ice_find_vsi_from_id(pf, vsi_id);
+ 
+ 	return (vsi && (vsi->vf == vf));
+ }
+ 
+ /**
+  * ice_vc_isvalid_q_id
+  * @vf: pointer to the VF info
+  * @vsi_id: VSI ID
+  * @qid: VSI relative queue ID
+  *
+  * check for the valid queue ID
+  */
+ static bool ice_vc_isvalid_q_id(struct ice_vf *vf, u16 vsi_id, u8 qid)
+ {
+ 	struct ice_vsi *vsi = ice_find_vsi_from_id(vf->pf, vsi_id);
+ 	/* allocated Tx and Rx queues should be always equal for VF VSI */
+ 	return (vsi && (qid < vsi->alloc_txq));
+ }
+ 
+ /**
+  * ice_vc_isvalid_ring_len
+  * @ring_len: length of ring
+  *
+  * check for the valid ring count, should be multiple of ICE_REQ_DESC_MULTIPLE
+  * or zero
+  */
+ static bool ice_vc_isvalid_ring_len(u16 ring_len)
+ {
+ 	return ring_len == 0 ||
+ 	       (ring_len >= ICE_MIN_NUM_DESC &&
+ 		ring_len <= ICE_MAX_NUM_DESC &&
+ 		!(ring_len % ICE_REQ_DESC_MULTIPLE));
+ }
+ 
+ /**
+  * ice_vc_validate_pattern
+  * @vf: pointer to the VF info
+  * @proto: virtchnl protocol headers
+  *
+  * validate the pattern is supported or not.
+  *
+  * Return: true on success, false on error.
+  */
+ bool
+ ice_vc_validate_pattern(struct ice_vf *vf, struct virtchnl_proto_hdrs *proto)
+ {
+ 	bool is_ipv4 = false;
+ 	bool is_ipv6 = false;
+ 	bool is_udp = false;
+ 	u16 ptype = -1;
+ 	int i = 0;
+ 
+ 	while (i < proto->count &&
+ 	       proto->proto_hdr[i].type != VIRTCHNL_PROTO_HDR_NONE) {
+ 		switch (proto->proto_hdr[i].type) {
+ 		case VIRTCHNL_PROTO_HDR_ETH:
+ 			ptype = ICE_PTYPE_MAC_PAY;
++>>>>>>> 7eb517e434c6 (ice: convert ice_reset_vf to take flags)
 +			break;
 +		case ICE_AQ_LINK_SPEED_100MB:
 +			speed = ICE_LINK_SPEED_100MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_1000MB:
 +			speed = ICE_LINK_SPEED_1000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_2500MB:
 +			speed = ICE_LINK_SPEED_2500MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_5GB:
 +			speed = ICE_LINK_SPEED_5000MBPS;
  			break;
 -		case VIRTCHNL_PROTO_HDR_IPV4:
 -			ptype = ICE_PTYPE_IPV4_PAY;
 -			is_ipv4 = true;
 +		case ICE_AQ_LINK_SPEED_10GB:
 +			speed = ICE_LINK_SPEED_10000MBPS;
  			break;
 -		case VIRTCHNL_PROTO_HDR_IPV6:
 -			ptype = ICE_PTYPE_IPV6_PAY;
 -			is_ipv6 = true;
 +		case ICE_AQ_LINK_SPEED_20GB:
 +			speed = ICE_LINK_SPEED_20000MBPS;
  			break;
 -		case VIRTCHNL_PROTO_HDR_UDP:
 -			if (is_ipv4)
 -				ptype = ICE_PTYPE_IPV4_UDP_PAY;
 -			else if (is_ipv6)
 -				ptype = ICE_PTYPE_IPV6_UDP_PAY;
 -			is_udp = true;
 +		case ICE_AQ_LINK_SPEED_25GB:
 +			speed = ICE_LINK_SPEED_25000MBPS;
  			break;
 -		case VIRTCHNL_PROTO_HDR_TCP:
 -			if (is_ipv4)
 -				ptype = ICE_PTYPE_IPV4_TCP_PAY;
 -			else if (is_ipv6)
 -				ptype = ICE_PTYPE_IPV6_TCP_PAY;
 +		case ICE_AQ_LINK_SPEED_40GB:
 +			speed = ICE_LINK_SPEED_40000MBPS;
  			break;
 -		case VIRTCHNL_PROTO_HDR_SCTP:
 -			if (is_ipv4)
 -				ptype = ICE_PTYPE_IPV4_SCTP_PAY;
 -			else if (is_ipv6)
 -				ptype = ICE_PTYPE_IPV6_SCTP_PAY;
 +		case ICE_AQ_LINK_SPEED_50GB:
 +			speed = ICE_LINK_SPEED_50000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_100GB:
 +			speed = ICE_LINK_SPEED_100000MBPS;
  			break;
 -		case VIRTCHNL_PROTO_HDR_GTPU_IP:
 -		case VIRTCHNL_PROTO_HDR_GTPU_EH:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_GTPU;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_GTPU;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_L2TPV3:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_L2TPV3;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_L2TPV3;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_ESP:
 -			if (is_ipv4)
 -				ptype = is_udp ? ICE_MAC_IPV4_NAT_T_ESP :
 -						ICE_MAC_IPV4_ESP;
 -			else if (is_ipv6)
 -				ptype = is_udp ? ICE_MAC_IPV6_NAT_T_ESP :
 -						ICE_MAC_IPV6_ESP;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_AH:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_AH;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_AH;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_PFCP:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_PFCP_SESSION;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_PFCP_SESSION;
 -			goto out;
  		default:
 +			speed = ICE_LINK_SPEED_UNKNOWN;
 +			break;
 +		}
 +	else
 +		/* Virtchnl speeds are not defined for every speed supported in
 +		 * the hardware. To maintain compatibility with older AVF
 +		 * drivers, while reporting the speed the new speed values are
 +		 * resolved to the closest known virtchnl speeds
 +		 */
 +		switch (link_speed) {
 +		case ICE_AQ_LINK_SPEED_10MB:
 +		case ICE_AQ_LINK_SPEED_100MB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_100MB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_1000MB:
 +		case ICE_AQ_LINK_SPEED_2500MB:
 +		case ICE_AQ_LINK_SPEED_5GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_1GB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_10GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_10GB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_20GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_20GB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_25GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_25GB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_40GB:
 +		case ICE_AQ_LINK_SPEED_50GB:
 +		case ICE_AQ_LINK_SPEED_100GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_40GB;
 +			break;
 +		default:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_UNKNOWN;
  			break;
  		}
 -		i++;
 -	}
  
 -out:
 -	return ice_hw_ptype_ena(&vf->pf->hw, ptype);
 +	return speed;
  }
  
 -/**
 - * ice_vc_parse_rss_cfg - parses hash fields and headers from
 - * a specific virtchnl RSS cfg
 - * @hw: pointer to the hardware
 - * @rss_cfg: pointer to the virtchnl RSS cfg
 - * @addl_hdrs: pointer to the protocol header fields (ICE_FLOW_SEG_HDR_*)
 - * to configure
 - * @hash_flds: pointer to the hash bit fields (ICE_FLOW_HASH_*) to configure
 +/* The mailbox overflow detection algorithm helps to check if there
 + * is a possibility of a malicious VF transmitting too many MBX messages to the
 + * PF.
 + * 1. The mailbox snapshot structure, ice_mbx_snapshot, is initialized during
 + * driver initialization in ice_init_hw() using ice_mbx_init_snapshot().
 + * The struct ice_mbx_snapshot helps to track and traverse a static window of
 + * messages within the mailbox queue while looking for a malicious VF.
 + *
 + * 2. When the caller starts processing its mailbox queue in response to an
 + * interrupt, the structure ice_mbx_snapshot is expected to be cleared before
 + * the algorithm can be run for the first time for that interrupt. This can be
 + * done via ice_mbx_reset_snapshot().
 + *
 + * 3. For every message read by the caller from the MBX Queue, the caller must
 + * call the detection algorithm's entry function ice_mbx_vf_state_handler().
 + * Before every call to ice_mbx_vf_state_handler() the struct ice_mbx_data is
 + * filled as it is required to be passed to the algorithm.
   *
 - * Return true if all the protocol header and hash fields in the RSS cfg could
 - * be parsed, else return false
 + * 4. Every time a message is read from the MBX queue, a VFId is received which
 + * is passed to the state handler. The boolean output is_malvf of the state
 + * handler ice_mbx_vf_state_handler() serves as an indicator to the caller
 + * whether this VF is malicious or not.
   *
 - * This function parses the virtchnl RSS cfg to be the intended
 - * hash fields and the intended header for RSS configuration
 + * 5. When a VF is identified to be malicious, the caller can send a message
 + * to the system administrator. The caller can invoke ice_mbx_report_malvf()
 + * to help determine if a malicious VF is to be reported or not. This function
 + * requires the caller to maintain a global bitmap to track all malicious VFs
 + * and pass that to ice_mbx_report_malvf() along with the VFID which was identified
 + * to be malicious by ice_mbx_vf_state_handler().
 + *
 + * 6. The global bitmap maintained by PF can be cleared completely if PF is in
 + * reset or the bit corresponding to a VF can be cleared if that VF is in reset.
 + * When a VF is shut down and brought back up, we assume that the new VF
 + * brought up is not malicious and hence report it if found malicious.
 + *
 + * 7. The function ice_mbx_reset_snapshot() is called to reset the information
 + * in ice_mbx_snapshot for every new mailbox interrupt handled.
 + *
 + * 8. The memory allocated for variables in ice_mbx_snapshot is de-allocated
 + * when driver is unloaded.
   */
 -static bool
 -ice_vc_parse_rss_cfg(struct ice_hw *hw, struct virtchnl_rss_cfg *rss_cfg,
 -		     u32 *addl_hdrs, u64 *hash_flds)
 -{
 -	const struct ice_vc_hash_field_match_type *hf_list;
 -	const struct ice_vc_hdr_match_type *hdr_list;
 -	int i, hf_list_len, hdr_list_len;
 -
 -	hf_list = ice_vc_hash_field_list;
 -	hf_list_len = ARRAY_SIZE(ice_vc_hash_field_list);
 -	hdr_list = ice_vc_hdr_list;
 -	hdr_list_len = ARRAY_SIZE(ice_vc_hdr_list);
 -
 -	for (i = 0; i < rss_cfg->proto_hdrs.count; i++) {
 -		struct virtchnl_proto_hdr *proto_hdr =
 -					&rss_cfg->proto_hdrs.proto_hdr[i];
 -		bool hdr_found = false;
 -		int j;
 -
 -		/* Find matched ice headers according to virtchnl headers. */
 -		for (j = 0; j < hdr_list_len; j++) {
 -			struct ice_vc_hdr_match_type hdr_map = hdr_list[j];
 -
 -			if (proto_hdr->type == hdr_map.vc_hdr) {
 -				*addl_hdrs |= hdr_map.ice_hdr;
 -				hdr_found = true;
 -			}
 -		}
 -
 -		if (!hdr_found)
 -			return false;
 +#define ICE_RQ_DATA_MASK(rq_data) ((rq_data) & PF_MBX_ARQH_ARQH_M)
 +/* Using the highest value for an unsigned 16-bit value 0xFFFF to indicate that
 + * the max messages check must be ignored in the algorithm
 + */
 +#define ICE_IGNORE_MAX_MSG_CNT	0xFFFF
  
 -		/* Find matched ice hash fields according to
 -		 * virtchnl hash fields.
 -		 */
 -		for (j = 0; j < hf_list_len; j++) {
 -			struct ice_vc_hash_field_match_type hf_map = hf_list[j];
 +/**
 + * ice_mbx_traverse - Pass through mailbox snapshot
 + * @hw: pointer to the HW struct
 + * @new_state: new algorithm state
 + *
 + * Traversing the mailbox static snapshot without checking
 + * for malicious VFs.
 + */
 +static void
 +ice_mbx_traverse(struct ice_hw *hw,
 +		 enum ice_mbx_snapshot_state *new_state)
 +{
 +	struct ice_mbx_snap_buffer_data *snap_buf;
 +	u32 num_iterations;
  
 -			if (proto_hdr->type == hf_map.vc_hdr &&
 -			    proto_hdr->field_selector == hf_map.vc_hash_field) {
 -				*hash_flds |= hf_map.ice_hash_field;
 -				break;
 -			}
 -		}
 -	}
 +	snap_buf = &hw->mbx_snapshot.mbx_buf;
  
 -	return true;
 +	/* As mailbox buffer is circular, applying a mask
 +	 * on the incremented iteration count.
 +	 */
 +	num_iterations = ICE_RQ_DATA_MASK(++snap_buf->num_iterations);
 +
 +	/* Checking either of the below conditions to exit snapshot traversal:
 +	 * Condition-1: If the number of iterations in the mailbox is equal to
 +	 * the mailbox head which would indicate that we have reached the end
 +	 * of the static snapshot.
 +	 * Condition-2: If the maximum messages serviced in the mailbox for a
 +	 * given interrupt is the highest possible value then there is no need
 +	 * to check if the number of messages processed is equal to it. If not
 +	 * check if the number of messages processed is greater than or equal
 +	 * to the maximum number of mailbox entries serviced in current work item.
 +	 */
 +	if (num_iterations == snap_buf->head ||
 +	    (snap_buf->max_num_msgs_mbx < ICE_IGNORE_MAX_MSG_CNT &&
 +	     ++snap_buf->num_msg_proc >= snap_buf->max_num_msgs_mbx))
 +		*new_state = ICE_MAL_VF_DETECT_STATE_NEW_SNAPSHOT;
  }
  
  /**
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.h
* Unmerged path drivers/net/ethernet/intel/ice/ice_main.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_sriov.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.h
