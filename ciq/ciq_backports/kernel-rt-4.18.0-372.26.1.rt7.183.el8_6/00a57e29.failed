ice: remove unused definitions from ice_sriov.h

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-372.26.1.rt7.183.el8_6
commit-author Jacob Keller <jacob.e.keller@intel.com>
commit 00a57e2959bd74e87a0e21dce746762d2f9c1987
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-372.26.1.rt7.183.el8_6/00a57e29.failed

A few more macros exist in ice_sriov.h which are not used anywhere.
These can be safely removed. Note that ICE_VIRTCHNL_VF_CAP_L2 capability
is set but never checked anywhere in the driver. Thus it is also safe to
remove.

	Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
	Tested-by: Konrad Jankowski <konrad0.jankowski@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 00a57e2959bd74e87a0e21dce746762d2f9c1987)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_sriov.c
#	drivers/net/ethernet/intel/ice/ice_sriov.h
diff --cc drivers/net/ethernet/intel/ice/ice_sriov.c
index 52c6bac41bf7,205d7e5003d8..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_sriov.c
+++ b/drivers/net/ethernet/intel/ice/ice_sriov.c
@@@ -1,42 -1,2379 +1,2017 @@@
  // SPDX-License-Identifier: GPL-2.0
  /* Copyright (c) 2018, Intel Corporation. */
  
 -#include "ice.h"
 -#include "ice_base.h"
 -#include "ice_lib.h"
 -#include "ice_fltr.h"
 -#include "ice_dcb_lib.h"
 -#include "ice_flow.h"
 -#include "ice_eswitch.h"
 -#include "ice_virtchnl_allowlist.h"
 -#include "ice_flex_pipe.h"
 -#include "ice_vf_vsi_vlan_ops.h"
 -#include "ice_vlan.h"
 -
 -#define FIELD_SELECTOR(proto_hdr_field) \
 -		BIT((proto_hdr_field) & PROTO_HDR_FIELD_MASK)
 -
 -struct ice_vc_hdr_match_type {
 -	u32 vc_hdr;	/* virtchnl headers (VIRTCHNL_PROTO_HDR_XXX) */
 -	u32 ice_hdr;	/* ice headers (ICE_FLOW_SEG_HDR_XXX) */
 -};
 -
 -static const struct ice_vc_hdr_match_type ice_vc_hdr_list[] = {
 -	{VIRTCHNL_PROTO_HDR_NONE,	ICE_FLOW_SEG_HDR_NONE},
 -	{VIRTCHNL_PROTO_HDR_ETH,	ICE_FLOW_SEG_HDR_ETH},
 -	{VIRTCHNL_PROTO_HDR_S_VLAN,	ICE_FLOW_SEG_HDR_VLAN},
 -	{VIRTCHNL_PROTO_HDR_C_VLAN,	ICE_FLOW_SEG_HDR_VLAN},
 -	{VIRTCHNL_PROTO_HDR_IPV4,	ICE_FLOW_SEG_HDR_IPV4 |
 -					ICE_FLOW_SEG_HDR_IPV_OTHER},
 -	{VIRTCHNL_PROTO_HDR_IPV6,	ICE_FLOW_SEG_HDR_IPV6 |
 -					ICE_FLOW_SEG_HDR_IPV_OTHER},
 -	{VIRTCHNL_PROTO_HDR_TCP,	ICE_FLOW_SEG_HDR_TCP},
 -	{VIRTCHNL_PROTO_HDR_UDP,	ICE_FLOW_SEG_HDR_UDP},
 -	{VIRTCHNL_PROTO_HDR_SCTP,	ICE_FLOW_SEG_HDR_SCTP},
 -	{VIRTCHNL_PROTO_HDR_PPPOE,	ICE_FLOW_SEG_HDR_PPPOE},
 -	{VIRTCHNL_PROTO_HDR_GTPU_IP,	ICE_FLOW_SEG_HDR_GTPU_IP},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH,	ICE_FLOW_SEG_HDR_GTPU_EH},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_DWN,
 -					ICE_FLOW_SEG_HDR_GTPU_DWN},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_UP,
 -					ICE_FLOW_SEG_HDR_GTPU_UP},
 -	{VIRTCHNL_PROTO_HDR_L2TPV3,	ICE_FLOW_SEG_HDR_L2TPV3},
 -	{VIRTCHNL_PROTO_HDR_ESP,	ICE_FLOW_SEG_HDR_ESP},
 -	{VIRTCHNL_PROTO_HDR_AH,		ICE_FLOW_SEG_HDR_AH},
 -	{VIRTCHNL_PROTO_HDR_PFCP,	ICE_FLOW_SEG_HDR_PFCP_SESSION},
 -};
 -
 -struct ice_vc_hash_field_match_type {
 -	u32 vc_hdr;		/* virtchnl headers
 -				 * (VIRTCHNL_PROTO_HDR_XXX)
 -				 */
 -	u32 vc_hash_field;	/* virtchnl hash fields selector
 -				 * FIELD_SELECTOR((VIRTCHNL_PROTO_HDR_ETH_XXX))
 -				 */
 -	u64 ice_hash_field;	/* ice hash fields
 -				 * (BIT_ULL(ICE_FLOW_FIELD_IDX_XXX))
 -				 */
 -};
 -
 -static const struct
 -ice_vc_hash_field_match_type ice_vc_hash_field_list[] = {
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_SA)},
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_DA)},
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_DST),
 -		ICE_FLOW_HASH_ETH},
 -	{VIRTCHNL_PROTO_HDR_ETH,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_ETHERTYPE),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_TYPE)},
 -	{VIRTCHNL_PROTO_HDR_S_VLAN,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_S_VLAN_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_S_VLAN)},
 -	{VIRTCHNL_PROTO_HDR_C_VLAN,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_C_VLAN_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_C_VLAN)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_SA)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_DA)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST),
 -		ICE_FLOW_HASH_IPV4},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_SA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_DA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		ICE_FLOW_HASH_IPV4 | BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_SA)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_DA)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST),
 -		ICE_FLOW_HASH_IPV6},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_SA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_DA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		ICE_FLOW_HASH_IPV6 | BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_TCP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_TCP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_DST_PORT),
 -		ICE_FLOW_HASH_TCP_PORT},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_UDP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_UDP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_DST_PORT),
 -		ICE_FLOW_HASH_UDP_PORT},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_SCTP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_SCTP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_DST_PORT),
 -		ICE_FLOW_HASH_SCTP_PORT},
 -	{VIRTCHNL_PROTO_HDR_PPPOE,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_PPPOE_SESS_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_PPPOE_SESS_ID)},
 -	{VIRTCHNL_PROTO_HDR_GTPU_IP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_GTPU_IP_TEID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_GTPU_IP_TEID)},
 -	{VIRTCHNL_PROTO_HDR_L2TPV3,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_L2TPV3_SESS_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_L2TPV3_SESS_ID)},
 -	{VIRTCHNL_PROTO_HDR_ESP, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ESP_SPI),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ESP_SPI)},
 -	{VIRTCHNL_PROTO_HDR_AH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_AH_SPI),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_AH_SPI)},
 -	{VIRTCHNL_PROTO_HDR_PFCP, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_PFCP_SEID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_PFCP_SEID)},
 -};
 +#include "ice_common.h"
 +#include "ice_sriov.h"
  
  /**
 - * ice_get_vf_vsi - get VF's VSI based on the stored index
 - * @vf: VF used to get VSI
 - */
 -struct ice_vsi *ice_get_vf_vsi(struct ice_vf *vf)
 -{
 -	return vf->pf->vsi[vf->lan_vsi_idx];
 -}
 -
 -/**
 - * ice_get_vf_by_id - Get pointer to VF by ID
 - * @pf: the PF private structure
 - * @vf_id: the VF ID to locate
 - *
 - * Locate and return a pointer to the VF structure associated with a given ID.
 - * Returns NULL if the ID does not have a valid VF structure associated with
 - * it.
 - *
 - * This function takes a reference to the VF, which must be released by
 - * calling ice_put_vf() once the caller is finished accessing the VF structure
 - * returned.
 - */
 -struct ice_vf *ice_get_vf_by_id(struct ice_pf *pf, u16 vf_id)
 -{
 -	struct ice_vf *vf;
 -
 -	rcu_read_lock();
 -	hash_for_each_possible_rcu(pf->vfs.table, vf, entry, vf_id) {
 -		if (vf->vf_id == vf_id) {
 -			struct ice_vf *found;
 -
 -			if (kref_get_unless_zero(&vf->refcnt))
 -				found = vf;
 -			else
 -				found = NULL;
 -
 -			rcu_read_unlock();
 -			return found;
 -		}
 -	}
 -	rcu_read_unlock();
 -
 -	return NULL;
 -}
 -
 -/**
 - * ice_release_vf - Release VF associated with a refcount
 - * @ref: the kref decremented to zero
 - *
 - * Callback function for kref_put to release a VF once its reference count has
 - * hit zero.
 - */
 -static void ice_release_vf(struct kref *ref)
 -{
 -	struct ice_vf *vf = container_of(ref, struct ice_vf, refcnt);
 -
 -	mutex_destroy(&vf->cfg_lock);
 -
 -	kfree_rcu(vf, rcu);
 -}
 -
 -/**
 - * ice_put_vf - Release a reference to a VF
 - * @vf: the VF structure to decrease reference count on
 - *
 - * This must be called after ice_get_vf_by_id() once the reference to the VF
 - * structure is no longer used. Otherwise, the VF structure will never be
 - * freed.
 - */
 -void ice_put_vf(struct ice_vf *vf)
 -{
 -	kref_put(&vf->refcnt, ice_release_vf);
 -}
 -
 -/**
 - * ice_has_vfs - Return true if the PF has any associated VFs
 - * @pf: the PF private structure
 - *
 - * Return whether or not the PF has any allocated VFs.
 - *
 - * Note that this function only guarantees that there are no VFs at the point
 - * of calling it. It does not guarantee that no more VFs will be added.
 - */
 -bool ice_has_vfs(struct ice_pf *pf)
 -{
 -	/* A simple check that the hash table is not empty does not require
 -	 * the mutex or rcu_read_lock.
 -	 */
 -	return !hash_empty(pf->vfs.table);
 -}
 -
 -/**
 - * ice_get_num_vfs - Get number of allocated VFs
 - * @pf: the PF private structure
 - *
 - * Return the total number of allocated VFs. NOTE: VF IDs are not guaranteed
 - * to be contiguous. Do not assume that a VF ID is guaranteed to be less than
 - * the output of this function.
 - */
 -u16 ice_get_num_vfs(struct ice_pf *pf)
 -{
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -	u16 num_vfs = 0;
 -
 -	rcu_read_lock();
 -	ice_for_each_vf_rcu(pf, bkt, vf)
 -		num_vfs++;
 -	rcu_read_unlock();
 -
 -	return num_vfs;
 -}
 -
 -/**
 - * ice_check_vf_init - helper to check if VF init complete
 - * @pf: pointer to the PF structure
 - * @vf: the pointer to the VF to check
 - */
 -static int ice_check_vf_init(struct ice_pf *pf, struct ice_vf *vf)
 -{
 -	if (!test_bit(ICE_VF_STATE_INIT, vf->vf_states)) {
 -		dev_err(ice_pf_to_dev(pf), "VF ID: %u in reset. Try again.\n",
 -			vf->vf_id);
 -		return -EBUSY;
 -	}
 -	return 0;
 -}
 -
 -/**
 - * ice_free_vf_entries - Free all VF entries from the hash table
 - * @pf: pointer to the PF structure
 - *
 - * Iterate over the VF hash table, removing and releasing all VF entries.
 - * Called during VF teardown or as cleanup during failed VF initialization.
 - */
 -static void ice_free_vf_entries(struct ice_pf *pf)
 -{
 -	struct ice_vfs *vfs = &pf->vfs;
 -	struct hlist_node *tmp;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	/* Remove all VFs from the hash table and release their main
 -	 * reference. Once all references to the VF are dropped, ice_put_vf()
 -	 * will call ice_release_vf which will remove the VF memory.
 -	 */
 -	lockdep_assert_held(&vfs->table_lock);
 -
 -	hash_for_each_safe(vfs->table, bkt, tmp, vf, entry) {
 -		hash_del_rcu(&vf->entry);
 -		ice_put_vf(vf);
 -	}
 -}
 -
 -/**
 - * ice_vc_vf_broadcast - Broadcast a message to all VFs on PF
 - * @pf: pointer to the PF structure
 - * @v_opcode: operation code
 - * @v_retval: return value
 + * ice_aq_send_msg_to_vf
 + * @hw: pointer to the hardware structure
 + * @vfid: VF ID to send msg
 + * @v_opcode: opcodes for VF-PF communication
 + * @v_retval: return error code
   * @msg: pointer to the msg buffer
   * @msglen: msg length
 - */
 -static void
 -ice_vc_vf_broadcast(struct ice_pf *pf, enum virtchnl_ops v_opcode,
 -		    enum virtchnl_status_code v_retval, u8 *msg, u16 msglen)
 -{
 -	struct ice_hw *hw = &pf->hw;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	mutex_lock(&pf->vfs.table_lock);
 -	ice_for_each_vf(pf, bkt, vf) {
 -		/* Not all vfs are enabled so skip the ones that are not */
 -		if (!test_bit(ICE_VF_STATE_INIT, vf->vf_states) &&
 -		    !test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states))
 -			continue;
 -
 -		/* Ignore return value on purpose - a given VF may fail, but
 -		 * we need to keep going and send to all of them
 -		 */
 -		ice_aq_send_msg_to_vf(hw, vf->vf_id, v_opcode, v_retval, msg,
 -				      msglen, NULL);
 -	}
 -	mutex_unlock(&pf->vfs.table_lock);
 -}
 -
 -/**
 - * ice_set_pfe_link - Set the link speed/status of the virtchnl_pf_event
 - * @vf: pointer to the VF structure
 - * @pfe: pointer to the virtchnl_pf_event to set link speed/status for
 - * @ice_link_speed: link speed specified by ICE_AQ_LINK_SPEED_*
 - * @link_up: whether or not to set the link up/down
 - */
 -static void
 -ice_set_pfe_link(struct ice_vf *vf, struct virtchnl_pf_event *pfe,
 -		 int ice_link_speed, bool link_up)
 -{
 -	if (vf->driver_caps & VIRTCHNL_VF_CAP_ADV_LINK_SPEED) {
 -		pfe->event_data.link_event_adv.link_status = link_up;
 -		/* Speed in Mbps */
 -		pfe->event_data.link_event_adv.link_speed =
 -			ice_conv_link_speed_to_virtchnl(true, ice_link_speed);
 -	} else {
 -		pfe->event_data.link_event.link_status = link_up;
 -		/* Legacy method for virtchnl link speeds */
 -		pfe->event_data.link_event.link_speed =
 -			(enum virtchnl_link_speed)
 -			ice_conv_link_speed_to_virtchnl(false, ice_link_speed);
 -	}
 -}
 -
 -/**
 - * ice_vf_has_no_qs_ena - check if the VF has any Rx or Tx queues enabled
 - * @vf: the VF to check
 + * @cd: pointer to command details
   *
++<<<<<<< HEAD
 + * Send message to VF driver (0x0802) using mailbox
 + * queue and asynchronously sending message via
 + * ice_sq_send_cmd() function
++=======
+  * Returns true if the VF has no Rx and no Tx queues enabled and returns false
+  * otherwise
+  */
+ static bool ice_vf_has_no_qs_ena(struct ice_vf *vf)
+ {
+ 	return (!bitmap_weight(vf->rxq_ena, ICE_MAX_RSS_QS_PER_VF) &&
+ 		!bitmap_weight(vf->txq_ena, ICE_MAX_RSS_QS_PER_VF));
+ }
+ 
+ /**
+  * ice_is_vf_link_up - check if the VF's link is up
+  * @vf: VF to check if link is up
+  */
+ static bool ice_is_vf_link_up(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	if (ice_check_vf_init(pf, vf))
+ 		return false;
+ 
+ 	if (ice_vf_has_no_qs_ena(vf))
+ 		return false;
+ 	else if (vf->link_forced)
+ 		return vf->link_up;
+ 	else
+ 		return pf->hw.port_info->phy.link_info.link_info &
+ 			ICE_AQ_LINK_UP;
+ }
+ 
+ /**
+  * ice_vc_notify_vf_link_state - Inform a VF of link status
+  * @vf: pointer to the VF structure
+  *
+  * send a link status message to a single VF
+  */
+ void ice_vc_notify_vf_link_state(struct ice_vf *vf)
+ {
+ 	struct virtchnl_pf_event pfe = { 0 };
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_LINK_CHANGE;
+ 	pfe.severity = PF_EVENT_SEVERITY_INFO;
+ 
+ 	if (ice_is_vf_link_up(vf))
+ 		ice_set_pfe_link(vf, &pfe,
+ 				 hw->port_info->phy.link_info.link_speed, true);
+ 	else
+ 		ice_set_pfe_link(vf, &pfe, ICE_AQ_LINK_SPEED_UNKNOWN, false);
+ 
+ 	ice_aq_send_msg_to_vf(hw, vf->vf_id, VIRTCHNL_OP_EVENT,
+ 			      VIRTCHNL_STATUS_SUCCESS, (u8 *)&pfe,
+ 			      sizeof(pfe), NULL);
+ }
+ 
+ /**
+  * ice_vf_invalidate_vsi - invalidate vsi_idx/vsi_num to remove VSI access
+  * @vf: VF to remove access to VSI for
+  */
+ static void ice_vf_invalidate_vsi(struct ice_vf *vf)
+ {
+ 	vf->lan_vsi_idx = ICE_NO_VSI;
+ 	vf->lan_vsi_num = ICE_NO_VSI;
+ }
+ 
+ /**
+  * ice_vf_vsi_release - invalidate the VF's VSI after freeing it
+  * @vf: invalidate this VF's VSI after freeing it
+  */
+ static void ice_vf_vsi_release(struct ice_vf *vf)
+ {
+ 	ice_vsi_release(ice_get_vf_vsi(vf));
+ 	ice_vf_invalidate_vsi(vf);
+ }
+ 
+ /**
+  * ice_vf_ctrl_invalidate_vsi - invalidate ctrl_vsi_idx to remove VSI access
+  * @vf: VF that control VSI is being invalidated on
+  */
+ static void ice_vf_ctrl_invalidate_vsi(struct ice_vf *vf)
+ {
+ 	vf->ctrl_vsi_idx = ICE_NO_VSI;
+ }
+ 
+ /**
+  * ice_vf_ctrl_vsi_release - invalidate the VF's control VSI after freeing it
+  * @vf: VF that control VSI is being released on
+  */
+ static void ice_vf_ctrl_vsi_release(struct ice_vf *vf)
+ {
+ 	ice_vsi_release(vf->pf->vsi[vf->ctrl_vsi_idx]);
+ 	ice_vf_ctrl_invalidate_vsi(vf);
+ }
+ 
+ /**
+  * ice_free_vf_res - Free a VF's resources
+  * @vf: pointer to the VF info
+  */
+ static void ice_free_vf_res(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	int i, last_vector_idx;
+ 
+ 	/* First, disable VF's configuration API to prevent OS from
+ 	 * accessing the VF's VSI after it's freed or invalidated.
+ 	 */
+ 	clear_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 	ice_vf_fdir_exit(vf);
+ 	/* free VF control VSI */
+ 	if (vf->ctrl_vsi_idx != ICE_NO_VSI)
+ 		ice_vf_ctrl_vsi_release(vf);
+ 
+ 	/* free VSI and disconnect it from the parent uplink */
+ 	if (vf->lan_vsi_idx != ICE_NO_VSI) {
+ 		ice_vf_vsi_release(vf);
+ 		vf->num_mac = 0;
+ 	}
+ 
+ 	last_vector_idx = vf->first_vector_idx + pf->vfs.num_msix_per - 1;
+ 
+ 	/* clear VF MDD event information */
+ 	memset(&vf->mdd_tx_events, 0, sizeof(vf->mdd_tx_events));
+ 	memset(&vf->mdd_rx_events, 0, sizeof(vf->mdd_rx_events));
+ 
+ 	/* Disable interrupts so that VF starts in a known state */
+ 	for (i = vf->first_vector_idx; i <= last_vector_idx; i++) {
+ 		wr32(&pf->hw, GLINT_DYN_CTL(i), GLINT_DYN_CTL_CLEARPBA_M);
+ 		ice_flush(&pf->hw);
+ 	}
+ 	/* reset some of the state variables keeping track of the resources */
+ 	clear_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states);
+ 	clear_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states);
+ }
+ 
+ /**
+  * ice_dis_vf_mappings
+  * @vf: pointer to the VF structure
+  */
+ static void ice_dis_vf_mappings(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	int first, last, v;
+ 	struct ice_hw *hw;
+ 
+ 	hw = &pf->hw;
+ 	vsi = ice_get_vf_vsi(vf);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	wr32(hw, VPINT_ALLOC(vf->vf_id), 0);
+ 	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), 0);
+ 
+ 	first = vf->first_vector_idx;
+ 	last = first + pf->vfs.num_msix_per - 1;
+ 	for (v = first; v <= last; v++) {
+ 		u32 reg;
+ 
+ 		reg = (((1 << GLINT_VECT2FUNC_IS_PF_S) &
+ 			GLINT_VECT2FUNC_IS_PF_M) |
+ 		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
+ 			GLINT_VECT2FUNC_PF_NUM_M));
+ 		wr32(hw, GLINT_VECT2FUNC(v), reg);
+ 	}
+ 
+ 	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG)
+ 		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), 0);
+ 	else
+ 		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
+ 
+ 	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG)
+ 		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), 0);
+ 	else
+ 		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
+ }
+ 
+ /**
+  * ice_sriov_free_msix_res - Reset/free any used MSIX resources
+  * @pf: pointer to the PF structure
+  *
+  * Since no MSIX entries are taken from the pf->irq_tracker then just clear
+  * the pf->sriov_base_vector.
+  *
+  * Returns 0 on success, and -EINVAL on error.
+  */
+ static int ice_sriov_free_msix_res(struct ice_pf *pf)
+ {
+ 	struct ice_res_tracker *res;
+ 
+ 	if (!pf)
+ 		return -EINVAL;
+ 
+ 	res = pf->irq_tracker;
+ 	if (!res)
+ 		return -EINVAL;
+ 
+ 	/* give back irq_tracker resources used */
+ 	WARN_ON(pf->sriov_base_vector < res->num_entries);
+ 
+ 	pf->sriov_base_vector = 0;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_set_vf_state_qs_dis - Set VF queues state to disabled
+  * @vf: pointer to the VF structure
+  */
+ void ice_set_vf_state_qs_dis(struct ice_vf *vf)
+ {
+ 	/* Clear Rx/Tx enabled queues flag */
+ 	bitmap_zero(vf->txq_ena, ICE_MAX_RSS_QS_PER_VF);
+ 	bitmap_zero(vf->rxq_ena, ICE_MAX_RSS_QS_PER_VF);
+ 	clear_bit(ICE_VF_STATE_QS_ENA, vf->vf_states);
+ }
+ 
+ /**
+  * ice_dis_vf_qs - Disable the VF queues
+  * @vf: pointer to the VF structure
+  */
+ static void ice_dis_vf_qs(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_vsi_stop_lan_tx_rings(vsi, ICE_NO_RESET, vf->vf_id);
+ 	ice_vsi_stop_all_rx_rings(vsi);
+ 	ice_set_vf_state_qs_dis(vf);
+ }
+ 
+ /**
+  * ice_free_vfs - Free all VFs
+  * @pf: pointer to the PF structure
+  */
+ void ice_free_vfs(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vfs *vfs = &pf->vfs;
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	if (!ice_has_vfs(pf))
+ 		return;
+ 
+ 	while (test_and_set_bit(ICE_VF_DIS, pf->state))
+ 		usleep_range(1000, 2000);
+ 
+ 	/* Disable IOV before freeing resources. This lets any VF drivers
+ 	 * running in the host get themselves cleaned up before we yank
+ 	 * the carpet out from underneath their feet.
+ 	 */
+ 	if (!pci_vfs_assigned(pf->pdev))
+ 		pci_disable_sriov(pf->pdev);
+ 	else
+ 		dev_warn(dev, "VFs are assigned - not disabling SR-IOV\n");
+ 
+ 	mutex_lock(&vfs->table_lock);
+ 
+ 	ice_eswitch_release(pf);
+ 
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		mutex_lock(&vf->cfg_lock);
+ 
+ 		ice_dis_vf_qs(vf);
+ 
+ 		if (test_bit(ICE_VF_STATE_INIT, vf->vf_states)) {
+ 			/* disable VF qp mappings and set VF disable state */
+ 			ice_dis_vf_mappings(vf);
+ 			set_bit(ICE_VF_STATE_DIS, vf->vf_states);
+ 			ice_free_vf_res(vf);
+ 		}
+ 
+ 		if (!pci_vfs_assigned(pf->pdev)) {
+ 			u32 reg_idx, bit_idx;
+ 
+ 			reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
+ 			bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
+ 			wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
+ 		}
+ 
+ 		/* clear malicious info since the VF is getting released */
+ 		if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
+ 					ICE_MAX_VF_COUNT, vf->vf_id))
+ 			dev_dbg(dev, "failed to clear malicious VF state for VF %u\n",
+ 				vf->vf_id);
+ 
+ 		mutex_unlock(&vf->cfg_lock);
+ 	}
+ 
+ 	if (ice_sriov_free_msix_res(pf))
+ 		dev_err(dev, "Failed to free MSIX resources used by SR-IOV\n");
+ 
+ 	vfs->num_qps_per = 0;
+ 	ice_free_vf_entries(pf);
+ 
+ 	mutex_unlock(&vfs->table_lock);
+ 
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 	clear_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
+ }
+ 
+ /**
+  * ice_trigger_vf_reset - Reset a VF on HW
+  * @vf: pointer to the VF structure
+  * @is_vflr: true if VFLR was issued, false if not
+  * @is_pfr: true if the reset was triggered due to a previous PFR
+  *
+  * Trigger hardware to start a reset for a particular VF. Expects the caller
+  * to wait the proper amount of time to allow hardware to reset the VF before
+  * it cleans up and restores VF functionality.
+  */
+ static void ice_trigger_vf_reset(struct ice_vf *vf, bool is_vflr, bool is_pfr)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	u32 reg, reg_idx, bit_idx;
+ 	unsigned int vf_abs_id, i;
+ 	struct device *dev;
+ 	struct ice_hw *hw;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	hw = &pf->hw;
+ 	vf_abs_id = vf->vf_id + hw->func_caps.vf_base_id;
+ 
+ 	/* Inform VF that it is no longer active, as a warning */
+ 	clear_bit(ICE_VF_STATE_ACTIVE, vf->vf_states);
+ 
+ 	/* Disable VF's configuration API during reset. The flag is re-enabled
+ 	 * when it's safe again to access VF's VSI.
+ 	 */
+ 	clear_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 
+ 	/* VF_MBX_ARQLEN and VF_MBX_ATQLEN are cleared by PFR, so the driver
+ 	 * needs to clear them in the case of VFR/VFLR. If this is done for
+ 	 * PFR, it can mess up VF resets because the VF driver may already
+ 	 * have started cleanup by the time we get here.
+ 	 */
+ 	if (!is_pfr) {
+ 		wr32(hw, VF_MBX_ARQLEN(vf->vf_id), 0);
+ 		wr32(hw, VF_MBX_ATQLEN(vf->vf_id), 0);
+ 	}
+ 
+ 	/* In the case of a VFLR, the HW has already reset the VF and we
+ 	 * just need to clean up, so don't hit the VFRTRIG register.
+ 	 */
+ 	if (!is_vflr) {
+ 		/* reset VF using VPGEN_VFRTRIG reg */
+ 		reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
+ 		reg |= VPGEN_VFRTRIG_VFSWR_M;
+ 		wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
+ 	}
+ 	/* clear the VFLR bit in GLGEN_VFLRSTAT */
+ 	reg_idx = (vf_abs_id) / 32;
+ 	bit_idx = (vf_abs_id) % 32;
+ 	wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
+ 	ice_flush(hw);
+ 
+ 	wr32(hw, PF_PCI_CIAA,
+ 	     VF_DEVICE_STATUS | (vf_abs_id << PF_PCI_CIAA_VF_NUM_S));
+ 	for (i = 0; i < ICE_PCI_CIAD_WAIT_COUNT; i++) {
+ 		reg = rd32(hw, PF_PCI_CIAD);
+ 		/* no transactions pending so stop polling */
+ 		if ((reg & VF_TRANS_PENDING_M) == 0)
+ 			break;
+ 
+ 		dev_err(dev, "VF %u PCI transactions stuck\n", vf->vf_id);
+ 		udelay(ICE_PCI_CIAD_WAIT_DELAY_US);
+ 	}
+ }
+ 
+ /**
+  * ice_vf_get_port_info - Get the VF's port info structure
+  * @vf: VF used to get the port info structure for
+  */
+ static struct ice_port_info *ice_vf_get_port_info(struct ice_vf *vf)
+ {
+ 	return vf->pf->hw.port_info;
+ }
+ 
+ /**
+  * ice_vf_vsi_setup - Set up a VF VSI
+  * @vf: VF to setup VSI for
+  *
+  * Returns pointer to the successfully allocated VSI struct on success,
+  * otherwise returns NULL on failure.
+  */
+ static struct ice_vsi *ice_vf_vsi_setup(struct ice_vf *vf)
+ {
+ 	struct ice_port_info *pi = ice_vf_get_port_info(vf);
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 
+ 	vsi = ice_vsi_setup(pf, pi, ICE_VSI_VF, vf, NULL);
+ 
+ 	if (!vsi) {
+ 		dev_err(ice_pf_to_dev(pf), "Failed to create VF VSI\n");
+ 		ice_vf_invalidate_vsi(vf);
+ 		return NULL;
+ 	}
+ 
+ 	vf->lan_vsi_idx = vsi->idx;
+ 	vf->lan_vsi_num = vsi->vsi_num;
+ 
+ 	return vsi;
+ }
+ 
+ /**
+  * ice_vf_ctrl_vsi_setup - Set up a VF control VSI
+  * @vf: VF to setup control VSI for
+  *
+  * Returns pointer to the successfully allocated VSI struct on success,
+  * otherwise returns NULL on failure.
+  */
+ struct ice_vsi *ice_vf_ctrl_vsi_setup(struct ice_vf *vf)
+ {
+ 	struct ice_port_info *pi = ice_vf_get_port_info(vf);
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 
+ 	vsi = ice_vsi_setup(pf, pi, ICE_VSI_CTRL, vf, NULL);
+ 	if (!vsi) {
+ 		dev_err(ice_pf_to_dev(pf), "Failed to create VF control VSI\n");
+ 		ice_vf_ctrl_invalidate_vsi(vf);
+ 	}
+ 
+ 	return vsi;
+ }
+ 
+ /**
+  * ice_calc_vf_first_vector_idx - Calculate MSIX vector index in the PF space
+  * @pf: pointer to PF structure
+  * @vf: pointer to VF that the first MSIX vector index is being calculated for
+  *
+  * This returns the first MSIX vector index in PF space that is used by this VF.
+  * This index is used when accessing PF relative registers such as
+  * GLINT_VECT2FUNC and GLINT_DYN_CTL.
+  * This will always be the OICR index in the AVF driver so any functionality
+  * using vf->first_vector_idx for queue configuration will have to increment by
+  * 1 to avoid meddling with the OICR index.
+  */
+ static int ice_calc_vf_first_vector_idx(struct ice_pf *pf, struct ice_vf *vf)
+ {
+ 	return pf->sriov_base_vector + vf->vf_id * pf->vfs.num_msix_per;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_tx_rate_cfg - re-apply the Tx rate limiting configuration
+  * @vf: VF to re-apply the configuration for
+  *
+  * Called after a VF VSI has been re-added/rebuild during reset. The PF driver
+  * needs to re-apply the host configured Tx rate limiting configuration.
+  */
+ static int ice_vf_rebuild_host_tx_rate_cfg(struct ice_vf *vf)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	int err;
+ 
+ 	if (vf->min_tx_rate) {
+ 		err = ice_set_min_bw_limit(vsi, (u64)vf->min_tx_rate * 1000);
+ 		if (err) {
+ 			dev_err(dev, "failed to set min Tx rate to %d Mbps for VF %u, error %d\n",
+ 				vf->min_tx_rate, vf->vf_id, err);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	if (vf->max_tx_rate) {
+ 		err = ice_set_max_bw_limit(vsi, (u64)vf->max_tx_rate * 1000);
+ 		if (err) {
+ 			dev_err(dev, "failed to set max Tx rate to %d Mbps for VF %u, error %d\n",
+ 				vf->max_tx_rate, vf->vf_id, err);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static u16 ice_vf_get_port_vlan_id(struct ice_vf *vf)
+ {
+ 	return vf->port_vlan_info.vid;
+ }
+ 
+ static u8 ice_vf_get_port_vlan_prio(struct ice_vf *vf)
+ {
+ 	return vf->port_vlan_info.prio;
+ }
+ 
+ bool ice_vf_is_port_vlan_ena(struct ice_vf *vf)
+ {
+ 	return (ice_vf_get_port_vlan_id(vf) || ice_vf_get_port_vlan_prio(vf));
+ }
+ 
+ static u16 ice_vf_get_port_vlan_tpid(struct ice_vf *vf)
+ {
+ 	return vf->port_vlan_info.tpid;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_vlan_cfg - add VLAN 0 filter or rebuild the Port VLAN
+  * @vf: VF to add MAC filters for
+  * @vsi: Pointer to VSI
+  *
+  * Called after a VF VSI has been re-added/rebuilt during reset. The PF driver
+  * always re-adds either a VLAN 0 or port VLAN based filter after reset.
+  */
+ static int ice_vf_rebuild_host_vlan_cfg(struct ice_vf *vf, struct ice_vsi *vsi)
+ {
+ 	struct ice_vsi_vlan_ops *vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	int err;
+ 
+ 	if (ice_vf_is_port_vlan_ena(vf)) {
+ 		err = vlan_ops->set_port_vlan(vsi, &vf->port_vlan_info);
+ 		if (err) {
+ 			dev_err(dev, "failed to configure port VLAN via VSI parameters for VF %u, error %d\n",
+ 				vf->vf_id, err);
+ 			return err;
+ 		}
+ 
+ 		err = vlan_ops->add_vlan(vsi, &vf->port_vlan_info);
+ 	} else {
+ 		err = ice_vsi_add_vlan_zero(vsi);
+ 	}
+ 
+ 	if (err) {
+ 		dev_err(dev, "failed to add VLAN %u filter for VF %u during VF rebuild, error %d\n",
+ 			ice_vf_is_port_vlan_ena(vf) ?
+ 			ice_vf_get_port_vlan_id(vf) : 0, vf->vf_id, err);
+ 		return err;
+ 	}
+ 
+ 	err = vlan_ops->ena_rx_filtering(vsi);
+ 	if (err)
+ 		dev_warn(dev, "failed to enable Rx VLAN filtering for VF %d VSI %d during VF rebuild, error %d\n",
+ 			 vf->vf_id, vsi->idx, err);
+ 
+ 	return 0;
+ }
+ 
+ static int ice_cfg_mac_antispoof(struct ice_vsi *vsi, bool enable)
+ {
+ 	struct ice_vsi_ctx *ctx;
+ 	int err;
+ 
+ 	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+ 	if (!ctx)
+ 		return -ENOMEM;
+ 
+ 	ctx->info.sec_flags = vsi->info.sec_flags;
+ 	ctx->info.valid_sections = cpu_to_le16(ICE_AQ_VSI_PROP_SECURITY_VALID);
+ 
+ 	if (enable)
+ 		ctx->info.sec_flags |= ICE_AQ_VSI_SEC_FLAG_ENA_MAC_ANTI_SPOOF;
+ 	else
+ 		ctx->info.sec_flags &= ~ICE_AQ_VSI_SEC_FLAG_ENA_MAC_ANTI_SPOOF;
+ 
+ 	err = ice_update_vsi(&vsi->back->hw, vsi->idx, ctx, NULL);
+ 	if (err)
+ 		dev_err(ice_pf_to_dev(vsi->back), "Failed to configure Tx MAC anti-spoof %s for VSI %d, error %d\n",
+ 			enable ? "ON" : "OFF", vsi->vsi_num, err);
+ 	else
+ 		vsi->info.sec_flags = ctx->info.sec_flags;
+ 
+ 	kfree(ctx);
+ 
+ 	return err;
+ }
+ 
+ /**
+  * ice_vsi_ena_spoofchk - enable Tx spoof checking for this VSI
+  * @vsi: VSI to enable Tx spoof checking for
+  */
+ static int ice_vsi_ena_spoofchk(struct ice_vsi *vsi)
+ {
+ 	struct ice_vsi_vlan_ops *vlan_ops;
+ 	int err;
+ 
+ 	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
+ 
+ 	err = vlan_ops->ena_tx_filtering(vsi);
+ 	if (err)
+ 		return err;
+ 
+ 	return ice_cfg_mac_antispoof(vsi, true);
+ }
+ 
+ /**
+  * ice_vsi_dis_spoofchk - disable Tx spoof checking for this VSI
+  * @vsi: VSI to disable Tx spoof checking for
+  */
+ static int ice_vsi_dis_spoofchk(struct ice_vsi *vsi)
+ {
+ 	struct ice_vsi_vlan_ops *vlan_ops;
+ 	int err;
+ 
+ 	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
+ 
+ 	err = vlan_ops->dis_tx_filtering(vsi);
+ 	if (err)
+ 		return err;
+ 
+ 	return ice_cfg_mac_antispoof(vsi, false);
+ }
+ 
+ /**
+  * ice_vf_set_spoofchk_cfg - apply Tx spoof checking setting
+  * @vf: VF set spoofchk for
+  * @vsi: VSI associated to the VF
+  */
+ static int
+ ice_vf_set_spoofchk_cfg(struct ice_vf *vf, struct ice_vsi *vsi)
+ {
+ 	int err;
+ 
+ 	if (vf->spoofchk)
+ 		err = ice_vsi_ena_spoofchk(vsi);
+ 	else
+ 		err = ice_vsi_dis_spoofchk(vsi);
+ 
+ 	return err;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_mac_cfg - add broadcast and the VF's perm_addr/LAA
+  * @vf: VF to add MAC filters for
+  *
+  * Called after a VF VSI has been re-added/rebuilt during reset. The PF driver
+  * always re-adds a broadcast filter and the VF's perm_addr/LAA after reset.
+  */
+ static int ice_vf_rebuild_host_mac_cfg(struct ice_vf *vf)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	u8 broadcast[ETH_ALEN];
+ 	int status;
+ 
+ 	if (ice_is_eswitch_mode_switchdev(vf->pf))
+ 		return 0;
+ 
+ 	eth_broadcast_addr(broadcast);
+ 	status = ice_fltr_add_mac(vsi, broadcast, ICE_FWD_TO_VSI);
+ 	if (status) {
+ 		dev_err(dev, "failed to add broadcast MAC filter for VF %u, error %d\n",
+ 			vf->vf_id, status);
+ 		return status;
+ 	}
+ 
+ 	vf->num_mac++;
+ 
+ 	if (is_valid_ether_addr(vf->hw_lan_addr.addr)) {
+ 		status = ice_fltr_add_mac(vsi, vf->hw_lan_addr.addr,
+ 					  ICE_FWD_TO_VSI);
+ 		if (status) {
+ 			dev_err(dev, "failed to add default unicast MAC filter %pM for VF %u, error %d\n",
+ 				&vf->hw_lan_addr.addr[0], vf->vf_id,
+ 				status);
+ 			return status;
+ 		}
+ 		vf->num_mac++;
+ 
+ 		ether_addr_copy(vf->dev_lan_addr.addr, vf->hw_lan_addr.addr);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_vf_set_host_trust_cfg - set trust setting based on pre-reset value
+  * @vf: VF to configure trust setting for
+  */
+ static void ice_vf_set_host_trust_cfg(struct ice_vf *vf)
+ {
+ 	if (vf->trusted)
+ 		set_bit(ICE_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps);
+ 	else
+ 		clear_bit(ICE_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps);
+ }
+ 
+ /**
+  * ice_ena_vf_msix_mappings - enable VF MSIX mappings in hardware
+  * @vf: VF to enable MSIX mappings for
+  *
+  * Some of the registers need to be indexed/configured using hardware global
+  * device values and other registers need 0-based values, which represent PF
+  * based values.
+  */
+ static void ice_ena_vf_msix_mappings(struct ice_vf *vf)
+ {
+ 	int device_based_first_msix, device_based_last_msix;
+ 	int pf_based_first_msix, pf_based_last_msix, v;
+ 	struct ice_pf *pf = vf->pf;
+ 	int device_based_vf_id;
+ 	struct ice_hw *hw;
+ 	u32 reg;
+ 
+ 	hw = &pf->hw;
+ 	pf_based_first_msix = vf->first_vector_idx;
+ 	pf_based_last_msix = (pf_based_first_msix + pf->vfs.num_msix_per) - 1;
+ 
+ 	device_based_first_msix = pf_based_first_msix +
+ 		pf->hw.func_caps.common_cap.msix_vector_first_id;
+ 	device_based_last_msix =
+ 		(device_based_first_msix + pf->vfs.num_msix_per) - 1;
+ 	device_based_vf_id = vf->vf_id + hw->func_caps.vf_base_id;
+ 
+ 	reg = (((device_based_first_msix << VPINT_ALLOC_FIRST_S) &
+ 		VPINT_ALLOC_FIRST_M) |
+ 	       ((device_based_last_msix << VPINT_ALLOC_LAST_S) &
+ 		VPINT_ALLOC_LAST_M) | VPINT_ALLOC_VALID_M);
+ 	wr32(hw, VPINT_ALLOC(vf->vf_id), reg);
+ 
+ 	reg = (((device_based_first_msix << VPINT_ALLOC_PCI_FIRST_S)
+ 		 & VPINT_ALLOC_PCI_FIRST_M) |
+ 	       ((device_based_last_msix << VPINT_ALLOC_PCI_LAST_S) &
+ 		VPINT_ALLOC_PCI_LAST_M) | VPINT_ALLOC_PCI_VALID_M);
+ 	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), reg);
+ 
+ 	/* map the interrupts to its functions */
+ 	for (v = pf_based_first_msix; v <= pf_based_last_msix; v++) {
+ 		reg = (((device_based_vf_id << GLINT_VECT2FUNC_VF_NUM_S) &
+ 			GLINT_VECT2FUNC_VF_NUM_M) |
+ 		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
+ 			GLINT_VECT2FUNC_PF_NUM_M));
+ 		wr32(hw, GLINT_VECT2FUNC(v), reg);
+ 	}
+ 
+ 	/* Map mailbox interrupt to VF MSI-X vector 0 */
+ 	wr32(hw, VPINT_MBX_CTL(device_based_vf_id), VPINT_MBX_CTL_CAUSE_ENA_M);
+ }
+ 
+ /**
+  * ice_ena_vf_q_mappings - enable Rx/Tx queue mappings for a VF
+  * @vf: VF to enable the mappings for
+  * @max_txq: max Tx queues allowed on the VF's VSI
+  * @max_rxq: max Rx queues allowed on the VF's VSI
+  */
+ static void ice_ena_vf_q_mappings(struct ice_vf *vf, u16 max_txq, u16 max_rxq)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 	u32 reg;
+ 
+ 	/* set regardless of mapping mode */
+ 	wr32(hw, VPLAN_TXQ_MAPENA(vf->vf_id), VPLAN_TXQ_MAPENA_TX_ENA_M);
+ 
+ 	/* VF Tx queues allocation */
+ 	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG) {
+ 		/* set the VF PF Tx queue range
+ 		 * VFNUMQ value should be set to (number of queues - 1). A value
+ 		 * of 0 means 1 queue and a value of 255 means 256 queues
+ 		 */
+ 		reg = (((vsi->txq_map[0] << VPLAN_TX_QBASE_VFFIRSTQ_S) &
+ 			VPLAN_TX_QBASE_VFFIRSTQ_M) |
+ 		       (((max_txq - 1) << VPLAN_TX_QBASE_VFNUMQ_S) &
+ 			VPLAN_TX_QBASE_VFNUMQ_M));
+ 		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), reg);
+ 	} else {
+ 		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
+ 	}
+ 
+ 	/* set regardless of mapping mode */
+ 	wr32(hw, VPLAN_RXQ_MAPENA(vf->vf_id), VPLAN_RXQ_MAPENA_RX_ENA_M);
+ 
+ 	/* VF Rx queues allocation */
+ 	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG) {
+ 		/* set the VF PF Rx queue range
+ 		 * VFNUMQ value should be set to (number of queues - 1). A value
+ 		 * of 0 means 1 queue and a value of 255 means 256 queues
+ 		 */
+ 		reg = (((vsi->rxq_map[0] << VPLAN_RX_QBASE_VFFIRSTQ_S) &
+ 			VPLAN_RX_QBASE_VFFIRSTQ_M) |
+ 		       (((max_rxq - 1) << VPLAN_RX_QBASE_VFNUMQ_S) &
+ 			VPLAN_RX_QBASE_VFNUMQ_M));
+ 		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), reg);
+ 	} else {
+ 		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
+ 	}
+ }
+ 
+ /**
+  * ice_ena_vf_mappings - enable VF MSIX and queue mapping
+  * @vf: pointer to the VF structure
+  */
+ static void ice_ena_vf_mappings(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_ena_vf_msix_mappings(vf);
+ 	ice_ena_vf_q_mappings(vf, vsi->alloc_txq, vsi->alloc_rxq);
+ }
+ 
+ /**
+  * ice_calc_vf_reg_idx - Calculate the VF's register index in the PF space
+  * @vf: VF to calculate the register index for
+  * @q_vector: a q_vector associated to the VF
+  */
+ int ice_calc_vf_reg_idx(struct ice_vf *vf, struct ice_q_vector *q_vector)
+ {
+ 	struct ice_pf *pf;
+ 
+ 	if (!vf || !q_vector)
+ 		return -EINVAL;
+ 
+ 	pf = vf->pf;
+ 
+ 	/* always add one to account for the OICR being the first MSIX */
+ 	return pf->sriov_base_vector + pf->vfs.num_msix_per * vf->vf_id +
+ 		q_vector->v_idx + 1;
+ }
+ 
+ /**
+  * ice_get_max_valid_res_idx - Get the max valid resource index
+  * @res: pointer to the resource to find the max valid index for
+  *
+  * Start from the end of the ice_res_tracker and return right when we find the
+  * first res->list entry with the ICE_RES_VALID_BIT set. This function is only
+  * valid for SR-IOV because it is the only consumer that manipulates the
+  * res->end and this is always called when res->end is set to res->num_entries.
+  */
+ static int ice_get_max_valid_res_idx(struct ice_res_tracker *res)
+ {
+ 	int i;
+ 
+ 	if (!res)
+ 		return -EINVAL;
+ 
+ 	for (i = res->num_entries - 1; i >= 0; i--)
+ 		if (res->list[i] & ICE_RES_VALID_BIT)
+ 			return i;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_set_msix_res - Set any used MSIX resources
+  * @pf: pointer to PF structure
+  * @num_msix_needed: number of MSIX vectors needed for all SR-IOV VFs
+  *
+  * This function allows SR-IOV resources to be taken from the end of the PF's
+  * allowed HW MSIX vectors so that the irq_tracker will not be affected. We
+  * just set the pf->sriov_base_vector and return success.
+  *
+  * If there are not enough resources available, return an error. This should
+  * always be caught by ice_set_per_vf_res().
+  *
+  * Return 0 on success, and -EINVAL when there are not enough MSIX vectors
+  * in the PF's space available for SR-IOV.
+  */
+ static int ice_sriov_set_msix_res(struct ice_pf *pf, u16 num_msix_needed)
+ {
+ 	u16 total_vectors = pf->hw.func_caps.common_cap.num_msix_vectors;
+ 	int vectors_used = pf->irq_tracker->num_entries;
+ 	int sriov_base_vector;
+ 
+ 	sriov_base_vector = total_vectors - num_msix_needed;
+ 
+ 	/* make sure we only grab irq_tracker entries from the list end and
+ 	 * that we have enough available MSIX vectors
+ 	 */
+ 	if (sriov_base_vector < vectors_used)
+ 		return -EINVAL;
+ 
+ 	pf->sriov_base_vector = sriov_base_vector;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_set_per_vf_res - check if vectors and queues are available
+  * @pf: pointer to the PF structure
+  * @num_vfs: the number of SR-IOV VFs being configured
+  *
+  * First, determine HW interrupts from common pool. If we allocate fewer VFs, we
+  * get more vectors and can enable more queues per VF. Note that this does not
+  * grab any vectors from the SW pool already allocated. Also note, that all
+  * vector counts include one for each VF's miscellaneous interrupt vector
+  * (i.e. OICR).
+  *
+  * Minimum VFs - 2 vectors, 1 queue pair
+  * Small VFs - 5 vectors, 4 queue pairs
+  * Medium VFs - 17 vectors, 16 queue pairs
+  *
+  * Second, determine number of queue pairs per VF by starting with a pre-defined
+  * maximum each VF supports. If this is not possible, then we adjust based on
+  * queue pairs available on the device.
+  *
+  * Lastly, set queue and MSI-X VF variables tracked by the PF so it can be used
+  * by each VF during VF initialization and reset.
+  */
+ static int ice_set_per_vf_res(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	int max_valid_res_idx = ice_get_max_valid_res_idx(pf->irq_tracker);
+ 	u16 num_msix_per_vf, num_txq, num_rxq, avail_qs;
+ 	int msix_avail_per_vf, msix_avail_for_sriov;
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	lockdep_assert_held(&pf->vfs.table_lock);
+ 
+ 	if (!num_vfs || max_valid_res_idx < 0)
+ 		return -EINVAL;
+ 
+ 	/* determine MSI-X resources per VF */
+ 	msix_avail_for_sriov = pf->hw.func_caps.common_cap.num_msix_vectors -
+ 		pf->irq_tracker->num_entries;
+ 	msix_avail_per_vf = msix_avail_for_sriov / num_vfs;
+ 	if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MED) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_MED;
+ 	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_SMALL) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_SMALL;
+ 	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MULTIQ_MIN) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_MULTIQ_MIN;
+ 	} else if (msix_avail_per_vf >= ICE_MIN_INTR_PER_VF) {
+ 		num_msix_per_vf = ICE_MIN_INTR_PER_VF;
+ 	} else {
+ 		dev_err(dev, "Only %d MSI-X interrupts available for SR-IOV. Not enough to support minimum of %d MSI-X interrupts per VF for %d VFs\n",
+ 			msix_avail_for_sriov, ICE_MIN_INTR_PER_VF,
+ 			num_vfs);
+ 		return -EIO;
+ 	}
+ 
+ 	num_txq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
+ 			ICE_MAX_RSS_QS_PER_VF);
+ 	avail_qs = ice_get_avail_txq_count(pf) / num_vfs;
+ 	if (!avail_qs)
+ 		num_txq = 0;
+ 	else if (num_txq > avail_qs)
+ 		num_txq = rounddown_pow_of_two(avail_qs);
+ 
+ 	num_rxq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
+ 			ICE_MAX_RSS_QS_PER_VF);
+ 	avail_qs = ice_get_avail_rxq_count(pf) / num_vfs;
+ 	if (!avail_qs)
+ 		num_rxq = 0;
+ 	else if (num_rxq > avail_qs)
+ 		num_rxq = rounddown_pow_of_two(avail_qs);
+ 
+ 	if (num_txq < ICE_MIN_QS_PER_VF || num_rxq < ICE_MIN_QS_PER_VF) {
+ 		dev_err(dev, "Not enough queues to support minimum of %d queue pairs per VF for %d VFs\n",
+ 			ICE_MIN_QS_PER_VF, num_vfs);
+ 		return -EIO;
+ 	}
+ 
+ 	if (ice_sriov_set_msix_res(pf, num_msix_per_vf * num_vfs)) {
+ 		dev_err(dev, "Unable to set MSI-X resources for %d VFs\n",
+ 			num_vfs);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* only allow equal Tx/Rx queue count (i.e. queue pairs) */
+ 	pf->vfs.num_qps_per = min_t(int, num_txq, num_rxq);
+ 	pf->vfs.num_msix_per = num_msix_per_vf;
+ 	dev_info(dev, "Enabling %d VFs with %d vectors and %d queues per VF\n",
+ 		 num_vfs, pf->vfs.num_msix_per, pf->vfs.num_qps_per);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_clear_vf_reset_trigger - enable VF to access hardware
+  * @vf: VF to enabled hardware access for
+  */
+ static void ice_clear_vf_reset_trigger(struct ice_vf *vf)
+ {
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 	u32 reg;
+ 
+ 	reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
+ 	reg &= ~VPGEN_VFRTRIG_VFSWR_M;
+ 	wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
+ 	ice_flush(hw);
+ }
+ 
+ static int
+ ice_vf_set_vsi_promisc(struct ice_vf *vf, struct ice_vsi *vsi, u8 promisc_m)
+ {
+ 	struct ice_hw *hw = &vsi->back->hw;
+ 	int status;
+ 
+ 	if (ice_vf_is_port_vlan_ena(vf))
+ 		status = ice_fltr_set_vsi_promisc(hw, vsi->idx, promisc_m,
+ 						  ice_vf_get_port_vlan_id(vf));
+ 	else if (ice_vsi_has_non_zero_vlans(vsi))
+ 		status = ice_fltr_set_vlan_vsi_promisc(hw, vsi, promisc_m);
+ 	else
+ 		status = ice_fltr_set_vsi_promisc(hw, vsi->idx, promisc_m, 0);
+ 
+ 	if (status && status != -EEXIST) {
+ 		dev_err(ice_pf_to_dev(vsi->back), "enable Tx/Rx filter promiscuous mode on VF-%u failed, error: %d\n",
+ 			vf->vf_id, status);
+ 		return status;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+ ice_vf_clear_vsi_promisc(struct ice_vf *vf, struct ice_vsi *vsi, u8 promisc_m)
+ {
+ 	struct ice_hw *hw = &vsi->back->hw;
+ 	int status;
+ 
+ 	if (ice_vf_is_port_vlan_ena(vf))
+ 		status = ice_fltr_clear_vsi_promisc(hw, vsi->idx, promisc_m,
+ 						    ice_vf_get_port_vlan_id(vf));
+ 	else if (ice_vsi_has_non_zero_vlans(vsi))
+ 		status = ice_fltr_clear_vlan_vsi_promisc(hw, vsi, promisc_m);
+ 	else
+ 		status = ice_fltr_clear_vsi_promisc(hw, vsi->idx, promisc_m, 0);
+ 
+ 	if (status && status != -ENOENT) {
+ 		dev_err(ice_pf_to_dev(vsi->back), "disable Tx/Rx filter promiscuous mode on VF-%u failed, error: %d\n",
+ 			vf->vf_id, status);
+ 		return status;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void ice_vf_clear_counters(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	vf->num_mac = 0;
+ 	vsi->num_vlan = 0;
+ 	memset(&vf->mdd_tx_events, 0, sizeof(vf->mdd_tx_events));
+ 	memset(&vf->mdd_rx_events, 0, sizeof(vf->mdd_rx_events));
+ }
+ 
+ /**
+  * ice_vf_pre_vsi_rebuild - tasks to be done prior to VSI rebuild
+  * @vf: VF to perform pre VSI rebuild tasks
+  *
+  * These tasks are items that don't need to be amortized since they are most
+  * likely called in a for loop with all VF(s) in the reset_all_vfs() case.
+  */
+ static void ice_vf_pre_vsi_rebuild(struct ice_vf *vf)
+ {
+ 	ice_vf_clear_counters(vf);
+ 	ice_clear_vf_reset_trigger(vf);
+ }
+ 
+ /**
+  * ice_vf_rebuild_aggregator_node_cfg - rebuild aggregator node config
+  * @vsi: Pointer to VSI
+  *
+  * This function moves VSI into corresponding scheduler aggregator node
+  * based on cached value of "aggregator node info" per VSI
+  */
+ static void ice_vf_rebuild_aggregator_node_cfg(struct ice_vsi *vsi)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct device *dev;
+ 	int status;
+ 
+ 	if (!vsi->agg_node)
+ 		return;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	if (vsi->agg_node->num_vsis == ICE_MAX_VSIS_IN_AGG_NODE) {
+ 		dev_dbg(dev,
+ 			"agg_id %u already has reached max_num_vsis %u\n",
+ 			vsi->agg_node->agg_id, vsi->agg_node->num_vsis);
+ 		return;
+ 	}
+ 
+ 	status = ice_move_vsi_to_agg(pf->hw.port_info, vsi->agg_node->agg_id,
+ 				     vsi->idx, vsi->tc_cfg.ena_tc);
+ 	if (status)
+ 		dev_dbg(dev, "unable to move VSI idx %u into aggregator %u node",
+ 			vsi->idx, vsi->agg_node->agg_id);
+ 	else
+ 		vsi->agg_node->num_vsis++;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_cfg - host admin configuration is persistent across reset
+  * @vf: VF to rebuild host configuration on
+  */
+ static void ice_vf_rebuild_host_cfg(struct ice_vf *vf)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_vf_set_host_trust_cfg(vf);
+ 
+ 	if (ice_vf_rebuild_host_mac_cfg(vf))
+ 		dev_err(dev, "failed to rebuild default MAC configuration for VF %d\n",
+ 			vf->vf_id);
+ 
+ 	if (ice_vf_rebuild_host_vlan_cfg(vf, vsi))
+ 		dev_err(dev, "failed to rebuild VLAN configuration for VF %u\n",
+ 			vf->vf_id);
+ 
+ 	if (ice_vf_rebuild_host_tx_rate_cfg(vf))
+ 		dev_err(dev, "failed to rebuild Tx rate limiting configuration for VF %u\n",
+ 			vf->vf_id);
+ 
+ 	if (ice_vf_set_spoofchk_cfg(vf, vsi))
+ 		dev_err(dev, "failed to rebuild spoofchk configuration for VF %d\n",
+ 			vf->vf_id);
+ 
+ 	/* rebuild aggregator node config for main VF VSI */
+ 	ice_vf_rebuild_aggregator_node_cfg(vsi);
+ }
+ 
+ /**
+  * ice_vf_rebuild_vsi_with_release - release and setup the VF's VSI
+  * @vf: VF to release and setup the VSI for
+  *
+  * This is only called when a single VF is being reset (i.e. VFR, VFLR, host VF
+  * configuration change, etc.).
+  */
+ static int ice_vf_rebuild_vsi_with_release(struct ice_vf *vf)
+ {
+ 	ice_vf_vsi_release(vf);
+ 	if (!ice_vf_vsi_setup(vf))
+ 		return -ENOMEM;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_vf_rebuild_vsi - rebuild the VF's VSI
+  * @vf: VF to rebuild the VSI for
+  *
+  * This is only called when all VF(s) are being reset (i.e. PCIe Reset on the
+  * host, PFR, CORER, etc.).
+  */
+ static int ice_vf_rebuild_vsi(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	if (ice_vsi_rebuild(vsi, true)) {
+ 		dev_err(ice_pf_to_dev(pf), "failed to rebuild VF %d VSI\n",
+ 			vf->vf_id);
+ 		return -EIO;
+ 	}
+ 	/* vsi->idx will remain the same in this case so don't update
+ 	 * vf->lan_vsi_idx
+ 	 */
+ 	vsi->vsi_num = ice_get_hw_vsi_num(&pf->hw, vsi->idx);
+ 	vf->lan_vsi_num = vsi->vsi_num;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_vf_set_initialized - VF is ready for VIRTCHNL communication
+  * @vf: VF to set in initialized state
+  *
+  * After this function the VF will be ready to receive/handle the
+  * VIRTCHNL_OP_GET_VF_RESOURCES message
+  */
+ static void ice_vf_set_initialized(struct ice_vf *vf)
+ {
+ 	ice_set_vf_state_qs_dis(vf);
+ 	clear_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states);
+ 	clear_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states);
+ 	clear_bit(ICE_VF_STATE_DIS, vf->vf_states);
+ 	set_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 	memset(&vf->vlan_v2_caps, 0, sizeof(vf->vlan_v2_caps));
+ }
+ 
+ /**
+  * ice_vf_post_vsi_rebuild - tasks to do after the VF's VSI have been rebuilt
+  * @vf: VF to perform tasks on
+  */
+ static void ice_vf_post_vsi_rebuild(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_hw *hw;
+ 
+ 	hw = &pf->hw;
+ 
+ 	ice_vf_rebuild_host_cfg(vf);
+ 
+ 	ice_vf_set_initialized(vf);
+ 	ice_ena_vf_mappings(vf);
+ 	wr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
+ }
+ 
+ /**
+  * ice_reset_all_vfs - reset all allocated VFs in one go
+  * @pf: pointer to the PF structure
+  * @is_vflr: true if VFLR was issued, false if not
+  *
+  * First, tell the hardware to reset each VF, then do all the waiting in one
+  * chunk, and finally finish restoring each VF after the wait. This is useful
+  * during PF routines which need to reset all VFs, as otherwise it must perform
+  * these resets in a serialized fashion.
+  *
+  * Returns true if any VFs were reset, and false otherwise.
+  */
+ bool ice_reset_all_vfs(struct ice_pf *pf, bool is_vflr)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	/* If we don't have any VFs, then there is nothing to reset */
+ 	if (!ice_has_vfs(pf))
+ 		return false;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 
+ 	/* clear all malicious info if the VFs are getting reset */
+ 	ice_for_each_vf(pf, bkt, vf)
+ 		if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
+ 					ICE_MAX_VF_COUNT, vf->vf_id))
+ 			dev_dbg(dev, "failed to clear malicious VF state for VF %u\n",
+ 				vf->vf_id);
+ 
+ 	/* If VFs have been disabled, there is no need to reset */
+ 	if (test_and_set_bit(ICE_VF_DIS, pf->state)) {
+ 		mutex_unlock(&pf->vfs.table_lock);
+ 		return false;
+ 	}
+ 
+ 	/* Begin reset on all VFs at once */
+ 	ice_for_each_vf(pf, bkt, vf)
+ 		ice_trigger_vf_reset(vf, is_vflr, true);
+ 
+ 	/* HW requires some time to make sure it can flush the FIFO for a VF
+ 	 * when it resets it. Poll the VPGEN_VFRSTAT register for each VF in
+ 	 * sequence to make sure that it has completed. We'll keep track of
+ 	 * the VFs using a simple iterator that increments once that VF has
+ 	 * finished resetting.
+ 	 */
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		bool done = false;
+ 		unsigned int i;
+ 		u32 reg;
+ 
+ 		for (i = 0; i < 10; i++) {
+ 			reg = rd32(&pf->hw, VPGEN_VFRSTAT(vf->vf_id));
+ 			if (reg & VPGEN_VFRSTAT_VFRD_M) {
+ 				done = true;
+ 				break;
+ 			}
+ 
+ 			/* only delay if check failed */
+ 			usleep_range(10, 20);
+ 		}
+ 
+ 		if (!done) {
+ 			/* Display a warning if at least one VF didn't manage
+ 			 * to reset in time, but continue on with the
+ 			 * operation.
+ 			 */
+ 			dev_warn(dev, "VF %u reset check timeout\n", vf->vf_id);
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* free VF resources to begin resetting the VSI state */
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		mutex_lock(&vf->cfg_lock);
+ 
+ 		vf->driver_caps = 0;
+ 		ice_vc_set_default_allowlist(vf);
+ 
+ 		ice_vf_fdir_exit(vf);
+ 		ice_vf_fdir_init(vf);
+ 		/* clean VF control VSI when resetting VFs since it should be
+ 		 * setup only when VF creates its first FDIR rule.
+ 		 */
+ 		if (vf->ctrl_vsi_idx != ICE_NO_VSI)
+ 			ice_vf_ctrl_invalidate_vsi(vf);
+ 
+ 		ice_vf_pre_vsi_rebuild(vf);
+ 		ice_vf_rebuild_vsi(vf);
+ 		ice_vf_post_vsi_rebuild(vf);
+ 
+ 		mutex_unlock(&vf->cfg_lock);
+ 	}
+ 
+ 	if (ice_is_eswitch_mode_switchdev(pf))
+ 		if (ice_eswitch_rebuild(pf))
+ 			dev_warn(dev, "eswitch rebuild failed\n");
+ 
+ 	ice_flush(hw);
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * ice_is_vf_disabled
+  * @vf: pointer to the VF info
+  *
+  * Returns true if the PF or VF is disabled, false otherwise.
+  */
+ bool ice_is_vf_disabled(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	/* If the PF has been disabled, there is no need resetting VF until
+ 	 * PF is active again. Similarly, if the VF has been disabled, this
+ 	 * means something else is resetting the VF, so we shouldn't continue.
+ 	 * Otherwise, set disable VF state bit for actual reset, and continue.
+ 	 */
+ 	return (test_bit(ICE_VF_DIS, pf->state) ||
+ 		test_bit(ICE_VF_STATE_DIS, vf->vf_states));
+ }
+ 
+ /**
+  * ice_reset_vf - Reset a particular VF
+  * @vf: pointer to the VF structure
+  * @is_vflr: true if VFLR was issued, false if not
+  *
+  * Returns true if the VF is currently in reset, resets successfully, or resets
+  * are disabled and false otherwise.
+  */
+ bool ice_reset_vf(struct ice_vf *vf, bool is_vflr)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	struct ice_hw *hw;
+ 	bool rsd = false;
+ 	u8 promisc_m;
+ 	u32 reg;
+ 	int i;
+ 
+ 	lockdep_assert_held(&vf->cfg_lock);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 
+ 	if (test_bit(ICE_VF_RESETS_DISABLED, pf->state)) {
+ 		dev_dbg(dev, "Trying to reset VF %d, but all VF resets are disabled\n",
+ 			vf->vf_id);
+ 		return true;
+ 	}
+ 
+ 	if (ice_is_vf_disabled(vf)) {
+ 		dev_dbg(dev, "VF is already disabled, there is no need for resetting it, telling VM, all is fine %d\n",
+ 			vf->vf_id);
+ 		return true;
+ 	}
+ 
+ 	/* Set VF disable bit state here, before triggering reset */
+ 	set_bit(ICE_VF_STATE_DIS, vf->vf_states);
+ 	ice_trigger_vf_reset(vf, is_vflr, false);
+ 
+ 	vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_dis_vf_qs(vf);
+ 
+ 	/* Call Disable LAN Tx queue AQ whether or not queues are
+ 	 * enabled. This is needed for successful completion of VFR.
+ 	 */
+ 	ice_dis_vsi_txq(vsi->port_info, vsi->idx, 0, 0, NULL, NULL,
+ 			NULL, ICE_VF_RESET, vf->vf_id, NULL);
+ 
+ 	hw = &pf->hw;
+ 	/* poll VPGEN_VFRSTAT reg to make sure
+ 	 * that reset is complete
+ 	 */
+ 	for (i = 0; i < 10; i++) {
+ 		/* VF reset requires driver to first reset the VF and then
+ 		 * poll the status register to make sure that the reset
+ 		 * completed successfully.
+ 		 */
+ 		reg = rd32(hw, VPGEN_VFRSTAT(vf->vf_id));
+ 		if (reg & VPGEN_VFRSTAT_VFRD_M) {
+ 			rsd = true;
+ 			break;
+ 		}
+ 
+ 		/* only sleep if the reset is not done */
+ 		usleep_range(10, 20);
+ 	}
+ 
+ 	vf->driver_caps = 0;
+ 	ice_vc_set_default_allowlist(vf);
+ 
+ 	/* Display a warning if VF didn't manage to reset in time, but need to
+ 	 * continue on with the operation.
+ 	 */
+ 	if (!rsd)
+ 		dev_warn(dev, "VF reset check timeout on VF %d\n", vf->vf_id);
+ 
+ 	/* disable promiscuous modes in case they were enabled
+ 	 * ignore any error if disabling process failed
+ 	 */
+ 	if (test_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states) ||
+ 	    test_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states)) {
+ 		if (ice_vf_is_port_vlan_ena(vf) || vsi->num_vlan)
+ 			promisc_m = ICE_UCAST_VLAN_PROMISC_BITS;
+ 		else
+ 			promisc_m = ICE_UCAST_PROMISC_BITS;
+ 
+ 		if (ice_vf_clear_vsi_promisc(vf, vsi, promisc_m))
+ 			dev_err(dev, "disabling promiscuous mode failed\n");
+ 	}
+ 
+ 	ice_eswitch_del_vf_mac_rule(vf);
+ 
+ 	ice_vf_fdir_exit(vf);
+ 	ice_vf_fdir_init(vf);
+ 	/* clean VF control VSI when resetting VF since it should be setup
+ 	 * only when VF creates its first FDIR rule.
+ 	 */
+ 	if (vf->ctrl_vsi_idx != ICE_NO_VSI)
+ 		ice_vf_ctrl_vsi_release(vf);
+ 
+ 	ice_vf_pre_vsi_rebuild(vf);
+ 
+ 	if (ice_vf_rebuild_vsi_with_release(vf)) {
+ 		dev_err(dev, "Failed to release and setup the VF%u's VSI\n", vf->vf_id);
+ 		return false;
+ 	}
+ 
+ 	ice_vf_post_vsi_rebuild(vf);
+ 	vsi = ice_get_vf_vsi(vf);
+ 	ice_eswitch_update_repr(vsi);
+ 	ice_eswitch_replay_vf_mac_rule(vf);
+ 
+ 	/* if the VF has been reset allow it to come up again */
+ 	if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
+ 				ICE_MAX_VF_COUNT, vf->vf_id))
+ 		dev_dbg(dev, "failed to clear malicious VF state for VF %u\n", i);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * ice_vc_notify_link_state - Inform all VFs on a PF of link status
+  * @pf: pointer to the PF structure
+  */
+ void ice_vc_notify_link_state(struct ice_pf *pf)
+ {
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 	ice_for_each_vf(pf, bkt, vf)
+ 		ice_vc_notify_vf_link_state(vf);
+ 	mutex_unlock(&pf->vfs.table_lock);
+ }
+ 
+ /**
+  * ice_vc_notify_reset - Send pending reset message to all VFs
+  * @pf: pointer to the PF structure
+  *
+  * indicate a pending reset to all VFs on a given PF
+  */
+ void ice_vc_notify_reset(struct ice_pf *pf)
+ {
+ 	struct virtchnl_pf_event pfe;
+ 
+ 	if (!ice_has_vfs(pf))
+ 		return;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
+ 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
+ 	ice_vc_vf_broadcast(pf, VIRTCHNL_OP_EVENT, VIRTCHNL_STATUS_SUCCESS,
+ 			    (u8 *)&pfe, sizeof(struct virtchnl_pf_event));
+ }
+ 
+ /**
+  * ice_vc_notify_vf_reset - Notify VF of a reset event
+  * @vf: pointer to the VF structure
+  */
+ static void ice_vc_notify_vf_reset(struct ice_vf *vf)
+ {
+ 	struct virtchnl_pf_event pfe;
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	/* Bail out if VF is in disabled state, neither initialized, nor active
+ 	 * state - otherwise proceed with notifications
+ 	 */
+ 	if ((!test_bit(ICE_VF_STATE_INIT, vf->vf_states) &&
+ 	     !test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) ||
+ 	    test_bit(ICE_VF_STATE_DIS, vf->vf_states))
+ 		return;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
+ 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
+ 	ice_aq_send_msg_to_vf(&pf->hw, vf->vf_id, VIRTCHNL_OP_EVENT,
+ 			      VIRTCHNL_STATUS_SUCCESS, (u8 *)&pfe, sizeof(pfe),
+ 			      NULL);
+ }
+ 
+ /**
+  * ice_init_vf_vsi_res - initialize/setup VF VSI resources
+  * @vf: VF to initialize/setup the VSI for
+  *
+  * This function creates a VSI for the VF, adds a VLAN 0 filter, and sets up the
+  * VF VSI's broadcast filter and is only used during initial VF creation.
+  */
+ static int ice_init_vf_vsi_res(struct ice_vf *vf)
+ {
+ 	struct ice_vsi_vlan_ops *vlan_ops;
+ 	struct ice_pf *pf = vf->pf;
+ 	u8 broadcast[ETH_ALEN];
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	int err;
+ 
+ 	vf->first_vector_idx = ice_calc_vf_first_vector_idx(pf, vf);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	vsi = ice_vf_vsi_setup(vf);
+ 	if (!vsi)
+ 		return -ENOMEM;
+ 
+ 	err = ice_vsi_add_vlan_zero(vsi);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to add VLAN 0 filter for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
+ 	err = vlan_ops->ena_rx_filtering(vsi);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to enable Rx VLAN filtering for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	eth_broadcast_addr(broadcast);
+ 	err = ice_fltr_add_mac(vsi, broadcast, ICE_FWD_TO_VSI);
+ 	if (err) {
+ 		dev_err(dev, "Failed to add broadcast MAC filter for VF %d, error %d\n",
+ 			vf->vf_id, err);
+ 		goto release_vsi;
+ 	}
+ 
+ 	err = ice_vf_set_spoofchk_cfg(vf, vsi);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to initialize spoofchk setting for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	vf->num_mac = 1;
+ 
+ 	return 0;
+ 
+ release_vsi:
+ 	ice_vf_vsi_release(vf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_start_vfs - start VFs so they are ready to be used by SR-IOV
+  * @pf: PF the VFs are associated with
+  */
+ static int ice_start_vfs(struct ice_pf *pf)
+ {
+ 	struct ice_hw *hw = &pf->hw;
+ 	unsigned int bkt, it_cnt;
+ 	struct ice_vf *vf;
+ 	int retval;
+ 
+ 	lockdep_assert_held(&pf->vfs.table_lock);
+ 
+ 	it_cnt = 0;
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		ice_clear_vf_reset_trigger(vf);
+ 
+ 		retval = ice_init_vf_vsi_res(vf);
+ 		if (retval) {
+ 			dev_err(ice_pf_to_dev(pf), "Failed to initialize VSI resources for VF %d, error %d\n",
+ 				vf->vf_id, retval);
+ 			goto teardown;
+ 		}
+ 
+ 		set_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 		ice_ena_vf_mappings(vf);
+ 		wr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
+ 		it_cnt++;
+ 	}
+ 
+ 	ice_flush(hw);
+ 	return 0;
+ 
+ teardown:
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		if (it_cnt == 0)
+ 			break;
+ 
+ 		ice_dis_vf_mappings(vf);
+ 		ice_vf_vsi_release(vf);
+ 		it_cnt--;
+ 	}
+ 
+ 	return retval;
+ }
+ 
+ /**
+  * ice_create_vf_entries - Allocate and insert VF entries
+  * @pf: pointer to the PF structure
+  * @num_vfs: the number of VFs to allocate
+  *
+  * Allocate new VF entries and insert them into the hash table. Set some
+  * basic default fields for initializing the new VFs.
+  *
+  * After this function exits, the hash table will have num_vfs entries
+  * inserted.
+  *
+  * Returns 0 on success or an integer error code on failure.
+  */
+ static int ice_create_vf_entries(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	struct ice_vfs *vfs = &pf->vfs;
+ 	struct ice_vf *vf;
+ 	u16 vf_id;
+ 	int err;
+ 
+ 	lockdep_assert_held(&vfs->table_lock);
+ 
+ 	for (vf_id = 0; vf_id < num_vfs; vf_id++) {
+ 		vf = kzalloc(sizeof(*vf), GFP_KERNEL);
+ 		if (!vf) {
+ 			err = -ENOMEM;
+ 			goto err_free_entries;
+ 		}
+ 		kref_init(&vf->refcnt);
+ 
+ 		vf->pf = pf;
+ 		vf->vf_id = vf_id;
+ 
+ 		vf->vf_sw_id = pf->first_sw;
+ 		/* assign default capabilities */
+ 		vf->spoofchk = true;
+ 		vf->num_vf_qs = pf->vfs.num_qps_per;
+ 		ice_vc_set_default_allowlist(vf);
+ 
+ 		/* ctrl_vsi_idx will be set to a valid value only when VF
+ 		 * creates its first fdir rule.
+ 		 */
+ 		ice_vf_ctrl_invalidate_vsi(vf);
+ 		ice_vf_fdir_init(vf);
+ 
+ 		ice_virtchnl_set_dflt_ops(vf);
+ 
+ 		mutex_init(&vf->cfg_lock);
+ 
+ 		hash_add_rcu(vfs->table, &vf->entry, vf_id);
+ 	}
+ 
+ 	return 0;
+ 
+ err_free_entries:
+ 	ice_free_vf_entries(pf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_ena_vfs - enable VFs so they are ready to be used
+  * @pf: pointer to the PF structure
+  * @num_vfs: number of VFs to enable
+  */
+ static int ice_ena_vfs(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_hw *hw = &pf->hw;
+ 	int ret;
+ 
+ 	/* Disable global interrupt 0 so we don't try to handle the VFLR. */
+ 	wr32(hw, GLINT_DYN_CTL(pf->oicr_idx),
+ 	     ICE_ITR_NONE << GLINT_DYN_CTL_ITR_INDX_S);
+ 	set_bit(ICE_OICR_INTR_DIS, pf->state);
+ 	ice_flush(hw);
+ 
+ 	ret = pci_enable_sriov(pf->pdev, num_vfs);
+ 	if (ret)
+ 		goto err_unroll_intr;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 
+ 	if (ice_set_per_vf_res(pf, num_vfs)) {
+ 		dev_err(dev, "Not enough resources for %d VFs, try with fewer number of VFs\n",
+ 			num_vfs);
+ 		ret = -ENOSPC;
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	ret = ice_create_vf_entries(pf, num_vfs);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to allocate VF entries for %d VFs\n",
+ 			num_vfs);
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	if (ice_start_vfs(pf)) {
+ 		dev_err(dev, "Failed to start VF(s)\n");
+ 		ret = -EAGAIN;
+ 		goto err_unroll_vf_entries;
+ 	}
+ 
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 
+ 	ret = ice_eswitch_configure(pf);
+ 	if (ret)
+ 		goto err_unroll_sriov;
+ 
+ 	/* rearm global interrupts */
+ 	if (test_and_clear_bit(ICE_OICR_INTR_DIS, pf->state))
+ 		ice_irq_dynamic_ena(hw, NULL, NULL);
+ 
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 
+ 	return 0;
+ 
+ err_unroll_vf_entries:
+ 	ice_free_vf_entries(pf);
+ err_unroll_sriov:
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 	pci_disable_sriov(pf->pdev);
+ err_unroll_intr:
+ 	/* rearm interrupts here */
+ 	ice_irq_dynamic_ena(hw, NULL, NULL);
+ 	clear_bit(ICE_OICR_INTR_DIS, pf->state);
+ 	return ret;
+ }
+ 
+ /**
+  * ice_pci_sriov_ena - Enable or change number of VFs
+  * @pf: pointer to the PF structure
+  * @num_vfs: number of VFs to allocate
+  *
+  * Returns 0 on success and negative on failure
+  */
+ static int ice_pci_sriov_ena(struct ice_pf *pf, int num_vfs)
+ {
+ 	int pre_existing_vfs = pci_num_vf(pf->pdev);
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	if (pre_existing_vfs && pre_existing_vfs != num_vfs)
+ 		ice_free_vfs(pf);
+ 	else if (pre_existing_vfs && pre_existing_vfs == num_vfs)
+ 		return 0;
+ 
+ 	if (num_vfs > pf->vfs.num_supported) {
+ 		dev_err(dev, "Can't enable %d VFs, max VFs supported is %d\n",
+ 			num_vfs, pf->vfs.num_supported);
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	dev_info(dev, "Enabling %d VFs\n", num_vfs);
+ 	err = ice_ena_vfs(pf, num_vfs);
+ 	if (err) {
+ 		dev_err(dev, "Failed to enable SR-IOV: %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	set_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
+ 	return 0;
+ }
+ 
+ /**
+  * ice_check_sriov_allowed - check if SR-IOV is allowed based on various checks
+  * @pf: PF to enabled SR-IOV on
+  */
+ static int ice_check_sriov_allowed(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	if (!test_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags)) {
+ 		dev_err(dev, "This device is not capable of SR-IOV\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (ice_is_safe_mode(pf)) {
+ 		dev_err(dev, "SR-IOV cannot be configured - Device is in Safe Mode\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (!ice_pf_state_is_nominal(pf)) {
+ 		dev_err(dev, "Cannot enable SR-IOV, device not ready\n");
+ 		return -EBUSY;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_configure - Enable or change number of VFs via sysfs
+  * @pdev: pointer to a pci_dev structure
+  * @num_vfs: number of VFs to allocate or 0 to free VFs
+  *
+  * This function is called when the user updates the number of VFs in sysfs. On
+  * success return whatever num_vfs was set to by the caller. Return negative on
+  * failure.
+  */
+ int ice_sriov_configure(struct pci_dev *pdev, int num_vfs)
+ {
+ 	struct ice_pf *pf = pci_get_drvdata(pdev);
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	err = ice_check_sriov_allowed(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	if (!num_vfs) {
+ 		if (!pci_vfs_assigned(pdev)) {
+ 			ice_mbx_deinit_snapshot(&pf->hw);
+ 			ice_free_vfs(pf);
+ 			if (pf->lag)
+ 				ice_enable_lag(pf->lag);
+ 			return 0;
+ 		}
+ 
+ 		dev_err(dev, "can't free VFs because some are assigned to VMs.\n");
+ 		return -EBUSY;
+ 	}
+ 
+ 	err = ice_mbx_init_snapshot(&pf->hw, num_vfs);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ice_pci_sriov_ena(pf, num_vfs);
+ 	if (err) {
+ 		ice_mbx_deinit_snapshot(&pf->hw);
+ 		return err;
+ 	}
+ 
+ 	if (pf->lag)
+ 		ice_disable_lag(pf->lag);
+ 	return num_vfs;
+ }
+ 
+ /**
+  * ice_process_vflr_event - Free VF resources via IRQ calls
+  * @pf: pointer to the PF structure
+  *
+  * called from the VFLR IRQ handler to
+  * free up VF resources and state variables
+  */
+ void ice_process_vflr_event(struct ice_pf *pf)
+ {
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 	u32 reg;
+ 
+ 	if (!test_and_clear_bit(ICE_VFLR_EVENT_PENDING, pf->state) ||
+ 	    !ice_has_vfs(pf))
+ 		return;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		u32 reg_idx, bit_idx;
+ 
+ 		reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
+ 		bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
+ 		/* read GLGEN_VFLRSTAT register to find out the flr VFs */
+ 		reg = rd32(hw, GLGEN_VFLRSTAT(reg_idx));
+ 		if (reg & BIT(bit_idx)) {
+ 			/* GLGEN_VFLRSTAT bit will be cleared in ice_reset_vf */
+ 			mutex_lock(&vf->cfg_lock);
+ 			ice_reset_vf(vf, true);
+ 			mutex_unlock(&vf->cfg_lock);
+ 		}
+ 	}
+ 	mutex_unlock(&pf->vfs.table_lock);
+ }
+ 
+ /**
+  * ice_vc_reset_vf - Perform software reset on the VF after informing the AVF
+  * @vf: pointer to the VF info
+  */
+ static void ice_vc_reset_vf(struct ice_vf *vf)
+ {
+ 	ice_vc_notify_vf_reset(vf);
+ 	ice_reset_vf(vf, false);
+ }
+ 
+ /**
+  * ice_get_vf_from_pfq - get the VF who owns the PF space queue passed in
+  * @pf: PF used to index all VFs
+  * @pfq: queue index relative to the PF's function space
+  *
+  * If no VF is found who owns the pfq then return NULL, otherwise return a
+  * pointer to the VF who owns the pfq
+  *
+  * If this function returns non-NULL, it acquires a reference count of the VF
+  * structure. The caller is responsible for calling ice_put_vf() to drop this
+  * reference.
+  */
+ static struct ice_vf *ice_get_vf_from_pfq(struct ice_pf *pf, u16 pfq)
+ {
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	rcu_read_lock();
+ 	ice_for_each_vf_rcu(pf, bkt, vf) {
+ 		struct ice_vsi *vsi;
+ 		u16 rxq_idx;
+ 
+ 		vsi = ice_get_vf_vsi(vf);
+ 
+ 		ice_for_each_rxq(vsi, rxq_idx)
+ 			if (vsi->rxq_map[rxq_idx] == pfq) {
+ 				struct ice_vf *found;
+ 
+ 				if (kref_get_unless_zero(&vf->refcnt))
+ 					found = vf;
+ 				else
+ 					found = NULL;
+ 				rcu_read_unlock();
+ 				return found;
+ 			}
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	return NULL;
+ }
+ 
+ /**
+  * ice_globalq_to_pfq - convert from global queue index to PF space queue index
+  * @pf: PF used for conversion
+  * @globalq: global queue index used to convert to PF space queue index
+  */
+ static u32 ice_globalq_to_pfq(struct ice_pf *pf, u32 globalq)
+ {
+ 	return globalq - pf->hw.func_caps.common_cap.rxq_first_id;
+ }
+ 
+ /**
+  * ice_vf_lan_overflow_event - handle LAN overflow event for a VF
+  * @pf: PF that the LAN overflow event happened on
+  * @event: structure holding the event information for the LAN overflow event
+  *
+  * Determine if the LAN overflow event was caused by a VF queue. If it was not
+  * caused by a VF, do nothing. If a VF caused this LAN overflow event trigger a
+  * reset on the offending VF.
+  */
+ void
+ ice_vf_lan_overflow_event(struct ice_pf *pf, struct ice_rq_event_info *event)
+ {
+ 	u32 gldcb_rtctq, queue;
+ 	struct ice_vf *vf;
+ 
+ 	gldcb_rtctq = le32_to_cpu(event->desc.params.lan_overflow.prtdcb_ruptq);
+ 	dev_dbg(ice_pf_to_dev(pf), "GLDCB_RTCTQ: 0x%08x\n", gldcb_rtctq);
+ 
+ 	/* event returns device global Rx queue number */
+ 	queue = (gldcb_rtctq & GLDCB_RTCTQ_RXQNUM_M) >>
+ 		GLDCB_RTCTQ_RXQNUM_S;
+ 
+ 	vf = ice_get_vf_from_pfq(pf, ice_globalq_to_pfq(pf, queue));
+ 	if (!vf)
+ 		return;
+ 
+ 	mutex_lock(&vf->cfg_lock);
+ 	ice_vc_reset_vf(vf);
+ 	mutex_unlock(&vf->cfg_lock);
+ 
+ 	ice_put_vf(vf);
+ }
+ 
+ /**
+  * ice_vc_send_msg_to_vf - Send message to VF
+  * @vf: pointer to the VF info
+  * @v_opcode: virtual channel opcode
+  * @v_retval: virtual channel return value
+  * @msg: pointer to the msg buffer
+  * @msglen: msg length
+  *
+  * send msg to VF
++>>>>>>> 00a57e2959bd (ice: remove unused definitions from ice_sriov.h)
   */
  int
 -ice_vc_send_msg_to_vf(struct ice_vf *vf, u32 v_opcode,
 -		      enum virtchnl_status_code v_retval, u8 *msg, u16 msglen)
 +ice_aq_send_msg_to_vf(struct ice_hw *hw, u16 vfid, u32 v_opcode, u32 v_retval,
 +		      u8 *msg, u16 msglen, struct ice_sq_cd *cd)
  {
 -	struct device *dev;
 -	struct ice_pf *pf;
 -	int aq_ret;
 +	struct ice_aqc_pf_vf_msg *cmd;
 +	struct ice_aq_desc desc;
  
 -	pf = vf->pf;
 -	dev = ice_pf_to_dev(pf);
 +	ice_fill_dflt_direct_cmd_desc(&desc, ice_mbx_opc_send_msg_to_vf);
  
 -	aq_ret = ice_aq_send_msg_to_vf(&pf->hw, vf->vf_id, v_opcode, v_retval,
 -				       msg, msglen, NULL);
 -	if (aq_ret && pf->hw.mailboxq.sq_last_status != ICE_AQ_RC_ENOSYS) {
 -		dev_info(dev, "Unable to send the message to VF %d ret %d aq_err %s\n",
 -			 vf->vf_id, aq_ret,
 -			 ice_aq_str(pf->hw.mailboxq.sq_last_status));
 -		return -EIO;
 -	}
 +	cmd = &desc.params.virt;
 +	cmd->id = cpu_to_le32(vfid);
  
 -	return 0;
 +	desc.cookie_high = cpu_to_le32(v_opcode);
 +	desc.cookie_low = cpu_to_le32(v_retval);
 +
 +	if (msglen)
 +		desc.flags |= cpu_to_le16(ICE_AQ_FLAG_RD);
 +
 +	return ice_sq_send_cmd(hw, &hw->mailboxq, &desc, msg, msglen, cd);
  }
  
  /**
diff --cc drivers/net/ethernet/intel/ice/ice_sriov.h
index 68686a3fd7e8,699690c1f6a0..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_sriov.h
+++ b/drivers/net/ethernet/intel/ice/ice_sriov.h
@@@ -3,40 -3,408 +3,238 @@@
  
  #ifndef _ICE_SRIOV_H_
  #define _ICE_SRIOV_H_
 -#include "ice_virtchnl_fdir.h"
 -#include "ice_vsi_vlan_ops.h"
  
 -/* Restrict number of MAC Addr and VLAN that non-trusted VF can programmed */
 -#define ICE_MAX_VLAN_PER_VF		8
 -/* MAC filters: 1 is reserved for the VF's default/perm_addr/LAA MAC, 1 for
 - * broadcast, and 16 for additional unicast/multicast filters
 +#include "ice_type.h"
 +#include "ice_controlq.h"
 +
 +/* Defining the mailbox message threshold as 63 asynchronous
 + * pending messages. Normal VF functionality does not require
 + * sending more than 63 asynchronous pending message.
   */
++<<<<<<< HEAD
 +#define ICE_ASYNC_VF_MSG_THRESHOLD	63
++=======
+ #define ICE_MAX_MACADDR_PER_VF		18
+ 
+ /* Static VF transaction/status register def */
+ #define VF_DEVICE_STATUS		0xAA
+ #define VF_TRANS_PENDING_M		0x20
+ 
+ /* wait defines for polling PF_PCI_CIAD register status */
+ #define ICE_PCI_CIAD_WAIT_COUNT		100
+ #define ICE_PCI_CIAD_WAIT_DELAY_US	1
+ 
+ /* VF resource constraints */
+ #define ICE_MAX_VF_COUNT		256
+ #define ICE_MIN_QS_PER_VF		1
+ #define ICE_NONQ_VECS_VF		1
+ #define ICE_MAX_RSS_QS_PER_VF		16
+ #define ICE_NUM_VF_MSIX_MED		17
+ #define ICE_NUM_VF_MSIX_SMALL		5
+ #define ICE_NUM_VF_MSIX_MULTIQ_MIN	3
+ #define ICE_MIN_INTR_PER_VF		(ICE_MIN_QS_PER_VF + 1)
+ #define ICE_MAX_VF_RESET_TRIES		40
+ #define ICE_MAX_VF_RESET_SLEEP_MS	20
+ 
+ /* VF Hash Table access functions
+  *
+  * These functions provide abstraction for interacting with the VF hash table.
+  * In general, direct access to the hash table should be avoided outside of
+  * these functions where possible.
+  *
+  * The VF entries in the hash table are protected by reference counting to
+  * track lifetime of accesses from the table. The ice_get_vf_by_id() function
+  * obtains a reference to the VF structure which must be dropped by using
+  * ice_put_vf().
+  */
+ 
+ /**
+  * ice_for_each_vf - Iterate over each VF entry
+  * @pf: pointer to the PF private structure
+  * @bkt: bucket index used for iteration
+  * @vf: pointer to the VF entry currently being processed in the loop.
+  *
+  * The bkt variable is an unsigned integer iterator used to traverse the VF
+  * entries. It is *not* guaranteed to be the VF's vf_id. Do not assume it is.
+  * Use vf->vf_id to get the id number if needed.
+  *
+  * The caller is expected to be under the table_lock mutex for the entire
+  * loop. Use this iterator if your loop is long or if it might sleep.
+  */
+ #define ice_for_each_vf(pf, bkt, vf) \
+ 	hash_for_each((pf)->vfs.table, (bkt), (vf), entry)
+ 
+ /**
+  * ice_for_each_vf_rcu - Iterate over each VF entry protected by RCU
+  * @pf: pointer to the PF private structure
+  * @bkt: bucket index used for iteration
+  * @vf: pointer to the VF entry currently being processed in the loop.
+  *
+  * The bkt variable is an unsigned integer iterator used to traverse the VF
+  * entries. It is *not* guaranteed to be the VF's vf_id. Do not assume it is.
+  * Use vf->vf_id to get the id number if needed.
+  *
+  * The caller is expected to be under rcu_read_lock() for the entire loop.
+  * Only use this iterator if your loop is short and you can guarantee it does
+  * not sleep.
+  */
+ #define ice_for_each_vf_rcu(pf, bkt, vf) \
+ 	hash_for_each_rcu((pf)->vfs.table, (bkt), (vf), entry)
+ 
+ /* Specific VF states */
+ enum ice_vf_states {
+ 	ICE_VF_STATE_INIT = 0,		/* PF is initializing VF */
+ 	ICE_VF_STATE_ACTIVE,		/* VF resources are allocated for use */
+ 	ICE_VF_STATE_QS_ENA,		/* VF queue(s) enabled */
+ 	ICE_VF_STATE_DIS,
+ 	ICE_VF_STATE_MC_PROMISC,
+ 	ICE_VF_STATE_UC_PROMISC,
+ 	ICE_VF_STATES_NBITS
+ };
+ 
+ /* VF capabilities */
+ enum ice_virtchnl_cap {
+ 	ICE_VIRTCHNL_VF_CAP_PRIVILEGE = 0,
+ };
+ 
+ struct ice_time_mac {
+ 	unsigned long time_modified;
+ 	u8 addr[ETH_ALEN];
+ };
+ 
+ /* VF MDD events print structure */
+ struct ice_mdd_vf_events {
+ 	u16 count;			/* total count of Rx|Tx events */
+ 	/* count number of the last printed event */
+ 	u16 last_printed;
+ };
+ 
+ struct ice_vf;
+ 
+ struct ice_virtchnl_ops {
+ 	int (*get_ver_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*get_vf_res_msg)(struct ice_vf *vf, u8 *msg);
+ 	void (*reset_vf)(struct ice_vf *vf);
+ 	int (*add_mac_addr_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*del_mac_addr_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*cfg_qs_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*ena_qs_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*dis_qs_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*request_qs_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*cfg_irq_map_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*config_rss_key)(struct ice_vf *vf, u8 *msg);
+ 	int (*config_rss_lut)(struct ice_vf *vf, u8 *msg);
+ 	int (*get_stats_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*cfg_promiscuous_mode_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*add_vlan_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*remove_vlan_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*ena_vlan_stripping)(struct ice_vf *vf);
+ 	int (*dis_vlan_stripping)(struct ice_vf *vf);
+ 	int (*handle_rss_cfg_msg)(struct ice_vf *vf, u8 *msg, bool add);
+ 	int (*add_fdir_fltr_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*del_fdir_fltr_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*get_offload_vlan_v2_caps)(struct ice_vf *vf);
+ 	int (*add_vlan_v2_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*remove_vlan_v2_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*ena_vlan_stripping_v2_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*dis_vlan_stripping_v2_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*ena_vlan_insertion_v2_msg)(struct ice_vf *vf, u8 *msg);
+ 	int (*dis_vlan_insertion_v2_msg)(struct ice_vf *vf, u8 *msg);
+ };
+ 
+ /* Virtchnl/SR-IOV config info */
+ struct ice_vfs {
+ 	DECLARE_HASHTABLE(table, 8);	/* table of VF entries */
+ 	struct mutex table_lock;	/* Lock for protecting the hash table */
+ 	u16 num_supported;		/* max supported VFs on this PF */
+ 	u16 num_qps_per;		/* number of queue pairs per VF */
+ 	u16 num_msix_per;		/* number of MSI-X vectors per VF */
+ 	unsigned long last_printed_mdd_jiffies;	/* MDD message rate limit */
+ 	DECLARE_BITMAP(malvfs, ICE_MAX_VF_COUNT); /* malicious VF indicator */
+ };
+ 
+ /* VF information structure */
+ struct ice_vf {
+ 	struct hlist_node entry;
+ 	struct rcu_head rcu;
+ 	struct kref refcnt;
+ 	struct ice_pf *pf;
+ 
+ 	/* Used during virtchnl message handling and NDO ops against the VF
+ 	 * that will trigger a VFR
+ 	 */
+ 	struct mutex cfg_lock;
+ 
+ 	u16 vf_id;			/* VF ID in the PF space */
+ 	u16 lan_vsi_idx;		/* index into PF struct */
+ 	u16 ctrl_vsi_idx;
+ 	struct ice_vf_fdir fdir;
+ 	/* first vector index of this VF in the PF space */
+ 	int first_vector_idx;
+ 	struct ice_sw *vf_sw_id;	/* switch ID the VF VSIs connect to */
+ 	struct virtchnl_version_info vf_ver;
+ 	u32 driver_caps;		/* reported by VF driver */
+ 	struct virtchnl_ether_addr dev_lan_addr;
+ 	struct virtchnl_ether_addr hw_lan_addr;
+ 	struct ice_time_mac legacy_last_added_umac;
+ 	DECLARE_BITMAP(txq_ena, ICE_MAX_RSS_QS_PER_VF);
+ 	DECLARE_BITMAP(rxq_ena, ICE_MAX_RSS_QS_PER_VF);
+ 	struct ice_vlan port_vlan_info;	/* Port VLAN ID, QoS, and TPID */
+ 	struct virtchnl_vlan_caps vlan_v2_caps;
+ 	u8 pf_set_mac:1;		/* VF MAC address set by VMM admin */
+ 	u8 trusted:1;
+ 	u8 spoofchk:1;
+ 	u8 link_forced:1;
+ 	u8 link_up:1;			/* only valid if VF link is forced */
+ 	/* VSI indices - actual VSI pointers are maintained in the PF structure
+ 	 * When assigned, these will be non-zero, because VSI 0 is always
+ 	 * the main LAN VSI for the PF.
+ 	 */
+ 	u16 lan_vsi_num;		/* ID as used by firmware */
+ 	unsigned int min_tx_rate;	/* Minimum Tx bandwidth limit in Mbps */
+ 	unsigned int max_tx_rate;	/* Maximum Tx bandwidth limit in Mbps */
+ 	DECLARE_BITMAP(vf_states, ICE_VF_STATES_NBITS);	/* VF runtime states */
+ 
+ 	unsigned long vf_caps;		/* VF's adv. capabilities */
+ 	u8 num_req_qs;			/* num of queue pairs requested by VF */
+ 	u16 num_mac;
+ 	u16 num_vf_qs;			/* num of queue configured per VF */
+ 	struct ice_mdd_vf_events mdd_rx_events;
+ 	struct ice_mdd_vf_events mdd_tx_events;
+ 	DECLARE_BITMAP(opcodes_allowlist, VIRTCHNL_OP_MAX);
+ 
+ 	struct ice_repr *repr;
+ 	const struct ice_virtchnl_ops *virtchnl_ops;
+ 
+ 	/* devlink port data */
+ 	struct devlink_port devlink_port;
+ };
++>>>>>>> 00a57e2959bd (ice: remove unused definitions from ice_sriov.h)
  
  #ifdef CONFIG_PCI_IOV
 -struct ice_vf *ice_get_vf_by_id(struct ice_pf *pf, u16 vf_id);
 -void ice_put_vf(struct ice_vf *vf);
 -bool ice_has_vfs(struct ice_pf *pf);
 -u16 ice_get_num_vfs(struct ice_pf *pf);
 -struct ice_vsi *ice_get_vf_vsi(struct ice_vf *vf);
 -void ice_process_vflr_event(struct ice_pf *pf);
 -int ice_sriov_configure(struct pci_dev *pdev, int num_vfs);
 -int ice_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac);
 -int
 -ice_get_vf_cfg(struct net_device *netdev, int vf_id, struct ifla_vf_info *ivi);
 -
 -void ice_free_vfs(struct ice_pf *pf);
 -void ice_vc_process_vf_msg(struct ice_pf *pf, struct ice_rq_event_info *event);
 -void ice_vc_notify_link_state(struct ice_pf *pf);
 -void ice_vc_notify_reset(struct ice_pf *pf);
 -void ice_vc_notify_vf_link_state(struct ice_vf *vf);
 -void ice_virtchnl_set_repr_ops(struct ice_vf *vf);
 -void ice_virtchnl_set_dflt_ops(struct ice_vf *vf);
 -bool ice_reset_all_vfs(struct ice_pf *pf, bool is_vflr);
 -bool ice_reset_vf(struct ice_vf *vf, bool is_vflr);
 -void ice_restore_all_vfs_msi_state(struct pci_dev *pdev);
 -bool
 -ice_is_malicious_vf(struct ice_pf *pf, struct ice_rq_event_info *event,
 -		    u16 num_msg_proc, u16 num_msg_pending);
 -
  int
 -ice_set_vf_port_vlan(struct net_device *netdev, int vf_id, u16 vlan_id, u8 qos,
 -		     __be16 vlan_proto);
 +ice_aq_send_msg_to_vf(struct ice_hw *hw, u16 vfid, u32 v_opcode, u32 v_retval,
 +		      u8 *msg, u16 msglen, struct ice_sq_cd *cd);
  
 +u32 ice_conv_link_speed_to_virtchnl(bool adv_link_support, u16 link_speed);
  int
 -ice_set_vf_bw(struct net_device *netdev, int vf_id, int min_tx_rate,
 -	      int max_tx_rate);
 -
 -int ice_set_vf_trust(struct net_device *netdev, int vf_id, bool trusted);
 -
 -int ice_set_vf_link_state(struct net_device *netdev, int vf_id, int link_state);
 -
 -int ice_check_vf_ready_for_cfg(struct ice_vf *vf);
 -
 -bool ice_is_vf_disabled(struct ice_vf *vf);
 -
 -int ice_set_vf_spoofchk(struct net_device *netdev, int vf_id, bool ena);
 -
 -int ice_calc_vf_reg_idx(struct ice_vf *vf, struct ice_q_vector *q_vector);
 -
 -void ice_set_vf_state_qs_dis(struct ice_vf *vf);
 +ice_mbx_vf_state_handler(struct ice_hw *hw, struct ice_mbx_data *mbx_data,
 +			 u16 vf_id, bool *is_mal_vf);
  int
 -ice_get_vf_stats(struct net_device *netdev, int vf_id,
 -		 struct ifla_vf_stats *vf_stats);
 -bool ice_is_any_vf_in_promisc(struct ice_pf *pf);
 -void
 -ice_vf_lan_overflow_event(struct ice_pf *pf, struct ice_rq_event_info *event);
 -void ice_print_vfs_mdd_events(struct ice_pf *pf);
 -void ice_print_vf_rx_mdd_event(struct ice_vf *vf);
 -bool
 -ice_vc_validate_pattern(struct ice_vf *vf, struct virtchnl_proto_hdrs *proto);
 -struct ice_vsi *ice_vf_ctrl_vsi_setup(struct ice_vf *vf);
 +ice_mbx_clear_malvf(struct ice_mbx_snapshot *snap, unsigned long *all_malvfs,
 +		    u16 bitmap_len, u16 vf_id);
 +int ice_mbx_init_snapshot(struct ice_hw *hw, u16 vf_count);
 +void ice_mbx_deinit_snapshot(struct ice_hw *hw);
  int
 -ice_vc_send_msg_to_vf(struct ice_vf *vf, u32 v_opcode,
 -		      enum virtchnl_status_code v_retval, u8 *msg, u16 msglen);
 -bool ice_vc_isvalid_vsi_id(struct ice_vf *vf, u16 vsi_id);
 -bool ice_vf_is_port_vlan_ena(struct ice_vf *vf);
 +ice_mbx_report_malvf(struct ice_hw *hw, unsigned long *all_malvfs,
 +		     u16 bitmap_len, u16 vf_id, bool *report_malvf);
  #else /* CONFIG_PCI_IOV */
 -static inline struct ice_vf *ice_get_vf_by_id(struct ice_pf *pf, u16 vf_id)
 -{
 -	return NULL;
 -}
 -
 -static inline void ice_put_vf(struct ice_vf *vf)
 -{
 -}
 -
 -static inline bool ice_has_vfs(struct ice_pf *pf)
 -{
 -	return false;
 -}
 -
 -static inline u16 ice_get_num_vfs(struct ice_pf *pf)
 -{
 -	return 0;
 -}
 -
 -static inline void ice_process_vflr_event(struct ice_pf *pf) { }
 -static inline void ice_free_vfs(struct ice_pf *pf) { }
 -static inline
 -void ice_vc_process_vf_msg(struct ice_pf *pf, struct ice_rq_event_info *event) { }
 -static inline void ice_vc_notify_link_state(struct ice_pf *pf) { }
 -static inline void ice_vc_notify_reset(struct ice_pf *pf) { }
 -static inline void ice_vc_notify_vf_link_state(struct ice_vf *vf) { }
 -static inline void ice_virtchnl_set_repr_ops(struct ice_vf *vf) { }
 -static inline void ice_virtchnl_set_dflt_ops(struct ice_vf *vf) { }
 -static inline void ice_set_vf_state_qs_dis(struct ice_vf *vf) { }
 -static inline
 -void ice_vf_lan_overflow_event(struct ice_pf *pf, struct ice_rq_event_info *event) { }
 -static inline void ice_print_vfs_mdd_events(struct ice_pf *pf) { }
 -static inline void ice_print_vf_rx_mdd_event(struct ice_vf *vf) { }
 -static inline void ice_restore_all_vfs_msi_state(struct pci_dev *pdev) { }
 -
 -static inline int ice_check_vf_ready_for_cfg(struct ice_vf *vf)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline bool ice_is_vf_disabled(struct ice_vf *vf)
 -{
 -	return true;
 -}
 -
 -static inline struct ice_vsi *ice_get_vf_vsi(struct ice_vf *vf)
 -{
 -	return NULL;
 -}
 -
 -static inline bool
 -ice_is_malicious_vf(struct ice_pf __always_unused *pf,
 -		    struct ice_rq_event_info __always_unused *event,
 -		    u16 __always_unused num_msg_proc,
 -		    u16 __always_unused num_msg_pending)
 -{
 -	return false;
 -}
 -
 -static inline bool
 -ice_reset_all_vfs(struct ice_pf __always_unused *pf,
 -		  bool __always_unused is_vflr)
 -{
 -	return true;
 -}
 -
 -static inline bool
 -ice_reset_vf(struct ice_vf __always_unused *vf, bool __always_unused is_vflr)
 -{
 -	return true;
 -}
 -
 -static inline int
 -ice_sriov_configure(struct pci_dev __always_unused *pdev,
 -		    int __always_unused num_vfs)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
  static inline int
 -ice_set_vf_mac(struct net_device __always_unused *netdev,
 -	       int __always_unused vf_id, u8 __always_unused *mac)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline int
 -ice_get_vf_cfg(struct net_device __always_unused *netdev,
 -	       int __always_unused vf_id,
 -	       struct ifla_vf_info __always_unused *ivi)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline int
 -ice_set_vf_trust(struct net_device __always_unused *netdev,
 -		 int __always_unused vf_id, bool __always_unused trusted)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline int
 -ice_set_vf_port_vlan(struct net_device __always_unused *netdev,
 -		     int __always_unused vf_id, u16 __always_unused vid,
 -		     u8 __always_unused qos, __be16 __always_unused v_proto)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline int
 -ice_set_vf_spoofchk(struct net_device __always_unused *netdev,
 -		    int __always_unused vf_id, bool __always_unused ena)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline int
 -ice_set_vf_link_state(struct net_device __always_unused *netdev,
 -		      int __always_unused vf_id, int __always_unused link_state)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline int
 -ice_set_vf_bw(struct net_device __always_unused *netdev,
 -	      int __always_unused vf_id, int __always_unused min_tx_rate,
 -	      int __always_unused max_tx_rate)
 -{
 -	return -EOPNOTSUPP;
 -}
 -
 -static inline int
 -ice_calc_vf_reg_idx(struct ice_vf __always_unused *vf,
 -		    struct ice_q_vector __always_unused *q_vector)
 +ice_aq_send_msg_to_vf(struct ice_hw __always_unused *hw,
 +		      u16 __always_unused vfid, u32 __always_unused v_opcode,
 +		      u32 __always_unused v_retval, u8 __always_unused *msg,
 +		      u16 __always_unused msglen,
 +		      struct ice_sq_cd __always_unused *cd)
  {
  	return 0;
  }
* Unmerged path drivers/net/ethernet/intel/ice/ice_sriov.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_sriov.h
