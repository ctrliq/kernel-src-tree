mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-487.el8
commit-author Ben Widawsky <ben.widawsky@intel.com>
commit 269fbe72cded0afce0090103e90d2ae8ef8ac5b5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-487.el8/269fbe72.failed

Current structure 'mempolicy' uses a union to store the node info for
bind/interleave/perfer policies.

	union {
		short 		 preferred_node; /* preferred */
		nodemask_t	 nodes;		/* interleave/bind */
		/* undefined for default */
	} v;

Since preferred node can also be represented by a nodemask_t with only ont
bit set, unify these policies with using one nodemask_t 'nodes', which can
remove a union, simplify the code and make it easier to support future's
new policy's node info.

Link: https://lore.kernel.org/r/20200630212517.308045-7-ben.widawsky@intel.com
Link: https://lkml.kernel.org/r/1623399825-75651-1-git-send-email-feng.tang@intel.com
Co-developed-by: Feng Tang <feng.tang@intel.com>
	Signed-off-by: Ben Widawsky <ben.widawsky@intel.com>
	Signed-off-by: Feng Tang <feng.tang@intel.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Dave Hansen <dave.hansen@intel.com>
	Cc: Andrea Arcangeli <aarcange@redhat.com>
	Cc: Mel Gorman <mgorman@techsingularity.net>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Andi Kleen <ak@linux.intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 269fbe72cded0afce0090103e90d2ae8ef8ac5b5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/mempolicy.c
diff --cc mm/mempolicy.c
index bdeec44db9fd,e32360e90274..000000000000
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@@ -204,12 -199,11 +204,20 @@@ static int mpol_new_interleave(struct m
  
  static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)
  {
++<<<<<<< HEAD
 +	if (!nodes)
 +		pol->flags |= MPOL_F_LOCAL;	/* local allocation */
 +	else if (nodes_empty(*nodes))
 +		return -EINVAL;			/*  no allowed nodes */
 +	else
 +		pol->v.preferred_node = first_node(*nodes);
++=======
+ 	if (nodes_empty(*nodes))
+ 		return -EINVAL;
+ 
+ 	nodes_clear(pol->nodes);
+ 	node_set(first_node(*nodes), pol->nodes);
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  	return 0;
  }
  
@@@ -334,7 -326,7 +342,11 @@@ static void mpol_rebind_nodemask(struc
  	else if (pol->flags & MPOL_F_RELATIVE_NODES)
  		mpol_relative_nodemask(&tmp, &pol->w.user_nodemask, nodes);
  	else {
++<<<<<<< HEAD
 +		nodes_remap(tmp, pol->v.nodes,pol->w.cpuset_mems_allowed,
++=======
+ 		nodes_remap(tmp, pol->nodes, pol->w.cpuset_mems_allowed,
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  								*nodes);
  		pol->w.cpuset_mems_allowed = *nodes;
  	}
@@@ -884,14 -898,12 +896,20 @@@ static void get_policy_nodemask(struct 
  
  	switch (p->mode) {
  	case MPOL_BIND:
 +		/* Fall through */
  	case MPOL_INTERLEAVE:
- 		*nodes = p->v.nodes;
+ 	case MPOL_PREFERRED:
+ 		*nodes = p->nodes;
  		break;
++<<<<<<< HEAD
 +	case MPOL_PREFERRED:
 +		if (!(p->flags & MPOL_F_LOCAL))
 +			node_set(p->v.preferred_node, *nodes);
 +		/* else return empty node mask for local allocation */
++=======
+ 	case MPOL_LOCAL:
+ 		/* return empty node mask for local allocation */
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  		break;
  	default:
  		BUG();
@@@ -1805,14 -1856,14 +1823,20 @@@ static int apply_policy_zone(struct mem
  	BUG_ON(dynamic_policy_zone == ZONE_MOVABLE);
  
  	/*
- 	 * if policy->v.nodes has movable memory only,
+ 	 * if policy->nodes has movable memory only,
  	 * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.
  	 *
++<<<<<<< HEAD
 +	 * policy->v.nodes is intersect with node_states[N_MEMORY].
 +	 * so if the following test faile, it implies
 +	 * policy->v.nodes has movable memory only.
++=======
+ 	 * policy->nodes is intersect with node_states[N_MEMORY].
+ 	 * so if the following test fails, it implies
+ 	 * policy->nodes has movable memory only.
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  	 */
- 	if (!nodes_intersects(policy->v.nodes, node_states[N_HIGH_MEMORY]))
+ 	if (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))
  		dynamic_policy_zone = ZONE_MOVABLE;
  
  	return zone >= dynamic_policy_zone;
@@@ -1836,9 -1887,9 +1860,15 @@@ nodemask_t *policy_nodemask(gfp_t gfp, 
  /* Return the node id preferred by the given mempolicy, or the given id */
  static int policy_node(gfp_t gfp, struct mempolicy *policy, int nd)
  {
++<<<<<<< HEAD
 +	if (policy->mode == MPOL_PREFERRED && !(policy->flags & MPOL_F_LOCAL))
 +		nd = policy->v.preferred_node;
 +	else {
++=======
+ 	if (policy->mode == MPOL_PREFERRED) {
+ 		nd = first_node(policy->nodes);
+ 	} else {
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  		/*
  		 * __GFP_THISNODE shouldn't even be used with the bind policy
  		 * because we might easily break the expectation to stay on the
@@@ -1880,10 -1931,7 +1910,14 @@@ unsigned int mempolicy_slab_node(void
  
  	switch (policy->mode) {
  	case MPOL_PREFERRED:
++<<<<<<< HEAD
 +		/*
 +		 * handled MPOL_F_LOCAL above
 +		 */
 +		return policy->v.preferred_node;
++=======
+ 		return first_node(policy->nodes);
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  
  	case MPOL_INTERLEAVE:
  		return interleave_nodes(policy);
@@@ -1899,9 -1947,11 +1933,9 @@@
  		enum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);
  		zonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];
  		z = first_zones_zonelist(zonelist, highest_zoneidx,
- 							&policy->v.nodes);
+ 							&policy->nodes);
  		return z->zone ? zone_to_nid(z->zone) : node;
  	}
 -	case MPOL_LOCAL:
 -		return node;
  
  	default:
  		BUG();
@@@ -2014,17 -2063,13 +2047,28 @@@ bool init_nodemask_of_mempolicy(nodemas
  	mempolicy = current->mempolicy;
  	switch (mempolicy->mode) {
  	case MPOL_PREFERRED:
++<<<<<<< HEAD
 +		if (mempolicy->flags & MPOL_F_LOCAL)
 +			nid = numa_node_id();
 +		else
 +			nid = mempolicy->v.preferred_node;
 +		init_nodemask_of_node(mask, nid);
 +		break;
 +
++=======
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  	case MPOL_BIND:
 +		/* Fall through */
  	case MPOL_INTERLEAVE:
++<<<<<<< HEAD
 +		*mask =  mempolicy->v.nodes;
++=======
+ 		*mask = mempolicy->nodes;
+ 		break;
+ 
+ 	case MPOL_LOCAL:
+ 		init_nodemask_of_node(mask, numa_node_id());
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  		break;
  
  	default:
@@@ -2054,29 -2099,13 +2098,35 @@@ bool mempolicy_nodemask_intersects(stru
  
  	if (!mask)
  		return ret;
 -
  	task_lock(tsk);
  	mempolicy = tsk->mempolicy;
++<<<<<<< HEAD
 +	if (!mempolicy)
 +		goto out;
++=======
+ 	if (mempolicy && mempolicy->mode == MPOL_BIND)
+ 		ret = nodes_intersects(mempolicy->nodes, *mask);
+ 	task_unlock(tsk);
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  
 +	switch (mempolicy->mode) {
 +	case MPOL_PREFERRED:
 +		/*
 +		 * MPOL_PREFERRED and MPOL_F_LOCAL are only preferred nodes to
 +		 * allocate from, they may fallback to other nodes when oom.
 +		 * Thus, it's possible for tsk to have allocated memory from
 +		 * nodes in mask.
 +		 */
 +		break;
 +	case MPOL_BIND:
 +	case MPOL_INTERLEAVE:
 +		ret = nodes_intersects(mempolicy->v.nodes, *mask);
 +		break;
 +	default:
 +		BUG();
 +	}
 +out:
 +	task_unlock(tsk);
  	return ret;
  }
  
@@@ -2155,9 -2176,8 +2205,14 @@@ alloc_pages_vma(gfp_t gfp, int order, s
  		 * If the policy is interleave, or does not allow the current
  		 * node in its nodemask, we allocate the standard way.
  		 */
++<<<<<<< HEAD
 +		if (pol->mode == MPOL_PREFERRED &&
 +						!(pol->flags & MPOL_F_LOCAL))
 +			hpage_node = pol->v.preferred_node;
++=======
+ 		if (pol->mode == MPOL_PREFERRED)
+ 			hpage_node = first_node(pol->nodes);
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  
  		nmask = policy_nodemask(gfp, pol);
  		if (!nmask || node_isset(hpage_node, *nmask)) {
@@@ -2303,14 -2309,11 +2358,19 @@@ bool __mpol_equal(struct mempolicy *a, 
  
  	switch (a->mode) {
  	case MPOL_BIND:
 +		/* Fall through */
  	case MPOL_INTERLEAVE:
- 		return !!nodes_equal(a->v.nodes, b->v.nodes);
  	case MPOL_PREFERRED:
++<<<<<<< HEAD
 +		/* a's ->flags is the same as b's */
 +		if (a->flags & MPOL_F_LOCAL)
 +			return true;
 +		return a->v.preferred_node == b->v.preferred_node;
++=======
+ 		return !!nodes_equal(a->nodes, b->nodes);
+ 	case MPOL_LOCAL:
+ 		return true;
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  	default:
  		BUG();
  		return false;
@@@ -2451,10 -2451,11 +2511,18 @@@ int mpol_misplaced(struct page *page, s
  		break;
  
  	case MPOL_PREFERRED:
++<<<<<<< HEAD
 +		if (pol->flags & MPOL_F_LOCAL)
 +			polnid = numa_node_id();
 +		else
 +			polnid = pol->v.preferred_node;
++=======
+ 		polnid = first_node(pol->nodes);
+ 		break;
+ 
+ 	case MPOL_LOCAL:
+ 		polnid = numa_node_id();
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  		break;
  
  	case MPOL_BIND:
@@@ -2947,12 -2937,14 +3015,23 @@@ int mpol_parse_str(char *str, struct me
  	 * Save nodes for mpol_to_str() to show the tmpfs mount options
  	 * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.
  	 */
++<<<<<<< HEAD
 +	if (mode != MPOL_PREFERRED)
 +		new->v.nodes = nodes;
 +	else if (nodelist)
 +		new->v.preferred_node = first_node(nodes);
 +	else
 +		new->flags |= MPOL_F_LOCAL;
++=======
+ 	if (mode != MPOL_PREFERRED) {
+ 		new->nodes = nodes;
+ 	} else if (nodelist) {
+ 		nodes_clear(new->nodes);
+ 		node_set(first_node(nodes), new->nodes);
+ 	} else {
+ 		new->mode = MPOL_LOCAL;
+ 	}
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  
  	/*
  	 * Save nodes for contextualization: this will be used to "clone"
@@@ -2998,16 -2990,12 +3077,19 @@@ void mpol_to_str(char *buffer, int maxl
  
  	switch (mode) {
  	case MPOL_DEFAULT:
 -	case MPOL_LOCAL:
  		break;
  	case MPOL_PREFERRED:
++<<<<<<< HEAD
 +		if (flags & MPOL_F_LOCAL)
 +			mode = MPOL_LOCAL;
 +		else
 +			node_set(pol->v.preferred_node, nodes);
 +		break;
++=======
++>>>>>>> 269fbe72cded (mm/mempolicy: use unified 'nodes' for bind/interleave/prefer policies)
  	case MPOL_BIND:
  	case MPOL_INTERLEAVE:
- 		nodes = pol->v.nodes;
+ 		nodes = pol->nodes;
  		break;
  	default:
  		WARN_ON_ONCE(1);
diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index e553c56f6be7..ca5b284f1bdd 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -46,11 +46,8 @@ struct mempolicy {
 	atomic_t refcnt;
 	unsigned short mode; 	/* See MPOL_* above */
 	unsigned short flags;	/* See set_mempolicy() MPOL_F_* above */
-	union {
-		short 		 preferred_node; /* preferred */
-		nodemask_t	 nodes;		/* interleave/bind */
-		/* undefined for default */
-	} v;
+	nodemask_t nodes;	/* interleave/bind/perfer */
+
 	union {
 		nodemask_t cpuset_mems_allowed;	/* relative to these nodes */
 		nodemask_t user_nodemask;	/* nodemask passed by user */
* Unmerged path mm/mempolicy.c
