mm/hugetlb: add support for mempolicy MPOL_PREFERRED_MANY

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-487.el8
commit-author Ben Widawsky <ben.widawsky@intel.com>
commit cfcaa66f803233c50e17239469f6c96136a673a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-487.el8/cfcaa66f.failed

Implement the missing huge page allocation functionality while obeying the
preferred node semantics.  This is similar to the implementation for
general page allocation, as it uses a fallback mechanism to try multiple
preferred nodes first, and then all other nodes.

To avoid adding too many "#ifdef CONFIG_NUMA" check, add a helper function
in mempolicy.h to check whether a mempolicy is MPOL_PREFERRED_MANY.

[akpm@linux-foundation.org: fix compiling issue when merging with other hugetlb patch]
[Thanks to 0day bot for catching the !CONFIG_NUMA compiling issue]
[mhocko@suse.com: suggest to remove the #ifdef CONFIG_NUMA check]
[ben.widawsky@intel.com: add helpers to avoid ifdefs]
  Link: https://lore.kernel.org/r/20200630212517.308045-12-ben.widawsky@intel.com
  Link: https://lkml.kernel.org/r/1627970362-61305-4-git-send-email-feng.tang@intel.com
  Link: https://lkml.kernel.org/r/20210809024430.GA46432@shbuild999.sh.intel.com
[nathan@kernel.org: initialize page to NULL in alloc_buddy_huge_page_with_mpol()]
  Link: https://lkml.kernel.org/r/20210810200632.3812797-1-nathan@kernel.org

Link: https://lore.kernel.org/r/20200630212517.308045-12-ben.widawsky@intel.com
Link: https://lkml.kernel.org/r/1627970362-61305-4-git-send-email-feng.tang@intel.com
Link: https://lkml.kernel.org/r/20210809024430.GA46432@shbuild999.sh.intel.com
	Signed-off-by: Ben Widawsky <ben.widawsky@intel.com>
	Signed-off-by: Feng Tang <feng.tang@intel.com>
	Signed-off-by: Nathan Chancellor <nathan@kernel.org>
Co-developed-by: Feng Tang <feng.tang@intel.com>
	Suggested-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit cfcaa66f803233c50e17239469f6c96136a673a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mempolicy.h
#	mm/hugetlb.c
diff --cc include/linux/mempolicy.h
index e553c56f6be7,4091692bed8c..000000000000
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@@ -214,6 -184,14 +214,17 @@@ static inline bool vma_migratable(struc
  extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);
  extern void mpol_put_task_policy(struct task_struct *);
  
++<<<<<<< HEAD
++=======
+ extern bool numa_demotion_enabled;
+ 
+ static inline bool mpol_is_preferred_many(struct mempolicy *pol)
+ {
+ 	return  (pol->mode == MPOL_PREFERRED_MANY);
+ }
+ 
+ 
++>>>>>>> cfcaa66f8032 (mm/hugetlb: add support for mempolicy MPOL_PREFERRED_MANY)
  #else
  
  struct mempolicy {};
@@@ -322,5 -300,13 +333,16 @@@ static inline nodemask_t *policy_nodema
  {
  	return NULL;
  }
++<<<<<<< HEAD
++=======
+ 
+ #define numa_demotion_enabled	false
+ 
+ static inline bool mpol_is_preferred_many(struct mempolicy *pol)
+ {
+ 	return  false;
+ }
+ 
++>>>>>>> cfcaa66f8032 (mm/hugetlb: add support for mempolicy MPOL_PREFERRED_MANY)
  #endif /* CONFIG_NUMA */
  #endif
diff --cc mm/hugetlb.c
index f99ef88a4311,95dc7b83381f..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1976,9 -2159,19 +1986,24 @@@ struct page *alloc_buddy_huge_page_with
  	nodemask_t *nodemask;
  
  	nid = huge_node(vma, addr, gfp_mask, &mpol, &nodemask);
++<<<<<<< HEAD
 +	page = alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);
 +	mpol_cond_put(mpol);
++=======
+ 	if (mpol_is_preferred_many(mpol)) {
+ 		gfp_t gfp = gfp_mask | __GFP_NOWARN;
++>>>>>>> cfcaa66f8032 (mm/hugetlb: add support for mempolicy MPOL_PREFERRED_MANY)
  
+ 		gfp &=  ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);
+ 		page = alloc_surplus_huge_page(h, gfp, nid, nodemask, false);
+ 
+ 		/* Fallback to all nodes if page==NULL */
+ 		nodemask = NULL;
+ 	}
+ 
+ 	if (!page)
+ 		page = alloc_surplus_huge_page(h, gfp_mask, nid, nodemask, false);
+ 	mpol_cond_put(mpol);
  	return page;
  }
  
* Unmerged path include/linux/mempolicy.h
* Unmerged path mm/hugetlb.c
