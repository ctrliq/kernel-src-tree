x86/smpboot: Make TSC synchronization function call based

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-553.5.1.el8_10
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 9d349d47f0e39b4d1b68793ded2459daa1f948f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.5.1.el8_10/9d349d47.failed

Spin-waiting on the control CPU until the AP reaches the TSC
synchronization is just a waste especially in the case that there is no
synchronization required.

As the synchronization has to run with interrupts disabled the control CPU
part can just be done from a SMP function call. The upcoming AP issues that
call async only in the case that synchronization is required.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Tested-by: Michael Kelley <mikelley@microsoft.com>
	Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
	Tested-by: Helge Deller <deller@gmx.de> # parisc
	Tested-by: Guilherme G. Piccoli <gpiccoli@igalia.com> # Steam Deck
Link: https://lore.kernel.org/r/20230512205256.148255496@linutronix.de
(cherry picked from commit 9d349d47f0e39b4d1b68793ded2459daa1f948f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/smpboot.c
diff --cc arch/x86/kernel/smpboot.c
index 5ff70ce6c143,b2f44a837ce1..000000000000
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@@ -243,20 -255,35 +243,24 @@@ static void notrace start_secondary(voi
  	load_cr3(swapper_pg_dir);
  	__flush_tlb_all();
  #endif
 -	cpu_init_exception_handling();
 -
 -	/*
 -	 * Sync point with wait_cpu_initialized(). Sets AP in
 -	 * cpu_initialized_mask and then waits for the control CPU
 -	 * to release it.
 -	 */
 -	wait_for_master_cpu(raw_smp_processor_id());
 -
 -	cpu_init();
 +	cpu_init_secondary();
  	rcu_cpu_starting(raw_smp_processor_id());
  	x86_cpuinit.early_percpu_clock_init();
 -
 -	/*
 -	 * Sync point with wait_cpu_callin(). The AP doesn't wait here
 -	 * but just sets the bit to let the controlling CPU (BSP) know that
 -	 * it's got this far.
 -	 */
 +	preempt_disable();
  	smp_callin();
  
 -	/* Check TSC synchronization with the control CPU. */
 -	check_tsc_sync_target();
++<<<<<<< HEAD
 +	enable_start_cpu0 = 0;
  
 +	/* otherwise gcc will move up smp_processor_id before the cpu_init */
 +	barrier();
  	/*
 -	 * Calibrate the delay loop after the TSC synchronization check.
 -	 * This allows to skip the calibration when TSC is synchronized
 -	 * across sockets.
 +	 * Check TSC synchronization with the boot CPU:
  	 */
 -	ap_calibrate_delay();
++=======
++	/* Check TSC synchronization with the control CPU. */
++>>>>>>> 9d349d47f0e3 (x86/smpboot: Make TSC synchronization function call based)
 +	check_tsc_sync_target();
  
  	speculative_store_bypass_ht_init();
  
@@@ -1150,66 -1077,83 +1154,116 @@@ static int do_boot_cpu(int apicid, int 
  	 * - Use a method from the APIC driver if one defined, with wakeup
  	 *   straight to 64-bit mode preferred over wakeup to RM.
  	 * Otherwise,
 -	 * - Use an INIT boot APIC message
 +	 * - Use an INIT boot APIC message for APs or NMI for BSP.
  	 */
  	if (apic->wakeup_secondary_cpu_64)
 -		return apic->wakeup_secondary_cpu_64(apicid, start_ip);
 +		boot_error = apic->wakeup_secondary_cpu_64(apicid, start_ip);
  	else if (apic->wakeup_secondary_cpu)
 -		return apic->wakeup_secondary_cpu(apicid, start_ip);
 -
 -	return wakeup_secondary_cpu_via_init(apicid, start_ip);
 -}
 +		boot_error = apic->wakeup_secondary_cpu(apicid, start_ip);
 +	else
 +		boot_error = wakeup_cpu_via_init_nmi(cpu, start_ip, apicid,
 +						     cpu0_nmi_registered);
  
 -static int wait_cpu_cpumask(unsigned int cpu, const struct cpumask *mask)
 -{
 -	unsigned long timeout;
 +	if (!boot_error) {
 +		/*
 +		 * Wait 10s total for first sign of life from AP
 +		 */
 +		boot_error = -1;
 +		timeout = jiffies + 10*HZ;
 +		while (time_before(jiffies, timeout)) {
 +			if (cpumask_test_cpu(cpu, cpu_initialized_mask)) {
 +				/*
 +				 * Tell AP to proceed with initialization
 +				 */
 +				cpumask_set_cpu(cpu, cpu_callout_mask);
 +				boot_error = 0;
 +				break;
 +			}
 +			schedule();
 +		}
 +	}
  
 -	/*
 -	 * Wait up to 10s for the CPU to report in.
 -	 */
 -	timeout = jiffies + 10*HZ;
 -	while (time_before(jiffies, timeout)) {
 -		if (cpumask_test_cpu(cpu, mask))
 -			return 0;
 +	if (!boot_error) {
 +		/*
 +		 * Wait till AP completes initial initialization
 +		 */
 +		while (!cpumask_test_cpu(cpu, cpu_callin_mask)) {
 +			/*
 +			 * Allow other tasks to run while we wait for the
 +			 * AP to come online. This also gives a chance
 +			 * for the MTRR work(triggered by the AP coming online)
 +			 * to be completed in the stop machine context.
 +			 */
 +			schedule();
 +		}
 +	}
  
 -		schedule();
 +	if (x86_platform.legacy.warm_reset) {
 +		/*
 +		 * Cleanup possible dangling ends...
 +		 */
 +		smpboot_restore_warm_reset_vector();
  	}
 -	return -1;
 +
 +	return boot_error;
  }
  
++<<<<<<< HEAD
 +int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
++=======
+ /*
+  * Bringup step two: Wait for the target AP to reach cpu_init_secondary()
+  * and thus wait_for_master_cpu(), then set cpu_callout_mask to allow it
+  * to proceed.  The AP will then proceed past setting its 'callin' bit
+  * and end up waiting in check_tsc_sync_target() until we reach
+  * do_wait_cpu_online() to tend to it.
+  */
+ static int wait_cpu_initialized(unsigned int cpu)
+ {
+ 	/*
+ 	 * Wait for first sign of life from AP.
+ 	 */
+ 	if (wait_cpu_cpumask(cpu, cpu_initialized_mask))
+ 		return -1;
+ 
+ 	cpumask_set_cpu(cpu, cpu_callout_mask);
+ 	return 0;
+ }
+ 
+ /*
+  * Bringup step three: Wait for the target AP to reach smp_callin().
+  * The AP is not waiting for us here so we don't need to parallelise
+  * this step. Not entirely clear why we care about this, since we just
+  * proceed directly to TSC synchronization which is the next sync
+  * point with the AP anyway.
+  */
+ static void wait_cpu_callin(unsigned int cpu)
+ {
+ 	while (!cpumask_test_cpu(cpu, cpu_callin_mask))
+ 		schedule();
+ }
+ 
+ /*
+  * Bringup step four: Wait for the target AP to reach set_cpu_online() in
+  * start_secondary().
+  */
+ static void wait_cpu_online(unsigned int cpu)
+ {
+ 	/*
+ 	 * Wait for the AP to mark itself online, so the core caller
+ 	 * can drop sparse_irq_lock.
+ 	 */
+ 	while (!cpu_online(cpu))
+ 		schedule();
+ }
+ 
+ static int native_kick_ap(unsigned int cpu, struct task_struct *tidle)
++>>>>>>> 9d349d47f0e3 (x86/smpboot: Make TSC synchronization function call based)
  {
  	int apicid = apic->cpu_present_to_apicid(cpu);
 -	int err;
 +	int cpu0_nmi_registered = 0;
 +	unsigned long flags;
 +	int err, ret = 0;
  
  	lockdep_assert_irqs_enabled();
  
diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h
index 8a0c25c6bf09..f4a6d116ba88 100644
--- a/arch/x86/include/asm/tsc.h
+++ b/arch/x86/include/asm/tsc.h
@@ -58,12 +58,10 @@ extern bool tsc_async_resets;
 #ifdef CONFIG_X86_TSC
 extern bool tsc_store_and_check_tsc_adjust(bool bootcpu);
 extern void tsc_verify_tsc_adjust(bool resume);
-extern void check_tsc_sync_source(int cpu);
 extern void check_tsc_sync_target(void);
 #else
 static inline bool tsc_store_and_check_tsc_adjust(bool bootcpu) { return false; }
 static inline void tsc_verify_tsc_adjust(bool resume) { }
-static inline void check_tsc_sync_source(int cpu) { }
 static inline void check_tsc_sync_target(void) { }
 #endif
 
* Unmerged path arch/x86/kernel/smpboot.c
diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c
index 37b291725df9..2f0b1ad84127 100644
--- a/arch/x86/kernel/tsc_sync.c
+++ b/arch/x86/kernel/tsc_sync.c
@@ -245,7 +245,6 @@ bool tsc_store_and_check_tsc_adjust(bool bootcpu)
  */
 static atomic_t start_count;
 static atomic_t stop_count;
-static atomic_t skip_test;
 static atomic_t test_runs;
 
 /*
@@ -345,20 +344,13 @@ static inline unsigned int loop_timeout(int cpu)
 }
 
 /*
- * Source CPU calls into this - it waits for the freshly booted
- * target CPU to arrive and then starts the measurement:
+ * The freshly booted CPU initiates this via an async SMP function call.
  */
-void check_tsc_sync_source(int cpu)
+static void check_tsc_sync_source(void *__cpu)
 {
+	unsigned int cpu = (unsigned long)__cpu;
 	int cpus = 2;
 
-	/*
-	 * No need to check if we already know that the TSC is not
-	 * synchronized or if we have no TSC.
-	 */
-	if (unsynchronized_tsc())
-		return;
-
 	/*
 	 * Set the maximum number of test runs to
 	 *  1 if the CPU does not provide the TSC_ADJUST MSR
@@ -369,16 +361,9 @@ void check_tsc_sync_source(int cpu)
 	else
 		atomic_set(&test_runs, 3);
 retry:
-	/*
-	 * Wait for the target to start or to skip the test:
-	 */
-	while (atomic_read(&start_count) != cpus - 1) {
-		if (atomic_read(&skip_test) > 0) {
-			atomic_set(&skip_test, 0);
-			return;
-		}
+	/* Wait for the target to start. */
+	while (atomic_read(&start_count) != cpus - 1)
 		cpu_relax();
-	}
 
 	/*
 	 * Trigger the target to continue into the measurement too:
@@ -398,14 +383,14 @@ void check_tsc_sync_source(int cpu)
 	if (!nr_warps) {
 		atomic_set(&test_runs, 0);
 
-		pr_debug("TSC synchronization [CPU#%d -> CPU#%d]: passed\n",
+		pr_debug("TSC synchronization [CPU#%d -> CPU#%u]: passed\n",
 			smp_processor_id(), cpu);
 
 	} else if (atomic_dec_and_test(&test_runs) || random_warps) {
 		/* Force it to 0 if random warps brought us here */
 		atomic_set(&test_runs, 0);
 
-		pr_warn("TSC synchronization [CPU#%d -> CPU#%d]:\n",
+		pr_warn("TSC synchronization [CPU#%d -> CPU#%u]:\n",
 			smp_processor_id(), cpu);
 		pr_warn("Measured %Ld cycles TSC warp between CPUs, "
 			"turning off TSC clock.\n", max_warp);
@@ -458,11 +443,12 @@ void check_tsc_sync_target(void)
 	 * SoCs the TSC is frequency synchronized, but still the TSC ADJUST
 	 * register might have been wreckaged by the BIOS..
 	 */
-	if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable) {
-		atomic_inc(&skip_test);
+	if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable)
 		return;
-	}
 
+	/* Kick the control CPU into the TSC synchronization function */
+	smp_call_function_single(cpumask_first(cpu_online_mask), check_tsc_sync_source,
+				 (unsigned long *)(unsigned long)cpu, 0);
 retry:
 	/*
 	 * Register this CPU's participation and wait for the
