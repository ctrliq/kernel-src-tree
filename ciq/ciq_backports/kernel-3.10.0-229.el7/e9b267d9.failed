blk-mq: add ->init_request and ->exit_request methods

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-229.el7
commit-author Christoph Hellwig <hch@lst.de>
commit e9b267d91f6ddbc694cb40aa962b0b2cec03971d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-229.el7/e9b267d9.failed

The current blk_mq_init_commands/blk_mq_free_commands interface has a
two problems:

 1) Because only the constructor is passed to blk_mq_init_commands there
    is no easy way to clean up when a comman initialization failed.  The
    current code simply leaks the allocations done in the constructor.

 2) There is no good place to call blk_mq_free_commands: before
    blk_cleanup_queue there is no guarantee that all outstanding
    commands have completed, so we can't free them yet.  After
    blk_cleanup_queue the queue has usually been freed.  This can be
    worked around by grabbing an unconditional reference before calling
    blk_cleanup_queue and dropping it after blk_mq_free_commands is
    done, although that's not exatly pretty and driver writers are
    guaranteed to get it wrong sooner or later.

Both issues are easily fixed by making the request constructor and
destructor normal blk_mq_ops methods.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jens Axboe <axboe@fb.com>
(cherry picked from commit e9b267d91f6ddbc694cb40aa962b0b2cec03971d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
#	drivers/block/virtio_blk.c
diff --cc block/blk-mq.c
index f4a318f9cbbe,48d2d8495f5e..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1021,77 -1029,22 +1021,23 @@@ static int blk_mq_hctx_notify(void *dat
  
  	blk_mq_run_hw_queue(hctx, true);
  	blk_mq_put_ctx(ctx);
 +	return NOTIFY_OK;
  }
  
- static int blk_mq_init_hw_commands(struct blk_mq_hw_ctx *hctx,
- 				   int (*init)(void *, struct blk_mq_hw_ctx *,
- 					struct request *, unsigned int),
- 				   void *data)
+ static void blk_mq_free_rq_map(struct blk_mq_hw_ctx *hctx, void *driver_data)
  {
- 	unsigned int i;
- 	int ret = 0;
- 
- 	for (i = 0; i < hctx->queue_depth; i++) {
- 		struct request *rq = hctx->rqs[i];
- 
- 		ret = init(data, hctx, rq, i);
- 		if (ret)
- 			break;
- 	}
- 
- 	return ret;
- }
- 
- int blk_mq_init_commands(struct request_queue *q,
- 			 int (*init)(void *, struct blk_mq_hw_ctx *,
- 					struct request *, unsigned int),
- 			 void *data)
- {
- 	struct blk_mq_hw_ctx *hctx;
- 	unsigned int i;
- 	int ret = 0;
- 
- 	queue_for_each_hw_ctx(q, hctx, i) {
- 		ret = blk_mq_init_hw_commands(hctx, init, data);
- 		if (ret)
- 			break;
- 	}
- 
- 	return ret;
- }
- EXPORT_SYMBOL(blk_mq_init_commands);
- 
- static void blk_mq_free_hw_commands(struct blk_mq_hw_ctx *hctx,
- 				    void (*free)(void *, struct blk_mq_hw_ctx *,
- 					struct request *, unsigned int),
- 				    void *data)
- {
- 	unsigned int i;
+ 	struct page *page;
  
- 	for (i = 0; i < hctx->queue_depth; i++) {
- 		struct request *rq = hctx->rqs[i];
+ 	if (hctx->rqs && hctx->queue->mq_ops->exit_request) {
+ 		int i;
  
- 		free(data, hctx, rq, i);
+ 		for (i = 0; i < hctx->queue_depth; i++) {
+ 			if (!hctx->rqs[i])
+ 				continue;
+ 			hctx->queue->mq_ops->exit_request(driver_data, hctx,
+ 							  hctx->rqs[i], i);
+ 		}
  	}
- }
- 
- void blk_mq_free_commands(struct request_queue *q,
- 			  void (*free)(void *, struct blk_mq_hw_ctx *,
- 					struct request *, unsigned int),
- 			  void *data)
- {
- 	struct blk_mq_hw_ctx *hctx;
- 	unsigned int i;
- 
- 	queue_for_each_hw_ctx(q, hctx, i)
- 		blk_mq_free_hw_commands(hctx, free, data);
- }
- EXPORT_SYMBOL(blk_mq_free_commands);
- 
- static void blk_mq_free_rq_map(struct blk_mq_hw_ctx *hctx)
- {
- 	struct page *page;
  
  	while (!list_empty(&hctx->page_list)) {
  		page = list_first_entry(&hctx->page_list, struct page, lru);
@@@ -1162,7 -1122,14 +1110,18 @@@ static int blk_mq_init_rq_map(struct bl
  		left -= to_do * rq_size;
  		for (j = 0; j < to_do; j++) {
  			hctx->rqs[i] = p;
++<<<<<<< HEAD
 +			blk_mq_rq_init(hctx, hctx->rqs[i]);
++=======
+ 			blk_rq_init(hctx->queue, hctx->rqs[i]);
+ 			if (reg->ops->init_request) {
+ 				error = reg->ops->init_request(driver_data,
+ 						hctx, hctx->rqs[i], i);
+ 				if (error)
+ 					goto err_rq_map;
+ 			}
+ 
++>>>>>>> e9b267d91f6d (blk-mq: add ->init_request and ->exit_request methods)
  			p += rq_size;
  			i++;
  		}
@@@ -1256,9 -1227,8 +1219,9 @@@ static int blk_mq_init_hw_queues(struc
  			reg->ops->exit_hctx(hctx, j);
  
  		blk_mq_unregister_cpu_notifier(&hctx->cpu_notifier);
- 		blk_mq_free_rq_map(hctx);
+ 		blk_mq_free_rq_map(hctx, driver_data);
  		kfree(hctx->ctxs);
 +		kfree(hctx->ctx_map);
  	}
  
  	return 1;
diff --cc drivers/block/virtio_blk.c
index 6d8a87f252de,d06206abd340..000000000000
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@@ -497,16 -508,6 +508,19 @@@ static struct blk_mq_reg virtio_mq_reg 
  };
  module_param_named(queue_depth, virtio_mq_reg.queue_depth, uint, 0444);
  
++<<<<<<< HEAD
 +static int virtblk_init_vbr(void *data, struct blk_mq_hw_ctx *hctx,
 +			     struct request *rq, unsigned int nr)
 +{
 +	struct virtio_blk *vblk = data;
 +	struct virtblk_req *vbr = rq->special;
 +
 +	sg_init_table(vbr->sg, vblk->sg_elems);
 +	return 0;
 +}
 +
++=======
++>>>>>>> e9b267d91f6d (blk-mq: add ->init_request and ->exit_request methods)
  static int virtblk_probe(struct virtio_device *vdev)
  {
  	struct virtio_blk *vblk;
* Unmerged path block/blk-mq.c
* Unmerged path drivers/block/virtio_blk.c
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index f63ea3c3d292..b3b6b486f1f3 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -67,6 +67,10 @@ typedef struct blk_mq_hw_ctx *(alloc_hctx_fn)(struct blk_mq_reg *,unsigned int);
 typedef void (free_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
 typedef int (init_hctx_fn)(struct blk_mq_hw_ctx *, void *, unsigned int);
 typedef void (exit_hctx_fn)(struct blk_mq_hw_ctx *, unsigned int);
+typedef int (init_request_fn)(void *, struct blk_mq_hw_ctx *,
+		struct request *, unsigned int);
+typedef void (exit_request_fn)(void *, struct blk_mq_hw_ctx *,
+		struct request *, unsigned int);
 
 struct blk_mq_ops {
 	/*
@@ -99,6 +103,14 @@ struct blk_mq_ops {
 	 */
 	init_hctx_fn		*init_hctx;
 	exit_hctx_fn		*exit_hctx;
+
+	/*
+	 * Called for every command allocated by the block layer to allow
+	 * the driver to set up driver specific data.
+	 * Ditto for exit/teardown.
+	 */
+	init_request_fn		*init_request;
+	exit_request_fn		*exit_request;
 };
 
 enum {
@@ -117,8 +129,6 @@ enum {
 struct request_queue *blk_mq_init_queue(struct blk_mq_reg *, void *);
 int blk_mq_register_disk(struct gendisk *);
 void blk_mq_unregister_disk(struct gendisk *);
-int blk_mq_init_commands(struct request_queue *, int (*init)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
-void blk_mq_free_commands(struct request_queue *, void (*free)(void *data, struct blk_mq_hw_ctx *, struct request *, unsigned int), void *data);
 
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);
 
