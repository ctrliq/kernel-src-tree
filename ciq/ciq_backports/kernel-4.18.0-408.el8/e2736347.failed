dm: factor out dm_io_complete

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Mike Snitzer <snitzer@redhat.com>
commit e27363472f9bc96db831ceb2c31cf8b9a7c5b6f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/e2736347.failed

Optimizes dm_io_dec_pending() slightly by avoiding local variables.

	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit e27363472f9bc96db831ceb2c31cf8b9a7c5b6f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm.c
diff --cc drivers/md/dm.c
index e7cb1b8972bd,f8df6e3bcdc5..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -872,66 -917,18 +942,76 @@@ static void dm_io_complete(struct dm_i
   */
  void dm_io_dec_pending(struct dm_io *io, blk_status_t error)
  {
++<<<<<<< HEAD
 +	unsigned long flags;
 +	blk_status_t io_error;
 +	struct bio *bio;
 +	struct mapped_device *md = io->md;
 +	unsigned long start_time = 0;
 +	struct dm_stats_aux stats_aux;
 +
++=======
++>>>>>>> e27363472f9b (dm: factor out dm_io_complete)
  	/* Push-back supersedes any I/O errors */
  	if (unlikely(error)) {
+ 		unsigned long flags;
  		spin_lock_irqsave(&io->endio_lock, flags);
- 		if (!(io->status == BLK_STS_DM_REQUEUE && __noflush_suspending(md)))
+ 		if (!(io->status == BLK_STS_DM_REQUEUE &&
+ 		      __noflush_suspending(io->md)))
  			io->status = error;
  		spin_unlock_irqrestore(&io->endio_lock, flags);
  	}
  
++<<<<<<< HEAD
 +	if (atomic_dec_and_test(&io->io_count)) {
 +		if (io->status == BLK_STS_DM_REQUEUE) {
 +			/*
 +			 * Target requested pushing back the I/O.
 +			 */
 +			spin_lock_irqsave(&md->deferred_lock, flags);
 +			if (__noflush_suspending(md))
 +				/* NOTE early return due to BLK_STS_DM_REQUEUE below */
 +				bio_list_add_head(&md->deferred, io->orig_bio);
 +			else
 +				/* noflush suspend was interrupted. */
 +				io->status = BLK_STS_IOERR;
 +			spin_unlock_irqrestore(&md->deferred_lock, flags);
 +		}
 +
 +		io_error = io->status;
 +		bio = io->orig_bio;
 +		start_time = io->start_time;
 +		stats_aux = io->stats_aux;
 +		free_io(md, io);
 +		end_io_acct(md, bio, start_time, &stats_aux);
 +		smp_wmb();
 +		this_cpu_dec(*md->pending_io);
 +
 +		/* nudge anyone waiting on suspend queue */
 +		if (unlikely(wq_has_sleeper(&md->wait)))
 +			wake_up(&md->wait);
 +
 +		if (io_error == BLK_STS_DM_REQUEUE)
 +			return;
 +
 +		if (bio_is_flush_with_data(bio)) {
 +			/*
 +			 * Preflush done for flush with data, reissue
 +			 * without REQ_PREFLUSH.
 +			 */
 +			bio->bi_opf &= ~REQ_PREFLUSH;
 +			queue_io(md, bio);
 +		} else {
 +			/* done with normal IO or empty flush */
 +			if (io_error)
 +				bio->bi_status = io_error;
 +			bio_endio(bio);
 +		}
 +	}
++=======
+ 	if (atomic_dec_and_test(&io->io_count))
+ 		dm_io_complete(io);
++>>>>>>> e27363472f9b (dm: factor out dm_io_complete)
  }
  
  void disable_discard(struct mapped_device *md)
@@@ -1619,44 -1567,50 +1699,87 @@@ static blk_qc_t __split_and_process_bio
  
  	if (bio->bi_opf & REQ_PREFLUSH) {
  		error = __send_empty_flush(&ci);
++<<<<<<< HEAD
 +		/* dm_io_dec_pending submits any data associated with flush */
 +	} else if (op_is_zone_mgmt(bio_op(bio))) {
 +		ci.bio = bio;
 +		ci.sector_count = 0;
 +		error = __split_and_process_non_flush(&ci);
 +	} else {
 +		ci.bio = bio;
 +		ci.sector_count = bio_sectors(bio);
 +		error = __split_and_process_non_flush(&ci);
 +		if (ci.sector_count && !error) {
 +			/*
 +			 * Remainder must be passed to generic_make_request()
 +			 * so that it gets handled *after* bios already submitted
 +			 * have been completely processed.
 +			 * We take a clone of the original to store in
 +			 * ci.io->orig_bio to be used by end_io_acct() and
 +			 * for dec_pending to use for completion handling.
 +			 */
 +			struct bio *b = bio_split(bio, bio_sectors(bio) - ci.sector_count,
 +						  GFP_NOIO, &md->queue->bio_split);
 +			ci.io->orig_bio = b;
 +
 +			bio_chain(b, bio);
 +			trace_block_split(md->queue, b, bio->bi_iter.bi_sector);
 +			ret = generic_make_request(bio);
 +		}
++=======
+ 		/* dm_io_complete submits any data associated with flush */
+ 		goto out;
++>>>>>>> e27363472f9b (dm: factor out dm_io_complete)
  	}
 +	start_io_acct(ci.io);
  
++<<<<<<< HEAD
 +	/* drop the extra reference count */
 +	dm_io_dec_pending(ci.io, errno_to_blk_status(error));
 +	return ret;
++=======
+ 	error = __split_and_process_bio(&ci);
+ 	ci.io->map_task = NULL;
+ 	if (error || !ci.sector_count)
+ 		goto out;
+ 
+ 	/*
+ 	 * Remainder must be passed to submit_bio_noacct() so it gets handled
+ 	 * *after* bios already submitted have been completely processed.
+ 	 * We take a clone of the original to store in ci.io->orig_bio to be
+ 	 * used by dm_end_io_acct() and for dm_io_complete() to use for
+ 	 * completion handling.
+ 	 */
+ 	orig_bio = bio_split(bio, bio_sectors(bio) - ci.sector_count,
+ 			     GFP_NOIO, &md->queue->bio_split);
+ 	bio_chain(orig_bio, bio);
+ 	trace_block_split(orig_bio, bio->bi_iter.bi_sector);
+ 	submit_bio_noacct(bio);
+ out:
+ 	if (!orig_bio)
+ 		orig_bio = bio;
+ 	smp_store_release(&ci.io->orig_bio, orig_bio);
+ 	if (ci.io->start_io_acct)
+ 		dm_start_io_acct(ci.io, NULL);
+ 
+ 	/*
+ 	 * Drop the extra reference count for non-POLLED bio, and hold one
+ 	 * reference for POLLED bio, which will be released in dm_poll_bio
+ 	 *
+ 	 * Add every dm_io instance into the hlist_head which is stored in
+ 	 * bio->bi_private, so that dm_poll_bio can poll them all.
+ 	 */
+ 	if (error || !ci.submit_as_polled)
+ 		dm_io_dec_pending(ci.io, errno_to_blk_status(error));
+ 	else
+ 		dm_queue_poll_io(bio, ci.io);
++>>>>>>> e27363472f9b (dm: factor out dm_io_complete)
  }
  
 -static void dm_submit_bio(struct bio *bio)
 +static blk_qc_t dm_make_request(struct request_queue *q, struct bio *bio)
  {
 -	struct mapped_device *md = bio->bi_bdev->bd_disk->private_data;
 +	struct mapped_device *md = q->queuedata;
 +	blk_qc_t ret = BLK_QC_T_NONE;
  	int srcu_idx;
  	struct dm_table *map;
  
* Unmerged path drivers/md/dm.c
