dmaengine: idxd: remove interrupt flag for completion list spinlock

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 9fce3b3a0ab4cad407a27b5e36603c23f1b5b278
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/9fce3b3a.failed

The list lock is never acquired in interrupt context. Therefore there is no
need to disable interrupts. Remove interrupt flags for lock operations.

	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/162826417450.3454650.3733188117742416238.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 9fce3b3a0ab4cad407a27b5e36603c23f1b5b278)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/irq.c
#	drivers/dma/idxd/submit.c
diff --cc drivers/dma/idxd/irq.c
index 7a2cf0512501,d221c2e37460..000000000000
--- a/drivers/dma/idxd/irq.c
+++ b/drivers/dma/idxd/irq.c
@@@ -257,63 -176,47 +257,78 @@@ static int irq_process_pending_llist(st
  {
  	struct idxd_desc *desc, *t;
  	struct llist_node *head;
 -
++<<<<<<< HEAD
 +	int queued = 0;
 +	unsigned long flags;
 +	enum idxd_complete_type reason;
++=======
++>>>>>>> 9fce3b3a0ab4 (dmaengine: idxd: remove interrupt flag for completion list spinlock)
 +
 +	*processed = 0;
  	head = llist_del_all(&irq_entry->pending_llist);
  	if (!head)
 -		return;
 -
 -	llist_for_each_entry_safe(desc, t, head, llnode) {
 -		u8 status = desc->completion->status & DSA_COMP_STATUS_MASK;
 +		goto out;
  
 -		if (status) {
 -			/*
 -			 * Check against the original status as ABORT is software defined
 -			 * and 0xff, which DSA_COMP_STATUS_MASK can mask out.
 -			 */
 -			if (unlikely(desc->completion->status == IDXD_COMP_DESC_ABORT)) {
 -				complete_desc(desc, IDXD_COMPLETE_ABORT);
 -				continue;
 -			}
 +	if (wtype == IRQ_WORK_NORMAL)
 +		reason = IDXD_COMPLETE_NORMAL;
 +	else
 +		reason = IDXD_COMPLETE_DEV_FAIL;
  
 -			complete_desc(desc, IDXD_COMPLETE_NORMAL);
 +	llist_for_each_entry_safe(desc, t, head, llnode) {
 +		if (desc->completion->status) {
 +			if ((desc->completion->status & DSA_COMP_STATUS_MASK) != DSA_COMP_SUCCESS)
 +				match_fault(desc, data);
 +			complete_desc(desc, reason);
 +			(*processed)++;
  		} else {
- 			spin_lock_irqsave(&irq_entry->list_lock, flags);
+ 			spin_lock(&irq_entry->list_lock);
  			list_add_tail(&desc->list,
  				      &irq_entry->work_list);
++<<<<<<< HEAD
 +			spin_unlock_irqrestore(&irq_entry->list_lock, flags);
 +			queued++;
++=======
+ 			spin_unlock(&irq_entry->list_lock);
++>>>>>>> 9fce3b3a0ab4 (dmaengine: idxd: remove interrupt flag for completion list spinlock)
  		}
  	}
 +
 + out:
 +	return queued;
  }
  
 -static void irq_process_work_list(struct idxd_irq_entry *irq_entry)
 +static int irq_process_work_list(struct idxd_irq_entry *irq_entry,
 +				 enum irq_work_type wtype,
 +				 int *processed, u64 data)
  {
++<<<<<<< HEAD
 +	int queued = 0;
 +	unsigned long flags;
++=======
++>>>>>>> 9fce3b3a0ab4 (dmaengine: idxd: remove interrupt flag for completion list spinlock)
  	LIST_HEAD(flist);
  	struct idxd_desc *desc, *n;
 +	enum idxd_complete_type reason;
 +
 +	*processed = 0;
 +	if (wtype == IRQ_WORK_NORMAL)
 +		reason = IDXD_COMPLETE_NORMAL;
 +	else
 +		reason = IDXD_COMPLETE_DEV_FAIL;
  
  	/*
  	 * This lock protects list corruption from access of list outside of the irq handler
  	 * thread.
  	 */
- 	spin_lock_irqsave(&irq_entry->list_lock, flags);
+ 	spin_lock(&irq_entry->list_lock);
  	if (list_empty(&irq_entry->work_list)) {
++<<<<<<< HEAD
 +		spin_unlock_irqrestore(&irq_entry->list_lock, flags);
 +		return 0;
++=======
+ 		spin_unlock(&irq_entry->list_lock);
+ 		return;
++>>>>>>> 9fce3b3a0ab4 (dmaengine: idxd: remove interrupt flag for completion list spinlock)
  	}
  
  	list_for_each_entry_safe(desc, n, &irq_entry->work_list, list) {
@@@ -326,20 -226,25 +341,20 @@@
  		}
  	}
  
- 	spin_unlock_irqrestore(&irq_entry->list_lock, flags);
+ 	spin_unlock(&irq_entry->list_lock);
  
  	list_for_each_entry(desc, &flist, list) {
 -		/*
 -		 * Check against the original status as ABORT is software defined
 -		 * and 0xff, which DSA_COMP_STATUS_MASK can mask out.
 -		 */
 -		if (unlikely(desc->completion->status == IDXD_COMP_DESC_ABORT)) {
 -			complete_desc(desc, IDXD_COMPLETE_ABORT);
 -			continue;
 -		}
 -
 -		complete_desc(desc, IDXD_COMPLETE_NORMAL);
 +		if ((desc->completion->status & DSA_COMP_STATUS_MASK) != DSA_COMP_SUCCESS)
 +			match_fault(desc, data);
 +		complete_desc(desc, reason);
  	}
 +
 +	return queued;
  }
  
 -irqreturn_t idxd_wq_thread(int irq, void *data)
 +static int idxd_desc_process(struct idxd_irq_entry *irq_entry)
  {
 -	struct idxd_irq_entry *irq_entry = data;
 +	int rc, processed, total = 0;
  
  	/*
  	 * There are two lists we are processing. The pending_llist is where
diff --cc drivers/dma/idxd/submit.c
index 0afcd1322339,4b514c63af15..000000000000
--- a/drivers/dma/idxd/submit.c
+++ b/drivers/dma/idxd/submit.c
@@@ -88,6 -79,59 +88,62 @@@ void idxd_free_desc(struct idxd_wq *wq
  	sbitmap_queue_clear(&wq->sbq, desc->id, cpu);
  }
  
++<<<<<<< HEAD
++=======
+ static struct idxd_desc *list_abort_desc(struct idxd_wq *wq, struct idxd_irq_entry *ie,
+ 					 struct idxd_desc *desc)
+ {
+ 	struct idxd_desc *d, *n;
+ 
+ 	lockdep_assert_held(&ie->list_lock);
+ 	list_for_each_entry_safe(d, n, &ie->work_list, list) {
+ 		if (d == desc) {
+ 			list_del(&d->list);
+ 			return d;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * At this point, the desc needs to be aborted is held by the completion
+ 	 * handler where it has taken it off the pending list but has not added to the
+ 	 * work list. It will be cleaned up by the interrupt handler when it sees the
+ 	 * IDXD_COMP_DESC_ABORT for completion status.
+ 	 */
+ 	return NULL;
+ }
+ 
+ static void llist_abort_desc(struct idxd_wq *wq, struct idxd_irq_entry *ie,
+ 			     struct idxd_desc *desc)
+ {
+ 	struct idxd_desc *d, *t, *found = NULL;
+ 	struct llist_node *head;
+ 
+ 	desc->completion->status = IDXD_COMP_DESC_ABORT;
+ 	/*
+ 	 * Grab the list lock so it will block the irq thread handler. This allows the
+ 	 * abort code to locate the descriptor need to be aborted.
+ 	 */
+ 	spin_lock(&ie->list_lock);
+ 	head = llist_del_all(&ie->pending_llist);
+ 	if (head) {
+ 		llist_for_each_entry_safe(d, t, head, llnode) {
+ 			if (d == desc) {
+ 				found = desc;
+ 				continue;
+ 			}
+ 			list_add_tail(&desc->list, &ie->work_list);
+ 		}
+ 	}
+ 
+ 	if (!found)
+ 		found = list_abort_desc(wq, ie, desc);
+ 	spin_unlock(&ie->list_lock);
+ 
+ 	if (found)
+ 		complete_desc(found, IDXD_COMPLETE_ABORT);
+ }
+ 
++>>>>>>> 9fce3b3a0ab4 (dmaengine: idxd: remove interrupt flag for completion list spinlock)
  int idxd_submit_desc(struct idxd_wq *wq, struct idxd_desc *desc)
  {
  	struct idxd_device *idxd = wq->idxd;
* Unmerged path drivers/dma/idxd/irq.c
* Unmerged path drivers/dma/idxd/submit.c
