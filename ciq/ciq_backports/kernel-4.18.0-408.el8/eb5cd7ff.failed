KVM: MMU: remove unnecessary argument to mmu_set_spte

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit eb5cd7ffe142a989c77e9989e3e9ea986dc418aa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/eb5cd7ff.failed

The level of the new SPTE can be found in the kvm_mmu_page struct; there
is no need to pass it down.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit eb5cd7ffe142a989c77e9989e3e9ea986dc418aa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/paging_tmpl.h
diff --cc arch/x86/kvm/mmu/mmu.c
index 8ec545b124be,91303006faaf..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2652,40 -2674,18 +2652,45 @@@ int mmu_try_to_unsync_pages(struct kvm_
  	return 0;
  }
  
 +static int set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 +		    unsigned int pte_access, int level,
 +		    gfn_t gfn, kvm_pfn_t pfn, bool speculative,
 +		    bool can_unsync, bool host_writable)
 +{
 +	u64 spte;
 +	struct kvm_mmu_page *sp;
 +	int ret;
 +
 +	sp = sptep_to_sp(sptep);
 +
 +	ret = make_spte(vcpu, pte_access, level, gfn, pfn, *sptep, speculative,
 +			can_unsync, host_writable, sp_ad_disabled(sp), &spte);
 +
 +	if (spte & PT_WRITABLE_MASK)
 +		kvm_vcpu_mark_page_dirty(vcpu, gfn);
 +
 +	if (*sptep == spte)
 +		ret |= SET_SPTE_SPURIOUS;
 +	else if (mmu_spte_update(sptep, spte))
 +		ret |= SET_SPTE_NEED_REMOTE_TLB_FLUSH;
 +	return ret;
 +}
 +
  static int mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
- 			unsigned int pte_access, bool write_fault, int level,
+ 			unsigned int pte_access, bool write_fault,
  			gfn_t gfn, kvm_pfn_t pfn, bool speculative,
  			bool host_writable)
  {
++<<<<<<< HEAD
++=======
+ 	struct kvm_mmu_page *sp = sptep_to_sp(sptep);
+ 	int level = sp->role.level;
++>>>>>>> eb5cd7ffe142 (KVM: MMU: remove unnecessary argument to mmu_set_spte)
  	int was_rmapped = 0;
 +	int rmap_count;
 +	int set_spte_ret;
  	int ret = RET_PF_FIXED;
  	bool flush = false;
 -	bool wrprot;
 -	u64 spte;
  
  	pgprintk("%s: spte %llx write_fault %d gfn %llx\n", __func__,
  		 *sptep, write_fault, gfn);
@@@ -2996,9 -2977,12 +3001,14 @@@ static int __direct_map(struct kvm_vcp
  			account_huge_nx_page(vcpu->kvm, sp);
  	}
  
 -	if (WARN_ON_ONCE(it.level != fault->goal_level))
 -		return -EFAULT;
 -
  	ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
++<<<<<<< HEAD
 +			   write, level, base_gfn, pfn, prefault,
 +			   map_writable);
++=======
+ 			   fault->write, base_gfn, fault->pfn,
+ 			   fault->prefault, fault->map_writable);
++>>>>>>> eb5cd7ffe142 (KVM: MMU: remove unnecessary argument to mmu_set_spte)
  	if (ret == RET_PF_SPURIOUS)
  		return ret;
  
diff --cc arch/x86/kvm/mmu/paging_tmpl.h
index de3ee26beb48,7f2c6eeed04f..000000000000
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@@ -766,8 -760,12 +766,17 @@@ static int FNAME(fetch)(struct kvm_vcp
  		}
  	}
  
++<<<<<<< HEAD
 +	ret = mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault,
 +			   it.level, base_gfn, pfn, prefault, map_writable);
++=======
+ 	if (WARN_ON_ONCE(it.level != fault->goal_level))
+ 		return -EFAULT;
+ 
+ 	ret = mmu_set_spte(vcpu, it.sptep, gw->pte_access, fault->write,
+ 			   base_gfn, fault->pfn, fault->prefault,
+ 			   fault->map_writable);
++>>>>>>> eb5cd7ffe142 (KVM: MMU: remove unnecessary argument to mmu_set_spte)
  	if (ret == RET_PF_SPURIOUS)
  		return ret;
  
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/paging_tmpl.h
