KVM: MMU: clean up make_spte return value

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit ad67e4806e4c2d920e2045b3fafc60ddbc3017f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/ad67e480.failed

Now that make_spte is called directly by the shadow MMU (rather than
wrapped by set_spte), it only has to return one boolean value.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit ad67e4806e4c2d920e2045b3fafc60ddbc3017f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 8ec545b124be,dcbe7df2f890..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2681,11 -2679,12 +2681,16 @@@ static int mmu_set_spte(struct kvm_vcp
  			gfn_t gfn, kvm_pfn_t pfn, bool speculative,
  			bool host_writable)
  {
 -	struct kvm_mmu_page *sp = sptep_to_sp(sptep);
  	int was_rmapped = 0;
 +	int rmap_count;
 +	int set_spte_ret;
  	int ret = RET_PF_FIXED;
  	bool flush = false;
++<<<<<<< HEAD
++=======
+ 	bool wrprot;
+ 	u64 spte;
++>>>>>>> ad67e4806e4c (KVM: MMU: clean up make_spte return value)
  
  	pgprintk("%s: spte %llx write_fault %d gfn %llx\n", __func__,
  		 *sptep, write_fault, gfn);
@@@ -2716,9 -2715,17 +2721,23 @@@
  			was_rmapped = 1;
  	}
  
++<<<<<<< HEAD
 +	set_spte_ret = set_spte(vcpu, sptep, pte_access, level, gfn, pfn,
 +				speculative, true, host_writable);
 +	if (set_spte_ret & SET_SPTE_WRITE_PROTECTED_PT) {
++=======
+ 	wrprot = make_spte(vcpu, pte_access, level, gfn, pfn, *sptep, speculative,
+ 			   true, host_writable, sp_ad_disabled(sp), &spte);
+ 
+ 	if (*sptep == spte) {
+ 		ret = RET_PF_SPURIOUS;
+ 	} else {
+ 		trace_kvm_mmu_set_spte(level, gfn, sptep);
+ 		flush |= mmu_spte_update(sptep, spte);
+ 	}
+ 
+ 	if (wrprot) {
++>>>>>>> ad67e4806e4c (KVM: MMU: clean up make_spte return value)
  		if (write_fault)
  			ret = RET_PF_EMULATE;
  	}
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index bc5924587138,6de2c957edd6..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -869,14 -898,14 +869,20 @@@ static int tdp_mmu_map_handle_target_le
  {
  	u64 new_spte;
  	int ret = RET_PF_FIXED;
- 	int make_spte_ret = 0;
+ 	bool wrprot = false;
  
 -	if (unlikely(!fault->slot))
 +	if (unlikely(is_noslot_pfn(pfn)))
  		new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
  	else
++<<<<<<< HEAD
 +		make_spte_ret = make_spte(vcpu, ACC_ALL, iter->level, iter->gfn,
 +					 pfn, iter->old_spte, prefault, true,
 +					 map_writable, !shadow_accessed_mask,
++=======
+ 		wrprot = make_spte(vcpu, ACC_ALL, iter->level, iter->gfn,
+ 					 fault->pfn, iter->old_spte, fault->prefault, true,
+ 					 fault->map_writable, !shadow_accessed_mask,
++>>>>>>> ad67e4806e4c (KVM: MMU: clean up make_spte return value)
  					 &new_spte);
  
  	if (new_spte == iter->old_spte)
@@@ -889,8 -918,8 +895,13 @@@
  	 * protected, emulation is needed. If the emulation was skipped,
  	 * the vCPU would have the same fault again.
  	 */
++<<<<<<< HEAD
 +	if (make_spte_ret & SET_SPTE_WRITE_PROTECTED_PT) {
 +		if (write)
++=======
+ 	if (wrprot) {
+ 		if (fault->write)
++>>>>>>> ad67e4806e4c (KVM: MMU: clean up make_spte return value)
  			ret = RET_PF_EMULATE;
  	}
  
* Unmerged path arch/x86/kvm/mmu/mmu.c
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index 5ac367b7f49f..6cb11ac90b2b 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -155,11 +155,6 @@ enum {
 	RET_PF_SPURIOUS,
 };
 
-/* Bits which may be returned by set_spte() */
-#define SET_SPTE_WRITE_PROTECTED_PT	BIT(0)
-#define SET_SPTE_NEED_REMOTE_TLB_FLUSH	BIT(1)
-#define SET_SPTE_SPURIOUS		BIT(2)
-
 int kvm_mmu_max_mapping_level(struct kvm *kvm,
 			      const struct kvm_memory_slot *slot, gfn_t gfn,
 			      kvm_pfn_t pfn, int max_level);
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 86a21eb85d25..debc4d13abe6 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -89,13 +89,13 @@ static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)
 				     E820_TYPE_RAM);
 }
 
-int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
+bool make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 		     gfn_t gfn, kvm_pfn_t pfn, u64 old_spte, bool speculative,
 		     bool can_unsync, bool host_writable, bool ad_disabled,
 		     u64 *new_spte)
 {
 	u64 spte = SPTE_MMU_PRESENT_MASK;
-	int ret = 0;
+	bool wrprot = false;
 
 	if (ad_disabled)
 		spte |= SPTE_TDP_AD_DISABLED_MASK;
@@ -162,7 +162,7 @@ int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 		if (mmu_try_to_unsync_pages(vcpu, gfn, can_unsync)) {
 			pgprintk("%s: found shadow page for %llx, marking ro\n",
 				 __func__, gfn);
-			ret |= SET_SPTE_WRITE_PROTECTED_PT;
+			wrprot = true;
 			pte_access &= ~ACC_WRITE_MASK;
 			spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
 		}
@@ -180,7 +180,7 @@ int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 		  get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));
 
 	*new_spte = spte;
-	return ret;
+	return wrprot;
 }
 
 u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index eb7b227fc6cf..1998ec559196 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -334,12 +334,7 @@ static inline u64 get_mmio_spte_generation(u64 spte)
 	return gen;
 }
 
-/* Bits which may be returned by set_spte() */
-#define SET_SPTE_WRITE_PROTECTED_PT    BIT(0)
-#define SET_SPTE_NEED_REMOTE_TLB_FLUSH BIT(1)
-#define SET_SPTE_SPURIOUS              BIT(2)
-
-int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
+bool make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 		     gfn_t gfn, kvm_pfn_t pfn, u64 old_spte, bool speculative,
 		     bool can_unsync, bool host_writable, bool ad_disabled,
 		     u64 *new_spte);
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
