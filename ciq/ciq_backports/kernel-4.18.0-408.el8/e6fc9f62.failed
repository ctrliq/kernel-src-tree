dm: flag clones created by __send_duplicate_bios

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Mike Snitzer <snitzer@redhat.com>
commit e6fc9f62ce6e412acb1699a5373174aa42ca2bd3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/e6fc9f62.failed

Formally disallow dm_accept_partial_bio() on clones created by
__send_duplicate_bios() because their len_ptr points to a shared
unsigned int.  __send_duplicate_bios() is only used for flush bios
and other "abnormal" bios (discards, writezeroes, etc). And
dm_accept_partial_bio() already didn't support flush bios.

Also refactor __send_changing_extent_only() to reflect it cannot fail.
As such __send_changing_extent_only() can update the clone_info before
__send_duplicate_bios() is called to fan-out __map_bio() calls.

	Reviewed-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
(cherry picked from commit e6fc9f62ce6e412acb1699a5373174aa42ca2bd3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-core.h
#	drivers/md/dm.c
diff --cc drivers/md/dm-core.h
index a9c78c74b3c7,33ef92e90462..000000000000
--- a/drivers/md/dm-core.h
+++ b/drivers/md/dm-core.h
@@@ -202,7 -215,9 +202,13 @@@ struct dm_target_io 
  	struct dm_io *io;
  	struct dm_target *ti;
  	unsigned int *len_ptr;
++<<<<<<< HEAD
 +	bool inside_dm_io;
++=======
+ 	bool inside_dm_io:1;
+ 	bool is_duplicate_bio:1;
+ 	sector_t old_sector;
++>>>>>>> e6fc9f62ce6e (dm: flag clones created by __send_duplicate_bios)
  	struct bio clone;
  };
  
diff --cc drivers/md/dm.c
index b06624dd4339,d1c618c3f6c6..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -651,15 -586,24 +651,21 @@@ static struct dm_target_io *alloc_tio(s
  	tio->io = ci->io;
  	tio->ti = ti;
  	tio->target_bio_nr = target_bio_nr;
++<<<<<<< HEAD
++=======
+ 	tio->is_duplicate_bio = false;
+ 	tio->len_ptr = len;
+ 	tio->old_sector = 0;
++>>>>>>> e6fc9f62ce6e (dm: flag clones created by __send_duplicate_bios)
  
 -	if (len) {
 -		clone->bi_iter.bi_size = to_bytes(*len);
 -		if (bio_integrity(clone))
 -			bio_integrity_trim(clone);
 -	}
 -
 -	return clone;
 +	return tio;
  }
  
 -static void free_tio(struct bio *clone)
 +static void free_tio(struct dm_target_io *tio)
  {
 -	if (clone_to_tio(clone)->inside_dm_io)
 +	if (tio->inside_dm_io)
  		return;
 -	bio_put(clone);
 +	bio_put(&tio->clone);
  }
  
  /*
@@@ -1213,8 -1087,9 +1219,14 @@@ static int dm_dax_zero_page_range(struc
  
  /*
   * A target may call dm_accept_partial_bio only from the map routine.  It is
++<<<<<<< HEAD
 + * allowed for all bio types except REQ_PREFLUSH, REQ_OP_ZONE_RESET,
 + * REQ_OP_ZONE_OPEN, REQ_OP_ZONE_CLOSE and REQ_OP_ZONE_FINISH.
++=======
+  * allowed for all bio types except REQ_PREFLUSH, REQ_OP_ZONE_* zone management
+  * operations, REQ_OP_ZONE_APPEND (zone append writes) and any bio serviced by
+  * __send_duplicate_bios().
++>>>>>>> e6fc9f62ce6e (dm: flag clones created by __send_duplicate_bios)
   *
   * dm_accept_partial_bio informs the dm that the target only wants to process
   * additional n_sectors sectors of the bio and the rest of the data should be
@@@ -1242,11 -1117,15 +1254,18 @@@
   */
  void dm_accept_partial_bio(struct bio *bio, unsigned n_sectors)
  {
 -	struct dm_target_io *tio = clone_to_tio(bio);
 +	struct dm_target_io *tio = container_of(bio, struct dm_target_io, clone);
  	unsigned bi_size = bio->bi_iter.bi_size >> SECTOR_SHIFT;
++<<<<<<< HEAD
 +	BUG_ON(bio->bi_opf & REQ_PREFLUSH);
++=======
+ 
+ 	BUG_ON(tio->is_duplicate_bio);
+ 	BUG_ON(op_is_zone_mgmt(bio_op(bio)));
+ 	BUG_ON(bio_op(bio) == REQ_OP_ZONE_APPEND);
++>>>>>>> e6fc9f62ce6e (dm: flag clones created by __send_duplicate_bios)
  	BUG_ON(bi_size > *tio->len_ptr);
  	BUG_ON(n_sectors > bi_size);
 -
  	*tio->len_ptr -= bi_size - n_sectors;
  	bio->bi_iter.bi_size = n_sectors << SECTOR_SHIFT;
  }
@@@ -1428,14 -1241,23 +1447,32 @@@ static void __send_duplicate_bios(struc
  				  unsigned num_bios, unsigned *len)
  {
  	struct bio_list blist = BIO_EMPTY_LIST;
 -	struct bio *clone;
 +	struct bio *bio;
 +	struct dm_target_io *tio;
  
++<<<<<<< HEAD
 +	alloc_multiple_bios(&blist, ci, ti, num_bios);
 +
 +	while ((bio = bio_list_pop(&blist))) {
 +		tio = container_of(bio, struct dm_target_io, clone);
 +		(void) __clone_and_map_simple_bio(ci, tio, len);
++=======
+ 	switch (num_bios) {
+ 	case 0:
+ 		break;
+ 	case 1:
+ 		clone = alloc_tio(ci, ti, 0, len, GFP_NOIO);
+ 		clone_to_tio(clone)->is_duplicate_bio = true;
+ 		__map_bio(clone);
+ 		break;
+ 	default:
+ 		alloc_multiple_bios(&blist, ci, ti, num_bios, len);
+ 		while ((clone = bio_list_pop(&blist))) {
+ 			clone_to_tio(clone)->is_duplicate_bio = true;
+ 			__map_bio(clone);
+ 		}
+ 		break;
++>>>>>>> e6fc9f62ce6e (dm: flag clones created by __send_duplicate_bios)
  	}
  }
  
@@@ -1472,52 -1285,18 +1509,64 @@@ static int __send_empty_flush(struct cl
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int __clone_and_map_data_bio(struct clone_info *ci, struct dm_target *ti,
 +				    sector_t sector, unsigned *len)
 +{
 +	struct bio *bio = ci->bio;
 +	struct dm_target_io *tio;
 +	int r;
 +
 +	tio = alloc_tio(ci, ti, 0, GFP_NOIO);
 +	tio->len_ptr = len;
 +	r = clone_bio(tio, bio, sector, *len);
 +	if (r < 0) {
 +		free_tio(tio);
 +		return r;
 +	}
 +	(void) __map_bio(tio);
 +
 +	return 0;
 +}
 +
 +static bool is_split_required_for_discard(struct dm_target *ti)
 +{
 +	return ti->split_discard_bios;
 +}
 +
 +static int __send_changing_extent_only(struct clone_info *ci, struct dm_target *ti,
 +				       unsigned num_bios, bool is_split_required)
 +{
 +	unsigned len;
 +
 +	/*
 +	 * Even though the device advertised support for this type of
 +	 * request, that does not mean every target supports it, and
 +	 * reconfiguration might also have changed that since the
 +	 * check was performed.
 +	 */
 +	if (!num_bios)
 +		return -EOPNOTSUPP;
 +
 +	if (!is_split_required)
 +		len = min_t(sector_t, ci->sector_count,
 +			    max_io_len_target_boundary(ti, dm_target_offset(ti, ci->sector)));
 +	else
 +		len = min_t(sector_t, ci->sector_count, max_io_len(ti, ci->sector));
++=======
+ static void __send_changing_extent_only(struct clone_info *ci, struct dm_target *ti,
+ 					unsigned num_bios)
+ {
+ 	unsigned len;
  
- 	__send_duplicate_bios(ci, ti, num_bios, &len);
+ 	len = min_t(sector_t, ci->sector_count,
+ 		    max_io_len_target_boundary(ti, dm_target_offset(ti, ci->sector)));
++>>>>>>> e6fc9f62ce6e (dm: flag clones created by __send_duplicate_bios)
  
+ 	/*
+ 	 * dm_accept_partial_bio cannot be used with duplicate bios,
+ 	 * so update clone_info cursor before __send_duplicate_bios().
+ 	 */
  	ci->sector += len;
  	ci->sector_count -= len;
  
@@@ -1543,14 -1322,11 +1592,13 @@@ static bool is_abnormal_io(struct bio *
  static bool __process_abnormal_io(struct clone_info *ci, struct dm_target *ti,
  				  int *result)
  {
- 	struct bio *bio = ci->bio;
  	unsigned num_bios = 0;
 +	bool is_split_required = false;
  
- 	switch (bio_op(bio)) {
+ 	switch (bio_op(ci->bio)) {
  	case REQ_OP_DISCARD:
  		num_bios = ti->num_discard_bios;
 +		is_split_required = is_split_required_for_discard(ti);
  		break;
  	case REQ_OP_SECURE_ERASE:
  		num_bios = ti->num_secure_erase_bios;
@@@ -1565,7 -1341,18 +1613,22 @@@
  		return false;
  	}
  
++<<<<<<< HEAD
 +	*result = __send_changing_extent_only(ci, ti, num_bios, is_split_required);
++=======
+ 	/*
+ 	 * Even though the device advertised support for this type of
+ 	 * request, that does not mean every target supports it, and
+ 	 * reconfiguration might also have changed that since the
+ 	 * check was performed.
+ 	 */
+ 	if (!num_bios)
+ 		*result = -EOPNOTSUPP;
+ 	else {
+ 		__send_changing_extent_only(ci, ti, num_bios);
+ 		*result = 0;
+ 	}
++>>>>>>> e6fc9f62ce6e (dm: flag clones created by __send_duplicate_bios)
  	return true;
  }
  
* Unmerged path drivers/md/dm-core.h
* Unmerged path drivers/md/dm.c
