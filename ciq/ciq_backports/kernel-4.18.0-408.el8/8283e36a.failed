KVM: x86/mmu: Propagate memslot const qualifier

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Ben Gardon <bgardon@google.com>
commit 8283e36abfff507c64fe8289ac30ea7ab59648aa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/8283e36a.failed

In preparation for implementing in-place hugepage promotion, various
functions will need to be called from zap_collapsible_spte_range, which
has the const qualifier on its memslot argument. Propagate the const
qualifier to the various functions which will be needed. This just serves
to simplify the following patch.

No functional change intended.

	Signed-off-by: Ben Gardon <bgardon@google.com>
Message-Id: <20211115234603.2908381-11-bgardon@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 8283e36abfff507c64fe8289ac30ea7ab59648aa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_page_track.h
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/mmu_internal.h
#	arch/x86/kvm/mmu/page_track.c
#	arch/x86/kvm/mmu/spte.c
#	arch/x86/kvm/mmu/spte.h
diff --cc arch/x86/include/asm/kvm_page_track.h
index 9cd9230e5cc8,eb186bc57f6a..000000000000
--- a/arch/x86/include/asm/kvm_page_track.h
+++ b/arch/x86/include/asm/kvm_page_track.h
@@@ -59,10 -63,9 +59,16 @@@ void kvm_slot_page_track_add_page(struc
  void kvm_slot_page_track_remove_page(struct kvm *kvm,
  				     struct kvm_memory_slot *slot, gfn_t gfn,
  				     enum kvm_page_track_mode mode);
++<<<<<<< HEAD
 +bool kvm_page_track_is_active(struct kvm_vcpu *vcpu, gfn_t gfn,
 +			      enum kvm_page_track_mode mode);
 +bool kvm_slot_page_track_is_active(struct kvm_memory_slot *slot, gfn_t gfn,
 +				   enum kvm_page_track_mode mode);
++=======
+ bool kvm_slot_page_track_is_active(struct kvm *kvm,
+ 				   const struct kvm_memory_slot *slot,
+ 				   gfn_t gfn, enum kvm_page_track_mode mode);
++>>>>>>> 8283e36abfff (KVM: x86/mmu: Propagate memslot const qualifier)
  
  void
  kvm_page_track_register_notifier(struct kvm *kvm,
diff --cc arch/x86/kvm/mmu/mmu.c
index 970fc4a0eb93,c28cf7eeb79d..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2585,9 -2580,11 +2585,14 @@@ static void kvm_unsync_page(struct kvm_
   * were marked unsync (or if there is no shadow page), -EPERM if the SPTE must
   * be write-protected.
   */
++<<<<<<< HEAD
 +int mmu_try_to_unsync_pages(struct kvm_vcpu *vcpu, gfn_t gfn, bool can_unsync)
++=======
+ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
+ 			    gfn_t gfn, bool can_unsync, bool prefetch)
++>>>>>>> 8283e36abfff (KVM: x86/mmu: Propagate memslot const qualifier)
  {
  	struct kvm_mmu_page *sp;
 -	bool locked = false;
  
  	/*
  	 * Force write-protection if the page is being tracked.  Note, the page
diff --cc arch/x86/kvm/mmu/mmu_internal.h
index a81dffc914d9,da6166b5c377..000000000000
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@@ -118,16 -117,11 +118,21 @@@ static inline bool kvm_mmu_page_ad_need
  	return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
  }
  
++<<<<<<< HEAD
 +extern int nx_huge_pages;
 +static inline bool is_nx_huge_page_enabled(void)
 +{
 +	return READ_ONCE(nx_huge_pages);
 +}
++=======
+ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
+ 			    gfn_t gfn, bool can_unsync, bool prefetch);
++>>>>>>> 8283e36abfff (KVM: x86/mmu: Propagate memslot const qualifier)
  
 -void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);
 -void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);
 +int mmu_try_to_unsync_pages(struct kvm_vcpu *vcpu, gfn_t gfn, bool can_unsync);
 +
 +void kvm_mmu_gfn_disallow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
 +void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
  bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
  				    struct kvm_memory_slot *slot, u64 gfn,
  				    int min_level);
diff --cc arch/x86/kvm/mmu/page_track.c
index 58985087b096,68eb1fb548b6..000000000000
--- a/arch/x86/kvm/mmu/page_track.c
+++ b/arch/x86/kvm/mmu/page_track.c
@@@ -135,8 -170,12 +135,17 @@@ void kvm_slot_page_track_remove_page(st
  }
  EXPORT_SYMBOL_GPL(kvm_slot_page_track_remove_page);
  
++<<<<<<< HEAD
 +bool kvm_slot_page_track_is_active(struct kvm_memory_slot *slot, gfn_t gfn,
 +				   enum kvm_page_track_mode mode)
++=======
+ /*
+  * check if the corresponding access on the specified guest page is tracked.
+  */
+ bool kvm_slot_page_track_is_active(struct kvm *kvm,
+ 				   const struct kvm_memory_slot *slot,
+ 				   gfn_t gfn, enum kvm_page_track_mode mode)
++>>>>>>> 8283e36abfff (KVM: x86/mmu: Propagate memslot const qualifier)
  {
  	int index;
  
diff --cc arch/x86/kvm/mmu/spte.c
index 62cb6713aea3,8a7b03207762..000000000000
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@@ -89,15 -89,17 +89,23 @@@ static bool kvm_is_mmio_pfn(kvm_pfn_t p
  				     E820_TYPE_RAM);
  }
  
++<<<<<<< HEAD
 +int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 +		     gfn_t gfn, kvm_pfn_t pfn, u64 old_spte, bool speculative,
 +		     bool can_unsync, bool host_writable, bool ad_disabled,
 +		     u64 *new_spte)
++=======
+ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
+ 	       const struct kvm_memory_slot *slot,
+ 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
+ 	       u64 old_spte, bool prefetch, bool can_unsync,
+ 	       bool host_writable, u64 *new_spte)
++>>>>>>> 8283e36abfff (KVM: x86/mmu: Propagate memslot const qualifier)
  {
 -	int level = sp->role.level;
  	u64 spte = SPTE_MMU_PRESENT_MASK;
 -	bool wrprot = false;
 +	int ret = 0;
  
 -	if (sp->role.ad_disabled)
 +	if (ad_disabled)
  		spte |= SPTE_TDP_AD_DISABLED_MASK;
  	else if (kvm_mmu_page_ad_need_write_protect(sp))
  		spte |= SPTE_TDP_AD_WRPROT_ONLY_MASK;
diff --cc arch/x86/kvm/mmu/spte.h
index 31d6456d8ac3,a4af2a42695c..000000000000
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@@ -329,15 -329,11 +329,23 @@@ static inline u64 get_mmio_spte_generat
  	return gen;
  }
  
++<<<<<<< HEAD
 +/* Bits which may be returned by set_spte() */
 +#define SET_SPTE_WRITE_PROTECTED_PT    BIT(0)
 +#define SET_SPTE_NEED_REMOTE_TLB_FLUSH BIT(1)
 +#define SET_SPTE_SPURIOUS              BIT(2)
 +
 +int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 +		     gfn_t gfn, kvm_pfn_t pfn, u64 old_spte, bool speculative,
 +		     bool can_unsync, bool host_writable, bool ad_disabled,
 +		     u64 *new_spte);
++=======
+ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
+ 	       const struct kvm_memory_slot *slot,
+ 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
+ 	       u64 old_spte, bool prefetch, bool can_unsync,
+ 	       bool host_writable, u64 *new_spte);
++>>>>>>> 8283e36abfff (KVM: x86/mmu: Propagate memslot const qualifier)
  u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled);
  u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access);
  u64 mark_spte_for_access_track(u64 spte);
* Unmerged path arch/x86/include/asm/kvm_page_track.h
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/mmu_internal.h
* Unmerged path arch/x86/kvm/mmu/page_track.c
* Unmerged path arch/x86/kvm/mmu/spte.c
* Unmerged path arch/x86/kvm/mmu/spte.h
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5a84e8a20ec4..5cd8d18d2c0d 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -387,7 +387,7 @@ struct kvm_memory_slot {
 	u16 as_id;
 };
 
-static inline bool kvm_slot_dirty_track_enabled(struct kvm_memory_slot *slot)
+static inline bool kvm_slot_dirty_track_enabled(const struct kvm_memory_slot *slot)
 {
 	return slot->flags & KVM_MEM_LOG_DIRTY_PAGES;
 }
@@ -810,9 +810,9 @@ void kvm_set_page_accessed(struct page *page);
 kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);
 kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable);
-kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);
-kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);
-kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+kvm_pfn_t gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t gfn_to_pfn_memslot_atomic(const struct kvm_memory_slot *slot, gfn_t gfn);
+kvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool *async, bool write_fault,
 			       bool *writable, hva_t *hva);
 
@@ -889,7 +889,7 @@ struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);
 bool kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);
 bool kvm_vcpu_is_visible_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_host_page_size(struct kvm_vcpu *vcpu, gfn_t gfn);
-void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot, gfn_t gfn);
+void mark_page_dirty_in_slot(struct kvm *kvm, const struct kvm_memory_slot *memslot, gfn_t gfn);
 void mark_page_dirty(struct kvm *kvm, gfn_t gfn);
 
 struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index b1373f69ce5e..eeab3b87faeb 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2012,12 +2012,12 @@ unsigned long kvm_host_page_size(struct kvm_vcpu *vcpu, gfn_t gfn)
 	return size;
 }
 
-static bool memslot_is_readonly(struct kvm_memory_slot *slot)
+static bool memslot_is_readonly(const struct kvm_memory_slot *slot)
 {
 	return slot->flags & KVM_MEM_READONLY;
 }
 
-static unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
+static unsigned long __gfn_to_hva_many(const struct kvm_memory_slot *slot, gfn_t gfn,
 				       gfn_t *nr_pages, bool write)
 {
 	if (!slot || slot->flags & KVM_MEMSLOT_INVALID)
@@ -2314,7 +2314,7 @@ static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
 	return pfn;
 }
 
-kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+kvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool *async, bool write_fault,
 			       bool *writable, hva_t *hva)
 {
@@ -2354,13 +2354,13 @@ kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_prot);
 
-kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
+kvm_pfn_t gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	return __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot);
 
-kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)
+kvm_pfn_t gfn_to_pfn_memslot_atomic(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	return __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL, NULL);
 }
@@ -2879,7 +2879,7 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
 void mark_page_dirty_in_slot(struct kvm *kvm,
-			     struct kvm_memory_slot *memslot,
+			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
 {
 	if (memslot && kvm_slot_dirty_track_enabled(memslot)) {
