KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit e2b5b21d3a815b7d88fc4c90e9ccc42ea9bac4f0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/e2b5b21d.failed

Add a dedicated helper for zapping a TDP MMU root, and use it in the three
flows that do "zap_all" and intentionally do not do a TLB flush if SPTEs
are zapped (zapping an entire root is safe if and only if it cannot be in
use by any vCPU).  Because a TLB flush is never required, unconditionally
pass "false" to tdp_mmu_iter_cond_resched() when potentially yielding.

Opportunistically document why KVM must not yield when zapping roots that
are being zapped by kvm_tdp_mmu_put_root(), i.e. roots whose refcount has
reached zero, and further harden the flow to detect improper KVM behavior
with respect to roots that are supposed to be unreachable.

In addition to hardening zapping of roots, isolating zapping of roots
will allow future simplification of zap_gfn_range() by having it zap only
leaf SPTEs, and by removing its tricky "zap all" heuristic.  By having
all paths that truly need to free _all_ SPs flow through the dedicated
root zapper, the generic zapper can be freed of those concerns.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Reviewed-by: Ben Gardon <bgardon@google.com>
Message-Id: <20220226001546.360188-16-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit e2b5b21d3a815b7d88fc4c90e9ccc42ea9bac4f0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 97bb57fe39ca,970376297b30..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -41,9 -56,6 +41,12 @@@ void kvm_mmu_uninit_tdp_mmu(struct kvm 
  	rcu_barrier();
  }
  
++<<<<<<< HEAD
 +static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 +			  gfn_t start, gfn_t end, bool can_yield, bool flush);
 +
++=======
++>>>>>>> e2b5b21d3a81 (KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page)
  static void tdp_mmu_free_sp(struct kvm_mmu_page *sp)
  {
  	free_page((unsigned long)sp->spt);
@@@ -66,11 -78,13 +69,19 @@@ static void tdp_mmu_free_sp_rcu_callbac
  	tdp_mmu_free_sp(sp);
  }
  
++<<<<<<< HEAD
 +void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root)
++=======
+ static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			     bool shared);
+ 
+ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			  bool shared)
++>>>>>>> e2b5b21d3a81 (KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page)
  {
 -	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 +	gfn_t max_gfn = 1ULL << (shadow_phys_bits - PAGE_SHIFT);
 +
 +	lockdep_assert_held_write(&kvm->mmu_lock);
  
  	if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
  		return;
@@@ -81,7 -95,15 +92,19 @@@
  	list_del_rcu(&root->link);
  	spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
  
++<<<<<<< HEAD
 +	zap_gfn_range(kvm, root, 0, max_gfn, false, false);
++=======
+ 	/*
+ 	 * A TLB flush is not necessary as KVM performs a local TLB flush when
+ 	 * allocating a new root (see kvm_mmu_load()), and when migrating vCPU
+ 	 * to a different pCPU.  Note, the local TLB flush on reuse also
+ 	 * invalidates any paging-structure-cache entries, i.e. TLB entries for
+ 	 * intermediate paging structures, that may be zapped, as such entries
+ 	 * are associated with the ASID on both VMX and SVM.
+ 	 */
+ 	tdp_mmu_zap_root(kvm, root, shared);
++>>>>>>> e2b5b21d3a81 (KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page)
  
  	call_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);
  }
@@@ -700,6 -736,103 +723,106 @@@ static inline bool __must_check tdp_mmu
  	return iter->yielded;
  }
  
++<<<<<<< HEAD
++=======
+ static inline gfn_t tdp_mmu_max_gfn_host(void)
+ {
+ 	/*
+ 	 * Bound TDP MMU walks at host.MAXPHYADDR, guest accesses beyond that
+ 	 * will hit a #PF(RSVD) and never hit an EPT Violation/Misconfig / #NPF,
+ 	 * and so KVM will never install a SPTE for such addresses.
+ 	 */
+ 	return 1ULL << (shadow_phys_bits - PAGE_SHIFT);
+ }
+ 
+ static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			     bool shared)
+ {
+ 	bool root_is_unreachable = !refcount_read(&root->tdp_mmu_root_count);
+ 	struct tdp_iter iter;
+ 
+ 	gfn_t end = tdp_mmu_max_gfn_host();
+ 	gfn_t start = 0;
+ 
+ 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ 
+ 	rcu_read_lock();
+ 
+ 	/*
+ 	 * No need to try to step down in the iterator when zapping an entire
+ 	 * root, zapping an upper-level SPTE will recurse on its children.
+ 	 */
+ 	for_each_tdp_pte_min_level(iter, root, root->role.level, start, end) {
+ retry:
+ 		/*
+ 		 * Yielding isn't allowed when zapping an unreachable root as
+ 		 * the root won't be processed by mmu_notifier callbacks.  When
+ 		 * handling an unmap/release mmu_notifier command, KVM must
+ 		 * drop all references to relevant pages prior to completing
+ 		 * the callback.  Dropping mmu_lock can result in zapping SPTEs
+ 		 * for an unreachable root after a relevant callback completes,
+ 		 * which leads to use-after-free as zapping a SPTE triggers
+ 		 * "writeback" of dirty/accessed bits to the SPTE's associated
+ 		 * struct page.
+ 		 */
+ 		if (!root_is_unreachable &&
+ 		    tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ 			continue;
+ 
+ 		if (!is_shadow_present_pte(iter.old_spte))
+ 			continue;
+ 
+ 		if (!shared) {
+ 			tdp_mmu_set_spte(kvm, &iter, 0);
+ 		} else if (tdp_mmu_set_spte_atomic(kvm, &iter, 0)) {
+ 			/*
+ 			 * cmpxchg() shouldn't fail if the root is unreachable.
+ 			 * Retry so as not to leak the page and its children.
+ 			 */
+ 			WARN_ONCE(root_is_unreachable,
+ 				  "Contended TDP MMU SPTE in unreachable root.");
+ 			goto retry;
+ 		}
+ 
+ 		/*
+ 		 * WARN if the root is invalid and is unreachable, all SPTEs
+ 		 * should've been zapped by kvm_tdp_mmu_zap_invalidated_roots(),
+ 		 * and inserting new SPTEs under an invalid root is a KVM bug.
+ 		 */
+ 		WARN_ON_ONCE(root_is_unreachable && root->role.invalid);
+ 	}
+ 
+ 	rcu_read_unlock();
+ }
+ 
+ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
+ {
+ 	u64 old_spte;
+ 
+ 	/*
+ 	 * This helper intentionally doesn't allow zapping a root shadow page,
+ 	 * which doesn't have a parent page table and thus no associated entry.
+ 	 */
+ 	if (WARN_ON_ONCE(!sp->ptep))
+ 		return false;
+ 
+ 	rcu_read_lock();
+ 
+ 	old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
+ 	if (WARN_ON_ONCE(!is_shadow_present_pte(old_spte))) {
+ 		rcu_read_unlock();
+ 		return false;
+ 	}
+ 
+ 	__tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0,
+ 			   sp->gfn, sp->role.level + 1, true, true);
+ 
+ 	rcu_read_unlock();
+ 
+ 	return true;
+ }
+ 
++>>>>>>> e2b5b21d3a81 (KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page)
  /*
   * Tears down the mappings for the range of gfns, [start, end), and frees the
   * non-root pages mapping GFNs strictly within that range. Returns true if
@@@ -709,20 -843,36 +832,36 @@@
   * scheduler needs the CPU or there is contention on the MMU lock. If this
   * function cannot yield, it will not release the MMU lock or reschedule and
   * the caller must ensure it does not supply too large a GFN range, or the
 - * operation can cause a soft lockup.
 - *
 - * If shared is true, this thread holds the MMU lock in read mode and must
 - * account for the possibility that other threads are modifying the paging
 - * structures concurrently. If shared is false, this thread should hold the
 - * MMU lock in write mode.
 + * operation can cause a soft lockup.  Note, in some use cases a flush may be
 + * required by prior actions.  Ensure the pending flush is performed prior to
 + * yielding.
   */
  static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 -			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 -			  bool shared)
 +			  gfn_t start, gfn_t end, bool can_yield, bool flush)
  {
++<<<<<<< HEAD
 +	struct tdp_iter iter;
 +
++=======
+ 	bool zap_all = (start == 0 && end >= tdp_mmu_max_gfn_host());
+ 	struct tdp_iter iter;
+ 
+ 	/*
+ 	 * No need to try to step down in the iterator when zapping all SPTEs,
+ 	 * zapping the top-level non-leaf SPTEs will recurse on their children.
+ 	 */
+ 	int min_level = zap_all ? root->role.level : PG_LEVEL_4K;
+ 
+ 	end = min(end, tdp_mmu_max_gfn_host());
+ 
+ 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ 
++>>>>>>> e2b5b21d3a81 (KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page)
  	rcu_read_lock();
  
 -	for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
 -retry:
 +	tdp_root_for_each_pte(iter, root, start, end) {
  		if (can_yield &&
 -		    tdp_mmu_iter_cond_resched(kvm, &iter, flush, shared)) {
 +		    tdp_mmu_iter_cond_resched(kvm, &iter, flush)) {
  			flush = false;
  			continue;
  		}
@@@ -767,15 -923,18 +906,30 @@@ bool __kvm_tdp_mmu_zap_gfn_range(struc
  
  void kvm_tdp_mmu_zap_all(struct kvm *kvm)
  {
++<<<<<<< HEAD
 +	gfn_t max_gfn = 1ULL << (shadow_phys_bits - PAGE_SHIFT);
 +	bool flush = false;
 +	int i;
 +
 +	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++)
 +		flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, 0, max_gfn, flush);
 +
 +	if (flush)
 +		kvm_flush_remote_tlbs(kvm);
++=======
+ 	struct kvm_mmu_page *root;
+ 	int i;
+ 
+ 	/*
+ 	 * A TLB flush is unnecessary, KVM zaps everything if and only the VM
+ 	 * is being destroyed or the userspace VMM has exited.  In both cases,
+ 	 * KVM_RUN is unreachable, i.e. no vCPUs will ever service the request.
+ 	 */
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		for_each_tdp_mmu_root_yield_safe(kvm, root, i)
+ 			tdp_mmu_zap_root(kvm, root, false);
+ 	}
++>>>>>>> e2b5b21d3a81 (KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page)
  }
  
  static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
@@@ -825,8 -982,16 +979,21 @@@ void kvm_tdp_mmu_zap_invalidated_roots(
  
  		rcu_read_unlock();
  
++<<<<<<< HEAD
 +		flush = zap_gfn_range(kvm, root, 0, max_gfn, true, flush,
 +				      true);
++=======
+ 		/*
+ 		 * A TLB flush is unnecessary, invalidated roots are guaranteed
+ 		 * to be unreachable by the guest (see kvm_tdp_mmu_put_root()
+ 		 * for more details), and unlike the legacy MMU, no vCPU kick
+ 		 * is needed to play nice with lockless shadow walks as the TDP
+ 		 * MMU protects its paging structures via RCU.  Note, zapping
+ 		 * will still flush on yield, but that's a minor performance
+ 		 * blip and not a functional issue.
+ 		 */
+ 		tdp_mmu_zap_root(kvm, root, true);
++>>>>>>> e2b5b21d3a81 (KVM: x86/mmu: Add dedicated helper to zap TDP MMU root shadow page)
  
  		/*
  		 * Put the reference acquired in
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
