KVM: x86: hyper-v: Avoid writing to TSC page without an active vCPU

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Vitaly Kuznetsov <vkuznets@redhat.com>
commit 42dcbe7d8bac997eef4c379e61d9121a15ed4e36
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/42dcbe7d.failed

The following WARN is triggered from kvm_vm_ioctl_set_clock():
 WARNING: CPU: 10 PID: 579353 at arch/x86/kvm/../../../virt/kvm/kvm_main.c:3161 mark_page_dirty_in_slot+0x6c/0x80 [kvm]
 ...
 CPU: 10 PID: 579353 Comm: qemu-system-x86 Tainted: G        W  O      5.16.0.stable #20
 Hardware name: LENOVO 20UF001CUS/20UF001CUS, BIOS R1CET65W(1.34 ) 06/17/2021
 RIP: 0010:mark_page_dirty_in_slot+0x6c/0x80 [kvm]
 ...
 Call Trace:
  <TASK>
  ? kvm_write_guest+0x114/0x120 [kvm]
  kvm_hv_invalidate_tsc_page+0x9e/0xf0 [kvm]
  kvm_arch_vm_ioctl+0xa26/0xc50 [kvm]
  ? schedule+0x4e/0xc0
  ? __cond_resched+0x1a/0x50
  ? futex_wait+0x166/0x250
  ? __send_signal+0x1f1/0x3d0
  kvm_vm_ioctl+0x747/0xda0 [kvm]
  ...

The WARN was introduced by commit 03c0304a86bc ("KVM: Warn if
mark_page_dirty() is called without an active vCPU") but the change seems
to be correct (unlike Hyper-V TSC page update mechanism). In fact, there's
no real need to actually write to guest memory to invalidate TSC page, this
can be done by the first vCPU which goes through kvm_guest_time_update().

	Reported-by: Maxim Levitsky <mlevitsk@redhat.com>
	Reported-by: Naresh Kamboju <naresh.kamboju@linaro.org>
	Suggested-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
Message-Id: <20220407201013.963226-1-vkuznets@redhat.com>
(cherry picked from commit 42dcbe7d8bac997eef4c379e61d9121a15ed4e36)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/hyperv.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/hyperv.c
index 95403166bf23,46f9dfb60469..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -1197,16 -1203,9 +1199,20 @@@ out_unlock
  	mutex_unlock(&hv->hv_lock);
  }
  
- void kvm_hv_invalidate_tsc_page(struct kvm *kvm)
+ void kvm_hv_request_tsc_page_update(struct kvm *kvm)
  {
++<<<<<<< HEAD
 +	struct kvm_hv *hv = &kvm->arch.hyperv;
 +	u64 gfn;
 +	int idx;
 +
 +	if (hv->hv_tsc_page_status == HV_TSC_PAGE_BROKEN ||
 +	    hv->hv_tsc_page_status == HV_TSC_PAGE_UNSET ||
 +	    tsc_page_update_unsafe(hv))
 +		return;
++=======
+ 	struct kvm_hv *hv = to_kvm_hv(kvm);
++>>>>>>> 42dcbe7d8bac (KVM: x86: hyper-v: Avoid writing to TSC page without an active vCPU)
  
  	mutex_lock(&hv->hv_lock);
  
diff --cc arch/x86/kvm/x86.c
index 7595a65a7c26,547ba00ef64f..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -2884,40 -2897,68 +2884,71 @@@ static void kvm_gen_update_masterclock(
  	/* guest entries allowed */
  	kvm_for_each_vcpu(i, vcpu, kvm)
  		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
++<<<<<<< HEAD
++=======
+ }
+ 
+ static void kvm_update_masterclock(struct kvm *kvm)
+ {
+ 	kvm_hv_request_tsc_page_update(kvm);
+ 	kvm_start_pvclock_update(kvm);
+ 	pvclock_update_vm_gtod_copy(kvm);
+ 	kvm_end_pvclock_update(kvm);
+ }
+ 
+ /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
+ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
+ {
+ 	struct kvm_arch *ka = &kvm->arch;
+ 	struct pvclock_vcpu_time_info hv_clock;
+ 
+ 	/* both __this_cpu_read() and rdtsc() should be on the same cpu */
+ 	get_cpu();
+ 
+ 	data->flags = 0;
+ 	if (ka->use_master_clock && __this_cpu_read(cpu_tsc_khz)) {
+ #ifdef CONFIG_X86_64
+ 		struct timespec64 ts;
+ 
+ 		if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+ 			data->realtime = ts.tv_nsec + NSEC_PER_SEC * ts.tv_sec;
+ 			data->flags |= KVM_CLOCK_REALTIME | KVM_CLOCK_HOST_TSC;
+ 		} else
++>>>>>>> 42dcbe7d8bac (KVM: x86: hyper-v: Avoid writing to TSC page without an active vCPU)
  #endif
 -		data->host_tsc = rdtsc();
 -
 -		data->flags |= KVM_CLOCK_TSC_STABLE;
 -		hv_clock.tsc_timestamp = ka->master_cycle_now;
 -		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
 -		kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
 -				   &hv_clock.tsc_shift,
 -				   &hv_clock.tsc_to_system_mul);
 -		data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
 -	} else {
 -		data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
 -	}
 -
 -	put_cpu();
  }
  
 -static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 +u64 get_kvmclock_ns(struct kvm *kvm)
  {
  	struct kvm_arch *ka = &kvm->arch;
 -	unsigned seq;
 +	struct pvclock_vcpu_time_info hv_clock;
 +	unsigned long flags;
 +	u64 ret;
  
 -	do {
 -		seq = read_seqcount_begin(&ka->pvclock_sc);
 -		__get_kvmclock(kvm, data);
 -	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 -}
 +	raw_spin_lock_irqsave(&ka->pvclock_gtod_sync_lock, flags);
 +	if (!ka->use_master_clock) {
 +		raw_spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
 +		return get_kvmclock_base_ns() + ka->kvmclock_offset;
 +	}
  
 -u64 get_kvmclock_ns(struct kvm *kvm)
 -{
 -	struct kvm_clock_data data;
 +	hv_clock.tsc_timestamp = ka->master_cycle_now;
 +	hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
 +	raw_spin_unlock_irqrestore(&ka->pvclock_gtod_sync_lock, flags);
  
 -	get_kvmclock(kvm, &data);
 -	return data.clock;
 +	/* both __this_cpu_read() and rdtsc() should be on the same cpu */
 +	get_cpu();
 +
 +	if (__this_cpu_read(cpu_tsc_khz)) {
 +		kvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,
 +				   &hv_clock.tsc_shift,
 +				   &hv_clock.tsc_to_system_mul);
 +		ret = __pvclock_read_cycles(&hv_clock, rdtsc());
 +	} else
 +		ret = get_kvmclock_base_ns() + ka->kvmclock_offset;
 +
 +	put_cpu();
 +
 +	return ret;
  }
  
  static void kvm_setup_pvclock_page(struct kvm_vcpu *v,
@@@ -6138,6 -6213,63 +6168,66 @@@ int kvm_arch_pm_notifier(struct kvm *kv
  }
  #endif /* CONFIG_HAVE_KVM_PM_NOTIFIER */
  
++<<<<<<< HEAD
++=======
+ static int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)
+ {
+ 	struct kvm_clock_data data = { 0 };
+ 
+ 	get_kvmclock(kvm, &data);
+ 	if (copy_to_user(argp, &data, sizeof(data)))
+ 		return -EFAULT;
+ 
+ 	return 0;
+ }
+ 
+ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
+ {
+ 	struct kvm_arch *ka = &kvm->arch;
+ 	struct kvm_clock_data data;
+ 	u64 now_raw_ns;
+ 
+ 	if (copy_from_user(&data, argp, sizeof(data)))
+ 		return -EFAULT;
+ 
+ 	/*
+ 	 * Only KVM_CLOCK_REALTIME is used, but allow passing the
+ 	 * result of KVM_GET_CLOCK back to KVM_SET_CLOCK.
+ 	 */
+ 	if (data.flags & ~KVM_CLOCK_VALID_FLAGS)
+ 		return -EINVAL;
+ 
+ 	kvm_hv_request_tsc_page_update(kvm);
+ 	kvm_start_pvclock_update(kvm);
+ 	pvclock_update_vm_gtod_copy(kvm);
+ 
+ 	/*
+ 	 * This pairs with kvm_guest_time_update(): when masterclock is
+ 	 * in use, we use master_kernel_ns + kvmclock_offset to set
+ 	 * unsigned 'system_time' so if we use get_kvmclock_ns() (which
+ 	 * is slightly ahead) here we risk going negative on unsigned
+ 	 * 'system_time' when 'data.clock' is very small.
+ 	 */
+ 	if (data.flags & KVM_CLOCK_REALTIME) {
+ 		u64 now_real_ns = ktime_get_real_ns();
+ 
+ 		/*
+ 		 * Avoid stepping the kvmclock backwards.
+ 		 */
+ 		if (now_real_ns > data.realtime)
+ 			data.clock += now_real_ns - data.realtime;
+ 	}
+ 
+ 	if (ka->use_master_clock)
+ 		now_raw_ns = ka->master_kernel_ns;
+ 	else
+ 		now_raw_ns = get_kvmclock_base_ns();
+ 	ka->kvmclock_offset = data.clock - now_raw_ns;
+ 	kvm_end_pvclock_update(kvm);
+ 	return 0;
+ }
+ 
++>>>>>>> 42dcbe7d8bac (KVM: x86: hyper-v: Avoid writing to TSC page without an active vCPU)
  long kvm_arch_vm_ioctl(struct file *filp,
  		       unsigned int ioctl, unsigned long arg)
  {
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 24742b8ee038..94fd218560db 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -970,12 +970,10 @@ enum hv_tsc_page_status {
 	HV_TSC_PAGE_UNSET = 0,
 	/* TSC page MSR was written by the guest, update pending */
 	HV_TSC_PAGE_GUEST_CHANGED,
-	/* TSC page MSR was written by KVM userspace, update pending */
+	/* TSC page update was triggered from the host side */
 	HV_TSC_PAGE_HOST_CHANGED,
 	/* TSC page was properly set up and is currently active  */
 	HV_TSC_PAGE_SET,
-	/* TSC page is currently being updated and therefore is inactive */
-	HV_TSC_PAGE_UPDATING,
 	/* TSC page was set up with an inaccessible GPA */
 	HV_TSC_PAGE_BROKEN,
 };
* Unmerged path arch/x86/kvm/hyperv.c
diff --git a/arch/x86/kvm/hyperv.h b/arch/x86/kvm/hyperv.h
index e19c00ee9ab3..da2737f2a956 100644
--- a/arch/x86/kvm/hyperv.h
+++ b/arch/x86/kvm/hyperv.h
@@ -137,7 +137,7 @@ void kvm_hv_process_stimers(struct kvm_vcpu *vcpu);
 
 void kvm_hv_setup_tsc_page(struct kvm *kvm,
 			   struct pvclock_vcpu_time_info *hv_clock);
-void kvm_hv_invalidate_tsc_page(struct kvm *kvm);
+void kvm_hv_request_tsc_page_update(struct kvm *kvm);
 
 void kvm_hv_init_vm(struct kvm *kvm);
 void kvm_hv_destroy_vm(struct kvm *kvm);
* Unmerged path arch/x86/kvm/x86.c
