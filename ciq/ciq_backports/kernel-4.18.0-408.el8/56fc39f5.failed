dmaengine: idxd: handle interrupt handle revoked event

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 56fc39f5a36794c4f27f5fee047b641eac3f5b89
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/56fc39f5.failed

"Interrupt handle revoked" is an event that happens when the driver is
running on a guest kernel and the VM is migrated to a new machine.
The device will trigger an interrupt that signals to the guest driver
that the interrupt handles need to be replaced.

The misc irq thread function calls a helper function to handle the
event. The function uses the WQ percpu_ref to quiesce the kernel
submissions. It then replaces the interrupt handles by requesting
interrupt handle command for each I/O MSIX vector. Once the handle is
updated, the driver will unblock the submission path to allow new
submissions.

The submitter will attempt to acquire a percpu_ref before submission. When
the request fails, it will wait on the wq_resurrect 'completion'.

The driver does anticipate the possibility of descriptors being submitted
before the WQ percpu_ref is killed. If a descriptor has already been
submitted, it will return with incorrect interrupt handle status. The
descriptor will be re-submitted with the new interrupt handle on the
completion path. For descriptors with incorrect interrupt handles,
completion interrupt won't be triggered.

At the completion of the interrupt handle refresh, the handling function
will call idxd_int_handle_refresh_drain() to issue drain descriptors to
each of the wq with associated interrupt handle. The drain descriptor will have
interrupt request set but without completion record. This will ensure all
descriptors with incorrect interrupt completion handle get drained and
a completion interrupt is triggered for the guest driver to process them.

	Reviewed-by: Kevin Tian <kevin.tian@intel.com>
Co-Developed-by: Sanjay Kumar <sanjay.k.kumar@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/163528420189.3925689.18212568593220415551.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 56fc39f5a36794c4f27f5fee047b641eac3f5b89)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/device.c
#	drivers/dma/idxd/idxd.h
#	drivers/dma/idxd/registers.h
#	drivers/dma/idxd/submit.c
diff --cc drivers/dma/idxd/device.c
index eadb0129f3b6,1dc5245107df..000000000000
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@@ -413,11 -413,20 +415,23 @@@ int idxd_wq_init_percpu_ref(struct idxd
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ void __idxd_wq_quiesce(struct idxd_wq *wq)
+ {
+ 	lockdep_assert_held(&wq->wq_lock);
+ 	reinit_completion(&wq->wq_resurrect);
+ 	percpu_ref_kill(&wq->wq_active);
+ 	complete_all(&wq->wq_resurrect);
+ 	wait_for_completion(&wq->wq_dead);
+ }
+ 
++>>>>>>> 56fc39f5a367 (dmaengine: idxd: handle interrupt handle revoked event)
  void idxd_wq_quiesce(struct idxd_wq *wq)
  {
 -	mutex_lock(&wq->wq_lock);
 -	__idxd_wq_quiesce(wq);
 -	mutex_unlock(&wq->wq_lock);
 +	percpu_ref_kill(&wq->wq_active);
 +	wait_for_completion(&wq->wq_dead);
 +	percpu_ref_exit(&wq->wq_active);
  }
  
  /* Device control bits */
diff --cc drivers/dma/idxd/idxd.h
index 03dc70e1f4d4,51e79201636c..000000000000
--- a/drivers/dma/idxd/idxd.h
+++ b/drivers/dma/idxd/idxd.h
@@@ -142,7 -171,8 +142,12 @@@ struct idxd_wq 
  	u32 portal_offset;
  	struct percpu_ref wq_active;
  	struct completion wq_dead;
++<<<<<<< HEAD
 +	struct device conf_dev;
++=======
+ 	struct completion wq_resurrect;
+ 	struct idxd_dev idxd_dev;
++>>>>>>> 56fc39f5a367 (dmaengine: idxd: handle interrupt handle revoked event)
  	struct idxd_cdev *idxd_cdev;
  	struct wait_queue_head err_queue;
  	struct idxd_device *idxd;
diff --cc drivers/dma/idxd/registers.h
index eeb11e6eb25b,8e396698c22b..000000000000
--- a/drivers/dma/idxd/registers.h
+++ b/drivers/dma/idxd/registers.h
@@@ -157,6 -157,8 +157,11 @@@ enum idxd_device_reset_type 
  #define IDXD_INTC_CMD			0x02
  #define IDXD_INTC_OCCUPY			0x04
  #define IDXD_INTC_PERFMON_OVFL		0x08
++<<<<<<< HEAD
++=======
+ #define IDXD_INTC_HALT_STATE		0x10
+ #define IDXD_INTC_INT_HANDLE_REVOKED	0x80000000
++>>>>>>> 56fc39f5a367 (dmaengine: idxd: handle interrupt handle revoked event)
  
  #define IDXD_CMD_OFFSET			0xa0
  union idxd_command_reg {
diff --cc drivers/dma/idxd/submit.c
index 0afcd1322339,776fa81db61d..000000000000
--- a/drivers/dma/idxd/submit.c
+++ b/drivers/dma/idxd/submit.c
@@@ -91,6 -73,61 +91,11 @@@ void idxd_free_desc(struct idxd_wq *wq
  int idxd_submit_desc(struct idxd_wq *wq, struct idxd_desc *desc)
  {
  	struct idxd_device *idxd = wq->idxd;
++<<<<<<< HEAD
++=======
+ 	struct idxd_irq_entry *ie = NULL;
+ 	u32 desc_flags = desc->hw->flags;
++>>>>>>> 56fc39f5a367 (dmaengine: idxd: handle interrupt handle revoked event)
  	void __iomem *portal;
  	int rc;
  
@@@ -108,6 -148,21 +116,24 @@@
  	 * even on UP because the recipient is a device.
  	 */
  	wmb();
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Pending the descriptor to the lockless list for the irq_entry
+ 	 * that we designated the descriptor to.
+ 	 */
+ 	if (desc_flags & IDXD_OP_FLAG_RCI) {
+ 		ie = wq->ie;
+ 		if (ie->int_handle == INVALID_INT_HANDLE)
+ 			desc->hw->int_handle = ie->id;
+ 		else
+ 			desc->hw->int_handle = ie->int_handle;
+ 
+ 		llist_add(&desc->llnode, &ie->pending_llist);
+ 	}
+ 
++>>>>>>> 56fc39f5a367 (dmaengine: idxd: handle interrupt handle revoked event)
  	if (wq_dedicated(wq)) {
  		iosubmit_cmds512(portal, desc->hw, 1);
  	} else {
* Unmerged path drivers/dma/idxd/device.c
* Unmerged path drivers/dma/idxd/idxd.h
diff --git a/drivers/dma/idxd/init.c b/drivers/dma/idxd/init.c
index b9299e45c20d..a5b297b75d63 100644
--- a/drivers/dma/idxd/init.c
+++ b/drivers/dma/idxd/init.c
@@ -233,6 +233,7 @@ static int idxd_setup_wqs(struct idxd_device *idxd)
 		mutex_init(&wq->wq_lock);
 		init_waitqueue_head(&wq->err_queue);
 		init_completion(&wq->wq_dead);
+		init_completion(&wq->wq_resurrect);
 		wq->max_xfer_bytes = idxd->max_xfer_bytes;
 		wq->max_batch_size = idxd->max_batch_size;
 		wq->wqcfg = kzalloc_node(idxd->wqcfg_size, GFP_KERNEL, dev_to_node(dev));
diff --git a/drivers/dma/idxd/irq.c b/drivers/dma/idxd/irq.c
index 7a2cf0512501..b2313e5c82c6 100644
--- a/drivers/dma/idxd/irq.c
+++ b/drivers/dma/idxd/irq.c
@@ -6,6 +6,7 @@
 #include <linux/pci.h>
 #include <linux/io-64-nonatomic-lo-hi.h>
 #include <linux/dmaengine.h>
+#include <linux/delay.h>
 #include <uapi/linux/idxd.h>
 #include "../dmaengine.h"
 #include "idxd.h"
@@ -29,6 +30,11 @@ static int irq_process_pending_llist(struct idxd_irq_entry *irq_entry,
 				     enum irq_work_type wtype,
 				     int *processed, u64 data);
 
+struct idxd_int_handle_revoke {
+	struct work_struct work;
+	struct idxd_device *idxd;
+};
+
 static void idxd_device_reinit(struct work_struct *work)
 {
 	struct idxd_device *idxd = container_of(work, struct idxd_device, work);
@@ -102,6 +108,120 @@ static int idxd_device_schedule_fault_process(struct idxd_device *idxd,
 	return 0;
 }
 
+static void idxd_abort_invalid_int_handle_descs(struct idxd_irq_entry *ie)
+{
+	LIST_HEAD(flist);
+	struct idxd_desc *d, *t;
+	struct llist_node *head;
+
+	spin_lock(&ie->list_lock);
+	head = llist_del_all(&ie->pending_llist);
+	if (head) {
+		llist_for_each_entry_safe(d, t, head, llnode)
+			list_add_tail(&d->list, &ie->work_list);
+	}
+
+	list_for_each_entry_safe(d, t, &ie->work_list, list) {
+		if (d->completion->status == DSA_COMP_INT_HANDLE_INVAL)
+			list_move_tail(&d->list, &flist);
+	}
+	spin_unlock(&ie->list_lock);
+
+	list_for_each_entry_safe(d, t, &flist, list) {
+		list_del(&d->list);
+		idxd_dma_complete_txd(d, IDXD_COMPLETE_ABORT, true);
+	}
+}
+
+static void idxd_int_handle_revoke(struct work_struct *work)
+{
+	struct idxd_int_handle_revoke *revoke =
+		container_of(work, struct idxd_int_handle_revoke, work);
+	struct idxd_device *idxd = revoke->idxd;
+	struct pci_dev *pdev = idxd->pdev;
+	struct device *dev = &pdev->dev;
+	int i, new_handle, rc;
+
+	if (!idxd->request_int_handles) {
+		kfree(revoke);
+		dev_warn(dev, "Unexpected int handle refresh interrupt.\n");
+		return;
+	}
+
+	/*
+	 * The loop attempts to acquire new interrupt handle for all interrupt
+	 * vectors that supports a handle. If a new interrupt handle is acquired and the
+	 * wq is kernel type, the driver will kill the percpu_ref to pause all
+	 * ongoing descriptor submissions. The interrupt handle is then changed.
+	 * After change, the percpu_ref is revived and all the pending submissions
+	 * are woken to try again. A drain is sent to for the interrupt handle
+	 * at the end to make sure all invalid int handle descriptors are processed.
+	 */
+	for (i = 1; i < idxd->irq_cnt; i++) {
+		struct idxd_irq_entry *ie = &idxd->irq_entries[i];
+		struct idxd_wq *wq = ie->wq;
+
+		rc = idxd_device_request_int_handle(idxd, i, &new_handle, IDXD_IRQ_MSIX);
+		if (rc < 0) {
+			dev_warn(dev, "get int handle %d failed: %d\n", i, rc);
+			/*
+			 * Failed to acquire new interrupt handle. Kill the WQ
+			 * and release all the pending submitters. The submitters will
+			 * get error return code and handle appropriately.
+			 */
+			ie->int_handle = INVALID_INT_HANDLE;
+			idxd_wq_quiesce(wq);
+			idxd_abort_invalid_int_handle_descs(ie);
+			continue;
+		}
+
+		/* No change in interrupt handle, nothing needs to be done */
+		if (ie->int_handle == new_handle)
+			continue;
+
+		if (wq->state != IDXD_WQ_ENABLED || wq->type != IDXD_WQT_KERNEL) {
+			/*
+			 * All the MSIX interrupts are allocated at once during probe.
+			 * Therefore we need to update all interrupts even if the WQ
+			 * isn't supporting interrupt operations.
+			 */
+			ie->int_handle = new_handle;
+			continue;
+		}
+
+		mutex_lock(&wq->wq_lock);
+		reinit_completion(&wq->wq_resurrect);
+
+		/* Kill percpu_ref to pause additional descriptor submissions */
+		percpu_ref_kill(&wq->wq_active);
+
+		/* Wait for all submitters quiesce before we change interrupt handle */
+		wait_for_completion(&wq->wq_dead);
+
+		ie->int_handle = new_handle;
+
+		/* Revive percpu ref and wake up all the waiting submitters */
+		percpu_ref_reinit(&wq->wq_active);
+		complete_all(&wq->wq_resurrect);
+		mutex_unlock(&wq->wq_lock);
+
+		/*
+		 * The delay here is to wait for all possible MOVDIR64B that
+		 * are issued before percpu_ref_kill() has happened to have
+		 * reached the PCIe domain before the drain is issued. The driver
+		 * needs to ensure that the drain descriptor issued does not pass
+		 * all the other issued descriptors that contain the invalid
+		 * interrupt handle in order to ensure that the drain descriptor
+		 * interrupt will allow the cleanup of all the descriptors with
+		 * invalid interrupt handle.
+		 */
+		if (wq_dedicated(wq))
+			udelay(100);
+		idxd_int_handle_revoke_drain(ie);
+	}
+	kfree(revoke);
+}
+
 static int process_misc_interrupts(struct idxd_device *idxd, u32 cause)
 {
 	struct device *dev = &idxd->pdev->dev;
@@ -145,6 +265,23 @@ static int process_misc_interrupts(struct idxd_device *idxd, u32 cause)
 		err = true;
 	}
 
+	if (cause & IDXD_INTC_INT_HANDLE_REVOKED) {
+		struct idxd_int_handle_revoke *revoke;
+
+		val |= IDXD_INTC_INT_HANDLE_REVOKED;
+
+		revoke = kzalloc(sizeof(*revoke), GFP_ATOMIC);
+		if (revoke) {
+			revoke->idxd = idxd;
+			INIT_WORK(&revoke->work, idxd_int_handle_revoke);
+			queue_work(idxd->wq, &revoke->work);
+
+		} else {
+			dev_err(dev, "Failed to allocate work for int handle revoke\n");
+			idxd_wqs_quiesce(idxd);
+		}
+	}
+
 	if (cause & IDXD_INTC_CMD) {
 		val |= IDXD_INTC_CMD;
 		complete(idxd->cmd_done);
* Unmerged path drivers/dma/idxd/registers.h
* Unmerged path drivers/dma/idxd/submit.c
