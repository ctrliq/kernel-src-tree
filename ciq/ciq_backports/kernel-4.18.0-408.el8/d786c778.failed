KVM: MMU: inline set_spte in mmu_set_spte

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit d786c7783b01a01346f77e8e785030b5096c191a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/d786c778.failed

Since the two callers of set_spte do different things with the results,
inlining it actually makes the code simpler to reason about.  For example,
mmu_set_spte looks quite like tdp_mmu_map_handle_target_level, but the
similarity is hidden by set_spte.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit d786c7783b01a01346f77e8e785030b5096c191a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 8ec545b124be,6ba7c60bd4f8..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2681,11 -2700,12 +2681,17 @@@ static int mmu_set_spte(struct kvm_vcp
  			gfn_t gfn, kvm_pfn_t pfn, bool speculative,
  			bool host_writable)
  {
+ 	struct kvm_mmu_page *sp = sptep_to_sp(sptep);
  	int was_rmapped = 0;
++<<<<<<< HEAD
 +	int rmap_count;
 +	int set_spte_ret;
++=======
++>>>>>>> d786c7783b01 (KVM: MMU: inline set_spte in mmu_set_spte)
  	int ret = RET_PF_FIXED;
  	bool flush = false;
+ 	int make_spte_ret;
+ 	u64 spte;
  
  	pgprintk("%s: spte %llx write_fault %d gfn %llx\n", __func__,
  		 *sptep, write_fault, gfn);
@@@ -2727,24 -2755,12 +2741,20 @@@
  		kvm_flush_remote_tlbs_with_address(vcpu->kvm, gfn,
  				KVM_PAGES_PER_HPAGE(level));
  
- 	/*
- 	 * The fault is fully spurious if and only if the new SPTE and old SPTE
- 	 * are identical, and emulation is not required.
- 	 */
- 	if ((set_spte_ret & SET_SPTE_SPURIOUS) && ret == RET_PF_FIXED) {
- 		WARN_ON_ONCE(!was_rmapped);
- 		return RET_PF_SPURIOUS;
- 	}
- 
  	pgprintk("%s: setting spte %llx\n", __func__, *sptep);
- 	trace_kvm_mmu_set_spte(level, gfn, sptep);
  
  	if (!was_rmapped) {
++<<<<<<< HEAD
 +		if (is_large_pte(*sptep))
 +			++vcpu->kvm->stat.lpages;
 +		rmap_count = rmap_add(vcpu, sptep, gfn);
 +		if (rmap_count > RMAP_RECYCLE_THRESHOLD)
 +			rmap_recycle(vcpu, sptep, gfn);
++=======
+ 		WARN_ON_ONCE(ret == RET_PF_SPURIOUS);
+ 		kvm_update_page_stats(vcpu->kvm, level, 1);
+ 		rmap_add(vcpu, sptep, gfn);
++>>>>>>> d786c7783b01 (KVM: MMU: inline set_spte in mmu_set_spte)
  	}
  
  	return ret;
* Unmerged path arch/x86/kvm/mmu/mmu.c
