KVM: x86/mmu: Rename __gfn_to_rmap to gfn_to_rmap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit 93e083d4f4bfe790eb1cdc87103bd6a84be9df75
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/93e083d4.failed

gfn_to_rmap was removed in the previous patch so there is no need to
retain the double underscore on __gfn_to_rmap.

	Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20210804222844.1419481-7-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 93e083d4f4bfe790eb1cdc87103bd6a84be9df75)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 4c612c0dc324,964c797dcc46..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -1030,8 -1035,8 +1030,13 @@@ out
  	return true;
  }
  
++<<<<<<< HEAD
 +static struct kvm_rmap_head *__gfn_to_rmap(gfn_t gfn, int level,
 +					   struct kvm_memory_slot *slot)
++=======
+ static struct kvm_rmap_head *gfn_to_rmap(gfn_t gfn, int level,
+ 					 const struct kvm_memory_slot *slot)
++>>>>>>> 93e083d4f4bf (KVM: x86/mmu: Rename __gfn_to_rmap to gfn_to_rmap)
  {
  	unsigned long idx;
  
@@@ -1297,9 -1302,13 +1302,9 @@@ static void kvm_mmu_write_protect_pt_ma
  	if (is_tdp_mmu_enabled(kvm))
  		kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
  				slot->base_gfn + gfn_offset, mask, true);
 -
 -	if (!kvm_memslots_have_rmaps(kvm))
 -		return;
 -
  	while (mask) {
- 		rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
- 					  PG_LEVEL_4K, slot);
+ 		rmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ 					PG_LEVEL_4K, slot);
  		__rmap_write_protect(kvm, rmap_head, false);
  
  		/* clear the first set bit */
@@@ -1326,9 -1335,13 +1331,9 @@@ static void kvm_mmu_clear_dirty_pt_mask
  	if (is_tdp_mmu_enabled(kvm))
  		kvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,
  				slot->base_gfn + gfn_offset, mask, false);
 -
 -	if (!kvm_memslots_have_rmaps(kvm))
 -		return;
 -
  	while (mask) {
- 		rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
- 					  PG_LEVEL_4K, slot);
+ 		rmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
+ 					PG_LEVEL_4K, slot);
  		__rmap_clear_dirty(kvm, rmap_head, slot);
  
  		/* clear the first set bit */
@@@ -1392,9 -1405,11 +1397,17 @@@ bool kvm_mmu_slot_gfn_write_protect(str
  	int i;
  	bool write_protected = false;
  
++<<<<<<< HEAD
 +	for (i = min_level; i <= KVM_MAX_HUGEPAGE_LEVEL; ++i) {
 +		rmap_head = __gfn_to_rmap(gfn, i, slot);
 +		write_protected |= __rmap_write_protect(kvm, rmap_head, true);
++=======
+ 	if (kvm_memslots_have_rmaps(kvm)) {
+ 		for (i = min_level; i <= KVM_MAX_HUGEPAGE_LEVEL; ++i) {
+ 			rmap_head = gfn_to_rmap(gfn, i, slot);
+ 			write_protected |= __rmap_write_protect(kvm, rmap_head, true);
+ 		}
++>>>>>>> 93e083d4f4bf (KVM: x86/mmu: Rename __gfn_to_rmap to gfn_to_rmap)
  	}
  
  	if (is_tdp_mmu_enabled(kvm))
@@@ -1648,9 -1629,9 +1660,9 @@@ static void rmap_recycle(struct kvm_vcp
  
  	sp = sptep_to_sp(spte);
  	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
- 	rmap_head = __gfn_to_rmap(gfn, sp->role.level, slot);
+ 	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
  
 -	kvm_unmap_rmapp(vcpu->kvm, rmap_head, NULL, gfn, sp->role.level, __pte(0));
 +	kvm_unmap_rmapp(vcpu->kvm, rmap_head, NULL, gfn, sp->role.level, 0);
  	kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
  			KVM_PAGES_PER_HPAGE(sp->role.level));
  }
* Unmerged path arch/x86/kvm/mmu/mmu.c
diff --git a/arch/x86/kvm/mmu/mmu_audit.c b/arch/x86/kvm/mmu/mmu_audit.c
index cedc17b2f60e..9e7dcf999f08 100644
--- a/arch/x86/kvm/mmu/mmu_audit.c
+++ b/arch/x86/kvm/mmu/mmu_audit.c
@@ -147,7 +147,7 @@ static void inspect_spte_has_rmap(struct kvm *kvm, u64 *sptep)
 		return;
 	}
 
-	rmap_head = __gfn_to_rmap(gfn, rev_sp->role.level, slot);
+	rmap_head = gfn_to_rmap(gfn, rev_sp->role.level, slot);
 	if (!rmap_head->val) {
 		if (!__ratelimit(&ratelimit_state))
 			return;
@@ -200,7 +200,7 @@ static void audit_write_protection(struct kvm *kvm, struct kvm_mmu_page *sp)
 
 	slots = kvm_memslots_for_spte_role(kvm, sp->role);
 	slot = __gfn_to_memslot(slots, sp->gfn);
-	rmap_head = __gfn_to_rmap(sp->gfn, PG_LEVEL_4K, slot);
+	rmap_head = gfn_to_rmap(sp->gfn, PG_LEVEL_4K, slot);
 
 	for_each_rmap_spte(rmap_head, &iter, sptep) {
 		if (is_writable_pte(*sptep))
