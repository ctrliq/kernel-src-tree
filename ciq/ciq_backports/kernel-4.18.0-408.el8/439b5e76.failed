dmaengine: idxd: move wq irq enabling to after device enable

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 439b5e765a00a546e8f6b6eedac69889d0b5a869
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/439b5e76.failed

Move the calling of request_irq() and other related irq setup code until
after the WQ is successfully enabled. This reduces the amount of
setup/teardown if the wq is not configured correctly and cannot be enabled.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/164642777730.179702.1880317757087484299.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 439b5e765a00a546e8f6b6eedac69889d0b5a869)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/dma.c
diff --cc drivers/dma/idxd/dma.c
index 0d6d3c7052df,644114465b33..000000000000
--- a/drivers/dma/idxd/dma.c
+++ b/drivers/dma/idxd/dma.c
@@@ -261,5 -275,100 +261,103 @@@ void idxd_unregister_dma_channel(struc
  	list_del(&chan->device_node);
  	kfree(wq->idxd_chan);
  	wq->idxd_chan = NULL;
 -	put_device(wq_confdev(wq));
 +	put_device(&wq->conf_dev);
  }
++<<<<<<< HEAD
++=======
+ 
+ static int idxd_dmaengine_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+ 	struct idxd_device *idxd = wq->idxd;
+ 	int rc;
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED)
+ 		return -ENXIO;
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	wq->type = IDXD_WQT_KERNEL;
+ 
+ 	rc = __drv_enable_wq(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Enable wq %d failed: %d\n", wq->id, rc);
+ 		rc = -ENXIO;
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_request_irq(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_IRQ_ERR;
+ 		dev_dbg(dev, "WQ %d irq setup failed: %d\n", wq->id, rc);
+ 		goto err_irq;
+ 	}
+ 
+ 	rc = idxd_wq_alloc_resources(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_RES_ALLOC_ERR;
+ 		dev_dbg(dev, "WQ resource alloc failed\n");
+ 		goto err_res_alloc;
+ 	}
+ 
+ 	rc = idxd_wq_init_percpu_ref(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_PERCPU_ERR;
+ 		dev_dbg(dev, "percpu_ref setup failed\n");
+ 		goto err_ref;
+ 	}
+ 
+ 	rc = idxd_register_dma_channel(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_DMA_CHAN_ERR;
+ 		dev_dbg(dev, "Failed to register dma channel\n");
+ 		goto err_dma;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	mutex_unlock(&wq->wq_lock);
+ 	return 0;
+ 
+ err_dma:
+ 	__idxd_wq_quiesce(wq);
+ 	percpu_ref_exit(&wq->wq_active);
+ err_ref:
+ 	idxd_wq_free_resources(wq);
+ err_res_alloc:
+ 	idxd_wq_free_irq(wq);
+ err_irq:
+ 	__drv_disable_wq(wq);
+ err:
+ 	wq->type = IDXD_WQT_NONE;
+ 	mutex_unlock(&wq->wq_lock);
+ 	return rc;
+ }
+ 
+ static void idxd_dmaengine_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	__idxd_wq_quiesce(wq);
+ 	idxd_unregister_dma_channel(wq);
+ 	idxd_wq_free_resources(wq);
+ 	__drv_disable_wq(wq);
+ 	percpu_ref_exit(&wq->wq_active);
+ 	idxd_wq_free_irq(wq);
+ 	wq->type = IDXD_WQT_NONE;
+ 	mutex_unlock(&wq->wq_lock);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_WQ,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_dmaengine_drv = {
+ 	.probe = idxd_dmaengine_drv_probe,
+ 	.remove = idxd_dmaengine_drv_remove,
+ 	.name = "dmaengine",
+ 	.type = dev_types,
+ };
+ EXPORT_SYMBOL_GPL(idxd_dmaengine_drv);
++>>>>>>> 439b5e765a00 (dmaengine: idxd: move wq irq enabling to after device enable)
* Unmerged path drivers/dma/idxd/dma.c
