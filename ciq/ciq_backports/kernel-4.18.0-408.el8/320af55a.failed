KVM: x86: Add wrappers for setting/clearing APICv inhibits

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit 320af55a930f30ba49d7cd663280d46705e11383
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/320af55a.failed

Add set/clear wrappers for toggling APICv inhibits to make the call sites
more readable, and opportunistically rename the inner helpers to align
with the new wrappers and to make them more readable as well.  Invert the
flag from "activate" to "set"; activate is painfully ambiguous as it's
not obvious if the inhibit is being activated, or if APICv is being
activated, in which case the inhibit is being deactivated.

For the functions that take @set, swap the order of the inhibit reason
and @set so that the call sites are visually similar to those that bounce
through the wrapper.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20220311043517.17027-3-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 320af55a930f30ba49d7cd663280d46705e11383)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/trace.h
#	arch/x86/kvm/x86.c
diff --cc arch/x86/include/asm/kvm_host.h
index 24742b8ee038,93a2671a57cf..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1775,11 -1799,22 +1775,30 @@@ gpa_t kvm_mmu_gva_to_gpa_system(struct 
  
  bool kvm_apicv_activated(struct kvm *kvm);
  void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu);
++<<<<<<< HEAD
 +void kvm_request_apicv_update(struct kvm *kvm, bool activate,
 +			      unsigned long bit);
 +
 +void __kvm_request_apicv_update(struct kvm *kvm, bool activate,
 +				unsigned long bit);
++=======
+ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
+ 				      enum kvm_apicv_inhibit reason, bool set);
+ void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
+ 				    enum kvm_apicv_inhibit reason, bool set);
+ 
+ static inline void kvm_set_apicv_inhibit(struct kvm *kvm,
+ 					 enum kvm_apicv_inhibit reason)
+ {
+ 	kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+ }
+ 
+ static inline void kvm_clear_apicv_inhibit(struct kvm *kvm,
+ 					   enum kvm_apicv_inhibit reason)
+ {
+ 	kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+ }
++>>>>>>> 320af55a930f (KVM: x86: Add wrappers for setting/clearing APICv inhibits)
  
  int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
  
diff --cc arch/x86/kvm/trace.h
index 193f5ba930d1,105037a251b5..000000000000
--- a/arch/x86/kvm/trace.h
+++ b/arch/x86/kvm/trace.h
@@@ -1340,22 -1340,22 +1340,36 @@@ TRACE_EVENT(kvm_hv_stimer_cleanup
  );
  
  TRACE_EVENT(kvm_apicv_update_request,
++<<<<<<< HEAD
 +	    TP_PROTO(bool activate, unsigned long bit),
 +	    TP_ARGS(activate, bit),
 +
 +	TP_STRUCT__entry(
 +		__field(bool, activate)
 +		__field(unsigned long, bit)
 +	),
 +
 +	TP_fast_assign(
 +		__entry->activate = activate;
 +		__entry->bit = bit;
++=======
+ 	    TP_PROTO(int reason, bool activate),
+ 	    TP_ARGS(reason, activate),
+ 
+ 	TP_STRUCT__entry(
+ 		__field(int, reason)
+ 		__field(bool, activate)
+ 	),
+ 
+ 	TP_fast_assign(
+ 		__entry->reason = reason;
+ 		__entry->activate = activate;
++>>>>>>> 320af55a930f (KVM: x86: Add wrappers for setting/clearing APICv inhibits)
  	),
  
 -	TP_printk("%s reason=%u",
 +	TP_printk("%s bit=%lu",
  		  __entry->activate ? "activate" : "deactivate",
 -		  __entry->reason)
 +		  __entry->bit)
  );
  
  TRACE_EVENT(kvm_apicv_accept_irq,
diff --cc arch/x86/kvm/x86.c
index e014b7ea74df,ede81264c06f..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -9700,7 -9744,8 +9700,12 @@@ out
  }
  EXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);
  
++<<<<<<< HEAD
 +void __kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
++=======
+ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
+ 				      enum kvm_apicv_inhibit reason, bool set)
++>>>>>>> 320af55a930f (KVM: x86: Add wrappers for setting/clearing APICv inhibits)
  {
  	unsigned long old, new;
  
@@@ -9711,13 -9756,13 +9716,23 @@@
  
  	old = new = kvm->arch.apicv_inhibit_reasons;
  
++<<<<<<< HEAD
 +	if (activate)
 +		__clear_bit(bit, &new);
 +	else
 +		__set_bit(bit, &new);
 +
 +	if (!!old != !!new) {
 +		trace_kvm_apicv_update_request(activate, bit);
++=======
+ 	if (set)
+ 		__set_bit(reason, &new);
+ 	else
+ 		__clear_bit(reason, &new);
+ 
+ 	if (!!old != !!new) {
+ 		trace_kvm_apicv_update_request(reason, !set);
++>>>>>>> 320af55a930f (KVM: x86: Add wrappers for setting/clearing APICv inhibits)
  		/*
  		 * Kick all vCPUs before setting apicv_inhibit_reasons to avoid
  		 * false positives in the sanity check WARN in svm_vcpu_run().
@@@ -9736,20 -9781,22 +9751,29 @@@
  			unsigned long gfn = gpa_to_gfn(APIC_DEFAULT_PHYS_BASE);
  			kvm_zap_gfn_range(kvm, gfn, gfn+1);
  		}
 -	} else {
 +	} else
  		kvm->arch.apicv_inhibit_reasons = new;
 -	}
  }
  
++<<<<<<< HEAD
 +void kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)
++=======
+ void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
+ 				    enum kvm_apicv_inhibit reason, bool set)
++>>>>>>> 320af55a930f (KVM: x86: Add wrappers for setting/clearing APICv inhibits)
  {
  	if (!enable_apicv)
  		return;
  
  	down_write(&kvm->arch.apicv_update_lock);
++<<<<<<< HEAD
 +	__kvm_request_apicv_update(kvm, activate, bit);
++=======
+ 	__kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
++>>>>>>> 320af55a930f (KVM: x86: Add wrappers for setting/clearing APICv inhibits)
  	up_write(&kvm->arch.apicv_update_lock);
  }
- EXPORT_SYMBOL_GPL(kvm_request_apicv_update);
+ EXPORT_SYMBOL_GPL(kvm_set_or_clear_apicv_inhibit);
  
  static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)
  {
@@@ -10869,9 -10944,9 +10893,9 @@@ int kvm_arch_vcpu_ioctl_set_sregs(struc
  
  static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
  {
- 	bool inhibit = false;
+ 	bool set = false;
  	struct kvm_vcpu *vcpu;
 -	unsigned long i;
 +	int i;
  
  	down_write(&kvm->arch.apicv_update_lock);
  
* Unmerged path arch/x86/include/asm/kvm_host.h
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 95403166bf23..e7f019aa0270 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -122,9 +122,13 @@ static void synic_update_vector(struct kvm_vcpu_hv_synic *synic,
 	else
 		hv->synic_auto_eoi_used--;
 
-	__kvm_request_apicv_update(vcpu->kvm,
-				   !hv->synic_auto_eoi_used,
-				   APICV_INHIBIT_REASON_HYPERV);
+	/*
+	 * Inhibit APICv if any vCPU is using SynIC's AutoEOI, which relies on
+	 * the hypervisor to manually inject IRQs.
+	 */
+	__kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+					 APICV_INHIBIT_REASON_HYPERV,
+					 !!hv->synic_auto_eoi_used);
 
 	up_write(&vcpu->kvm->arch.apicv_update_lock);
 }
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index 5a69cce4d72d..d28f5808630f 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -305,15 +305,13 @@ void kvm_pit_set_reinject(struct kvm_pit *pit, bool reinject)
 	 * So, deactivate APICv when PIT is in reinject mode.
 	 */
 	if (reinject) {
-		kvm_request_apicv_update(kvm, false,
-					 APICV_INHIBIT_REASON_PIT_REINJ);
+		kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
 		/* The initial state is preserved while ps->reinject == 0. */
 		kvm_pit_reset_reinject(pit);
 		kvm_register_irq_ack_notifier(kvm, &ps->irq_ack_notifier);
 		kvm_register_irq_mask_notifier(kvm, 0, &pit->mask_notifier);
 	} else {
-		kvm_request_apicv_update(kvm, true,
-					 APICV_INHIBIT_REASON_PIT_REINJ);
+		kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
 		kvm_unregister_irq_ack_notifier(kvm, &ps->irq_ack_notifier);
 		kvm_unregister_irq_mask_notifier(kvm, 0, &pit->mask_notifier);
 	}
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 9b0fddf02548..ca49fe2f3fbb 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -2923,7 +2923,7 @@ static int interrupt_window_interception(struct kvm_vcpu *vcpu)
 	 * In this case AVIC was temporarily disabled for
 	 * requesting the IRQ window and we have to re-enable it.
 	 */
-	kvm_request_apicv_update(vcpu->kvm, true, APICV_INHIBIT_REASON_IRQWIN);
+	kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
 
 	++vcpu->stat.irq_window_exits;
 	return 1;
@@ -3487,7 +3487,7 @@ static void svm_enable_irq_window(struct kvm_vcpu *vcpu)
 		 * via AVIC. In such case, we need to temporarily disable AVIC,
 		 * and fallback to injecting IRQ via V_IRQ.
 		 */
-		kvm_request_apicv_update(vcpu->kvm, false, APICV_INHIBIT_REASON_IRQWIN);
+		kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
 		svm_set_vintr(svm);
 	}
 }
@@ -3934,6 +3934,7 @@ static void svm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 	struct kvm_cpuid_entry2 *best;
+	struct kvm *kvm = vcpu->kvm;
 
 	vcpu->arch.xsaves_enabled = guest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&
 				    boot_cpu_has(X86_FEATURE_XSAVE) &&
@@ -3960,16 +3961,14 @@ static void svm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 		 * is exposed to the guest, disable AVIC.
 		 */
 		if (guest_cpuid_has(vcpu, X86_FEATURE_X2APIC))
-			kvm_request_apicv_update(vcpu->kvm, false,
-						 APICV_INHIBIT_REASON_X2APIC);
+			kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_X2APIC);
 
 		/*
 		 * Currently, AVIC does not work with nested virtualization.
 		 * So, we disable AVIC when cpuid for SVM is set in the L1 guest.
 		 */
 		if (nested && guest_cpuid_has(vcpu, X86_FEATURE_SVM))
-			kvm_request_apicv_update(vcpu->kvm, false,
-						 APICV_INHIBIT_REASON_NESTED);
+			kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_NESTED);
 	}
 	init_vmcb_after_set_cpuid(vcpu);
 }
* Unmerged path arch/x86/kvm/trace.h
* Unmerged path arch/x86/kvm/x86.c
