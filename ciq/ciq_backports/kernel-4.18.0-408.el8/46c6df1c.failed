dmaengine: idxd: add helper for per interrupt handle drain

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 46c6df1c958e55558212cfa94cad201eae48d684
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/46c6df1c.failed

The helper is called at the completion of the interrupt handle refresh
event. It issues drain descriptors to each of the wq with associated
interrupt handle. The drain descriptor will have interrupt request set but
without completion record. This will ensure all descriptors with incorrect
interrupt completion handle get drained and a completion interrupt is
triggered for the guest driver to process them.

	Reviewed-by: Kevin Tian <kevin.tian@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/163528418315.3925689.7944718440052849626.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 46c6df1c958e55558212cfa94cad201eae48d684)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/irq.c
diff --cc drivers/dma/idxd/irq.c
index 7a2cf0512501,5434f702901a..000000000000
--- a/drivers/dma/idxd/irq.c
+++ b/drivers/dma/idxd/irq.c
@@@ -62,44 -55,43 +62,84 @@@ static void idxd_device_reinit(struct w
  	idxd_device_clear_state(idxd);
  }
  
++<<<<<<< HEAD
 +static void idxd_device_fault_work(struct work_struct *work)
 +{
 +	struct idxd_fault *fault = container_of(work, struct idxd_fault, work);
 +	struct idxd_irq_entry *ie;
 +	int i;
 +	int processed;
 +	int irqcnt = fault->idxd->num_wq_irqs + 1;
 +
 +	for (i = 1; i < irqcnt; i++) {
 +		ie = &fault->idxd->irq_entries[i];
 +		irq_process_work_list(ie, IRQ_WORK_PROCESS_FAULT,
 +				      &processed, fault->addr);
 +		if (processed)
 +			break;
 +
 +		irq_process_pending_llist(ie, IRQ_WORK_PROCESS_FAULT,
 +					  &processed, fault->addr);
 +		if (processed)
 +			break;
 +	}
 +
 +	kfree(fault);
 +}
 +
 +static int idxd_device_schedule_fault_process(struct idxd_device *idxd,
 +					      u64 fault_addr)
 +{
 +	struct idxd_fault *fault;
 +
 +	fault = kmalloc(sizeof(*fault), GFP_ATOMIC);
 +	if (!fault)
 +		return -ENOMEM;
 +
 +	fault->addr = fault_addr;
 +	fault->idxd = idxd;
 +	INIT_WORK(&fault->work, idxd_device_fault_work);
 +	queue_work(idxd->wq, &fault->work);
 +	return 0;
++=======
+ /*
+  * The function sends a drain descriptor for the interrupt handle. The drain ensures
+  * all descriptors with this interrupt handle is flushed and the interrupt
+  * will allow the cleanup of the outstanding descriptors.
+  */
+ static void idxd_int_handle_revoke_drain(struct idxd_irq_entry *ie)
+ {
+ 	struct idxd_wq *wq = ie->wq;
+ 	struct idxd_device *idxd = ie->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	struct dsa_hw_desc desc = {};
+ 	void __iomem *portal;
+ 	int rc;
+ 
+ 	/* Issue a simple drain operation with interrupt but no completion record */
+ 	desc.flags = IDXD_OP_FLAG_RCI;
+ 	desc.opcode = DSA_OPCODE_DRAIN;
+ 	desc.priv = 1;
+ 
+ 	if (ie->pasid != INVALID_IOASID)
+ 		desc.pasid = ie->pasid;
+ 	desc.int_handle = ie->int_handle;
+ 	portal = idxd_wq_portal_addr(wq);
+ 
+ 	/*
+ 	 * The wmb() makes sure that the descriptor is all there before we
+ 	 * issue.
+ 	 */
+ 	wmb();
+ 	if (wq_dedicated(wq)) {
+ 		iosubmit_cmds512(portal, &desc, 1);
+ 	} else {
+ 		rc = enqcmds(portal, &desc);
+ 		/* This should not fail unless hardware failed. */
+ 		if (rc < 0)
+ 			dev_warn(dev, "Failed to submit drain desc on wq %d\n", wq->id);
+ 	}
++>>>>>>> 46c6df1c958e (dmaengine: idxd: add helper for per interrupt handle drain)
  }
  
  static int process_misc_interrupts(struct idxd_device *idxd, u32 cause)
* Unmerged path drivers/dma/idxd/irq.c
