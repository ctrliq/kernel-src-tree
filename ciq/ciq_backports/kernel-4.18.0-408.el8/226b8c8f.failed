KVM: x86/mmu: Require mmu_lock be held for write in unyielding root iter

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit 226b8c8f85e4246f31947be1c11bf36208fe9052
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/226b8c8f.failed

Assert that mmu_lock is held for write by users of the yield-unfriendly
TDP iterator.  The nature of a shared walk means that the caller needs to
play nice with other tasks modifying the page tables, which is more or
less the same thing as playing nice with yielding.  Theoretically, KVM
could gain a flow where it could legitimately take mmu_lock for read in
a non-preemptible context, but that's highly unlikely and any such case
should be viewed with a fair amount of scrutiny.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Reviewed-by: Ben Gardon <bgardon@google.com>
Message-Id: <20220226001546.360188-7-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 226b8c8f85e4246f31947be1c11bf36208fe9052)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index abdf98ed849c,30424fbceb5f..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -25,6 -25,20 +25,23 @@@ void kvm_mmu_init_tdp_mmu(struct kvm *k
  	INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
  	spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
  	INIT_LIST_HEAD(&kvm->arch.tdp_mmu_pages);
++<<<<<<< HEAD
++=======
+ 
+ 	return true;
+ }
+ 
+ /* Arbitrarily returns true so that this may be used in if statements. */
+ static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
+ 							     bool shared)
+ {
+ 	if (shared)
+ 		lockdep_assert_held_read(&kvm->mmu_lock);
+ 	else
+ 		lockdep_assert_held_write(&kvm->mmu_lock);
+ 
+ 	return true;
++>>>>>>> 226b8c8f85e4 (KVM: x86/mmu: Require mmu_lock be held for write in unyielding root iter)
  }
  
  void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
@@@ -132,42 -157,53 +149,62 @@@ static struct kvm_mmu_page *tdp_mmu_nex
   * This makes it safe to release the MMU lock and yield within the loop, but
   * if exiting the loop early, the caller must drop the reference to the most
   * recent root. (Unless keeping a live reference is desirable.)
 - *
 - * If shared is set, this function is operating under the MMU lock in read
 - * mode. In the unlikely event that this thread must free a root, the lock
 - * will be temporarily dropped and reacquired in write mode.
   */
 -#define __for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, _only_valid)\
 -	for (_root = tdp_mmu_next_root(_kvm, NULL, _shared, _only_valid);	\
 -	     _root;								\
 -	     _root = tdp_mmu_next_root(_kvm, _root, _shared, _only_valid))	\
 -		if (kvm_mmu_page_as_id(_root) != _as_id) {			\
 +#define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id)	\
 +	for (_root = tdp_mmu_next_root(_kvm, NULL);		\
 +	     _root;						\
 +	     _root = tdp_mmu_next_root(_kvm, _root))		\
 +		if (kvm_mmu_page_as_id(_root) != _as_id) {	\
  		} else
  
++<<<<<<< HEAD
 +#define for_each_tdp_mmu_root(_kvm, _root, _as_id)				\
 +	list_for_each_entry_rcu(_root, &_kvm->arch.tdp_mmu_roots, link,		\
 +				lockdep_is_held_type(&kvm->mmu_lock, 0) ||	\
 +				lockdep_is_held(&kvm->arch.tdp_mmu_pages_lock))	\
 +		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
++=======
+ #define for_each_valid_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)	\
+ 	__for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, true)
+ 
+ #define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)		\
+ 	__for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, false)
+ 
+ /*
+  * Iterate over all TDP MMU roots.  Requires that mmu_lock be held for write,
+  * the implication being that any flow that holds mmu_lock for read is
+  * inherently yield-friendly and should use the yield-safe variant above.
+  * Holding mmu_lock for write obviates the need for RCU protection as the list
+  * is guaranteed to be stable.
+  */
+ #define for_each_tdp_mmu_root(_kvm, _root, _as_id)			\
+ 	list_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link)	\
+ 		if (kvm_lockdep_assert_mmu_lock_held(_kvm, false) &&	\
+ 		    kvm_mmu_page_as_id(_root) != _as_id) {		\
++>>>>>>> 226b8c8f85e4 (KVM: x86/mmu: Require mmu_lock be held for write in unyielding root iter)
  		} else
  
 -static struct kvm_mmu_page *tdp_mmu_alloc_sp(struct kvm_vcpu *vcpu)
 +static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 +						   int level)
  {
 -	struct kvm_mmu_page *sp;
 +	union kvm_mmu_page_role role;
  
 -	sp = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
 -	sp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);
 +	role = vcpu->arch.mmu->mmu_role.base;
 +	role.level = level;
  
 -	return sp;
 +	return role;
  }
  
 -static void tdp_mmu_init_sp(struct kvm_mmu_page *sp, gfn_t gfn,
 -			      union kvm_mmu_page_role role)
 +static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 +					       int level)
  {
 +	struct kvm_mmu_page *sp;
 +
 +	sp = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);
 +	sp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);
  	set_page_private(virt_to_page(sp->spt), (unsigned long)sp);
  
 -	sp->role = role;
 +	sp->role.word = page_role_for_level(vcpu, level).word;
  	sp->gfn = gfn;
  	sp->tdp_mmu_page = true;
  
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
