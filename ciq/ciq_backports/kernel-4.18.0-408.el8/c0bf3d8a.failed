net/smc: Transitional solution for clcsock race issue

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Wen Gu <guwen@linux.alibaba.com>
commit c0bf3d8a943b6f2e912b7c1de03e2ef28e76f760
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/c0bf3d8a.failed

We encountered a crash in smc_setsockopt() and it is caused by
accessing smc->clcsock after clcsock was released.

 BUG: kernel NULL pointer dereference, address: 0000000000000020
 #PF: supervisor read access in kernel mode
 #PF: error_code(0x0000) - not-present page
 PGD 0 P4D 0
 Oops: 0000 [#1] PREEMPT SMP PTI
 CPU: 1 PID: 50309 Comm: nginx Kdump: loaded Tainted: G E     5.16.0-rc4+ #53
 RIP: 0010:smc_setsockopt+0x59/0x280 [smc]
 Call Trace:
  <TASK>
  __sys_setsockopt+0xfc/0x190
  __x64_sys_setsockopt+0x20/0x30
  do_syscall_64+0x34/0x90
  entry_SYSCALL_64_after_hwframe+0x44/0xae
 RIP: 0033:0x7f16ba83918e
  </TASK>

This patch tries to fix it by holding clcsock_release_lock and
checking whether clcsock has already been released before access.

In case that a crash of the same reason happens in smc_getsockopt()
or smc_switch_to_fallback(), this patch also checkes smc->clcsock
in them too. And the caller of smc_switch_to_fallback() will identify
whether fallback succeeds according to the return value.

Fixes: fd57770dd198 ("net/smc: wait for pending work before clcsock release_sock")
Link: https://lore.kernel.org/lkml/5dd7ffd1-28e2-24cc-9442-1defec27375e@linux.ibm.com/T/
	Signed-off-by: Wen Gu <guwen@linux.alibaba.com>
	Acked-by: Karsten Graul <kgraul@linux.ibm.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit c0bf3d8a943b6f2e912b7c1de03e2ef28e76f760)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/smc/af_smc.c
diff --cc net/smc/af_smc.c
index edef9e8bdc0d,d5ea62b82bb8..000000000000
--- a/net/smc/af_smc.c
+++ b/net/smc/af_smc.c
@@@ -563,17 -566,40 +563,47 @@@ static void smc_stat_fallback(struct sm
  	mutex_unlock(&net->smc.mutex_fback_rsn);
  }
  
- static void smc_switch_to_fallback(struct smc_sock *smc, int reason_code)
+ static int smc_switch_to_fallback(struct smc_sock *smc, int reason_code)
  {
++<<<<<<< HEAD
++=======
+ 	wait_queue_head_t *smc_wait = sk_sleep(&smc->sk);
+ 	wait_queue_head_t *clc_wait;
+ 	unsigned long flags;
+ 
+ 	mutex_lock(&smc->clcsock_release_lock);
+ 	if (!smc->clcsock) {
+ 		mutex_unlock(&smc->clcsock_release_lock);
+ 		return -EBADF;
+ 	}
++>>>>>>> c0bf3d8a943b (net/smc: Transitional solution for clcsock race issue)
  	smc->use_fallback = true;
  	smc->fallback_rsn = reason_code;
  	smc_stat_fallback(smc);
  	if (smc->sk.sk_socket && smc->sk.sk_socket->file) {
  		smc->clcsock->file = smc->sk.sk_socket->file;
  		smc->clcsock->file->private_data = smc->clcsock;
++<<<<<<< HEAD
 +		smc->clcsock->wq->fasync_list =
 +			smc->sk.sk_socket->wq->fasync_list;
- 	}
++=======
+ 		smc->clcsock->wq.fasync_list =
+ 			smc->sk.sk_socket->wq.fasync_list;
+ 
+ 		/* There may be some entries remaining in
+ 		 * smc socket->wq, which should be removed
+ 		 * to clcsocket->wq during the fallback.
+ 		 */
+ 		clc_wait = sk_sleep(smc->clcsock->sk);
+ 		spin_lock_irqsave(&smc_wait->lock, flags);
+ 		spin_lock_nested(&clc_wait->lock, SINGLE_DEPTH_NESTING);
+ 		list_splice_init(&smc_wait->head, &clc_wait->head);
+ 		spin_unlock(&clc_wait->lock);
+ 		spin_unlock_irqrestore(&smc_wait->lock, flags);
++>>>>>>> c0bf3d8a943b (net/smc: Transitional solution for clcsock race issue)
+ 	}
+ 	mutex_unlock(&smc->clcsock_release_lock);
+ 	return 0;
  }
  
  /* fall back during connect */
@@@ -2427,8 -2470,16 +2472,21 @@@ static int smc_setsockopt(struct socke
  	/* generic setsockopts reaching us here always apply to the
  	 * CLC socket
  	 */
++<<<<<<< HEAD
 +	rc = smc->clcsock->ops->setsockopt(smc->clcsock, level, optname,
 +					   optval, optlen);
++=======
+ 	mutex_lock(&smc->clcsock_release_lock);
+ 	if (!smc->clcsock) {
+ 		mutex_unlock(&smc->clcsock_release_lock);
+ 		return -EBADF;
+ 	}
+ 	if (unlikely(!smc->clcsock->ops->setsockopt))
+ 		rc = -EOPNOTSUPP;
+ 	else
+ 		rc = smc->clcsock->ops->setsockopt(smc->clcsock, level, optname,
+ 						   optval, optlen);
++>>>>>>> c0bf3d8a943b (net/smc: Transitional solution for clcsock race issue)
  	if (smc->clcsock->sk->sk_err) {
  		sk->sk_err = smc->clcsock->sk->sk_err;
  		sk_error_report(sk);
@@@ -2493,11 -2544,23 +2552,28 @@@ static int smc_getsockopt(struct socke
  			  char __user *optval, int __user *optlen)
  {
  	struct smc_sock *smc;
+ 	int rc;
  
  	smc = smc_sk(sock->sk);
+ 	mutex_lock(&smc->clcsock_release_lock);
+ 	if (!smc->clcsock) {
+ 		mutex_unlock(&smc->clcsock_release_lock);
+ 		return -EBADF;
+ 	}
  	/* socket options apply to the CLC socket */
++<<<<<<< HEAD
 +	return smc->clcsock->ops->getsockopt(smc->clcsock, level, optname,
 +					     optval, optlen);
++=======
+ 	if (unlikely(!smc->clcsock->ops->getsockopt)) {
+ 		mutex_unlock(&smc->clcsock_release_lock);
+ 		return -EOPNOTSUPP;
+ 	}
+ 	rc = smc->clcsock->ops->getsockopt(smc->clcsock, level, optname,
+ 					   optval, optlen);
+ 	mutex_unlock(&smc->clcsock_release_lock);
+ 	return rc;
++>>>>>>> c0bf3d8a943b (net/smc: Transitional solution for clcsock race issue)
  }
  
  static int smc_ioctl(struct socket *sock, unsigned int cmd,
* Unmerged path net/smc/af_smc.c
