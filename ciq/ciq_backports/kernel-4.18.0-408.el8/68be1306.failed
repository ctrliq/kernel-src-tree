KVM: x86/mmu: Fold rmap_recycle into rmap_add

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit 68be1306caea8948738cab04014ca4506b590d38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/68be1306.failed

Consolidate rmap_recycle and rmap_add into a single function since they
are only ever called together (and only from one place). This has a nice
side effect of eliminating an extra kvm_vcpu_gfn_to_memslot(). In
addition it makes mmu_set_spte(), which is a very long function, a
little shorter.

No functional change intended.

	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20210813203504.2742757-3-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 68be1306caea8948738cab04014ca4506b590d38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 4d7446c022d2,8b6dc276935f..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -1047,20 -1071,6 +1047,23 @@@ static bool rmap_can_add(struct kvm_vcp
  	return kvm_mmu_memory_cache_nr_free_objects(mc);
  }
  
++<<<<<<< HEAD
 +static int rmap_add(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
 +{
 +	struct kvm_memory_slot *slot;
 +	struct kvm_mmu_page *sp;
 +	struct kvm_rmap_head *rmap_head;
 +
 +	sp = sptep_to_sp(spte);
 +	kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);
 +	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
 +	rmap_head = __gfn_to_rmap(gfn, sp->role.level, slot);
 +	return pte_list_add(vcpu, spte, rmap_head);
 +}
 +
 +
++=======
++>>>>>>> 68be1306caea (KVM: x86/mmu: Fold rmap_recycle into rmap_add)
  static void rmap_remove(struct kvm *kvm, u64 *spte)
  {
  	struct kvm_memslots *slots;
@@@ -1640,28 -1625,35 +1643,41 @@@ static int kvm_test_age_rmapp(struct kv
  
  #define RMAP_RECYCLE_THRESHOLD 1000
  
- static void rmap_recycle(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
+ static void rmap_add(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
  {
  	struct kvm_memory_slot *slot;
- 	struct kvm_rmap_head *rmap_head;
  	struct kvm_mmu_page *sp;
+ 	struct kvm_rmap_head *rmap_head;
+ 	int rmap_count;
  
  	sp = sptep_to_sp(spte);
+ 	kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);
  	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
++<<<<<<< HEAD
 +	rmap_head = __gfn_to_rmap(gfn, sp->role.level, slot);
 +
 +	kvm_unmap_rmapp(vcpu->kvm, rmap_head, NULL, gfn, sp->role.level, 0);
 +	kvm_flush_remote_tlbs_with_address(vcpu->kvm, sp->gfn,
 +			KVM_PAGES_PER_HPAGE(sp->role.level));
++=======
+ 	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
+ 	rmap_count = pte_list_add(vcpu, spte, rmap_head);
+ 
+ 	if (rmap_count > RMAP_RECYCLE_THRESHOLD) {
+ 		kvm_unmap_rmapp(vcpu->kvm, rmap_head, NULL, gfn, sp->role.level, __pte(0));
+ 		kvm_flush_remote_tlbs_with_address(
+ 				vcpu->kvm, sp->gfn, KVM_PAGES_PER_HPAGE(sp->role.level));
+ 	}
++>>>>>>> 68be1306caea (KVM: x86/mmu: Fold rmap_recycle into rmap_add)
  }
  
 -bool kvm_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 +int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end)
  {
 -	bool young = false;
 -
 -	if (kvm_memslots_have_rmaps(kvm))
 -		young = kvm_handle_gfn_range(kvm, range, kvm_age_rmapp);
 +	int young = false;
  
 +	young = kvm_handle_hva_range(kvm, start, end, 0, kvm_age_rmapp);
  	if (is_tdp_mmu_enabled(kvm))
 -		young |= kvm_tdp_mmu_age_gfn_range(kvm, range);
 +		young |= kvm_tdp_mmu_age_hva_range(kvm, start, end);
  
  	return young;
  }
@@@ -2740,11 -2761,8 +2755,16 @@@ static int mmu_set_spte(struct kvm_vcp
  	trace_kvm_mmu_set_spte(level, gfn, sptep);
  
  	if (!was_rmapped) {
++<<<<<<< HEAD
 +		if (is_large_pte(*sptep))
 +			++vcpu->kvm->stat.lpages;
 +		rmap_count = rmap_add(vcpu, sptep, gfn);
 +		if (rmap_count > RMAP_RECYCLE_THRESHOLD)
 +			rmap_recycle(vcpu, sptep, gfn);
++=======
+ 		kvm_update_page_stats(vcpu->kvm, level, 1);
+ 		rmap_add(vcpu, sptep, gfn);
++>>>>>>> 68be1306caea (KVM: x86/mmu: Fold rmap_recycle into rmap_add)
  	}
  
  	return ret;
* Unmerged path arch/x86/kvm/mmu/mmu.c
