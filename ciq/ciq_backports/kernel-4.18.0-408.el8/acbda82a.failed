KVM: x86/mmu: Require mmu_lock be held for write to zap TDP MMU range

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit acbda82a81c716ff5df4450d997340a4fe5650c4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/acbda82a.failed

Now that all callers of zap_gfn_range() hold mmu_lock for write, drop
support for zapping with mmu_lock held for read.  That all callers hold
mmu_lock for write isn't a random coincidence; now that the paths that
need to zap _everything_ have their own path, the only callers left are
those that need to zap for functional correctness.  And when zapping is
required for functional correctness, mmu_lock must be held for write,
otherwise the caller has no guarantees about the state of the TDP MMU
page tables after it has run, e.g. the SPTE(s) it zapped can be
immediately replaced by a vCPU faulting in a page.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Reviewed-by: Ben Gardon <bgardon@google.com>
Message-Id: <20220226001546.360188-17-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit acbda82a81c716ff5df4450d997340a4fe5650c4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 97bb57fe39ca,f3939ce4a115..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -709,20 -843,29 +709,42 @@@ static inline bool __must_check tdp_mmu
   * scheduler needs the CPU or there is contention on the MMU lock. If this
   * function cannot yield, it will not release the MMU lock or reschedule and
   * the caller must ensure it does not supply too large a GFN range, or the
++<<<<<<< HEAD
 + * operation can cause a soft lockup.  Note, in some use cases a flush may be
 + * required by prior actions.  Ensure the pending flush is performed prior to
 + * yielding.
++=======
+  * operation can cause a soft lockup.
++>>>>>>> acbda82a81c7 (KVM: x86/mmu: Require mmu_lock be held for write to zap TDP MMU range)
   */
  static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
  			  gfn_t start, gfn_t end, bool can_yield, bool flush)
  {
 -	bool zap_all = (start == 0 && end >= tdp_mmu_max_gfn_host());
  	struct tdp_iter iter;
  
++<<<<<<< HEAD
 +	rcu_read_lock();
 +
 +	tdp_root_for_each_pte(iter, root, start, end) {
 +		if (can_yield &&
 +		    tdp_mmu_iter_cond_resched(kvm, &iter, flush)) {
++=======
+ 	/*
+ 	 * No need to try to step down in the iterator when zapping all SPTEs,
+ 	 * zapping the top-level non-leaf SPTEs will recurse on their children.
+ 	 */
+ 	int min_level = zap_all ? root->role.level : PG_LEVEL_4K;
+ 
+ 	end = min(end, tdp_mmu_max_gfn_host());
+ 
+ 	lockdep_assert_held_write(&kvm->mmu_lock);
+ 
+ 	rcu_read_lock();
+ 
+ 	for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
+ 		if (can_yield &&
+ 		    tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {
++>>>>>>> acbda82a81c7 (KVM: x86/mmu: Require mmu_lock be held for write to zap TDP MMU range)
  			flush = false;
  			continue;
  		}
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
