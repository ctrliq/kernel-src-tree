KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit cb00a70bd4b7e42dcbd6cd80b3f1697b10cdb44e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/cb00a70b.failed

When using KVM_DIRTY_LOG_INITIALLY_SET, huge pages are not
write-protected when dirty logging is enabled on the memslot. Instead
they are write-protected once userspace invokes KVM_CLEAR_DIRTY_LOG for
the first time and only for the specific sub-region being cleared.

Enhance KVM_CLEAR_DIRTY_LOG to also try to split huge pages prior to
write-protecting to avoid causing write-protection faults on vCPU
threads. This also allows userspace to smear the cost of huge page
splitting across multiple ioctls, rather than splitting the entire
memslot as is the case when initially-all-set is not used.

	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20220119230739.2234394-17-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit cb00a70bd4b7e42dcbd6cd80b3f1697b10cdb44e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/admin-guide/kernel-parameters.txt
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.h
#	arch/x86/kvm/x86.c
diff --cc Documentation/admin-guide/kernel-parameters.txt
index 1a1eb4e957f1,2a9746fe6c4a..000000000000
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@@ -2114,6 -2339,32 +2114,35 @@@
  	kvm.ignore_msrs=[KVM] Ignore guest accesses to unhandled MSRs.
  			Default is 0 (don't ignore, but inject #GP)
  
++<<<<<<< HEAD
++=======
+ 	kvm.eager_page_split=
+ 			[KVM,X86] Controls whether or not KVM will try to
+ 			proactively split all huge pages during dirty logging.
+ 			Eager page splitting reduces interruptions to vCPU
+ 			execution by eliminating the write-protection faults
+ 			and MMU lock contention that would otherwise be
+ 			required to split huge pages lazily.
+ 
+ 			VM workloads that rarely perform writes or that write
+ 			only to a small region of VM memory may benefit from
+ 			disabling eager page splitting to allow huge pages to
+ 			still be used for reads.
+ 
+ 			The behavior of eager page splitting depends on whether
+ 			KVM_DIRTY_LOG_INITIALLY_SET is enabled or disabled. If
+ 			disabled, all huge pages in a memslot will be eagerly
+ 			split when dirty logging is enabled on that memslot. If
+ 			enabled, eager page splitting will be performed during
+ 			the KVM_CLEAR_DIRTY ioctl, and only for the pages being
+ 			cleared.
+ 
+ 			Eager page splitting currently only supports splitting
+ 			huge pages mapped by the TDP MMU.
+ 
+ 			Default is Y (on).
+ 
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
  	kvm.enable_vmware_backdoor=[KVM] Support VMware backdoor PV interface.
  				   Default is false (don't support).
  
diff --cc arch/x86/include/asm/kvm_host.h
index 96ecf6f2c1a5,10815b672a26..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1574,8 -1585,15 +1574,18 @@@ void kvm_mmu_uninit_vm(struct kvm *kvm)
  void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu);
  void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
  void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 -				      const struct kvm_memory_slot *memslot,
 +				      struct kvm_memory_slot *memslot,
  				      int start_level);
++<<<<<<< HEAD
++=======
+ void kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,
+ 				       const struct kvm_memory_slot *memslot,
+ 				       int target_level);
+ void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
+ 				  const struct kvm_memory_slot *memslot,
+ 				  u64 start, u64 end,
+ 				  int target_level);
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
  void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
  				   const struct kvm_memory_slot *memslot);
  void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
diff --cc arch/x86/kvm/mmu/mmu.c
index e1bc11990a29,296f8723f9ae..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -5740,9 -5833,49 +5743,52 @@@ void kvm_mmu_slot_remove_write_access(s
  		kvm_arch_flush_remote_tlbs_memslot(kvm, memslot);
  }
  
++<<<<<<< HEAD
++=======
+ /* Must be called with the mmu_lock held in write-mode. */
+ void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
+ 				   const struct kvm_memory_slot *memslot,
+ 				   u64 start, u64 end,
+ 				   int target_level)
+ {
+ 	if (is_tdp_mmu_enabled(kvm))
+ 		kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end,
+ 						 target_level, false);
+ 
+ 	/*
+ 	 * A TLB flush is unnecessary at this point for the same resons as in
+ 	 * kvm_mmu_slot_try_split_huge_pages().
+ 	 */
+ }
+ 
+ void kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,
+ 					const struct kvm_memory_slot *memslot,
+ 					int target_level)
+ {
+ 	u64 start = memslot->base_gfn;
+ 	u64 end = start + memslot->npages;
+ 
+ 	if (is_tdp_mmu_enabled(kvm)) {
+ 		read_lock(&kvm->mmu_lock);
+ 		kvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, true);
+ 		read_unlock(&kvm->mmu_lock);
+ 	}
+ 
+ 	/*
+ 	 * No TLB flush is necessary here. KVM will flush TLBs after
+ 	 * write-protecting and/or clearing dirty on the newly split SPTEs to
+ 	 * ensure that guest writes are reflected in the dirty log before the
+ 	 * ioctl to enable dirty logging on this memslot completes. Since the
+ 	 * split SPTEs retain the write and dirty bits of the huge SPTE, it is
+ 	 * safe for KVM to decide if a TLB flush is necessary based on the split
+ 	 * SPTEs.
+ 	 */
+ }
+ 
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
  static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,
  					 struct kvm_rmap_head *rmap_head,
 -					 const struct kvm_memory_slot *slot)
 +					 struct kvm_memory_slot *slot)
  {
  	u64 *sptep;
  	struct rmap_iterator iter;
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 00e34bc04af0,dae2cebcf8b5..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -919,6 -963,44 +919,47 @@@ static int tdp_mmu_map_handle_target_le
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * tdp_mmu_link_sp - Replace the given spte with an spte pointing to the
+  * provided page table.
+  *
+  * @kvm: kvm instance
+  * @iter: a tdp_iter instance currently on the SPTE that should be set
+  * @sp: The new TDP page table to install.
+  * @account_nx: True if this page table is being installed to split a
+  *              non-executable huge page.
+  * @shared: This operation is running under the MMU lock in read mode.
+  *
+  * Returns: 0 if the new page table was installed. Non-0 if the page table
+  *          could not be installed (e.g. the atomic compare-exchange failed).
+  */
+ static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
+ 			   struct kvm_mmu_page *sp, bool account_nx,
+ 			   bool shared)
+ {
+ 	u64 spte = make_nonleaf_spte(sp->spt, !shadow_accessed_mask);
+ 	int ret = 0;
+ 
+ 	if (shared) {
+ 		ret = tdp_mmu_set_spte_atomic(kvm, iter, spte);
+ 		if (ret)
+ 			return ret;
+ 	} else {
+ 		tdp_mmu_set_spte(kvm, iter, spte);
+ 	}
+ 
+ 	spin_lock(&kvm->arch.tdp_mmu_pages_lock);
+ 	list_add(&sp->link, &kvm->arch.tdp_mmu_pages);
+ 	if (account_nx)
+ 		account_huge_nx_page(kvm, sp);
+ 	spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
+ 
+ 	return 0;
+ }
+ 
+ /*
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
   * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
   * page tables and SPTEs to translate the faulting guest physical address.
   */
@@@ -982,20 -1054,10 +1023,24 @@@ int kvm_tdp_mmu_map(struct kvm_vcpu *vc
  			if (is_removed_spte(iter.old_spte))
  				break;
  
 -			sp = tdp_mmu_alloc_sp(vcpu);
 -			tdp_mmu_init_child_sp(sp, &iter);
 +			sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
 +			child_pt = sp->spt;
 +
++<<<<<<< HEAD
 +			new_spte = make_nonleaf_spte(child_pt,
 +						     !shadow_accessed_mask);
  
 +			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter,
 +						    new_spte)) {
 +				tdp_mmu_link_page(vcpu->kvm, sp, true,
 +						  huge_page_disallowed &&
 +						  req_level >= iter.level);
 +
 +				trace_kvm_mmu_get_page(sp, true);
 +			} else {
++=======
+ 			if (tdp_mmu_link_sp(vcpu->kvm, &iter, sp, account_nx, true)) {
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
  				tdp_mmu_free_sp(sp);
  				break;
  			}
@@@ -1268,6 -1263,186 +1313,189 @@@ bool kvm_tdp_mmu_wrprot_slot(struct kv
  	return spte_set;
  }
  
++<<<<<<< HEAD
++=======
+ static struct kvm_mmu_page *__tdp_mmu_alloc_sp_for_split(gfp_t gfp)
+ {
+ 	struct kvm_mmu_page *sp;
+ 
+ 	gfp |= __GFP_ZERO;
+ 
+ 	sp = kmem_cache_alloc(mmu_page_header_cache, gfp);
+ 	if (!sp)
+ 		return NULL;
+ 
+ 	sp->spt = (void *)__get_free_page(gfp);
+ 	if (!sp->spt) {
+ 		kmem_cache_free(mmu_page_header_cache, sp);
+ 		return NULL;
+ 	}
+ 
+ 	return sp;
+ }
+ 
+ static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
+ 						       struct tdp_iter *iter,
+ 						       bool shared)
+ {
+ 	struct kvm_mmu_page *sp;
+ 
+ 	/*
+ 	 * Since we are allocating while under the MMU lock we have to be
+ 	 * careful about GFP flags. Use GFP_NOWAIT to avoid blocking on direct
+ 	 * reclaim and to avoid making any filesystem callbacks (which can end
+ 	 * up invoking KVM MMU notifiers, resulting in a deadlock).
+ 	 *
+ 	 * If this allocation fails we drop the lock and retry with reclaim
+ 	 * allowed.
+ 	 */
+ 	sp = __tdp_mmu_alloc_sp_for_split(GFP_NOWAIT | __GFP_ACCOUNT);
+ 	if (sp)
+ 		return sp;
+ 
+ 	rcu_read_unlock();
+ 
+ 	if (shared)
+ 		read_unlock(&kvm->mmu_lock);
+ 	else
+ 		write_unlock(&kvm->mmu_lock);
+ 
+ 	iter->yielded = true;
+ 	sp = __tdp_mmu_alloc_sp_for_split(GFP_KERNEL_ACCOUNT);
+ 
+ 	if (shared)
+ 		read_lock(&kvm->mmu_lock);
+ 	else
+ 		write_lock(&kvm->mmu_lock);
+ 
+ 	rcu_read_lock();
+ 
+ 	return sp;
+ }
+ 
+ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
+ 				   struct kvm_mmu_page *sp, bool shared)
+ {
+ 	const u64 huge_spte = iter->old_spte;
+ 	const int level = iter->level;
+ 	int ret, i;
+ 
+ 	tdp_mmu_init_child_sp(sp, iter);
+ 
+ 	/*
+ 	 * No need for atomics when writing to sp->spt since the page table has
+ 	 * not been linked in yet and thus is not reachable from any other CPU.
+ 	 */
+ 	for (i = 0; i < PT64_ENT_PER_PAGE; i++)
+ 		sp->spt[i] = make_huge_page_split_spte(huge_spte, level, i);
+ 
+ 	/*
+ 	 * Replace the huge spte with a pointer to the populated lower level
+ 	 * page table. Since we are making this change without a TLB flush vCPUs
+ 	 * will see a mix of the split mappings and the original huge mapping,
+ 	 * depending on what's currently in their TLB. This is fine from a
+ 	 * correctness standpoint since the translation will be the same either
+ 	 * way.
+ 	 */
+ 	ret = tdp_mmu_link_sp(kvm, iter, sp, false, shared);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * tdp_mmu_link_sp_atomic() will handle subtracting the huge page we
+ 	 * are overwriting from the page stats. But we have to manually update
+ 	 * the page stats with the new present child pages.
+ 	 */
+ 	kvm_update_page_stats(kvm, level - 1, PT64_ENT_PER_PAGE);
+ 
+ 	return 0;
+ }
+ 
+ static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
+ 					 struct kvm_mmu_page *root,
+ 					 gfn_t start, gfn_t end,
+ 					 int target_level, bool shared)
+ {
+ 	struct kvm_mmu_page *sp = NULL;
+ 	struct tdp_iter iter;
+ 	int ret = 0;
+ 
+ 	rcu_read_lock();
+ 
+ 	/*
+ 	 * Traverse the page table splitting all huge pages above the target
+ 	 * level into one lower level. For example, if we encounter a 1GB page
+ 	 * we split it into 512 2MB pages.
+ 	 *
+ 	 * Since the TDP iterator uses a pre-order traversal, we are guaranteed
+ 	 * to visit an SPTE before ever visiting its children, which means we
+ 	 * will correctly recursively split huge pages that are more than one
+ 	 * level above the target level (e.g. splitting a 1GB to 512 2MB pages,
+ 	 * and then splitting each of those to 512 4KB pages).
+ 	 */
+ 	for_each_tdp_pte_min_level(iter, root, target_level + 1, start, end) {
+ retry:
+ 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ 			continue;
+ 
+ 		if (!is_shadow_present_pte(iter.old_spte) || !is_large_pte(iter.old_spte))
+ 			continue;
+ 
+ 		if (!sp) {
+ 			sp = tdp_mmu_alloc_sp_for_split(kvm, &iter, shared);
+ 			if (!sp) {
+ 				ret = -ENOMEM;
+ 				break;
+ 			}
+ 
+ 			if (iter.yielded)
+ 				continue;
+ 		}
+ 
+ 		if (tdp_mmu_split_huge_page(kvm, &iter, sp, shared))
+ 			goto retry;
+ 
+ 		sp = NULL;
+ 	}
+ 
+ 	rcu_read_unlock();
+ 
+ 	/*
+ 	 * It's possible to exit the loop having never used the last sp if, for
+ 	 * example, a vCPU doing HugePage NX splitting wins the race and
+ 	 * installs its own sp in place of the last sp we tried to split.
+ 	 */
+ 	if (sp)
+ 		tdp_mmu_free_sp(sp);
+ 
+ 
+ 	return ret;
+ }
+ 
+ 
+ /*
+  * Try to split all huge pages mapped by the TDP MMU down to the target level.
+  */
+ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
+ 				      const struct kvm_memory_slot *slot,
+ 				      gfn_t start, gfn_t end,
+ 				      int target_level, bool shared)
+ {
+ 	struct kvm_mmu_page *root;
+ 	int r = 0;
+ 
+ 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ 
+ 	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, shared) {
+ 		r = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);
+ 		if (r) {
+ 			kvm_tdp_mmu_put_root(kvm, root, shared);
+ 			break;
+ 		}
+ 	}
+ }
+ 
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
  /*
   * Clear the dirty status of all the SPTEs mapping GFNs in the memslot. If
   * AD bits are enabled, this will involve clearing the dirty bit on each SPTE.
diff --cc arch/x86/kvm/mmu/tdp_mmu.h
index e1f1ae8ec3e2,3f987785702a..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@@ -74,6 -67,11 +74,14 @@@ bool kvm_tdp_mmu_write_protect_gfn(stru
  				   struct kvm_memory_slot *slot, gfn_t gfn,
  				   int min_level);
  
++<<<<<<< HEAD
++=======
+ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
+ 				      const struct kvm_memory_slot *slot,
+ 				      gfn_t start, gfn_t end,
+ 				      int target_level, bool shared);
+ 
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
  static inline void kvm_tdp_mmu_walk_lockless_begin(void)
  {
  	rcu_read_lock();
diff --cc arch/x86/kvm/x86.c
index b7a417ad06ef,803b2e4c7b75..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -191,6 -192,9 +191,12 @@@ bool __read_mostly enable_pmu = true
  EXPORT_SYMBOL_GPL(enable_pmu);
  module_param(enable_pmu, bool, 0444);
  
++<<<<<<< HEAD
++=======
+ bool __read_mostly eager_page_split = true;
+ module_param(eager_page_split, bool, 0644);
+ 
++>>>>>>> cb00a70bd4b7 (KVM: x86/mmu: Split huge pages mapped by the TDP MMU during KVM_CLEAR_DIRTY_LOG)
  /*
   * Restoring the host value for MSRs that are only consumed when running in
   * usermode, e.g. SYSCALL MSRs and TSC_AUX, can be deferred until the CPU
* Unmerged path Documentation/admin-guide/kernel-parameters.txt
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.h
* Unmerged path arch/x86/kvm/x86.c
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 3b4de769abc5..6c3eebde39a9 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -352,6 +352,8 @@ extern int pi_inject_timer;
 
 extern bool report_ignored_msrs;
 
+extern bool eager_page_split;
+
 static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 {
 	return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
