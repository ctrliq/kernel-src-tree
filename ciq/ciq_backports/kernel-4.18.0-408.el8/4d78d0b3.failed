KVM: x86/mmu: Remove need for a vcpu from mmu_try_to_unsync_pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Ben Gardon <bgardon@google.com>
commit 4d78d0b39ad03e7357452a669938653a379cfebd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/4d78d0b3.failed

The vCPU argument to mmu_try_to_unsync_pages is now only used to get a
pointer to the associated struct kvm, so pass in the kvm pointer from
the beginning to remove the need for a vCPU when calling the function.

	Signed-off-by: Ben Gardon <bgardon@google.com>
Message-Id: <20211115234603.2908381-7-bgardon@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 4d78d0b39ad03e7357452a669938653a379cfebd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/mmu_internal.h
#	arch/x86/kvm/mmu/spte.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 970fc4a0eb93,29bcf26b0cb3..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2585,16 -2580,18 +2585,25 @@@ static void kvm_unsync_page(struct kvm 
   * were marked unsync (or if there is no shadow page), -EPERM if the SPTE must
   * be write-protected.
   */
++<<<<<<< HEAD
 +int mmu_try_to_unsync_pages(struct kvm_vcpu *vcpu, gfn_t gfn, bool can_unsync)
++=======
+ int mmu_try_to_unsync_pages(struct kvm *kvm, struct kvm_memory_slot *slot,
+ 			    gfn_t gfn, bool can_unsync, bool prefetch)
++>>>>>>> 4d78d0b39ad0 (KVM: x86/mmu: Remove need for a vcpu from mmu_try_to_unsync_pages)
  {
  	struct kvm_mmu_page *sp;
 -	bool locked = false;
  
  	/*
  	 * Force write-protection if the page is being tracked.  Note, the page
  	 * track machinery is used to write-protect upper-level shadow pages,
  	 * i.e. this guards the role.level == 4K assertion below!
  	 */
++<<<<<<< HEAD
 +	if (kvm_page_track_is_active(vcpu, gfn, KVM_PAGE_TRACK_WRITE))
++=======
+ 	if (kvm_slot_page_track_is_active(kvm, slot, gfn, KVM_PAGE_TRACK_WRITE))
++>>>>>>> 4d78d0b39ad0 (KVM: x86/mmu: Remove need for a vcpu from mmu_try_to_unsync_pages)
  		return -EPERM;
  
  	/*
@@@ -2610,9 -2607,37 +2619,43 @@@
  		if (sp->unsync)
  			continue;
  
++<<<<<<< HEAD
++=======
+ 		if (prefetch)
+ 			return -EEXIST;
+ 
+ 		/*
+ 		 * TDP MMU page faults require an additional spinlock as they
+ 		 * run with mmu_lock held for read, not write, and the unsync
+ 		 * logic is not thread safe.  Take the spinklock regardless of
+ 		 * the MMU type to avoid extra conditionals/parameters, there's
+ 		 * no meaningful penalty if mmu_lock is held for write.
+ 		 */
+ 		if (!locked) {
+ 			locked = true;
+ 			spin_lock(&kvm->arch.mmu_unsync_pages_lock);
+ 
+ 			/*
+ 			 * Recheck after taking the spinlock, a different vCPU
+ 			 * may have since marked the page unsync.  A false
+ 			 * positive on the unprotected check above is not
+ 			 * possible as clearing sp->unsync _must_ hold mmu_lock
+ 			 * for write, i.e. unsync cannot transition from 0->1
+ 			 * while this CPU holds mmu_lock for read (or write).
+ 			 */
+ 			if (READ_ONCE(sp->unsync))
+ 				continue;
+ 		}
+ 
++>>>>>>> 4d78d0b39ad0 (KVM: x86/mmu: Remove need for a vcpu from mmu_try_to_unsync_pages)
  		WARN_ON(sp->role.level != PG_LEVEL_4K);
- 		kvm_unsync_page(vcpu, sp);
+ 		kvm_unsync_page(kvm, sp);
  	}
++<<<<<<< HEAD
++=======
+ 	if (locked)
+ 		spin_unlock(&kvm->arch.mmu_unsync_pages_lock);
++>>>>>>> 4d78d0b39ad0 (KVM: x86/mmu: Remove need for a vcpu from mmu_try_to_unsync_pages)
  
  	/*
  	 * We need to ensure that the marking of unsync pages is visible
diff --cc arch/x86/kvm/mmu/mmu_internal.h
index a81dffc914d9,787b8c553b9e..000000000000
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@@ -118,16 -117,11 +118,21 @@@ static inline bool kvm_mmu_page_ad_need
  	return kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;
  }
  
++<<<<<<< HEAD
 +extern int nx_huge_pages;
 +static inline bool is_nx_huge_page_enabled(void)
 +{
 +	return READ_ONCE(nx_huge_pages);
 +}
++=======
+ int mmu_try_to_unsync_pages(struct kvm *kvm, struct kvm_memory_slot *slot,
+ 			    gfn_t gfn, bool can_unsync, bool prefetch);
++>>>>>>> 4d78d0b39ad0 (KVM: x86/mmu: Remove need for a vcpu from mmu_try_to_unsync_pages)
  
 -void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);
 -void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);
 +int mmu_try_to_unsync_pages(struct kvm_vcpu *vcpu, gfn_t gfn, bool can_unsync);
 +
 +void kvm_mmu_gfn_disallow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
 +void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
  bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
  				    struct kvm_memory_slot *slot, u64 gfn,
  				    int min_level);
diff --cc arch/x86/kvm/mmu/spte.c
index 62cb6713aea3,8d3fe4311bc1..000000000000
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@@ -159,10 -161,10 +159,14 @@@ int make_spte(struct kvm_vcpu *vcpu, un
  		 * e.g. it's write-tracked (upper-level SPs) or has one or more
  		 * shadow pages and unsync'ing pages is not allowed.
  		 */
++<<<<<<< HEAD
 +		if (mmu_try_to_unsync_pages(vcpu, gfn, can_unsync)) {
++=======
+ 		if (mmu_try_to_unsync_pages(vcpu->kvm, slot, gfn, can_unsync, prefetch)) {
++>>>>>>> 4d78d0b39ad0 (KVM: x86/mmu: Remove need for a vcpu from mmu_try_to_unsync_pages)
  			pgprintk("%s: found shadow page for %llx, marking ro\n",
  				 __func__, gfn);
 -			wrprot = true;
 +			ret |= SET_SPTE_WRITE_PROTECTED_PT;
  			pte_access &= ~ACC_WRITE_MASK;
  			spte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);
  		}
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/mmu_internal.h
* Unmerged path arch/x86/kvm/mmu/spte.c
