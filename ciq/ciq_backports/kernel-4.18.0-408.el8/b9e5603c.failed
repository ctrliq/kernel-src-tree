KVM: x86: use struct kvm_mmu_root_info for mmu->root

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit b9e5603c2a3accbadfec570ac501a54431a6bdba
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/b9e5603c.failed

The root_hpa and root_pgd fields form essentially a struct kvm_mmu_root_info.
Use the struct to have more consistency between mmu->root and
mmu->prev_roots.

The patch is entirely search and replace except for cached_root_available,
which does not need a temporary struct kvm_mmu_root_info anymore.

	Reviewed-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit b9e5603c2a3accbadfec570ac501a54431a6bdba)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index bdbeb3101d0e,5b5933331491..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -3931,6 -3927,34 +3931,37 @@@ out_retry
  	return true;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Returns true if the page fault is stale and needs to be retried, i.e. if the
+  * root was invalidated by a memslot update or a relevant mmu_notifier fired.
+  */
+ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
+ 				struct kvm_page_fault *fault, int mmu_seq)
+ {
+ 	struct kvm_mmu_page *sp = to_shadow_page(vcpu->arch.mmu->root.hpa);
+ 
+ 	/* Special roots, e.g. pae_root, are not backed by shadow pages. */
+ 	if (sp && is_obsolete_sp(vcpu->kvm, sp))
+ 		return true;
+ 
+ 	/*
+ 	 * Roots without an associated shadow page are considered invalid if
+ 	 * there is a pending request to free obsolete roots.  The request is
+ 	 * only a hint that the current root _may_ be obsolete and needs to be
+ 	 * reloaded, e.g. if the guest frees a PGD that KVM is tracking as a
+ 	 * previous root, then __kvm_mmu_prepare_zap_page() signals all vCPUs
+ 	 * to reload even if no vCPU is actively using the root.
+ 	 */
+ 	if (!sp && kvm_test_request(KVM_REQ_MMU_RELOAD, vcpu))
+ 		return true;
+ 
+ 	return fault->slot &&
+ 	       mmu_notifier_retry_hva(vcpu->kvm, mmu_seq, fault->hva);
+ }
+ 
++>>>>>>> b9e5603c2a3a (KVM: x86: use struct kvm_mmu_root_info for mmu->root)
  static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
  {
  	bool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);
@@@ -5456,9 -5468,8 +5480,14 @@@ static int __kvm_mmu_create(struct kvm_
  	struct page *page;
  	int i;
  
++<<<<<<< HEAD
 +	mmu->root_hpa = INVALID_PAGE;
 +	mmu->root_pgd = 0;
 +	mmu->translate_gpa = translate_gpa;
++=======
+ 	mmu->root.hpa = INVALID_PAGE;
+ 	mmu->root.pgd = 0;
++>>>>>>> b9e5603c2a3a (KVM: x86: use struct kvm_mmu_root_info for mmu->root)
  	for (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)
  		mmu->prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;
  
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 00e34bc04af0,debf08212f12..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -642,8 -657,7 +642,12 @@@ static inline void tdp_mmu_set_spte_no_
  		else
  
  #define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)		\
++<<<<<<< HEAD
 +	for_each_tdp_pte(_iter, __va(_mmu->root_hpa),		\
 +			 _mmu->shadow_root_level, _start, _end)
++=======
+ 	for_each_tdp_pte(_iter, to_shadow_page(_mmu->root.hpa), _start, _end)
++>>>>>>> b9e5603c2a3a (KVM: x86: use struct kvm_mmu_root_info for mmu->root)
  
  /*
   * Yield if the MMU lock is contended or this thread needs to return control
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 96ecf6f2c1a5..bac0d3ecd708 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -434,8 +434,7 @@ struct kvm_mmu {
 	int (*sync_page)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp);
 	void (*invlpg)(struct kvm_vcpu *vcpu, gva_t gva, hpa_t root_hpa);
-	hpa_t root_hpa;
-	gpa_t root_pgd;
+	struct kvm_mmu_root_info root;
 	union kvm_mmu_role mmu_role;
 	u8 root_level;
 	u8 shadow_root_level;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index c605a3c2b84d..638e79f95787 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -85,7 +85,7 @@ void kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu);
 
 static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
 {
-	if (likely(vcpu->arch.mmu->root_hpa != INVALID_PAGE))
+	if (likely(vcpu->arch.mmu->root.hpa != INVALID_PAGE))
 		return 0;
 
 	return kvm_mmu_load(vcpu);
@@ -107,7 +107,7 @@ static inline unsigned long kvm_get_active_pcid(struct kvm_vcpu *vcpu)
 
 static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
 {
-	u64 root_hpa = vcpu->arch.mmu->root_hpa;
+	u64 root_hpa = vcpu->arch.mmu->root.hpa;
 
 	if (!VALID_PAGE(root_hpa))
 		return;
* Unmerged path arch/x86/kvm/mmu/mmu.c
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index 55af99407e31..8260ee505603 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -674,7 +674,7 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, gpa_t addr,
 	if (FNAME(gpte_changed)(vcpu, gw, top_level))
 		goto out_gpte_changed;
 
-	if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root_hpa)))
+	if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root.hpa)))
 		goto out_gpte_changed;
 
 	for (shadow_walk_init(&it, vcpu, addr);
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
diff --git a/arch/x86/kvm/mmu/tdp_mmu.h b/arch/x86/kvm/mmu/tdp_mmu.h
index e1f1ae8ec3e2..aa17a138ce7d 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@ -98,7 +98,7 @@ static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu
 static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
 {
 	struct kvm_mmu_page *sp;
-	hpa_t hpa = mmu->root_hpa;
+	hpa_t hpa = mmu->root.hpa;
 
 	if (WARN_ON(!VALID_PAGE(hpa)))
 		return false;
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index f90142afce43..08c8144827c8 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -5471,7 +5471,7 @@ static int handle_invept(struct kvm_vcpu *vcpu)
 				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 
 		roots_to_free = 0;
-		if (nested_ept_root_matches(mmu->root_hpa, mmu->root_pgd,
+		if (nested_ept_root_matches(mmu->root.hpa, mmu->root.pgd,
 					    operand.eptp))
 			roots_to_free |= KVM_MMU_ROOT_CURRENT;
 
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 88b802ce679c..0922b930b320 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -2949,7 +2949,7 @@ static inline int vmx_get_current_vpid(struct kvm_vcpu *vcpu)
 static void vmx_flush_tlb_current(struct kvm_vcpu *vcpu)
 {
 	struct kvm_mmu *mmu = vcpu->arch.mmu;
-	u64 root_hpa = mmu->root_hpa;
+	u64 root_hpa = mmu->root.hpa;
 
 	/* No flush required if the current context is invalid. */
 	if (!VALID_PAGE(root_hpa))
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index bd9418884df2..fc3a6e8c1eea 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -755,7 +755,7 @@ bool kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 	if ((fault->error_code & PFERR_PRESENT_MASK) &&
 	    !(fault->error_code & PFERR_RSVD_MASK))
 		kvm_mmu_invalidate_gva(vcpu, fault_mmu, fault->address,
-				       fault_mmu->root_hpa);
+				       fault_mmu->root.hpa);
 
 	fault_mmu->inject_page_fault(vcpu, fault);
 	return fault->nested_page_fault;
