KVM: x86/mmu: Verify shadow walk doesn't terminate early in page faults

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit b1a429fb18011505acad931f409415c8bb5b5c28
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/b1a429fb.failed

WARN and bail if the shadow walk for faulting in a SPTE terminates early,
i.e. doesn't reach the expected level because the walk encountered a
terminal SPTE.  The shadow walks for page faults are subtle in that they
install non-leaf SPTEs (zapping leaf SPTEs if necessary!) in the loop
body, and consume the newly created non-leaf SPTE in the loop control,
e.g. __shadow_walk_next().  In other words, the walks guarantee that the
walk will stop if and only if the target level is reached by installing
non-leaf SPTEs to guarantee the walk remains valid.

Opportunistically use fault->goal-level instead of it.level in
FNAME(fetch) to further clarify that KVM always installs the leaf SPTE at
the target level.

	Reviewed-by: Lai Jiangshan <jiangshanlai@gmail.com>
	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Lai Jiangshan <laijs@linux.alibaba.com>
Message-Id: <20210906122547.263316-1-jiangshanlai@gmail.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit b1a429fb18011505acad931f409415c8bb5b5c28)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/paging_tmpl.h
diff --cc arch/x86/kvm/mmu/paging_tmpl.h
index de3ee26beb48,7a8a2d14a3c7..000000000000
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@@ -766,8 -760,12 +766,17 @@@ static int FNAME(fetch)(struct kvm_vcp
  		}
  	}
  
++<<<<<<< HEAD
 +	ret = mmu_set_spte(vcpu, it.sptep, gw->pte_access, write_fault,
 +			   it.level, base_gfn, pfn, prefault, map_writable);
++=======
+ 	if (WARN_ON_ONCE(it.level != fault->goal_level))
+ 		return -EFAULT;
+ 
+ 	ret = mmu_set_spte(vcpu, it.sptep, gw->pte_access, fault->write,
+ 			   fault->goal_level, base_gfn, fault->pfn,
+ 			   fault->prefault, fault->map_writable);
++>>>>>>> b1a429fb1801 (KVM: x86/mmu: Verify shadow walk doesn't terminate early in page faults)
  	if (ret == RET_PF_SPURIOUS)
  		return ret;
  
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 4d7446c022d2..08b748b65cc7 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -2996,6 +2996,9 @@ static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 			account_huge_nx_page(vcpu->kvm, sp);
 	}
 
+	if (WARN_ON_ONCE(it.level != fault->goal_level))
+		return -EFAULT;
+
 	ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
 			   write, level, base_gfn, pfn, prefault,
 			   map_writable);
* Unmerged path arch/x86/kvm/mmu/paging_tmpl.h
