KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 73a3c659478a2eae331b63ce1d61fd0a43fe7d8c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/73a3c659.failed

Pass struct kvm_page_fault to kvm_mmu_hugepage_adjust() instead of
extracting the arguments from the struct; the results are also stored
in the struct, so the callers are adjusted consequently.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 73a3c659478a2eae331b63ce1d61fd0a43fe7d8c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/mmu_internal.h
#	arch/x86/kvm/mmu/paging_tmpl.h
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 4d7446c022d2,877d0bda0f5e..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2952,35 -2979,27 +2949,56 @@@ void disallowed_hugepage_adjust(u64 spt
  	}
  }
  
 -static int __direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 +static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 +			int map_writable, int max_level, kvm_pfn_t pfn,
 +			bool prefault, bool is_tdp)
  {
++<<<<<<< HEAD
 +	bool nx_huge_page_workaround_enabled = is_nx_huge_page_enabled();
 +	bool write = error_code & PFERR_WRITE_MASK;
 +	bool exec = error_code & PFERR_FETCH_MASK;
 +	bool huge_page_disallowed = exec && nx_huge_page_workaround_enabled;
 +	struct kvm_shadow_walk_iterator it;
 +	struct kvm_mmu_page *sp;
 +	int level, req_level, ret;
 +	gfn_t gfn = gpa >> PAGE_SHIFT;
 +	gfn_t base_gfn = gfn;
 +
 +	level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn,
 +					huge_page_disallowed, &req_level);
 +
 +	trace_kvm_mmu_spte_requested(gpa, level, pfn);
 +	for_each_shadow_entry(vcpu, gpa, it) {
++=======
+ 	struct kvm_shadow_walk_iterator it;
+ 	struct kvm_mmu_page *sp;
+ 	int ret;
+ 	gfn_t base_gfn = fault->gfn;
+ 
+ 	kvm_mmu_hugepage_adjust(vcpu, fault);
+ 
+ 	trace_kvm_mmu_spte_requested(fault->addr, fault->goal_level, fault->pfn);
+ 	for_each_shadow_entry(vcpu, fault->addr, it) {
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  		/*
  		 * We cannot overwrite existing page tables with an NX
  		 * large page, as the leaf could be executable.
  		 */
++<<<<<<< HEAD
 +		if (nx_huge_page_workaround_enabled)
 +			disallowed_hugepage_adjust(*it.sptep, gfn, it.level,
 +						   &pfn, &level);
 +
 +		base_gfn = gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
 +		if (it.level == level)
++=======
+ 		if (fault->nx_huge_page_workaround_enabled)
+ 			disallowed_hugepage_adjust(*it.sptep, fault->gfn, it.level,
+ 						   &fault->pfn, &fault->goal_level);
+ 
+ 		base_gfn = fault->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
+ 		if (it.level == fault->goal_level)
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  			break;
  
  		drop_large_spte(vcpu, it.sptep);
@@@ -2991,14 -3010,14 +3009,24 @@@
  				      it.level - 1, true, ACC_ALL);
  
  		link_shadow_page(vcpu, it.sptep, sp);
++<<<<<<< HEAD
 +		if (is_tdp && huge_page_disallowed &&
 +		    req_level >= it.level)
++=======
+ 		if (fault->is_tdp && fault->huge_page_disallowed &&
+ 		    fault->req_level >= it.level)
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  			account_huge_nx_page(vcpu->kvm, sp);
  	}
  
  	ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
++<<<<<<< HEAD
 +			   write, level, base_gfn, pfn, prefault,
 +			   map_writable);
++=======
+ 			   fault->write, fault->goal_level, base_gfn, fault->pfn,
+ 			   fault->prefault, fault->map_writable);
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  	if (ret == RET_PF_SPURIOUS)
  		return ret;
  
diff --cc arch/x86/kvm/mmu/mmu_internal.h
index 5ac367b7f49f,ae0c7bc3b19b..000000000000
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@@ -119,16 -118,11 +119,21 @@@ static inline bool kvm_vcpu_ad_need_wri
  	       kvm_x86_ops.cpu_dirty_log_size;
  }
  
++<<<<<<< HEAD
 +extern int nx_huge_pages;
 +static inline bool is_nx_huge_page_enabled(void)
 +{
 +	return READ_ONCE(nx_huge_pages);
 +}
 +
 +int mmu_try_to_unsync_pages(struct kvm_vcpu *vcpu, gfn_t gfn, bool can_unsync);
++=======
+ int mmu_try_to_unsync_pages(struct kvm_vcpu *vcpu, gfn_t gfn, bool can_unsync,
+ 			    bool speculative);
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  
 -void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);
 -void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);
 +void kvm_mmu_gfn_disallow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
 +void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
  bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
  				    struct kvm_memory_slot *slot, u64 gfn,
  				    int min_level);
diff --cc arch/x86/kvm/mmu/paging_tmpl.h
index de3ee26beb48,20f616963ff4..000000000000
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@@ -655,21 -655,16 +655,29 @@@ static void FNAME(pte_prefetch)(struct 
   * If the guest tries to write a write-protected page, we need to
   * emulate this operation, return 1 to indicate this case.
   */
 -static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 -			 struct guest_walker *gw)
 +static int FNAME(fetch)(struct kvm_vcpu *vcpu, gpa_t addr,
 +			 struct guest_walker *gw, u32 error_code,
 +			 int max_level, kvm_pfn_t pfn, bool map_writable,
 +			 bool prefault)
  {
++<<<<<<< HEAD
 +	bool nx_huge_page_workaround_enabled = is_nx_huge_page_enabled();
 +	bool write_fault = error_code & PFERR_WRITE_MASK;
 +	bool exec = error_code & PFERR_FETCH_MASK;
 +	bool huge_page_disallowed = exec && nx_huge_page_workaround_enabled;
 +	struct kvm_mmu_page *sp = NULL;
 +	struct kvm_shadow_walk_iterator it;
 +	unsigned int direct_access, access;
 +	int top_level, level, req_level, ret;
 +	gfn_t base_gfn = gw->gfn;
++=======
+ 	struct kvm_mmu_page *sp = NULL;
+ 	struct kvm_shadow_walk_iterator it;
+ 	unsigned int direct_access, access;
+ 	int top_level, ret;
+ 	gfn_t base_gfn = fault->gfn;
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  
 -	WARN_ON_ONCE(gw->gfn != base_gfn);
  	direct_access = gw->pte_access;
  
  	top_level = vcpu->arch.mmu->root_level;
@@@ -733,10 -728,9 +741,14 @@@
  			link_shadow_page(vcpu, it.sptep, sp);
  	}
  
++<<<<<<< HEAD
 +	level = kvm_mmu_hugepage_adjust(vcpu, gw->gfn, max_level, &pfn,
 +					huge_page_disallowed, &req_level);
++=======
+ 	kvm_mmu_hugepage_adjust(vcpu, fault);
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  
 -	trace_kvm_mmu_spte_requested(fault->addr, gw->level, fault->pfn);
 +	trace_kvm_mmu_spte_requested(addr, gw->level, pfn);
  
  	for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
  		clear_sp_write_flooding_count(it.sptep);
@@@ -745,12 -739,12 +757,21 @@@
  		 * We cannot overwrite existing page tables with an NX
  		 * large page, as the leaf could be executable.
  		 */
++<<<<<<< HEAD
 +		if (nx_huge_page_workaround_enabled)
 +			disallowed_hugepage_adjust(*it.sptep, gw->gfn, it.level,
 +						   &pfn, &level);
 +
 +		base_gfn = gw->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
 +		if (it.level == level)
++=======
+ 		if (fault->nx_huge_page_workaround_enabled)
+ 			disallowed_hugepage_adjust(*it.sptep, fault->gfn, it.level,
+ 						   &fault->pfn, &fault->goal_level);
+ 
+ 		base_gfn = fault->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
+ 		if (it.level == fault->goal_level)
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  			break;
  
  		validate_direct_spte(vcpu, it.sptep, direct_access);
@@@ -758,10 -752,11 +779,11 @@@
  		drop_large_spte(vcpu, it.sptep);
  
  		if (!is_shadow_present_pte(*it.sptep)) {
 -			sp = kvm_mmu_get_page(vcpu, base_gfn, fault->addr,
 +			sp = kvm_mmu_get_page(vcpu, base_gfn, addr,
  					      it.level - 1, true, direct_access);
  			link_shadow_page(vcpu, it.sptep, sp);
- 			if (huge_page_disallowed && req_level >= it.level)
+ 			if (fault->huge_page_disallowed &&
+ 			    fault->req_level >= it.level)
  				account_huge_nx_page(vcpu->kvm, sp);
  		}
  	}
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index bc5924587138,b48256b88930..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -918,37 -984,27 +918,54 @@@ static int tdp_mmu_map_handle_target_le
   * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
   * page tables and SPTEs to translate the faulting guest physical address.
   */
 -int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 +int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
 +		    int map_writable, int max_level, kvm_pfn_t pfn,
 +		    bool prefault)
  {
++<<<<<<< HEAD
 +	bool nx_huge_page_workaround_enabled = is_nx_huge_page_enabled();
 +	bool write = error_code & PFERR_WRITE_MASK;
 +	bool exec = error_code & PFERR_FETCH_MASK;
 +	bool huge_page_disallowed = exec && nx_huge_page_workaround_enabled;
++=======
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  	struct kvm_mmu *mmu = vcpu->arch.mmu;
  	struct tdp_iter iter;
  	struct kvm_mmu_page *sp;
  	u64 *child_pt;
  	u64 new_spte;
  	int ret;
++<<<<<<< HEAD
 +	gfn_t gfn = gpa >> PAGE_SHIFT;
 +	int level;
 +	int req_level;
 +
 +	level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn,
 +					huge_page_disallowed, &req_level);
 +
 +	trace_kvm_mmu_spte_requested(gpa, level, pfn);
 +
 +	rcu_read_lock();
 +
 +	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 +		if (nx_huge_page_workaround_enabled)
 +			disallowed_hugepage_adjust(iter.old_spte, gfn,
 +						   iter.level, &pfn, &level);
++=======
  
- 		if (iter.level == level)
+ 	kvm_mmu_hugepage_adjust(vcpu, fault);
+ 
+ 	trace_kvm_mmu_spte_requested(fault->addr, fault->goal_level, fault->pfn);
+ 
+ 	rcu_read_lock();
+ 
+ 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ 		if (fault->nx_huge_page_workaround_enabled)
+ 			disallowed_hugepage_adjust(iter.old_spte, fault->gfn,
+ 						   iter.level, &fault->pfn, &fault->goal_level);
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
+ 
+ 		if (iter.level == fault->goal_level)
  			break;
  
  		/*
@@@ -984,11 -1040,10 +1001,18 @@@
  			new_spte = make_nonleaf_spte(child_pt,
  						     !shadow_accessed_mask);
  
++<<<<<<< HEAD
 +			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter,
 +						    new_spte)) {
 +				tdp_mmu_link_page(vcpu->kvm, sp, true,
 +						  huge_page_disallowed &&
 +						  req_level >= iter.level);
++=======
+ 			if (tdp_mmu_set_spte_atomic_no_dirty_log(vcpu->kvm, &iter, new_spte)) {
+ 				tdp_mmu_link_page(vcpu->kvm, sp,
+ 						  fault->huge_page_disallowed &&
+ 						  fault->req_level >= iter.level);
++>>>>>>> 73a3c659478a (KVM: MMU: change kvm_mmu_hugepage_adjust() arguments to kvm_page_fault)
  
  				trace_kvm_mmu_get_page(sp, true);
  			} else {
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 58b438826461..9c58ca575775 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -126,18 +126,46 @@ struct kvm_page_fault {
 	const bool rsvd;
 	const bool user;
 
-	/* Derived from mmu.  */
+	/* Derived from mmu and global state.  */
 	const bool is_tdp;
+	const bool nx_huge_page_workaround_enabled;
 
-	/* Input to FNAME(fetch), __direct_map and kvm_tdp_mmu_map.  */
+	/*
+	 * Whether a >4KB mapping can be created or is forbidden due to NX
+	 * hugepages.
+	 */
+	bool huge_page_disallowed;
+
+	/*
+	 * Maximum page size that can be created for this fault; input to
+	 * FNAME(fetch), __direct_map and kvm_tdp_mmu_map.
+	 */
 	u8 max_level;
 
+	/*
+	 * Page size that can be created based on the max_level and the
+	 * page size used by the host mapping.
+	 */
+	u8 req_level;
+
+	/*
+	 * Page size that will be created based on the req_level and
+	 * huge_page_disallowed.
+	 */
+	u8 goal_level;
+
 	/* Shifted addr, or result of guest page table walk if addr is a gva.  */
 	gfn_t gfn;
 };
 
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
 
+extern int nx_huge_pages;
+static inline bool is_nx_huge_page_enabled(void)
+{
+	return READ_ONCE(nx_huge_pages);
+}
+
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefault)
 {
@@ -151,8 +179,11 @@ static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 		.user = err & PFERR_USER_MASK,
 		.prefault = prefault,
 		.is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+		.nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(),
 
 		.max_level = KVM_MAX_HUGEPAGE_LEVEL,
+		.req_level = PG_LEVEL_4K,
+		.goal_level = PG_LEVEL_4K,
 	};
 #ifdef CONFIG_RETPOLINE
 	if (fault.is_tdp)
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/mmu_internal.h
* Unmerged path arch/x86/kvm/mmu/paging_tmpl.h
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
