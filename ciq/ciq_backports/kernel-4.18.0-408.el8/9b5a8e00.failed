mm: convert insert_pfn() to vm_fault_t

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Matthew Wilcox <willy@infradead.org>
commit 9b5a8e00d479bb5e55f6902bf50877c080d9506d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/9b5a8e00.failed

All callers convert its errno into a vm_fault_t, so convert it to return a
vm_fault_t directly.

Link: http://lkml.kernel.org/r/20180828145728.11873-11-willy@infradead.org
	Signed-off-by: Matthew Wilcox <willy@infradead.org>
	Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
	Cc: Souptick Joarder <jrdr.linux@gmail.com>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 9b5a8e00d479bb5e55f6902bf50877c080d9506d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memory.c
diff --cc mm/memory.c
index e5b3a3c27f46,6abc74f41bc0..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -1668,88 -1520,7 +1668,92 @@@ int vm_insert_page(struct vm_area_struc
  }
  EXPORT_SYMBOL(vm_insert_page);
  
++<<<<<<< HEAD
 +/*
 + * __vm_map_pages - maps range of kernel pages into user vma
 + * @vma: user vma to map to
 + * @pages: pointer to array of source kernel pages
 + * @num: number of pages in page array
 + * @offset: user's requested vm_pgoff
 + *
 + * This allows drivers to map range of kernel pages into a user vma.
 + *
 + * Return: 0 on success and error code otherwise.
 + */
 +static int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,
 +				unsigned long num, unsigned long offset)
 +{
 +	unsigned long count = vma_pages(vma);
 +	unsigned long uaddr = vma->vm_start;
 +	int ret, i;
 +
 +	/* Fail if the user requested offset is beyond the end of the object */
 +	if (offset > num)
 +		return -ENXIO;
 +
 +	/* Fail if the user requested size exceeds available object size */
 +	if (count > num - offset)
 +		return -ENXIO;
 +
 +	for (i = 0; i < count; i++) {
 +		ret = vm_insert_page(vma, uaddr, pages[offset + i]);
 +		if (ret < 0)
 +			return ret;
 +		uaddr += PAGE_SIZE;
 +	}
 +
 +	return 0;
 +}
 +
 +/**
 + * vm_map_pages - maps range of kernel pages starts with non zero offset
 + * @vma: user vma to map to
 + * @pages: pointer to array of source kernel pages
 + * @num: number of pages in page array
 + *
 + * Maps an object consisting of @num pages, catering for the user's
 + * requested vm_pgoff
 + *
 + * If we fail to insert any page into the vma, the function will return
 + * immediately leaving any previously inserted pages present.  Callers
 + * from the mmap handler may immediately return the error as their caller
 + * will destroy the vma, removing any successfully inserted pages. Other
 + * callers should make their own arrangements for calling unmap_region().
 + *
 + * Context: Process context. Called by mmap handlers.
 + * Return: 0 on success and error code otherwise.
 + */
 +int vm_map_pages(struct vm_area_struct *vma, struct page **pages,
 +				unsigned long num)
 +{
 +	return __vm_map_pages(vma, pages, num, vma->vm_pgoff);
 +}
 +EXPORT_SYMBOL(vm_map_pages);
 +
 +/**
 + * vm_map_pages_zero - map range of kernel pages starts with zero offset
 + * @vma: user vma to map to
 + * @pages: pointer to array of source kernel pages
 + * @num: number of pages in page array
 + *
 + * Similar to vm_map_pages(), except that it explicitly sets the offset
 + * to 0. This function is intended for the drivers that did not consider
 + * vm_pgoff.
 + *
 + * Context: Process context. Called by mmap handlers.
 + * Return: 0 on success and error code otherwise.
 + */
 +int vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,
 +				unsigned long num)
 +{
 +	return __vm_map_pages(vma, pages, num, 0);
 +}
 +EXPORT_SYMBOL(vm_map_pages_zero);
 +
 +static int insert_pfn(struct vm_area_struct *vma, unsigned long addr,
++=======
+ static vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,
++>>>>>>> 9b5a8e00d479 (mm: convert insert_pfn() to vm_fault_t)
  			pfn_t pfn, pgprot_t prot, bool mkwrite)
  {
  	struct mm_struct *mm = vma->vm_mm;
* Unmerged path mm/memory.c
