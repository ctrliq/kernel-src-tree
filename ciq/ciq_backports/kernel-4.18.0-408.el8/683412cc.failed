KVM: SEV: add cache flush to solve SEV cache incoherency issues

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Mingwei Zhang <mizhang@google.com>
commit 683412ccf61294d727ead4a73d97397396e69a6b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/683412cc.failed

Flush the CPU caches when memory is reclaimed from an SEV guest (where
reclaim also includes it being unmapped from KVM's memslots).  Due to lack
of coherency for SEV encrypted memory, failure to flush results in silent
data corruption if userspace is malicious/broken and doesn't ensure SEV
guest memory is properly pinned and unpinned.

Cache coherency is not enforced across the VM boundary in SEV (AMD APM
vol.2 Section 15.34.7). Confidential cachelines, generated by confidential
VM guests have to be explicitly flushed on the host side. If a memory page
containing dirty confidential cachelines was released by VM and reallocated
to another user, the cachelines may corrupt the new user at a later time.

KVM takes a shortcut by assuming all confidential memory remain pinned
until the end of VM lifetime. Therefore, KVM does not flush cache at
mmu_notifier invalidation events. Because of this incorrect assumption and
the lack of cache flushing, malicous userspace can crash the host kernel:
creating a malicious VM and continuously allocates/releases unpinned
confidential memory pages when the VM is running.

Add cache flush operations to mmu_notifier operations to ensure that any
physical memory leaving the guest VM get flushed. In particular, hook
mmu_notifier_invalidate_range_start and mmu_notifier_release events and
flush cache accordingly. The hook after releasing the mmu lock to avoid
contention with other vCPUs.

	Cc: stable@vger.kernel.org
	Suggested-by: Sean Christpherson <seanjc@google.com>
	Reported-by: Mingwei Zhang <mizhang@google.com>
	Signed-off-by: Mingwei Zhang <mizhang@google.com>
Message-Id: <20220421031407.2516575-4-mizhang@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 683412ccf61294d727ead4a73d97397396e69a6b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm-x86-ops.h
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/svm/svm.h
#	virt/kvm/kvm_main.c
diff --cc arch/x86/include/asm/kvm-x86-ops.h
index ab393e44c1e3,1a6d7e3f6c32..000000000000
--- a/arch/x86/include/asm/kvm-x86-ops.h
+++ b/arch/x86/include/asm/kvm-x86-ops.h
@@@ -114,9 -113,12 +114,18 @@@ KVM_X86_OP(smi_allowed
  KVM_X86_OP(enter_smm)
  KVM_X86_OP(leave_smm)
  KVM_X86_OP(enable_smi_window)
++<<<<<<< HEAD
 +KVM_X86_OP_NULL(mem_enc_op)
 +KVM_X86_OP_NULL(mem_enc_reg_region)
 +KVM_X86_OP_NULL(mem_enc_unreg_region)
++=======
+ KVM_X86_OP_OPTIONAL(mem_enc_ioctl)
+ KVM_X86_OP_OPTIONAL(mem_enc_register_region)
+ KVM_X86_OP_OPTIONAL(mem_enc_unregister_region)
+ KVM_X86_OP_OPTIONAL(vm_copy_enc_context_from)
+ KVM_X86_OP_OPTIONAL(vm_move_enc_context_from)
+ KVM_X86_OP_OPTIONAL(guest_memory_reclaimed)
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  KVM_X86_OP(get_msr_feature)
  KVM_X86_OP(can_emulate_instruction)
  KVM_X86_OP(apic_init_signal_blocked)
diff --cc arch/x86/include/asm/kvm_host.h
index 24742b8ee038,4ff36610af6a..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1469,10 -1479,12 +1469,15 @@@ struct kvm_x86_ops 
  	int (*leave_smm)(struct kvm_vcpu *vcpu, const char *smstate);
  	void (*enable_smi_window)(struct kvm_vcpu *vcpu);
  
 -	int (*mem_enc_ioctl)(struct kvm *kvm, void __user *argp);
 -	int (*mem_enc_register_region)(struct kvm *kvm, struct kvm_enc_region *argp);
 -	int (*mem_enc_unregister_region)(struct kvm *kvm, struct kvm_enc_region *argp);
 +	int (*mem_enc_op)(struct kvm *kvm, void __user *argp);
 +	int (*mem_enc_reg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
 +	int (*mem_enc_unreg_region)(struct kvm *kvm, struct kvm_enc_region *argp);
  	int (*vm_copy_enc_context_from)(struct kvm *kvm, unsigned int source_fd);
++<<<<<<< HEAD
++=======
+ 	int (*vm_move_enc_context_from)(struct kvm *kvm, unsigned int source_fd);
+ 	void (*guest_memory_reclaimed)(struct kvm *kvm);
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  
  	int (*get_msr_feature)(struct kvm_msr_entry *entry);
  
diff --cc arch/x86/kvm/svm/svm.c
index 0ab1b5b9b96a,7e45d03cd018..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -4572,11 -4617,13 +4572,18 @@@ static struct kvm_x86_ops svm_x86_ops _
  	.leave_smm = svm_leave_smm,
  	.enable_smi_window = svm_enable_smi_window,
  
++<<<<<<< HEAD
 +	.mem_enc_op = svm_mem_enc_op,
 +	.mem_enc_reg_region = svm_register_enc_region,
 +	.mem_enc_unreg_region = svm_unregister_enc_region,
++=======
+ 	.mem_enc_ioctl = sev_mem_enc_ioctl,
+ 	.mem_enc_register_region = sev_mem_enc_register_region,
+ 	.mem_enc_unregister_region = sev_mem_enc_unregister_region,
+ 	.guest_memory_reclaimed = sev_guest_memory_reclaimed,
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  
 -	.vm_copy_enc_context_from = sev_vm_copy_enc_context_from,
 -	.vm_move_enc_context_from = sev_vm_move_enc_context_from,
 +	.vm_copy_enc_context_from = svm_vm_copy_asid_from,
  
  	.can_emulate_instruction = svm_can_emulate_instruction,
  
diff --cc arch/x86/kvm/svm/svm.h
index 79b08bed324a,f76deff71002..000000000000
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@@ -610,12 -602,15 +610,24 @@@ void svm_vcpu_unblocking(struct kvm_vcp
  extern unsigned int max_sev_asid;
  
  void sev_vm_destroy(struct kvm *kvm);
++<<<<<<< HEAD
 +int svm_mem_enc_op(struct kvm *kvm, void __user *argp);
 +int svm_register_enc_region(struct kvm *kvm,
 +			    struct kvm_enc_region *range);
 +int svm_unregister_enc_region(struct kvm *kvm,
 +			      struct kvm_enc_region *range);
 +int svm_vm_copy_asid_from(struct kvm *kvm, unsigned int source_fd);
++=======
+ int sev_mem_enc_ioctl(struct kvm *kvm, void __user *argp);
+ int sev_mem_enc_register_region(struct kvm *kvm,
+ 				struct kvm_enc_region *range);
+ int sev_mem_enc_unregister_region(struct kvm *kvm,
+ 				  struct kvm_enc_region *range);
+ int sev_vm_copy_enc_context_from(struct kvm *kvm, unsigned int source_fd);
+ int sev_vm_move_enc_context_from(struct kvm *kvm, unsigned int source_fd);
+ void sev_guest_memory_reclaimed(struct kvm *kvm);
+ 
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  void pre_sev_run(struct vcpu_svm *svm, int cpu);
  void __init sev_set_cpu_caps(void);
  void __init sev_hardware_setup(void);
diff --cc virt/kvm/kvm_main.c
index 437d5876f611,f30bb8c16f26..000000000000
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@@ -487,6 -490,159 +497,162 @@@ static void kvm_mmu_notifier_invalidate
  	srcu_read_unlock(&kvm->srcu, idx);
  }
  
++<<<<<<< HEAD
++=======
+ typedef bool (*hva_handler_t)(struct kvm *kvm, struct kvm_gfn_range *range);
+ 
+ typedef void (*on_lock_fn_t)(struct kvm *kvm, unsigned long start,
+ 			     unsigned long end);
+ 
+ typedef void (*on_unlock_fn_t)(struct kvm *kvm);
+ 
+ struct kvm_hva_range {
+ 	unsigned long start;
+ 	unsigned long end;
+ 	pte_t pte;
+ 	hva_handler_t handler;
+ 	on_lock_fn_t on_lock;
+ 	on_unlock_fn_t on_unlock;
+ 	bool flush_on_ret;
+ 	bool may_block;
+ };
+ 
+ /*
+  * Use a dedicated stub instead of NULL to indicate that there is no callback
+  * function/handler.  The compiler technically can't guarantee that a real
+  * function will have a non-zero address, and so it will generate code to
+  * check for !NULL, whereas comparing against a stub will be elided at compile
+  * time (unless the compiler is getting long in the tooth, e.g. gcc 4.9).
+  */
+ static void kvm_null_fn(void)
+ {
+ 
+ }
+ #define IS_KVM_NULL_FN(fn) ((fn) == (void *)kvm_null_fn)
+ 
+ /* Iterate over each memslot intersecting [start, last] (inclusive) range */
+ #define kvm_for_each_memslot_in_hva_range(node, slots, start, last)	     \
+ 	for (node = interval_tree_iter_first(&slots->hva_tree, start, last); \
+ 	     node;							     \
+ 	     node = interval_tree_iter_next(node, start, last))	     \
+ 
+ static __always_inline int __kvm_handle_hva_range(struct kvm *kvm,
+ 						  const struct kvm_hva_range *range)
+ {
+ 	bool ret = false, locked = false;
+ 	struct kvm_gfn_range gfn_range;
+ 	struct kvm_memory_slot *slot;
+ 	struct kvm_memslots *slots;
+ 	int i, idx;
+ 
+ 	if (WARN_ON_ONCE(range->end <= range->start))
+ 		return 0;
+ 
+ 	/* A null handler is allowed if and only if on_lock() is provided. */
+ 	if (WARN_ON_ONCE(IS_KVM_NULL_FN(range->on_lock) &&
+ 			 IS_KVM_NULL_FN(range->handler)))
+ 		return 0;
+ 
+ 	idx = srcu_read_lock(&kvm->srcu);
+ 
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		struct interval_tree_node *node;
+ 
+ 		slots = __kvm_memslots(kvm, i);
+ 		kvm_for_each_memslot_in_hva_range(node, slots,
+ 						  range->start, range->end - 1) {
+ 			unsigned long hva_start, hva_end;
+ 
+ 			slot = container_of(node, struct kvm_memory_slot, hva_node[slots->node_idx]);
+ 			hva_start = max(range->start, slot->userspace_addr);
+ 			hva_end = min(range->end, slot->userspace_addr +
+ 						  (slot->npages << PAGE_SHIFT));
+ 
+ 			/*
+ 			 * To optimize for the likely case where the address
+ 			 * range is covered by zero or one memslots, don't
+ 			 * bother making these conditional (to avoid writes on
+ 			 * the second or later invocation of the handler).
+ 			 */
+ 			gfn_range.pte = range->pte;
+ 			gfn_range.may_block = range->may_block;
+ 
+ 			/*
+ 			 * {gfn(page) | page intersects with [hva_start, hva_end)} =
+ 			 * {gfn_start, gfn_start+1, ..., gfn_end-1}.
+ 			 */
+ 			gfn_range.start = hva_to_gfn_memslot(hva_start, slot);
+ 			gfn_range.end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, slot);
+ 			gfn_range.slot = slot;
+ 
+ 			if (!locked) {
+ 				locked = true;
+ 				KVM_MMU_LOCK(kvm);
+ 				if (!IS_KVM_NULL_FN(range->on_lock))
+ 					range->on_lock(kvm, range->start, range->end);
+ 				if (IS_KVM_NULL_FN(range->handler))
+ 					break;
+ 			}
+ 			ret |= range->handler(kvm, &gfn_range);
+ 		}
+ 	}
+ 
+ 	if (range->flush_on_ret && ret)
+ 		kvm_flush_remote_tlbs(kvm);
+ 
+ 	if (locked) {
+ 		KVM_MMU_UNLOCK(kvm);
+ 		if (!IS_KVM_NULL_FN(range->on_unlock))
+ 			range->on_unlock(kvm);
+ 	}
+ 
+ 	srcu_read_unlock(&kvm->srcu, idx);
+ 
+ 	/* The notifiers are averse to booleans. :-( */
+ 	return (int)ret;
+ }
+ 
+ static __always_inline int kvm_handle_hva_range(struct mmu_notifier *mn,
+ 						unsigned long start,
+ 						unsigned long end,
+ 						pte_t pte,
+ 						hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= pte,
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.on_unlock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= true,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
+ 
+ static __always_inline int kvm_handle_hva_range_no_flush(struct mmu_notifier *mn,
+ 							 unsigned long start,
+ 							 unsigned long end,
+ 							 hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= __pte(0),
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.on_unlock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= false,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
  					struct mm_struct *mm,
  					unsigned long address,
@@@ -551,23 -693,48 +717,36 @@@ static void kvm_mmu_notifier_invalidate
  		kvm->mmu_notifier_range_end =
  			max(kvm->mmu_notifier_range_end, end);
  	}
 +	need_tlb_flush = kvm_unmap_hva_range(kvm, start, end, 0);
 +	/* we've to flush the tlb before the pages can be freed */
 +	if (need_tlb_flush || kvm->tlbs_dirty)
 +		kvm_flush_remote_tlbs(kvm);
 +
 +	KVM_MMU_UNLOCK(kvm);
 +	srcu_read_unlock(&kvm->srcu, idx);
  }
  
 -static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 -					const struct mmu_notifier_range *range)
 +static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
 +						  struct mm_struct *mm,
 +						  unsigned long start,
 +						  unsigned long end)
  {
  	struct kvm *kvm = mmu_notifier_to_kvm(mn);
++<<<<<<< HEAD
++=======
+ 	const struct kvm_hva_range hva_range = {
+ 		.start		= range->start,
+ 		.end		= range->end,
+ 		.pte		= __pte(0),
+ 		.handler	= kvm_unmap_gfn_range,
+ 		.on_lock	= kvm_inc_notifier_count,
+ 		.on_unlock	= kvm_arch_guest_memory_reclaimed,
+ 		.flush_on_ret	= true,
+ 		.may_block	= mmu_notifier_range_blockable(range),
+ 	};
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  
 -	trace_kvm_unmap_hva_range(range->start, range->end);
 -
 -	/*
 -	 * Prevent memslot modification between range_start() and range_end()
 -	 * so that conditionally locking provides the same result in both
 -	 * functions.  Without that guarantee, the mmu_notifier_count
 -	 * adjustments will be imbalanced.
 -	 *
 -	 * Pairs with the decrement in range_end().
 -	 */
 -	spin_lock(&kvm->mn_invalidate_lock);
 -	kvm->mn_active_invalidate_count++;
 -	spin_unlock(&kvm->mn_invalidate_lock);
 -
 -	gfn_to_pfn_cache_invalidate_start(kvm, range->start, range->end,
 -					  hva_range.may_block);
 -
 -	__kvm_handle_hva_range(kvm, &hva_range);
 -
 -	return 0;
 -}
 -
 -void kvm_dec_notifier_count(struct kvm *kvm, unsigned long start,
 -				   unsigned long end)
 -{
 +	KVM_MMU_LOCK(kvm);
  	/*
  	 * This sequence increase will notify the kvm page fault that
  	 * the page that is going to be mapped in the spte could have
@@@ -581,7 -748,37 +760,41 @@@
  	 * in conjunction with the smp_rmb in mmu_notifier_retry().
  	 */
  	kvm->mmu_notifier_count--;
++<<<<<<< HEAD
 +	KVM_MMU_UNLOCK(kvm);
++=======
+ }
+ 
+ static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
+ 					const struct mmu_notifier_range *range)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range hva_range = {
+ 		.start		= range->start,
+ 		.end		= range->end,
+ 		.pte		= __pte(0),
+ 		.handler	= (void *)kvm_null_fn,
+ 		.on_lock	= kvm_dec_notifier_count,
+ 		.on_unlock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= false,
+ 		.may_block	= mmu_notifier_range_blockable(range),
+ 	};
+ 	bool wake;
+ 
+ 	__kvm_handle_hva_range(kvm, &hva_range);
+ 
+ 	/* Pairs with the increment in range_start(). */
+ 	spin_lock(&kvm->mn_invalidate_lock);
+ 	wake = (--kvm->mn_active_invalidate_count == 0);
+ 	spin_unlock(&kvm->mn_invalidate_lock);
+ 
+ 	/*
+ 	 * There can only be one waiter, since the wait happens under
+ 	 * slots_lock.
+ 	 */
+ 	if (wake)
+ 		rcuwait_wake_up(&kvm->mn_memslots_update_rcuwait);
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  
  	BUG_ON(kvm->mmu_notifier_count < 0);
  }
@@@ -1064,8 -1234,18 +1277,8 @@@ static void kvm_destroy_vm(struct kvm *
  	kvm_coalesced_mmio_free(kvm);
  #if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
  	mmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);
 -	/*
 -	 * At this point, pending calls to invalidate_range_start()
 -	 * have completed but no more MMU notifiers will run, so
 -	 * mn_active_invalidate_count may remain unbalanced.
 -	 * No threads can be waiting in install_new_memslots as the
 -	 * last reference on KVM has been dropped, but freeing
 -	 * memslots would deadlock without this manual intervention.
 -	 */
 -	WARN_ON(rcuwait_active(&kvm->mn_memslots_update_rcuwait));
 -	kvm->mn_active_invalidate_count = 0;
  #else
- 	kvm_arch_flush_shadow_all(kvm);
+ 	kvm_flush_shadow_all(kvm);
  #endif
  	kvm_arch_destroy_vm(kvm);
  	kvm_destroy_devices(kvm);
@@@ -1370,37 -1612,125 +1583,113 @@@ static size_t kvm_memslots_size(int slo
  }
  
  /*
 - * Activate @new, which must be installed in the inactive slots by the caller,
 - * by swapping the active slots and then propagating @new to @old once @old is
 - * unreachable and can be safely modified.
 - *
 - * With NULL @old this simply adds @new to @active (while swapping the sets).
 - * With NULL @new this simply removes @old from @active and frees it
 - * (while also swapping the sets).
 + * Note, at a minimum, the current number of used slots must be allocated, even
 + * when deleting a memslot, as we need a complete duplicate of the memslots for
 + * use when invalidating a memslot prior to deleting/moving the memslot.
   */
 -static void kvm_activate_memslot(struct kvm *kvm,
 -				 struct kvm_memory_slot *old,
 -				 struct kvm_memory_slot *new)
 +static struct kvm_memslots *kvm_dup_memslots(struct kvm_memslots *old,
 +					     enum kvm_mr_change change)
  {
 -	int as_id = kvm_memslots_get_as_id(old, new);
 +	struct kvm_memslots *slots;
 +	size_t new_size;
  
 -	kvm_swap_active_memslots(kvm, as_id);
 +	if (change == KVM_MR_CREATE)
 +		new_size = kvm_memslots_size(old->used_slots + 1);
 +	else
 +		new_size = kvm_memslots_size(old->used_slots);
  
 -	/* Propagate the new memslot to the now inactive memslots. */
 -	kvm_replace_memslot(kvm, old, new);
 -}
 +	slots = kvzalloc(new_size, GFP_KERNEL_ACCOUNT);
 +	if (likely(slots))
 +		memcpy(slots, old, kvm_memslots_size(old->used_slots));
  
 -static void kvm_copy_memslot(struct kvm_memory_slot *dest,
 -			     const struct kvm_memory_slot *src)
 -{
 -	dest->base_gfn = src->base_gfn;
 -	dest->npages = src->npages;
 -	dest->dirty_bitmap = src->dirty_bitmap;
 -	dest->arch = src->arch;
 -	dest->userspace_addr = src->userspace_addr;
 -	dest->flags = src->flags;
 -	dest->id = src->id;
 -	dest->as_id = src->as_id;
 +	return slots;
  }
  
 -static void kvm_invalidate_memslot(struct kvm *kvm,
 -				   struct kvm_memory_slot *old,
 -				   struct kvm_memory_slot *invalid_slot)
 +static void kvm_copy_memslots_arch(struct kvm_memslots *to,
 +				   struct kvm_memslots *from)
  {
 -	/*
 -	 * Mark the current slot INVALID.  As with all memslot modifications,
 -	 * this must be done on an unreachable slot to avoid modifying the
 -	 * current slot in the active tree.
 -	 */
 -	kvm_copy_memslot(invalid_slot, old);
 -	invalid_slot->flags |= KVM_MEMSLOT_INVALID;
 -	kvm_replace_memslot(kvm, old, invalid_slot);
 +	int i;
 +
 +	WARN_ON_ONCE(to->used_slots != from->used_slots);
  
++<<<<<<< HEAD
 +	for (i = 0; i < from->used_slots; i++)
 +		to->memslots[i].arch = from->memslots[i].arch;
++=======
+ 	/*
+ 	 * Activate the slot that is now marked INVALID, but don't propagate
+ 	 * the slot to the now inactive slots. The slot is either going to be
+ 	 * deleted or recreated as a new slot.
+ 	 */
+ 	kvm_swap_active_memslots(kvm, old->as_id);
+ 
+ 	/*
+ 	 * From this point no new shadow pages pointing to a deleted, or moved,
+ 	 * memslot will be created.  Validation of sp->gfn happens in:
+ 	 *	- gfn_to_hva (kvm_read_guest, gfn_to_pfn)
+ 	 *	- kvm_is_visible_gfn (mmu_check_root)
+ 	 */
+ 	kvm_arch_flush_shadow_memslot(kvm, old);
+ 	kvm_arch_guest_memory_reclaimed(kvm);
+ 
+ 	/* Was released by kvm_swap_active_memslots, reacquire. */
+ 	mutex_lock(&kvm->slots_arch_lock);
+ 
+ 	/*
+ 	 * Copy the arch-specific field of the newly-installed slot back to the
+ 	 * old slot as the arch data could have changed between releasing
+ 	 * slots_arch_lock in install_new_memslots() and re-acquiring the lock
+ 	 * above.  Writers are required to retrieve memslots *after* acquiring
+ 	 * slots_arch_lock, thus the active slot's data is guaranteed to be fresh.
+ 	 */
+ 	old->arch = invalid_slot->arch;
+ }
+ 
+ static void kvm_create_memslot(struct kvm *kvm,
+ 			       struct kvm_memory_slot *new)
+ {
+ 	/* Add the new memslot to the inactive set and activate. */
+ 	kvm_replace_memslot(kvm, NULL, new);
+ 	kvm_activate_memslot(kvm, NULL, new);
+ }
+ 
+ static void kvm_delete_memslot(struct kvm *kvm,
+ 			       struct kvm_memory_slot *old,
+ 			       struct kvm_memory_slot *invalid_slot)
+ {
+ 	/*
+ 	 * Remove the old memslot (in the inactive memslots) by passing NULL as
+ 	 * the "new" slot, and for the invalid version in the active slots.
+ 	 */
+ 	kvm_replace_memslot(kvm, old, NULL);
+ 	kvm_activate_memslot(kvm, invalid_slot, NULL);
+ }
+ 
+ static void kvm_move_memslot(struct kvm *kvm,
+ 			     struct kvm_memory_slot *old,
+ 			     struct kvm_memory_slot *new,
+ 			     struct kvm_memory_slot *invalid_slot)
+ {
+ 	/*
+ 	 * Replace the old memslot in the inactive slots, and then swap slots
+ 	 * and replace the current INVALID with the new as well.
+ 	 */
+ 	kvm_replace_memslot(kvm, old, new);
+ 	kvm_activate_memslot(kvm, invalid_slot, new);
+ }
+ 
+ static void kvm_update_flags_memslot(struct kvm *kvm,
+ 				     struct kvm_memory_slot *old,
+ 				     struct kvm_memory_slot *new)
+ {
+ 	/*
+ 	 * Similar to the MOVE case, but the slot doesn't need to be zapped as
+ 	 * an intermediate step. Instead, the old memslot is simply replaced
+ 	 * with a new, updated copy in both memslot sets.
+ 	 */
+ 	kvm_replace_memslot(kvm, old, new);
+ 	kvm_activate_memslot(kvm, old, new);
++>>>>>>> 683412ccf612 (KVM: SEV: add cache flush to solve SEV cache incoherency issues)
  }
  
  static int kvm_set_memslot(struct kvm *kvm,
* Unmerged path arch/x86/include/asm/kvm-x86-ops.h
* Unmerged path arch/x86/include/asm/kvm_host.h
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index de9d31d38cf7..bb63c21efca6 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -2014,6 +2014,14 @@ static void sev_flush_encrypted_page(struct kvm_vcpu *vcpu, void *va)
 	wbinvd_on_all_cpus();
 }
 
+void sev_guest_memory_reclaimed(struct kvm *kvm)
+{
+	if (!sev_guest(kvm))
+		return;
+
+	wbinvd_on_all_cpus();
+}
+
 void sev_free_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm;
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/svm/svm.h
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 1c43097b329c..951887595e0d 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -9833,6 +9833,11 @@ void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
 		kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
 }
 
+void kvm_arch_guest_memory_reclaimed(struct kvm *kvm)
+{
+	static_call_cond(kvm_x86_guest_memory_reclaimed)(kvm);
+}
+
 static void kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)
 {
 	if (!lapic_in_kernel(vcpu))
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 905b4457d163..4b310a15fd9b 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -1793,6 +1793,8 @@ static inline long kvm_arch_vcpu_async_ioctl(struct file *filp,
 void kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,
 					    unsigned long start, unsigned long end);
 
+void kvm_arch_guest_memory_reclaimed(struct kvm *kvm);
+
 #ifdef CONFIG_HAVE_KVM_VCPU_RUN_PID_CHANGE
 int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu);
 #else
* Unmerged path virt/kvm/kvm_main.c
