KVM: x86/mmu: Always use current mmu's role when loading new PGD

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit d2e5f3334169ab1bbbf13902e9fc8ef605ba824d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/d2e5f333.failed

Since the guest PGD is now loaded after the MMU has been set up
completely, the desired role for a cache hit is simply the current
mmu_role.  There is no need to compute it again, so __kvm_mmu_new_pgd
can be folded in kvm_mmu_new_pgd.

	Reviewed-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit d2e5f3334169ab1bbbf13902e9fc8ef605ba824d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 8286e4fdd9e2,a1768a831d97..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -4112,18 -4158,22 +4110,25 @@@ static bool fast_pgd_switch(struct kvm_
  	 * having to deal with PDPTEs. We may add support for 32-bit hosts/VMs
  	 * later if necessary.
  	 */
 -	if (VALID_PAGE(mmu->root.hpa) && !to_shadow_page(mmu->root.hpa))
 -		kvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);
 +	if (mmu->shadow_root_level >= PT64_ROOT_4LEVEL &&
 +	    mmu->root_level >= PT64_ROOT_4LEVEL)
 +		return cached_root_available(vcpu, new_pgd, new_role);
  
 -	if (VALID_PAGE(mmu->root.hpa))
 -		return cached_root_find_and_keep_current(kvm, mmu, new_pgd, new_role);
 -	else
 -		return cached_root_find_without_current(kvm, mmu, new_pgd, new_role);
 +	return false;
  }
  
- static void __kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd,
- 			      union kvm_mmu_page_role new_role)
+ void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
  {
++<<<<<<< HEAD
 +	if (!fast_pgd_switch(vcpu, new_pgd, new_role)) {
 +		kvm_mmu_free_roots(vcpu, vcpu->arch.mmu, KVM_MMU_ROOT_CURRENT);
++=======
+ 	struct kvm_mmu *mmu = vcpu->arch.mmu;
+ 	union kvm_mmu_page_role new_role = mmu->mmu_role.base;
+ 
+ 	if (!fast_pgd_switch(vcpu->kvm, mmu, new_pgd, new_role)) {
+ 		/* kvm_mmu_ensure_valid_pgd will set up a new root.  */
++>>>>>>> d2e5f3334169 (KVM: x86/mmu: Always use current mmu's role when loading new PGD)
  		return;
  	}
  
@@@ -4154,13 -4204,8 +4159,8 @@@
  	 */
  	if (!new_role.direct)
  		__clear_sp_write_flooding_count(
 -				to_shadow_page(vcpu->arch.mmu->root.hpa));
 +				to_shadow_page(vcpu->arch.mmu->root_hpa));
  }
- 
- void kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)
- {
- 	__kvm_mmu_new_pgd(vcpu, new_pgd, kvm_mmu_calc_root_page_role(vcpu));
- }
  EXPORT_SYMBOL_GPL(kvm_mmu_new_pgd);
  
  static unsigned long get_cr3(struct kvm_vcpu *vcpu)
* Unmerged path arch/x86/kvm/mmu/mmu.c
