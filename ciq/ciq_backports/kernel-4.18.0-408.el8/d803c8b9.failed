dmaengine: idxd: make I/O interrupt handler one shot

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit d803c8b9f3f2b8e5c047f2d0a27a9ea3ef91510f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/d803c8b9.failed

The interrupt thread handler currently loops forever to process outstanding
completions. This causes either an "irq X: nobody cared" kernel splat or
the NMI watchdog kicks in due to running too long in the function. The irq
thread handler is expected to run again after exiting if there are
interrupts fired while the thread handler is running. So the handler code
can process all the completed I/O in a single pass and exit without losing
the follow on completed I/O.

	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/162802977005.3084234.11836261157026497585.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit d803c8b9f3f2b8e5c047f2d0a27a9ea3ef91510f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/irq.c
diff --cc drivers/dma/idxd/irq.c
index 7a2cf0512501,11addb394793..000000000000
--- a/drivers/dma/idxd/irq.c
+++ b/drivers/dma/idxd/irq.c
@@@ -22,13 -22,6 +22,16 @@@ struct idxd_fault 
  	struct idxd_device *idxd;
  };
  
++<<<<<<< HEAD
 +static int irq_process_work_list(struct idxd_irq_entry *irq_entry,
 +				 enum irq_work_type wtype,
 +				 int *processed, u64 data);
 +static int irq_process_pending_llist(struct idxd_irq_entry *irq_entry,
 +				     enum irq_work_type wtype,
 +				     int *processed, u64 data);
 +
++=======
++>>>>>>> d803c8b9f3f2 (dmaengine: idxd: make I/O interrupt handler one shot)
  static void idxd_device_reinit(struct work_struct *work)
  {
  	struct idxd_device *idxd = container_of(work, struct idxd_device, work);
@@@ -228,84 -172,45 +231,104 @@@ irqreturn_t idxd_misc_thread(int vec, v
  	return IRQ_HANDLED;
  }
  
++<<<<<<< HEAD
 +static inline bool match_fault(struct idxd_desc *desc, u64 fault_addr)
 +{
 +	/*
 +	 * Completion address can be bad as well. Check fault address match for descriptor
 +	 * and completion address.
 +	 */
 +	if ((u64)desc->hw == fault_addr || (u64)desc->completion == fault_addr) {
 +		struct idxd_device *idxd = desc->wq->idxd;
 +		struct device *dev = &idxd->pdev->dev;
 +
 +		dev_warn(dev, "desc with fault address: %#llx\n", fault_addr);
 +		return true;
 +	}
 +
 +	return false;
 +}
 +
 +static inline void complete_desc(struct idxd_desc *desc, enum idxd_complete_type reason)
 +{
 +	idxd_dma_complete_txd(desc, reason);
 +	idxd_free_desc(desc->wq, desc);
 +}
 +
 +static int irq_process_pending_llist(struct idxd_irq_entry *irq_entry,
 +				     enum irq_work_type wtype,
 +				     int *processed, u64 data)
++=======
+ static void irq_process_pending_llist(struct idxd_irq_entry *irq_entry)
++>>>>>>> d803c8b9f3f2 (dmaengine: idxd: make I/O interrupt handler one shot)
  {
  	struct idxd_desc *desc, *t;
  	struct llist_node *head;
- 	int queued = 0;
  	unsigned long flags;
 +	enum idxd_complete_type reason;
  
- 	*processed = 0;
  	head = llist_del_all(&irq_entry->pending_llist);
  	if (!head)
- 		goto out;
+ 		return;
  
 +	if (wtype == IRQ_WORK_NORMAL)
 +		reason = IDXD_COMPLETE_NORMAL;
 +	else
 +		reason = IDXD_COMPLETE_DEV_FAIL;
 +
  	llist_for_each_entry_safe(desc, t, head, llnode) {
++<<<<<<< HEAD
 +		if (desc->completion->status) {
 +			if ((desc->completion->status & DSA_COMP_STATUS_MASK) != DSA_COMP_SUCCESS)
 +				match_fault(desc, data);
 +			complete_desc(desc, reason);
 +			(*processed)++;
++=======
+ 		u8 status = desc->completion->status & DSA_COMP_STATUS_MASK;
+ 
+ 		if (status) {
+ 			/*
+ 			 * Check against the original status as ABORT is software defined
+ 			 * and 0xff, which DSA_COMP_STATUS_MASK can mask out.
+ 			 */
+ 			if (unlikely(desc->completion->status == IDXD_COMP_DESC_ABORT)) {
+ 				complete_desc(desc, IDXD_COMPLETE_ABORT);
+ 				continue;
+ 			}
+ 
+ 			complete_desc(desc, IDXD_COMPLETE_NORMAL);
++>>>>>>> d803c8b9f3f2 (dmaengine: idxd: make I/O interrupt handler one shot)
  		} else {
  			spin_lock_irqsave(&irq_entry->list_lock, flags);
  			list_add_tail(&desc->list,
  				      &irq_entry->work_list);
  			spin_unlock_irqrestore(&irq_entry->list_lock, flags);
- 			queued++;
  		}
  	}
- 
-  out:
- 	return queued;
  }
  
++<<<<<<< HEAD
 +static int irq_process_work_list(struct idxd_irq_entry *irq_entry,
 +				 enum irq_work_type wtype,
 +				 int *processed, u64 data)
++=======
+ static void irq_process_work_list(struct idxd_irq_entry *irq_entry)
++>>>>>>> d803c8b9f3f2 (dmaengine: idxd: make I/O interrupt handler one shot)
  {
- 	int queued = 0;
  	unsigned long flags;
  	LIST_HEAD(flist);
  	struct idxd_desc *desc, *n;
 +	enum idxd_complete_type reason;
  
++<<<<<<< HEAD
 +	*processed = 0;
 +	if (wtype == IRQ_WORK_NORMAL)
 +		reason = IDXD_COMPLETE_NORMAL;
 +	else
 +		reason = IDXD_COMPLETE_DEV_FAIL;
 +
++=======
++>>>>>>> d803c8b9f3f2 (dmaengine: idxd: make I/O interrupt handler one shot)
  	/*
  	 * This lock protects list corruption from access of list outside of the irq handler
  	 * thread.
@@@ -329,17 -231,22 +349,15 @@@
  	spin_unlock_irqrestore(&irq_entry->list_lock, flags);
  
  	list_for_each_entry(desc, &flist, list) {
 -		/*
 -		 * Check against the original status as ABORT is software defined
 -		 * and 0xff, which DSA_COMP_STATUS_MASK can mask out.
 -		 */
 -		if (unlikely(desc->completion->status == IDXD_COMP_DESC_ABORT)) {
 -			complete_desc(desc, IDXD_COMPLETE_ABORT);
 -			continue;
 -		}
 -
 -		complete_desc(desc, IDXD_COMPLETE_NORMAL);
 +		if ((desc->completion->status & DSA_COMP_STATUS_MASK) != DSA_COMP_SUCCESS)
 +			match_fault(desc, data);
 +		complete_desc(desc, reason);
  	}
- 
- 	return queued;
  }
  
- static int idxd_desc_process(struct idxd_irq_entry *irq_entry)
+ irqreturn_t idxd_wq_thread(int irq, void *data)
  {
- 	int rc, processed, total = 0;
+ 	struct idxd_irq_entry *irq_entry = data;
  
  	/*
  	 * There are two lists we are processing. The pending_llist is where
@@@ -358,31 -265,9 +376,35 @@@
  	 *    and process the completed entries.
  	 * 4. If the entry is still waiting on hardware, list_add_tail() to
  	 *    the work_list.
- 	 * 5. Repeat until no more descriptors.
  	 */
++<<<<<<< HEAD
 +	do {
 +		rc = irq_process_work_list(irq_entry, IRQ_WORK_NORMAL,
 +					   &processed, 0);
 +		total += processed;
 +		if (rc != 0)
 +			continue;
 +
 +		rc = irq_process_pending_llist(irq_entry, IRQ_WORK_NORMAL,
 +					       &processed, 0);
 +		total += processed;
 +	} while (rc != 0);
 +
 +	return total;
 +}
 +
 +irqreturn_t idxd_wq_thread(int irq, void *data)
 +{
 +	struct idxd_irq_entry *irq_entry = data;
 +	int processed;
 +
 +	processed = idxd_desc_process(irq_entry);
 +	if (processed == 0)
 +		return IRQ_NONE;
++=======
+ 	irq_process_work_list(irq_entry);
+ 	irq_process_pending_llist(irq_entry);
++>>>>>>> d803c8b9f3f2 (dmaengine: idxd: make I/O interrupt handler one shot)
  
  	return IRQ_HANDLED;
  }
* Unmerged path drivers/dma/idxd/irq.c
