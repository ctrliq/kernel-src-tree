KVM: x86/mmu: Zap only TDP MMU leafs in zap range and mmu_notifier unmap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit f47e5bbbc92f5d234bbab317523c64a65b6ac4e2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/f47e5bbb.failed

Re-introduce zapping only leaf SPTEs in kvm_zap_gfn_range() and
kvm_tdp_mmu_unmap_gfn_range(), this time without losing a pending TLB
flush when processing multiple roots (including nested TDP shadow roots).
Dropping the TLB flush resulted in random crashes when running Hyper-V
Server 2019 in a guest with KSM enabled in the host (or any source of
mmu_notifier invalidations, KSM is just the easiest to force).

This effectively revert commits 873dd122172f8cce329113cfb0dfe3d2344d80c0
and fcb93eb6d09dd302cbef22bd95a5858af75e4156, and thus restores commit
cf3e26427c08ad9015956293ab389004ac6a338e, plus this delta on top:

bool kvm_tdp_mmu_zap_leafs(struct kvm *kvm, int as_id, gfn_t start, gfn_t end,
        struct kvm_mmu_page *root;

        for_each_tdp_mmu_root_yield_safe(kvm, root, as_id)
-               flush = tdp_mmu_zap_leafs(kvm, root, start, end, can_yield, false);
+               flush = tdp_mmu_zap_leafs(kvm, root, start, end, can_yield, flush);

        return flush;
 }

	Cc: Ben Gardon <bgardon@google.com>
	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Tested-by: Vitaly Kuznetsov <vkuznets@redhat.com>
Message-Id: <20220325230348.2587437-1-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit f47e5bbbc92f5d234bbab317523c64a65b6ac4e2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.h
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 97bb57fe39ca,d71d177ae6b8..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -700,43 -815,128 +700,63 @@@ static inline bool __must_check tdp_mmu
  	return iter->yielded;
  }
  
 -static inline gfn_t tdp_mmu_max_gfn_host(void)
 -{
 -	/*
 -	 * Bound TDP MMU walks at host.MAXPHYADDR, guest accesses beyond that
 -	 * will hit a #PF(RSVD) and never hit an EPT Violation/Misconfig / #NPF,
 -	 * and so KVM will never install a SPTE for such addresses.
 -	 */
 -	return 1ULL << (shadow_phys_bits - PAGE_SHIFT);
 -}
 -
 -static void __tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 -			       bool shared, int zap_level)
 -{
 -	struct tdp_iter iter;
 -
 -	gfn_t end = tdp_mmu_max_gfn_host();
 -	gfn_t start = 0;
 -
 -	for_each_tdp_pte_min_level(iter, root, zap_level, start, end) {
 -retry:
 -		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
 -			continue;
 -
 -		if (!is_shadow_present_pte(iter.old_spte))
 -			continue;
 -
 -		if (iter.level > zap_level)
 -			continue;
 -
 -		if (!shared)
 -			tdp_mmu_set_spte(kvm, &iter, 0);
 -		else if (tdp_mmu_set_spte_atomic(kvm, &iter, 0))
 -			goto retry;
 -	}
 -}
 -
 -static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 -			     bool shared)
 -{
 -
 -	/*
 -	 * The root must have an elevated refcount so that it's reachable via
 -	 * mmu_notifier callbacks, which allows this path to yield and drop
 -	 * mmu_lock.  When handling an unmap/release mmu_notifier command, KVM
 -	 * must drop all references to relevant pages prior to completing the
 -	 * callback.  Dropping mmu_lock with an unreachable root would result
 -	 * in zapping SPTEs after a relevant mmu_notifier callback completes
 -	 * and lead to use-after-free as zapping a SPTE triggers "writeback" of
 -	 * dirty accessed bits to the SPTE's associated struct page.
 -	 */
 -	WARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));
 -
 -	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 -
 -	rcu_read_lock();
 -
 -	/*
 -	 * To avoid RCU stalls due to recursively removing huge swaths of SPs,
 -	 * split the zap into two passes.  On the first pass, zap at the 1gb
 -	 * level, and then zap top-level SPs on the second pass.  "1gb" is not
 -	 * arbitrary, as KVM must be able to zap a 1gb shadow page without
 -	 * inducing a stall to allow in-place replacement with a 1gb hugepage.
 -	 *
 -	 * Because zapping a SP recurses on its children, stepping down to
 -	 * PG_LEVEL_4K in the iterator itself is unnecessary.
 -	 */
 -	__tdp_mmu_zap_root(kvm, root, shared, PG_LEVEL_1G);
 -	__tdp_mmu_zap_root(kvm, root, shared, root->role.level);
 -
 -	rcu_read_unlock();
 -}
 -
 -bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 -{
 -	u64 old_spte;
 -
 -	/*
 -	 * This helper intentionally doesn't allow zapping a root shadow page,
 -	 * which doesn't have a parent page table and thus no associated entry.
 -	 */
 -	if (WARN_ON_ONCE(!sp->ptep))
 -		return false;
 -
 -	old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
 -	if (WARN_ON_ONCE(!is_shadow_present_pte(old_spte)))
 -		return false;
 -
 -	__tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0,
 -			   sp->gfn, sp->role.level + 1, true, true);
 -
 -	return true;
 -}
 -
  /*
++<<<<<<< HEAD
 + * Tears down the mappings for the range of gfns, [start, end), and frees the
 + * non-root pages mapping GFNs strictly within that range. Returns true if
 + * SPTEs have been cleared and a TLB flush is needed before releasing the
 + * MMU lock.
++=======
+  * Zap leafs SPTEs for the range of gfns, [start, end). Returns true if SPTEs
+  * have been cleared and a TLB flush is needed before releasing the MMU lock.
+  *
++>>>>>>> f47e5bbbc92f (KVM: x86/mmu: Zap only TDP MMU leafs in zap range and mmu_notifier unmap)
   * If can_yield is true, will release the MMU lock and reschedule if the
   * scheduler needs the CPU or there is contention on the MMU lock. If this
   * function cannot yield, it will not release the MMU lock or reschedule and
   * the caller must ensure it does not supply too large a GFN range, or the
 - * operation can cause a soft lockup.
 + * operation can cause a soft lockup.  Note, in some use cases a flush may be
 + * required by prior actions.  Ensure the pending flush is performed prior to
 + * yielding.
   */
- static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
- 			  gfn_t start, gfn_t end, bool can_yield, bool flush)
+ static bool tdp_mmu_zap_leafs(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			      gfn_t start, gfn_t end, bool can_yield, bool flush)
  {
  	struct tdp_iter iter;
  
++<<<<<<< HEAD
 +	rcu_read_lock();
 +
 +	tdp_root_for_each_pte(iter, root, start, end) {
++=======
+ 	end = min(end, tdp_mmu_max_gfn_host());
+ 
+ 	lockdep_assert_held_write(&kvm->mmu_lock);
+ 
+ 	rcu_read_lock();
+ 
+ 	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end) {
++>>>>>>> f47e5bbbc92f (KVM: x86/mmu: Zap only TDP MMU leafs in zap range and mmu_notifier unmap)
  		if (can_yield &&
 -		    tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {
 +		    tdp_mmu_iter_cond_resched(kvm, &iter, flush)) {
  			flush = false;
  			continue;
  		}
  
++<<<<<<< HEAD
 +		if (!is_shadow_present_pte(iter.old_spte))
 +			continue;
 +
 +		/*
 +		 * If this is a non-last-level SPTE that covers a larger range
 +		 * than should be zapped, continue, and zap the mappings at a
 +		 * lower level.
 +		 */
 +		if ((iter.gfn < start ||
 +		     iter.gfn + KVM_PAGES_PER_HPAGE(iter.level) > end) &&
++=======
+ 		if (!is_shadow_present_pte(iter.old_spte) ||
++>>>>>>> f47e5bbbc92f (KVM: x86/mmu: Zap only TDP MMU leafs in zap range and mmu_notifier unmap)
  		    !is_last_spte(iter.old_spte, iter.level))
  			continue;
  
@@@ -745,6 -945,11 +765,14 @@@
  	}
  
  	rcu_read_unlock();
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Because this flow zaps _only_ leaf SPTEs, the caller doesn't need
+ 	 * to provide RCU protection as no 'struct kvm_mmu_page' will be freed.
+ 	 */
++>>>>>>> f47e5bbbc92f (KVM: x86/mmu: Zap only TDP MMU leafs in zap range and mmu_notifier unmap)
  	return flush;
  }
  
@@@ -1032,71 -1210,38 +1060,80 @@@ int kvm_tdp_mmu_map(struct kvm_vcpu *vc
  	return ret;
  }
  
++<<<<<<< HEAD
 +typedef int (*tdp_handler_t)(struct kvm *kvm, struct kvm_memory_slot *slot,
 +			     struct kvm_mmu_page *root, gfn_t start, gfn_t end,
 +			     unsigned long data);
++=======
+ bool kvm_tdp_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range,
+ 				 bool flush)
+ {
+ 	return kvm_tdp_mmu_zap_leafs(kvm, range->slot->as_id, range->start,
+ 				     range->end, range->may_block, flush);
+ }
++>>>>>>> f47e5bbbc92f (KVM: x86/mmu: Zap only TDP MMU leafs in zap range and mmu_notifier unmap)
  
 -typedef bool (*tdp_handler_t)(struct kvm *kvm, struct tdp_iter *iter,
 -			      struct kvm_gfn_range *range);
 -
 -static __always_inline bool kvm_tdp_mmu_handle_gfn(struct kvm *kvm,
 -						   struct kvm_gfn_range *range,
 -						   tdp_handler_t handler)
 +static __always_inline int kvm_tdp_mmu_handle_hva_range(struct kvm *kvm,
 +							unsigned long start,
 +							unsigned long end,
 +							unsigned long data,
 +							tdp_handler_t handler)
  {
 +	struct kvm_memslots *slots;
 +	struct kvm_memory_slot *memslot;
  	struct kvm_mmu_page *root;
 -	struct tdp_iter iter;
 -	bool ret = false;
 +	int ret = 0;
 +	int as_id;
 +
 +	for (as_id = 0; as_id < KVM_ADDRESS_SPACE_NUM; as_id++) {
 +		for_each_tdp_mmu_root_yield_safe(kvm, root, as_id) {
 +			slots = __kvm_memslots(kvm, as_id);
 +			kvm_for_each_memslot(memslot, slots) {
 +				unsigned long hva_start, hva_end;
 +				gfn_t gfn_start, gfn_end;
 +
 +				hva_start = max(start, memslot->userspace_addr);
 +				hva_end = min(end, memslot->userspace_addr +
 +					(memslot->npages << PAGE_SHIFT));
 +				if (hva_start >= hva_end)
 +					continue;
 +				/*
 +				 * {gfn(page) | page intersects with [hva_start, hva_end)} =
 +				 * {gfn_start, gfn_start+1, ..., gfn_end-1}.
 +				 */
 +				gfn_start = hva_to_gfn_memslot(hva_start, memslot);
 +				gfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);
 +
 +				ret |= handler(kvm, memslot, root, gfn_start,
 +					gfn_end, data);
 +			}
 +		}
 +	}
  
 -	/*
 -	 * Don't support rescheduling, none of the MMU notifiers that funnel
 -	 * into this helper allow blocking; it'd be dead, wasteful code.
 -	 */
 -	for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
 -		rcu_read_lock();
 +	return ret;
 +}
  
 -		tdp_root_for_each_leaf_pte(iter, root, range->start, range->end)
 -			ret |= handler(kvm, &iter, range);
 +static __always_inline int kvm_tdp_mmu_handle_hva(struct kvm *kvm,
 +						  unsigned long addr,
 +						  unsigned long data,
 +						  tdp_handler_t handler)
 +{
 +	return kvm_tdp_mmu_handle_hva_range(kvm, addr, addr + 1, data, handler);
 +}
  
 -		rcu_read_unlock();
 -	}
 +static int zap_gfn_range_hva_wrapper(struct kvm *kvm,
 +				     struct kvm_memory_slot *slot,
 +				     struct kvm_mmu_page *root, gfn_t start,
 +				     gfn_t end, unsigned long unused)
 +{
 +	return zap_gfn_range(kvm, root, start, end, false, false);
 +}
  
 -	return ret;
 +int kvm_tdp_mmu_zap_hva_range(struct kvm *kvm, unsigned long start,
 +			      unsigned long end)
 +{
 +	return kvm_tdp_mmu_handle_hva_range(kvm, start, end, 0,
 +					    zap_gfn_range_hva_wrapper);
  }
  
  /*
diff --cc arch/x86/kvm/mmu/tdp_mmu.h
index e1f1ae8ec3e2,c163f7cc23ca..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@@ -13,33 -12,12 +13,37 @@@ __must_check static inline bool kvm_tdp
  	return refcount_inc_not_zero(&root->tdp_mmu_root_count);
  }
  
 -void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
 -			  bool shared);
 +void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root);
  
- bool __kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id, gfn_t start,
+ bool kvm_tdp_mmu_zap_leafs(struct kvm *kvm, int as_id, gfn_t start,
  				 gfn_t end, bool can_yield, bool flush);
++<<<<<<< HEAD
 +static inline bool kvm_tdp_mmu_zap_gfn_range(struct kvm *kvm, int as_id,
 +					     gfn_t start, gfn_t end, bool flush)
 +{
 +	return __kvm_tdp_mmu_zap_gfn_range(kvm, as_id, start, end, true, flush);
 +}
 +static inline bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 +{
 +	gfn_t end = sp->gfn + KVM_PAGES_PER_HPAGE(sp->role.level + 1);
 +
 +	/*
 +	 * Don't allow yielding, as the caller may have a flush pending.  Note,
 +	 * if mmu_lock is held for write, zapping will never yield in this case,
 +	 * but explicitly disallow it for safety.  The TDP MMU does not yield
 +	 * until it has made forward progress (steps sideways), and when zapping
 +	 * a single shadow page that it's guaranteed to see (thus the mmu_lock
 +	 * requirement), its "step sideways" will always step beyond the bounds
 +	 * of the shadow page's gfn range and stop iterating before yielding.
 +	 */
 +	lockdep_assert_held_write(&kvm->mmu_lock);
 +	return __kvm_tdp_mmu_zap_gfn_range(kvm, kvm_mmu_page_as_id(sp),
 +					   sp->gfn, end, false, false);
 +}
 +
++=======
+ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp);
++>>>>>>> f47e5bbbc92f (KVM: x86/mmu: Zap only TDP MMU leafs in zap range and mmu_notifier unmap)
  void kvm_tdp_mmu_zap_all(struct kvm *kvm);
  void kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm);
  void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm);
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index ad983809b045..bb37acf09ec0 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -5720,8 +5720,8 @@ void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
 
 	if (is_tdp_mmu_enabled(kvm)) {
 		for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++)
-			flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, gfn_start,
-							  gfn_end, flush);
+			flush = kvm_tdp_mmu_zap_leafs(kvm, i, gfn_start,
+						      gfn_end, true, flush);
 	}
 
 	if (flush)
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.h
