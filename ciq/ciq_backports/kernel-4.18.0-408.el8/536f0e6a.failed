KVM: MMU: change disallowed_hugepage_adjust() arguments to kvm_page_fault

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 536f0e6ace95aa8d7b6d5522f0d56ae34e9fc39c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/536f0e6a.failed

Pass struct kvm_page_fault to disallowed_hugepage_adjust() instead of
extracting the arguments from the struct.  Tweak a bit the conditions
to avoid long lines.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 536f0e6ace95aa8d7b6d5522f0d56ae34e9fc39c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/mmu_internal.h
#	arch/x86/kvm/mmu/paging_tmpl.h
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 4d7446c022d2,7491dc685842..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2923,19 -2951,16 +2923,22 @@@ int kvm_mmu_hugepage_adjust(struct kvm_
  	 * mmu_notifier_retry() was successful and mmu_lock is held, so
  	 * the pmd can't be split from under us.
  	 */
 -	fault->goal_level = fault->req_level;
 -	mask = KVM_PAGES_PER_HPAGE(fault->goal_level) - 1;
 -	VM_BUG_ON((fault->gfn & mask) != (fault->pfn & mask));
 -	fault->pfn &= ~mask;
 +	mask = KVM_PAGES_PER_HPAGE(level) - 1;
 +	VM_BUG_ON((gfn & mask) != (pfn & mask));
 +	*pfnp = pfn & ~mask;
 +
 +	return level;
  }
  
++<<<<<<< HEAD
 +void disallowed_hugepage_adjust(u64 spte, gfn_t gfn, int cur_level,
 +				kvm_pfn_t *pfnp, int *goal_levelp)
++=======
+ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)
++>>>>>>> 536f0e6ace95 (KVM: MMU: change disallowed_hugepage_adjust() arguments to kvm_page_fault)
  {
- 	int level = *goal_levelp;
- 
- 	if (cur_level == level && level > PG_LEVEL_4K &&
+ 	if (cur_level > PG_LEVEL_4K &&
+ 	    cur_level == fault->goal_level &&
  	    is_shadow_present_pte(spte) &&
  	    !is_large_pte(spte)) {
  		/*
@@@ -2975,12 -2992,11 +2978,17 @@@ static int __direct_map(struct kvm_vcp
  		 * We cannot overwrite existing page tables with an NX
  		 * large page, as the leaf could be executable.
  		 */
++<<<<<<< HEAD
 +		if (nx_huge_page_workaround_enabled)
 +			disallowed_hugepage_adjust(*it.sptep, gfn, it.level,
 +						   &pfn, &level);
++=======
+ 		if (fault->nx_huge_page_workaround_enabled)
+ 			disallowed_hugepage_adjust(fault, *it.sptep, it.level);
++>>>>>>> 536f0e6ace95 (KVM: MMU: change disallowed_hugepage_adjust() arguments to kvm_page_fault)
  
 -		base_gfn = fault->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
 -		if (it.level == fault->goal_level)
 +		base_gfn = gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
 +		if (it.level == level)
  			break;
  
  		drop_large_spte(vcpu, it.sptep);
diff --cc arch/x86/kvm/mmu/mmu_internal.h
index 5ac367b7f49f,f0295ad51f69..000000000000
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@@ -163,11 -158,8 +163,16 @@@ enum 
  int kvm_mmu_max_mapping_level(struct kvm *kvm,
  			      const struct kvm_memory_slot *slot, gfn_t gfn,
  			      kvm_pfn_t pfn, int max_level);
++<<<<<<< HEAD
 +int kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, gfn_t gfn,
 +			    int max_level, kvm_pfn_t *pfnp,
 +			    bool huge_page_disallowed, int *req_level);
 +void disallowed_hugepage_adjust(u64 spte, gfn_t gfn, int cur_level,
 +				kvm_pfn_t *pfnp, int *goal_levelp);
++=======
+ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
+ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level);
++>>>>>>> 536f0e6ace95 (KVM: MMU: change disallowed_hugepage_adjust() arguments to kvm_page_fault)
  
  void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc);
  
diff --cc arch/x86/kvm/mmu/paging_tmpl.h
index de3ee26beb48,4a263f4511a5..000000000000
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@@ -745,12 -739,11 +745,17 @@@ static int FNAME(fetch)(struct kvm_vcp
  		 * We cannot overwrite existing page tables with an NX
  		 * large page, as the leaf could be executable.
  		 */
++<<<<<<< HEAD
 +		if (nx_huge_page_workaround_enabled)
 +			disallowed_hugepage_adjust(*it.sptep, gw->gfn, it.level,
 +						   &pfn, &level);
++=======
+ 		if (fault->nx_huge_page_workaround_enabled)
+ 			disallowed_hugepage_adjust(fault, *it.sptep, it.level);
++>>>>>>> 536f0e6ace95 (KVM: MMU: change disallowed_hugepage_adjust() arguments to kvm_page_fault)
  
 -		base_gfn = fault->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
 -		if (it.level == fault->goal_level)
 +		base_gfn = gw->gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
 +		if (it.level == level)
  			break;
  
  		validate_direct_spte(vcpu, it.sptep, direct_access);
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index bc5924587138,737af596adaf..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -943,12 -999,11 +943,18 @@@ int kvm_tdp_mmu_map(struct kvm_vcpu *vc
  
  	rcu_read_lock();
  
++<<<<<<< HEAD
 +	tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
 +		if (nx_huge_page_workaround_enabled)
 +			disallowed_hugepage_adjust(iter.old_spte, gfn,
 +						   iter.level, &pfn, &level);
++=======
+ 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
+ 		if (fault->nx_huge_page_workaround_enabled)
+ 			disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
++>>>>>>> 536f0e6ace95 (KVM: MMU: change disallowed_hugepage_adjust() arguments to kvm_page_fault)
  
 -		if (iter.level == fault->goal_level)
 +		if (iter.level == level)
  			break;
  
  		/*
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/mmu_internal.h
* Unmerged path arch/x86/kvm/mmu/paging_tmpl.h
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
