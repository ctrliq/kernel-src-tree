dmaengine: idxd: don't load pasid config until needed

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 3157dd0a366183adaea2f4d8721961637d562fee
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/3157dd0a.failed

The driver currently programs the system pasid to the WQ preemptively when
system pasid is enabled. Given that a dwq will reprogram the pasid and
possibly a different pasid, the programming is not necessary. The pasid_en
bit can be set for swq as it does not need pasid programming but
needs the pasid_en bit. Remove system pasid programming on device config
write. Add pasid programming for kernel wq type on wq driver enable. The
char dev driver already reprograms the dwq on ->open() call so there's no
change.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/164935607115.1660372.6734518676950372366.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 3157dd0a366183adaea2f4d8721961637d562fee)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/device.c
diff --cc drivers/dma/idxd/device.c
index 1936e3c950be,2903f8bb30e1..000000000000
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@@ -323,25 -299,46 +323,51 @@@ void idxd_wqs_unmap_portal(struct idxd_
  	}
  }
  
- int idxd_wq_set_pasid(struct idxd_wq *wq, int pasid)
+ static void __idxd_wq_set_priv_locked(struct idxd_wq *wq, int priv)
  {
  	struct idxd_device *idxd = wq->idxd;
- 	int rc;
  	union wqcfg wqcfg;
  	unsigned int offset;
 +	unsigned long flags;
  
- 	rc = idxd_wq_disable(wq, false);
- 	if (rc < 0)
- 		return rc;
+ 	offset = WQCFG_OFFSET(idxd, wq->id, WQCFG_PRIVL_IDX);
+ 	spin_lock(&idxd->dev_lock);
+ 	wqcfg.bits[WQCFG_PRIVL_IDX] = ioread32(idxd->reg_base + offset);
+ 	wqcfg.priv = priv;
+ 	wq->wqcfg->bits[WQCFG_PRIVL_IDX] = wqcfg.bits[WQCFG_PRIVL_IDX];
+ 	iowrite32(wqcfg.bits[WQCFG_PRIVL_IDX], idxd->reg_base + offset);
+ 	spin_unlock(&idxd->dev_lock);
+ }
+ 
+ static void __idxd_wq_set_pasid_locked(struct idxd_wq *wq, int pasid)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	union wqcfg wqcfg;
+ 	unsigned int offset;
  
  	offset = WQCFG_OFFSET(idxd, wq->id, WQCFG_PASID_IDX);
 -	spin_lock(&idxd->dev_lock);
 +	spin_lock_irqsave(&idxd->dev_lock, flags);
  	wqcfg.bits[WQCFG_PASID_IDX] = ioread32(idxd->reg_base + offset);
  	wqcfg.pasid_en = 1;
  	wqcfg.pasid = pasid;
+ 	wq->wqcfg->bits[WQCFG_PASID_IDX] = wqcfg.bits[WQCFG_PASID_IDX];
  	iowrite32(wqcfg.bits[WQCFG_PASID_IDX], idxd->reg_base + offset);
++<<<<<<< HEAD
 +	spin_unlock_irqrestore(&idxd->dev_lock, flags);
++=======
+ 	spin_unlock(&idxd->dev_lock);
+ }
+ 
+ int idxd_wq_set_pasid(struct idxd_wq *wq, int pasid)
+ {
+ 	int rc;
+ 
+ 	rc = idxd_wq_disable(wq, false);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	__idxd_wq_set_pasid_locked(wq, pasid);
++>>>>>>> 3157dd0a3661 (dmaengine: idxd: don't load pasid config until needed)
  
  	rc = idxd_wq_enable(wq);
  	if (rc < 0)
@@@ -832,9 -819,12 +858,9 @@@ static int idxd_wq_config_write(struct 
  	 */
  	for (i = 0; i < WQCFG_STRIDES(idxd); i++) {
  		wq_offset = WQCFG_OFFSET(idxd, wq->id, i);
- 		wq->wqcfg->bits[i] = ioread32(idxd->reg_base + wq_offset);
+ 		wq->wqcfg->bits[i] |= ioread32(idxd->reg_base + wq_offset);
  	}
  
 -	if (wq->size == 0 && wq->type != IDXD_WQT_NONE)
 -		wq->size = WQ_DEFAULT_QUEUE_DEPTH;
 -
  	/* byte 0-3 */
  	wq->wqcfg->wq_size = wq->size;
  
@@@ -851,10 -835,22 +877,29 @@@
  	if (wq_dedicated(wq))
  		wq->wqcfg->mode = 1;
  
++<<<<<<< HEAD
 +	if (device_pasid_enabled(idxd)) {
 +		wq->wqcfg->pasid_en = 1;
 +		if (wq->type == IDXD_WQT_KERNEL && wq_dedicated(wq))
 +			wq->wqcfg->pasid = idxd->pasid;
++=======
+ 	/*
+ 	 * The WQ priv bit is set depending on the WQ type. priv = 1 if the
+ 	 * WQ type is kernel to indicate privileged access. This setting only
+ 	 * matters for dedicated WQ. According to the DSA spec:
+ 	 * If the WQ is in dedicated mode, WQ PASID Enable is 1, and the
+ 	 * Privileged Mode Enable field of the PCI Express PASID capability
+ 	 * is 0, this field must be 0.
+ 	 *
+ 	 * In the case of a dedicated kernel WQ that is not able to support
+ 	 * the PASID cap, then the configuration will be rejected.
+ 	 */
+ 	if (wq_dedicated(wq) && wq->wqcfg->pasid_en &&
+ 	    !idxd_device_pasid_priv_enabled(idxd) &&
+ 	    wq->type == IDXD_WQT_KERNEL) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_PRIV;
+ 		return -EOPNOTSUPP;
++>>>>>>> 3157dd0a3661 (dmaengine: idxd: don't load pasid config until needed)
  	}
  
  	wq->wqcfg->priority = wq->priority;
@@@ -1126,3 -1123,326 +1171,329 @@@ int idxd_device_load_config(struct idxd
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void idxd_flush_pending_descs(struct idxd_irq_entry *ie)
+ {
+ 	struct idxd_desc *desc, *itr;
+ 	struct llist_node *head;
+ 	LIST_HEAD(flist);
+ 	enum idxd_complete_type ctype;
+ 
+ 	spin_lock(&ie->list_lock);
+ 	head = llist_del_all(&ie->pending_llist);
+ 	if (head) {
+ 		llist_for_each_entry_safe(desc, itr, head, llnode)
+ 			list_add_tail(&desc->list, &ie->work_list);
+ 	}
+ 
+ 	list_for_each_entry_safe(desc, itr, &ie->work_list, list)
+ 		list_move_tail(&desc->list, &flist);
+ 	spin_unlock(&ie->list_lock);
+ 
+ 	list_for_each_entry_safe(desc, itr, &flist, list) {
+ 		list_del(&desc->list);
+ 		ctype = desc->completion->status ? IDXD_COMPLETE_NORMAL : IDXD_COMPLETE_ABORT;
+ 		idxd_dma_complete_txd(desc, ctype, true);
+ 	}
+ }
+ 
+ static void idxd_device_set_perm_entry(struct idxd_device *idxd,
+ 				       struct idxd_irq_entry *ie)
+ {
+ 	union msix_perm mperm;
+ 
+ 	if (ie->pasid == INVALID_IOASID)
+ 		return;
+ 
+ 	mperm.bits = 0;
+ 	mperm.pasid = ie->pasid;
+ 	mperm.pasid_en = 1;
+ 	iowrite32(mperm.bits, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ static void idxd_device_clear_perm_entry(struct idxd_device *idxd,
+ 					 struct idxd_irq_entry *ie)
+ {
+ 	iowrite32(0, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ void idxd_wq_free_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct idxd_irq_entry *ie = &wq->ie;
+ 
+ 	synchronize_irq(ie->vector);
+ 	free_irq(ie->vector, ie);
+ 	idxd_flush_pending_descs(ie);
+ 	if (idxd->request_int_handles)
+ 		idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->vector = -1;
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	ie->pasid = INVALID_IOASID;
+ }
+ 
+ int idxd_wq_request_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct pci_dev *pdev = idxd->pdev;
+ 	struct device *dev = &pdev->dev;
+ 	struct idxd_irq_entry *ie;
+ 	int rc;
+ 
+ 	ie = &wq->ie;
+ 	ie->vector = pci_irq_vector(pdev, ie->id);
+ 	ie->pasid = device_pasid_enabled(idxd) ? idxd->pasid : INVALID_IOASID;
+ 	idxd_device_set_perm_entry(idxd, ie);
+ 
+ 	rc = request_threaded_irq(ie->vector, NULL, idxd_wq_thread, 0, "idxd-portal", ie);
+ 	if (rc < 0) {
+ 		dev_err(dev, "Failed to request irq %d.\n", ie->vector);
+ 		goto err_irq;
+ 	}
+ 
+ 	if (idxd->request_int_handles) {
+ 		rc = idxd_device_request_int_handle(idxd, ie->id, &ie->int_handle,
+ 						    IDXD_IRQ_MSIX);
+ 		if (rc < 0)
+ 			goto err_int_handle;
+ 	} else {
+ 		ie->int_handle = ie->id;
+ 	}
+ 
+ 	return 0;
+ 
+ err_int_handle:
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	free_irq(ie->vector, ie);
+ err_irq:
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->pasid = INVALID_IOASID;
+ 	return rc;
+ }
+ 
+ int __drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	int rc = -ENXIO;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_NOT_ENABLED;
+ 		goto err;
+ 	}
+ 
+ 	if (wq->state != IDXD_WQ_DISABLED) {
+ 		dev_dbg(dev, "wq %d already enabled.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_ENABLED;
+ 		rc = -EBUSY;
+ 		goto err;
+ 	}
+ 
+ 	if (!wq->group) {
+ 		dev_dbg(dev, "wq %d not attached to group.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_GRP;
+ 		goto err;
+ 	}
+ 
+ 	if (strlen(wq->name) == 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_NAME;
+ 		dev_dbg(dev, "wq %d name not set.\n", wq->id);
+ 		goto err;
+ 	}
+ 
+ 	/* Shared WQ checks */
+ 	if (wq_shared(wq)) {
+ 		if (!device_swq_supported(idxd)) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_SVM;
+ 			dev_dbg(dev, "PASID not enabled and shared wq.\n");
+ 			goto err;
+ 		}
+ 		/*
+ 		 * Shared wq with the threshold set to 0 means the user
+ 		 * did not set the threshold or transitioned from a
+ 		 * dedicated wq but did not set threshold. A value
+ 		 * of 0 would effectively disable the shared wq. The
+ 		 * driver does not allow a value of 0 to be set for
+ 		 * threshold via sysfs.
+ 		 */
+ 		if (wq->threshold == 0) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_THRESH;
+ 			dev_dbg(dev, "Shared wq and threshold 0.\n");
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * In the event that the WQ is configurable for pasid and priv bits.
+ 	 * For kernel wq, the driver should setup the pasid, pasid_en, and priv bit.
+ 	 * However, for non-kernel wq, the driver should only set the pasid_en bit for
+ 	 * shared wq. A dedicated wq that is not 'kernel' type will configure pasid and
+ 	 * pasid_en later on so there is no need to setup.
+ 	 */
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags)) {
+ 		int priv = 0;
+ 
+ 		if (device_pasid_enabled(idxd)) {
+ 			if (is_idxd_wq_kernel(wq) || wq_shared(wq)) {
+ 				u32 pasid = wq_dedicated(wq) ? idxd->pasid : 0;
+ 
+ 				__idxd_wq_set_pasid_locked(wq, pasid);
+ 			}
+ 		}
+ 
+ 		if (is_idxd_wq_kernel(wq))
+ 			priv = 1;
+ 		__idxd_wq_set_priv_locked(wq, priv);
+ 	}
+ 
+ 	rc = 0;
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Writing wq %d config failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_enable(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "wq %d enabling failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_map_portal(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_PORTAL_ERR;
+ 		dev_dbg(dev, "wq %d portal mapping failed: %d\n", wq->id, rc);
+ 		goto err_map_portal;
+ 	}
+ 
+ 	wq->client_count = 0;
+ 	return 0;
+ 
+ err_map_portal:
+ 	rc = idxd_wq_disable(wq, false);
+ 	if (rc < 0)
+ 		dev_dbg(dev, "wq %s disable failed\n", dev_name(wq_confdev(wq)));
+ err:
+ 	return rc;
+ }
+ 
+ int drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	int rc;
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	rc = __drv_enable_wq(wq);
+ 	mutex_unlock(&wq->wq_lock);
+ 	return rc;
+ }
+ 
+ void __drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd_wq_refcount(wq))
+ 		dev_warn(dev, "Clients has claim on wq %d: %d\n",
+ 			 wq->id, idxd_wq_refcount(wq));
+ 
+ 	idxd_wq_unmap_portal(wq);
+ 
+ 	idxd_wq_drain(wq);
+ 	idxd_wq_reset(wq);
+ 
+ 	wq->client_count = 0;
+ }
+ 
+ void drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	mutex_lock(&wq->wq_lock);
+ 	__drv_disable_wq(wq);
+ 	mutex_unlock(&wq->wq_lock);
+ }
+ 
+ int idxd_device_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int rc = 0;
+ 
+ 	/*
+ 	 * Device should be in disabled state for the idxd_drv to load. If it's in
+ 	 * enabled state, then the device was altered outside of driver's control.
+ 	 * If the state is in halted state, then we don't want to proceed.
+ 	 */
+ 	if (idxd->state != IDXD_DEV_DISABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_ENABLED;
+ 		return -ENXIO;
+ 	}
+ 
+ 	/* Device configuration */
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0)
+ 		return -ENXIO;
+ 
+ 	/* Start device */
+ 	rc = idxd_device_enable(idxd);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	/* Setup DMA device without channels */
+ 	rc = idxd_register_dma_device(idxd);
+ 	if (rc < 0) {
+ 		idxd_device_disable(idxd);
+ 		idxd->cmd_status = IDXD_SCMD_DEV_DMA_ERR;
+ 		return rc;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	return 0;
+ }
+ 
+ void idxd_device_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int i;
+ 
+ 	for (i = 0; i < idxd->max_wqs; i++) {
+ 		struct idxd_wq *wq = idxd->wqs[i];
+ 		struct device *wq_dev = wq_confdev(wq);
+ 
+ 		if (wq->state == IDXD_WQ_DISABLED)
+ 			continue;
+ 		dev_warn(dev, "Active wq %d on disable %s.\n", i, dev_name(wq_dev));
+ 		device_release_driver(wq_dev);
+ 	}
+ 
+ 	idxd_unregister_dma_device(idxd);
+ 	idxd_device_disable(idxd);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		idxd_device_reset(idxd);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_DSA,
+ 	IDXD_DEV_IAX,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_drv = {
+ 	.type = dev_types,
+ 	.probe = idxd_device_drv_probe,
+ 	.remove = idxd_device_drv_remove,
+ 	.name = "idxd",
+ };
+ EXPORT_SYMBOL_GPL(idxd_drv);
++>>>>>>> 3157dd0a3661 (dmaengine: idxd: don't load pasid config until needed)
* Unmerged path drivers/dma/idxd/device.c
diff --git a/drivers/dma/idxd/registers.h b/drivers/dma/idxd/registers.h
index 5b41dbcd3169..52a0bb364b97 100644
--- a/drivers/dma/idxd/registers.h
+++ b/drivers/dma/idxd/registers.h
@@ -351,6 +351,7 @@ union wqcfg {
 } __packed;
 
 #define WQCFG_PASID_IDX                2
+#define WQCFG_PRIVL_IDX		2
 #define WQCFG_OCCUP_IDX		6
 
 #define WQCFG_OCCUP_MASK	0xffff
