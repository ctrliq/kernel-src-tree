KVM: x86/mmu: Derive page role for TDP MMU shadow pages from parent

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit a3aca4de0da99699c5b94fc3fc4e1817e756edd1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/a3aca4de.failed

Derive the page role from the parent shadow page, since the only thing
that changes is the level. This is in preparation for splitting huge
pages during VM-ioctls which do not have access to the vCPU MMU context.

No functional change intended.

	Reviewed-by: Peter Xu <peterx@redhat.com>
	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20220119230739.2234394-14-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit a3aca4de0da99699c5b94fc3fc4e1817e756edd1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 00e34bc04af0,472ea9994e3f..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -147,19 -171,8 +147,24 @@@ static struct kvm_mmu_page *tdp_mmu_nex
  		if (kvm_mmu_page_as_id(_root) != _as_id) {		\
  		} else
  
++<<<<<<< HEAD
 +static union kvm_mmu_page_role page_role_for_level(struct kvm_vcpu *vcpu,
 +						   int level)
 +{
 +	union kvm_mmu_page_role role;
 +
 +	role = vcpu->arch.mmu->mmu_role.base;
 +	role.level = level;
 +
 +	return role;
 +}
 +
 +static struct kvm_mmu_page *alloc_tdp_mmu_page(struct kvm_vcpu *vcpu, gfn_t gfn,
 +					       int level)
++=======
+ static struct kvm_mmu_page *tdp_mmu_alloc_sp(struct kvm_vcpu *vcpu, gfn_t gfn,
+ 					     union kvm_mmu_page_role role)
++>>>>>>> a3aca4de0da9 (KVM: x86/mmu: Derive page role for TDP MMU shadow pages from parent)
  {
  	struct kvm_mmu_page *sp;
  
@@@ -196,7 -221,7 +213,11 @@@ hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(str
  			goto out;
  	}
  
++<<<<<<< HEAD
 +	root = alloc_tdp_mmu_page(vcpu, 0, vcpu->arch.mmu->shadow_root_level);
++=======
+ 	root = tdp_mmu_alloc_sp(vcpu, 0, role);
++>>>>>>> a3aca4de0da9 (KVM: x86/mmu: Derive page role for TDP MMU shadow pages from parent)
  	refcount_set(&root->tdp_mmu_root_count, 1);
  
  	spin_lock(&kvm->arch.tdp_mmu_pages_lock);
@@@ -982,20 -1042,8 +1003,25 @@@ int kvm_tdp_mmu_map(struct kvm_vcpu *vc
  			if (is_removed_spte(iter.old_spte))
  				break;
  
++<<<<<<< HEAD
 +			sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
 +			child_pt = sp->spt;
 +
 +			new_spte = make_nonleaf_spte(child_pt,
 +						     !shadow_accessed_mask);
 +
 +			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter,
 +						    new_spte)) {
 +				tdp_mmu_link_page(vcpu->kvm, sp, true,
 +						  huge_page_disallowed &&
 +						  req_level >= iter.level);
 +
 +				trace_kvm_mmu_get_page(sp, true);
 +			} else {
++=======
+ 			sp = tdp_mmu_alloc_child_sp(vcpu, &iter);
+ 			if (tdp_mmu_link_sp_atomic(vcpu->kvm, &iter, sp, account_nx)) {
++>>>>>>> a3aca4de0da9 (KVM: x86/mmu: Derive page role for TDP MMU shadow pages from parent)
  				tdp_mmu_free_sp(sp);
  				break;
  			}
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
