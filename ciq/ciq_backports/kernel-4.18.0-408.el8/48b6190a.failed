net/smc: Limit SMC visits when handshake workqueue congested

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author D. Wythe <alibuda@linux.alibaba.com>
commit 48b6190a00425a1bebac9f7ae4b338a1e20f50f3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/48b6190a.failed

This patch intends to provide a mechanism to put constraint on SMC
connections visit according to the pressure of SMC handshake process.
At present, frequent visits will cause the incoming connections to be
backlogged in SMC handshake queue, raise the connections established
time. Which is quite unacceptable for those applications who base on
short lived connections.

There are two ways to implement this mechanism:

1. Put limitation after TCP established.
2. Put limitation before TCP established.

In the first way, we need to wait and receive CLC messages that the
client will potentially send, and then actively reply with a decline
message, in a sense, which is also a sort of SMC handshake, affect the
connections established time on its way.

In the second way, the only problem is that we need to inject SMC logic
into TCP when it is about to reply the incoming SYN, since we already do
that, it's seems not a problem anymore. And advantage is obvious, few
additional processes are required to complete the constraint.

This patch use the second way. After this patch, connections who beyond
constraint will not informed any SMC indication, and SMC will not be
involved in any of its subsequent processes.

Link: https://lore.kernel.org/all/1641301961-59331-1-git-send-email-alibuda@linux.alibaba.com/
	Signed-off-by: D. Wythe <alibuda@linux.alibaba.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 48b6190a00425a1bebac9f7ae4b338a1e20f50f3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/smc/af_smc.c
diff --cc net/smc/af_smc.c
index b7264bf208bb,a05ffb268c3e..000000000000
--- a/net/smc/af_smc.c
+++ b/net/smc/af_smc.c
@@@ -71,6 -73,51 +71,54 @@@ static void smc_set_keepalive(struct so
  	smc->clcsock->sk->sk_prot->keepalive(smc->clcsock->sk, val);
  }
  
++<<<<<<< HEAD
++=======
+ static struct sock *smc_tcp_syn_recv_sock(const struct sock *sk,
+ 					  struct sk_buff *skb,
+ 					  struct request_sock *req,
+ 					  struct dst_entry *dst,
+ 					  struct request_sock *req_unhash,
+ 					  bool *own_req)
+ {
+ 	struct smc_sock *smc;
+ 
+ 	smc = smc_clcsock_user_data(sk);
+ 
+ 	if (READ_ONCE(sk->sk_ack_backlog) + atomic_read(&smc->queued_smc_hs) >
+ 				sk->sk_max_ack_backlog)
+ 		goto drop;
+ 
+ 	if (sk_acceptq_is_full(&smc->sk)) {
+ 		NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
+ 		goto drop;
+ 	}
+ 
+ 	/* passthrough to original syn recv sock fct */
+ 	return smc->ori_af_ops->syn_recv_sock(sk, skb, req, dst, req_unhash,
+ 					      own_req);
+ 
+ drop:
+ 	dst_release(dst);
+ 	tcp_listendrop(sk);
+ 	return NULL;
+ }
+ 
+ static bool smc_hs_congested(const struct sock *sk)
+ {
+ 	const struct smc_sock *smc;
+ 
+ 	smc = smc_clcsock_user_data(sk);
+ 
+ 	if (!smc)
+ 		return true;
+ 
+ 	if (workqueue_congested(WORK_CPU_UNBOUND, smc_hs_wq))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
++>>>>>>> 48b6190a0042 (net/smc: Limit SMC visits when handshake workqueue congested)
  static struct smc_hashinfo smc_v4_hashinfo = {
  	.lock = __RW_LOCK_UNLOCKED(smc_v4_hashinfo.lock),
  };
@@@ -2125,6 -2317,17 +2173,20 @@@ static int smc_listen(struct socket *so
  	smc->clcsock->sk->sk_data_ready = smc_clcsock_data_ready;
  	smc->clcsock->sk->sk_user_data =
  		(void *)((uintptr_t)smc | SK_USER_DATA_NOCOPY);
++<<<<<<< HEAD
++=======
+ 
+ 	/* save original ops */
+ 	smc->ori_af_ops = inet_csk(smc->clcsock->sk)->icsk_af_ops;
+ 
+ 	smc->af_ops = *smc->ori_af_ops;
+ 	smc->af_ops.syn_recv_sock = smc_tcp_syn_recv_sock;
+ 
+ 	inet_csk(smc->clcsock->sk)->icsk_af_ops = &smc->af_ops;
+ 
+ 	tcp_sk(smc->clcsock->sk)->smc_hs_congested = smc_hs_congested;
+ 
++>>>>>>> 48b6190a0042 (net/smc: Limit SMC visits when handshake workqueue congested)
  	rc = kernel_listen(smc->clcsock, backlog);
  	if (rc) {
  		smc->clcsock->sk->sk_data_ready = smc->clcsk_data_ready;
diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 4260a4e64658..b72e8ef2a9a2 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -395,6 +395,7 @@ struct tcp_sock {
 	bool	is_mptcp;
 #endif
 #if IS_ENABLED(CONFIG_SMC)
+	bool	(*smc_hs_congested)(const struct sock *sk);
 	bool	syn_smc;	/* SYN includes SMC */
 #endif
 
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index 040dc4f9858b..64ea63be86ec 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -6531,7 +6531,8 @@ static void tcp_openreq_init(struct request_sock *req,
 	ireq->ir_num = ntohs(tcp_hdr(skb)->dest);
 	ireq->ir_mark = inet_request_mark(sk, skb);
 #if IS_ENABLED(CONFIG_SMC)
-	ireq->smc_ok = rx_opt->smc_ok;
+	ireq->smc_ok = rx_opt->smc_ok && !(tcp_sk(sk)->smc_hs_congested &&
+			tcp_sk(sk)->smc_hs_congested(sk));
 #endif
 }
 
* Unmerged path net/smc/af_smc.c
