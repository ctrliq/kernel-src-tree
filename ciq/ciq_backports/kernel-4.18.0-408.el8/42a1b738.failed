dmaengine: idxd: Separate user and kernel pasid enabling

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 42a1b73852c4a176d233a192422b5e1d0ba67cbf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/42a1b738.failed

The idxd driver always gated the pasid enabling under a single knob and
this assumption is incorrect. The pasid used for kernel operation can be
independently toggled and has no dependency on the user pasid (and vice
versa). Split the two so they are independent "enabled" flags.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/165231431746.986466.5666862038354800551.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 42a1b73852c4a176d233a192422b5e1d0ba67cbf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/device.c
diff --cc drivers/dma/idxd/device.c
index cac50f25271e,49ee36038cca..000000000000
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@@ -969,10 -965,9 +969,15 @@@ static int idxd_wqs_setup(struct idxd_d
  
  		if (!wq->group)
  			continue;
 +		if (!wq->size)
 +			continue;
  
++<<<<<<< HEAD
 +		if (wq_shared(wq) && !device_swq_supported(idxd)) {
++=======
+ 		if (wq_shared(wq) && !wq_shared_supported(wq)) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_SWQ_SUPPORT;
++>>>>>>> 42a1b73852c4 (dmaengine: idxd: Separate user and kernel pasid enabling)
  			dev_warn(dev, "No shared wq support but configured.\n");
  			return -EINVAL;
  		}
@@@ -1129,3 -1126,342 +1134,345 @@@ int idxd_device_load_config(struct idxd
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void idxd_flush_pending_descs(struct idxd_irq_entry *ie)
+ {
+ 	struct idxd_desc *desc, *itr;
+ 	struct llist_node *head;
+ 	LIST_HEAD(flist);
+ 	enum idxd_complete_type ctype;
+ 
+ 	spin_lock(&ie->list_lock);
+ 	head = llist_del_all(&ie->pending_llist);
+ 	if (head) {
+ 		llist_for_each_entry_safe(desc, itr, head, llnode)
+ 			list_add_tail(&desc->list, &ie->work_list);
+ 	}
+ 
+ 	list_for_each_entry_safe(desc, itr, &ie->work_list, list)
+ 		list_move_tail(&desc->list, &flist);
+ 	spin_unlock(&ie->list_lock);
+ 
+ 	list_for_each_entry_safe(desc, itr, &flist, list) {
+ 		list_del(&desc->list);
+ 		ctype = desc->completion->status ? IDXD_COMPLETE_NORMAL : IDXD_COMPLETE_ABORT;
+ 		idxd_dma_complete_txd(desc, ctype, true);
+ 	}
+ }
+ 
+ static void idxd_device_set_perm_entry(struct idxd_device *idxd,
+ 				       struct idxd_irq_entry *ie)
+ {
+ 	union msix_perm mperm;
+ 
+ 	if (ie->pasid == INVALID_IOASID)
+ 		return;
+ 
+ 	mperm.bits = 0;
+ 	mperm.pasid = ie->pasid;
+ 	mperm.pasid_en = 1;
+ 	iowrite32(mperm.bits, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ static void idxd_device_clear_perm_entry(struct idxd_device *idxd,
+ 					 struct idxd_irq_entry *ie)
+ {
+ 	iowrite32(0, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ void idxd_wq_free_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct idxd_irq_entry *ie = &wq->ie;
+ 
+ 	synchronize_irq(ie->vector);
+ 	free_irq(ie->vector, ie);
+ 	idxd_flush_pending_descs(ie);
+ 	if (idxd->request_int_handles)
+ 		idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->vector = -1;
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	ie->pasid = INVALID_IOASID;
+ }
+ 
+ int idxd_wq_request_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct pci_dev *pdev = idxd->pdev;
+ 	struct device *dev = &pdev->dev;
+ 	struct idxd_irq_entry *ie;
+ 	int rc;
+ 
+ 	if (wq->type != IDXD_WQT_KERNEL)
+ 		return 0;
+ 
+ 	ie = &wq->ie;
+ 	ie->vector = pci_irq_vector(pdev, ie->id);
+ 	ie->pasid = device_pasid_enabled(idxd) ? idxd->pasid : INVALID_IOASID;
+ 	idxd_device_set_perm_entry(idxd, ie);
+ 
+ 	rc = request_threaded_irq(ie->vector, NULL, idxd_wq_thread, 0, "idxd-portal", ie);
+ 	if (rc < 0) {
+ 		dev_err(dev, "Failed to request irq %d.\n", ie->vector);
+ 		goto err_irq;
+ 	}
+ 
+ 	if (idxd->request_int_handles) {
+ 		rc = idxd_device_request_int_handle(idxd, ie->id, &ie->int_handle,
+ 						    IDXD_IRQ_MSIX);
+ 		if (rc < 0)
+ 			goto err_int_handle;
+ 	} else {
+ 		ie->int_handle = ie->id;
+ 	}
+ 
+ 	return 0;
+ 
+ err_int_handle:
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	free_irq(ie->vector, ie);
+ err_irq:
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->pasid = INVALID_IOASID;
+ 	return rc;
+ }
+ 
+ int drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	int rc = -ENXIO;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_NOT_ENABLED;
+ 		goto err;
+ 	}
+ 
+ 	if (wq->state != IDXD_WQ_DISABLED) {
+ 		dev_dbg(dev, "wq %d already enabled.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_ENABLED;
+ 		rc = -EBUSY;
+ 		goto err;
+ 	}
+ 
+ 	if (!wq->group) {
+ 		dev_dbg(dev, "wq %d not attached to group.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_GRP;
+ 		goto err;
+ 	}
+ 
+ 	if (strlen(wq->name) == 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_NAME;
+ 		dev_dbg(dev, "wq %d name not set.\n", wq->id);
+ 		goto err;
+ 	}
+ 
+ 	/* Shared WQ checks */
+ 	if (wq_shared(wq)) {
+ 		if (!wq_shared_supported(wq)) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_SVM;
+ 			dev_dbg(dev, "PASID not enabled and shared wq.\n");
+ 			goto err;
+ 		}
+ 		/*
+ 		 * Shared wq with the threshold set to 0 means the user
+ 		 * did not set the threshold or transitioned from a
+ 		 * dedicated wq but did not set threshold. A value
+ 		 * of 0 would effectively disable the shared wq. The
+ 		 * driver does not allow a value of 0 to be set for
+ 		 * threshold via sysfs.
+ 		 */
+ 		if (wq->threshold == 0) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_THRESH;
+ 			dev_dbg(dev, "Shared wq and threshold 0.\n");
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * In the event that the WQ is configurable for pasid and priv bits.
+ 	 * For kernel wq, the driver should setup the pasid, pasid_en, and priv bit.
+ 	 * However, for non-kernel wq, the driver should only set the pasid_en bit for
+ 	 * shared wq. A dedicated wq that is not 'kernel' type will configure pasid and
+ 	 * pasid_en later on so there is no need to setup.
+ 	 */
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags)) {
+ 		int priv = 0;
+ 
+ 		if (wq_pasid_enabled(wq)) {
+ 			if (is_idxd_wq_kernel(wq) || wq_shared(wq)) {
+ 				u32 pasid = wq_dedicated(wq) ? idxd->pasid : 0;
+ 
+ 				__idxd_wq_set_pasid_locked(wq, pasid);
+ 			}
+ 		}
+ 
+ 		if (is_idxd_wq_kernel(wq))
+ 			priv = 1;
+ 		__idxd_wq_set_priv_locked(wq, priv);
+ 	}
+ 
+ 	rc = 0;
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Writing wq %d config failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_enable(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "wq %d enabling failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_map_portal(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_PORTAL_ERR;
+ 		dev_dbg(dev, "wq %d portal mapping failed: %d\n", wq->id, rc);
+ 		goto err_map_portal;
+ 	}
+ 
+ 	wq->client_count = 0;
+ 
+ 	rc = idxd_wq_request_irq(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_IRQ_ERR;
+ 		dev_dbg(dev, "WQ %d irq setup failed: %d\n", wq->id, rc);
+ 		goto err_irq;
+ 	}
+ 
+ 	rc = idxd_wq_alloc_resources(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_RES_ALLOC_ERR;
+ 		dev_dbg(dev, "WQ resource alloc failed\n");
+ 		goto err_res_alloc;
+ 	}
+ 
+ 	rc = idxd_wq_init_percpu_ref(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_PERCPU_ERR;
+ 		dev_dbg(dev, "percpu_ref setup failed\n");
+ 		goto err_ref;
+ 	}
+ 
+ 	return 0;
+ 
+ err_ref:
+ 	idxd_wq_free_resources(wq);
+ err_res_alloc:
+ 	idxd_wq_free_irq(wq);
+ err_irq:
+ 	idxd_wq_unmap_portal(wq);
+ err_map_portal:
+ 	rc = idxd_wq_disable(wq, false);
+ 	if (rc < 0)
+ 		dev_dbg(dev, "wq %s disable failed\n", dev_name(wq_confdev(wq)));
+ err:
+ 	return rc;
+ }
+ 
+ void drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd_wq_refcount(wq))
+ 		dev_warn(dev, "Clients has claim on wq %d: %d\n",
+ 			 wq->id, idxd_wq_refcount(wq));
+ 
+ 	idxd_wq_free_resources(wq);
+ 	idxd_wq_unmap_portal(wq);
+ 	idxd_wq_drain(wq);
+ 	idxd_wq_reset(wq);
+ 	percpu_ref_exit(&wq->wq_active);
+ 	idxd_wq_free_irq(wq);
+ 	wq->type = IDXD_WQT_NONE;
+ 	wq->client_count = 0;
+ }
+ 
+ int idxd_device_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int rc = 0;
+ 
+ 	/*
+ 	 * Device should be in disabled state for the idxd_drv to load. If it's in
+ 	 * enabled state, then the device was altered outside of driver's control.
+ 	 * If the state is in halted state, then we don't want to proceed.
+ 	 */
+ 	if (idxd->state != IDXD_DEV_DISABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_ENABLED;
+ 		return -ENXIO;
+ 	}
+ 
+ 	/* Device configuration */
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0)
+ 		return -ENXIO;
+ 
+ 	/* Start device */
+ 	rc = idxd_device_enable(idxd);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	/* Setup DMA device without channels */
+ 	rc = idxd_register_dma_device(idxd);
+ 	if (rc < 0) {
+ 		idxd_device_disable(idxd);
+ 		idxd->cmd_status = IDXD_SCMD_DEV_DMA_ERR;
+ 		return rc;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	return 0;
+ }
+ 
+ void idxd_device_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int i;
+ 
+ 	for (i = 0; i < idxd->max_wqs; i++) {
+ 		struct idxd_wq *wq = idxd->wqs[i];
+ 		struct device *wq_dev = wq_confdev(wq);
+ 
+ 		if (wq->state == IDXD_WQ_DISABLED)
+ 			continue;
+ 		dev_warn(dev, "Active wq %d on disable %s.\n", i, dev_name(wq_dev));
+ 		device_release_driver(wq_dev);
+ 	}
+ 
+ 	idxd_unregister_dma_device(idxd);
+ 	idxd_device_disable(idxd);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		idxd_device_reset(idxd);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_DSA,
+ 	IDXD_DEV_IAX,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_drv = {
+ 	.type = dev_types,
+ 	.probe = idxd_device_drv_probe,
+ 	.remove = idxd_device_drv_remove,
+ 	.name = "idxd",
+ };
+ EXPORT_SYMBOL_GPL(idxd_drv);
++>>>>>>> 42a1b73852c4 (dmaengine: idxd: Separate user and kernel pasid enabling)
diff --git a/drivers/dma/idxd/cdev.c b/drivers/dma/idxd/cdev.c
index 6ecf0e5f3285..2b2fd539411b 100644
--- a/drivers/dma/idxd/cdev.c
+++ b/drivers/dma/idxd/cdev.c
@@ -99,7 +99,7 @@ static int idxd_cdev_open(struct inode *inode, struct file *filp)
 	ctx->wq = wq;
 	filp->private_data = ctx;
 
-	if (device_pasid_enabled(idxd)) {
+	if (device_user_pasid_enabled(idxd)) {
 		sva = iommu_sva_bind_device(dev, current->mm, NULL);
 		if (IS_ERR(sva)) {
 			rc = PTR_ERR(sva);
@@ -152,7 +152,7 @@ static int idxd_cdev_release(struct inode *node, struct file *filep)
 	if (wq_shared(wq)) {
 		idxd_device_drain_pasid(idxd, ctx->pasid);
 	} else {
-		if (device_pasid_enabled(idxd)) {
+		if (device_user_pasid_enabled(idxd)) {
 			/* The wq disable in the disable pasid function will drain the wq */
 			rc = idxd_wq_disable_pasid(wq);
 			if (rc < 0)
* Unmerged path drivers/dma/idxd/device.c
diff --git a/drivers/dma/idxd/idxd.h b/drivers/dma/idxd/idxd.h
index 7bd9aa9ff9c5..c8ed5d08379f 100644
--- a/drivers/dma/idxd/idxd.h
+++ b/drivers/dma/idxd/idxd.h
@@ -203,6 +203,7 @@ enum idxd_device_flag {
 	IDXD_FLAG_CONFIGURABLE = 0,
 	IDXD_FLAG_CMD_RUNNING,
 	IDXD_FLAG_PASID_ENABLED,
+	IDXD_FLAG_USER_PASID_ENABLED,
 };
 
 struct idxd_dma_dev {
@@ -355,9 +356,20 @@ static inline bool device_pasid_enabled(struct idxd_device *idxd)
 	return test_bit(IDXD_FLAG_PASID_ENABLED, &idxd->flags);
 }
 
-static inline bool device_swq_supported(struct idxd_device *idxd)
+static inline bool device_user_pasid_enabled(struct idxd_device *idxd)
 {
-	return (support_enqcmd && device_pasid_enabled(idxd));
+	return test_bit(IDXD_FLAG_USER_PASID_ENABLED, &idxd->flags);
+}
+
+static inline bool wq_pasid_enabled(struct idxd_wq *wq)
+{
+	return (is_idxd_wq_kernel(wq) && device_pasid_enabled(wq->idxd)) ||
+	       (is_idxd_wq_user(wq) && device_user_pasid_enabled(wq->idxd));
+}
+
+static inline bool wq_shared_supported(struct idxd_wq *wq)
+{
+	return (support_enqcmd && wq_pasid_enabled(wq));
 }
 
 enum idxd_portal_prot {
diff --git a/drivers/dma/idxd/init.c b/drivers/dma/idxd/init.c
index 0cd405a07e4e..3163fd16d6dc 100644
--- a/drivers/dma/idxd/init.c
+++ b/drivers/dma/idxd/init.c
@@ -555,18 +555,15 @@ static int idxd_probe(struct idxd_device *idxd)
 	dev_dbg(dev, "IDXD reset complete\n");
 
 	if (IS_ENABLED(CONFIG_INTEL_IDXD_SVM) && sva) {
-		rc = iommu_dev_enable_feature(dev, IOMMU_DEV_FEAT_SVA);
-		if (rc == 0) {
-			rc = idxd_enable_system_pasid(idxd);
-			if (rc < 0) {
-				iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_SVA);
-				dev_warn(dev, "Failed to enable PASID. No SVA support: %d\n", rc);
-			} else {
-				set_bit(IDXD_FLAG_PASID_ENABLED, &idxd->flags);
-			}
-		} else {
-			dev_warn(dev, "Unable to turn on SVA feature.\n");
-		}
+		if (iommu_dev_enable_feature(dev, IOMMU_DEV_FEAT_SVA))
+			dev_warn(dev, "Unable to turn on user SVA feature.\n");
+		else
+			set_bit(IDXD_FLAG_USER_PASID_ENABLED, &idxd->flags);
+
+		if (idxd_enable_system_pasid(idxd))
+			dev_warn(dev, "No in-kernel DMA with PASID.\n");
+		else
+			set_bit(IDXD_FLAG_PASID_ENABLED, &idxd->flags);
 	} else if (!sva) {
 		dev_warn(dev, "User forced SVA off via module param.\n");
 	}
@@ -606,7 +603,8 @@ static int idxd_probe(struct idxd_device *idxd)
  err:
 	if (device_pasid_enabled(idxd))
 		idxd_disable_system_pasid(idxd);
-	iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_SVA);
+	if (device_user_pasid_enabled(idxd))
+		iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_SVA);
 	return rc;
 }
 
@@ -619,7 +617,8 @@ static void idxd_cleanup(struct idxd_device *idxd)
 	idxd_cleanup_internals(idxd);
 	if (device_pasid_enabled(idxd))
 		idxd_disable_system_pasid(idxd);
-	iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_SVA);
+	if (device_user_pasid_enabled(idxd))
+		iommu_dev_disable_feature(dev, IOMMU_DEV_FEAT_SVA);
 }
 
 static int idxd_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
@@ -788,7 +787,8 @@ static void idxd_remove(struct pci_dev *pdev)
 	idxd_release_int_handles(idxd);
 	pci_free_irq_vectors(pdev);
 	pci_iounmap(pdev, idxd->reg_base);
-	iommu_dev_disable_feature(&pdev->dev, IOMMU_DEV_FEAT_SVA);
+	if (device_user_pasid_enabled(idxd))
+		iommu_dev_disable_feature(&pdev->dev, IOMMU_DEV_FEAT_SVA);
 	pci_disable_device(pdev);
 	destroy_workqueue(idxd->wq);
 	perfmon_pmu_remove(idxd);
diff --git a/drivers/dma/idxd/sysfs.c b/drivers/dma/idxd/sysfs.c
index dd3959b5e570..845824a2a679 100644
--- a/drivers/dma/idxd/sysfs.c
+++ b/drivers/dma/idxd/sysfs.c
@@ -829,7 +829,7 @@ static ssize_t wq_mode_store(struct device *dev,
 	if (sysfs_streq(buf, "dedicated")) {
 		set_bit(WQ_FLAG_DEDICATED, &wq->flags);
 		wq->threshold = 0;
-	} else if (sysfs_streq(buf, "shared") && device_swq_supported(idxd)) {
+	} else if (sysfs_streq(buf, "shared")) {
 		clear_bit(WQ_FLAG_DEDICATED, &wq->flags);
 	} else {
 		return -EINVAL;
