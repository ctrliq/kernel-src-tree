KVM: x86/mmu: Refactor TDP MMU iterators to take kvm_mmu_page root

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit 77aa60753a7b11502430c65a9d10d18af019a5b0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/77aa6075.failed

Instead of passing a pointer to the root page table and the root level
separately, pass in a pointer to the root kvm_mmu_page struct.  This
reduces the number of arguments by 1, cutting down on line lengths.

No functional change intended.

	Reviewed-by: Ben Gardon <bgardon@google.com>
	Reviewed-by: Peter Xu <peterx@redhat.com>
	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20220119230739.2234394-12-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 77aa60753a7b11502430c65a9d10d18af019a5b0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 2d278f0eaa20,653315d34964..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -699,20 -711,42 +698,25 @@@ static inline bool __must_check tdp_mmu
   * scheduler needs the CPU or there is contention on the MMU lock. If this
   * function cannot yield, it will not release the MMU lock or reschedule and
   * the caller must ensure it does not supply too large a GFN range, or the
 - * operation can cause a soft lockup.
 - *
 - * If shared is true, this thread holds the MMU lock in read mode and must
 - * account for the possibility that other threads are modifying the paging
 - * structures concurrently. If shared is false, this thread should hold the
 - * MMU lock in write mode.
 + * operation can cause a soft lockup.  Note, in some use cases a flush may be
 + * required by prior actions.  Ensure the pending flush is performed prior to
 + * yielding.
   */
  static bool zap_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,
 -			  gfn_t start, gfn_t end, bool can_yield, bool flush,
 -			  bool shared)
 +			  gfn_t start, gfn_t end, bool can_yield, bool flush)
  {
 -	gfn_t max_gfn_host = 1ULL << (shadow_phys_bits - PAGE_SHIFT);
 -	bool zap_all = (start == 0 && end >= max_gfn_host);
  	struct tdp_iter iter;
  
 -	/*
 -	 * No need to try to step down in the iterator when zapping all SPTEs,
 -	 * zapping the top-level non-leaf SPTEs will recurse on their children.
 -	 */
 -	int min_level = zap_all ? root->role.level : PG_LEVEL_4K;
 -
 -	/*
 -	 * Bound the walk at host.MAXPHYADDR, guest accesses beyond that will
 -	 * hit a #PF(RSVD) and never get to an EPT Violation/Misconfig / #NPF,
 -	 * and so KVM will never install a SPTE for such addresses.
 -	 */
 -	end = min(end, max_gfn_host);
 -
 -	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 -
  	rcu_read_lock();
  
++<<<<<<< HEAD
 +	tdp_root_for_each_pte(iter, root, start, end) {
++=======
+ 	for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
+ retry:
++>>>>>>> 77aa60753a7b (KVM: x86/mmu: Refactor TDP MMU iterators to take kvm_mmu_page root)
  		if (can_yield &&
 -		    tdp_mmu_iter_cond_resched(kvm, &iter, flush, shared)) {
 +		    tdp_mmu_iter_cond_resched(kvm, &iter, flush)) {
  			flush = false;
  			continue;
  		}
@@@ -1234,9 -1210,9 +1238,15 @@@ static bool wrprot_gfn_range(struct kv
  
  	BUG_ON(min_level > KVM_MAX_HUGEPAGE_LEVEL);
  
++<<<<<<< HEAD
 +	for_each_tdp_pte_min_level(iter, root->spt, root->role.level,
 +				   min_level, start, end) {
 +		if (tdp_mmu_iter_cond_resched(kvm, &iter, false))
++=======
+ 	for_each_tdp_pte_min_level(iter, root, min_level, start, end) {
+ retry:
+ 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))
++>>>>>>> 77aa60753a7b (KVM: x86/mmu: Refactor TDP MMU iterators to take kvm_mmu_page root)
  			continue;
  
  		if (!is_shadow_present_pte(iter.old_spte) ||
diff --git a/arch/x86/kvm/mmu/tdp_iter.c b/arch/x86/kvm/mmu/tdp_iter.c
index caa96c270b95..be3f096db2eb 100644
--- a/arch/x86/kvm/mmu/tdp_iter.c
+++ b/arch/x86/kvm/mmu/tdp_iter.c
@@ -40,17 +40,19 @@ void tdp_iter_restart(struct tdp_iter *iter)
  * Sets a TDP iterator to walk a pre-order traversal of the paging structure
  * rooted at root_pt, starting with the walk to translate next_last_level_gfn.
  */
-void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
+void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 		    int min_level, gfn_t next_last_level_gfn)
 {
+	int root_level = root->role.level;
+
 	WARN_ON(root_level < 1);
 	WARN_ON(root_level > PT64_ROOT_MAX_LEVEL);
 
 	iter->next_last_level_gfn = next_last_level_gfn;
 	iter->root_level = root_level;
 	iter->min_level = min_level;
-	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root_pt;
-	iter->as_id = kvm_mmu_page_as_id(sptep_to_sp(root_pt));
+	iter->pt_path[iter->root_level - 1] = (tdp_ptep_t)root->spt;
+	iter->as_id = kvm_mmu_page_as_id(root);
 
 	tdp_iter_restart(iter);
 }
diff --git a/arch/x86/kvm/mmu/tdp_iter.h b/arch/x86/kvm/mmu/tdp_iter.h
index e19cabbcb65c..216ebbe76ddd 100644
--- a/arch/x86/kvm/mmu/tdp_iter.h
+++ b/arch/x86/kvm/mmu/tdp_iter.h
@@ -57,17 +57,17 @@ struct tdp_iter {
  * Iterates over every SPTE mapping the GFN range [start, end) in a
  * preorder traversal.
  */
-#define for_each_tdp_pte_min_level(iter, root, root_level, min_level, start, end) \
-	for (tdp_iter_start(&iter, root, root_level, min_level, start); \
+#define for_each_tdp_pte_min_level(iter, root, min_level, start, end) \
+	for (tdp_iter_start(&iter, root, min_level, start); \
 	     iter.valid && iter.gfn < end;		     \
 	     tdp_iter_next(&iter))
 
-#define for_each_tdp_pte(iter, root, root_level, start, end) \
-	for_each_tdp_pte_min_level(iter, root, root_level, PG_LEVEL_4K, start, end)
+#define for_each_tdp_pte(iter, root, start, end) \
+	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)
 
 tdp_ptep_t spte_to_child_pt(u64 pte, int level);
 
-void tdp_iter_start(struct tdp_iter *iter, u64 *root_pt, int root_level,
+void tdp_iter_start(struct tdp_iter *iter, struct kvm_mmu_page *root,
 		    int min_level, gfn_t next_last_level_gfn);
 void tdp_iter_next(struct tdp_iter *iter);
 void tdp_iter_restart(struct tdp_iter *iter);
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
