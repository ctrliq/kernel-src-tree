KVM: Keep memslots in tree-based structures instead of array-based ones

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Maciej S. Szmigiero <maciej.szmigiero@oracle.com>
commit a54d806688fe1e482350ce759a8a0fc9ebf814b0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/a54d8066.failed

The current memslot code uses a (reverse gfn-ordered) memslot array for
keeping track of them.

Because the memslot array that is currently in use cannot be modified
every memslot management operation (create, delete, move, change flags)
has to make a copy of the whole array so it has a scratch copy to work on.

Strictly speaking, however, it is only necessary to make copy of the
memslot that is being modified, copying all the memslots currently present
is just a limitation of the array-based memslot implementation.

Two memslot sets, however, are still needed so the VM continues to run
on the currently active set while the requested operation is being
performed on the second, currently inactive one.

In order to have two memslot sets, but only one copy of actual memslots
it is necessary to split out the memslot data from the memslot sets.

The memslots themselves should be also kept independent of each other
so they can be individually added or deleted.

These two memslot sets should normally point to the same set of
memslots. They can, however, be desynchronized when performing a
memslot management operation by replacing the memslot to be modified
by its copy.  After the operation is complete, both memslot sets once
again point to the same, common set of memslot data.

This commit implements the aforementioned idea.

For tracking of gfns an ordinary rbtree is used since memslots cannot
overlap in the guest address space and so this data structure is
sufficient for ensuring that lookups are done quickly.

The "last used slot" mini-caches (both per-slot set one and per-vCPU one),
that keep track of the last found-by-gfn memslot, are still present in the
new code.

Co-developed-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Maciej S. Szmigiero <maciej.szmigiero@oracle.com>
Message-Id: <17c0cf3663b760a0d3753d4ac08c0753e941b811.1638817641.git.maciej.szmigiero@oracle.com>
(cherry picked from commit a54d806688fe1e482350ce759a8a0fc9ebf814b0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/debugfs.c
#	arch/x86/kvm/mmu/mmu.c
#	include/linux/kvm_host.h
#	virt/kvm/kvm_main.c
diff --cc arch/x86/kvm/debugfs.c
index 95a98413dc32,543a8c04025c..000000000000
--- a/arch/x86/kvm/debugfs.c
+++ b/arch/x86/kvm/debugfs.c
@@@ -73,3 -75,112 +73,115 @@@ void kvm_arch_create_vcpu_debugfs(struc
  				    &vcpu_tsc_scaling_frac_fops);
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * This covers statistics <1024 (11=log(1024)+1), which should be enough to
+  * cover RMAP_RECYCLE_THRESHOLD.
+  */
+ #define  RMAP_LOG_SIZE  11
+ 
+ static const char *kvm_lpage_str[KVM_NR_PAGE_SIZES] = { "4K", "2M", "1G" };
+ 
+ static int kvm_mmu_rmaps_stat_show(struct seq_file *m, void *v)
+ {
+ 	struct kvm_rmap_head *rmap;
+ 	struct kvm *kvm = m->private;
+ 	struct kvm_memory_slot *slot;
+ 	struct kvm_memslots *slots;
+ 	unsigned int lpage_size, index;
+ 	/* Still small enough to be on the stack */
+ 	unsigned int *log[KVM_NR_PAGE_SIZES], *cur;
+ 	int i, j, k, l, ret;
+ 
+ 	ret = -ENOMEM;
+ 	memset(log, 0, sizeof(log));
+ 	for (i = 0; i < KVM_NR_PAGE_SIZES; i++) {
+ 		log[i] = kcalloc(RMAP_LOG_SIZE, sizeof(unsigned int), GFP_KERNEL);
+ 		if (!log[i])
+ 			goto out;
+ 	}
+ 
+ 	mutex_lock(&kvm->slots_lock);
+ 	write_lock(&kvm->mmu_lock);
+ 
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		int bkt;
+ 
+ 		slots = __kvm_memslots(kvm, i);
+ 		kvm_for_each_memslot(slot, bkt, slots)
+ 			for (k = 0; k < KVM_NR_PAGE_SIZES; k++) {
+ 				rmap = slot->arch.rmap[k];
+ 				lpage_size = kvm_mmu_slot_lpages(slot, k + 1);
+ 				cur = log[k];
+ 				for (l = 0; l < lpage_size; l++) {
+ 					index = ffs(pte_list_count(&rmap[l]));
+ 					if (WARN_ON_ONCE(index >= RMAP_LOG_SIZE))
+ 						index = RMAP_LOG_SIZE - 1;
+ 					cur[index]++;
+ 				}
+ 			}
+ 	}
+ 
+ 	write_unlock(&kvm->mmu_lock);
+ 	mutex_unlock(&kvm->slots_lock);
+ 
+ 	/* index=0 counts no rmap; index=1 counts 1 rmap */
+ 	seq_printf(m, "Rmap_Count:\t0\t1\t");
+ 	for (i = 2; i < RMAP_LOG_SIZE; i++) {
+ 		j = 1 << (i - 1);
+ 		k = (1 << i) - 1;
+ 		seq_printf(m, "%d-%d\t", j, k);
+ 	}
+ 	seq_printf(m, "\n");
+ 
+ 	for (i = 0; i < KVM_NR_PAGE_SIZES; i++) {
+ 		seq_printf(m, "Level=%s:\t", kvm_lpage_str[i]);
+ 		cur = log[i];
+ 		for (j = 0; j < RMAP_LOG_SIZE; j++)
+ 			seq_printf(m, "%d\t", cur[j]);
+ 		seq_printf(m, "\n");
+ 	}
+ 
+ 	ret = 0;
+ out:
+ 	for (i = 0; i < KVM_NR_PAGE_SIZES; i++)
+ 		kfree(log[i]);
+ 
+ 	return ret;
+ }
+ 
+ static int kvm_mmu_rmaps_stat_open(struct inode *inode, struct file *file)
+ {
+ 	struct kvm *kvm = inode->i_private;
+ 
+ 	if (!kvm_get_kvm_safe(kvm))
+ 		return -ENOENT;
+ 
+ 	return single_open(file, kvm_mmu_rmaps_stat_show, kvm);
+ }
+ 
+ static int kvm_mmu_rmaps_stat_release(struct inode *inode, struct file *file)
+ {
+ 	struct kvm *kvm = inode->i_private;
+ 
+ 	kvm_put_kvm(kvm);
+ 
+ 	return single_release(inode, file);
+ }
+ 
+ static const struct file_operations mmu_rmaps_stat_fops = {
+ 	.open		= kvm_mmu_rmaps_stat_open,
+ 	.read		= seq_read,
+ 	.llseek		= seq_lseek,
+ 	.release	= kvm_mmu_rmaps_stat_release,
+ };
+ 
+ int kvm_arch_create_vm_debugfs(struct kvm *kvm)
+ {
+ 	debugfs_create_file("mmu_rmaps_stat", 0644, kvm->debugfs_dentry, kvm,
+ 			    &mmu_rmaps_stat_fops);
+ 	return 0;
+ }
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
diff --cc arch/x86/kvm/mmu/mmu.c
index 970fc4a0eb93,c61430994d19..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -3436,6 -3405,67 +3436,70 @@@ out_unlock
  	return r;
  }
  
++<<<<<<< HEAD
++=======
+ static int mmu_first_shadow_root_alloc(struct kvm *kvm)
+ {
+ 	struct kvm_memslots *slots;
+ 	struct kvm_memory_slot *slot;
+ 	int r = 0, i, bkt;
+ 
+ 	/*
+ 	 * Check if this is the first shadow root being allocated before
+ 	 * taking the lock.
+ 	 */
+ 	if (kvm_shadow_root_allocated(kvm))
+ 		return 0;
+ 
+ 	mutex_lock(&kvm->slots_arch_lock);
+ 
+ 	/* Recheck, under the lock, whether this is the first shadow root. */
+ 	if (kvm_shadow_root_allocated(kvm))
+ 		goto out_unlock;
+ 
+ 	/*
+ 	 * Check if anything actually needs to be allocated, e.g. all metadata
+ 	 * will be allocated upfront if TDP is disabled.
+ 	 */
+ 	if (kvm_memslots_have_rmaps(kvm) &&
+ 	    kvm_page_track_write_tracking_enabled(kvm))
+ 		goto out_success;
+ 
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		slots = __kvm_memslots(kvm, i);
+ 		kvm_for_each_memslot(slot, bkt, slots) {
+ 			/*
+ 			 * Both of these functions are no-ops if the target is
+ 			 * already allocated, so unconditionally calling both
+ 			 * is safe.  Intentionally do NOT free allocations on
+ 			 * failure to avoid having to track which allocations
+ 			 * were made now versus when the memslot was created.
+ 			 * The metadata is guaranteed to be freed when the slot
+ 			 * is freed, and will be kept/used if userspace retries
+ 			 * KVM_RUN instead of killing the VM.
+ 			 */
+ 			r = memslot_rmap_alloc(slot, slot->npages);
+ 			if (r)
+ 				goto out_unlock;
+ 			r = kvm_page_track_write_tracking_alloc(slot);
+ 			if (r)
+ 				goto out_unlock;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Ensure that shadow_root_allocated becomes true strictly after
+ 	 * all the related pointers are set.
+ 	 */
+ out_success:
+ 	smp_store_release(&kvm->arch.shadow_root_allocated, true);
+ 
+ out_unlock:
+ 	mutex_unlock(&kvm->slots_arch_lock);
+ 	return r;
+ }
+ 
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
  {
  	struct kvm_mmu *mmu = vcpu->arch.mmu;
@@@ -5678,6 -5724,34 +5742,37 @@@ void kvm_mmu_uninit_vm(struct kvm *kvm
  	kvm_mmu_uninit_tdp_mmu(kvm);
  }
  
++<<<<<<< HEAD
++=======
+ static bool __kvm_zap_rmaps(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)
+ {
+ 	const struct kvm_memory_slot *memslot;
+ 	struct kvm_memslots *slots;
+ 	bool flush = false;
+ 	gfn_t start, end;
+ 	int i, bkt;
+ 
+ 	if (!kvm_memslots_have_rmaps(kvm))
+ 		return flush;
+ 
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		slots = __kvm_memslots(kvm, i);
+ 		kvm_for_each_memslot(memslot, bkt, slots) {
+ 			start = max(gfn_start, memslot->base_gfn);
+ 			end = min(gfn_end, memslot->base_gfn + memslot->npages);
+ 			if (start >= end)
+ 				continue;
+ 
+ 			flush = slot_handle_level_range(kvm, memslot, kvm_zap_rmapp,
+ 							PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,
+ 							start, end - 1, true, flush);
+ 		}
+ 	}
+ 
+ 	return flush;
+ }
+ 
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  /*
   * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end
   * (not including it)
diff --cc include/linux/kvm_host.h
index 5a84e8a20ec4,9eda8a63feae..000000000000
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@@ -32,6 -29,10 +32,13 @@@
  #include <linux/refcount.h>
  #include <linux/nospec.h>
  #include <linux/notifier.h>
++<<<<<<< HEAD
++=======
+ #include <linux/hashtable.h>
+ #include <linux/interval_tree.h>
+ #include <linux/rbtree.h>
+ #include <linux/xarray.h>
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  #include <asm/signal.h>
  
  #include <linux/kvm.h>
@@@ -352,13 -359,60 +359,15 @@@ struct kvm_vcpu 
  	struct kvm_dirty_ring dirty_ring;
  
  	/*
- 	 * The index of the most recently used memslot by this vCPU. It's ok
- 	 * if this becomes stale due to memslot changes since we always check
- 	 * it is a valid slot.
+ 	 * The most recently used memslot by this vCPU and the slots generation
+ 	 * for which it is valid.
+ 	 * No wraparound protection is needed since generations won't overflow in
+ 	 * thousands of years, even assuming 1M memslot operations per second.
  	 */
- 	int last_used_slot;
+ 	struct kvm_memory_slot *last_used_slot;
+ 	u64 last_used_slot_gen;
  };
  
 -/* must be called with irqs disabled */
 -static __always_inline void guest_enter_irqoff(void)
 -{
 -	/*
 -	 * This is running in ioctl context so its safe to assume that it's the
 -	 * stime pending cputime to flush.
 -	 */
 -	instrumentation_begin();
 -	vtime_account_guest_enter();
 -	instrumentation_end();
 -
 -	/*
 -	 * KVM does not hold any references to rcu protected data when it
 -	 * switches CPU into a guest mode. In fact switching to a guest mode
 -	 * is very similar to exiting to userspace from rcu point of view. In
 -	 * addition CPU may stay in a guest mode for quite a long time (up to
 -	 * one time slice). Lets treat guest mode as quiescent state, just like
 -	 * we do with user-mode execution.
 -	 */
 -	if (!context_tracking_guest_enter()) {
 -		instrumentation_begin();
 -		rcu_virt_note_context_switch(smp_processor_id());
 -		instrumentation_end();
 -	}
 -}
 -
 -static __always_inline void guest_exit_irqoff(void)
 -{
 -	context_tracking_guest_exit();
 -
 -	instrumentation_begin();
 -	/* Flush the guest cputime we spent on the guest */
 -	vtime_account_guest_exit();
 -	instrumentation_end();
 -}
 -
 -static inline void guest_exit(void)
 -{
 -	unsigned long flags;
 -
 -	local_irq_save(flags);
 -	guest_exit_irqoff();
 -	local_irq_restore(flags);
 -}
 -
  static inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)
  {
  	/*
@@@ -376,7 -430,26 +385,29 @@@
   */
  #define KVM_MEM_MAX_NR_PAGES ((1UL << 31) - 1)
  
+ /*
+  * Since at idle each memslot belongs to two memslot sets it has to contain
+  * two embedded nodes for each data structure that it forms a part of.
+  *
+  * Two memslot sets (one active and one inactive) are necessary so the VM
+  * continues to run on one memslot set while the other is being modified.
+  *
+  * These two memslot sets normally point to the same set of memslots.
+  * They can, however, be desynchronized when performing a memslot management
+  * operation by replacing the memslot to be modified by its copy.
+  * After the operation is complete, both memslot sets once again point to
+  * the same, common set of memslot data.
+  *
+  * The memslots themselves are independent of each other so they can be
+  * individually added or deleted.
+  */
  struct kvm_memory_slot {
++<<<<<<< HEAD
++=======
+ 	struct hlist_node id_node[2];
+ 	struct interval_tree_node hva_node[2];
+ 	struct rb_node gfn_node[2];
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  	gfn_t base_gfn;
  	unsigned long npages;
  	unsigned long *dirty_bitmap;
@@@ -471,18 -544,21 +502,29 @@@ static inline int kvm_arch_vcpu_memslot
  }
  #endif
  
- /*
-  * Note:
-  * memslots are not sorted by id anymore, please use id_to_memslot()
-  * to get the memslot by its id.
-  */
  struct kvm_memslots {
  	u64 generation;
++<<<<<<< HEAD
 +	/* The mapping table from slot id to the index in memslots[]. */
 +	short id_to_index[KVM_MEM_SLOTS_NUM];
 +	atomic_t last_used_slot;
 +	int used_slots;
 +	struct kvm_memory_slot memslots[];
++=======
+ 	atomic_long_t last_used_slot;
+ 	struct rb_root_cached hva_tree;
+ 	struct rb_root gfn_tree;
+ 	/*
+ 	 * The mapping table from slot id to memslot.
+ 	 *
+ 	 * 7-bit bucket count matches the size of the old id to index array for
+ 	 * 512 slots, while giving good performance with this slot count.
+ 	 * Higher bucket counts bring only small performance improvements but
+ 	 * always result in higher memory usage (even for lower memslot counts).
+ 	 */
+ 	DECLARE_HASHTABLE(id_hash, 7);
+ 	int node_idx;
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  };
  
  struct kvm {
@@@ -504,8 -580,16 +546,11 @@@
  	struct mutex slots_arch_lock;
  	struct mm_struct *mm; /* userspace tied to this vm */
  	unsigned long nr_memslot_pages;
+ 	/* The two memslot sets - active and inactive (per address space) */
+ 	struct kvm_memslots __memslots[KVM_ADDRESS_SPACE_NUM][2];
+ 	/* The current active memslot set for each address space */
  	struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
 -	struct xarray vcpu_array;
 -
 -	/* Used to wait for completion of MMU notifiers.  */
 -	spinlock_t mn_invalidate_lock;
 -	unsigned long mn_active_invalidate_count;
 -	struct rcuwait mn_memslots_update_rcuwait;
 +	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
  
  	/*
  	 * created_vcpus is protected by kvm->lock, and is incremented
@@@ -674,13 -757,12 +719,12 @@@ static inline struct kvm_vcpu *kvm_get_
  	return NULL;
  }
  
- #define kvm_for_each_memslot(memslot, slots)				\
- 	for (memslot = &slots->memslots[0];				\
- 	     memslot < slots->memslots + slots->used_slots; memslot++)	\
- 		if (WARN_ON_ONCE(!memslot->npages)) {			\
- 		} else
+ static inline int kvm_vcpu_get_idx(struct kvm_vcpu *vcpu)
+ {
+ 	return vcpu->vcpu_idx;
+ }
  
 -void kvm_destroy_vcpus(struct kvm *kvm);
 +void kvm_vcpu_destroy(struct kvm_vcpu *vcpu);
  
  void vcpu_load(struct kvm_vcpu *vcpu);
  void vcpu_put(struct kvm_vcpu *vcpu);
@@@ -743,16 -835,15 +797,24 @@@ static inline bool kvm_memslots_empty(s
  static inline
  struct kvm_memory_slot *id_to_memslot(struct kvm_memslots *slots, int id)
  {
 +	int index = slots->id_to_index[id];
  	struct kvm_memory_slot *slot;
+ 	int idx = slots->node_idx;
  
++<<<<<<< HEAD
 +	if (index < 0)
 +		return NULL;
++=======
+ 	hash_for_each_possible(slots->id_hash, slot, id_node[idx], id) {
+ 		if (slot->id == id)
+ 			return slot;
+ 	}
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  
 -	return NULL;
 +	slot = &slots->memslots[index];
 +
 +	WARN_ON(slot->id != id);
 +	return slot;
  }
  
  /*
diff --cc virt/kvm/kvm_main.c
index b1373f69ce5e,a87df97e0b14..000000000000
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@@ -452,10 -433,10 +452,10 @@@ static void kvm_vcpu_init(struct kvm_vc
  	vcpu->preempted = false;
  	vcpu->ready = false;
  	preempt_notifier_init(&vcpu->preempt_notifier, &kvm_preempt_ops);
- 	vcpu->last_used_slot = 0;
+ 	vcpu->last_used_slot = NULL;
  }
  
 -static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 +void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
  {
  	kvm_dirty_ring_free(&vcpu->dirty_ring);
  	kvm_arch_vcpu_destroy(vcpu);
@@@ -490,6 -484,151 +490,154 @@@ static void kvm_mmu_notifier_invalidate
  	srcu_read_unlock(&kvm->srcu, idx);
  }
  
++<<<<<<< HEAD
++=======
+ typedef bool (*hva_handler_t)(struct kvm *kvm, struct kvm_gfn_range *range);
+ 
+ typedef void (*on_lock_fn_t)(struct kvm *kvm, unsigned long start,
+ 			     unsigned long end);
+ 
+ struct kvm_hva_range {
+ 	unsigned long start;
+ 	unsigned long end;
+ 	pte_t pte;
+ 	hva_handler_t handler;
+ 	on_lock_fn_t on_lock;
+ 	bool flush_on_ret;
+ 	bool may_block;
+ };
+ 
+ /*
+  * Use a dedicated stub instead of NULL to indicate that there is no callback
+  * function/handler.  The compiler technically can't guarantee that a real
+  * function will have a non-zero address, and so it will generate code to
+  * check for !NULL, whereas comparing against a stub will be elided at compile
+  * time (unless the compiler is getting long in the tooth, e.g. gcc 4.9).
+  */
+ static void kvm_null_fn(void)
+ {
+ 
+ }
+ #define IS_KVM_NULL_FN(fn) ((fn) == (void *)kvm_null_fn)
+ 
+ /* Iterate over each memslot intersecting [start, last] (inclusive) range */
+ #define kvm_for_each_memslot_in_hva_range(node, slots, start, last)	     \
+ 	for (node = interval_tree_iter_first(&slots->hva_tree, start, last); \
+ 	     node;							     \
+ 	     node = interval_tree_iter_next(node, start, last))	     \
+ 
+ static __always_inline int __kvm_handle_hva_range(struct kvm *kvm,
+ 						  const struct kvm_hva_range *range)
+ {
+ 	bool ret = false, locked = false;
+ 	struct kvm_gfn_range gfn_range;
+ 	struct kvm_memory_slot *slot;
+ 	struct kvm_memslots *slots;
+ 	int i, idx;
+ 
+ 	if (WARN_ON_ONCE(range->end <= range->start))
+ 		return 0;
+ 
+ 	/* A null handler is allowed if and only if on_lock() is provided. */
+ 	if (WARN_ON_ONCE(IS_KVM_NULL_FN(range->on_lock) &&
+ 			 IS_KVM_NULL_FN(range->handler)))
+ 		return 0;
+ 
+ 	idx = srcu_read_lock(&kvm->srcu);
+ 
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		struct interval_tree_node *node;
+ 
+ 		slots = __kvm_memslots(kvm, i);
+ 		kvm_for_each_memslot_in_hva_range(node, slots,
+ 						  range->start, range->end - 1) {
+ 			unsigned long hva_start, hva_end;
+ 
+ 			slot = container_of(node, struct kvm_memory_slot, hva_node[slots->node_idx]);
+ 			hva_start = max(range->start, slot->userspace_addr);
+ 			hva_end = min(range->end, slot->userspace_addr +
+ 						  (slot->npages << PAGE_SHIFT));
+ 
+ 			/*
+ 			 * To optimize for the likely case where the address
+ 			 * range is covered by zero or one memslots, don't
+ 			 * bother making these conditional (to avoid writes on
+ 			 * the second or later invocation of the handler).
+ 			 */
+ 			gfn_range.pte = range->pte;
+ 			gfn_range.may_block = range->may_block;
+ 
+ 			/*
+ 			 * {gfn(page) | page intersects with [hva_start, hva_end)} =
+ 			 * {gfn_start, gfn_start+1, ..., gfn_end-1}.
+ 			 */
+ 			gfn_range.start = hva_to_gfn_memslot(hva_start, slot);
+ 			gfn_range.end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, slot);
+ 			gfn_range.slot = slot;
+ 
+ 			if (!locked) {
+ 				locked = true;
+ 				KVM_MMU_LOCK(kvm);
+ 				if (!IS_KVM_NULL_FN(range->on_lock))
+ 					range->on_lock(kvm, range->start, range->end);
+ 				if (IS_KVM_NULL_FN(range->handler))
+ 					break;
+ 			}
+ 			ret |= range->handler(kvm, &gfn_range);
+ 		}
+ 	}
+ 
+ 	if (range->flush_on_ret && ret)
+ 		kvm_flush_remote_tlbs(kvm);
+ 
+ 	if (locked)
+ 		KVM_MMU_UNLOCK(kvm);
+ 
+ 	srcu_read_unlock(&kvm->srcu, idx);
+ 
+ 	/* The notifiers are averse to booleans. :-( */
+ 	return (int)ret;
+ }
+ 
+ static __always_inline int kvm_handle_hva_range(struct mmu_notifier *mn,
+ 						unsigned long start,
+ 						unsigned long end,
+ 						pte_t pte,
+ 						hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= pte,
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= true,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
+ 
+ static __always_inline int kvm_handle_hva_range_no_flush(struct mmu_notifier *mn,
+ 							 unsigned long start,
+ 							 unsigned long end,
+ 							 hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= __pte(0),
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= false,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
  					struct mm_struct *mm,
  					unsigned long address,
@@@ -732,21 -876,6 +880,24 @@@ static void kvm_destroy_pm_notifier(str
  }
  #endif /* CONFIG_HAVE_KVM_PM_NOTIFIER */
  
++<<<<<<< HEAD
 +static struct kvm_memslots *kvm_alloc_memslots(void)
 +{
 +	int i;
 +	struct kvm_memslots *slots;
 +
 +	slots = kvzalloc(sizeof(struct kvm_memslots), GFP_KERNEL_ACCOUNT);
 +	if (!slots)
 +		return NULL;
 +
 +	for (i = 0; i < KVM_MEM_SLOTS_NUM; i++)
 +		slots->id_to_index[i] = -1;
 +
 +	return slots;
 +}
 +
++=======
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  static void kvm_destroy_dirty_bitmap(struct kvm_memory_slot *memslot)
  {
  	if (!memslot->dirty_bitmap)
@@@ -1127,167 -1284,136 +1292,300 @@@ static int kvm_alloc_dirty_bitmap(struc
  	return 0;
  }
  
++<<<<<<< HEAD
 +/*
 + * Delete a memslot by decrementing the number of used slots and shifting all
 + * other entries in the array forward one spot.
 + */
 +static inline void kvm_memslot_delete(struct kvm_memslots *slots,
 +				      struct kvm_memory_slot *memslot)
 +{
 +	struct kvm_memory_slot *mslots = slots->memslots;
 +	int i;
 +
 +	if (WARN_ON(slots->id_to_index[memslot->id] == -1))
 +		return;
 +
 +	slots->used_slots--;
 +
 +	if (atomic_read(&slots->last_used_slot) >= slots->used_slots)
 +		atomic_set(&slots->last_used_slot, 0);
 +
 +	for (i = slots->id_to_index[memslot->id]; i < slots->used_slots; i++) {
 +		mslots[i] = mslots[i + 1];
 +		slots->id_to_index[mslots[i].id] = i;
 +	}
 +	mslots[i] = *memslot;
 +	slots->id_to_index[memslot->id] = -1;
 +}
 +
 +/*
 + * "Insert" a new memslot by incrementing the number of used slots.  Returns
 + * the new slot's initial index into the memslots array.
 + */
 +static inline int kvm_memslot_insert_back(struct kvm_memslots *slots)
 +{
 +	return slots->used_slots++;
 +}
 +
 +/*
 + * Move a changed memslot backwards in the array by shifting existing slots
 + * with a higher GFN toward the front of the array.  Note, the changed memslot
 + * itself is not preserved in the array, i.e. not swapped at this time, only
 + * its new index into the array is tracked.  Returns the changed memslot's
 + * current index into the memslots array.
 + */
 +static inline int kvm_memslot_move_backward(struct kvm_memslots *slots,
 +					    struct kvm_memory_slot *memslot)
 +{
 +	struct kvm_memory_slot *mslots = slots->memslots;
 +	int i;
 +
 +	if (slots->id_to_index[memslot->id] == -1 || !slots->used_slots)
 +		return -1;
 +
 +	/*
 +	 * Move the target memslot backward in the array by shifting existing
 +	 * memslots with a higher GFN (than the target memslot) towards the
 +	 * front of the array.
 +	 */
 +	for (i = slots->id_to_index[memslot->id]; i < slots->used_slots - 1; i++) {
 +		if (memslot->base_gfn > mslots[i + 1].base_gfn)
 +			break;
 +
 +		WARN_ON_ONCE(memslot->base_gfn == mslots[i + 1].base_gfn);
 +
 +		/* Shift the next memslot forward one and update its index. */
 +		mslots[i] = mslots[i + 1];
 +		slots->id_to_index[mslots[i].id] = i;
 +	}
 +	return i;
 +}
 +
 +/*
 + * Move a changed memslot forwards in the array by shifting existing slots with
 + * a lower GFN toward the back of the array.  Note, the changed memslot itself
 + * is not preserved in the array, i.e. not swapped at this time, only its new
 + * index into the array is tracked.  Returns the changed memslot's final index
 + * into the memslots array.
 + */
 +static inline int kvm_memslot_move_forward(struct kvm_memslots *slots,
 +					   struct kvm_memory_slot *memslot,
 +					   int start)
 +{
 +	struct kvm_memory_slot *mslots = slots->memslots;
 +	int i;
 +
 +	for (i = start; i > 0; i--) {
 +		if (memslot->base_gfn < mslots[i - 1].base_gfn)
 +			break;
 +
 +		WARN_ON_ONCE(memslot->base_gfn == mslots[i - 1].base_gfn);
 +
 +		/* Shift the next memslot back one and update its index. */
 +		mslots[i] = mslots[i - 1];
 +		slots->id_to_index[mslots[i].id] = i;
 +	}
 +	return i;
 +}
 +
 +/*
 + * Re-sort memslots based on their GFN to account for an added, deleted, or
 + * moved memslot.  Sorting memslots by GFN allows using a binary search during
 + * memslot lookup.
 + *
 + * IMPORTANT: Slots are sorted from highest GFN to lowest GFN!  I.e. the entry
 + * at memslots[0] has the highest GFN.
 + *
 + * The sorting algorithm takes advantage of having initially sorted memslots
 + * and knowing the position of the changed memslot.  Sorting is also optimized
 + * by not swapping the updated memslot and instead only shifting other memslots
 + * and tracking the new index for the update memslot.  Only once its final
 + * index is known is the updated memslot copied into its position in the array.
 + *
 + *  - When deleting a memslot, the deleted memslot simply needs to be moved to
 + *    the end of the array.
 + *
 + *  - When creating a memslot, the algorithm "inserts" the new memslot at the
 + *    end of the array and then it forward to its correct location.
 + *
 + *  - When moving a memslot, the algorithm first moves the updated memslot
 + *    backward to handle the scenario where the memslot's GFN was changed to a
 + *    lower value.  update_memslots() then falls through and runs the same flow
 + *    as creating a memslot to move the memslot forward to handle the scenario
 + *    where its GFN was changed to a higher value.
 + *
 + * Note, slots are sorted from highest->lowest instead of lowest->highest for
 + * historical reasons.  Originally, invalid memslots where denoted by having
 + * GFN=0, thus sorting from highest->lowest naturally sorted invalid memslots
 + * to the end of the array.  The current algorithm uses dedicated logic to
 + * delete a memslot and thus does not rely on invalid memslots having GFN=0.
 + *
 + * The other historical motiviation for highest->lowest was to improve the
 + * performance of memslot lookup.  KVM originally used a linear search starting
 + * at memslots[0].  On x86, the largest memslot usually has one of the highest,
 + * if not *the* highest, GFN, as the bulk of the guest's RAM is located in a
 + * single memslot above the 4gb boundary.  As the largest memslot is also the
 + * most likely to be referenced, sorting it to the front of the array was
 + * advantageous.  The current binary search starts from the middle of the array
 + * and uses an LRU pointer to improve performance for all memslots and GFNs.
 + */
 +static void update_memslots(struct kvm_memslots *slots,
 +			    struct kvm_memory_slot *memslot,
 +			    enum kvm_mr_change change)
 +{
 +	int i;
 +
 +	if (change == KVM_MR_DELETE) {
 +		kvm_memslot_delete(slots, memslot);
 +	} else {
 +		if (change == KVM_MR_CREATE)
 +			i = kvm_memslot_insert_back(slots);
 +		else
 +			i = kvm_memslot_move_backward(slots, memslot);
 +		i = kvm_memslot_move_forward(slots, memslot, i);
 +
 +		if (WARN_ON_ONCE(i < 0))
 +			return;
 +
 +		/*
 +		 * Copy the memslot to its new position in memslots and update
 +		 * its index accordingly.
 +		 */
 +		slots->memslots[i] = *memslot;
 +		slots->id_to_index[memslot->id] = i;
++=======
+ static struct kvm_memslots *kvm_get_inactive_memslots(struct kvm *kvm, int as_id)
+ {
+ 	struct kvm_memslots *active = __kvm_memslots(kvm, as_id);
+ 	int node_idx_inactive = active->node_idx ^ 1;
+ 
+ 	return &kvm->__memslots[as_id][node_idx_inactive];
+ }
+ 
+ /*
+  * Helper to get the address space ID when one of memslot pointers may be NULL.
+  * This also serves as a sanity that at least one of the pointers is non-NULL,
+  * and that their address space IDs don't diverge.
+  */
+ static int kvm_memslots_get_as_id(struct kvm_memory_slot *a,
+ 				  struct kvm_memory_slot *b)
+ {
+ 	if (WARN_ON_ONCE(!a && !b))
+ 		return 0;
+ 
+ 	if (!a)
+ 		return b->as_id;
+ 	if (!b)
+ 		return a->as_id;
+ 
+ 	WARN_ON_ONCE(a->as_id != b->as_id);
+ 	return a->as_id;
+ }
+ 
+ static void kvm_insert_gfn_node(struct kvm_memslots *slots,
+ 				struct kvm_memory_slot *slot)
+ {
+ 	struct rb_root *gfn_tree = &slots->gfn_tree;
+ 	struct rb_node **node, *parent;
+ 	int idx = slots->node_idx;
+ 
+ 	parent = NULL;
+ 	for (node = &gfn_tree->rb_node; *node; ) {
+ 		struct kvm_memory_slot *tmp;
+ 
+ 		tmp = container_of(*node, struct kvm_memory_slot, gfn_node[idx]);
+ 		parent = *node;
+ 		if (slot->base_gfn < tmp->base_gfn)
+ 			node = &(*node)->rb_left;
+ 		else if (slot->base_gfn > tmp->base_gfn)
+ 			node = &(*node)->rb_right;
+ 		else
+ 			BUG();
+ 	}
+ 
+ 	rb_link_node(&slot->gfn_node[idx], parent, node);
+ 	rb_insert_color(&slot->gfn_node[idx], gfn_tree);
+ }
+ 
+ static void kvm_erase_gfn_node(struct kvm_memslots *slots,
+ 			       struct kvm_memory_slot *slot)
+ {
+ 	rb_erase(&slot->gfn_node[slots->node_idx], &slots->gfn_tree);
+ }
+ 
+ static void kvm_replace_gfn_node(struct kvm_memslots *slots,
+ 				 struct kvm_memory_slot *old,
+ 				 struct kvm_memory_slot *new)
+ {
+ 	int idx = slots->node_idx;
+ 
+ 	WARN_ON_ONCE(old->base_gfn != new->base_gfn);
+ 
+ 	rb_replace_node(&old->gfn_node[idx], &new->gfn_node[idx],
+ 			&slots->gfn_tree);
+ }
+ 
+ /*
+  * Replace @old with @new in the inactive memslots.
+  *
+  * With NULL @old this simply adds @new.
+  * With NULL @new this simply removes @old.
+  *
+  * If @new is non-NULL its hva_node[slots_idx] range has to be set
+  * appropriately.
+  */
+ static void kvm_replace_memslot(struct kvm *kvm,
+ 				struct kvm_memory_slot *old,
+ 				struct kvm_memory_slot *new)
+ {
+ 	int as_id = kvm_memslots_get_as_id(old, new);
+ 	struct kvm_memslots *slots = kvm_get_inactive_memslots(kvm, as_id);
+ 	int idx = slots->node_idx;
+ 
+ 	if (old) {
+ 		hash_del(&old->id_node[idx]);
+ 		interval_tree_remove(&old->hva_node[idx], &slots->hva_tree);
+ 
+ 		if ((long)old == atomic_long_read(&slots->last_used_slot))
+ 			atomic_long_set(&slots->last_used_slot, (long)new);
+ 
+ 		if (!new) {
+ 			kvm_erase_gfn_node(slots, old);
+ 			return;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Initialize @new's hva range.  Do this even when replacing an @old
+ 	 * slot, kvm_copy_memslot() deliberately does not touch node data.
+ 	 */
+ 	new->hva_node[idx].start = new->userspace_addr;
+ 	new->hva_node[idx].last = new->userspace_addr +
+ 				  (new->npages << PAGE_SHIFT) - 1;
+ 
+ 	/*
+ 	 * (Re)Add the new memslot.  There is no O(1) interval_tree_replace(),
+ 	 * hva_node needs to be swapped with remove+insert even though hva can't
+ 	 * change when replacing an existing slot.
+ 	 */
+ 	hash_add(slots->id_hash, &new->id_node[idx], new->id);
+ 	interval_tree_insert(&new->hva_node[idx], &slots->hva_tree);
+ 
+ 	/*
+ 	 * If the memslot gfn is unchanged, rb_replace_node() can be used to
+ 	 * switch the node in the gfn tree instead of removing the old and
+ 	 * inserting the new as two separate operations. Replacement is a
+ 	 * single O(1) operation versus two O(log(n)) operations for
+ 	 * remove+insert.
+ 	 */
+ 	if (old && old->base_gfn == new->base_gfn) {
+ 		kvm_replace_gfn_node(slots, old, new);
+ 	} else {
+ 		if (old)
+ 			kvm_erase_gfn_node(slots, old);
+ 		kvm_insert_gfn_node(slots, new);
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  	}
  }
  
@@@ -1345,52 -1487,246 +1644,295 @@@ static void kvm_swap_active_memslots(st
  	kvm_arch_memslots_updated(kvm, gen);
  
  	slots->generation = gen;
++<<<<<<< HEAD
 +
 +	return old_memslots;
 +}
 +
 +static size_t kvm_memslots_size(int slots)
 +{
 +	return sizeof(struct kvm_memslots) +
 +	       (sizeof(struct kvm_memory_slot) * slots);
 +}
 +
 +/*
 + * Note, at a minimum, the current number of used slots must be allocated, even
 + * when deleting a memslot, as we need a complete duplicate of the memslots for
 + * use when invalidating a memslot prior to deleting/moving the memslot.
 + */
 +static struct kvm_memslots *kvm_dup_memslots(struct kvm_memslots *old,
 +					     enum kvm_mr_change change)
 +{
 +	struct kvm_memslots *slots;
 +	size_t new_size;
 +
 +	if (change == KVM_MR_CREATE)
 +		new_size = kvm_memslots_size(old->used_slots + 1);
 +	else
 +		new_size = kvm_memslots_size(old->used_slots);
 +
 +	slots = kvzalloc(new_size, GFP_KERNEL_ACCOUNT);
 +	if (likely(slots))
 +		memcpy(slots, old, kvm_memslots_size(old->used_slots));
 +
 +	return slots;
 +}
 +
 +static void kvm_copy_memslots_arch(struct kvm_memslots *to,
 +				   struct kvm_memslots *from)
 +{
 +	int i;
 +
 +	WARN_ON_ONCE(to->used_slots != from->used_slots);
 +
 +	for (i = 0; i < from->used_slots; i++)
 +		to->memslots[i].arch = from->memslots[i].arch;
 +}
 +
 +static int kvm_set_memslot(struct kvm *kvm,
 +			   const struct kvm_userspace_memory_region *mem,
++=======
+ }
+ 
+ static int kvm_prepare_memory_region(struct kvm *kvm,
+ 				     const struct kvm_memory_slot *old,
+ 				     struct kvm_memory_slot *new,
+ 				     enum kvm_mr_change change)
+ {
+ 	int r;
+ 
+ 	/*
+ 	 * If dirty logging is disabled, nullify the bitmap; the old bitmap
+ 	 * will be freed on "commit".  If logging is enabled in both old and
+ 	 * new, reuse the existing bitmap.  If logging is enabled only in the
+ 	 * new and KVM isn't using a ring buffer, allocate and initialize a
+ 	 * new bitmap.
+ 	 */
+ 	if (!(new->flags & KVM_MEM_LOG_DIRTY_PAGES))
+ 		new->dirty_bitmap = NULL;
+ 	else if (old->dirty_bitmap)
+ 		new->dirty_bitmap = old->dirty_bitmap;
+ 	else if (!kvm->dirty_ring_size) {
+ 		r = kvm_alloc_dirty_bitmap(new);
+ 		if (r)
+ 			return r;
+ 
+ 		if (kvm_dirty_log_manual_protect_and_init_set(kvm))
+ 			bitmap_set(new->dirty_bitmap, 0, new->npages);
+ 	}
+ 
+ 	r = kvm_arch_prepare_memory_region(kvm, old, new, change);
+ 
+ 	/* Free the bitmap on failure if it was allocated above. */
+ 	if (r && new->dirty_bitmap && !old->dirty_bitmap)
+ 		kvm_destroy_dirty_bitmap(new);
+ 
+ 	return r;
+ }
+ 
+ static void kvm_commit_memory_region(struct kvm *kvm,
+ 				     struct kvm_memory_slot *old,
+ 				     const struct kvm_memory_slot *new,
+ 				     enum kvm_mr_change change)
+ {
+ 	/*
+ 	 * Update the total number of memslot pages before calling the arch
+ 	 * hook so that architectures can consume the result directly.
+ 	 */
+ 	if (change == KVM_MR_DELETE)
+ 		kvm->nr_memslot_pages -= old->npages;
+ 	else if (change == KVM_MR_CREATE)
+ 		kvm->nr_memslot_pages += new->npages;
+ 
+ 	kvm_arch_commit_memory_region(kvm, old, new, change);
+ 
+ 	switch (change) {
+ 	case KVM_MR_CREATE:
+ 		/* Nothing more to do. */
+ 		break;
+ 	case KVM_MR_DELETE:
+ 		/* Free the old memslot and all its metadata. */
+ 		kvm_free_memslot(kvm, old);
+ 		break;
+ 	case KVM_MR_MOVE:
+ 	case KVM_MR_FLAGS_ONLY:
+ 		/*
+ 		 * Free the dirty bitmap as needed; the below check encompasses
+ 		 * both the flags and whether a ring buffer is being used)
+ 		 */
+ 		if (old->dirty_bitmap && !new->dirty_bitmap)
+ 			kvm_destroy_dirty_bitmap(old);
+ 
+ 		/*
+ 		 * The final quirk.  Free the detached, old slot, but only its
+ 		 * memory, not any metadata.  Metadata, including arch specific
+ 		 * data, may be reused by @new.
+ 		 */
+ 		kfree(old);
+ 		break;
+ 	default:
+ 		BUG();
+ 	}
+ }
+ 
+ /*
+  * Activate @new, which must be installed in the inactive slots by the caller,
+  * by swapping the active slots and then propagating @new to @old once @old is
+  * unreachable and can be safely modified.
+  *
+  * With NULL @old this simply adds @new to @active (while swapping the sets).
+  * With NULL @new this simply removes @old from @active and frees it
+  * (while also swapping the sets).
+  */
+ static void kvm_activate_memslot(struct kvm *kvm,
+ 				 struct kvm_memory_slot *old,
+ 				 struct kvm_memory_slot *new)
+ {
+ 	int as_id = kvm_memslots_get_as_id(old, new);
+ 
+ 	kvm_swap_active_memslots(kvm, as_id);
+ 
+ 	/* Propagate the new memslot to the now inactive memslots. */
+ 	kvm_replace_memslot(kvm, old, new);
+ }
+ 
+ static void kvm_copy_memslot(struct kvm_memory_slot *dest,
+ 			     const struct kvm_memory_slot *src)
+ {
+ 	dest->base_gfn = src->base_gfn;
+ 	dest->npages = src->npages;
+ 	dest->dirty_bitmap = src->dirty_bitmap;
+ 	dest->arch = src->arch;
+ 	dest->userspace_addr = src->userspace_addr;
+ 	dest->flags = src->flags;
+ 	dest->id = src->id;
+ 	dest->as_id = src->as_id;
+ }
+ 
+ static void kvm_invalidate_memslot(struct kvm *kvm,
+ 				   struct kvm_memory_slot *old,
+ 				   struct kvm_memory_slot *working_slot)
+ {
+ 	/*
+ 	 * Mark the current slot INVALID.  As with all memslot modifications,
+ 	 * this must be done on an unreachable slot to avoid modifying the
+ 	 * current slot in the active tree.
+ 	 */
+ 	kvm_copy_memslot(working_slot, old);
+ 	working_slot->flags |= KVM_MEMSLOT_INVALID;
+ 	kvm_replace_memslot(kvm, old, working_slot);
+ 
+ 	/*
+ 	 * Activate the slot that is now marked INVALID, but don't propagate
+ 	 * the slot to the now inactive slots. The slot is either going to be
+ 	 * deleted or recreated as a new slot.
+ 	 */
+ 	kvm_swap_active_memslots(kvm, old->as_id);
+ 
+ 	/*
+ 	 * From this point no new shadow pages pointing to a deleted, or moved,
+ 	 * memslot will be created.  Validation of sp->gfn happens in:
+ 	 *	- gfn_to_hva (kvm_read_guest, gfn_to_pfn)
+ 	 *	- kvm_is_visible_gfn (mmu_check_root)
+ 	 */
+ 	kvm_arch_flush_shadow_memslot(kvm, working_slot);
+ 
+ 	/* Was released by kvm_swap_active_memslots, reacquire. */
+ 	mutex_lock(&kvm->slots_arch_lock);
+ 
+ 	/*
+ 	 * Copy the arch-specific field of the newly-installed slot back to the
+ 	 * old slot as the arch data could have changed between releasing
+ 	 * slots_arch_lock in install_new_memslots() and re-acquiring the lock
+ 	 * above.  Writers are required to retrieve memslots *after* acquiring
+ 	 * slots_arch_lock, thus the active slot's data is guaranteed to be fresh.
+ 	 */
+ 	old->arch = working_slot->arch;
+ }
+ 
+ static void kvm_create_memslot(struct kvm *kvm,
+ 			       const struct kvm_memory_slot *new,
+ 			       struct kvm_memory_slot *working)
+ {
+ 	/*
+ 	 * Add the new memslot to the inactive set as a copy of the
+ 	 * new memslot data provided by userspace.
+ 	 */
+ 	kvm_copy_memslot(working, new);
+ 	kvm_replace_memslot(kvm, NULL, working);
+ 	kvm_activate_memslot(kvm, NULL, working);
+ }
+ 
+ static void kvm_delete_memslot(struct kvm *kvm,
+ 			       struct kvm_memory_slot *old,
+ 			       struct kvm_memory_slot *invalid_slot)
+ {
+ 	/*
+ 	 * Remove the old memslot (in the inactive memslots) by passing NULL as
+ 	 * the "new" slot.
+ 	 */
+ 	kvm_replace_memslot(kvm, old, NULL);
+ 
+ 	/* And do the same for the invalid version in the active slot. */
+ 	kvm_activate_memslot(kvm, invalid_slot, NULL);
+ 
+ 	/* Free the invalid slot, the caller will clean up the old slot. */
+ 	kfree(invalid_slot);
+ }
+ 
+ static struct kvm_memory_slot *kvm_move_memslot(struct kvm *kvm,
+ 						struct kvm_memory_slot *old,
+ 						const struct kvm_memory_slot *new,
+ 						struct kvm_memory_slot *invalid_slot)
+ {
+ 	struct kvm_memslots *slots = kvm_get_inactive_memslots(kvm, old->as_id);
+ 
+ 	/*
+ 	 * The memslot's gfn is changing, remove it from the inactive tree, it
+ 	 * will be re-added with its updated gfn. Because its range is
+ 	 * changing, an in-place replace is not possible.
+ 	 */
+ 	kvm_erase_gfn_node(slots, old);
+ 
+ 	/*
+ 	 * The old slot is now fully disconnected, reuse its memory for the
+ 	 * persistent copy of "new".
+ 	 */
+ 	kvm_copy_memslot(old, new);
+ 
+ 	/* Re-add to the gfn tree with the updated gfn */
+ 	kvm_insert_gfn_node(slots, old);
+ 
+ 	/* Replace the current INVALID slot with the updated memslot. */
+ 	kvm_activate_memslot(kvm, invalid_slot, old);
+ 
+ 	/*
+ 	 * Clear the INVALID flag so that the invalid_slot is now a perfect
+ 	 * copy of the old slot.  Return it for cleanup in the caller.
+ 	 */
+ 	WARN_ON_ONCE(!(invalid_slot->flags & KVM_MEMSLOT_INVALID));
+ 	invalid_slot->flags &= ~KVM_MEMSLOT_INVALID;
+ 	return invalid_slot;
+ }
+ 
+ static void kvm_update_flags_memslot(struct kvm *kvm,
+ 				     struct kvm_memory_slot *old,
+ 				     const struct kvm_memory_slot *new,
+ 				     struct kvm_memory_slot *working_slot)
+ {
+ 	/*
+ 	 * Similar to the MOVE case, but the slot doesn't need to be zapped as
+ 	 * an intermediate step. Instead, the old memslot is simply replaced
+ 	 * with a new, updated copy in both memslot sets.
+ 	 */
+ 	kvm_copy_memslot(working_slot, new);
+ 	kvm_replace_memslot(kvm, old, working_slot);
+ 	kvm_activate_memslot(kvm, old, working_slot);
+ }
+ 
+ static int kvm_set_memslot(struct kvm *kvm,
+ 			   struct kvm_memory_slot *old,
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  			   struct kvm_memory_slot *new,
  			   enum kvm_mr_change change)
  {
@@@ -1460,57 -1788,31 +1994,75 @@@
  	}
  
  	/*
- 	 * Make a full copy of the old memslot, the pointer will become stale
- 	 * when the memslots are re-sorted by update_memslots(), and the old
- 	 * memslot needs to be referenced after calling update_memslots(), e.g.
- 	 * to free its resources and for arch specific behavior.  This needs to
- 	 * happen *after* (re)acquiring slots_arch_lock.
+ 	 * For DELETE and MOVE, the working slot is now active as the INVALID
+ 	 * version of the old slot.  MOVE is particularly special as it reuses
+ 	 * the old slot and returns a copy of the old slot (in working_slot).
+ 	 * For CREATE, there is no old slot.  For DELETE and FLAGS_ONLY, the
+ 	 * old slot is detached but otherwise preserved.
  	 */
++<<<<<<< HEAD
 +	slot = id_to_memslot(slots, new->id);
 +	if (slot) {
 +		old = *slot;
 +	} else {
 +		WARN_ON_ONCE(change != KVM_MR_CREATE);
 +		memset(&old, 0, sizeof(old));
 +		old.id = new->id;
 +		old.as_id = new->as_id;
 +	}
 +
 +	/* Copy the arch-specific data, again after (re)acquiring slots_arch_lock. */
 +	memcpy(&new->arch, &old.arch, sizeof(old.arch));
 +
 +	r = kvm_arch_prepare_memory_region(kvm, new, mem, change);
 +	if (r)
 +		goto out_slots;
 +
 +	update_memslots(slots, new, change);
 +	slots = install_new_memslots(kvm, new->as_id, slots);
 +
 +	/*
 +	 * Update the total number of memslot pages before calling the arch
 +	 * hook so that architectures can consume the result directly.
 +	 */
 +	if (change == KVM_MR_DELETE)
 +		kvm->nr_memslot_pages -= old.npages;
 +	else if (change == KVM_MR_CREATE)
 +		kvm->nr_memslot_pages += new->npages;
 +
 +	kvm_arch_commit_memory_region(kvm, mem, &old, new, change);
 +
 +	/* Free the old memslot's metadata.  Note, this is the full copy!!! */
 +	if (change == KVM_MR_DELETE)
 +		kvm_free_memslot(kvm, &old);
 +
 +	kvfree(slots);
 +	return 0;
 +
 +out_slots:
 +	if (change == KVM_MR_DELETE || change == KVM_MR_MOVE)
 +		slots = install_new_memslots(kvm, new->as_id, slots);
++=======
+ 	if (change == KVM_MR_CREATE)
+ 		kvm_create_memslot(kvm, new, working);
+ 	else if (change == KVM_MR_DELETE)
+ 		kvm_delete_memslot(kvm, old, working);
+ 	else if (change == KVM_MR_MOVE)
+ 		old = kvm_move_memslot(kvm, old, new, working);
+ 	else if (change == KVM_MR_FLAGS_ONLY)
+ 		kvm_update_flags_memslot(kvm, old, new, working);
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  	else
- 		mutex_unlock(&kvm->slots_arch_lock);
- 	kvfree(slots);
- 	return r;
+ 		BUG();
+ 
+ 	/*
+ 	 * No need to refresh new->arch, changes after dropping slots_arch_lock
+ 	 * will directly hit the final, active memsot.  Architectures are
+ 	 * responsible for knowing that new->arch may be stale.
+ 	 */
+ 	kvm_commit_memory_region(kvm, old, new, change);
+ 
+ 	return 0;
  }
  
  /*
@@@ -1571,7 -1873,7 +2123,11 @@@ int __kvm_set_memory_region(struct kvm 
  		new.id = id;
  		new.as_id = as_id;
  
++<<<<<<< HEAD
 +		return kvm_set_memslot(kvm, mem, &new, KVM_MR_DELETE);
++=======
+ 		return kvm_set_memslot(kvm, old, &new, KVM_MR_DELETE);
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  	}
  
  	new.as_id = as_id;
@@@ -1622,30 -1922,7 +2180,34 @@@
  		}
  	}
  
++<<<<<<< HEAD
 +	/* Allocate/free page dirty bitmap as needed */
 +	if (!(new.flags & KVM_MEM_LOG_DIRTY_PAGES))
 +		new.dirty_bitmap = NULL;
 +	else if (!new.dirty_bitmap && !kvm->dirty_ring_size) {
 +		r = kvm_alloc_dirty_bitmap(&new);
 +		if (r)
 +			return r;
 +
 +		if (kvm_dirty_log_manual_protect_and_init_set(kvm))
 +			bitmap_set(new.dirty_bitmap, 0, new.npages);
 +	}
 +
 +	r = kvm_set_memslot(kvm, mem, &new, change);
 +	if (r)
 +		goto out_bitmap;
 +
 +	if (old.dirty_bitmap && !new.dirty_bitmap)
 +		kvm_destroy_dirty_bitmap(&old);
 +	return 0;
 +
 +out_bitmap:
 +	if (new.dirty_bitmap && !old.dirty_bitmap)
 +		kvm_destroy_dirty_bitmap(&new);
 +	return r;
++=======
+ 	return kvm_set_memslot(kvm, old, &new, change);
++>>>>>>> a54d806688fe (KVM: Keep memslots in tree-based structures instead of array-based ones)
  }
  EXPORT_SYMBOL_GPL(__kvm_set_memory_region);
  
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index 60713ffdcad4..cd5e399b5b56 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -426,13 +426,13 @@ static void stage2_flush_vm(struct kvm *kvm)
 {
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot;
-	int idx;
+	int idx, bkt;
 
 	idx = srcu_read_lock(&kvm->srcu);
 	spin_lock(&kvm->mmu_lock);
 
 	slots = kvm_memslots(kvm);
-	kvm_for_each_memslot(memslot, slots)
+	kvm_for_each_memslot(memslot, bkt, slots)
 		stage2_flush_memslot(kvm, memslot);
 
 	spin_unlock(&kvm->mmu_lock);
@@ -962,14 +962,14 @@ void stage2_unmap_vm(struct kvm *kvm)
 {
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot;
-	int idx;
+	int idx, bkt;
 
 	idx = srcu_read_lock(&kvm->srcu);
 	mmap_read_lock(current->mm);
 	spin_lock(&kvm->mmu_lock);
 
 	slots = kvm_memslots(kvm);
-	kvm_for_each_memslot(memslot, slots)
+	kvm_for_each_memslot(memslot, bkt, slots)
 		stage2_unmap_memslot(kvm, memslot);
 
 	spin_unlock(&kvm->mmu_lock);
diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c
index d1c5e6a0588d..6b6111511eef 100644
--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c
+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c
@@ -746,11 +746,11 @@ void kvmppc_rmap_reset(struct kvm *kvm)
 {
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot;
-	int srcu_idx;
+	int srcu_idx, bkt;
 
 	srcu_idx = srcu_read_lock(&kvm->srcu);
 	slots = kvm_memslots(kvm);
-	kvm_for_each_memslot(memslot, slots) {
+	kvm_for_each_memslot(memslot, bkt, slots) {
 		/* Mutual exclusion with kvm_unmap_hva_range etc. */
 		spin_lock(&kvm->mmu_lock);
 		/*
diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c
index fbfb23772025..ad9d884a0798 100644
--- a/arch/powerpc/kvm/book3s_hv.c
+++ b/arch/powerpc/kvm/book3s_hv.c
@@ -5661,11 +5661,12 @@ static int kvmhv_svm_off(struct kvm *kvm)
 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
 		struct kvm_memory_slot *memslot;
 		struct kvm_memslots *slots = __kvm_memslots(kvm, i);
+		int bkt;
 
 		if (!slots)
 			continue;
 
-		kvm_for_each_memslot(memslot, slots) {
+		kvm_for_each_memslot(memslot, bkt, slots) {
 			kvmppc_uvmem_drop_pages(memslot, kvm, true);
 			uv_unregister_mem_slot(kvm->arch.lpid, memslot->id);
 		}
diff --git a/arch/powerpc/kvm/book3s_hv_nested.c b/arch/powerpc/kvm/book3s_hv_nested.c
index efe1fb3eae8a..50f668e05de8 100644
--- a/arch/powerpc/kvm/book3s_hv_nested.c
+++ b/arch/powerpc/kvm/book3s_hv_nested.c
@@ -721,7 +721,7 @@ void kvmhv_release_all_nested(struct kvm *kvm)
 	struct kvm_nested_guest *gp;
 	struct kvm_nested_guest *freelist = NULL;
 	struct kvm_memory_slot *memslot;
-	int srcu_idx;
+	int srcu_idx, bkt;
 
 	spin_lock(&kvm->mmu_lock);
 	for (i = 0; i <= kvm->arch.max_nested_lpid; i++) {
@@ -742,7 +742,7 @@ void kvmhv_release_all_nested(struct kvm *kvm)
 	}
 
 	srcu_idx = srcu_read_lock(&kvm->srcu);
-	kvm_for_each_memslot(memslot, kvm_memslots(kvm))
+	kvm_for_each_memslot(memslot, bkt, kvm_memslots(kvm))
 		kvmhv_free_memslot_nest_rmap(memslot);
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
 }
diff --git a/arch/powerpc/kvm/book3s_hv_uvmem.c b/arch/powerpc/kvm/book3s_hv_uvmem.c
index 34720b79588f..e371ee1644c1 100644
--- a/arch/powerpc/kvm/book3s_hv_uvmem.c
+++ b/arch/powerpc/kvm/book3s_hv_uvmem.c
@@ -458,7 +458,7 @@ unsigned long kvmppc_h_svm_init_start(struct kvm *kvm)
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot, *m;
 	int ret = H_SUCCESS;
-	int srcu_idx;
+	int srcu_idx, bkt;
 
 	kvm->arch.secure_guest = KVMPPC_SECURE_INIT_START;
 
@@ -477,7 +477,7 @@ unsigned long kvmppc_h_svm_init_start(struct kvm *kvm)
 
 	/* register the memslot */
 	slots = kvm_memslots(kvm);
-	kvm_for_each_memslot(memslot, slots) {
+	kvm_for_each_memslot(memslot, bkt, slots) {
 		ret = __kvmppc_uvmem_memslot_create(kvm, memslot);
 		if (ret)
 			break;
@@ -485,7 +485,7 @@ unsigned long kvmppc_h_svm_init_start(struct kvm *kvm)
 
 	if (ret) {
 		slots = kvm_memslots(kvm);
-		kvm_for_each_memslot(m, slots) {
+		kvm_for_each_memslot(m, bkt, slots) {
 			if (m == memslot)
 				break;
 			__kvmppc_uvmem_memslot_delete(kvm, memslot);
@@ -646,7 +646,7 @@ void kvmppc_uvmem_drop_pages(const struct kvm_memory_slot *slot,
 
 unsigned long kvmppc_h_svm_init_abort(struct kvm *kvm)
 {
-	int srcu_idx;
+	int srcu_idx, bkt;
 	struct kvm_memory_slot *memslot;
 
 	/*
@@ -661,7 +661,7 @@ unsigned long kvmppc_h_svm_init_abort(struct kvm *kvm)
 
 	srcu_idx = srcu_read_lock(&kvm->srcu);
 
-	kvm_for_each_memslot(memslot, kvm_memslots(kvm))
+	kvm_for_each_memslot(memslot, bkt, kvm_memslots(kvm))
 		kvmppc_uvmem_drop_pages(memslot, kvm, false);
 
 	srcu_read_unlock(&kvm->srcu, srcu_idx);
@@ -820,7 +820,7 @@ unsigned long kvmppc_h_svm_init_done(struct kvm *kvm)
 {
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot;
-	int srcu_idx;
+	int srcu_idx, bkt;
 	long ret = H_SUCCESS;
 
 	if (!(kvm->arch.secure_guest & KVMPPC_SECURE_INIT_START))
@@ -829,7 +829,7 @@ unsigned long kvmppc_h_svm_init_done(struct kvm *kvm)
 	/* migrate any unmoved normal pfn to device pfns*/
 	srcu_idx = srcu_read_lock(&kvm->srcu);
 	slots = kvm_memslots(kvm);
-	kvm_for_each_memslot(memslot, slots) {
+	kvm_for_each_memslot(memslot, bkt, slots) {
 		ret = kvmppc_uv_migrate_mem_slot(kvm, memslot);
 		if (ret) {
 			/*
diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c
index 45ca4f9acf2d..c29903ad10bf 100644
--- a/arch/s390/kvm/kvm-s390.c
+++ b/arch/s390/kvm/kvm-s390.c
@@ -1035,13 +1035,13 @@ static int kvm_s390_vm_start_migration(struct kvm *kvm)
 	struct kvm_memory_slot *ms;
 	struct kvm_memslots *slots;
 	unsigned long ram_pages = 0;
-	int slotnr;
+	int bkt;
 
 	/* migration mode already enabled */
 	if (kvm->arch.migration_mode)
 		return 0;
 	slots = kvm_memslots(kvm);
-	if (!slots || !slots->used_slots)
+	if (!slots || kvm_memslots_empty(slots))
 		return -EINVAL;
 
 	if (!kvm->arch.use_cmma) {
@@ -1049,8 +1049,7 @@ static int kvm_s390_vm_start_migration(struct kvm *kvm)
 		return 0;
 	}
 	/* mark all the pages in active slots as dirty */
-	for (slotnr = 0; slotnr < slots->used_slots; slotnr++) {
-		ms = slots->memslots + slotnr;
+	kvm_for_each_memslot(ms, bkt, slots) {
 		if (!ms->dirty_bitmap)
 			return -EINVAL;
 		/*
@@ -1974,22 +1973,21 @@ static unsigned long kvm_s390_next_dirty_cmma(struct kvm_memslots *slots,
 					      unsigned long cur_gfn)
 {
 	struct kvm_memory_slot *ms = gfn_to_memslot_approx(slots, cur_gfn);
-	int slotidx = ms - slots->memslots;
 	unsigned long ofs = cur_gfn - ms->base_gfn;
+	struct rb_node *mnode = &ms->gfn_node[slots->node_idx];
 
 	if (ms->base_gfn + ms->npages <= cur_gfn) {
-		slotidx--;
+		mnode = rb_next(mnode);
 		/* If we are above the highest slot, wrap around */
-		if (slotidx < 0)
-			slotidx = slots->used_slots - 1;
+		if (!mnode)
+			mnode = rb_first(&slots->gfn_tree);
 
-		ms = slots->memslots + slotidx;
+		ms = container_of(mnode, struct kvm_memory_slot, gfn_node[slots->node_idx]);
 		ofs = 0;
 	}
 	ofs = find_next_bit(kvm_second_dirty_bitmap(ms), ms->npages, ofs);
-	while ((slotidx > 0) && (ofs >= ms->npages)) {
-		slotidx--;
-		ms = slots->memslots + slotidx;
+	while (ofs >= ms->npages && (mnode = rb_next(mnode))) {
+		ms = container_of(mnode, struct kvm_memory_slot, gfn_node[slots->node_idx]);
 		ofs = find_next_bit(kvm_second_dirty_bitmap(ms), ms->npages, 0);
 	}
 	return ms->base_gfn + ofs;
@@ -2002,7 +2000,7 @@ static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,
 	struct kvm_memslots *slots = kvm_memslots(kvm);
 	struct kvm_memory_slot *ms;
 
-	if (unlikely(!slots->used_slots))
+	if (unlikely(kvm_memslots_empty(slots)))
 		return 0;
 
 	cur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);
diff --git a/arch/s390/kvm/kvm-s390.h b/arch/s390/kvm/kvm-s390.h
index d2bb3e5db9d2..095b605882e0 100644
--- a/arch/s390/kvm/kvm-s390.h
+++ b/arch/s390/kvm/kvm-s390.h
@@ -220,12 +220,14 @@ static inline void kvm_s390_set_user_cpu_state_ctrl(struct kvm *kvm)
 /* get the end gfn of the last (highest gfn) memslot */
 static inline unsigned long kvm_s390_get_gfn_end(struct kvm_memslots *slots)
 {
+	struct rb_node *node;
 	struct kvm_memory_slot *ms;
 
-	if (WARN_ON(!slots->used_slots))
+	if (WARN_ON(kvm_memslots_empty(slots)))
 		return 0;
 
-	ms = slots->memslots;
+	node = rb_last(&slots->gfn_tree);
+	ms = container_of(node, struct kvm_memory_slot, gfn_node[slots->node_idx]);
 	return ms->base_gfn + ms->npages;
 }
 
* Unmerged path arch/x86/kvm/debugfs.c
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path include/linux/kvm_host.h
* Unmerged path virt/kvm/kvm_main.c
