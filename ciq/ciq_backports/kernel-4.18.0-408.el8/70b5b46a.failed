mm, memory_hotplug: reorganize new pgdat initialization

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Michal Hocko <mhocko@suse.com>
commit 70b5b46a754245d383811b4d2f2c76c34bb7e145
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/70b5b46a.failed

When a !node_online node is brought up it needs a hotplug specific
initialization because the node could be either uninitialized yet or it
could have been recycled after previous hotremove.  hotadd_init_pgdat is
responsible for that.

Internal pgdat state is initialized at two places currently
	- hotadd_init_pgdat
	- free_area_init_core_hotplug

There is no real clear cut what should go where but this patch's chosen to
move the whole internal state initialization into
free_area_init_core_hotplug.  hotadd_init_pgdat is still responsible to
pull all the parts together - most notably to initialize zonelists because
those depend on the overall topology.

This patch doesn't introduce any functional change.

Link: https://lkml.kernel.org/r/20220127085305.20890-5-mhocko@kernel.org
	Signed-off-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Rafael Aquini <raquini@redhat.com>
	Acked-by: David Hildenbrand <david@redhat.com>
	Reviewed-by: Oscar Salvador <osalvador@suse.de>
	Cc: Alexey Makhalov <amakhalov@vmware.com>
	Cc: Christoph Lameter <cl@linux.com>
	Cc: Dennis Zhou <dennis@kernel.org>
	Cc: Eric Dumazet <eric.dumazet@gmail.com>
	Cc: Mike Rapoport <rppt@linux.ibm.com>
	Cc: Nico Pache <npache@redhat.com>
	Cc: Tejun Heo <tj@kernel.org>
	Cc: Wei Yang <richard.weiyang@gmail.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 70b5b46a754245d383811b4d2f2c76c34bb7e145)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memory_hotplug.h
#	mm/memory_hotplug.c
diff --cc include/linux/memory_hotplug.h
index d16c9fe0f6df,76bf2de86def..000000000000
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@@ -320,15 -312,21 +320,27 @@@ static inline int remove_memory(int nid
  	return -EBUSY;
  }
  
 -static inline void __remove_memory(u64 start, u64 size) {}
 +static inline void __remove_memory(int nid, u64 start, u64 size) {}
  #endif /* CONFIG_MEMORY_HOTREMOVE */
  
++<<<<<<< HEAD
 +extern void __ref free_area_init_core_hotplug(int nid);
 +extern int __add_memory(int nid, u64 start, u64 size);
 +extern int add_memory(int nid, u64 start, u64 size);
 +extern int add_memory_resource(int nid, struct resource *resource);
++=======
+ extern void set_zone_contiguous(struct zone *zone);
+ extern void clear_zone_contiguous(struct zone *zone);
+ 
+ #ifdef CONFIG_MEMORY_HOTPLUG
+ extern void __ref free_area_init_core_hotplug(struct pglist_data *pgdat);
+ extern int __add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags);
+ extern int add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags);
+ extern int add_memory_resource(int nid, struct resource *resource,
+ 			       mhp_t mhp_flags);
++>>>>>>> 70b5b46a7542 (mm, memory_hotplug: reorganize new pgdat initialization)
  extern int add_memory_driver_managed(int nid, u64 start, u64 size,
 -				     const char *resource_name,
 -				     mhp_t mhp_flags);
 +				     const char *resource_name);
  extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
  				   unsigned long nr_pages,
  				   struct vmem_altmap *altmap, int migratetype);
diff --cc mm/memory_hotplug.c
index 27b7636cc032,a4f69d399929..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -956,39 -1166,16 +956,49 @@@ static pg_data_t __ref *hotadd_new_pgda
  {
  	struct pglist_data *pgdat;
  
++<<<<<<< HEAD
 +	pgdat = NODE_DATA(nid);
 +	if (!pgdat) {
 +		pgdat = arch_alloc_nodedata(nid);
 +		if (!pgdat)
 +			return NULL;
 +
 +		pgdat->per_cpu_nodestats =
 +			alloc_percpu(struct per_cpu_nodestat);
 +		arch_refresh_nodedata(nid, pgdat);
 +	} else {
 +		int cpu;
 +		/*
 +		 * Reset the nr_zones, order and highest_zoneidx before reuse.
 +		 * Note that kswapd will init kswapd_highest_zoneidx properly
 +		 * when it starts in the near future.
 +		 */
 +		pgdat->nr_zones = 0;
 +		pgdat->kswapd_order = 0;
 +		pgdat->kswapd_highest_zoneidx = 0;
 +		for_each_online_cpu(cpu) {
 +			struct per_cpu_nodestat *p;
 +
 +			p = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu);
 +			memset(p, 0, sizeof(*p));
 +		}
 +	}
 +
 +	/* we can use NODE_DATA(nid) from here */
 +	pgdat->node_id = nid;
 +	pgdat->node_start_pfn = 0;
++=======
+ 	/*
+ 	 * NODE_DATA is preallocated (free_area_init) but its internal
+ 	 * state is not allocated completely. Add missing pieces.
+ 	 * Completely offline nodes stay around and they just need
+ 	 * reintialization.
+ 	 */
+ 	pgdat = NODE_DATA(nid);
++>>>>>>> 70b5b46a7542 (mm, memory_hotplug: reorganize new pgdat initialization)
  
  	/* init node's zones as empty zones, we don't have any present pages.*/
- 	free_area_init_core_hotplug(nid);
+ 	free_area_init_core_hotplug(pgdat);
  
  	/*
  	 * The node we allocated has no zone fallback lists. For avoiding
* Unmerged path include/linux/memory_hotplug.h
* Unmerged path mm/memory_hotplug.c
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 4fb2330e1382..d03648b9370b 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -6880,12 +6880,33 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,
  * NOTE: this function is only called during memory hotplug
  */
 #ifdef CONFIG_MEMORY_HOTPLUG
-void __ref free_area_init_core_hotplug(int nid)
+void __ref free_area_init_core_hotplug(struct pglist_data *pgdat)
 {
+	int nid = pgdat->node_id;
 	enum zone_type z;
-	pg_data_t *pgdat = NODE_DATA(nid);
+	int cpu;
 
 	pgdat_init_internals(pgdat);
+
+	if (pgdat->per_cpu_nodestats == &boot_nodestats)
+		pgdat->per_cpu_nodestats = alloc_percpu(struct per_cpu_nodestat);
+
+	/*
+	 * Reset the nr_zones, order and highest_zoneidx before reuse.
+	 * Note that kswapd will init kswapd_highest_zoneidx properly
+	 * when it starts in the near future.
+	 */
+	pgdat->nr_zones = 0;
+	pgdat->kswapd_order = 0;
+	pgdat->kswapd_highest_zoneidx = 0;
+	pgdat->node_start_pfn = 0;
+	for_each_online_cpu(cpu) {
+		struct per_cpu_nodestat *p;
+
+		p = per_cpu_ptr(pgdat->per_cpu_nodestats, cpu);
+		memset(p, 0, sizeof(*p));
+	}
+
 	for (z = 0; z < MAX_NR_ZONES; z++)
 		zone_init_internals(&pgdat->node_zones[z], z, nid, 0);
 }
