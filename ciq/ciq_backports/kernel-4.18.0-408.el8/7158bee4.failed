KVM: MMU: pass kvm_mmu_page struct to make_spte

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 7158bee4b47519430f3ccad7cffea578533f364e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/7158bee4.failed

The level and A/D bit support of the new SPTE can be found in the role,
which is stored in the kvm_mmu_page struct.  This merges two arguments
into one.

For the TDP MMU, the kvm_mmu_page was not used (kvm_tdp_mmu_map does
not use it if the SPTE is already present) so we fetch it just before
calling make_spte.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 7158bee4b47519430f3ccad7cffea578533f364e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/paging_tmpl.h
#	arch/x86/kvm/mmu/spte.c
#	arch/x86/kvm/mmu/spte.h
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/mmu.c
index 8ec545b124be,c208f001c302..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -2716,9 -2716,17 +2716,23 @@@ static int mmu_set_spte(struct kvm_vcp
  			was_rmapped = 1;
  	}
  
++<<<<<<< HEAD
 +	set_spte_ret = set_spte(vcpu, sptep, pte_access, level, gfn, pfn,
 +				speculative, true, host_writable);
 +	if (set_spte_ret & SET_SPTE_WRITE_PROTECTED_PT) {
++=======
+ 	wrprot = make_spte(vcpu, sp, pte_access, gfn, pfn, *sptep, speculative,
+ 			   true, host_writable, &spte);
+ 
+ 	if (*sptep == spte) {
+ 		ret = RET_PF_SPURIOUS;
+ 	} else {
+ 		trace_kvm_mmu_set_spte(level, gfn, sptep);
+ 		flush |= mmu_spte_update(sptep, spte);
+ 	}
+ 
+ 	if (wrprot) {
++>>>>>>> 7158bee4b475 (KVM: MMU: pass kvm_mmu_page struct to make_spte)
  		if (write_fault)
  			ret = RET_PF_EMULATE;
  	}
diff --cc arch/x86/kvm/mmu/paging_tmpl.h
index de3ee26beb48,fbbaa3f5fb4e..000000000000
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@@ -1133,15 -1125,17 +1133,24 @@@ static int FNAME(sync_page)(struct kvm_
  			continue;
  		}
  
++<<<<<<< HEAD
 +		host_writable = sp->spt[i] & shadow_host_writable_mask;
++=======
+ 		sptep = &sp->spt[i];
+ 		spte = *sptep;
+ 		host_writable = spte & shadow_host_writable_mask;
+ 		make_spte(vcpu, sp, pte_access, gfn,
+ 			  spte_to_pfn(spte), spte, true, false,
+ 			  host_writable, &spte);
++>>>>>>> 7158bee4b475 (KVM: MMU: pass kvm_mmu_page struct to make_spte)
  
 -		flush |= mmu_spte_update(sptep, spte);
 +		set_spte_ret |= set_spte(vcpu, &sp->spt[i],
 +					 pte_access, PG_LEVEL_4K,
 +					 gfn, spte_to_pfn(sp->spt[i]),
 +					 true, false, host_writable);
  	}
  
 -	return flush;
 +	return set_spte_ret & SET_SPTE_NEED_REMOTE_TLB_FLUSH;
  }
  
  #undef pt_element_t
diff --cc arch/x86/kvm/mmu/spte.c
index 86a21eb85d25,2c5c14fbfbe9..000000000000
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@@ -89,15 -89,16 +89,23 @@@ static bool kvm_is_mmio_pfn(kvm_pfn_t p
  				     E820_TYPE_RAM);
  }
  
++<<<<<<< HEAD
 +int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 +		     gfn_t gfn, kvm_pfn_t pfn, u64 old_spte, bool speculative,
 +		     bool can_unsync, bool host_writable, bool ad_disabled,
 +		     u64 *new_spte)
++=======
+ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
+ 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
+ 	       u64 old_spte, bool speculative, bool can_unsync,
+ 	       bool host_writable, u64 *new_spte)
++>>>>>>> 7158bee4b475 (KVM: MMU: pass kvm_mmu_page struct to make_spte)
  {
+ 	int level = sp->role.level;
  	u64 spte = SPTE_MMU_PRESENT_MASK;
 -	bool wrprot = false;
 +	int ret = 0;
  
- 	if (ad_disabled)
+ 	if (sp->role.ad_disabled)
  		spte |= SPTE_TDP_AD_DISABLED_MASK;
  	else if (kvm_vcpu_ad_need_write_protect(vcpu))
  		spte |= SPTE_TDP_AD_WRPROT_ONLY_MASK;
diff --cc arch/x86/kvm/mmu/spte.h
index eb7b227fc6cf,cbb02a961ac2..000000000000
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@@ -334,15 -334,10 +334,22 @@@ static inline u64 get_mmio_spte_generat
  	return gen;
  }
  
++<<<<<<< HEAD
 +/* Bits which may be returned by set_spte() */
 +#define SET_SPTE_WRITE_PROTECTED_PT    BIT(0)
 +#define SET_SPTE_NEED_REMOTE_TLB_FLUSH BIT(1)
 +#define SET_SPTE_SPURIOUS              BIT(2)
 +
 +int make_spte(struct kvm_vcpu *vcpu, unsigned int pte_access, int level,
 +		     gfn_t gfn, kvm_pfn_t pfn, u64 old_spte, bool speculative,
 +		     bool can_unsync, bool host_writable, bool ad_disabled,
 +		     u64 *new_spte);
++=======
+ bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
+ 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
+ 	       u64 old_spte, bool speculative, bool can_unsync,
+ 	       bool host_writable, u64 *new_spte);
++>>>>>>> 7158bee4b475 (KVM: MMU: pass kvm_mmu_page struct to make_spte)
  u64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled);
  u64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access);
  u64 mark_spte_for_access_track(u64 spte);
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 8e1489964d2a,6dbf28924bc2..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -863,22 -893,22 +863,33 @@@ void kvm_tdp_mmu_invalidate_all_roots(s
   * Installs a last-level SPTE to handle a TDP page fault.
   * (NPT/EPT violation/misconfiguration)
   */
 -static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 -					  struct kvm_page_fault *fault,
 -					  struct tdp_iter *iter)
 +static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu, int write,
 +					  int map_writable,
 +					  struct tdp_iter *iter,
 +					  kvm_pfn_t pfn, bool prefault)
  {
+ 	struct kvm_mmu_page *sp = sptep_to_sp(iter->sptep);
  	u64 new_spte;
  	int ret = RET_PF_FIXED;
 -	bool wrprot = false;
 +	int make_spte_ret = 0;
  
++<<<<<<< HEAD
 +	if (unlikely(is_noslot_pfn(pfn)))
 +		new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
 +	else
 +		make_spte_ret = make_spte(vcpu, ACC_ALL, iter->level, iter->gfn,
 +					 pfn, iter->old_spte, prefault, true,
 +					 map_writable, !shadow_accessed_mask,
 +					 &new_spte);
++=======
+ 	WARN_ON(sp->role.level != fault->goal_level);
+ 	if (unlikely(!fault->slot))
+ 		new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
+ 	else
+ 		wrprot = make_spte(vcpu, sp, ACC_ALL, iter->gfn,
+ 					 fault->pfn, iter->old_spte, fault->prefault, true,
+ 					 fault->map_writable, &new_spte);
++>>>>>>> 7158bee4b475 (KVM: MMU: pass kvm_mmu_page struct to make_spte)
  
  	if (new_spte == iter->old_spte)
  		ret = RET_PF_SPURIOUS;
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/paging_tmpl.h
* Unmerged path arch/x86/kvm/mmu/spte.c
* Unmerged path arch/x86/kvm/mmu/spte.h
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
