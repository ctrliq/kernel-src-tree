KVM: Use interval tree to do fast hva lookup in memslots

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Maciej S. Szmigiero <maciej.szmigiero@oracle.com>
commit ed922739c9199bf515a3e7fec3e319ce1edeef2a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/ed922739.failed

The current memslots implementation only allows quick binary search by gfn,
quick lookup by hva is not possible - the implementation has to do a linear
scan of the whole memslots array, even though the operation being performed
might apply just to a single memslot.

This significantly hurts performance of per-hva operations with higher
memslot counts.

Since hva ranges can overlap between memslots an interval tree is needed
for tracking them.

[sean: handle interval tree updates in kvm_replace_memslot()]
	Signed-off-by: Maciej S. Szmigiero <maciej.szmigiero@oracle.com>
Message-Id: <d66b9974becaa9839be9c4e1a5de97b177b4ac20.1638817640.git.maciej.szmigiero@oracle.com>
(cherry picked from commit ed922739c9199bf515a3e7fec3e319ce1edeef2a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kvm/Kconfig
#	arch/mips/kvm/Kconfig
#	arch/s390/kvm/Kconfig
#	include/linux/kvm_host.h
#	virt/kvm/kvm_main.c
diff --cc arch/arm64/kvm/Kconfig
index 7ba5fb770b11,f1f8fc069a97..000000000000
--- a/arch/arm64/kvm/Kconfig
+++ b/arch/arm64/kvm/Kconfig
@@@ -38,9 -38,9 +38,15 @@@ menuconfig KV
  	select IRQ_BYPASS_MANAGER
  	select HAVE_KVM_IRQ_BYPASS
  	select HAVE_KVM_VCPU_RUN_PID_CHANGE
++<<<<<<< HEAD
 +	select TASKSTATS
 +	select TASK_DELAY_ACCT
 +	---help---
++=======
+ 	select SCHED_INFO
+ 	select INTERVAL_TREE
+ 	help
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  	  Support hosting virtualized guest machines.
  
  	  If unsure, say N.
diff --cc arch/mips/kvm/Kconfig
index 12b8487e3090,91d197bee9c0..000000000000
--- a/arch/mips/kvm/Kconfig
+++ b/arch/mips/kvm/Kconfig
@@@ -25,42 -27,10 +25,47 @@@ config KV
  	select KVM_MMIO
  	select MMU_NOTIFIER
  	select SRCU
++<<<<<<< HEAD
 +	---help---
++=======
+ 	select INTERVAL_TREE
+ 	help
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  	  Support for hosting Guest kernels.
  
 +choice
 +	prompt "Virtualization mode"
 +	depends on KVM
 +	default KVM_MIPS_TE
 +
 +config KVM_MIPS_TE
 +	bool "Trap & Emulate"
 +	---help---
 +	  Use trap and emulate to virtualize 32-bit guests in user mode. This
 +	  does not require any special hardware Virtualization support beyond
 +	  standard MIPS32/64 r2 or later, but it does require the guest kernel
 +	  to be configured with CONFIG_KVM_GUEST=y so that it resides in the
 +	  user address segment.
 +
 +config KVM_MIPS_VZ
 +	bool "MIPS Virtualization (VZ) ASE"
 +	---help---
 +	  Use the MIPS Virtualization (VZ) ASE to virtualize guests. This
 +	  supports running unmodified guest kernels (with CONFIG_KVM_GUEST=n),
 +	  but requires hardware support.
 +
 +endchoice
 +
 +config KVM_MIPS_DYN_TRANS
 +	bool "KVM/MIPS: Dynamic binary translation to reduce traps"
 +	depends on KVM_MIPS_TE
 +	default y
 +	---help---
 +	  When running in Trap & Emulate mode patch privileged
 +	  instructions to reduce the number of traps.
 +
 +	  If unsure, say Y.
 +
  config KVM_MIPS_DEBUG_COP0_COUNTERS
  	bool "Maintain counters for COP0 accesses"
  	depends on KVM
diff --cc arch/s390/kvm/Kconfig
index def3b60f1fe8,2e84d3922f7c..000000000000
--- a/arch/s390/kvm/Kconfig
+++ b/arch/s390/kvm/Kconfig
@@@ -33,7 -33,8 +33,12 @@@ config KV
  	select HAVE_KVM_NO_POLL
  	select SRCU
  	select KVM_VFIO
++<<<<<<< HEAD
 +	---help---
++=======
+ 	select INTERVAL_TREE
+ 	help
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  	  Support hosting paravirtualized guest machines using the SIE
  	  virtualization capability on the mainframe. This should work
  	  on any 64bit machine.
diff --cc include/linux/kvm_host.h
index 5a84e8a20ec4,9552ad6d6652..000000000000
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@@ -32,6 -29,9 +32,12 @@@
  #include <linux/refcount.h>
  #include <linux/nospec.h>
  #include <linux/notifier.h>
++<<<<<<< HEAD
++=======
+ #include <linux/hashtable.h>
+ #include <linux/interval_tree.h>
+ #include <linux/xarray.h>
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  #include <asm/signal.h>
  
  #include <linux/kvm.h>
@@@ -377,6 -428,8 +383,11 @@@ static inline int kvm_vcpu_exiting_gues
  #define KVM_MEM_MAX_NR_PAGES ((1UL << 31) - 1)
  
  struct kvm_memory_slot {
++<<<<<<< HEAD
++=======
+ 	struct hlist_node id_node;
+ 	struct interval_tree_node hva_node;
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  	gfn_t base_gfn;
  	unsigned long npages;
  	unsigned long *dirty_bitmap;
@@@ -478,8 -531,16 +489,21 @@@ static inline int kvm_arch_vcpu_memslot
   */
  struct kvm_memslots {
  	u64 generation;
++<<<<<<< HEAD
 +	/* The mapping table from slot id to the index in memslots[]. */
 +	short id_to_index[KVM_MEM_SLOTS_NUM];
++=======
+ 	struct rb_root_cached hva_tree;
+ 	/*
+ 	 * The mapping table from slot id to the index in memslots[].
+ 	 *
+ 	 * 7-bit bucket count matches the size of the old id to index array for
+ 	 * 512 slots, while giving good performance with this slot count.
+ 	 * Higher bucket counts bring only small performance improvements but
+ 	 * always result in higher memory usage (even for lower memslot counts).
+ 	 */
+ 	DECLARE_HASHTABLE(id_hash, 7);
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  	atomic_t last_used_slot;
  	int used_slots;
  	struct kvm_memory_slot memslots[];
diff --cc virt/kvm/kvm_main.c
index b1373f69ce5e,6ba7468bdbe3..000000000000
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@@ -490,6 -484,151 +490,154 @@@ static void kvm_mmu_notifier_invalidate
  	srcu_read_unlock(&kvm->srcu, idx);
  }
  
++<<<<<<< HEAD
++=======
+ typedef bool (*hva_handler_t)(struct kvm *kvm, struct kvm_gfn_range *range);
+ 
+ typedef void (*on_lock_fn_t)(struct kvm *kvm, unsigned long start,
+ 			     unsigned long end);
+ 
+ struct kvm_hva_range {
+ 	unsigned long start;
+ 	unsigned long end;
+ 	pte_t pte;
+ 	hva_handler_t handler;
+ 	on_lock_fn_t on_lock;
+ 	bool flush_on_ret;
+ 	bool may_block;
+ };
+ 
+ /*
+  * Use a dedicated stub instead of NULL to indicate that there is no callback
+  * function/handler.  The compiler technically can't guarantee that a real
+  * function will have a non-zero address, and so it will generate code to
+  * check for !NULL, whereas comparing against a stub will be elided at compile
+  * time (unless the compiler is getting long in the tooth, e.g. gcc 4.9).
+  */
+ static void kvm_null_fn(void)
+ {
+ 
+ }
+ #define IS_KVM_NULL_FN(fn) ((fn) == (void *)kvm_null_fn)
+ 
+ /* Iterate over each memslot intersecting [start, last] (inclusive) range */
+ #define kvm_for_each_memslot_in_hva_range(node, slots, start, last)	     \
+ 	for (node = interval_tree_iter_first(&slots->hva_tree, start, last); \
+ 	     node;							     \
+ 	     node = interval_tree_iter_next(node, start, last))	     \
+ 
+ static __always_inline int __kvm_handle_hva_range(struct kvm *kvm,
+ 						  const struct kvm_hva_range *range)
+ {
+ 	bool ret = false, locked = false;
+ 	struct kvm_gfn_range gfn_range;
+ 	struct kvm_memory_slot *slot;
+ 	struct kvm_memslots *slots;
+ 	int i, idx;
+ 
+ 	if (WARN_ON_ONCE(range->end <= range->start))
+ 		return 0;
+ 
+ 	/* A null handler is allowed if and only if on_lock() is provided. */
+ 	if (WARN_ON_ONCE(IS_KVM_NULL_FN(range->on_lock) &&
+ 			 IS_KVM_NULL_FN(range->handler)))
+ 		return 0;
+ 
+ 	idx = srcu_read_lock(&kvm->srcu);
+ 
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		struct interval_tree_node *node;
+ 
+ 		slots = __kvm_memslots(kvm, i);
+ 		kvm_for_each_memslot_in_hva_range(node, slots,
+ 						  range->start, range->end - 1) {
+ 			unsigned long hva_start, hva_end;
+ 
+ 			slot = container_of(node, struct kvm_memory_slot, hva_node);
+ 			hva_start = max(range->start, slot->userspace_addr);
+ 			hva_end = min(range->end, slot->userspace_addr +
+ 						  (slot->npages << PAGE_SHIFT));
+ 
+ 			/*
+ 			 * To optimize for the likely case where the address
+ 			 * range is covered by zero or one memslots, don't
+ 			 * bother making these conditional (to avoid writes on
+ 			 * the second or later invocation of the handler).
+ 			 */
+ 			gfn_range.pte = range->pte;
+ 			gfn_range.may_block = range->may_block;
+ 
+ 			/*
+ 			 * {gfn(page) | page intersects with [hva_start, hva_end)} =
+ 			 * {gfn_start, gfn_start+1, ..., gfn_end-1}.
+ 			 */
+ 			gfn_range.start = hva_to_gfn_memslot(hva_start, slot);
+ 			gfn_range.end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, slot);
+ 			gfn_range.slot = slot;
+ 
+ 			if (!locked) {
+ 				locked = true;
+ 				KVM_MMU_LOCK(kvm);
+ 				if (!IS_KVM_NULL_FN(range->on_lock))
+ 					range->on_lock(kvm, range->start, range->end);
+ 				if (IS_KVM_NULL_FN(range->handler))
+ 					break;
+ 			}
+ 			ret |= range->handler(kvm, &gfn_range);
+ 		}
+ 	}
+ 
+ 	if (range->flush_on_ret && ret)
+ 		kvm_flush_remote_tlbs(kvm);
+ 
+ 	if (locked)
+ 		KVM_MMU_UNLOCK(kvm);
+ 
+ 	srcu_read_unlock(&kvm->srcu, idx);
+ 
+ 	/* The notifiers are averse to booleans. :-( */
+ 	return (int)ret;
+ }
+ 
+ static __always_inline int kvm_handle_hva_range(struct mmu_notifier *mn,
+ 						unsigned long start,
+ 						unsigned long end,
+ 						pte_t pte,
+ 						hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= pte,
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= true,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
+ 
+ static __always_inline int kvm_handle_hva_range_no_flush(struct mmu_notifier *mn,
+ 							 unsigned long start,
+ 							 unsigned long end,
+ 							 hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= __pte(0),
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= false,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
  					struct mm_struct *mm,
  					unsigned long address,
@@@ -741,8 -884,8 +889,13 @@@ static struct kvm_memslots *kvm_alloc_m
  	if (!slots)
  		return NULL;
  
++<<<<<<< HEAD
 +	for (i = 0; i < KVM_MEM_SLOTS_NUM; i++)
 +		slots->id_to_index[i] = -1;
++=======
+ 	slots->hva_tree = RB_ROOT_CACHED;
+ 	hash_init(slots->id_hash);
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  
  	return slots;
  }
@@@ -1127,6 -1284,42 +1280,45 @@@ static int kvm_alloc_dirty_bitmap(struc
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static void kvm_replace_memslot(struct kvm_memslots *slots,
+ 				struct kvm_memory_slot *old,
+ 				struct kvm_memory_slot *new)
+ {
+ 	/*
+ 	 * Remove the old memslot from the hash list and interval tree, copying
+ 	 * the node data would corrupt the structures.
+ 	 */
+ 	if (old) {
+ 		hash_del(&old->id_node);
+ 		interval_tree_remove(&old->hva_node, &slots->hva_tree);
+ 
+ 		if (!new)
+ 			return;
+ 
+ 		/* Copy the source *data*, not the pointer, to the destination. */
+ 		*new = *old;
+ 	} else {
+ 		/* If @old is NULL, initialize @new's hva range. */
+ 		new->hva_node.start = new->userspace_addr;
+ 		new->hva_node.last = new->userspace_addr +
+ 			(new->npages << PAGE_SHIFT) - 1;
+ 	}
+ 
+ 	/* (Re)Add the new memslot. */
+ 	hash_add(slots->id_hash, &new->id_node, new->id);
+ 	interval_tree_insert(&new->hva_node, &slots->hva_tree);
+ }
+ 
+ static void kvm_shift_memslot(struct kvm_memslots *slots, int dst, int src)
+ {
+ 	struct kvm_memory_slot *mslots = slots->memslots;
+ 
+ 	kvm_replace_memslot(slots, &mslots[src], &mslots[dst]);
+ }
+ 
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  /*
   * Delete a memslot by decrementing the number of used slots and shifting all
   * other entries in the array forward one spot.
@@@ -1145,12 -1340,17 +1337,25 @@@ static inline void kvm_memslot_delete(s
  	if (atomic_read(&slots->last_used_slot) >= slots->used_slots)
  		atomic_set(&slots->last_used_slot, 0);
  
++<<<<<<< HEAD
 +	for (i = slots->id_to_index[memslot->id]; i < slots->used_slots; i++) {
 +		mslots[i] = mslots[i + 1];
 +		slots->id_to_index[mslots[i].id] = i;
 +	}
++=======
+ 	/*
+ 	 * Remove the to-be-deleted memslot from the list/tree _before_ shifting
+ 	 * the trailing memslots forward, its data will be overwritten.
+ 	 * Defer the (somewhat pointless) copying of the memslot until after
+ 	 * the last slot has been shifted to avoid overwriting said last slot.
+ 	 */
+ 	kvm_replace_memslot(slots, oldslot, NULL);
+ 
+ 	for (i = oldslot - mslots; i < slots->used_slots; i++)
+ 		kvm_shift_memslot(slots, i, i + 1);
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  	mslots[i] = *memslot;
 +	slots->id_to_index[memslot->id] = -1;
  }
  
  /*
@@@ -1168,6 -1368,9 +1373,12 @@@ static inline int kvm_memslot_insert_ba
   * itself is not preserved in the array, i.e. not swapped at this time, only
   * its new index into the array is tracked.  Returns the changed memslot's
   * current index into the memslots array.
++<<<<<<< HEAD
++=======
+  * The memslot at the returned index will not be in @slots->hva_tree or
+  * @slots->id_hash by then.
+  * @memslot is a detached struct with desired final data of the changed slot.
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
   */
  static inline int kvm_memslot_move_backward(struct kvm_memslots *slots,
  					    struct kvm_memory_slot *memslot)
@@@ -1179,6 -1383,14 +1390,17 @@@
  		return -1;
  
  	/*
++<<<<<<< HEAD
++=======
+ 	 * Delete the slot from the hash table and interval tree before sorting
+ 	 * the remaining slots, the slot's data may be overwritten when copying
+ 	 * slots as part of the sorting proccess.  update_memslots() will
+ 	 * unconditionally rewrite and re-add the entire slot.
+ 	 */
+ 	kvm_replace_memslot(slots, oldslot, NULL);
+ 
+ 	/*
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  	 * Move the target memslot backward in the array by shifting existing
  	 * memslots with a higher GFN (than the target memslot) towards the
  	 * front of the array.
@@@ -1202,6 -1412,12 +1424,15 @@@
   * is not preserved in the array, i.e. not swapped at this time, only its new
   * index into the array is tracked.  Returns the changed memslot's final index
   * into the memslots array.
++<<<<<<< HEAD
++=======
+  * The memslot at the returned index will not be in @slots->hva_tree or
+  * @slots->id_hash by then.
+  * @memslot is a detached struct with desired final data of the new or
+  * changed slot.
+  * Assumes that the memslot at @start index is not in @slots->hva_tree or
+  * @slots->id_hash.
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
   */
  static inline int kvm_memslot_move_forward(struct kvm_memslots *slots,
  					   struct kvm_memory_slot *memslot,
@@@ -1372,8 -1605,17 +1603,22 @@@ static struct kvm_memslots *kvm_dup_mem
  		new_size = kvm_memslots_size(old->used_slots);
  
  	slots = kvzalloc(new_size, GFP_KERNEL_ACCOUNT);
++<<<<<<< HEAD
 +	if (likely(slots))
 +		memcpy(slots, old, kvm_memslots_size(old->used_slots));
++=======
+ 	if (unlikely(!slots))
+ 		return NULL;
+ 
+ 	memcpy(slots, old, kvm_memslots_size(old->used_slots));
+ 
+ 	slots->hva_tree = RB_ROOT_CACHED;
+ 	hash_init(slots->id_hash);
+ 	kvm_for_each_memslot(memslot, slots) {
+ 		interval_tree_insert(&memslot->hva_node, &slots->hva_tree);
+ 		hash_add(slots->id_hash, &memslot->id_node, memslot->id);
+ 	}
++>>>>>>> ed922739c919 (KVM: Use interval tree to do fast hva lookup in memslots)
  
  	return slots;
  }
* Unmerged path arch/arm64/kvm/Kconfig
* Unmerged path arch/mips/kvm/Kconfig
diff --git a/arch/powerpc/kvm/Kconfig b/arch/powerpc/kvm/Kconfig
index b1cf015dfd17..362fc5dd6b52 100644
--- a/arch/powerpc/kvm/Kconfig
+++ b/arch/powerpc/kvm/Kconfig
@@ -26,6 +26,7 @@ config KVM
 	select KVM_VFIO
 	select IRQ_BYPASS_MANAGER
 	select HAVE_KVM_IRQ_BYPASS
+	select INTERVAL_TREE
 
 config KVM_BOOK3S_HANDLER
 	bool
* Unmerged path arch/s390/kvm/Kconfig
diff --git a/arch/x86/kvm/Kconfig b/arch/x86/kvm/Kconfig
index cd0b05b72440..412b63b697f4 100644
--- a/arch/x86/kvm/Kconfig
+++ b/arch/x86/kvm/Kconfig
@@ -45,6 +45,7 @@ config KVM
 	select KVM_GENERIC_DIRTYLOG_READ_PROTECT
 	select KVM_VFIO
 	select SRCU
+	select INTERVAL_TREE
 	select HAVE_KVM_PM_NOTIFIER if PM
 	---help---
 	  Support hosting fully virtualized guest machines using hardware
* Unmerged path include/linux/kvm_host.h
* Unmerged path virt/kvm/kvm_main.c
