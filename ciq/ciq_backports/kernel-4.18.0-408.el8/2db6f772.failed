KVM: x86/mmu: Allow zapping collapsible SPTEs to use MMU read lock

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Ben Gardon <bgardon@google.com>
commit 2db6f772b530eedcf69069e63dd7c4fdf05305fc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/2db6f772.failed

To reduce the impact of disabling dirty logging, change the TDP MMU
function which zaps collapsible SPTEs to run under the MMU read lock.
This way, page faults on zapped SPTEs can proceed in parallel with
kvm_mmu_zap_collapsible_sptes.

	Signed-off-by: Ben Gardon <bgardon@google.com>
Message-Id: <20210401233736.638171-11-bgardon@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 2db6f772b530eedcf69069e63dd7c4fdf05305fc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 83e4cca9867b,6a3b7ba4aa3c..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -1306,7 -1257,8 +1306,12 @@@ static bool zap_collapsible_spte_range(
  	rcu_read_lock();
  
  	tdp_root_for_each_pte(iter, root, start, end) {
++<<<<<<< HEAD
 +		if (tdp_mmu_iter_cond_resched(kvm, &iter, flush)) {
++=======
+ retry:
+ 		if (tdp_mmu_iter_cond_resched(kvm, &iter, flush, true)) {
++>>>>>>> 2db6f772b530 (KVM: x86/mmu: Allow zapping collapsible SPTEs to use MMU read lock)
  			flush = false;
  			continue;
  		}
@@@ -1341,7 -1299,9 +1352,13 @@@ bool kvm_tdp_mmu_zap_collapsible_sptes(
  {
  	struct kvm_mmu_page *root;
  
++<<<<<<< HEAD
 +	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id)
++=======
+ 	lockdep_assert_held_read(&kvm->mmu_lock);
+ 
+ 	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)
++>>>>>>> 2db6f772b530 (KVM: x86/mmu: Allow zapping collapsible SPTEs to use MMU read lock)
  		flush = zap_collapsible_spte_range(kvm, root, slot, flush);
  
  	return flush;
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 4401a1a9e22e..c4da43ccf41f 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -5796,13 +5796,19 @@ void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
 	write_lock(&kvm->mmu_lock);
 	flush = slot_handle_leaf(kvm, slot, kvm_mmu_zap_collapsible_spte, true);
 
-	if (is_tdp_mmu_enabled(kvm))
-		flush = kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot, flush);
-
 	if (flush)
 		kvm_arch_flush_remote_tlbs_memslot(kvm, slot);
-
 	write_unlock(&kvm->mmu_lock);
+
+	if (is_tdp_mmu_enabled(kvm)) {
+		flush = false;
+
+		read_lock(&kvm->mmu_lock);
+		flush = kvm_tdp_mmu_zap_collapsible_sptes(kvm, slot, flush);
+		if (flush)
+			kvm_arch_flush_remote_tlbs_memslot(kvm, slot);
+		read_unlock(&kvm->mmu_lock);
+	}
 }
 
 void kvm_arch_flush_remote_tlbs_memslot(struct kvm *kvm,
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
