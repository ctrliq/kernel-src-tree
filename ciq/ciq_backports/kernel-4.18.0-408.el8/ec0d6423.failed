dmaengine: idxd: embed irq_entry in idxd_wq struct

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit ec0d64231615e50539d83516b974e7947d45fbce
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/ec0d6423.failed

With irq_entry already being associated with the wq in a 1:1 relationship,
embed the irq_entry in the idxd_wq struct and remove back pointers for
idxe_wq and idxd_device. In the process of this work, clean up the interrupt
handle assignment so that there's no decision to be made during submit
call on where interrupt handle value comes from. Set the interrupt handle
during irq request initialization time.

irq_entry 0 is designated as special and is tied to the device itself.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/163942148362.2412839.12055447853311267866.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit ec0d64231615e50539d83516b974e7947d45fbce)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/device.c
#	drivers/dma/idxd/idxd.h
#	drivers/dma/idxd/init.c
#	drivers/dma/idxd/irq.c
#	drivers/dma/idxd/submit.c
#	drivers/dma/idxd/sysfs.c
diff --cc drivers/dma/idxd/device.c
index eadb0129f3b6,8233a29f859d..000000000000
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@@ -1114,3 -1157,203 +1120,206 @@@ int idxd_device_load_config(struct idxd
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ int __drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	int rc = -ENXIO;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_NOT_ENABLED;
+ 		goto err;
+ 	}
+ 
+ 	if (wq->state != IDXD_WQ_DISABLED) {
+ 		dev_dbg(dev, "wq %d already enabled.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_ENABLED;
+ 		rc = -EBUSY;
+ 		goto err;
+ 	}
+ 
+ 	if (!wq->group) {
+ 		dev_dbg(dev, "wq %d not attached to group.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_GRP;
+ 		goto err;
+ 	}
+ 
+ 	if (strlen(wq->name) == 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_NAME;
+ 		dev_dbg(dev, "wq %d name not set.\n", wq->id);
+ 		goto err;
+ 	}
+ 
+ 	/* Shared WQ checks */
+ 	if (wq_shared(wq)) {
+ 		if (!device_swq_supported(idxd)) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_SVM;
+ 			dev_dbg(dev, "PASID not enabled and shared wq.\n");
+ 			goto err;
+ 		}
+ 		/*
+ 		 * Shared wq with the threshold set to 0 means the user
+ 		 * did not set the threshold or transitioned from a
+ 		 * dedicated wq but did not set threshold. A value
+ 		 * of 0 would effectively disable the shared wq. The
+ 		 * driver does not allow a value of 0 to be set for
+ 		 * threshold via sysfs.
+ 		 */
+ 		if (wq->threshold == 0) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_THRESH;
+ 			dev_dbg(dev, "Shared wq and threshold 0.\n");
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	rc = 0;
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Writing wq %d config failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_enable(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "wq %d enabling failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_map_portal(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_PORTAL_ERR;
+ 		dev_dbg(dev, "wq %d portal mapping failed: %d\n", wq->id, rc);
+ 		goto err_map_portal;
+ 	}
+ 
+ 	wq->client_count = 0;
+ 	return 0;
+ 
+ err_map_portal:
+ 	rc = idxd_wq_disable(wq, false);
+ 	if (rc < 0)
+ 		dev_dbg(dev, "wq %s disable failed\n", dev_name(wq_confdev(wq)));
+ err:
+ 	return rc;
+ }
+ 
+ int drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	int rc;
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	rc = __drv_enable_wq(wq);
+ 	mutex_unlock(&wq->wq_lock);
+ 	return rc;
+ }
+ 
+ void __drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd_wq_refcount(wq))
+ 		dev_warn(dev, "Clients has claim on wq %d: %d\n",
+ 			 wq->id, idxd_wq_refcount(wq));
+ 
+ 	idxd_wq_unmap_portal(wq);
+ 
+ 	idxd_wq_drain(wq);
+ 	idxd_wq_reset(wq);
+ 
+ 	wq->client_count = 0;
+ }
+ 
+ void drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	mutex_lock(&wq->wq_lock);
+ 	__drv_disable_wq(wq);
+ 	mutex_unlock(&wq->wq_lock);
+ }
+ 
+ int idxd_device_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int rc = 0;
+ 
+ 	/*
+ 	 * Device should be in disabled state for the idxd_drv to load. If it's in
+ 	 * enabled state, then the device was altered outside of driver's control.
+ 	 * If the state is in halted state, then we don't want to proceed.
+ 	 */
+ 	if (idxd->state != IDXD_DEV_DISABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_ENABLED;
+ 		return -ENXIO;
+ 	}
+ 
+ 	/* Device configuration */
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0)
+ 		return -ENXIO;
+ 
+ 	/* Start device */
+ 	rc = idxd_device_enable(idxd);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	/* Setup DMA device without channels */
+ 	rc = idxd_register_dma_device(idxd);
+ 	if (rc < 0) {
+ 		idxd_device_disable(idxd);
+ 		idxd->cmd_status = IDXD_SCMD_DEV_DMA_ERR;
+ 		return rc;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	return 0;
+ }
+ 
+ void idxd_device_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int i;
+ 
+ 	for (i = 0; i < idxd->max_wqs; i++) {
+ 		struct idxd_wq *wq = idxd->wqs[i];
+ 		struct device *wq_dev = wq_confdev(wq);
+ 
+ 		if (wq->state == IDXD_WQ_DISABLED)
+ 			continue;
+ 		dev_warn(dev, "Active wq %d on disable %s.\n", i, dev_name(wq_dev));
+ 		device_release_driver(wq_dev);
+ 	}
+ 
+ 	idxd_unregister_dma_device(idxd);
+ 	idxd_device_disable(idxd);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		idxd_device_reset(idxd);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_DSA,
+ 	IDXD_DEV_IAX,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_drv = {
+ 	.type = dev_types,
+ 	.probe = idxd_device_drv_probe,
+ 	.remove = idxd_device_drv_remove,
+ 	.name = "idxd",
+ };
+ EXPORT_SYMBOL_GPL(idxd_drv);
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
diff --cc drivers/dma/idxd/idxd.h
index 03dc70e1f4d4,d77be03dd8b0..000000000000
--- a/drivers/dma/idxd/idxd.h
+++ b/drivers/dma/idxd/idxd.h
@@@ -40,8 -60,16 +40,7 @@@ struct idxd_device_driver 
  	struct device_driver drv;
  };
  
 -extern struct idxd_device_driver dsa_drv;
 -extern struct idxd_device_driver idxd_drv;
 -extern struct idxd_device_driver idxd_dmaengine_drv;
 -extern struct idxd_device_driver idxd_user_drv;
 -
 -#define INVALID_INT_HANDLE	-1
  struct idxd_irq_entry {
- 	struct idxd_device *idxd;
  	int id;
  	int vector;
  	struct llist_head pending_llist;
@@@ -51,6 -79,8 +50,11 @@@
  	 * and irq thread processing error descriptor.
  	 */
  	spinlock_t list_lock;
++<<<<<<< HEAD
++=======
+ 	int int_handle;
+ 	ioasid_t pasid;
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  };
  
  struct idxd_group {
@@@ -147,6 -183,7 +151,10 @@@ struct idxd_wq 
  	struct wait_queue_head err_queue;
  	struct idxd_device *idxd;
  	int id;
++<<<<<<< HEAD
++=======
+ 	struct idxd_irq_entry ie;
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  	enum idxd_wq_type type;
  	struct idxd_group *group;
  	int client_count;
@@@ -227,7 -263,8 +235,12 @@@ struct idxd_device 
  	unsigned long flags;
  	int id;
  	int major;
++<<<<<<< HEAD
 +	u8 cmd_status;
++=======
+ 	u32 cmd_status;
+ 	struct idxd_irq_entry ie;	/* misc irq, msix 0 */
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  
  	struct pci_dev *pdev;
  	void __iomem *reg_base;
@@@ -295,9 -329,84 +306,24 @@@ struct idxd_desc 
  	struct idxd_wq *wq;
  };
  
 -/*
 - * This is software defined error for the completion status. We overload the error code
 - * that will never appear in completion status and only SWERR register.
 - */
 -enum idxd_completion_status {
 -	IDXD_COMP_DESC_ABORT = 0xff,
 -};
 -
 -#define idxd_confdev(idxd) &idxd->idxd_dev.conf_dev
 -#define wq_confdev(wq) &wq->idxd_dev.conf_dev
 -#define engine_confdev(engine) &engine->idxd_dev.conf_dev
 -#define group_confdev(group) &group->idxd_dev.conf_dev
 -#define cdev_dev(cdev) &cdev->idxd_dev.conf_dev
 -
 -#define confdev_to_idxd_dev(dev) container_of(dev, struct idxd_dev, conf_dev)
 -#define idxd_dev_to_idxd(idxd_dev) container_of(idxd_dev, struct idxd_device, idxd_dev)
 -#define idxd_dev_to_wq(idxd_dev) container_of(idxd_dev, struct idxd_wq, idxd_dev)
 -
 -static inline struct idxd_device *confdev_to_idxd(struct device *dev)
 -{
 -	struct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);
 -
 -	return idxd_dev_to_idxd(idxd_dev);
 -}
 -
 -static inline struct idxd_wq *confdev_to_wq(struct device *dev)
 -{
 -	struct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);
 -
 -	return idxd_dev_to_wq(idxd_dev);
 -}
 -
 -static inline struct idxd_engine *confdev_to_engine(struct device *dev)
 -{
 -	struct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);
 -
 -	return container_of(idxd_dev, struct idxd_engine, idxd_dev);
 -}
 -
 -static inline struct idxd_group *confdev_to_group(struct device *dev)
 -{
 -	struct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);
 -
 -	return container_of(idxd_dev, struct idxd_group, idxd_dev);
 -}
 -
 -static inline struct idxd_cdev *dev_to_cdev(struct device *dev)
 -{
 -	struct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);
 -
 -	return container_of(idxd_dev, struct idxd_cdev, idxd_dev);
 -}
 -
 -static inline void idxd_dev_set_type(struct idxd_dev *idev, int type)
 -{
 -	if (type >= IDXD_DEV_MAX_TYPE) {
 -		idev->type = IDXD_DEV_NONE;
 -		return;
 -	}
 -
 -	idev->type = type;
 -}
 +#define confdev_to_idxd(dev) container_of(dev, struct idxd_device, conf_dev)
 +#define confdev_to_wq(dev) container_of(dev, struct idxd_wq, conf_dev)
  
+ static inline struct idxd_irq_entry *idxd_get_ie(struct idxd_device *idxd, int idx)
+ {
+ 	return (idx == 0) ? &idxd->ie : &idxd->wqs[idx - 1]->ie;
+ }
+ 
+ static inline struct idxd_wq *ie_to_wq(struct idxd_irq_entry *ie)
+ {
+ 	return container_of(ie, struct idxd_wq, ie);
+ }
+ 
+ static inline struct idxd_device *ie_to_idxd(struct idxd_irq_entry *ie)
+ {
+ 	return container_of(ie, struct idxd_device, ie);
+ }
+ 
  extern struct bus_type dsa_bus_type;
  
  extern bool support_enqcmd;
diff --cc drivers/dma/idxd/init.c
index b9299e45c20d,29c732a94027..000000000000
--- a/drivers/dma/idxd/init.c
+++ b/drivers/dma/idxd/init.c
@@@ -88,29 -90,11 +88,32 @@@ static int idxd_setup_interrupts(struc
  	}
  	dev_dbg(dev, "Enabled %d msix vectors\n", msixcnt);
  
++<<<<<<< HEAD
 +	/*
 +	 * We implement 1 completion list per MSI-X entry except for
 +	 * entry 0, which is for errors and others.
 +	 */
 +	idxd->irq_entries = kcalloc_node(msixcnt, sizeof(struct idxd_irq_entry),
 +					 GFP_KERNEL, dev_to_node(dev));
 +	if (!idxd->irq_entries) {
 +		rc = -ENOMEM;
 +		goto err_irq_entries;
 +	}
 +
 +	for (i = 0; i < msixcnt; i++) {
 +		idxd->irq_entries[i].id = i;
 +		idxd->irq_entries[i].idxd = idxd;
 +		idxd->irq_entries[i].vector = pci_irq_vector(pdev, i);
 +		spin_lock_init(&idxd->irq_entries[i].list_lock);
 +	}
 +
++=======
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  	idxd_msix_perm_setup(idxd);
  
- 	irq_entry = &idxd->irq_entries[0];
- 	rc = request_threaded_irq(irq_entry->vector, NULL, idxd_misc_thread,
- 				  0, "idxd-misc", irq_entry);
+ 	ie = idxd_get_ie(idxd, 0);
+ 	ie->vector = pci_irq_vector(pdev, 0);
+ 	rc = request_threaded_irq(ie->vector, NULL, idxd_misc_thread, 0, "idxd-misc", ie);
  	if (rc < 0) {
  		dev_err(dev, "Failed to allocate misc interrupt.\n");
  		goto err_misc_irq;
@@@ -133,24 -125,19 +144,37 @@@
  			goto err_wq_irqs;
  		}
  
++<<<<<<< HEAD
 +		dev_dbg(dev, "Allocated idxd-msix %d for vector %d\n", i, irq_entry->vector);
 +		if (idxd->hw.cmd_cap & BIT(IDXD_CMD_REQUEST_INT_HANDLE)) {
 +			/*
 +			 * The MSIX vector enumeration starts at 1 with vector 0 being the
 +			 * misc interrupt that handles non I/O completion events. The
 +			 * interrupt handles are for IMS enumeration on guest. The misc
 +			 * interrupt vector does not require a handle and therefore we start
 +			 * the int_handles at index 0. Since 'i' starts at 1, the first
 +			 * int_handles index will be 0.
 +			 */
 +			rc = idxd_device_request_int_handle(idxd, i, &idxd->int_handles[i - 1],
++=======
+ 		dev_dbg(dev, "Allocated idxd-msix %d for vector %d\n", i, ie->vector);
+ 		if (idxd->request_int_handles) {
+ 			rc = idxd_device_request_int_handle(idxd, i, &ie->int_handle,
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  							    IDXD_IRQ_MSIX);
  			if (rc < 0) {
- 				free_irq(irq_entry->vector, irq_entry);
+ 				free_irq(ie->vector, ie);
  				goto err_wq_irqs;
  			}
++<<<<<<< HEAD
 +			dev_dbg(dev, "int handle requested: %u\n", idxd->int_handles[i - 1]);
++=======
+ 			dev_dbg(dev, "int handle requested: %u\n", ie->int_handle);
+ 		} else {
+ 			ie->int_handle = msix_idx;
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  		}
+ 
  	}
  
  	idxd_unmask_error_interrupts(idxd);
@@@ -158,11 -145,14 +182,22 @@@
  
   err_wq_irqs:
  	while (--i >= 0) {
++<<<<<<< HEAD
 +		irq_entry = &idxd->irq_entries[i];
 +		free_irq(irq_entry->vector, irq_entry);
 +		if (i != 0)
 +			idxd_device_release_int_handle(idxd,
 +						       idxd->int_handles[i], IDXD_IRQ_MSIX);
++=======
+ 		ie = &idxd->wqs[i]->ie;
+ 		free_irq(ie->vector, ie);
+ 		if (ie->int_handle != INVALID_INT_HANDLE) {
+ 			idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
+ 			ie->int_handle = INVALID_INT_HANDLE;
+ 			ie->pasid = INVALID_IOASID;
+ 		}
+ 		ie->vector = -1;
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  	}
   err_misc_irq:
  	/* Disable error interrupt generation */
@@@ -177,23 -166,18 +211,38 @@@
  static void idxd_cleanup_interrupts(struct idxd_device *idxd)
  {
  	struct pci_dev *pdev = idxd->pdev;
++<<<<<<< HEAD
 +	struct idxd_irq_entry *irq_entry;
 +	int i, msixcnt;
 +
 +	msixcnt = pci_msix_vec_count(pdev);
 +	if (msixcnt <= 0)
 +		return;
 +
 +	irq_entry = &idxd->irq_entries[0];
 +	free_irq(irq_entry->vector, irq_entry);
 +
 +	for (i = 1; i < msixcnt; i++) {
 +
 +		irq_entry = &idxd->irq_entries[i];
 +		if (idxd->hw.cmd_cap & BIT(IDXD_CMD_RELEASE_INT_HANDLE))
 +			idxd_device_release_int_handle(idxd, idxd->int_handles[i],
 +						       IDXD_IRQ_MSIX);
 +		free_irq(irq_entry->vector, irq_entry);
++=======
+ 	struct idxd_irq_entry *ie;
+ 	int i;
+ 
+ 	for (i = 0; i < idxd->irq_cnt; i++) {
+ 		ie = idxd_get_ie(idxd, i);
+ 		if (ie->int_handle != INVALID_INT_HANDLE) {
+ 			idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
+ 			ie->int_handle = INVALID_INT_HANDLE;
+ 			ie->pasid = INVALID_IOASID;
+ 		}
+ 		free_irq(ie->vector, ie);
+ 		ie->vector = -1;
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  	}
  
  	idxd_mask_error_interrupts(idxd);
@@@ -730,15 -729,15 +779,23 @@@ static void idxd_release_int_handles(st
  	struct device *dev = &idxd->pdev->dev;
  	int i, rc;
  
++<<<<<<< HEAD
 +	for (i = 0; i < idxd->num_wq_irqs; i++) {
 +		if (idxd->hw.cmd_cap & BIT(IDXD_CMD_RELEASE_INT_HANDLE)) {
 +			rc = idxd_device_release_int_handle(idxd, idxd->int_handles[i],
 +							    IDXD_IRQ_MSIX);
++=======
+ 	for (i = 1; i < idxd->irq_cnt; i++) {
+ 		struct idxd_irq_entry *ie = idxd_get_ie(idxd, i);
+ 
+ 		if (ie->int_handle != INVALID_INT_HANDLE) {
+ 			rc = idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  			if (rc < 0)
 -				dev_warn(dev, "irq handle %d release failed\n", ie->int_handle);
 +				dev_warn(dev, "irq handle %d release failed\n",
 +					 idxd->int_handles[i]);
  			else
 -				dev_dbg(dev, "int handle released: %u\n", ie->int_handle);
 +				dev_dbg(dev, "int handle requested: %u\n", idxd->int_handles[i]);
  		}
  	}
  }
@@@ -780,10 -788,9 +837,10 @@@ static void idxd_remove(struct pci_dev 
  	idxd_shutdown(pdev);
  	if (device_pasid_enabled(idxd))
  		idxd_disable_system_pasid(idxd);
 +	idxd_unregister_devices(idxd);
  
  	for (i = 0; i < msixcnt; i++) {
- 		irq_entry = &idxd->irq_entries[i];
+ 		irq_entry = idxd_get_ie(idxd, i);
  		free_irq(irq_entry->vector, irq_entry);
  	}
  	idxd_msix_perm_clear(idxd);
diff --cc drivers/dma/idxd/irq.c
index 7a2cf0512501,a1316f341dd6..000000000000
--- a/drivers/dma/idxd/irq.c
+++ b/drivers/dma/idxd/irq.c
@@@ -62,44 -66,157 +62,155 @@@ static void idxd_device_reinit(struct w
  	idxd_device_clear_state(idxd);
  }
  
 -/*
 - * The function sends a drain descriptor for the interrupt handle. The drain ensures
 - * all descriptors with this interrupt handle is flushed and the interrupt
 - * will allow the cleanup of the outstanding descriptors.
 - */
 -static void idxd_int_handle_revoke_drain(struct idxd_irq_entry *ie)
 +static void idxd_device_fault_work(struct work_struct *work)
  {
++<<<<<<< HEAD
 +	struct idxd_fault *fault = container_of(work, struct idxd_fault, work);
 +	struct idxd_irq_entry *ie;
 +	int i;
 +	int processed;
 +	int irqcnt = fault->idxd->num_wq_irqs + 1;
++=======
+ 	struct idxd_wq *wq = ie_to_wq(ie);
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	struct dsa_hw_desc desc = {};
+ 	void __iomem *portal;
+ 	int rc;
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  
 -	/* Issue a simple drain operation with interrupt but no completion record */
 -	desc.flags = IDXD_OP_FLAG_RCI;
 -	desc.opcode = DSA_OPCODE_DRAIN;
 -	desc.priv = 1;
 -
 -	if (ie->pasid != INVALID_IOASID)
 -		desc.pasid = ie->pasid;
 -	desc.int_handle = ie->int_handle;
 -	portal = idxd_wq_portal_addr(wq);
 +	for (i = 1; i < irqcnt; i++) {
 +		ie = &fault->idxd->irq_entries[i];
 +		irq_process_work_list(ie, IRQ_WORK_PROCESS_FAULT,
 +				      &processed, fault->addr);
 +		if (processed)
 +			break;
  
 -	/*
 -	 * The wmb() makes sure that the descriptor is all there before we
 -	 * issue.
 -	 */
 -	wmb();
 -	if (wq_dedicated(wq)) {
 -		iosubmit_cmds512(portal, &desc, 1);
 -	} else {
 -		rc = idxd_enqcmds(wq, portal, &desc);
 -		/* This should not fail unless hardware failed. */
 -		if (rc < 0)
 -			dev_warn(dev, "Failed to submit drain desc on wq %d\n", wq->id);
 +		irq_process_pending_llist(ie, IRQ_WORK_PROCESS_FAULT,
 +					  &processed, fault->addr);
 +		if (processed)
 +			break;
  	}
 +
 +	kfree(fault);
  }
  
 -static void idxd_abort_invalid_int_handle_descs(struct idxd_irq_entry *ie)
 +static int idxd_device_schedule_fault_process(struct idxd_device *idxd,
 +					      u64 fault_addr)
  {
 -	LIST_HEAD(flist);
 -	struct idxd_desc *d, *t;
 -	struct llist_node *head;
 +	struct idxd_fault *fault;
  
 -	spin_lock(&ie->list_lock);
 -	head = llist_del_all(&ie->pending_llist);
 -	if (head) {
 -		llist_for_each_entry_safe(d, t, head, llnode)
 -			list_add_tail(&d->list, &ie->work_list);
 -	}
 +	fault = kmalloc(sizeof(*fault), GFP_ATOMIC);
 +	if (!fault)
 +		return -ENOMEM;
  
++<<<<<<< HEAD
 +	fault->addr = fault_addr;
 +	fault->idxd = idxd;
 +	INIT_WORK(&fault->work, idxd_device_fault_work);
 +	queue_work(idxd->wq, &fault->work);
 +	return 0;
++=======
+ 	list_for_each_entry_safe(d, t, &ie->work_list, list) {
+ 		if (d->completion->status == DSA_COMP_INT_HANDLE_INVAL)
+ 			list_move_tail(&d->list, &flist);
+ 	}
+ 	spin_unlock(&ie->list_lock);
+ 
+ 	list_for_each_entry_safe(d, t, &flist, list) {
+ 		list_del(&d->list);
+ 		idxd_dma_complete_txd(d, IDXD_COMPLETE_ABORT, true);
+ 	}
+ }
+ 
+ static void idxd_int_handle_revoke(struct work_struct *work)
+ {
+ 	struct idxd_int_handle_revoke *revoke =
+ 		container_of(work, struct idxd_int_handle_revoke, work);
+ 	struct idxd_device *idxd = revoke->idxd;
+ 	struct pci_dev *pdev = idxd->pdev;
+ 	struct device *dev = &pdev->dev;
+ 	int i, new_handle, rc;
+ 
+ 	if (!idxd->request_int_handles) {
+ 		kfree(revoke);
+ 		dev_warn(dev, "Unexpected int handle refresh interrupt.\n");
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * The loop attempts to acquire new interrupt handle for all interrupt
+ 	 * vectors that supports a handle. If a new interrupt handle is acquired and the
+ 	 * wq is kernel type, the driver will kill the percpu_ref to pause all
+ 	 * ongoing descriptor submissions. The interrupt handle is then changed.
+ 	 * After change, the percpu_ref is revived and all the pending submissions
+ 	 * are woken to try again. A drain is sent to for the interrupt handle
+ 	 * at the end to make sure all invalid int handle descriptors are processed.
+ 	 */
+ 	for (i = 1; i < idxd->irq_cnt; i++) {
+ 		struct idxd_irq_entry *ie = idxd_get_ie(idxd, i);
+ 		struct idxd_wq *wq = ie_to_wq(ie);
+ 
+ 		rc = idxd_device_request_int_handle(idxd, i, &new_handle, IDXD_IRQ_MSIX);
+ 		if (rc < 0) {
+ 			dev_warn(dev, "get int handle %d failed: %d\n", i, rc);
+ 			/*
+ 			 * Failed to acquire new interrupt handle. Kill the WQ
+ 			 * and release all the pending submitters. The submitters will
+ 			 * get error return code and handle appropriately.
+ 			 */
+ 			ie->int_handle = INVALID_INT_HANDLE;
+ 			idxd_wq_quiesce(wq);
+ 			idxd_abort_invalid_int_handle_descs(ie);
+ 			continue;
+ 		}
+ 
+ 		/* No change in interrupt handle, nothing needs to be done */
+ 		if (ie->int_handle == new_handle)
+ 			continue;
+ 
+ 		if (wq->state != IDXD_WQ_ENABLED || wq->type != IDXD_WQT_KERNEL) {
+ 			/*
+ 			 * All the MSIX interrupts are allocated at once during probe.
+ 			 * Therefore we need to update all interrupts even if the WQ
+ 			 * isn't supporting interrupt operations.
+ 			 */
+ 			ie->int_handle = new_handle;
+ 			continue;
+ 		}
+ 
+ 		mutex_lock(&wq->wq_lock);
+ 		reinit_completion(&wq->wq_resurrect);
+ 
+ 		/* Kill percpu_ref to pause additional descriptor submissions */
+ 		percpu_ref_kill(&wq->wq_active);
+ 
+ 		/* Wait for all submitters quiesce before we change interrupt handle */
+ 		wait_for_completion(&wq->wq_dead);
+ 
+ 		ie->int_handle = new_handle;
+ 
+ 		/* Revive percpu ref and wake up all the waiting submitters */
+ 		percpu_ref_reinit(&wq->wq_active);
+ 		complete_all(&wq->wq_resurrect);
+ 		mutex_unlock(&wq->wq_lock);
+ 
+ 		/*
+ 		 * The delay here is to wait for all possible MOVDIR64B that
+ 		 * are issued before percpu_ref_kill() has happened to have
+ 		 * reached the PCIe domain before the drain is issued. The driver
+ 		 * needs to ensure that the drain descriptor issued does not pass
+ 		 * all the other issued descriptors that contain the invalid
+ 		 * interrupt handle in order to ensure that the drain descriptor
+ 		 * interrupt will allow the cleanup of all the descriptors with
+ 		 * invalid interrupt handle.
+ 		 */
+ 		if (wq_dedicated(wq))
+ 			udelay(100);
+ 		idxd_int_handle_revoke_drain(ie);
+ 	}
+ 	kfree(revoke);
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  }
  
  static int process_misc_interrupts(struct idxd_device *idxd, u32 cause)
diff --cc drivers/dma/idxd/submit.c
index 0afcd1322339,e289fd48711a..000000000000
--- a/drivers/dma/idxd/submit.c
+++ b/drivers/dma/idxd/submit.c
@@@ -108,6 -187,17 +108,20 @@@ int idxd_submit_desc(struct idxd_wq *wq
  	 * even on UP because the recipient is a device.
  	 */
  	wmb();
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Pending the descriptor to the lockless list for the irq_entry
+ 	 * that we designated the descriptor to.
+ 	 */
+ 	if (desc_flags & IDXD_OP_FLAG_RCI) {
+ 		ie = &wq->ie;
+ 		desc->hw->int_handle = ie->int_handle;
+ 		llist_add(&desc->llnode, &ie->pending_llist);
+ 	}
+ 
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  	if (wq_dedicated(wq)) {
  		iosubmit_cmds512(portal, desc->hw, 1);
  	} else {
diff --cc drivers/dma/idxd/sysfs.c
index 4cfaba0faeca,13404532131b..000000000000
--- a/drivers/dma/idxd/sysfs.c
+++ b/drivers/dma/idxd/sysfs.c
@@@ -1584,8 -1304,6 +1584,11 @@@ static void idxd_conf_device_release(st
  	kfree(idxd->groups);
  	kfree(idxd->wqs);
  	kfree(idxd->engines);
++<<<<<<< HEAD
 +	kfree(idxd->irq_entries);
 +	kfree(idxd->int_handles);
++=======
++>>>>>>> ec0d64231615 (dmaengine: idxd: embed irq_entry in idxd_wq struct)
  	ida_free(&idxd_ida, idxd->id);
  	kfree(idxd);
  }
* Unmerged path drivers/dma/idxd/device.c
* Unmerged path drivers/dma/idxd/idxd.h
* Unmerged path drivers/dma/idxd/init.c
* Unmerged path drivers/dma/idxd/irq.c
* Unmerged path drivers/dma/idxd/submit.c
* Unmerged path drivers/dma/idxd/sysfs.c
