x86,kvm/xen: Remove superfluous .fixup usage

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit adb759e599990416e42e659c024a654b76c84617
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/adb759e5.failed

Commit 14243b387137 ("KVM: x86/xen: Add KVM_IRQ_ROUTING_XEN_EVTCHN and
event channel delivery") adds superfluous .fixup usage after the whole
.fixup section was removed in commit e5eefda5aa51 ("x86: Remove .fixup
section").

Fixes: 14243b387137 ("KVM: x86/xen: Add KVM_IRQ_ROUTING_XEN_EVTCHN and event channel delivery")
	Reported-by: Borislav Petkov <bp@alien8.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Message-Id: <20220123124219.GH20638@worktop.programming.kicks-ass.net>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit adb759e599990416e42e659c024a654b76c84617)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/xen.c
diff --cc arch/x86/kvm/xen.c
index 966f32314d76,bad57535fad0..000000000000
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@@ -273,11 -278,76 +273,81 @@@ int __kvm_xen_has_interrupt(struct kvm_
  	 * and we'll end up getting called again from a context where we *can*
  	 * fault in the page and wait for it.
  	 */
 -	if (atomic)
 +	if (in_atomic() || !task_is_running(current))
  		return 1;
  
++<<<<<<< HEAD
 +	kvm_read_guest_offset_cached(v->kvm, ghc, &rc, offset,
 +				     sizeof(rc));
++=======
+ 	if (!ghc_valid) {
+ 		err = kvm_gfn_to_hva_cache_init(v->kvm, ghc, ghc->gpa, ghc->len);
+ 		if (err || !ghc->memslot) {
+ 			/*
+ 			 * If this failed, userspace has screwed up the
+ 			 * vcpu_info mapping. No interrupts for you.
+ 			 */
+ 			return 0;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Now we have a valid (protected by srcu) userspace HVA in
+ 	 * ghc->hva which points to the struct vcpu_info. If there
+ 	 * are any bits in the in-kernel evtchn_pending_sel then
+ 	 * we need to write those to the guest vcpu_info and set
+ 	 * its evtchn_upcall_pending flag. If there aren't any bits
+ 	 * to add, we only want to *check* evtchn_upcall_pending.
+ 	 */
+ 	if (evtchn_pending_sel) {
+ 		bool long_mode = v->kvm->arch.xen.long_mode;
+ 
+ 		if (!user_access_begin((void __user *)ghc->hva, sizeof(struct vcpu_info)))
+ 			return 0;
+ 
+ 		if (IS_ENABLED(CONFIG_64BIT) && long_mode) {
+ 			struct vcpu_info __user *vi = (void __user *)ghc->hva;
+ 
+ 			/* Attempt to set the evtchn_pending_sel bits in the
+ 			 * guest, and if that succeeds then clear the same
+ 			 * bits in the in-kernel version. */
+ 			asm volatile("1:\t" LOCK_PREFIX "orq %0, %1\n"
+ 				     "\tnotq %0\n"
+ 				     "\t" LOCK_PREFIX "andq %0, %2\n"
+ 				     "2:\n"
+ 				     _ASM_EXTABLE_UA(1b, 2b)
+ 				     : "=r" (evtchn_pending_sel),
+ 				       "+m" (vi->evtchn_pending_sel),
+ 				       "+m" (v->arch.xen.evtchn_pending_sel)
+ 				     : "0" (evtchn_pending_sel));
+ 		} else {
+ 			struct compat_vcpu_info __user *vi = (void __user *)ghc->hva;
+ 			u32 evtchn_pending_sel32 = evtchn_pending_sel;
+ 
+ 			/* Attempt to set the evtchn_pending_sel bits in the
+ 			 * guest, and if that succeeds then clear the same
+ 			 * bits in the in-kernel version. */
+ 			asm volatile("1:\t" LOCK_PREFIX "orl %0, %1\n"
+ 				     "\tnotl %0\n"
+ 				     "\t" LOCK_PREFIX "andl %0, %2\n"
+ 				     "2:\n"
+ 				     _ASM_EXTABLE_UA(1b, 2b)
+ 				     : "=r" (evtchn_pending_sel32),
+ 				       "+m" (vi->evtchn_pending_sel),
+ 				       "+m" (v->arch.xen.evtchn_pending_sel)
+ 				     : "0" (evtchn_pending_sel32));
+ 		}
+ 		rc = 1;
+ 		unsafe_put_user(rc, (u8 __user *)ghc->hva + offset, err);
+ 
+ 	err:
+ 		user_access_end();
+ 
+ 		mark_page_dirty_in_slot(v->kvm, ghc->memslot, ghc->gpa >> PAGE_SHIFT);
+ 	} else {
+ 		__get_user(rc, (u8 __user *)ghc->hva + offset);
+ 	}
++>>>>>>> adb759e59999 (x86,kvm/xen: Remove superfluous .fixup usage)
  
  	return rc;
  }
* Unmerged path arch/x86/kvm/xen.c
