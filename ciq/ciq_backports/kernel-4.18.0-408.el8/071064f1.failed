KVM: Don't take mmu_lock for range invalidation unless necessary

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 071064f14d87536e38235df1bdeabe404023203f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/071064f1.failed

Avoid taking mmu_lock for .invalidate_range_{start,end}() notifications
that are unrelated to KVM.  This is possible now that memslot updates are
blocked from range_start() to range_end(); that ensures that lock elision
happens in both or none, and therefore that mmu_notifier_count updates
(which must occur while holding mmu_lock for write) are always paired
across start->end.

Based on patches originally written by Ben Gardon.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 071064f14d87536e38235df1bdeabe404023203f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	virt/kvm/kvm_main.c
diff --cc virt/kvm/kvm_main.c
index 6f04c4e5e88c,930aeb8d3c3e..000000000000
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@@ -482,6 -452,140 +482,143 @@@ static void kvm_mmu_notifier_invalidate
  	srcu_read_unlock(&kvm->srcu, idx);
  }
  
++<<<<<<< HEAD
++=======
+ typedef bool (*hva_handler_t)(struct kvm *kvm, struct kvm_gfn_range *range);
+ 
+ typedef void (*on_lock_fn_t)(struct kvm *kvm, unsigned long start,
+ 			     unsigned long end);
+ 
+ struct kvm_hva_range {
+ 	unsigned long start;
+ 	unsigned long end;
+ 	pte_t pte;
+ 	hva_handler_t handler;
+ 	on_lock_fn_t on_lock;
+ 	bool flush_on_ret;
+ 	bool may_block;
+ };
+ 
+ /*
+  * Use a dedicated stub instead of NULL to indicate that there is no callback
+  * function/handler.  The compiler technically can't guarantee that a real
+  * function will have a non-zero address, and so it will generate code to
+  * check for !NULL, whereas comparing against a stub will be elided at compile
+  * time (unless the compiler is getting long in the tooth, e.g. gcc 4.9).
+  */
+ static void kvm_null_fn(void)
+ {
+ 
+ }
+ #define IS_KVM_NULL_FN(fn) ((fn) == (void *)kvm_null_fn)
+ 
+ static __always_inline int __kvm_handle_hva_range(struct kvm *kvm,
+ 						  const struct kvm_hva_range *range)
+ {
+ 	bool ret = false, locked = false;
+ 	struct kvm_gfn_range gfn_range;
+ 	struct kvm_memory_slot *slot;
+ 	struct kvm_memslots *slots;
+ 	int i, idx;
+ 
+ 	/* A null handler is allowed if and only if on_lock() is provided. */
+ 	if (WARN_ON_ONCE(IS_KVM_NULL_FN(range->on_lock) &&
+ 			 IS_KVM_NULL_FN(range->handler)))
+ 		return 0;
+ 
+ 	idx = srcu_read_lock(&kvm->srcu);
+ 
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		slots = __kvm_memslots(kvm, i);
+ 		kvm_for_each_memslot(slot, slots) {
+ 			unsigned long hva_start, hva_end;
+ 
+ 			hva_start = max(range->start, slot->userspace_addr);
+ 			hva_end = min(range->end, slot->userspace_addr +
+ 						  (slot->npages << PAGE_SHIFT));
+ 			if (hva_start >= hva_end)
+ 				continue;
+ 
+ 			/*
+ 			 * To optimize for the likely case where the address
+ 			 * range is covered by zero or one memslots, don't
+ 			 * bother making these conditional (to avoid writes on
+ 			 * the second or later invocation of the handler).
+ 			 */
+ 			gfn_range.pte = range->pte;
+ 			gfn_range.may_block = range->may_block;
+ 
+ 			/*
+ 			 * {gfn(page) | page intersects with [hva_start, hva_end)} =
+ 			 * {gfn_start, gfn_start+1, ..., gfn_end-1}.
+ 			 */
+ 			gfn_range.start = hva_to_gfn_memslot(hva_start, slot);
+ 			gfn_range.end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, slot);
+ 			gfn_range.slot = slot;
+ 
+ 			if (!locked) {
+ 				locked = true;
+ 				KVM_MMU_LOCK(kvm);
+ 				if (!IS_KVM_NULL_FN(range->on_lock))
+ 					range->on_lock(kvm, range->start, range->end);
+ 				if (IS_KVM_NULL_FN(range->handler))
+ 					break;
+ 			}
+ 			ret |= range->handler(kvm, &gfn_range);
+ 		}
+ 	}
+ 
+ 	if (range->flush_on_ret && (ret || kvm->tlbs_dirty))
+ 		kvm_flush_remote_tlbs(kvm);
+ 
+ 	if (locked)
+ 		KVM_MMU_UNLOCK(kvm);
+ 
+ 	srcu_read_unlock(&kvm->srcu, idx);
+ 
+ 	/* The notifiers are averse to booleans. :-( */
+ 	return (int)ret;
+ }
+ 
+ static __always_inline int kvm_handle_hva_range(struct mmu_notifier *mn,
+ 						unsigned long start,
+ 						unsigned long end,
+ 						pte_t pte,
+ 						hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= pte,
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= true,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
+ 
+ static __always_inline int kvm_handle_hva_range_no_flush(struct mmu_notifier *mn,
+ 							 unsigned long start,
+ 							 unsigned long end,
+ 							 hva_handler_t handler)
+ {
+ 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
+ 	const struct kvm_hva_range range = {
+ 		.start		= start,
+ 		.end		= end,
+ 		.pte		= __pte(0),
+ 		.handler	= handler,
+ 		.on_lock	= (void *)kvm_null_fn,
+ 		.flush_on_ret	= false,
+ 		.may_block	= false,
+ 	};
+ 
+ 	return __kvm_handle_hva_range(kvm, &range);
+ }
++>>>>>>> 071064f14d87 (KVM: Don't take mmu_lock for range invalidation unless necessary)
  static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
  					struct mm_struct *mm,
  					unsigned long address,
@@@ -493,35 -596,22 +630,47 @@@
  	trace_kvm_set_spte_hva(address);
  
  	/*
++<<<<<<< HEAD
 +	 * .change_pte() must be surrounded by .invalidate_range_{start,end}(),
 +	 * and so always runs with an elevated notifier count.  This obviates
 +	 * the need to bump the sequence count.
 +	 */
 +	WARN_ON_ONCE(!kvm->mmu_notifier_count);
++=======
+ 	 * .change_pte() must be surrounded by .invalidate_range_{start,end}().
+ 	 * If mmu_notifier_count is zero, then no in-progress invalidations,
+ 	 * including this one, found a relevant memslot at start(); rechecking
+ 	 * memslots here is unnecessary.  Note, a false positive (count elevated
+ 	 * by a different invalidation) is sub-optimal but functionally ok.
+ 	 */
+ 	WARN_ON_ONCE(!READ_ONCE(kvm->mn_active_invalidate_count));
+ 	if (!READ_ONCE(kvm->mmu_notifier_count))
+ 		return;
++>>>>>>> 071064f14d87 (KVM: Don't take mmu_lock for range invalidation unless necessary)
 +
 +	idx = srcu_read_lock(&kvm->srcu);
  
 -	kvm_handle_hva_range(mn, address, address + 1, pte, kvm_set_spte_gfn);
 +	KVM_MMU_LOCK(kvm);
 +
 +	if (kvm_set_spte_hva(kvm, address, pte))
 +		kvm_flush_remote_tlbs(kvm);
 +
 +	KVM_MMU_UNLOCK(kvm);
 +	srcu_read_unlock(&kvm->srcu, idx);
  }
  
 -static void kvm_inc_notifier_count(struct kvm *kvm, unsigned long start,
 -				   unsigned long end)
 +static void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 +						    struct mm_struct *mm,
 +						    unsigned long start,
 +						    unsigned long end)
  {
 +	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 +	int need_tlb_flush = 0, idx;
 +
 +	trace_kvm_unmap_hva_range(range->start, range->end);
 +
 +	idx = srcu_read_lock(&kvm->srcu);
 +	KVM_MMU_LOCK(kvm);
  	/*
  	 * The count increase must become visible at unlock time as no
  	 * spte can be established without taking the mmu_lock and
@@@ -1304,7 -1394,22 +1453,24 @@@ static struct kvm_memslots *install_new
  	WARN_ON(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);
  	slots->generation = gen | KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Do not store the new memslots while there are invalidations in
+ 	 * progress, otherwise the locking in invalidate_range_start and
+ 	 * invalidate_range_end will be unbalanced.
+ 	 */
+ 	spin_lock(&kvm->mn_invalidate_lock);
+ 	prepare_to_rcuwait(&kvm->mn_memslots_update_rcuwait);
+ 	while (kvm->mn_active_invalidate_count) {
+ 		set_current_state(TASK_UNINTERRUPTIBLE);
+ 		spin_unlock(&kvm->mn_invalidate_lock);
+ 		schedule();
+ 		spin_lock(&kvm->mn_invalidate_lock);
+ 	}
+ 	finish_rcuwait(&kvm->mn_memslots_update_rcuwait);
++>>>>>>> 071064f14d87 (KVM: Don't take mmu_lock for range invalidation unless necessary)
  	rcu_assign_pointer(kvm->memslots[as_id], slots);
 -	spin_unlock(&kvm->mn_invalidate_lock);
  
  	/*
  	 * Acquired in kvm_set_memslot. Must be released before synchronize
* Unmerged path virt/kvm/kvm_main.c
