KVM: x86/mmu: Add tracepoint for splitting huge pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit e0b728b1f1a951e0d23eef08cfa920a8104e39db
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/e0b728b1.failed

Add a tracepoint that records whenever KVM eagerly splits a huge page
and the error status of the split to indicate if it succeeded or failed
and why.

	Reviewed-by: Peter Xu <peterx@redhat.com>
	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20220119230739.2234394-18-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit e0b728b1f1a951e0d23eef08cfa920a8104e39db)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 00e34bc04af0,8def8f810cb0..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -1268,6 -1263,190 +1268,193 @@@ bool kvm_tdp_mmu_wrprot_slot(struct kv
  	return spte_set;
  }
  
++<<<<<<< HEAD
++=======
+ static struct kvm_mmu_page *__tdp_mmu_alloc_sp_for_split(gfp_t gfp)
+ {
+ 	struct kvm_mmu_page *sp;
+ 
+ 	gfp |= __GFP_ZERO;
+ 
+ 	sp = kmem_cache_alloc(mmu_page_header_cache, gfp);
+ 	if (!sp)
+ 		return NULL;
+ 
+ 	sp->spt = (void *)__get_free_page(gfp);
+ 	if (!sp->spt) {
+ 		kmem_cache_free(mmu_page_header_cache, sp);
+ 		return NULL;
+ 	}
+ 
+ 	return sp;
+ }
+ 
+ static struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,
+ 						       struct tdp_iter *iter,
+ 						       bool shared)
+ {
+ 	struct kvm_mmu_page *sp;
+ 
+ 	/*
+ 	 * Since we are allocating while under the MMU lock we have to be
+ 	 * careful about GFP flags. Use GFP_NOWAIT to avoid blocking on direct
+ 	 * reclaim and to avoid making any filesystem callbacks (which can end
+ 	 * up invoking KVM MMU notifiers, resulting in a deadlock).
+ 	 *
+ 	 * If this allocation fails we drop the lock and retry with reclaim
+ 	 * allowed.
+ 	 */
+ 	sp = __tdp_mmu_alloc_sp_for_split(GFP_NOWAIT | __GFP_ACCOUNT);
+ 	if (sp)
+ 		return sp;
+ 
+ 	rcu_read_unlock();
+ 
+ 	if (shared)
+ 		read_unlock(&kvm->mmu_lock);
+ 	else
+ 		write_unlock(&kvm->mmu_lock);
+ 
+ 	iter->yielded = true;
+ 	sp = __tdp_mmu_alloc_sp_for_split(GFP_KERNEL_ACCOUNT);
+ 
+ 	if (shared)
+ 		read_lock(&kvm->mmu_lock);
+ 	else
+ 		write_lock(&kvm->mmu_lock);
+ 
+ 	rcu_read_lock();
+ 
+ 	return sp;
+ }
+ 
+ static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
+ 				   struct kvm_mmu_page *sp, bool shared)
+ {
+ 	const u64 huge_spte = iter->old_spte;
+ 	const int level = iter->level;
+ 	int ret, i;
+ 
+ 	tdp_mmu_init_child_sp(sp, iter);
+ 
+ 	/*
+ 	 * No need for atomics when writing to sp->spt since the page table has
+ 	 * not been linked in yet and thus is not reachable from any other CPU.
+ 	 */
+ 	for (i = 0; i < PT64_ENT_PER_PAGE; i++)
+ 		sp->spt[i] = make_huge_page_split_spte(huge_spte, level, i);
+ 
+ 	/*
+ 	 * Replace the huge spte with a pointer to the populated lower level
+ 	 * page table. Since we are making this change without a TLB flush vCPUs
+ 	 * will see a mix of the split mappings and the original huge mapping,
+ 	 * depending on what's currently in their TLB. This is fine from a
+ 	 * correctness standpoint since the translation will be the same either
+ 	 * way.
+ 	 */
+ 	ret = tdp_mmu_link_sp(kvm, iter, sp, false, shared);
+ 	if (ret)
+ 		goto out;
+ 
+ 	/*
+ 	 * tdp_mmu_link_sp_atomic() will handle subtracting the huge page we
+ 	 * are overwriting from the page stats. But we have to manually update
+ 	 * the page stats with the new present child pages.
+ 	 */
+ 	kvm_update_page_stats(kvm, level - 1, PT64_ENT_PER_PAGE);
+ 
+ out:
+ 	trace_kvm_mmu_split_huge_page(iter->gfn, huge_spte, level, ret);
+ 	return ret;
+ }
+ 
+ static int tdp_mmu_split_huge_pages_root(struct kvm *kvm,
+ 					 struct kvm_mmu_page *root,
+ 					 gfn_t start, gfn_t end,
+ 					 int target_level, bool shared)
+ {
+ 	struct kvm_mmu_page *sp = NULL;
+ 	struct tdp_iter iter;
+ 	int ret = 0;
+ 
+ 	rcu_read_lock();
+ 
+ 	/*
+ 	 * Traverse the page table splitting all huge pages above the target
+ 	 * level into one lower level. For example, if we encounter a 1GB page
+ 	 * we split it into 512 2MB pages.
+ 	 *
+ 	 * Since the TDP iterator uses a pre-order traversal, we are guaranteed
+ 	 * to visit an SPTE before ever visiting its children, which means we
+ 	 * will correctly recursively split huge pages that are more than one
+ 	 * level above the target level (e.g. splitting a 1GB to 512 2MB pages,
+ 	 * and then splitting each of those to 512 4KB pages).
+ 	 */
+ 	for_each_tdp_pte_min_level(iter, root, target_level + 1, start, end) {
+ retry:
+ 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ 			continue;
+ 
+ 		if (!is_shadow_present_pte(iter.old_spte) || !is_large_pte(iter.old_spte))
+ 			continue;
+ 
+ 		if (!sp) {
+ 			sp = tdp_mmu_alloc_sp_for_split(kvm, &iter, shared);
+ 			if (!sp) {
+ 				ret = -ENOMEM;
+ 				trace_kvm_mmu_split_huge_page(iter.gfn,
+ 							      iter.old_spte,
+ 							      iter.level, ret);
+ 				break;
+ 			}
+ 
+ 			if (iter.yielded)
+ 				continue;
+ 		}
+ 
+ 		if (tdp_mmu_split_huge_page(kvm, &iter, sp, shared))
+ 			goto retry;
+ 
+ 		sp = NULL;
+ 	}
+ 
+ 	rcu_read_unlock();
+ 
+ 	/*
+ 	 * It's possible to exit the loop having never used the last sp if, for
+ 	 * example, a vCPU doing HugePage NX splitting wins the race and
+ 	 * installs its own sp in place of the last sp we tried to split.
+ 	 */
+ 	if (sp)
+ 		tdp_mmu_free_sp(sp);
+ 
+ 	return ret;
+ }
+ 
+ 
+ /*
+  * Try to split all huge pages mapped by the TDP MMU down to the target level.
+  */
+ void kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,
+ 				      const struct kvm_memory_slot *slot,
+ 				      gfn_t start, gfn_t end,
+ 				      int target_level, bool shared)
+ {
+ 	struct kvm_mmu_page *root;
+ 	int r = 0;
+ 
+ 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ 
+ 	for_each_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, shared) {
+ 		r = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);
+ 		if (r) {
+ 			kvm_tdp_mmu_put_root(kvm, root, shared);
+ 			break;
+ 		}
+ 	}
+ }
+ 
++>>>>>>> e0b728b1f1a9 (KVM: x86/mmu: Add tracepoint for splitting huge pages)
  /*
   * Clear the dirty status of all the SPTEs mapping GFNs in the memslot. If
   * AD bits are enabled, this will involve clearing the dirty bit on each SPTE.
diff --git a/arch/x86/kvm/mmu/mmutrace.h b/arch/x86/kvm/mmu/mmutrace.h
index 9863290dcba1..c5c79fa4fb30 100644
--- a/arch/x86/kvm/mmu/mmutrace.h
+++ b/arch/x86/kvm/mmu/mmutrace.h
@@ -416,6 +416,29 @@ TRACE_EVENT(
 	)
 );
 
+TRACE_EVENT(
+	kvm_mmu_split_huge_page,
+	TP_PROTO(u64 gfn, u64 spte, int level, int errno),
+	TP_ARGS(gfn, spte, level, errno),
+
+	TP_STRUCT__entry(
+		__field(u64, gfn)
+		__field(u64, spte)
+		__field(int, level)
+		__field(int, errno)
+	),
+
+	TP_fast_assign(
+		__entry->gfn = gfn;
+		__entry->spte = spte;
+		__entry->level = level;
+		__entry->errno = errno;
+	),
+
+	TP_printk("gfn %llx spte %llx level %d errno %d",
+		  __entry->gfn, __entry->spte, __entry->level, __entry->errno)
+);
+
 #endif /* _TRACE_KVMMMU_H */
 
 #undef TRACE_INCLUDE_PATH
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
