KVM: x86/mmu: Do not create SPTEs for GFNs that exceed host.MAXPHYADDR

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit 86931ff7207bc045fa5439ef97b31859613dc303
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/86931ff7.failed

Disallow memslots and MMIO SPTEs whose gpa range would exceed the host's
MAXPHYADDR, i.e. don't create SPTEs for gfns that exceed host.MAXPHYADDR.
The TDP MMU bounds its zapping based on host.MAXPHYADDR, and so if the
guest, possibly with help from userspace, manages to coerce KVM into
creating a SPTE for an "impossible" gfn, KVM will leak the associated
shadow pages (page tables):

  WARNING: CPU: 10 PID: 1122 at arch/x86/kvm/mmu/tdp_mmu.c:57
                                kvm_mmu_uninit_tdp_mmu+0x4b/0x60 [kvm]
  Modules linked in: kvm_intel kvm irqbypass
  CPU: 10 PID: 1122 Comm: set_memory_regi Tainted: G        W         5.18.0-rc1+ #293
  Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 0.0.0 02/06/2015
  RIP: 0010:kvm_mmu_uninit_tdp_mmu+0x4b/0x60 [kvm]
  Call Trace:
   <TASK>
   kvm_arch_destroy_vm+0x130/0x1b0 [kvm]
   kvm_destroy_vm+0x162/0x2d0 [kvm]
   kvm_vm_release+0x1d/0x30 [kvm]
   __fput+0x82/0x240
   task_work_run+0x5b/0x90
   exit_to_user_mode_prepare+0xd2/0xe0
   syscall_exit_to_user_mode+0x1d/0x40
   entry_SYSCALL_64_after_hwframe+0x44/0xae
   </TASK>

On bare metal, encountering an impossible gpa in the page fault path is
well and truly impossible, barring CPU bugs, as the CPU will signal #PF
during the gva=>gpa translation (or a similar failure when stuffing a
physical address into e.g. the VMCS/VMCB).  But if KVM is running as a VM
itself, the MAXPHYADDR enumerated to KVM may not be the actual MAXPHYADDR
of the underlying hardware, in which case the hardware will not fault on
the illegal-from-KVM's-perspective gpa.

Alternatively, KVM could continue allowing the dodgy behavior and simply
zap the max possible range.  But, for hosts with MAXPHYADDR < 52, that's
a (minor) waste of cycles, and more importantly, KVM can't reasonably
support impossible memslots when running on bare metal (or with an
accurate MAXPHYADDR as a VM).  Note, limiting the overhead by checking if
KVM is running as a guest is not a safe option as the host isn't required
to announce itself to the guest in any way, e.g. doesn't need to set the
HYPERVISOR CPUID bit.

A second alternative to disallowing the memslot behavior would be to
disallow creating a VM with guest.MAXPHYADDR > host.MAXPHYADDR.  That
restriction is undesirable as there are legitimate use cases for doing
so, e.g. using the highest host.MAXPHYADDR out of a pool of heterogeneous
systems so that VMs can be migrated between hosts with different
MAXPHYADDRs without running afoul of the allow_smaller_maxphyaddr mess.

Note that any guest.MAXPHYADDR is valid with shadow paging, and it is
even useful in order to test KVM with MAXPHYADDR=52 (i.e. without
any reserved physical address bits).

The now common kvm_mmu_max_gfn() is inclusive instead of exclusive.
The memslot and TDP MMU code want an exclusive value, but the name
implies the returned value is inclusive, and the MMIO path needs an
inclusive check.

Fixes: faaf05b00aec ("kvm: x86/mmu: Support zapping SPTEs in the TDP MMU")
Fixes: 524a1e4e381f ("KVM: x86/mmu: Don't leak non-leaf SPTEs when zapping all SPTEs")
	Cc: stable@vger.kernel.org
	Cc: Maxim Levitsky <mlevitsk@redhat.com>
	Cc: Ben Gardon <bgardon@google.com>
	Cc: David Matlack <dmatlack@google.com>
	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20220428233416.2446833-1-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 86931ff7207bc045fa5439ef97b31859613dc303)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 97bb57fe39ca,edc68538819b..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -700,11 -815,104 +700,108 @@@ static inline bool __must_check tdp_mmu
  	return iter->yielded;
  }
  
++<<<<<<< HEAD
++=======
+ static inline gfn_t tdp_mmu_max_gfn_exclusive(void)
+ {
+ 	/*
+ 	 * Bound TDP MMU walks at host.MAXPHYADDR.  KVM disallows memslots with
+ 	 * a gpa range that would exceed the max gfn, and KVM does not create
+ 	 * MMIO SPTEs for "impossible" gfns, instead sending such accesses down
+ 	 * the slow emulation path every time.
+ 	 */
+ 	return kvm_mmu_max_gfn() + 1;
+ }
+ 
+ static void __tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			       bool shared, int zap_level)
+ {
+ 	struct tdp_iter iter;
+ 
+ 	gfn_t end = tdp_mmu_max_gfn_exclusive();
+ 	gfn_t start = 0;
+ 
+ 	for_each_tdp_pte_min_level(iter, root, zap_level, start, end) {
+ retry:
+ 		if (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))
+ 			continue;
+ 
+ 		if (!is_shadow_present_pte(iter.old_spte))
+ 			continue;
+ 
+ 		if (iter.level > zap_level)
+ 			continue;
+ 
+ 		if (!shared)
+ 			tdp_mmu_set_spte(kvm, &iter, 0);
+ 		else if (tdp_mmu_set_spte_atomic(kvm, &iter, 0))
+ 			goto retry;
+ 	}
+ }
+ 
+ static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			     bool shared)
+ {
+ 
+ 	/*
+ 	 * The root must have an elevated refcount so that it's reachable via
+ 	 * mmu_notifier callbacks, which allows this path to yield and drop
+ 	 * mmu_lock.  When handling an unmap/release mmu_notifier command, KVM
+ 	 * must drop all references to relevant pages prior to completing the
+ 	 * callback.  Dropping mmu_lock with an unreachable root would result
+ 	 * in zapping SPTEs after a relevant mmu_notifier callback completes
+ 	 * and lead to use-after-free as zapping a SPTE triggers "writeback" of
+ 	 * dirty accessed bits to the SPTE's associated struct page.
+ 	 */
+ 	WARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));
+ 
+ 	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
+ 
+ 	rcu_read_lock();
+ 
+ 	/*
+ 	 * To avoid RCU stalls due to recursively removing huge swaths of SPs,
+ 	 * split the zap into two passes.  On the first pass, zap at the 1gb
+ 	 * level, and then zap top-level SPs on the second pass.  "1gb" is not
+ 	 * arbitrary, as KVM must be able to zap a 1gb shadow page without
+ 	 * inducing a stall to allow in-place replacement with a 1gb hugepage.
+ 	 *
+ 	 * Because zapping a SP recurses on its children, stepping down to
+ 	 * PG_LEVEL_4K in the iterator itself is unnecessary.
+ 	 */
+ 	__tdp_mmu_zap_root(kvm, root, shared, PG_LEVEL_1G);
+ 	__tdp_mmu_zap_root(kvm, root, shared, root->role.level);
+ 
+ 	rcu_read_unlock();
+ }
+ 
+ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
+ {
+ 	u64 old_spte;
+ 
+ 	/*
+ 	 * This helper intentionally doesn't allow zapping a root shadow page,
+ 	 * which doesn't have a parent page table and thus no associated entry.
+ 	 */
+ 	if (WARN_ON_ONCE(!sp->ptep))
+ 		return false;
+ 
+ 	old_spte = kvm_tdp_mmu_read_spte(sp->ptep);
+ 	if (WARN_ON_ONCE(!is_shadow_present_pte(old_spte)))
+ 		return false;
+ 
+ 	__tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0,
+ 			   sp->gfn, sp->role.level + 1, true, true);
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 86931ff7207b (KVM: x86/mmu: Do not create SPTEs for GFNs that exceed host.MAXPHYADDR)
  /*
 - * Zap leafs SPTEs for the range of gfns, [start, end). Returns true if SPTEs
 - * have been cleared and a TLB flush is needed before releasing the MMU lock.
 - *
 + * Tears down the mappings for the range of gfns, [start, end), and frees the
 + * non-root pages mapping GFNs strictly within that range. Returns true if
 + * SPTEs have been cleared and a TLB flush is needed before releasing the
 + * MMU lock.
   * If can_yield is true, will release the MMU lock and reschedule if the
   * scheduler needs the CPU or there is contention on the MMU lock. If this
   * function cannot yield, it will not release the MMU lock or reschedule and
@@@ -718,11 -924,15 +815,18 @@@ static bool zap_gfn_range(struct kvm *k
  {
  	struct tdp_iter iter;
  
++<<<<<<< HEAD
++=======
+ 	end = min(end, tdp_mmu_max_gfn_exclusive());
+ 
+ 	lockdep_assert_held_write(&kvm->mmu_lock);
+ 
++>>>>>>> 86931ff7207b (KVM: x86/mmu: Do not create SPTEs for GFNs that exceed host.MAXPHYADDR)
  	rcu_read_lock();
  
 -	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end) {
 +	tdp_root_for_each_pte(iter, root, start, end) {
  		if (can_yield &&
 -		    tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {
 +		    tdp_mmu_iter_cond_resched(kvm, &iter, flush)) {
  			flush = false;
  			continue;
  		}
diff --cc arch/x86/kvm/x86.c
index 1c43097b329c,43174a8d9497..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -11931,13 -11991,22 +11931,28 @@@ void kvm_arch_memslots_updated(struct k
  }
  
  int kvm_arch_prepare_memory_region(struct kvm *kvm,
 -				   const struct kvm_memory_slot *old,
 -				   struct kvm_memory_slot *new,
 -				   enum kvm_mr_change change)
 +				struct kvm_memory_slot *memslot,
 +				const struct kvm_userspace_memory_region *mem,
 +				enum kvm_mr_change change)
  {
++<<<<<<< HEAD
 +	if (change == KVM_MR_CREATE || change == KVM_MR_MOVE)
 +		return kvm_alloc_memslot_metadata(kvm, memslot,
 +						  mem->memory_size >> PAGE_SHIFT);
++=======
+ 	if (change == KVM_MR_CREATE || change == KVM_MR_MOVE) {
+ 		if ((new->base_gfn + new->npages - 1) > kvm_mmu_max_gfn())
+ 			return -EINVAL;
+ 
+ 		return kvm_alloc_memslot_metadata(kvm, new);
+ 	}
+ 
+ 	if (change == KVM_MR_FLAGS_ONLY)
+ 		memcpy(&new->arch, &old->arch, sizeof(old->arch));
+ 	else if (WARN_ON_ONCE(change != KVM_MR_DELETE))
+ 		return -EIO;
+ 
++>>>>>>> 86931ff7207b (KVM: x86/mmu: Do not create SPTEs for GFNs that exceed host.MAXPHYADDR)
  	return 0;
  }
  
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index aa216128806e..7c42f9a0f419 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -65,6 +65,30 @@ static __always_inline u64 rsvd_bits(int s, int e)
 	return ((2ULL << (e - s)) - 1) << s;
 }
 
+/*
+ * The number of non-reserved physical address bits irrespective of features
+ * that repurpose legal bits, e.g. MKTME.
+ */
+extern u8 __read_mostly shadow_phys_bits;
+
+static inline gfn_t kvm_mmu_max_gfn(void)
+{
+	/*
+	 * Note that this uses the host MAXPHYADDR, not the guest's.
+	 * EPT/NPT cannot support GPAs that would exceed host.MAXPHYADDR;
+	 * assuming KVM is running on bare metal, guest accesses beyond
+	 * host.MAXPHYADDR will hit a #PF(RSVD) and never cause a vmexit
+	 * (either EPT Violation/Misconfig or #NPF), and so KVM will never
+	 * install a SPTE for such addresses.  If KVM is running as a VM
+	 * itself, on the other hand, it might see a MAXPHYADDR that is less
+	 * than hardware's real MAXPHYADDR.  Using the host MAXPHYADDR
+	 * disallows such SPTEs entirely and simplifies the TDP MMU.
+	 */
+	int max_gpa_bits = likely(tdp_enabled) ? shadow_phys_bits : 52;
+
+	return (1ULL << (max_gpa_bits - PAGE_SHIFT)) - 1;
+}
+
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask);
 void kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only);
 
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 978c125f417f..3527444b37c1 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -3031,9 +3031,15 @@ static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,
 		/*
 		 * If MMIO caching is disabled, emulate immediately without
 		 * touching the shadow page tables as attempting to install an
-		 * MMIO SPTE will just be an expensive nop.
+		 * MMIO SPTE will just be an expensive nop.  Do not cache MMIO
+		 * whose gfn is greater than host.MAXPHYADDR, any guest that
+		 * generates such gfns is running nested and is being tricked
+		 * by L0 userspace (you can observe gfn > L1.MAXPHYADDR if
+		 * and only if L1's MAXPHYADDR is inaccurate with respect to
+		 * the hardware's).
 		 */
-		if (unlikely(!shadow_mmio_value)) {
+		if (unlikely(!shadow_mmio_value) ||
+		    unlikely(fault->gfn > kvm_mmu_max_gfn())) {
 			*ret_val = RET_PF_EMULATE;
 			return true;
 		}
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index e2953f371916..03d3189a4514 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -221,12 +221,6 @@ static inline bool is_removed_spte(u64 spte)
  */
 extern u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
-/*
- * The number of non-reserved physical address bits irrespective of features
- * that repurpose legal bits, e.g. MKTME.
- */
-extern u8 __read_mostly shadow_phys_bits;
-
 static inline bool is_mmio_spte(u64 spte)
 {
 	return (spte & shadow_mmio_mask) == shadow_mmio_value &&
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
* Unmerged path arch/x86/kvm/x86.c
