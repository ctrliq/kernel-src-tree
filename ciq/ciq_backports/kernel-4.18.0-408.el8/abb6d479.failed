KVM: x86: make several APIC virtualization callbacks optional

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit abb6d479e22642c82d552970d85edd9b5fe8beb6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/abb6d479.failed

All their invocations are conditional on vcpu->arch.apicv_active,
meaning that they need not be implemented by vendor code: even
though at the moment both vendors implement APIC virtualization,
all of them can be optional.  In fact SVM does not need many of
them, and their implementation can be deleted now.

	Reviewed-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit abb6d479e22642c82d552970d85edd9b5fe8beb6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm-x86-ops.h
#	arch/x86/kvm/svm/avic.c
#	arch/x86/kvm/svm/svm.c
#	arch/x86/kvm/svm/svm.h
diff --cc arch/x86/include/asm/kvm-x86-ops.h
index ab393e44c1e3,c0ec066a8599..000000000000
--- a/arch/x86/include/asm/kvm-x86-ops.h
+++ b/arch/x86/include/asm/kvm-x86-ops.h
@@@ -73,17 -72,17 +73,26 @@@ KVM_X86_OP(get_nmi_mask
  KVM_X86_OP(set_nmi_mask)
  KVM_X86_OP(enable_nmi_window)
  KVM_X86_OP(enable_irq_window)
 -KVM_X86_OP_OPTIONAL(update_cr8_intercept)
 +KVM_X86_OP(update_cr8_intercept)
  KVM_X86_OP(check_apicv_inhibit_reasons)
  KVM_X86_OP(refresh_apicv_exec_ctrl)
++<<<<<<< HEAD
 +KVM_X86_OP(hwapic_irr_update)
 +KVM_X86_OP(hwapic_isr_update)
 +KVM_X86_OP_NULL(guest_apic_has_interrupt)
 +KVM_X86_OP(load_eoi_exitmap)
 +KVM_X86_OP(set_virtual_apic_mode)
 +KVM_X86_OP_NULL(set_apic_access_page_addr)
++=======
+ KVM_X86_OP_OPTIONAL(hwapic_irr_update)
+ KVM_X86_OP_OPTIONAL(hwapic_isr_update)
+ KVM_X86_OP_OPTIONAL(guest_apic_has_interrupt)
+ KVM_X86_OP_OPTIONAL(load_eoi_exitmap)
+ KVM_X86_OP_OPTIONAL(set_virtual_apic_mode)
+ KVM_X86_OP_OPTIONAL(set_apic_access_page_addr)
++>>>>>>> abb6d479e226 (KVM: x86: make several APIC virtualization callbacks optional)
  KVM_X86_OP(deliver_interrupt)
 -KVM_X86_OP_OPTIONAL(sync_pir_to_irr)
 +KVM_X86_OP_NULL(sync_pir_to_irr)
  KVM_X86_OP(set_tss_addr)
  KVM_X86_OP(set_identity_map_addr)
  KVM_X86_OP(get_mt_mask)
@@@ -96,19 -95,17 +105,31 @@@ KVM_X86_OP(write_tsc_multiplier
  KVM_X86_OP(get_exit_info)
  KVM_X86_OP(check_intercept)
  KVM_X86_OP(handle_exit_irqoff)
 -KVM_X86_OP(request_immediate_exit)
 +KVM_X86_OP_NULL(request_immediate_exit)
  KVM_X86_OP(sched_in)
++<<<<<<< HEAD
 +KVM_X86_OP_NULL(update_cpu_dirty_logging)
 +KVM_X86_OP_NULL(pre_block)
 +KVM_X86_OP_NULL(post_block)
 +KVM_X86_OP_NULL(vcpu_blocking)
 +KVM_X86_OP_NULL(vcpu_unblocking)
 +KVM_X86_OP_NULL(pi_update_irte)
 +KVM_X86_OP_NULL(pi_start_assignment)
 +KVM_X86_OP_NULL(apicv_post_state_restore)
 +KVM_X86_OP_NULL(dy_apicv_has_pending_interrupt)
 +KVM_X86_OP_NULL(set_hv_timer)
 +KVM_X86_OP_NULL(cancel_hv_timer)
++=======
+ KVM_X86_OP_OPTIONAL(update_cpu_dirty_logging)
+ KVM_X86_OP_OPTIONAL(vcpu_blocking)
+ KVM_X86_OP_OPTIONAL(vcpu_unblocking)
+ KVM_X86_OP_OPTIONAL(pi_update_irte)
+ KVM_X86_OP_OPTIONAL(pi_start_assignment)
+ KVM_X86_OP_OPTIONAL(apicv_post_state_restore)
+ KVM_X86_OP_OPTIONAL(dy_apicv_has_pending_interrupt)
+ KVM_X86_OP_OPTIONAL(set_hv_timer)
+ KVM_X86_OP_OPTIONAL(cancel_hv_timer)
++>>>>>>> abb6d479e226 (KVM: x86: make several APIC virtualization callbacks optional)
  KVM_X86_OP(setup_mce)
  KVM_X86_OP(smi_allowed)
  KVM_X86_OP(enter_smm)
diff --cc arch/x86/kvm/svm/avic.c
index a77f6cee3cf0,4245cb99b497..000000000000
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@@ -585,20 -586,7 +585,24 @@@ void avic_post_state_restore(struct kvm
  	avic_handle_ldr_update(vcpu);
  }
  
++<<<<<<< HEAD
 +void svm_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 +{
 +	return;
 +}
 +
 +void svm_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)
 +{
 +}
 +
 +void svm_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 +{
 +}
 +
 +static int svm_set_pi_irte_mode(struct kvm_vcpu *vcpu, bool activate)
++=======
+ static int avic_set_pi_irte_mode(struct kvm_vcpu *vcpu, bool activate)
++>>>>>>> abb6d479e226 (KVM: x86: make several APIC virtualization callbacks optional)
  {
  	int ret = 0;
  	unsigned long flags;
@@@ -659,59 -647,10 +663,63 @@@ void svm_refresh_apicv_exec_ctrl(struc
  	else
  		avic_vcpu_put(vcpu);
  
 -	avic_set_pi_irte_mode(vcpu, activated);
 +	svm_set_pi_irte_mode(vcpu, activated);
 +}
 +
++<<<<<<< HEAD
 +void svm_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)
 +{
 +	return;
 +}
 +
 +int svm_deliver_avic_intr(struct kvm_vcpu *vcpu, int vec)
 +{
 +	if (!vcpu->arch.apicv_active)
 +		return -1;
 +
 +	/*
 +	 * Pairs with the smp_mb_*() after setting vcpu->guest_mode in
 +	 * vcpu_enter_guest() to ensure the write to the vIRR is ordered before
 +	 * the read of guest_mode, which guarantees that either VMRUN will see
 +	 * and process the new vIRR entry, or that the below code will signal
 +	 * the doorbell if the vCPU is already running in the guest.
 +	 */
 +	smp_mb__after_atomic();
 +
 +	/*
 +	 * Signal the doorbell to tell hardware to inject the IRQ if the vCPU
 +	 * is in the guest.  If the vCPU is not in the guest, hardware will
 +	 * automatically process AVIC interrupts at VMRUN.
 +	 */
 +	if (vcpu->mode == IN_GUEST_MODE) {
 +		int cpu = READ_ONCE(vcpu->cpu);
 +
 +		/*
 +		 * Note, the vCPU could get migrated to a different pCPU at any
 +		 * point, which could result in signalling the wrong/previous
 +		 * pCPU.  But if that happens the vCPU is guaranteed to do a
 +		 * VMRUN (after being migrated) and thus will process pending
 +		 * interrupts, i.e. a doorbell is not needed (and the spurious
 +		 * one is harmless).
 +		 */
 +		if (cpu != get_cpu())
 +			wrmsrl(SVM_AVIC_DOORBELL, kvm_cpu_get_apicid(cpu));
 +		put_cpu();
 +	} else {
 +		/*
 +		 * Wake the vCPU if it was blocking.  KVM will then detect the
 +		 * pending IRQ when checking if the vCPU has a wake event.
 +		 */
 +		kvm_vcpu_wake_up(vcpu);
 +	}
 +
 +	return 0;
  }
  
 +bool svm_dy_apicv_has_pending_interrupt(struct kvm_vcpu *vcpu)
++=======
+ bool avic_dy_apicv_has_pending_interrupt(struct kvm_vcpu *vcpu)
++>>>>>>> abb6d479e226 (KVM: x86: make several APIC virtualization callbacks optional)
  {
  	return false;
  }
diff --cc arch/x86/kvm/svm/svm.c
index 3765ba922ed5,3daca34020fa..000000000000
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@@ -4520,13 -4589,9 +4520,19 @@@ static struct kvm_x86_ops svm_x86_ops _
  	.enable_nmi_window = svm_enable_nmi_window,
  	.enable_irq_window = svm_enable_irq_window,
  	.update_cr8_intercept = svm_update_cr8_intercept,
++<<<<<<< HEAD
 +	.set_virtual_apic_mode = svm_set_virtual_apic_mode,
 +	.refresh_apicv_exec_ctrl = svm_refresh_apicv_exec_ctrl,
 +	.check_apicv_inhibit_reasons = svm_check_apicv_inhibit_reasons,
 +	.load_eoi_exitmap = svm_load_eoi_exitmap,
 +	.hwapic_irr_update = svm_hwapic_irr_update,
 +	.hwapic_isr_update = svm_hwapic_isr_update,
 +	.apicv_post_state_restore = avic_post_state_restore,
++=======
+ 	.refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ 	.check_apicv_inhibit_reasons = avic_check_apicv_inhibit_reasons,
+ 	.apicv_post_state_restore = avic_apicv_post_state_restore,
++>>>>>>> abb6d479e226 (KVM: x86: make several APIC virtualization callbacks optional)
  
  	.set_tss_addr = svm_set_tss_addr,
  	.set_identity_map_addr = svm_set_identity_map_addr,
diff --cc arch/x86/kvm/svm/svm.h
index b72dd1b3a3c6,70850cbe5bcb..000000000000
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@@ -585,19 -578,18 +585,34 @@@ int avic_unaccelerated_access_intercept
  int avic_init_vcpu(struct vcpu_svm *svm);
  void avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
  void avic_vcpu_put(struct kvm_vcpu *vcpu);
++<<<<<<< HEAD
 +void avic_post_state_restore(struct kvm_vcpu *vcpu);
 +void svm_set_virtual_apic_mode(struct kvm_vcpu *vcpu);
 +void svm_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu);
 +bool svm_check_apicv_inhibit_reasons(ulong bit);
 +void svm_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);
 +void svm_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr);
 +void svm_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr);
 +int svm_deliver_avic_intr(struct kvm_vcpu *vcpu, int vec);
 +bool svm_dy_apicv_has_pending_interrupt(struct kvm_vcpu *vcpu);
 +int svm_update_pi_irte(struct kvm *kvm, unsigned int host_irq,
 +		       uint32_t guest_irq, bool set);
 +void svm_vcpu_blocking(struct kvm_vcpu *vcpu);
 +void svm_vcpu_unblocking(struct kvm_vcpu *vcpu);
++=======
+ void avic_apicv_post_state_restore(struct kvm_vcpu *vcpu);
+ void avic_set_virtual_apic_mode(struct kvm_vcpu *vcpu);
+ void avic_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu);
+ bool avic_check_apicv_inhibit_reasons(ulong bit);
+ void avic_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr);
+ void avic_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr);
+ bool avic_dy_apicv_has_pending_interrupt(struct kvm_vcpu *vcpu);
+ int avic_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
+ 			uint32_t guest_irq, bool set);
+ void avic_vcpu_blocking(struct kvm_vcpu *vcpu);
+ void avic_vcpu_unblocking(struct kvm_vcpu *vcpu);
+ void avic_ring_doorbell(struct kvm_vcpu *vcpu);
++>>>>>>> abb6d479e226 (KVM: x86: make several APIC virtualization callbacks optional)
  
  /* sev.c */
  
* Unmerged path arch/x86/include/asm/kvm-x86-ops.h
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 98df2c2dc106..ade2995f5fa5 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -492,8 +492,7 @@ static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 	if (unlikely(vcpu->arch.apicv_active)) {
 		/* need to update RVI */
 		kvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);
-		static_call(kvm_x86_hwapic_irr_update)(vcpu,
-				apic_find_highest_irr(apic));
+		static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));
 	} else {
 		apic->irr_pending = false;
 		kvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);
@@ -523,7 +522,7 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	 * just set SVI.
 	 */
 	if (unlikely(vcpu->arch.apicv_active))
-		static_call(kvm_x86_hwapic_isr_update)(vcpu, vec);
+		static_call_cond(kvm_x86_hwapic_isr_update)(vcpu, vec);
 	else {
 		++apic->isr_count;
 		BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
@@ -571,8 +570,7 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	 * and must be left alone.
 	 */
 	if (unlikely(vcpu->arch.apicv_active))
-		static_call(kvm_x86_hwapic_isr_update)(vcpu,
-						apic_find_highest_isr(apic));
+		static_call_cond(kvm_x86_hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
 	else {
 		--apic->isr_count;
 		BUG_ON(apic->isr_count < 0);
@@ -2288,7 +2286,7 @@ void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 		kvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);
 
 	if ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE))
-		static_call(kvm_x86_set_virtual_apic_mode)(vcpu);
+		static_call_cond(kvm_x86_set_virtual_apic_mode)(vcpu);
 
 	apic->base_address = apic->vcpu->arch.apic_base &
 			     MSR_IA32_APICBASE_BASE;
@@ -2374,9 +2372,9 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 	vcpu->arch.pv_eoi.msr_val = 0;
 	apic_update_ppr(apic);
 	if (vcpu->arch.apicv_active) {
-		static_call(kvm_x86_apicv_post_state_restore)(vcpu);
-		static_call(kvm_x86_hwapic_irr_update)(vcpu, -1);
-		static_call(kvm_x86_hwapic_isr_update)(vcpu, -1);
+		static_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);
+		static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, -1);
+		static_call_cond(kvm_x86_hwapic_isr_update)(vcpu, -1);
 	}
 
 	vcpu->arch.apic_arb_prio = 0;
@@ -2639,11 +2637,9 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	kvm_apic_update_apicv(vcpu);
 	apic->highest_isr_cache = -1;
 	if (vcpu->arch.apicv_active) {
-		static_call(kvm_x86_apicv_post_state_restore)(vcpu);
-		static_call(kvm_x86_hwapic_irr_update)(vcpu,
-				apic_find_highest_irr(apic));
-		static_call(kvm_x86_hwapic_isr_update)(vcpu,
-				apic_find_highest_isr(apic));
+		static_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);
+		static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));
+		static_call_cond(kvm_x86_hwapic_isr_update)(vcpu, apic_find_highest_isr(apic));
 	}
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 	if (ioapic_in_kernel(vcpu->kvm))
* Unmerged path arch/x86/kvm/svm/avic.c
* Unmerged path arch/x86/kvm/svm/svm.c
* Unmerged path arch/x86/kvm/svm/svm.h
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index a3ef48365669..ca73dbdc9cc5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -9763,11 +9763,11 @@ static void vcpu_load_eoi_exitmap(struct kvm_vcpu *vcpu)
 		bitmap_or((ulong *)eoi_exit_bitmap,
 			  vcpu->arch.ioapic_handled_vectors,
 			  to_hv_synic(vcpu)->vec_bitmap, 256);
-		static_call(kvm_x86_load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
+		static_call_cond(kvm_x86_load_eoi_exitmap)(vcpu, eoi_exit_bitmap);
 		return;
 	}
 
-	static_call(kvm_x86_load_eoi_exitmap)(
+	static_call_cond(kvm_x86_load_eoi_exitmap)(
 		vcpu, (u64 *)vcpu->arch.ioapic_handled_vectors);
 }
 
