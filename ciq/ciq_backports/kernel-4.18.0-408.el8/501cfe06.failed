KVM: SEV: unify cgroup cleanup code for svm_vm_migrate_from

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 501cfe06790651af4470d8ca77f162515867cd4d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/501cfe06.failed

Use the same cleanup code independent of whether the cgroup to be
uncharged and unref'd is the source or the destination cgroup.  Use a
bool to track whether the destination cgroup has been charged, which also
fixes a bug in the error case: the destination cgroup must be uncharged
only if it does not match the source.

Fixes: b56639318bb2 ("KVM: SEV: Add support for SEV intra host migration")
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 501cfe06790651af4470d8ca77f162515867cd4d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/svm/sev.c
diff --cc arch/x86/kvm/svm/sev.c
index d9c58c859be1,902c52a8dd0c..000000000000
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@@ -1545,6 -1544,201 +1545,204 @@@ static bool cmd_allowed_from_miror(u32 
  	return false;
  }
  
++<<<<<<< HEAD
++=======
+ static int sev_lock_for_migration(struct kvm *kvm)
+ {
+ 	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+ 
+ 	/*
+ 	 * Bail if this VM is already involved in a migration to avoid deadlock
+ 	 * between two VMs trying to migrate to/from each other.
+ 	 */
+ 	if (atomic_cmpxchg_acquire(&sev->migration_in_progress, 0, 1))
+ 		return -EBUSY;
+ 
+ 	mutex_lock(&kvm->lock);
+ 
+ 	return 0;
+ }
+ 
+ static void sev_unlock_after_migration(struct kvm *kvm)
+ {
+ 	struct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;
+ 
+ 	mutex_unlock(&kvm->lock);
+ 	atomic_set_release(&sev->migration_in_progress, 0);
+ }
+ 
+ 
+ static int sev_lock_vcpus_for_migration(struct kvm *kvm)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	int i, j;
+ 
+ 	kvm_for_each_vcpu(i, vcpu, kvm) {
+ 		if (mutex_lock_killable(&vcpu->mutex))
+ 			goto out_unlock;
+ 	}
+ 
+ 	return 0;
+ 
+ out_unlock:
+ 	kvm_for_each_vcpu(j, vcpu, kvm) {
+ 		if (i == j)
+ 			break;
+ 
+ 		mutex_unlock(&vcpu->mutex);
+ 	}
+ 	return -EINTR;
+ }
+ 
+ static void sev_unlock_vcpus_for_migration(struct kvm *kvm)
+ {
+ 	struct kvm_vcpu *vcpu;
+ 	int i;
+ 
+ 	kvm_for_each_vcpu(i, vcpu, kvm) {
+ 		mutex_unlock(&vcpu->mutex);
+ 	}
+ }
+ 
+ static void sev_migrate_from(struct kvm_sev_info *dst,
+ 			      struct kvm_sev_info *src)
+ {
+ 	dst->active = true;
+ 	dst->asid = src->asid;
+ 	dst->handle = src->handle;
+ 	dst->pages_locked = src->pages_locked;
+ 
+ 	src->asid = 0;
+ 	src->active = false;
+ 	src->handle = 0;
+ 	src->pages_locked = 0;
+ 
+ 	INIT_LIST_HEAD(&dst->regions_list);
+ 	list_replace_init(&src->regions_list, &dst->regions_list);
+ }
+ 
+ static int sev_es_migrate_from(struct kvm *dst, struct kvm *src)
+ {
+ 	int i;
+ 	struct kvm_vcpu *dst_vcpu, *src_vcpu;
+ 	struct vcpu_svm *dst_svm, *src_svm;
+ 
+ 	if (atomic_read(&src->online_vcpus) != atomic_read(&dst->online_vcpus))
+ 		return -EINVAL;
+ 
+ 	kvm_for_each_vcpu(i, src_vcpu, src) {
+ 		if (!src_vcpu->arch.guest_state_protected)
+ 			return -EINVAL;
+ 	}
+ 
+ 	kvm_for_each_vcpu(i, src_vcpu, src) {
+ 		src_svm = to_svm(src_vcpu);
+ 		dst_vcpu = kvm_get_vcpu(dst, i);
+ 		dst_svm = to_svm(dst_vcpu);
+ 
+ 		/*
+ 		 * Transfer VMSA and GHCB state to the destination.  Nullify and
+ 		 * clear source fields as appropriate, the state now belongs to
+ 		 * the destination.
+ 		 */
+ 		memcpy(&dst_svm->sev_es, &src_svm->sev_es, sizeof(src_svm->sev_es));
+ 		dst_svm->vmcb->control.ghcb_gpa = src_svm->vmcb->control.ghcb_gpa;
+ 		dst_svm->vmcb->control.vmsa_pa = src_svm->vmcb->control.vmsa_pa;
+ 		dst_vcpu->arch.guest_state_protected = true;
+ 
+ 		memset(&src_svm->sev_es, 0, sizeof(src_svm->sev_es));
+ 		src_svm->vmcb->control.ghcb_gpa = INVALID_PAGE;
+ 		src_svm->vmcb->control.vmsa_pa = INVALID_PAGE;
+ 		src_vcpu->arch.guest_state_protected = false;
+ 	}
+ 	to_kvm_svm(src)->sev_info.es_active = false;
+ 	to_kvm_svm(dst)->sev_info.es_active = true;
+ 
+ 	return 0;
+ }
+ 
+ int svm_vm_migrate_from(struct kvm *kvm, unsigned int source_fd)
+ {
+ 	struct kvm_sev_info *dst_sev = &to_kvm_svm(kvm)->sev_info;
+ 	struct kvm_sev_info *src_sev, *cg_cleanup_sev;
+ 	struct file *source_kvm_file;
+ 	struct kvm *source_kvm;
+ 	bool charged = false;
+ 	int ret;
+ 
+ 	ret = sev_lock_for_migration(kvm);
+ 	if (ret)
+ 		return ret;
+ 
+ 	if (sev_guest(kvm)) {
+ 		ret = -EINVAL;
+ 		goto out_unlock;
+ 	}
+ 
+ 	source_kvm_file = fget(source_fd);
+ 	if (!file_is_kvm(source_kvm_file)) {
+ 		ret = -EBADF;
+ 		goto out_fput;
+ 	}
+ 
+ 	source_kvm = source_kvm_file->private_data;
+ 	ret = sev_lock_for_migration(source_kvm);
+ 	if (ret)
+ 		goto out_fput;
+ 
+ 	if (!sev_guest(source_kvm)) {
+ 		ret = -EINVAL;
+ 		goto out_source;
+ 	}
+ 
+ 	src_sev = &to_kvm_svm(source_kvm)->sev_info;
+ 	dst_sev->misc_cg = get_current_misc_cg();
+ 	cg_cleanup_sev = dst_sev;
+ 	if (dst_sev->misc_cg != src_sev->misc_cg) {
+ 		ret = sev_misc_cg_try_charge(dst_sev);
+ 		if (ret)
+ 			goto out_dst_cgroup;
+ 		charged = true;
+ 	}
+ 
+ 	ret = sev_lock_vcpus_for_migration(kvm);
+ 	if (ret)
+ 		goto out_dst_cgroup;
+ 	ret = sev_lock_vcpus_for_migration(source_kvm);
+ 	if (ret)
+ 		goto out_dst_vcpu;
+ 
+ 	if (sev_es_guest(source_kvm)) {
+ 		ret = sev_es_migrate_from(kvm, source_kvm);
+ 		if (ret)
+ 			goto out_source_vcpu;
+ 	}
+ 	sev_migrate_from(dst_sev, src_sev);
+ 	kvm_vm_dead(source_kvm);
+ 	cg_cleanup_sev = src_sev;
+ 	ret = 0;
+ 
+ out_source_vcpu:
+ 	sev_unlock_vcpus_for_migration(source_kvm);
+ out_dst_vcpu:
+ 	sev_unlock_vcpus_for_migration(kvm);
+ out_dst_cgroup:
+ 	/* Operates on the source on success, on the destination on failure.  */
+ 	if (charged)
+ 		sev_misc_cg_uncharge(cg_cleanup_sev);
+ 	put_misc_cg(cg_cleanup_sev->misc_cg);
+ 	cg_cleanup_sev->misc_cg = NULL;
+ out_source:
+ 	sev_unlock_after_migration(source_kvm);
+ out_fput:
+ 	if (source_kvm_file)
+ 		fput(source_kvm_file);
+ out_unlock:
+ 	sev_unlock_after_migration(kvm);
+ 	return ret;
+ }
+ 
++>>>>>>> 501cfe067906 (KVM: SEV: unify cgroup cleanup code for svm_vm_migrate_from)
  int svm_mem_enc_op(struct kvm *kvm, void __user *argp)
  {
  	struct kvm_sev_cmd sev_cmd;
* Unmerged path arch/x86/kvm/svm/sev.c
