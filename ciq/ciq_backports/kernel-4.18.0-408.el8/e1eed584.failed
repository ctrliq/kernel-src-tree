KVM: x86/mmu: Allow yielding during MMU notifier unmap/zap, if possible

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit e1eed5847b09fe41d4db4b86f9d840aba869c905
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/e1eed584.failed

Let the TDP MMU yield when unmapping a range in response to a MMU
notification, if yielding is allowed by said notification.  There is no
reason to disallow yielding in this case, and in theory the range being
invalidated could be quite large.

	Cc: Ben Gardon <bgardon@google.com>
	Signed-off-by: Sean Christopherson <seanjc@google.com>
Message-Id: <20210402005658.3024832-11-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit e1eed5847b09fe41d4db4b86f9d840aba869c905)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 83e4cca9867b,10cb16c55dd0..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -913,47 -873,42 +913,76 @@@ int kvm_tdp_mmu_map(struct kvm_vcpu *vc
  	return ret;
  }
  
 -bool kvm_tdp_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range,
 -				 bool flush)
 +typedef int (*tdp_handler_t)(struct kvm *kvm, struct kvm_memory_slot *slot,
 +			     struct kvm_mmu_page *root, gfn_t start, gfn_t end,
 +			     unsigned long data);
 +
 +static __always_inline int kvm_tdp_mmu_handle_hva_range(struct kvm *kvm,
 +							unsigned long start,
 +							unsigned long end,
 +							unsigned long data,
 +							tdp_handler_t handler)
  {
 +	struct kvm_memslots *slots;
 +	struct kvm_memory_slot *memslot;
  	struct kvm_mmu_page *root;
 -
 +	int ret = 0;
 +	int as_id;
 +
++<<<<<<< HEAD
 +	for (as_id = 0; as_id < KVM_ADDRESS_SPACE_NUM; as_id++) {
 +		for_each_tdp_mmu_root_yield_safe(kvm, root, as_id) {
 +			slots = __kvm_memslots(kvm, as_id);
 +			kvm_for_each_memslot(memslot, slots) {
 +				unsigned long hva_start, hva_end;
 +				gfn_t gfn_start, gfn_end;
++=======
+ 	for_each_tdp_mmu_root(kvm, root, range->slot->as_id)
+ 		flush |= zap_gfn_range(kvm, root, range->start, range->end,
+ 				       range->may_block, flush);
 -
 -	return flush;
 -}
 -
++>>>>>>> e1eed5847b09 (KVM: x86/mmu: Allow yielding during MMU notifier unmap/zap, if possible)
 +
 +				hva_start = max(start, memslot->userspace_addr);
 +				hva_end = min(end, memslot->userspace_addr +
 +					(memslot->npages << PAGE_SHIFT));
 +				if (hva_start >= hva_end)
 +					continue;
 +				/*
 +				 * {gfn(page) | page intersects with [hva_start, hva_end)} =
 +				 * {gfn_start, gfn_start+1, ..., gfn_end-1}.
 +				 */
 +				gfn_start = hva_to_gfn_memslot(hva_start, memslot);
 +				gfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);
 +
++<<<<<<< HEAD
 +				ret |= handler(kvm, memslot, root, gfn_start,
 +					gfn_end, data);
 +			}
 +		}
++=======
+ typedef bool (*tdp_handler_t)(struct kvm *kvm, struct tdp_iter *iter,
+ 			      struct kvm_gfn_range *range);
+ 
+ static __always_inline bool kvm_tdp_mmu_handle_gfn(struct kvm *kvm,
+ 						   struct kvm_gfn_range *range,
+ 						   tdp_handler_t handler)
+ {
+ 	struct kvm_mmu_page *root;
+ 	struct tdp_iter iter;
+ 	bool ret = false;
+ 
+ 	rcu_read_lock();
+ 
+ 	/*
+ 	 * Don't support rescheduling, none of the MMU notifiers that funnel
+ 	 * into this helper allow blocking; it'd be dead, wasteful code.
+ 	 */
+ 	for_each_tdp_mmu_root(kvm, root, range->slot->as_id) {
+ 		tdp_root_for_each_leaf_pte(iter, root, range->start, range->end)
+ 			ret |= handler(kvm, &iter, range);
++>>>>>>> e1eed5847b09 (KVM: x86/mmu: Allow yielding during MMU notifier unmap/zap, if possible)
  	}
  
 -	rcu_read_unlock();
 -
  	return ret;
  }
  
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
