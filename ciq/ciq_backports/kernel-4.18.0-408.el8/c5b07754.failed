KVM: Convert the kvm->vcpus array to a xarray

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Marc Zyngier <maz@kernel.org>
commit c5b077549136584618a66258f09d8d4b41e7409c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/c5b07754.failed

At least on arm64 and x86, the vcpus array is pretty huge (up to
1024 entries on x86) and is mostly empty in the majority of the cases
(running 1k vcpu VMs is not that common).

This mean that we end-up with a 4kB block of unused memory in the
middle of the kvm structure.

Instead of wasting away this memory, let's use an xarray instead,
which gives us almost the same flexibility as a normal array, but
with a reduced memory usage with smaller VMs.

	Signed-off-by: Marc Zyngier <maz@kernel.org>
Message-Id: <20211116160403.4074052-6-maz@kernel.org>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit c5b077549136584618a66258f09d8d4b41e7409c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	virt/kvm/kvm_main.c
diff --cc virt/kvm/kvm_main.c
index 13b1084d09e4,594f90307b20..000000000000
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@@ -470,7 -450,20 +470,24 @@@ void kvm_vcpu_destroy(struct kvm_vcpu *
  	free_page((unsigned long)vcpu->run);
  	kmem_cache_free(kvm_vcpu_cache, vcpu);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(kvm_vcpu_destroy);
++=======
+ 
+ void kvm_destroy_vcpus(struct kvm *kvm)
+ {
+ 	unsigned int i;
+ 	struct kvm_vcpu *vcpu;
+ 
+ 	kvm_for_each_vcpu(i, vcpu, kvm) {
+ 		kvm_vcpu_destroy(vcpu);
+ 		xa_erase(&kvm->vcpu_array, i);
+ 	}
+ 
+ 	atomic_set(&kvm->online_vcpus, 0);
+ }
+ EXPORT_SYMBOL_GPL(kvm_destroy_vcpus);
++>>>>>>> c5b077549136 (KVM: Convert the kvm->vcpus array to a xarray)
  
  #if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)
  static inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)
@@@ -929,6 -1061,10 +946,13 @@@ static struct kvm *kvm_create_vm(unsign
  	mutex_init(&kvm->irq_lock);
  	mutex_init(&kvm->slots_lock);
  	mutex_init(&kvm->slots_arch_lock);
++<<<<<<< HEAD
++=======
+ 	spin_lock_init(&kvm->mn_invalidate_lock);
+ 	rcuwait_init(&kvm->mn_memslots_update_rcuwait);
+ 	xa_init(&kvm->vcpu_array);
+ 
++>>>>>>> c5b077549136 (KVM: Convert the kvm->vcpus array to a xarray)
  	INIT_LIST_HEAD(&kvm->devices);
  
  	BUILD_BUG_ON(KVM_MEM_SLOTS_NUM > SHRT_MAX);
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 470cdd19398b..615d909c194a 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -32,6 +32,7 @@
 #include <linux/refcount.h>
 #include <linux/nospec.h>
 #include <linux/notifier.h>
+#include <linux/xarray.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -504,7 +505,7 @@ struct kvm {
 	struct mutex slots_arch_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
 	struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
-	struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];
+	struct xarray vcpu_array;
 
 	/*
 	 * created_vcpus is protected by kvm->lock, and is incremented
@@ -649,7 +650,7 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 
 	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu.  */
 	smp_rmb();
-	return kvm->vcpus[i];
+	return xa_load(&kvm->vcpu_array, i);
 }
 
 #define kvm_for_each_vcpu(idx, vcpup, kvm) \
* Unmerged path virt/kvm/kvm_main.c
