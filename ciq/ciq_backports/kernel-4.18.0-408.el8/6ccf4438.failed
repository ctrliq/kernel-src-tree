KVM: MMU: unify tdp_mmu_map_set_spte_atomic and tdp_mmu_set_spte_atomic_no_dirty_log

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 6ccf44388206e60bd0ba46d00f8570a0588d812e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/6ccf4438.failed

tdp_mmu_map_set_spte_atomic is not taking care of dirty logging anymore,
the only difference that remains is that it takes a vCPU instead of
the struct kvm.  Merge the two functions.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 6ccf44388206e60bd0ba46d00f8570a0588d812e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index bc5924587138,2d92a5b54ded..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -494,8 -489,9 +494,14 @@@ static void handle_changed_spte(struct 
  }
  
  /*
++<<<<<<< HEAD
 + * tdp_mmu_set_spte_atomic - Set a TDP MMU SPTE atomically and handle the
 + * associated bookkeeping
++=======
+  * tdp_mmu_set_spte_atomic - Set a TDP MMU SPTE atomically
+  * and handle the associated bookkeeping.  Do not mark the page dirty
+  * in KVM's dirty bitmaps.
++>>>>>>> 6ccf44388206 (KVM: MMU: unify tdp_mmu_map_set_spte_atomic and tdp_mmu_set_spte_atomic_no_dirty_log)
   *
   * @kvm: kvm instance
   * @iter: a tdp_iter instance currently on the SPTE that should be set
@@@ -984,11 -1002,10 +990,18 @@@ int kvm_tdp_mmu_map(struct kvm_vcpu *vc
  			new_spte = make_nonleaf_spte(child_pt,
  						     !shadow_accessed_mask);
  
++<<<<<<< HEAD
 +			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter,
 +						    new_spte)) {
 +				tdp_mmu_link_page(vcpu->kvm, sp, true,
 +						  huge_page_disallowed &&
 +						  req_level >= iter.level);
++=======
+ 			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter, new_spte)) {
+ 				tdp_mmu_link_page(vcpu->kvm, sp,
+ 						  fault->huge_page_disallowed &&
+ 						  fault->req_level >= iter.level);
++>>>>>>> 6ccf44388206 (KVM: MMU: unify tdp_mmu_map_set_spte_atomic and tdp_mmu_set_spte_atomic_no_dirty_log)
  
  				trace_kvm_mmu_get_page(sp, true);
  			} else {
@@@ -1238,7 -1190,14 +1251,18 @@@ static bool wrprot_gfn_range(struct kv
  
  		new_spte = iter.old_spte & ~PT_WRITABLE_MASK;
  
++<<<<<<< HEAD
 +		tdp_mmu_set_spte_no_dirty_log(kvm, &iter, new_spte);
++=======
+ 		if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ 			/*
+ 			 * The iter must explicitly re-read the SPTE because
+ 			 * the atomic cmpxchg failed.
+ 			 */
+ 			iter.old_spte = READ_ONCE(*rcu_dereference(iter.sptep));
+ 			goto retry;
+ 		}
++>>>>>>> 6ccf44388206 (KVM: MMU: unify tdp_mmu_map_set_spte_atomic and tdp_mmu_set_spte_atomic_no_dirty_log)
  		spte_set = true;
  	}
  
@@@ -1296,7 -1258,14 +1320,18 @@@ static bool clear_dirty_gfn_range(struc
  				continue;
  		}
  
++<<<<<<< HEAD
 +		tdp_mmu_set_spte_no_dirty_log(kvm, &iter, new_spte);
++=======
+ 		if (!tdp_mmu_set_spte_atomic(kvm, &iter, new_spte)) {
+ 			/*
+ 			 * The iter must explicitly re-read the SPTE because
+ 			 * the atomic cmpxchg failed.
+ 			 */
+ 			iter.old_spte = READ_ONCE(*rcu_dereference(iter.sptep));
+ 			goto retry;
+ 		}
++>>>>>>> 6ccf44388206 (KVM: MMU: unify tdp_mmu_map_set_spte_atomic and tdp_mmu_set_spte_atomic_no_dirty_log)
  		spte_set = true;
  	}
  
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
