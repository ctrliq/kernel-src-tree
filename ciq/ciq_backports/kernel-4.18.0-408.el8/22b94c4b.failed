KVM: x86/mmu: Zap invalidated roots via asynchronous worker

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit 22b94c4b63ebf2cf976d6f4eba230105984a7eb6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/22b94c4b.failed

Use the system worker threads to zap the roots invalidated
by the TDP MMU's "fast zap" mechanism, implemented by
kvm_tdp_mmu_invalidate_all_roots().

At this point, apart from allowing some parallelism in the zapping of
roots, the workqueue is a glorified linked list: work items are added and
flushed entirely within a single kvm->slots_lock critical section.  However,
the workqueue fixes a latent issue where kvm_mmu_zap_all_invalidated_roots()
assumes that it owns a reference to all invalid roots; therefore, no
one can set the invalid bit outside kvm_mmu_zap_all_fast().  Putting the
invalidated roots on a linked list... erm, on a workqueue ensures that
tdp_mmu_zap_root_work() only puts back those extra references that
kvm_mmu_zap_all_invalidated_roots() had gifted to it.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 22b94c4b63ebf2cf976d6f4eba230105984a7eb6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu_internal.h
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/mmu_internal.h
index a81dffc914d9,1bff453f7cbe..000000000000
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@@ -59,8 -61,17 +59,22 @@@ struct kvm_mmu_page 
  		refcount_t tdp_mmu_root_count;
  	};
  	unsigned int unsync_children;
++<<<<<<< HEAD
 +	struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
 +	DECLARE_BITMAP(unsync_child_bitmap, 512);
++=======
+ 	union {
+ 		struct kvm_rmap_head parent_ptes; /* rmap pointers to parent sptes */
+ 		tdp_ptep_t ptep;
+ 	};
+ 	union {
+ 		DECLARE_BITMAP(unsync_child_bitmap, 512);
+ 		struct {
+ 			struct work_struct tdp_mmu_async_work;
+ 			void *tdp_mmu_async_data;
+ 		};
+ 	};
++>>>>>>> 22b94c4b63eb (KVM: x86/mmu: Zap invalidated roots via asynchronous worker)
  
  	struct list_head lpage_disallowed_link;
  #ifdef CONFIG_X86_32
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 97bb57fe39ca,7c17b6a4f30f..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -25,6 -25,22 +25,25 @@@ void kvm_mmu_init_tdp_mmu(struct kvm *k
  	INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
  	spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
  	INIT_LIST_HEAD(&kvm->arch.tdp_mmu_pages);
++<<<<<<< HEAD
++=======
+ 	kvm->arch.tdp_mmu_zap_wq =
+ 		alloc_workqueue("kvm", WQ_UNBOUND|WQ_MEM_RECLAIM|WQ_CPU_INTENSIVE, 0);
+ 
+ 	return true;
+ }
+ 
+ /* Arbitrarily returns true so that this may be used in if statements. */
+ static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
+ 							     bool shared)
+ {
+ 	if (shared)
+ 		lockdep_assert_held_read(&kvm->mmu_lock);
+ 	else
+ 		lockdep_assert_held_write(&kvm->mmu_lock);
+ 
+ 	return true;
++>>>>>>> 22b94c4b63eb (KVM: x86/mmu: Zap invalidated roots via asynchronous worker)
  }
  
  void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
@@@ -32,6 -48,10 +51,13 @@@
  	if (!kvm->arch.tdp_mmu_enabled)
  		return;
  
++<<<<<<< HEAD
++=======
+ 	flush_workqueue(kvm->arch.tdp_mmu_zap_wq);
+ 	destroy_workqueue(kvm->arch.tdp_mmu_zap_wq);
+ 
+ 	WARN_ON(!list_empty(&kvm->arch.tdp_mmu_pages));
++>>>>>>> 22b94c4b63eb (KVM: x86/mmu: Zap invalidated roots via asynchronous worker)
  	WARN_ON(!list_empty(&kvm->arch.tdp_mmu_roots));
  
  	/*
@@@ -66,11 -84,50 +93,56 @@@ static void tdp_mmu_free_sp_rcu_callbac
  	tdp_mmu_free_sp(sp);
  }
  
++<<<<<<< HEAD
 +void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root)
++=======
+ static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			     bool shared);
+ 
+ static void tdp_mmu_zap_root_work(struct work_struct *work)
+ {
+ 	struct kvm_mmu_page *root = container_of(work, struct kvm_mmu_page,
+ 						 tdp_mmu_async_work);
+ 	struct kvm *kvm = root->tdp_mmu_async_data;
+ 
+ 	read_lock(&kvm->mmu_lock);
+ 
+ 	/*
+ 	 * A TLB flush is not necessary as KVM performs a local TLB flush when
+ 	 * allocating a new root (see kvm_mmu_load()), and when migrating vCPU
+ 	 * to a different pCPU.  Note, the local TLB flush on reuse also
+ 	 * invalidates any paging-structure-cache entries, i.e. TLB entries for
+ 	 * intermediate paging structures, that may be zapped, as such entries
+ 	 * are associated with the ASID on both VMX and SVM.
+ 	 */
+ 	tdp_mmu_zap_root(kvm, root, true);
+ 
+ 	/*
+ 	 * Drop the refcount using kvm_tdp_mmu_put_root() to test its logic for
+ 	 * avoiding an infinite loop.  By design, the root is reachable while
+ 	 * it's being asynchronously zapped, thus a different task can put its
+ 	 * last reference, i.e. flowing through kvm_tdp_mmu_put_root() for an
+ 	 * asynchronously zapped root is unavoidable.
+ 	 */
+ 	kvm_tdp_mmu_put_root(kvm, root, true);
+ 
+ 	read_unlock(&kvm->mmu_lock);
+ }
+ 
+ static void tdp_mmu_schedule_zap_root(struct kvm *kvm, struct kvm_mmu_page *root)
+ {
+ 	root->tdp_mmu_async_data = kvm;
+ 	INIT_WORK(&root->tdp_mmu_async_work, tdp_mmu_zap_root_work);
+ 	queue_work(kvm->arch.tdp_mmu_zap_wq, &root->tdp_mmu_async_work);
+ }
+ 
+ void kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,
+ 			  bool shared)
++>>>>>>> 22b94c4b63eb (KVM: x86/mmu: Zap invalidated roots via asynchronous worker)
  {
 -	kvm_lockdep_assert_mmu_lock_held(kvm, shared);
 +	gfn_t max_gfn = 1ULL << (shadow_phys_bits - PAGE_SHIFT);
 +
 +	lockdep_assert_held_write(&kvm->mmu_lock);
  
  	if (!refcount_dec_and_test(&root->tdp_mmu_root_count))
  		return;
@@@ -767,37 -931,25 +839,34 @@@ bool __kvm_tdp_mmu_zap_gfn_range(struc
  
  void kvm_tdp_mmu_zap_all(struct kvm *kvm)
  {
 -	struct kvm_mmu_page *root;
 +	gfn_t max_gfn = 1ULL << (shadow_phys_bits - PAGE_SHIFT);
 +	bool flush = false;
  	int i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++)
 +		flush = kvm_tdp_mmu_zap_gfn_range(kvm, i, 0, max_gfn, flush);
 +
 +	if (flush)
 +		kvm_flush_remote_tlbs(kvm);
- }
- 
- static struct kvm_mmu_page *next_invalidated_root(struct kvm *kvm,
- 						  struct kvm_mmu_page *prev_root)
- {
- 	struct kvm_mmu_page *next_root;
- 
- 	if (prev_root)
- 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
- 						  &prev_root->link,
- 						  typeof(*prev_root), link);
- 	else
- 		next_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,
- 						   typeof(*next_root), link);
- 
- 	while (next_root && !(next_root->role.invalid &&
- 			      refcount_read(&next_root->tdp_mmu_root_count)))
- 		next_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,
- 						  &next_root->link,
- 						  typeof(*next_root), link);
- 
- 	return next_root;
++=======
+ 	/*
+ 	 * Zap all roots, including invalid roots, as all SPTEs must be dropped
+ 	 * before returning to the caller.  Zap directly even if the root is
+ 	 * also being zapped by a worker.  Walking zapped top-level SPTEs isn't
+ 	 * all that expensive and mmu_lock is already held, which means the
+ 	 * worker has yielded, i.e. flushing the work instead of zapping here
+ 	 * isn't guaranteed to be any faster.
+ 	 *
+ 	 * A TLB flush is unnecessary, KVM zaps everything if and only the VM
+ 	 * is being destroyed or the userspace VMM has exited.  In both cases,
+ 	 * KVM_RUN is unreachable, i.e. no vCPUs will ever service the request.
+ 	 */
+ 	for (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {
+ 		for_each_tdp_mmu_root_yield_safe(kvm, root, i)
+ 			tdp_mmu_zap_root(kvm, root, false);
+ 	}
++>>>>>>> 22b94c4b63eb (KVM: x86/mmu: Zap invalidated roots via asynchronous worker)
  }
  
  /*
@@@ -809,40 -958,7 +875,44 @@@
   */
  void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
  {
++<<<<<<< HEAD
 +	gfn_t max_gfn = 1ULL << (shadow_phys_bits - PAGE_SHIFT);
 +	struct kvm_mmu_page *next_root;
 +	struct kvm_mmu_page *root;
 +	bool flush = false;
 +
 +	lockdep_assert_held_read(&kvm->mmu_lock);
 +
 +	rcu_read_lock();
 +
 +	root = next_invalidated_root(kvm, NULL);
 +
 +	while (root) {
 +		next_root = next_invalidated_root(kvm, root);
 +
 +		rcu_read_unlock();
 +
 +		flush = zap_gfn_range(kvm, root, 0, max_gfn, true, flush,
 +				      true);
 +
 +		/*
 +		 * Put the reference acquired in
 +		 * kvm_tdp_mmu_invalidate_roots
 +		 */
 +		kvm_tdp_mmu_put_root(kvm, root, true);
 +
 +		root = next_root;
 +
 +		rcu_read_lock();
 +	}
 +
 +	rcu_read_unlock();
 +
 +	if (flush)
 +		kvm_flush_remote_tlbs(kvm);
++=======
+ 	flush_workqueue(kvm->arch.tdp_mmu_zap_wq);
++>>>>>>> 22b94c4b63eb (KVM: x86/mmu: Zap invalidated roots via asynchronous worker)
  }
  
  /*
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 96ecf6f2c1a5..4ee879e98396 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -18,6 +18,7 @@
 #include <linux/cpumask.h>
 #include <linux/irq_work.h>
 #include <linux/irq.h>
+#include <linux/workqueue.h>
 
 #include <linux/kvm.h>
 #include <linux/kvm_para.h>
@@ -1208,6 +1209,7 @@ struct kvm_arch {
 	 * the thread holds the MMU lock in write mode.
 	 */
 	spinlock_t tdp_mmu_pages_lock;
+	struct workqueue_struct *tdp_mmu_zap_wq;
 #endif /* CONFIG_X86_64 */
 
 	/*
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index ad983809b045..1e7792405fcb 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -5648,11 +5648,8 @@ static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 	 * Deferring the zap until the final reference to the root is put would
 	 * lead to use-after-free.
 	 */
-	if (is_tdp_mmu_enabled(kvm)) {
-		read_lock(&kvm->mmu_lock);
+	if (is_tdp_mmu_enabled(kvm))
 		kvm_tdp_mmu_zap_invalidated_roots(kvm);
-		read_unlock(&kvm->mmu_lock);
-	}
 }
 
 static bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)
* Unmerged path arch/x86/kvm/mmu/mmu_internal.h
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
