blk-mq: move blk_mq_flush_plug_list

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
Rebuild_CHGLOG: - blk-mq: fix blk_mq_flush_plug_list (Ming Lei) [2088397]
Rebuild_FUZZ: 89.86%
commit-author Christoph Hellwig <hch@lst.de>
commit b84c5b50d329bf7cfdba6bd5c8a99f1b8604e301
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/b84c5b50.failed

Move blk_mq_flush_plug_list and blk_mq_plug_issue_direct down in blk-mq.c
to prepare for marking blk_mq_request_issue_directly static without the
need of a forward declaration.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
Link: https://lore.kernel.org/r/20211117061404.331732-5-hch@lst.de
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit b84c5b50d329bf7cfdba6bd5c8a99f1b8604e301)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-mq.c
diff --cc block/blk-mq.c
index d59c2d4474e5,df28e5ef8c2d..000000000000
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@@ -1930,57 -2299,21 +1930,62 @@@ void blk_mq_insert_requests(struct blk_
  	spin_unlock(&ctx->lock);
  }
  
 -static void blk_mq_commit_rqs(struct blk_mq_hw_ctx *hctx, int *queued,
 -			      bool from_schedule)
 +static int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)
  {
 -	if (hctx->queue->mq_ops->commit_rqs) {
 -		trace_block_unplug(hctx->queue, *queued, !from_schedule);
 -		hctx->queue->mq_ops->commit_rqs(hctx);
 -	}
 -	*queued = 0;
 +	struct request *rqa = container_of(a, struct request, queuelist);
 +	struct request *rqb = container_of(b, struct request, queuelist);
 +
++<<<<<<< HEAD
 +	if (rqa->mq_ctx != rqb->mq_ctx)
 +		return rqa->mq_ctx > rqb->mq_ctx;
 +	if (rqa->mq_hctx != rqb->mq_hctx)
 +		return rqa->mq_hctx > rqb->mq_hctx;
 +
 +	return blk_rq_pos(rqa) > blk_rq_pos(rqb);
 +}
 +
 +void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 +{
 +	LIST_HEAD(list);
 +
 +	if (list_empty(&plug->mq_list))
 +		return;
 +	list_splice_init(&plug->mq_list, &list);
 +
 +	if (plug->rq_count > 2 && plug->multiple_queues)
 +		list_sort(NULL, &list, plug_rq_cmp);
 +
 +	plug->rq_count = 0;
 +
 +	do {
 +		struct list_head rq_list;
 +		struct request *rq, *head_rq = list_entry_rq(list.next);
 +		struct list_head *pos = &head_rq->queuelist; /* skip first */
 +		struct blk_mq_hw_ctx *this_hctx = head_rq->mq_hctx;
 +		struct blk_mq_ctx *this_ctx = head_rq->mq_ctx;
 +		unsigned int depth = 1;
 +
 +		list_for_each_continue(pos, &list) {
 +			rq = list_entry_rq(pos);
 +			BUG_ON(!rq->q);
 +			if (rq->mq_hctx != this_hctx || rq->mq_ctx != this_ctx)
 +				break;
 +			depth++;
 +		}
 +
 +		list_cut_before(&rq_list, &list, pos);
 +		trace_block_unplug(head_rq->q, depth, !from_schedule);
 +		blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
 +						from_schedule);
 +	} while(!list_empty(&list));
  }
  
 +static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
++=======
+ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio,
+ 		unsigned int nr_segs)
++>>>>>>> b84c5b50d329 (blk-mq: move blk_mq_flush_plug_list)
  {
 -	int err;
 -
  	if (bio->bi_opf & REQ_RAHEAD)
  		rq->cmd_flags |= REQ_FAILFAST_MASK;
  
* Unmerged path block/blk-mq.c
