x86/hyperv: Add Write/Read MSR registers via ghcb page

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Tianyu Lan <Tianyu.Lan@microsoft.com>
commit faff44069ff538ccdfef187c4d7ec83d22dfb3a4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/faff4406.failed

Hyperv provides GHCB protocol to write Synthetic Interrupt
Controller MSR registers in Isolation VM with AMD SEV SNP
and these registers are emulated by hypervisor directly.
Hyperv requires to write SINTx MSR registers twice. First
writes MSR via GHCB page to communicate with hypervisor
and then writes wrmsr instruction to talk with paravisor
which runs in VMPL0. Guest OS ID MSR also needs to be set
via GHCB page.

	Reviewed-by: Michael Kelley <mikelley@microsoft.com>
	Signed-off-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
Link: https://lore.kernel.org/r/20211025122116.264793-7-ltykernel@gmail.com
	Signed-off-by: Wei Liu <wei.liu@kernel.org>
(cherry picked from commit faff44069ff538ccdfef187c4d7ec83d22dfb3a4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/hyperv/hv_init.c
#	arch/x86/hyperv/ivm.c
#	arch/x86/include/asm/mshyperv.h
diff --cc arch/x86/hyperv/hv_init.c
index a366a7958291,a16a83e46a30..000000000000
--- a/arch/x86/hyperv/hv_init.c
+++ b/arch/x86/hyperv/hv_init.c
@@@ -45,6 -37,8 +45,11 @@@ EXPORT_SYMBOL_GPL(hv_current_partition_
  void *hv_hypercall_pg;
  EXPORT_SYMBOL_GPL(hv_hypercall_pg);
  
++<<<<<<< HEAD
++=======
+ union hv_ghcb __percpu **hv_ghcb_pg;
+ 
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
  /* Storage to save the hypercall page temporarily for hibernation */
  static void *hv_hypercall_pg_saved;
  
@@@ -356,6 -405,12 +361,15 @@@ void __init hyperv_init(void
  		goto common_free;
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (hv_isolation_type_snp()) {
+ 		hv_ghcb_pg = alloc_percpu(union hv_ghcb *);
+ 		if (!hv_ghcb_pg)
+ 			goto free_vp_assist_page;
+ 	}
+ 
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
  	cpuhp = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, "x86/hyperv_init:online",
  				  hv_cpu_init, hv_cpu_die);
  	if (cpuhp < 0)
@@@ -369,13 -424,15 +383,16 @@@
  	guest_id = generate_guest_id(0, LINUX_VERSION_CODE, 0);
  	wrmsrl(HV_X64_MSR_GUEST_OS_ID, guest_id);
  
+ 	/* Hyper-V requires to write guest os id via ghcb in SNP IVM. */
+ 	hv_ghcb_msr_write(HV_X64_MSR_GUEST_OS_ID, guest_id);
+ 
  	hv_hypercall_pg = __vmalloc_node_range(PAGE_SIZE, 1, VMALLOC_START,
  			VMALLOC_END, GFP_KERNEL, PAGE_KERNEL_ROX,
 -			VM_FLUSH_RESET_PERMS, NUMA_NO_NODE,
 -			__builtin_return_address(0));
 -	if (hv_hypercall_pg == NULL)
 -		goto clean_guest_os_id;
 +			VM_FLUSH_RESET_PERMS, NUMA_NO_NODE, __func__);
 +	if (hv_hypercall_pg == NULL) {
 +		wrmsrl(HV_X64_MSR_GUEST_OS_ID, 0);
 +		goto remove_cpuhp_state;
 +	}
  
  	rdmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);
  	hypercall_msr.enable = 1;
@@@ -436,8 -502,12 +453,14 @@@
  	hv_query_ext_cap(0);
  	return;
  
++<<<<<<< HEAD
 +remove_cpuhp_state:
++=======
+ clean_guest_os_id:
+ 	wrmsrl(HV_X64_MSR_GUEST_OS_ID, 0);
+ 	hv_ghcb_msr_write(HV_X64_MSR_GUEST_OS_ID, 0);
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
  	cpuhp_remove_state(cpuhp);
 -free_ghcb_page:
 -	free_percpu(hv_ghcb_pg);
  free_vp_assist_page:
  	kfree(hv_vp_assist_page);
  	hv_vp_assist_page = NULL;
@@@ -526,16 -597,3 +550,19 @@@ bool hv_is_hyperv_initialized(void
  	return hypercall_msr.enable;
  }
  EXPORT_SYMBOL_GPL(hv_is_hyperv_initialized);
++<<<<<<< HEAD
 +
 +enum hv_isolation_type hv_get_isolation_type(void)
 +{
 +	if (!(ms_hyperv.priv_high & HV_ISOLATION))
 +		return HV_ISOLATION_TYPE_NONE;
 +	return FIELD_GET(HV_ISOLATION_TYPE, ms_hyperv.isolation_config_b);
 +}
 +EXPORT_SYMBOL_GPL(hv_get_isolation_type);
 +
 +bool hv_is_isolation_supported(void)
 +{
 +	return hv_get_isolation_type() != HV_ISOLATION_TYPE_NONE;
 +}
++=======
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
diff --cc arch/x86/include/asm/mshyperv.h
index 50ff5ae8218d,da3972fe5a7a..000000000000
--- a/arch/x86/include/asm/mshyperv.h
+++ b/arch/x86/include/asm/mshyperv.h
@@@ -11,40 -11,18 +11,34 @@@
  #include <asm/paravirt.h>
  #include <asm/mshyperv.h>
  
++<<<<<<< HEAD
++=======
+ union hv_ghcb;
+ 
+ DECLARE_STATIC_KEY_FALSE(isolation_type_snp);
+ 
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
  typedef int (*hyperv_fill_flush_list_func)(
  		struct hv_guest_mapping_flush_list *flush,
  		void *data);
  
- static inline void hv_set_register(unsigned int reg, u64 value)
- {
- 	wrmsrl(reg, value);
- }
- 
- static inline u64 hv_get_register(unsigned int reg)
- {
- 	u64 value;
- 
- 	rdmsrl(reg, value);
- 	return value;
- }
- 
  #define hv_get_raw_timer() rdtsc_ordered()
  
 +void hyperv_callback_vector(void);
 +void hyperv_reenlightenment_vector(void);
 +#ifdef CONFIG_TRACING
 +#define trace_hyperv_callback_vector hyperv_callback_vector
 +#endif
 +
  void hyperv_vector_handler(struct pt_regs *regs);
  
 +/*
 + * Routines for stimer0 Direct Mode handling.
 + * On x86/x64, there are no percpu actions to take.
 + */
 +void hv_stimer0_vector_handler(struct pt_regs *regs);
 +void hv_stimer0_callback_vector(void);
 +
  #if IS_ENABLED(CONFIG_HYPERV)
  extern int hyperv_init_cpuhp;
  
@@@ -52,6 -30,8 +46,11 @@@ extern void *hv_hypercall_pg
  
  extern u64 hv_current_partition_id;
  
++<<<<<<< HEAD
++=======
+ extern union hv_ghcb  __percpu **hv_ghcb_pg;
+ 
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
  int hv_call_deposit_pages(int node, u64 partition_id, u32 num_pages);
  int hv_call_add_logical_proc(int node, u32 lp_index, u32 acpi_id);
  int hv_call_create_vp(int node, u64 partition_id, u32 vp_index, u32 flags);
@@@ -189,6 -169,63 +188,66 @@@ bool hv_vcpu_is_preempted(int vcpu)
  static inline void hv_apic_init(void) {}
  #endif
  
++<<<<<<< HEAD
++=======
+ static inline void hv_set_msi_entry_from_desc(union hv_msi_entry *msi_entry,
+ 					      struct msi_desc *msi_desc)
+ {
+ 	msi_entry->address.as_uint32 = msi_desc->msg.address_lo;
+ 	msi_entry->data.as_uint32 = msi_desc->msg.data;
+ }
+ 
+ struct irq_domain *hv_create_pci_msi_domain(void);
+ 
+ int hv_map_ioapic_interrupt(int ioapic_id, bool level, int vcpu, int vector,
+ 		struct hv_interrupt_entry *entry);
+ int hv_unmap_ioapic_interrupt(int ioapic_id, struct hv_interrupt_entry *entry);
+ int hv_set_mem_host_visibility(unsigned long addr, int numpages, bool visible);
+ 
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ void hv_ghcb_msr_write(u64 msr, u64 value);
+ void hv_ghcb_msr_read(u64 msr, u64 *value);
+ #else
+ static inline void hv_ghcb_msr_write(u64 msr, u64 value) {}
+ static inline void hv_ghcb_msr_read(u64 msr, u64 *value) {}
+ #endif
+ 
+ extern bool hv_isolation_type_snp(void);
+ 
+ static inline bool hv_is_synic_reg(unsigned int reg)
+ {
+ 	if ((reg >= HV_REGISTER_SCONTROL) &&
+ 	    (reg <= HV_REGISTER_SINT15))
+ 		return true;
+ 	return false;
+ }
+ 
+ static inline u64 hv_get_register(unsigned int reg)
+ {
+ 	u64 value;
+ 
+ 	if (hv_is_synic_reg(reg) && hv_isolation_type_snp())
+ 		hv_ghcb_msr_read(reg, &value);
+ 	else
+ 		rdmsrl(reg, value);
+ 	return value;
+ }
+ 
+ static inline void hv_set_register(unsigned int reg, u64 value)
+ {
+ 	if (hv_is_synic_reg(reg) && hv_isolation_type_snp()) {
+ 		hv_ghcb_msr_write(reg, value);
+ 
+ 		/* Write proxy bit via wrmsl instruction */
+ 		if (reg >= HV_REGISTER_SINT0 &&
+ 		    reg <= HV_REGISTER_SINT15)
+ 			wrmsrl(reg, value | 1 << 20);
+ 	} else {
+ 		wrmsrl(reg, value);
+ 	}
+ }
+ 
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
  #else /* CONFIG_HYPERV */
  static inline void hyperv_init(void) {}
  static inline void hyperv_setup_mmu_ops(void) {}
@@@ -205,6 -242,13 +264,16 @@@ static inline int hyperv_flush_guest_ma
  {
  	return -1;
  }
++<<<<<<< HEAD
++=======
+ static inline void hv_set_register(unsigned int reg, u64 value) { }
+ static inline u64 hv_get_register(unsigned int reg) { return 0; }
+ static inline int hv_set_mem_host_visibility(unsigned long addr, int numpages,
+ 					     bool visible)
+ {
+ 	return -1;
+ }
++>>>>>>> faff44069ff5 (x86/hyperv: Add Write/Read MSR registers via ghcb page)
  #endif /* CONFIG_HYPERV */
  
  
* Unmerged path arch/x86/hyperv/ivm.c
* Unmerged path arch/x86/hyperv/hv_init.c
* Unmerged path arch/x86/hyperv/ivm.c
* Unmerged path arch/x86/include/asm/mshyperv.h
diff --git a/drivers/hv/hv.c b/drivers/hv/hv.c
index 267b4b9078b3..775a1944ba5a 100644
--- a/drivers/hv/hv.c
+++ b/drivers/hv/hv.c
@@ -21,6 +21,7 @@
  */
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
+#include <linux/io.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
@@ -149,17 +150,24 @@ int hv_synic_alloc(void)
 		tasklet_init(&hv_cpu->msg_dpc,
 			     vmbus_on_msg_dpc, (unsigned long) hv_cpu);
 
-		hv_cpu->synic_message_page =
-			(void *)get_zeroed_page(GFP_ATOMIC);
-		if (hv_cpu->synic_message_page == NULL) {
-			pr_err("Unable to allocate SYNIC message page\n");
-			goto err;
-		}
+		/*
+		 * Synic message and event pages are allocated by paravisor.
+		 * Skip these pages allocation here.
+		 */
+		if (!hv_isolation_type_snp()) {
+			hv_cpu->synic_message_page =
+				(void *)get_zeroed_page(GFP_ATOMIC);
+			if (hv_cpu->synic_message_page == NULL) {
+				pr_err("Unable to allocate SYNIC message page\n");
+				goto err;
+			}
 
-		hv_cpu->synic_event_page = (void *)get_zeroed_page(GFP_ATOMIC);
-		if (hv_cpu->synic_event_page == NULL) {
-			pr_err("Unable to allocate SYNIC event page\n");
-			goto err;
+			hv_cpu->synic_event_page =
+				(void *)get_zeroed_page(GFP_ATOMIC);
+			if (hv_cpu->synic_event_page == NULL) {
+				pr_err("Unable to allocate SYNIC event page\n");
+				goto err;
+			}
 		}
 
 		hv_cpu->post_msg_page = (void *)get_zeroed_page(GFP_ATOMIC);
@@ -214,16 +222,35 @@ void hv_synic_enable_regs(unsigned int cpu)
 	/* Setup the Synic's message page */
 	simp.as_uint64 = hv_get_register(HV_REGISTER_SIMP);
 	simp.simp_enabled = 1;
-	simp.base_simp_gpa = virt_to_phys(hv_cpu->synic_message_page)
-		>> HV_HYP_PAGE_SHIFT;
+
+	if (hv_isolation_type_snp()) {
+		hv_cpu->synic_message_page
+			= memremap(simp.base_simp_gpa << HV_HYP_PAGE_SHIFT,
+				   HV_HYP_PAGE_SIZE, MEMREMAP_WB);
+		if (!hv_cpu->synic_message_page)
+			pr_err("Fail to map syinc message page.\n");
+	} else {
+		simp.base_simp_gpa = virt_to_phys(hv_cpu->synic_message_page)
+			>> HV_HYP_PAGE_SHIFT;
+	}
 
 	hv_set_register(HV_REGISTER_SIMP, simp.as_uint64);
 
 	/* Setup the Synic's event page */
 	siefp.as_uint64 = hv_get_register(HV_REGISTER_SIEFP);
 	siefp.siefp_enabled = 1;
-	siefp.base_siefp_gpa = virt_to_phys(hv_cpu->synic_event_page)
-		>> HV_HYP_PAGE_SHIFT;
+
+	if (hv_isolation_type_snp()) {
+		hv_cpu->synic_event_page =
+			memremap(siefp.base_siefp_gpa << HV_HYP_PAGE_SHIFT,
+				 HV_HYP_PAGE_SIZE, MEMREMAP_WB);
+
+		if (!hv_cpu->synic_event_page)
+			pr_err("Fail to map syinc event page.\n");
+	} else {
+		siefp.base_siefp_gpa = virt_to_phys(hv_cpu->synic_event_page)
+			>> HV_HYP_PAGE_SHIFT;
+	}
 
 	hv_set_register(HV_REGISTER_SIEFP, siefp.as_uint64);
 
@@ -270,6 +297,8 @@ int hv_synic_init(unsigned int cpu)
  */
 void hv_synic_disable_regs(unsigned int cpu)
 {
+	struct hv_per_cpu_context *hv_cpu
+		= per_cpu_ptr(hv_context.cpu_context, cpu);
 	union hv_synic_sint shared_sint;
 	union hv_synic_simp simp;
 	union hv_synic_siefp siefp;
@@ -286,14 +315,27 @@ void hv_synic_disable_regs(unsigned int cpu)
 				shared_sint.as_uint64);
 
 	simp.as_uint64 = hv_get_register(HV_REGISTER_SIMP);
+	/*
+	 * In Isolation VM, sim and sief pages are allocated by
+	 * paravisor. These pages also will be used by kdump
+	 * kernel. So just reset enable bit here and keep page
+	 * addresses.
+	 */
 	simp.simp_enabled = 0;
-	simp.base_simp_gpa = 0;
+	if (hv_isolation_type_snp())
+		memunmap(hv_cpu->synic_message_page);
+	else
+		simp.base_simp_gpa = 0;
 
 	hv_set_register(HV_REGISTER_SIMP, simp.as_uint64);
 
 	siefp.as_uint64 = hv_get_register(HV_REGISTER_SIEFP);
 	siefp.siefp_enabled = 0;
-	siefp.base_siefp_gpa = 0;
+
+	if (hv_isolation_type_snp())
+		memunmap(hv_cpu->synic_event_page);
+	else
+		siefp.base_siefp_gpa = 0;
 
 	hv_set_register(HV_REGISTER_SIEFP, siefp.as_uint64);
 
diff --git a/drivers/hv/hv_common.c b/drivers/hv/hv_common.c
index 171ccfcf81f1..423019785eaf 100644
--- a/drivers/hv/hv_common.c
+++ b/drivers/hv/hv_common.c
@@ -259,6 +259,12 @@ bool __weak hv_is_isolation_supported(void)
 }
 EXPORT_SYMBOL_GPL(hv_is_isolation_supported);
 
+bool __weak hv_isolation_type_snp(void)
+{
+	return false;
+}
+EXPORT_SYMBOL_GPL(hv_isolation_type_snp);
+
 void __weak hv_setup_vmbus_handler(void (*handler)(void))
 {
 }
diff --git a/include/asm-generic/mshyperv.h b/include/asm-generic/mshyperv.h
index a9b23e7e7e9f..9889a0d33eb5 100644
--- a/include/asm-generic/mshyperv.h
+++ b/include/asm-generic/mshyperv.h
@@ -54,6 +54,7 @@ extern void  __percpu  **hyperv_pcpu_output_arg;
 
 extern u64 hv_do_hypercall(u64 control, void *inputaddr, void *outputaddr);
 extern u64 hv_do_fast_hypercall8(u16 control, u64 input8);
+extern bool hv_isolation_type_snp(void);
 
 /* Helper functions that provide a consistent pattern for checking Hyper-V hypercall status. */
 static inline int hv_result(u64 status)
