KVM: x86/mmu: Leverage vcpu->last_used_slot in tdp_mmu_map_handle_target_level

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit 081de470f1e6e83f9f460ba5ae8f57ff07f37692
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/081de470.failed

The existing TDP MMU methods to handle dirty logging are vcpu-agnostic
since they can be driven by MMU notifiers and other non-vcpu-specific
events in addition to page faults. However this means that the TDP MMU
is not benefiting from the new vcpu->last_used_slot. Fix that by
introducing a tdp_mmu_map_set_spte_atomic() which is only called during
a TDP page fault and has access to the kvm_vcpu for fast slot lookups.

This improves "Populate memory time" in dirty_log_perf_test by 5%:

Command                         | Before           | After
------------------------------- | ---------------- | -------------
./dirty_log_perf_test -v64 -x64 | 5.472321072s     | 5.169832886s

	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20210804222844.1419481-5-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 081de470f1e6e83f9f460ba5ae8f57ff07f37692)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 506d3bdb9486,dab6cb46cdb2..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -532,6 -542,43 +532,46 @@@ static inline bool tdp_mmu_set_spte_ato
  	return true;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * tdp_mmu_map_set_spte_atomic - Set a leaf TDP MMU SPTE atomically to resolve a
+  * TDP page fault.
+  *
+  * @vcpu: The vcpu instance that took the TDP page fault.
+  * @iter: a tdp_iter instance currently on the SPTE that should be set
+  * @new_spte: The value the SPTE should be set to
+  *
+  * Returns: true if the SPTE was set, false if it was not. If false is returned,
+  *	    this function will have no side-effects.
+  */
+ static inline bool tdp_mmu_map_set_spte_atomic(struct kvm_vcpu *vcpu,
+ 					       struct tdp_iter *iter,
+ 					       u64 new_spte)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 
+ 	if (!tdp_mmu_set_spte_atomic_no_dirty_log(kvm, iter, new_spte))
+ 		return false;
+ 
+ 	/*
+ 	 * Use kvm_vcpu_gfn_to_memslot() instead of going through
+ 	 * handle_changed_spte_dirty_log() to leverage vcpu->last_used_slot.
+ 	 */
+ 	if (is_writable_pte(new_spte)) {
+ 		struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, iter->gfn);
+ 
+ 		if (slot && kvm_slot_dirty_track_enabled(slot)) {
+ 			/* Enforced by kvm_mmu_hugepage_adjust. */
+ 			WARN_ON_ONCE(iter->level > PG_LEVEL_4K);
+ 			mark_page_dirty_in_slot(kvm, slot, iter->gfn);
+ 		}
+ 	}
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 081de470f1e6 (KVM: x86/mmu: Leverage vcpu->last_used_slot in tdp_mmu_map_handle_target_level)
  static inline bool tdp_mmu_zap_spte_atomic(struct kvm *kvm,
  					   struct tdp_iter *iter)
  {
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
