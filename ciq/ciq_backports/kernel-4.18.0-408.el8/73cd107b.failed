KVM: x86: Drop current_vcpu for kvm_running_vcpu + kvm_arch_vcpu variable

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit 73cd107b9685c5308e864061772e4a78a629e4a0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/73cd107b.failed

Use the generic kvm_running_vcpu plus a new 'handling_intr_from_guest'
variable in kvm_arch_vcpu instead of the semi-redundant current_vcpu.
kvm_before/after_interrupt() must be called while the vCPU is loaded,
(which protects against preemption), thus kvm_running_vcpu is guaranteed
to be non-NULL when handling_intr_from_guest is non-zero.

Switching to kvm_get_running_vcpu() will allows moving KVM's perf
callbacks to generic code, and the new flag will be used in a future
patch to more precisely identify the "NMI from guest" case.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
Link: https://lore.kernel.org/r/20211111020738.2512932-11-seanjc@google.com
(cherry picked from commit 73cd107b9685c5308e864061772e4a78a629e4a0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/kvm_host.h
#	arch/x86/kvm/pmu.c
#	arch/x86/kvm/x86.c
diff --cc arch/x86/include/asm/kvm_host.h
index 4110a606df35,38f01b00d82a..000000000000
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@@ -1876,8 -1896,6 +1877,11 @@@ int kvm_skip_emulated_instruction(struc
  int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);
  void __kvm_request_immediate_exit(struct kvm_vcpu *vcpu);
  
++<<<<<<< HEAD
 +int kvm_is_in_guest(void);
 +
++=======
++>>>>>>> 73cd107b9685 (KVM: x86: Drop current_vcpu for kvm_running_vcpu + kvm_arch_vcpu variable)
  void __user *__x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,
  				     u32 size);
  bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);
diff --cc arch/x86/kvm/pmu.c
index 09873f6488f7,0c2133eb4cf6..000000000000
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@@ -87,7 -87,7 +87,11 @@@ static void kvm_perf_overflow_intr(stru
  		 * woken up. So we should wake it, but this is impossible from
  		 * NMI context. Do it from irq work instead.
  		 */
++<<<<<<< HEAD
 +		if (!kvm_is_in_guest())
++=======
+ 		if (!kvm_handling_nmi_from_guest(pmc->vcpu))
++>>>>>>> 73cd107b9685 (KVM: x86: Drop current_vcpu for kvm_running_vcpu + kvm_arch_vcpu variable)
  			irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
  		else
  			kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
diff --cc arch/x86/kvm/x86.c
index fa9a4666a113,bb71e10fdb6a..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -8587,38 -8469,45 +8587,74 @@@ static void kvm_timer_init(void
  			  kvmclock_cpu_online, kvmclock_cpu_down_prep);
  }
  
++<<<<<<< HEAD
 +DEFINE_PER_CPU(struct kvm_vcpu *, current_vcpu);
 +EXPORT_PER_CPU_SYMBOL_GPL(current_vcpu);
 +
 +int kvm_is_in_guest(void)
 +{
 +	return __this_cpu_read(current_vcpu) != NULL;
++=======
+ static inline bool kvm_pmi_in_guest(struct kvm_vcpu *vcpu)
+ {
+ 	return vcpu && vcpu->arch.handling_intr_from_guest;
+ }
+ 
+ static unsigned int kvm_guest_state(void)
+ {
+ 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ 	unsigned int state;
+ 
+ 	if (!kvm_pmi_in_guest(vcpu))
+ 		return 0;
+ 
+ 	state = PERF_GUEST_ACTIVE;
+ 	if (static_call(kvm_x86_get_cpl)(vcpu))
+ 		state |= PERF_GUEST_USER;
+ 
+ 	return state;
++>>>>>>> 73cd107b9685 (KVM: x86: Drop current_vcpu for kvm_running_vcpu + kvm_arch_vcpu variable)
  }
  
 -static unsigned long kvm_guest_get_ip(void)
 +static int kvm_is_user_mode(void)
  {
++<<<<<<< HEAD
 +	int user_mode = 3;
 +
 +	if (__this_cpu_read(current_vcpu))
 +		user_mode = static_call(kvm_x86_get_cpl)(__this_cpu_read(current_vcpu));
++=======
+ 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ 
+ 	/* Retrieving the IP must be guarded by a call to kvm_guest_state(). */
+ 	if (WARN_ON_ONCE(!kvm_pmi_in_guest(vcpu)))
+ 		return 0;
++>>>>>>> 73cd107b9685 (KVM: x86: Drop current_vcpu for kvm_running_vcpu + kvm_arch_vcpu variable)
  
 -	return kvm_rip_read(vcpu);
 +	return user_mode != 0;
  }
  
 -static unsigned int kvm_handle_intel_pt_intr(void)
 +static unsigned long kvm_get_guest_ip(void)
 +{
 +	unsigned long ip = 0;
 +
 +	if (__this_cpu_read(current_vcpu))
 +		ip = kvm_rip_read(__this_cpu_read(current_vcpu));
 +
 +	return ip;
 +}
 +
 +static void kvm_handle_intel_pt_intr(void)
  {
- 	struct kvm_vcpu *vcpu = __this_cpu_read(current_vcpu);
+ 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
+ 
++<<<<<<< HEAD
++=======
+ 	/* '0' on failure so that the !PT case can use a RET0 static call. */
+ 	if (!kvm_pmi_in_guest(vcpu))
+ 		return 0;
  
++>>>>>>> 73cd107b9685 (KVM: x86: Drop current_vcpu for kvm_running_vcpu + kvm_arch_vcpu variable)
  	kvm_make_request(KVM_REQ_PMI, vcpu);
  	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
  			(unsigned long *)&vcpu->arch.pmu.global_status);
* Unmerged path arch/x86/include/asm/kvm_host.h
* Unmerged path arch/x86/kvm/pmu.c
* Unmerged path arch/x86/kvm/x86.c
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 072bc69d2fd3..a9197e5b64f5 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -385,18 +385,20 @@ static inline bool kvm_cstate_in_guest(struct kvm *kvm)
 	return kvm->arch.cstate_in_guest;
 }
 
-DECLARE_PER_CPU(struct kvm_vcpu *, current_vcpu);
-
 static inline void kvm_before_interrupt(struct kvm_vcpu *vcpu)
 {
-	__this_cpu_write(current_vcpu, vcpu);
+	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 1);
 }
 
 static inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 {
-	__this_cpu_write(current_vcpu, NULL);
+	WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
 }
 
+static inline bool kvm_handling_nmi_from_guest(struct kvm_vcpu *vcpu)
+{
+	return !!vcpu->arch.handling_intr_from_guest;
+}
 
 static inline bool kvm_pat_valid(u64 data)
 {
