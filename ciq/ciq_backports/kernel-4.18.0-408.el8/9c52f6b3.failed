KVM: x86: Shove vp_bitmap handling down into sparse_set_to_vcpu_mask()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sean Christopherson <seanjc@google.com>
commit 9c52f6b3d8c09df75b72dab9a0e6eb2b70435ae1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/9c52f6b3.failed

Move the vp_bitmap "allocation" that's needed to handle mismatched vp_index
values down into sparse_set_to_vcpu_mask() and drop __always_inline from
said helper.  The need for an intermediate vp_bitmap is a detail that's
specific to the sparse translation with mismatched VP<=>vCPU indexes and
does not need to be exposed to the caller.

Regarding the __always_inline, prior to commit f21dd494506a ("KVM: x86:
hyperv: optimize sparse VP set processing") the helper, then named
hv_vcpu_in_sparse_set(), was a tiny bit of code that effectively boiled
down to a handful of bit ops.  The __always_inline was understandable, if
not justifiable.  Since the aforementioned change, sparse_set_to_vcpu_mask()
is a chunky 350-450+ bytes of code without KASAN=y, and balloons to 1100+
with KASAN=y.  In other words, it has no business being forcefully inlined.

	Signed-off-by: Sean Christopherson <seanjc@google.com>
	Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
Message-Id: <20211207220926.718794-7-seanjc@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 9c52f6b3d8c09df75b72dab9a0e6eb2b70435ae1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/hyperv.c
diff --cc arch/x86/kvm/hyperv.c
index 2dbbb7fecc6e,e46f267585f4..000000000000
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@@ -1713,31 -1713,47 +1713,51 @@@ int kvm_hv_get_msr_common(struct kvm_vc
  		return kvm_hv_get_msr(vcpu, msr, pdata, host);
  }
  
- static __always_inline unsigned long *sparse_set_to_vcpu_mask(
- 	struct kvm *kvm, u64 *sparse_banks, u64 valid_bank_mask,
- 	u64 *vp_bitmap, unsigned long *vcpu_bitmap)
+ static void sparse_set_to_vcpu_mask(struct kvm *kvm, u64 *sparse_banks,
+ 				    u64 valid_bank_mask, unsigned long *vcpu_mask)
  {
  	struct kvm_hv *hv = to_kvm_hv(kvm);
+ 	bool has_mismatch = atomic_read(&hv->num_mismatched_vp_indexes);
+ 	u64 vp_bitmap[KVM_HV_MAX_SPARSE_VCPU_SET_BITS];
  	struct kvm_vcpu *vcpu;
++<<<<<<< HEAD
 +	int i, bank, sbank = 0;
++=======
+ 	int bank, sbank = 0;
+ 	unsigned long i;
+ 	u64 *bitmap;
++>>>>>>> 9c52f6b3d8c0 (KVM: x86: Shove vp_bitmap handling down into sparse_set_to_vcpu_mask())
+ 
+ 	BUILD_BUG_ON(sizeof(vp_bitmap) >
+ 		     sizeof(*vcpu_mask) * BITS_TO_LONGS(KVM_MAX_VCPUS));
+ 
+ 	/*
+ 	 * If vp_index == vcpu_idx for all vCPUs, fill vcpu_mask directly, else
+ 	 * fill a temporary buffer and manually test each vCPU's VP index.
+ 	 */
+ 	if (likely(!has_mismatch))
+ 		bitmap = (u64 *)vcpu_mask;
+ 	else
+ 		bitmap = vp_bitmap;
  
- 	memset(vp_bitmap, 0,
- 	       KVM_HV_MAX_SPARSE_VCPU_SET_BITS * sizeof(*vp_bitmap));
+ 	/*
+ 	 * Each set of 64 VPs is packed into sparse_banks, with valid_bank_mask
+ 	 * having a '1' for each bank that exists in sparse_banks.  Sets must
+ 	 * be in ascending order, i.e. bank0..bankN.
+ 	 */
+ 	memset(bitmap, 0, sizeof(vp_bitmap));
  	for_each_set_bit(bank, (unsigned long *)&valid_bank_mask,
  			 KVM_HV_MAX_SPARSE_VCPU_SET_BITS)
- 		vp_bitmap[bank] = sparse_banks[sbank++];
+ 		bitmap[bank] = sparse_banks[sbank++];
  
- 	if (likely(!atomic_read(&hv->num_mismatched_vp_indexes))) {
- 		/* for all vcpus vp_index == vcpu_idx */
- 		return (unsigned long *)vp_bitmap;
- 	}
+ 	if (likely(!has_mismatch))
+ 		return;
  
- 	bitmap_zero(vcpu_bitmap, KVM_MAX_VCPUS);
+ 	bitmap_zero(vcpu_mask, KVM_MAX_VCPUS);
  	kvm_for_each_vcpu(i, vcpu, kvm) {
  		if (test_bit(kvm_hv_get_vpindex(vcpu), (unsigned long *)vp_bitmap))
- 			__set_bit(i, vcpu_bitmap);
+ 			__set_bit(i, vcpu_mask);
  	}
- 	return vcpu_bitmap;
  }
  
  struct kvm_hv_hcall {
* Unmerged path arch/x86/kvm/hyperv.c
