smp: Make softirq handling RT safe in flush_smp_call_function_queue()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Sebastian Andrzej Siewior <bigeasy@linutronix.de>
commit 1a90bfd220201fbe050dfc15deaac20ca5f15638
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/1a90bfd2.failed

flush_smp_call_function_queue() invokes do_softirq() which is not available
on PREEMPT_RT. flush_smp_call_function_queue() is invoked from the idle
task and the migration task with preemption or interrupts disabled.

So RT kernels cannot process soft interrupts in that context as that has to
acquire 'sleeping spinlocks' which is not possible with preemption or
interrupts disabled and forbidden from the idle task anyway.

The currently known SMP function call which raises a soft interrupt is in
the block layer, but this functionality is not enabled on RT kernels due to
latency and performance reasons.

RT could wake up ksoftirqd unconditionally, but this wants to be avoided if
there were soft interrupts pending already when this is invoked in the
context of the migration task. The migration task might have preempted a
threaded interrupt handler which raised a soft interrupt, but did not reach
the local_bh_enable() to process it. The "running" ksoftirqd might prevent
the handling in the interrupt thread context which is causing latency
issues.

Add a new function which handles this case explicitely for RT and falls
back to do_softirq() on !RT kernels. In the RT case this warns when one of
the flushed SMP function calls raised a soft interrupt so this can be
investigated.

[ tglx: Moved the RT part out of SMP code ]

	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lore.kernel.org/r/YgKgL6aPj8aBES6G@linutronix.de
Link: https://lore.kernel.org/r/20220413133024.356509586@linutronix.de

(cherry picked from commit 1a90bfd220201fbe050dfc15deaac20ca5f15638)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/interrupt.h
#	kernel/smp.c
#	kernel/softirq.c
diff --cc include/linux/interrupt.h
index a5bc05a494fd,a49fe8d88676..000000000000
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@@ -577,12 -607,12 +577,21 @@@ struct softirq_actio
  asmlinkage void do_softirq(void);
  asmlinkage void __do_softirq(void);
  
++<<<<<<< HEAD
 +#ifdef __ARCH_HAS_DO_SOFTIRQ
 +void do_softirq_own_stack(void);
 +#else
 +static inline void do_softirq_own_stack(void)
 +{
 +	__do_softirq();
++=======
+ #ifdef CONFIG_PREEMPT_RT
+ extern void do_softirq_post_smp_call_flush(unsigned int was_pending);
+ #else
+ static inline void do_softirq_post_smp_call_flush(unsigned int unused)
+ {
+ 	do_softirq();
++>>>>>>> 1a90bfd22020 (smp: Make softirq handling RT safe in flush_smp_call_function_queue())
  }
  #endif
  
diff --cc kernel/smp.c
index e676d27c98b1,d54c2fe51ada..000000000000
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@@ -312,19 -676,40 +312,26 @@@ static void flush_smp_call_function_que
  	 */
  	if (entry)
  		sched_ttwu_pending(entry);
 -
 -	cfd_seq_store(this_cpu_ptr(&cfd_seq_local)->hdlend, CFD_SEQ_NOCPU,
 -		      smp_processor_id(), CFD_SEQ_HDLEND);
  }
  
 -
 -/**
 - * flush_smp_call_function_queue - Flush pending smp-call-function callbacks
 - *				   from task context (idle, migration thread)
 - *
 - * When TIF_POLLING_NRFLAG is supported and a CPU is in idle and has it
 - * set, then remote CPUs can avoid sending IPIs and wake the idle CPU by
 - * setting TIF_NEED_RESCHED. The idle task on the woken up CPU has to
 - * handle queued SMP function calls before scheduling.
 - *
 - * The migration thread has to ensure that an eventually pending wakeup has
 - * been handled before it migrates a task.
 - */
 -void flush_smp_call_function_queue(void)
 +void flush_smp_call_function_from_idle(void)
  {
+ 	unsigned int was_pending;
  	unsigned long flags;
  
  	if (llist_empty(this_cpu_ptr(&call_single_queue)))
  		return;
  
 -	cfd_seq_store(this_cpu_ptr(&cfd_seq_local)->idle, CFD_SEQ_NOCPU,
 -		      smp_processor_id(), CFD_SEQ_IDLE);
  	local_irq_save(flags);
++<<<<<<< HEAD
 +	flush_smp_call_function_queue(true);
++=======
+ 	/* Get the already pending soft interrupts for RT enabled kernels */
+ 	was_pending = local_softirq_pending();
+ 	__flush_smp_call_function_queue(true);
++>>>>>>> 1a90bfd22020 (smp: Make softirq handling RT safe in flush_smp_call_function_queue())
  	if (local_softirq_pending())
- 		do_softirq();
+ 		do_softirq_post_smp_call_flush(was_pending);
  
  	local_irq_restore(flags);
  }
diff --cc kernel/softirq.c
index 0eaf37f88c9f,9f0aef8aa9ff..000000000000
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@@ -108,12 -116,204 +108,206 @@@ EXPORT_PER_CPU_SYMBOL_GPL(hardirq_conte
   * This lets us distinguish between whether we are currently processing
   * softirq and whether we just have bh disabled.
   */
 -#ifdef CONFIG_PREEMPT_RT
  
 +#ifdef CONFIG_TRACE_IRQFLAGS
  /*
++<<<<<<< HEAD
 + * This is for softirq.c-internal use, where hardirqs are disabled
++=======
+  * RT accounts for BH disabled sections in task::softirqs_disabled_cnt and
+  * also in per CPU softirq_ctrl::cnt. This is necessary to allow tasks in a
+  * softirq disabled section to be preempted.
+  *
+  * The per task counter is used for softirq_count(), in_softirq() and
+  * in_serving_softirqs() because these counts are only valid when the task
+  * holding softirq_ctrl::lock is running.
+  *
+  * The per CPU counter prevents pointless wakeups of ksoftirqd in case that
+  * the task which is in a softirq disabled section is preempted or blocks.
+  */
+ struct softirq_ctrl {
+ 	local_lock_t	lock;
+ 	int		cnt;
+ };
+ 
+ static DEFINE_PER_CPU(struct softirq_ctrl, softirq_ctrl) = {
+ 	.lock	= INIT_LOCAL_LOCK(softirq_ctrl.lock),
+ };
+ 
+ /**
+  * local_bh_blocked() - Check for idle whether BH processing is blocked
+  *
+  * Returns false if the per CPU softirq::cnt is 0 otherwise true.
+  *
+  * This is invoked from the idle task to guard against false positive
+  * softirq pending warnings, which would happen when the task which holds
+  * softirq_ctrl::lock was the only running task on the CPU and blocks on
+  * some other lock.
+  */
+ bool local_bh_blocked(void)
+ {
+ 	return __this_cpu_read(softirq_ctrl.cnt) != 0;
+ }
+ 
+ void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
+ {
+ 	unsigned long flags;
+ 	int newcnt;
+ 
+ 	WARN_ON_ONCE(in_hardirq());
+ 
+ 	/* First entry of a task into a BH disabled section? */
+ 	if (!current->softirq_disable_cnt) {
+ 		if (preemptible()) {
+ 			local_lock(&softirq_ctrl.lock);
+ 			/* Required to meet the RCU bottomhalf requirements. */
+ 			rcu_read_lock();
+ 		} else {
+ 			DEBUG_LOCKS_WARN_ON(this_cpu_read(softirq_ctrl.cnt));
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * Track the per CPU softirq disabled state. On RT this is per CPU
+ 	 * state to allow preemption of bottom half disabled sections.
+ 	 */
+ 	newcnt = __this_cpu_add_return(softirq_ctrl.cnt, cnt);
+ 	/*
+ 	 * Reflect the result in the task state to prevent recursion on the
+ 	 * local lock and to make softirq_count() & al work.
+ 	 */
+ 	current->softirq_disable_cnt = newcnt;
+ 
+ 	if (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && newcnt == cnt) {
+ 		raw_local_irq_save(flags);
+ 		lockdep_softirqs_off(ip);
+ 		raw_local_irq_restore(flags);
+ 	}
+ }
+ EXPORT_SYMBOL(__local_bh_disable_ip);
+ 
+ static void __local_bh_enable(unsigned int cnt, bool unlock)
+ {
+ 	unsigned long flags;
+ 	int newcnt;
+ 
+ 	DEBUG_LOCKS_WARN_ON(current->softirq_disable_cnt !=
+ 			    this_cpu_read(softirq_ctrl.cnt));
+ 
+ 	if (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && softirq_count() == cnt) {
+ 		raw_local_irq_save(flags);
+ 		lockdep_softirqs_on(_RET_IP_);
+ 		raw_local_irq_restore(flags);
+ 	}
+ 
+ 	newcnt = __this_cpu_sub_return(softirq_ctrl.cnt, cnt);
+ 	current->softirq_disable_cnt = newcnt;
+ 
+ 	if (!newcnt && unlock) {
+ 		rcu_read_unlock();
+ 		local_unlock(&softirq_ctrl.lock);
+ 	}
+ }
+ 
+ void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
+ {
+ 	bool preempt_on = preemptible();
+ 	unsigned long flags;
+ 	u32 pending;
+ 	int curcnt;
+ 
+ 	WARN_ON_ONCE(in_hardirq());
+ 	lockdep_assert_irqs_enabled();
+ 
+ 	local_irq_save(flags);
+ 	curcnt = __this_cpu_read(softirq_ctrl.cnt);
+ 
+ 	/*
+ 	 * If this is not reenabling soft interrupts, no point in trying to
+ 	 * run pending ones.
+ 	 */
+ 	if (curcnt != cnt)
+ 		goto out;
+ 
+ 	pending = local_softirq_pending();
+ 	if (!pending || ksoftirqd_running(pending))
+ 		goto out;
+ 
+ 	/*
+ 	 * If this was called from non preemptible context, wake up the
+ 	 * softirq daemon.
+ 	 */
+ 	if (!preempt_on) {
+ 		wakeup_softirqd();
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * Adjust softirq count to SOFTIRQ_OFFSET which makes
+ 	 * in_serving_softirq() become true.
+ 	 */
+ 	cnt = SOFTIRQ_OFFSET;
+ 	__local_bh_enable(cnt, false);
+ 	__do_softirq();
+ 
+ out:
+ 	__local_bh_enable(cnt, preempt_on);
+ 	local_irq_restore(flags);
+ }
+ EXPORT_SYMBOL(__local_bh_enable_ip);
+ 
+ /*
+  * Invoked from ksoftirqd_run() outside of the interrupt disabled section
+  * to acquire the per CPU local lock for reentrancy protection.
+  */
+ static inline void ksoftirqd_run_begin(void)
+ {
+ 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
+ 	local_irq_disable();
+ }
+ 
+ /* Counterpart to ksoftirqd_run_begin() */
+ static inline void ksoftirqd_run_end(void)
+ {
+ 	__local_bh_enable(SOFTIRQ_OFFSET, true);
+ 	WARN_ON_ONCE(in_interrupt());
+ 	local_irq_enable();
+ }
+ 
+ static inline void softirq_handle_begin(void) { }
+ static inline void softirq_handle_end(void) { }
+ 
+ static inline bool should_wake_ksoftirqd(void)
+ {
+ 	return !this_cpu_read(softirq_ctrl.cnt);
+ }
+ 
+ static inline void invoke_softirq(void)
+ {
+ 	if (should_wake_ksoftirqd())
+ 		wakeup_softirqd();
+ }
+ 
+ /*
+  * flush_smp_call_function_queue() can raise a soft interrupt in a function
+  * call. On RT kernels this is undesired and the only known functionality
+  * in the block layer which does this is disabled on RT. If soft interrupts
+  * get raised which haven't been raised before the flush, warn so it can be
+  * investigated.
+  */
+ void do_softirq_post_smp_call_flush(unsigned int was_pending)
+ {
+ 	if (WARN_ON_ONCE(was_pending != local_softirq_pending()))
+ 		invoke_softirq();
+ }
+ 
+ #else /* CONFIG_PREEMPT_RT */
+ 
+ /*
+  * This one is for softirq.c-internal use, where hardirqs are disabled
++>>>>>>> 1a90bfd22020 (smp: Make softirq handling RT safe in flush_smp_call_function_queue())
   * legitimately:
   */
 -#ifdef CONFIG_TRACE_IRQFLAGS
  void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
  {
  	unsigned long flags;
* Unmerged path include/linux/interrupt.h
* Unmerged path kernel/smp.c
* Unmerged path kernel/softirq.c
