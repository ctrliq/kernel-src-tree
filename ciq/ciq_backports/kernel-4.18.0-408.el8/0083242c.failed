sched/topology: Skip updating masks for non-online nodes

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Valentin Schneider <valentin.schneider@arm.com>
commit 0083242c93759dde353a963a90cb351c5c283379
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/0083242c.failed

The scheduler currently expects NUMA node distances to be stable from
init onwards, and as a consequence builds the related data structures
once-and-for-all at init (see sched_init_numa()).

Unfortunately, on some architectures node distance is unreliable for
offline nodes and may very well change upon onlining.

Skip over offline nodes during sched_init_numa(). Track nodes that have
been onlined at least once, and trigger a build of a node's NUMA masks
when it is first onlined post-init.

	Reported-by: Geetika Moolchandani <Geetika.Moolchandani1@ibm.com>
	Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
	Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20210818074333.48645-1-srikar@linux.vnet.ibm.com
(cherry picked from commit 0083242c93759dde353a963a90cb351c5c283379)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/topology.c
diff --cc kernel/sched/topology.c
index 73761a03de28,4e8698e62f07..000000000000
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@@ -1376,6 -1481,9 +1376,12 @@@ static int			sched_domains_curr_level
  int				sched_max_numa_distance;
  static int			*sched_domains_numa_distance;
  static struct cpumask		***sched_domains_numa_masks;
++<<<<<<< HEAD
++=======
+ int __read_mostly		node_reclaim_distance = RECLAIM_DISTANCE;
+ 
+ static unsigned long __read_mostly *sched_numa_onlined_nodes;
++>>>>>>> 0083242c9375 (sched/topology: Skip updating masks for non-online nodes)
  #endif
  
  /*
@@@ -1746,6 -1835,19 +1752,22 @@@ void sched_init_numa(void
  			sched_domains_numa_masks[i][j] = mask;
  
  			for_each_node(k) {
++<<<<<<< HEAD
++=======
+ 				/*
+ 				 * Distance information can be unreliable for
+ 				 * offline nodes, defer building the node
+ 				 * masks to its bringup.
+ 				 * This relies on all unique distance values
+ 				 * still being visible at init time.
+ 				 */
+ 				if (!node_online(j))
+ 					continue;
+ 
+ 				if (sched_debug() && (node_distance(j, k) != node_distance(k, j)))
+ 					sched_numa_warn("Node-distance not symmetric");
+ 
++>>>>>>> 0083242c9375 (sched/topology: Skip updating masks for non-online nodes)
  				if (node_distance(j, k) > sched_domains_numa_distance[i])
  					continue;
  
@@@ -1792,10 -1894,57 +1814,57 @@@
  
  	sched_domain_topology = tl;
  
 -	sched_domains_numa_levels = nr_levels;
 -	sched_max_numa_distance = sched_domains_numa_distance[nr_levels - 1];
 +	sched_domains_numa_levels = level;
 +	sched_max_numa_distance = sched_domains_numa_distance[level - 1];
  
  	init_numa_topology_type();
+ 
+ 	sched_numa_onlined_nodes = bitmap_alloc(nr_node_ids, GFP_KERNEL);
+ 	if (!sched_numa_onlined_nodes)
+ 		return;
+ 
+ 	bitmap_zero(sched_numa_onlined_nodes, nr_node_ids);
+ 	for_each_online_node(i)
+ 		bitmap_set(sched_numa_onlined_nodes, i, 1);
+ }
+ 
+ static void __sched_domains_numa_masks_set(unsigned int node)
+ {
+ 	int i, j;
+ 
+ 	/*
+ 	 * NUMA masks are not built for offline nodes in sched_init_numa().
+ 	 * Thus, when a CPU of a never-onlined-before node gets plugged in,
+ 	 * adding that new CPU to the right NUMA masks is not sufficient: the
+ 	 * masks of that CPU's node must also be updated.
+ 	 */
+ 	if (test_bit(node, sched_numa_onlined_nodes))
+ 		return;
+ 
+ 	bitmap_set(sched_numa_onlined_nodes, node, 1);
+ 
+ 	for (i = 0; i < sched_domains_numa_levels; i++) {
+ 		for (j = 0; j < nr_node_ids; j++) {
+ 			if (!node_online(j) || node == j)
+ 				continue;
+ 
+ 			if (node_distance(j, node) > sched_domains_numa_distance[i])
+ 				continue;
+ 
+ 			/* Add remote nodes in our masks */
+ 			cpumask_or(sched_domains_numa_masks[i][node],
+ 				   sched_domains_numa_masks[i][node],
+ 				   sched_domains_numa_masks[0][j]);
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * A new node has been brought up, potentially changing the topology
+ 	 * classification.
+ 	 *
+ 	 * Note that this is racy vs any use of sched_numa_topology_type :/
+ 	 */
+ 	init_numa_topology_type();
  }
  
  void sched_domains_numa_masks_set(unsigned int cpu)
* Unmerged path kernel/sched/topology.c
