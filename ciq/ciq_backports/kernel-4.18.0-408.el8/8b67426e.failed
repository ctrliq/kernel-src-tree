dmaengine: idxd: int handle management refactoring

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 8b67426e05584e956775f4b134596b56bc0d35e0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/8b67426e.failed

Attach int_handle to irq_entry. This removes the separate management of int
handles and reduces the confusion of interating through int handles that is
off by 1 count.

	Reviewed-by: Kevin Tian <kevin.tian@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/163528417065.3925689.11505755433684476288.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 8b67426e05584e956775f4b134596b56bc0d35e0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/device.c
#	drivers/dma/idxd/idxd.h
#	drivers/dma/idxd/submit.c
diff --cc drivers/dma/idxd/device.c
index eadb0129f3b6,f381319615fd..000000000000
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@@ -1114,3 -1141,211 +1114,214 @@@ int idxd_device_load_config(struct idxd
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ int __drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	int rc = -ENXIO;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_NOT_ENABLED;
+ 		goto err;
+ 	}
+ 
+ 	if (wq->state != IDXD_WQ_DISABLED) {
+ 		dev_dbg(dev, "wq %d already enabled.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_ENABLED;
+ 		rc = -EBUSY;
+ 		goto err;
+ 	}
+ 
+ 	if (!wq->group) {
+ 		dev_dbg(dev, "wq %d not attached to group.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_GRP;
+ 		goto err;
+ 	}
+ 
+ 	if (strlen(wq->name) == 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_NAME;
+ 		dev_dbg(dev, "wq %d name not set.\n", wq->id);
+ 		goto err;
+ 	}
+ 
+ 	/* Shared WQ checks */
+ 	if (wq_shared(wq)) {
+ 		if (!device_swq_supported(idxd)) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_SVM;
+ 			dev_dbg(dev, "PASID not enabled and shared wq.\n");
+ 			goto err;
+ 		}
+ 		/*
+ 		 * Shared wq with the threshold set to 0 means the user
+ 		 * did not set the threshold or transitioned from a
+ 		 * dedicated wq but did not set threshold. A value
+ 		 * of 0 would effectively disable the shared wq. The
+ 		 * driver does not allow a value of 0 to be set for
+ 		 * threshold via sysfs.
+ 		 */
+ 		if (wq->threshold == 0) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_THRESH;
+ 			dev_dbg(dev, "Shared wq and threshold 0.\n");
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	rc = 0;
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Writing wq %d config failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	/*
+ 	 * Device has 1 misc interrupt and N interrupts for descriptor completion. To
+ 	 * assign WQ to interrupt, we will take the N+1 interrupt since vector 0 is
+ 	 * for the misc interrupt.
+ 	 */
+ 	wq->ie = &idxd->irq_entries[wq->id + 1];
+ 
+ 	rc = idxd_wq_enable(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "wq %d enabling failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_map_portal(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_PORTAL_ERR;
+ 		dev_dbg(dev, "wq %d portal mapping failed: %d\n", wq->id, rc);
+ 		goto err_map_portal;
+ 	}
+ 
+ 	wq->client_count = 0;
+ 	return 0;
+ 
+ err_map_portal:
+ 	rc = idxd_wq_disable(wq, false);
+ 	if (rc < 0)
+ 		dev_dbg(dev, "wq %s disable failed\n", dev_name(wq_confdev(wq)));
+ err:
+ 	return rc;
+ }
+ 
+ int drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	int rc;
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	rc = __drv_enable_wq(wq);
+ 	mutex_unlock(&wq->wq_lock);
+ 	return rc;
+ }
+ 
+ void __drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd_wq_refcount(wq))
+ 		dev_warn(dev, "Clients has claim on wq %d: %d\n",
+ 			 wq->id, idxd_wq_refcount(wq));
+ 
+ 	idxd_wq_unmap_portal(wq);
+ 
+ 	idxd_wq_drain(wq);
+ 	idxd_wq_reset(wq);
+ 
+ 	wq->ie = NULL;
+ 	wq->client_count = 0;
+ }
+ 
+ void drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	mutex_lock(&wq->wq_lock);
+ 	__drv_disable_wq(wq);
+ 	mutex_unlock(&wq->wq_lock);
+ }
+ 
+ int idxd_device_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int rc = 0;
+ 
+ 	/*
+ 	 * Device should be in disabled state for the idxd_drv to load. If it's in
+ 	 * enabled state, then the device was altered outside of driver's control.
+ 	 * If the state is in halted state, then we don't want to proceed.
+ 	 */
+ 	if (idxd->state != IDXD_DEV_DISABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_ENABLED;
+ 		return -ENXIO;
+ 	}
+ 
+ 	/* Device configuration */
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0)
+ 		return -ENXIO;
+ 
+ 	/* Start device */
+ 	rc = idxd_device_enable(idxd);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	/* Setup DMA device without channels */
+ 	rc = idxd_register_dma_device(idxd);
+ 	if (rc < 0) {
+ 		idxd_device_disable(idxd);
+ 		idxd->cmd_status = IDXD_SCMD_DEV_DMA_ERR;
+ 		return rc;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	return 0;
+ }
+ 
+ void idxd_device_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int i;
+ 
+ 	for (i = 0; i < idxd->max_wqs; i++) {
+ 		struct idxd_wq *wq = idxd->wqs[i];
+ 		struct device *wq_dev = wq_confdev(wq);
+ 
+ 		if (wq->state == IDXD_WQ_DISABLED)
+ 			continue;
+ 		dev_warn(dev, "Active wq %d on disable %s.\n", i, dev_name(wq_dev));
+ 		device_release_driver(wq_dev);
+ 	}
+ 
+ 	idxd_unregister_dma_device(idxd);
+ 	idxd_device_disable(idxd);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		idxd_device_reset(idxd);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_DSA,
+ 	IDXD_DEV_IAX,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_drv = {
+ 	.type = dev_types,
+ 	.probe = idxd_device_drv_probe,
+ 	.remove = idxd_device_drv_remove,
+ 	.name = "idxd",
+ };
+ EXPORT_SYMBOL_GPL(idxd_drv);
++>>>>>>> 8b67426e0558 (dmaengine: idxd: int handle management refactoring)
diff --cc drivers/dma/idxd/idxd.h
index 03dc70e1f4d4,355159d4ee68..000000000000
--- a/drivers/dma/idxd/idxd.h
+++ b/drivers/dma/idxd/idxd.h
@@@ -40,6 -57,15 +41,15 @@@ struct idxd_device_driver 
  	struct device_driver drv;
  };
  
++<<<<<<< HEAD
++=======
+ extern struct idxd_device_driver dsa_drv;
+ extern struct idxd_device_driver idxd_drv;
+ extern struct idxd_device_driver idxd_dmaengine_drv;
+ extern struct idxd_device_driver idxd_user_drv;
+ 
+ #define INVALID_INT_HANDLE	-1
++>>>>>>> 8b67426e0558 (dmaengine: idxd: int handle management refactoring)
  struct idxd_irq_entry {
  	struct idxd_device *idxd;
  	int id;
diff --cc drivers/dma/idxd/submit.c
index 0afcd1322339,d4688f369bc2..000000000000
--- a/drivers/dma/idxd/submit.c
+++ b/drivers/dma/idxd/submit.c
@@@ -23,21 -22,13 +23,28 @@@ static struct idxd_desc *__get_desc(str
  		desc->hw->pasid = idxd->pasid;
  
  	/*
 -	 * On host, MSIX vecotr 0 is used for misc interrupt. Therefore when we match
 -	 * vector 1:1 to the WQ id, we need to add 1
 +	 * Descriptor completion vectors are 1...N for MSIX. We will round
 +	 * robin through the N vectors.
  	 */
++<<<<<<< HEAD
 +	wq->vec_ptr = desc->vector = (wq->vec_ptr % idxd->num_wq_irqs) + 1;
 +	if (!idxd->int_handles) {
 +		desc->hw->int_handle = wq->vec_ptr;
 +	} else {
 +		/*
 +		 * int_handles are only for descriptor completion. However for device
 +		 * MSIX enumeration, vec 0 is used for misc interrupts. Therefore even
 +		 * though we are rotating through 1...N for descriptor interrupts, we
 +		 * need to acqurie the int_handles from 0..N-1.
 +		 */
 +		desc->hw->int_handle = idxd->int_handles[desc->vector - 1];
 +	}
++=======
+ 	if (wq->ie->int_handle == INVALID_INT_HANDLE)
+ 		desc->hw->int_handle = wq->id + 1;
+ 	else
+ 		desc->hw->int_handle = wq->ie->int_handle;
++>>>>>>> 8b67426e0558 (dmaengine: idxd: int handle management refactoring)
  
  	return desc;
  }
@@@ -108,6 -153,16 +115,19 @@@ int idxd_submit_desc(struct idxd_wq *wq
  	 * even on UP because the recipient is a device.
  	 */
  	wmb();
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Pending the descriptor to the lockless list for the irq_entry
+ 	 * that we designated the descriptor to.
+ 	 */
+ 	if (desc->hw->flags & IDXD_OP_FLAG_RCI) {
+ 		ie = wq->ie;
+ 		llist_add(&desc->llnode, &ie->pending_llist);
+ 	}
+ 
++>>>>>>> 8b67426e0558 (dmaengine: idxd: int handle management refactoring)
  	if (wq_dedicated(wq)) {
  		iosubmit_cmds512(portal, desc->hw, 1);
  	} else {
* Unmerged path drivers/dma/idxd/device.c
* Unmerged path drivers/dma/idxd/idxd.h
diff --git a/drivers/dma/idxd/init.c b/drivers/dma/idxd/init.c
index b9299e45c20d..2a539175519f 100644
--- a/drivers/dma/idxd/init.c
+++ b/drivers/dma/idxd/init.c
@@ -80,6 +80,7 @@ static int idxd_setup_interrupts(struct idxd_device *idxd)
 		dev_err(dev, "Not MSI-X interrupt capable.\n");
 		return -ENOSPC;
 	}
+	idxd->irq_cnt = msixcnt;
 
 	rc = pci_alloc_irq_vectors(pdev, msixcnt, msixcnt, PCI_IRQ_MSIX);
 	if (rc != msixcnt) {
@@ -102,7 +103,18 @@ static int idxd_setup_interrupts(struct idxd_device *idxd)
 	for (i = 0; i < msixcnt; i++) {
 		idxd->irq_entries[i].id = i;
 		idxd->irq_entries[i].idxd = idxd;
+		/*
+		 * Association of WQ should be assigned starting with irq_entry 1.
+		 * irq_entry 0 is for misc interrupts and has no wq association
+		 */
+		if (i > 0)
+			idxd->irq_entries[i].wq = idxd->wqs[i - 1];
 		idxd->irq_entries[i].vector = pci_irq_vector(pdev, i);
+		idxd->irq_entries[i].int_handle = INVALID_INT_HANDLE;
+		if (device_pasid_enabled(idxd) && i > 0)
+			idxd->irq_entries[i].pasid = idxd->pasid;
+		else
+			idxd->irq_entries[i].pasid = INVALID_IOASID;
 		spin_lock_init(&idxd->irq_entries[i].list_lock);
 	}
 
@@ -134,22 +146,14 @@ static int idxd_setup_interrupts(struct idxd_device *idxd)
 		}
 
 		dev_dbg(dev, "Allocated idxd-msix %d for vector %d\n", i, irq_entry->vector);
-		if (idxd->hw.cmd_cap & BIT(IDXD_CMD_REQUEST_INT_HANDLE)) {
-			/*
-			 * The MSIX vector enumeration starts at 1 with vector 0 being the
-			 * misc interrupt that handles non I/O completion events. The
-			 * interrupt handles are for IMS enumeration on guest. The misc
-			 * interrupt vector does not require a handle and therefore we start
-			 * the int_handles at index 0. Since 'i' starts at 1, the first
-			 * int_handles index will be 0.
-			 */
-			rc = idxd_device_request_int_handle(idxd, i, &idxd->int_handles[i - 1],
+		if (idxd->request_int_handles) {
+			rc = idxd_device_request_int_handle(idxd, i, &irq_entry->int_handle,
 							    IDXD_IRQ_MSIX);
 			if (rc < 0) {
 				free_irq(irq_entry->vector, irq_entry);
 				goto err_wq_irqs;
 			}
-			dev_dbg(dev, "int handle requested: %u\n", idxd->int_handles[i - 1]);
+			dev_dbg(dev, "int handle requested: %u\n", irq_entry->int_handle);
 		}
 	}
 
@@ -160,9 +164,15 @@ static int idxd_setup_interrupts(struct idxd_device *idxd)
 	while (--i >= 0) {
 		irq_entry = &idxd->irq_entries[i];
 		free_irq(irq_entry->vector, irq_entry);
-		if (i != 0)
-			idxd_device_release_int_handle(idxd,
-						       idxd->int_handles[i], IDXD_IRQ_MSIX);
+		if (irq_entry->int_handle != INVALID_INT_HANDLE) {
+			idxd_device_release_int_handle(idxd, irq_entry->int_handle,
+						       IDXD_IRQ_MSIX);
+			irq_entry->int_handle = INVALID_INT_HANDLE;
+			irq_entry->pasid = INVALID_IOASID;
+		}
+		irq_entry->vector = -1;
+		irq_entry->wq = NULL;
+		irq_entry->idxd = NULL;
 	}
  err_misc_irq:
 	/* Disable error interrupt generation */
@@ -178,21 +188,19 @@ static void idxd_cleanup_interrupts(struct idxd_device *idxd)
 {
 	struct pci_dev *pdev = idxd->pdev;
 	struct idxd_irq_entry *irq_entry;
-	int i, msixcnt;
-
-	msixcnt = pci_msix_vec_count(pdev);
-	if (msixcnt <= 0)
-		return;
-
-	irq_entry = &idxd->irq_entries[0];
-	free_irq(irq_entry->vector, irq_entry);
-
-	for (i = 1; i < msixcnt; i++) {
+	int i;
 
+	for (i = 0; i < idxd->irq_cnt; i++) {
 		irq_entry = &idxd->irq_entries[i];
-		if (idxd->hw.cmd_cap & BIT(IDXD_CMD_RELEASE_INT_HANDLE))
-			idxd_device_release_int_handle(idxd, idxd->int_handles[i],
+		if (irq_entry->int_handle != INVALID_INT_HANDLE) {
+			idxd_device_release_int_handle(idxd, irq_entry->int_handle,
 						       IDXD_IRQ_MSIX);
+			irq_entry->int_handle = INVALID_INT_HANDLE;
+			irq_entry->pasid = INVALID_IOASID;
+		}
+		irq_entry->vector = -1;
+		irq_entry->wq = NULL;
+		irq_entry->idxd = NULL;
 		free_irq(irq_entry->vector, irq_entry);
 	}
 
@@ -361,13 +369,6 @@ static int idxd_setup_internals(struct idxd_device *idxd)
 
 	init_waitqueue_head(&idxd->cmd_waitq);
 
-	if (idxd->hw.cmd_cap & BIT(IDXD_CMD_REQUEST_INT_HANDLE)) {
-		idxd->int_handles = kcalloc_node(idxd->max_wqs, sizeof(int), GFP_KERNEL,
-						 dev_to_node(dev));
-		if (!idxd->int_handles)
-			return -ENOMEM;
-	}
-
 	rc = idxd_setup_wqs(idxd);
 	if (rc < 0)
 		goto err_wqs;
@@ -398,7 +399,6 @@ static int idxd_setup_internals(struct idxd_device *idxd)
 	for (i = 0; i < idxd->max_wqs; i++)
 		put_device(&idxd->wqs[i]->conf_dev);
  err_wqs:
-	kfree(idxd->int_handles);
 	return rc;
 }
 
@@ -433,6 +433,10 @@ static void idxd_read_caps(struct idxd_device *idxd)
 		dev_dbg(dev, "cmd_cap: %#x\n", idxd->hw.cmd_cap);
 	}
 
+	/* reading command capabilities */
+	if (idxd->hw.cmd_cap & BIT(IDXD_CMD_REQUEST_INT_HANDLE))
+		idxd->request_int_handles = true;
+
 	idxd->max_xfer_bytes = 1ULL << idxd->hw.gen_cap.max_xfer_shift;
 	dev_dbg(dev, "max xfer size: %llu bytes\n", idxd->max_xfer_bytes);
 	idxd->max_batch_size = 1U << idxd->hw.gen_cap.max_batch_shift;
@@ -730,15 +734,15 @@ static void idxd_release_int_handles(struct idxd_device *idxd)
 	struct device *dev = &idxd->pdev->dev;
 	int i, rc;
 
-	for (i = 0; i < idxd->num_wq_irqs; i++) {
-		if (idxd->hw.cmd_cap & BIT(IDXD_CMD_RELEASE_INT_HANDLE)) {
-			rc = idxd_device_release_int_handle(idxd, idxd->int_handles[i],
-							    IDXD_IRQ_MSIX);
+	for (i = 1; i < idxd->irq_cnt; i++) {
+		struct idxd_irq_entry *ie = &idxd->irq_entries[i];
+
+		if (ie->int_handle != INVALID_INT_HANDLE) {
+			rc = idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
 			if (rc < 0)
-				dev_warn(dev, "irq handle %d release failed\n",
-					 idxd->int_handles[i]);
+				dev_warn(dev, "irq handle %d release failed\n", ie->int_handle);
 			else
-				dev_dbg(dev, "int handle requested: %u\n", idxd->int_handles[i]);
+				dev_dbg(dev, "int handle released: %u\n", ie->int_handle);
 		}
 	}
 }
* Unmerged path drivers/dma/idxd/submit.c
diff --git a/drivers/dma/idxd/sysfs.c b/drivers/dma/idxd/sysfs.c
index 4cfaba0faeca..1808a80031b2 100644
--- a/drivers/dma/idxd/sysfs.c
+++ b/drivers/dma/idxd/sysfs.c
@@ -1585,7 +1585,6 @@ static void idxd_conf_device_release(struct device *dev)
 	kfree(idxd->wqs);
 	kfree(idxd->engines);
 	kfree(idxd->irq_entries);
-	kfree(idxd->int_handles);
 	ida_free(&idxd_ida, idxd->id);
 	kfree(idxd);
 }
