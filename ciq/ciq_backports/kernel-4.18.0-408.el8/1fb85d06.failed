x86: Share definition of __is_canonical_address()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Adrian Hunter <adrian.hunter@intel.com>
commit 1fb85d06ad6754796cd1b920639ca9d8840abefd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/1fb85d06.failed

Reduce code duplication by moving canonical address code to a common header
file.

	Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lore.kernel.org/r/20220131072453.2839535-3-adrian.hunter@intel.com
(cherry picked from commit 1fb85d06ad6754796cd1b920639ca9d8840abefd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/maccess.c
diff --cc arch/x86/mm/maccess.c
index f5b85bdc0535,5a53c2cc169c..000000000000
--- a/arch/x86/mm/maccess.c
+++ b/arch/x86/mm/maccess.c
@@@ -4,40 -4,21 +4,49 @@@
  #include <linux/kernel.h>
  
  #ifdef CONFIG_X86_64
 -bool copy_from_kernel_nofault_allowed(const void *unsafe_src, size_t size)
++<<<<<<< HEAD
 +static __always_inline u64 canonical_address(u64 vaddr, u8 vaddr_bits)
  {
 -	unsigned long vaddr = (unsigned long)unsafe_src;
 +	return ((s64)vaddr << (64 - vaddr_bits)) >> (64 - vaddr_bits);
 +}
  
 +static __always_inline bool invalid_probe_range(u64 vaddr)
++=======
++bool copy_from_kernel_nofault_allowed(const void *unsafe_src, size_t size)
++>>>>>>> 1fb85d06ad67 (x86: Share definition of __is_canonical_address())
 +{
  	/*
  	 * Range covering the highest possible canonical userspace address
  	 * as well as non-canonical address range. For the canonical range
  	 * we also need to include the userspace guard page.
  	 */
++<<<<<<< HEAD
 +	return vaddr < TASK_SIZE_MAX + PAGE_SIZE ||
 +	       canonical_address(vaddr, boot_cpu_data.x86_virt_bits) != vaddr;
++=======
+ 	return vaddr >= TASK_SIZE_MAX + PAGE_SIZE &&
+ 	       __is_canonical_address(vaddr, boot_cpu_data.x86_virt_bits);
++>>>>>>> 1fb85d06ad67 (x86: Share definition of __is_canonical_address())
  }
  #else
 -bool copy_from_kernel_nofault_allowed(const void *unsafe_src, size_t size)
 +static __always_inline bool invalid_probe_range(u64 vaddr)
  {
 -	return (unsigned long)unsafe_src >= TASK_SIZE_MAX;
 +	return vaddr < TASK_SIZE_MAX;
  }
  #endif
 +
 +long probe_kernel_read_strict(void *dst, const void *src, size_t size)
 +{
 +	if (unlikely(invalid_probe_range((unsigned long)src)))
 +		return -EFAULT;
 +
 +	return __probe_kernel_read(dst, src, size);
 +}
 +
 +long strncpy_from_unsafe_strict(char *dst, const void *unsafe_addr, long count)
 +{
 +	if (unlikely(invalid_probe_range((unsigned long)unsafe_addr)))
 +		return -EFAULT;
 +
 +	return __strncpy_from_unsafe(dst, unsafe_addr, count);
 +}
diff --git a/arch/x86/events/intel/pt.c b/arch/x86/events/intel/pt.c
index a1a2d8ad3972..a863785466e7 100644
--- a/arch/x86/events/intel/pt.c
+++ b/arch/x86/events/intel/pt.c
@@ -1358,20 +1358,10 @@ static void pt_addr_filters_fini(struct perf_event *event)
 }
 
 #ifdef CONFIG_X86_64
-static u64 canonical_address(u64 vaddr, u8 vaddr_bits)
-{
-	return ((s64)vaddr << (64 - vaddr_bits)) >> (64 - vaddr_bits);
-}
-
-static u64 is_canonical_address(u64 vaddr, u8 vaddr_bits)
-{
-	return canonical_address(vaddr, vaddr_bits) == vaddr;
-}
-
 /* Clamp to a canonical address greater-than-or-equal-to the address given */
 static u64 clamp_to_ge_canonical_addr(u64 vaddr, u8 vaddr_bits)
 {
-	return is_canonical_address(vaddr, vaddr_bits) ?
+	return __is_canonical_address(vaddr, vaddr_bits) ?
 	       vaddr :
 	       -BIT_ULL(vaddr_bits - 1);
 }
@@ -1379,7 +1369,7 @@ static u64 clamp_to_ge_canonical_addr(u64 vaddr, u8 vaddr_bits)
 /* Clamp to a canonical address less-than-or-equal-to the address given */
 static u64 clamp_to_le_canonical_addr(u64 vaddr, u8 vaddr_bits)
 {
-	return is_canonical_address(vaddr, vaddr_bits) ?
+	return __is_canonical_address(vaddr, vaddr_bits) ?
 	       vaddr :
 	       BIT_ULL(vaddr_bits - 1) - 1;
 }
diff --git a/arch/x86/include/asm/page.h b/arch/x86/include/asm/page.h
index 7555b48803a8..ffae5ea9fd4e 100644
--- a/arch/x86/include/asm/page.h
+++ b/arch/x86/include/asm/page.h
@@ -71,6 +71,16 @@ static inline void copy_user_page(void *to, void *from, unsigned long vaddr,
 extern bool __virt_addr_valid(unsigned long kaddr);
 #define virt_addr_valid(kaddr)	__virt_addr_valid((unsigned long) (kaddr))
 
+static __always_inline u64 __canonical_address(u64 vaddr, u8 vaddr_bits)
+{
+	return ((s64)vaddr << (64 - vaddr_bits)) >> (64 - vaddr_bits);
+}
+
+static __always_inline u64 __is_canonical_address(u64 vaddr, u8 vaddr_bits)
+{
+	return __canonical_address(vaddr, vaddr_bits) == vaddr;
+}
+
 #endif	/* __ASSEMBLY__ */
 
 #include <asm-generic/memory_model.h>
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 7170185bc489..33d1ae6ce19c 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -673,7 +673,7 @@ static inline u8 ctxt_virt_addr_bits(struct x86_emulate_ctxt *ctxt)
 static inline bool emul_is_noncanonical_address(u64 la,
 						struct x86_emulate_ctxt *ctxt)
 {
-	return get_canonical(la, ctxt_virt_addr_bits(ctxt)) != la;
+	return !__is_canonical_address(la, ctxt_virt_addr_bits(ctxt));
 }
 
 /*
@@ -723,7 +723,7 @@ static __always_inline int __linearize(struct x86_emulate_ctxt *ctxt,
 	case X86EMUL_MODE_PROT64:
 		*linear = la;
 		va_bits = ctxt_virt_addr_bits(ctxt);
-		if (get_canonical(la, va_bits) != la)
+		if (!__is_canonical_address(la, va_bits))
 			goto bad;
 
 		*max_size = min_t(u64, ~0u, (1ull << va_bits) - la);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 2be0dcdb93ab..fdaf25bb9378 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -1747,7 +1747,7 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 		 * value, and that something deterministic happens if the guest
 		 * invokes 64-bit SYSENTER.
 		 */
-		data = get_canonical(data, vcpu_virt_addr_bits(vcpu));
+		data = __canonical_address(data, vcpu_virt_addr_bits(vcpu));
 		break;
 	case MSR_TSC_AUX:
 		if (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 3b4de769abc5..7a8098f0a89f 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -211,14 +211,9 @@ static inline u8 vcpu_virt_addr_bits(struct kvm_vcpu *vcpu)
 	return kvm_read_cr4_bits(vcpu, X86_CR4_LA57) ? 57 : 48;
 }
 
-static inline u64 get_canonical(u64 la, u8 vaddr_bits)
-{
-	return ((int64_t)la << (64 - vaddr_bits)) >> (64 - vaddr_bits);
-}
-
 static inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)
 {
-	return get_canonical(la, vcpu_virt_addr_bits(vcpu)) != la;
+	return !__is_canonical_address(la, vcpu_virt_addr_bits(vcpu));
 }
 
 static inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,
* Unmerged path arch/x86/mm/maccess.c
