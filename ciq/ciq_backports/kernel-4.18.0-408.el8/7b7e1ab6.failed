KVM: x86/mmu: Consolidate logic to atomically install a new TDP MMU page table

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author David Matlack <dmatlack@google.com>
commit 7b7e1ab6fdc554f77f98c597aa9375b52b3a5454
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/7b7e1ab6.failed

Consolidate the logic to atomically replace an SPTE with an SPTE that
points to a new page table into a single helper function. This will be
used in a follow-up commit to split huge pages, which involves replacing
each huge page SPTE with an SPTE that points to a page table.

Opportunistically drop the call to trace_kvm_mmu_get_page() in
kvm_tdp_mmu_map() since it is redundant with the identical tracepoint in
tdp_mmu_alloc_sp().

No functional change intended.

	Reviewed-by: Peter Xu <peterx@redhat.com>
	Signed-off-by: David Matlack <dmatlack@google.com>
Message-Id: <20220119230739.2234394-8-dmatlack@google.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 7b7e1ab6fdc554f77f98c597aa9375b52b3a5454)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/tdp_mmu.c
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 2d278f0eaa20,bcba4adbafac..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -245,34 -269,7 +245,38 @@@ static void handle_changed_spte_dirty_l
  }
  
  /**
++<<<<<<< HEAD
 + * tdp_mmu_link_page - Add a new page to the list of pages used by the TDP MMU
 + *
 + * @kvm: kvm instance
 + * @sp: the new page
 + * @shared: This operation may not be running under the exclusive use of
 + *	    the MMU lock and the operation must synchronize with other
 + *	    threads that might be adding or removing pages.
 + * @account_nx: This page replaces a NX large page and should be marked for
 + *		eventual reclaim.
 + */
 +static void tdp_mmu_link_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 +			      bool shared, bool account_nx)
 +{
 +	if (shared)
 +		spin_lock(&kvm->arch.tdp_mmu_pages_lock);
 +	else
 +		lockdep_assert_held_write(&kvm->mmu_lock);
 +
 +	list_add(&sp->link, &kvm->arch.tdp_mmu_pages);
 +	if (account_nx)
 +		account_huge_nx_page(kvm, sp);
 +
 +	if (shared)
 +		spin_unlock(&kvm->arch.tdp_mmu_pages_lock);
 +}
 +
 +/**
 + * tdp_mmu_unlink_page - Remove page from the list of pages used by the TDP MMU
++=======
+  * tdp_mmu_unlink_sp() - Remove a shadow page from the list of used pages
++>>>>>>> 7b7e1ab6fdc5 (KVM: x86/mmu: Consolidate logic to atomically install a new TDP MMU page table)
   *
   * @kvm: kvm instance
   * @sp: the page to be removed
@@@ -937,17 -1002,11 +973,15 @@@ int kvm_tdp_mmu_map(struct kvm_vcpu *vc
  	struct kvm_mmu *mmu = vcpu->arch.mmu;
  	struct tdp_iter iter;
  	struct kvm_mmu_page *sp;
- 	u64 *child_pt;
- 	u64 new_spte;
  	int ret;
 +	gfn_t gfn = gpa >> PAGE_SHIFT;
 +	int level;
 +	int req_level;
  
 -	kvm_mmu_hugepage_adjust(vcpu, fault);
 +	level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn,
 +					huge_page_disallowed, &req_level);
  
 -	trace_kvm_mmu_spte_requested(fault);
 +	trace_kvm_mmu_spte_requested(gpa, level, pfn);
  
  	rcu_read_lock();
  
@@@ -978,28 -1036,19 +1012,36 @@@
  		}
  
  		if (!is_shadow_present_pte(iter.old_spte)) {
+ 			bool account_nx = fault->huge_page_disallowed &&
+ 					  fault->req_level >= iter.level;
+ 
  			/*
 -			 * If SPTE has been frozen by another thread, just
 +			 * If SPTE has been forzen by another thread, just
  			 * give up and retry, avoiding unnecessary page table
  			 * allocation and free.
  			 */
  			if (is_removed_spte(iter.old_spte))
  				break;
  
++<<<<<<< HEAD
 +			sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
 +			child_pt = sp->spt;
 +
 +			new_spte = make_nonleaf_spte(child_pt,
 +						     !shadow_accessed_mask);
 +
 +			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter,
 +						    new_spte)) {
 +				tdp_mmu_link_page(vcpu->kvm, sp, true,
 +						  huge_page_disallowed &&
 +						  req_level >= iter.level);
 +
 +				trace_kvm_mmu_get_page(sp, true);
 +			} else {
++=======
+ 			sp = tdp_mmu_alloc_sp(vcpu, iter.gfn, iter.level - 1);
+ 			if (tdp_mmu_link_sp_atomic(vcpu->kvm, &iter, sp, account_nx)) {
++>>>>>>> 7b7e1ab6fdc5 (KVM: x86/mmu: Consolidate logic to atomically install a new TDP MMU page table)
  				tdp_mmu_free_sp(sp);
  				break;
  			}
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
