dmaengine: idxd: refactor wq driver enable/disable operations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit 63c14ae6c161dec8ff3be49277edc75a769e054a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/63c14ae6.failed

Move the core driver operations from wq driver to the drv_enable_wq() and
drv_disable_wq() functions. The move should reduce the wq driver's
knowledge of the core driver operations and prevent code confusion for
future wq drivers.

	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/165047301643.3841827.11222723219862233060.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit 63c14ae6c161dec8ff3be49277edc75a769e054a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/cdev.c
#	drivers/dma/idxd/device.c
#	drivers/dma/idxd/dma.c
#	drivers/dma/idxd/idxd.h
diff --cc drivers/dma/idxd/cdev.c
index 6ecf0e5f3285,b670b75885ad..000000000000
--- a/drivers/dma/idxd/cdev.c
+++ b/drivers/dma/idxd/cdev.c
@@@ -299,10 -299,67 +299,70 @@@ void idxd_wq_del_cdev(struct idxd_wq *w
  
  	idxd_cdev = wq->idxd_cdev;
  	wq->idxd_cdev = NULL;
 -	cdev_device_del(&idxd_cdev->cdev, cdev_dev(idxd_cdev));
 -	put_device(cdev_dev(idxd_cdev));
 +	cdev_device_del(&idxd_cdev->cdev, &idxd_cdev->dev);
 +	put_device(&idxd_cdev->dev);
  }
  
++<<<<<<< HEAD
++=======
+ static int idxd_user_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+ 	struct idxd_device *idxd = wq->idxd;
+ 	int rc;
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED)
+ 		return -ENXIO;
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	wq->type = IDXD_WQT_USER;
+ 	rc = drv_enable_wq(wq);
+ 	if (rc < 0)
+ 		goto err;
+ 
+ 	rc = idxd_wq_add_cdev(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_CDEV_ERR;
+ 		goto err_cdev;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	mutex_unlock(&wq->wq_lock);
+ 	return 0;
+ 
+ err_cdev:
+ 	drv_disable_wq(wq);
+ err:
+ 	wq->type = IDXD_WQT_NONE;
+ 	mutex_unlock(&wq->wq_lock);
+ 	return rc;
+ }
+ 
+ static void idxd_user_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	idxd_wq_del_cdev(wq);
+ 	drv_disable_wq(wq);
+ 	wq->type = IDXD_WQT_NONE;
+ 	mutex_unlock(&wq->wq_lock);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_WQ,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_user_drv = {
+ 	.probe = idxd_user_drv_probe,
+ 	.remove = idxd_user_drv_remove,
+ 	.name = "user",
+ 	.type = dev_types,
+ };
+ EXPORT_SYMBOL_GPL(idxd_user_drv);
+ 
++>>>>>>> 63c14ae6c161 (dmaengine: idxd: refactor wq driver enable/disable operations)
  int idxd_cdev_register(void)
  {
  	int rc, i;
diff --cc drivers/dma/idxd/device.c
index cac50f25271e,22ad9ee383e2..000000000000
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@@ -1129,3 -1126,342 +1129,345 @@@ int idxd_device_load_config(struct idxd
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void idxd_flush_pending_descs(struct idxd_irq_entry *ie)
+ {
+ 	struct idxd_desc *desc, *itr;
+ 	struct llist_node *head;
+ 	LIST_HEAD(flist);
+ 	enum idxd_complete_type ctype;
+ 
+ 	spin_lock(&ie->list_lock);
+ 	head = llist_del_all(&ie->pending_llist);
+ 	if (head) {
+ 		llist_for_each_entry_safe(desc, itr, head, llnode)
+ 			list_add_tail(&desc->list, &ie->work_list);
+ 	}
+ 
+ 	list_for_each_entry_safe(desc, itr, &ie->work_list, list)
+ 		list_move_tail(&desc->list, &flist);
+ 	spin_unlock(&ie->list_lock);
+ 
+ 	list_for_each_entry_safe(desc, itr, &flist, list) {
+ 		list_del(&desc->list);
+ 		ctype = desc->completion->status ? IDXD_COMPLETE_NORMAL : IDXD_COMPLETE_ABORT;
+ 		idxd_dma_complete_txd(desc, ctype, true);
+ 	}
+ }
+ 
+ static void idxd_device_set_perm_entry(struct idxd_device *idxd,
+ 				       struct idxd_irq_entry *ie)
+ {
+ 	union msix_perm mperm;
+ 
+ 	if (ie->pasid == INVALID_IOASID)
+ 		return;
+ 
+ 	mperm.bits = 0;
+ 	mperm.pasid = ie->pasid;
+ 	mperm.pasid_en = 1;
+ 	iowrite32(mperm.bits, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ static void idxd_device_clear_perm_entry(struct idxd_device *idxd,
+ 					 struct idxd_irq_entry *ie)
+ {
+ 	iowrite32(0, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ void idxd_wq_free_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct idxd_irq_entry *ie = &wq->ie;
+ 
+ 	synchronize_irq(ie->vector);
+ 	free_irq(ie->vector, ie);
+ 	idxd_flush_pending_descs(ie);
+ 	if (idxd->request_int_handles)
+ 		idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->vector = -1;
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	ie->pasid = INVALID_IOASID;
+ }
+ 
+ int idxd_wq_request_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct pci_dev *pdev = idxd->pdev;
+ 	struct device *dev = &pdev->dev;
+ 	struct idxd_irq_entry *ie;
+ 	int rc;
+ 
+ 	if (wq->type != IDXD_WQT_KERNEL)
+ 		return 0;
+ 
+ 	ie = &wq->ie;
+ 	ie->vector = pci_irq_vector(pdev, ie->id);
+ 	ie->pasid = device_pasid_enabled(idxd) ? idxd->pasid : INVALID_IOASID;
+ 	idxd_device_set_perm_entry(idxd, ie);
+ 
+ 	rc = request_threaded_irq(ie->vector, NULL, idxd_wq_thread, 0, "idxd-portal", ie);
+ 	if (rc < 0) {
+ 		dev_err(dev, "Failed to request irq %d.\n", ie->vector);
+ 		goto err_irq;
+ 	}
+ 
+ 	if (idxd->request_int_handles) {
+ 		rc = idxd_device_request_int_handle(idxd, ie->id, &ie->int_handle,
+ 						    IDXD_IRQ_MSIX);
+ 		if (rc < 0)
+ 			goto err_int_handle;
+ 	} else {
+ 		ie->int_handle = ie->id;
+ 	}
+ 
+ 	return 0;
+ 
+ err_int_handle:
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	free_irq(ie->vector, ie);
+ err_irq:
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->pasid = INVALID_IOASID;
+ 	return rc;
+ }
+ 
+ int drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	int rc = -ENXIO;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_NOT_ENABLED;
+ 		goto err;
+ 	}
+ 
+ 	if (wq->state != IDXD_WQ_DISABLED) {
+ 		dev_dbg(dev, "wq %d already enabled.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_ENABLED;
+ 		rc = -EBUSY;
+ 		goto err;
+ 	}
+ 
+ 	if (!wq->group) {
+ 		dev_dbg(dev, "wq %d not attached to group.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_GRP;
+ 		goto err;
+ 	}
+ 
+ 	if (strlen(wq->name) == 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_NAME;
+ 		dev_dbg(dev, "wq %d name not set.\n", wq->id);
+ 		goto err;
+ 	}
+ 
+ 	/* Shared WQ checks */
+ 	if (wq_shared(wq)) {
+ 		if (!device_swq_supported(idxd)) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_SVM;
+ 			dev_dbg(dev, "PASID not enabled and shared wq.\n");
+ 			goto err;
+ 		}
+ 		/*
+ 		 * Shared wq with the threshold set to 0 means the user
+ 		 * did not set the threshold or transitioned from a
+ 		 * dedicated wq but did not set threshold. A value
+ 		 * of 0 would effectively disable the shared wq. The
+ 		 * driver does not allow a value of 0 to be set for
+ 		 * threshold via sysfs.
+ 		 */
+ 		if (wq->threshold == 0) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_THRESH;
+ 			dev_dbg(dev, "Shared wq and threshold 0.\n");
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * In the event that the WQ is configurable for pasid and priv bits.
+ 	 * For kernel wq, the driver should setup the pasid, pasid_en, and priv bit.
+ 	 * However, for non-kernel wq, the driver should only set the pasid_en bit for
+ 	 * shared wq. A dedicated wq that is not 'kernel' type will configure pasid and
+ 	 * pasid_en later on so there is no need to setup.
+ 	 */
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags)) {
+ 		int priv = 0;
+ 
+ 		if (device_pasid_enabled(idxd)) {
+ 			if (is_idxd_wq_kernel(wq) || wq_shared(wq)) {
+ 				u32 pasid = wq_dedicated(wq) ? idxd->pasid : 0;
+ 
+ 				__idxd_wq_set_pasid_locked(wq, pasid);
+ 			}
+ 		}
+ 
+ 		if (is_idxd_wq_kernel(wq))
+ 			priv = 1;
+ 		__idxd_wq_set_priv_locked(wq, priv);
+ 	}
+ 
+ 	rc = 0;
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Writing wq %d config failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_enable(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "wq %d enabling failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_map_portal(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_PORTAL_ERR;
+ 		dev_dbg(dev, "wq %d portal mapping failed: %d\n", wq->id, rc);
+ 		goto err_map_portal;
+ 	}
+ 
+ 	wq->client_count = 0;
+ 
+ 	rc = idxd_wq_request_irq(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_IRQ_ERR;
+ 		dev_dbg(dev, "WQ %d irq setup failed: %d\n", wq->id, rc);
+ 		goto err_irq;
+ 	}
+ 
+ 	rc = idxd_wq_alloc_resources(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_RES_ALLOC_ERR;
+ 		dev_dbg(dev, "WQ resource alloc failed\n");
+ 		goto err_res_alloc;
+ 	}
+ 
+ 	rc = idxd_wq_init_percpu_ref(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_PERCPU_ERR;
+ 		dev_dbg(dev, "percpu_ref setup failed\n");
+ 		goto err_ref;
+ 	}
+ 
+ 	return 0;
+ 
+ err_ref:
+ 	idxd_wq_free_resources(wq);
+ err_res_alloc:
+ 	idxd_wq_free_irq(wq);
+ err_irq:
+ 	idxd_wq_unmap_portal(wq);
+ err_map_portal:
+ 	rc = idxd_wq_disable(wq, false);
+ 	if (rc < 0)
+ 		dev_dbg(dev, "wq %s disable failed\n", dev_name(wq_confdev(wq)));
+ err:
+ 	return rc;
+ }
+ 
+ void drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd_wq_refcount(wq))
+ 		dev_warn(dev, "Clients has claim on wq %d: %d\n",
+ 			 wq->id, idxd_wq_refcount(wq));
+ 
+ 	idxd_wq_free_resources(wq);
+ 	idxd_wq_unmap_portal(wq);
+ 	idxd_wq_drain(wq);
+ 	idxd_wq_reset(wq);
+ 	percpu_ref_exit(&wq->wq_active);
+ 	idxd_wq_free_irq(wq);
+ 	wq->type = IDXD_WQT_NONE;
+ 	wq->client_count = 0;
+ }
+ 
+ int idxd_device_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int rc = 0;
+ 
+ 	/*
+ 	 * Device should be in disabled state for the idxd_drv to load. If it's in
+ 	 * enabled state, then the device was altered outside of driver's control.
+ 	 * If the state is in halted state, then we don't want to proceed.
+ 	 */
+ 	if (idxd->state != IDXD_DEV_DISABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_ENABLED;
+ 		return -ENXIO;
+ 	}
+ 
+ 	/* Device configuration */
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0)
+ 		return -ENXIO;
+ 
+ 	/* Start device */
+ 	rc = idxd_device_enable(idxd);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	/* Setup DMA device without channels */
+ 	rc = idxd_register_dma_device(idxd);
+ 	if (rc < 0) {
+ 		idxd_device_disable(idxd);
+ 		idxd->cmd_status = IDXD_SCMD_DEV_DMA_ERR;
+ 		return rc;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	return 0;
+ }
+ 
+ void idxd_device_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int i;
+ 
+ 	for (i = 0; i < idxd->max_wqs; i++) {
+ 		struct idxd_wq *wq = idxd->wqs[i];
+ 		struct device *wq_dev = wq_confdev(wq);
+ 
+ 		if (wq->state == IDXD_WQ_DISABLED)
+ 			continue;
+ 		dev_warn(dev, "Active wq %d on disable %s.\n", i, dev_name(wq_dev));
+ 		device_release_driver(wq_dev);
+ 	}
+ 
+ 	idxd_unregister_dma_device(idxd);
+ 	idxd_device_disable(idxd);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		idxd_device_reset(idxd);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_DSA,
+ 	IDXD_DEV_IAX,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_drv = {
+ 	.type = dev_types,
+ 	.probe = idxd_device_drv_probe,
+ 	.remove = idxd_device_drv_remove,
+ 	.name = "idxd",
+ };
+ EXPORT_SYMBOL_GPL(idxd_drv);
++>>>>>>> 63c14ae6c161 (dmaengine: idxd: refactor wq driver enable/disable operations)
diff --cc drivers/dma/idxd/dma.c
index 0d6d3c7052df,950f06c8aad5..000000000000
--- a/drivers/dma/idxd/dma.c
+++ b/drivers/dma/idxd/dma.c
@@@ -261,5 -275,68 +261,71 @@@ void idxd_unregister_dma_channel(struc
  	list_del(&chan->device_node);
  	kfree(wq->idxd_chan);
  	wq->idxd_chan = NULL;
 -	put_device(wq_confdev(wq));
 +	put_device(&wq->conf_dev);
  }
++<<<<<<< HEAD
++=======
+ 
+ static int idxd_dmaengine_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+ 	struct idxd_device *idxd = wq->idxd;
+ 	int rc;
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED)
+ 		return -ENXIO;
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	wq->type = IDXD_WQT_KERNEL;
+ 
+ 	rc = drv_enable_wq(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Enable wq %d failed: %d\n", wq->id, rc);
+ 		rc = -ENXIO;
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_register_dma_channel(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_DMA_CHAN_ERR;
+ 		dev_dbg(dev, "Failed to register dma channel\n");
+ 		goto err_dma;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	mutex_unlock(&wq->wq_lock);
+ 	return 0;
+ 
+ err_dma:
+ 	drv_disable_wq(wq);
+ err:
+ 	wq->type = IDXD_WQT_NONE;
+ 	mutex_unlock(&wq->wq_lock);
+ 	return rc;
+ }
+ 
+ static void idxd_dmaengine_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);
+ 
+ 	mutex_lock(&wq->wq_lock);
+ 	__idxd_wq_quiesce(wq);
+ 	idxd_unregister_dma_channel(wq);
+ 	drv_disable_wq(wq);
+ 	mutex_unlock(&wq->wq_lock);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_WQ,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_dmaengine_drv = {
+ 	.probe = idxd_dmaengine_drv_probe,
+ 	.remove = idxd_dmaengine_drv_remove,
+ 	.name = "dmaengine",
+ 	.type = dev_types,
+ };
+ EXPORT_SYMBOL_GPL(idxd_dmaengine_drv);
++>>>>>>> 63c14ae6c161 (dmaengine: idxd: refactor wq driver enable/disable operations)
diff --cc drivers/dma/idxd/idxd.h
index 7bd9aa9ff9c5,8e03fb548d13..000000000000
--- a/drivers/dma/idxd/idxd.h
+++ b/drivers/dma/idxd/idxd.h
@@@ -436,11 -552,14 +436,20 @@@ irqreturn_t idxd_misc_thread(int vec, v
  irqreturn_t idxd_wq_thread(int irq, void *data);
  void idxd_mask_error_interrupts(struct idxd_device *idxd);
  void idxd_unmask_error_interrupts(struct idxd_device *idxd);
 +void idxd_mask_msix_vectors(struct idxd_device *idxd);
 +void idxd_mask_msix_vector(struct idxd_device *idxd, int vec_id);
 +void idxd_unmask_msix_vector(struct idxd_device *idxd, int vec_id);
  
  /* device control */
++<<<<<<< HEAD
++=======
+ int idxd_register_idxd_drv(void);
+ void idxd_unregister_idxd_drv(void);
+ int idxd_device_drv_probe(struct idxd_dev *idxd_dev);
+ void idxd_device_drv_remove(struct idxd_dev *idxd_dev);
+ int drv_enable_wq(struct idxd_wq *wq);
+ void drv_disable_wq(struct idxd_wq *wq);
++>>>>>>> 63c14ae6c161 (dmaengine: idxd: refactor wq driver enable/disable operations)
  int idxd_device_init_reset(struct idxd_device *idxd);
  int idxd_device_enable(struct idxd_device *idxd);
  int idxd_device_disable(struct idxd_device *idxd);
* Unmerged path drivers/dma/idxd/cdev.c
* Unmerged path drivers/dma/idxd/device.c
* Unmerged path drivers/dma/idxd/dma.c
* Unmerged path drivers/dma/idxd/idxd.h
