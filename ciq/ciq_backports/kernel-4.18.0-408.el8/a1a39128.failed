KVM: MMU: propagate alloc_workqueue failure

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Paolo Bonzini <pbonzini@redhat.com>
commit a1a39128faabc9883a7f9e3f8777b3fbd560fa5f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/a1a39128.failed

If kvm->arch.tdp_mmu_zap_wq cannot be created, the failure has
to be propagated up to kvm_mmu_init_vm and kvm_arch_init_vm.
kvm_arch_init_vm also has to undo all the initialization, so
group all the MMU initialization code at the beginning and
handle cleaning up of kvm_page_track_init.

	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit a1a39128faabc9883a7f9e3f8777b3fbd560fa5f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/mmu/mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.c
#	arch/x86/kvm/mmu/tdp_mmu.h
diff --cc arch/x86/kvm/mmu/mmu.c
index ad983809b045,857ba93b5c92..000000000000
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@@ -5667,13 -5768,19 +5667,25 @@@ static void kvm_mmu_invalidate_zap_page
  	kvm_mmu_zap_all_fast(kvm);
  }
  
- void kvm_mmu_init_vm(struct kvm *kvm)
+ int kvm_mmu_init_vm(struct kvm *kvm)
  {
  	struct kvm_page_track_notifier_node *node = &kvm->arch.mmu_sp_tracker;
+ 	int r;
  
++<<<<<<< HEAD
 +	kvm_mmu_init_tdp_mmu(kvm);
++=======
+ 	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+ 	INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
+ 	INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
+ 	spin_lock_init(&kvm->arch.mmu_unsync_pages_lock);
+ 
+ 	r = kvm_mmu_init_tdp_mmu(kvm);
+ 	if (r < 0)
+ 		return r;
++>>>>>>> a1a39128faab (KVM: MMU: propagate alloc_workqueue failure)
 +
 +	kvm->arch.memslots_have_rmaps = true;
  
  	node->track_write = kvm_mmu_pte_write;
  	node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
diff --cc arch/x86/kvm/mmu/tdp_mmu.c
index 97bb57fe39ca,4be517a9f22f..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@@ -14,17 -14,36 +14,47 @@@ static bool __read_mostly tdp_mmu_enabl
  module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0644);
  
  /* Initializes the TDP MMU for the VM, if enabled. */
++<<<<<<< HEAD
 +void kvm_mmu_init_tdp_mmu(struct kvm *kvm)
++=======
+ int kvm_mmu_init_tdp_mmu(struct kvm *kvm)
++>>>>>>> a1a39128faab (KVM: MMU: propagate alloc_workqueue failure)
  {
+ 	struct workqueue_struct *wq;
+ 
  	if (!tdp_enabled || !READ_ONCE(tdp_mmu_enabled))
++<<<<<<< HEAD
 +		return;
++=======
+ 		return 0;
+ 
+ 	wq = alloc_workqueue("kvm", WQ_UNBOUND|WQ_MEM_RECLAIM|WQ_CPU_INTENSIVE, 0);
+ 	if (!wq)
+ 		return -ENOMEM;
++>>>>>>> a1a39128faab (KVM: MMU: propagate alloc_workqueue failure)
  
  	/* This should not be changed for the lifetime of the VM. */
  	kvm->arch.tdp_mmu_enabled = true;
  	INIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);
  	spin_lock_init(&kvm->arch.tdp_mmu_pages_lock);
  	INIT_LIST_HEAD(&kvm->arch.tdp_mmu_pages);
++<<<<<<< HEAD
++=======
+ 	kvm->arch.tdp_mmu_zap_wq = wq;
+ 	return 1;
+ }
+ 
+ /* Arbitrarily returns true so that this may be used in if statements. */
+ static __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,
+ 							     bool shared)
+ {
+ 	if (shared)
+ 		lockdep_assert_held_read(&kvm->mmu_lock);
+ 	else
+ 		lockdep_assert_held_write(&kvm->mmu_lock);
+ 
+ 	return true;
++>>>>>>> a1a39128faab (KVM: MMU: propagate alloc_workqueue failure)
  }
  
  void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)
diff --cc arch/x86/kvm/mmu/tdp_mmu.h
index e1f1ae8ec3e2,647926541e38..000000000000
--- a/arch/x86/kvm/mmu/tdp_mmu.h
+++ b/arch/x86/kvm/mmu/tdp_mmu.h
@@@ -90,9 -72,8 +90,13 @@@ u64 *kvm_tdp_mmu_fast_pf_get_last_sptep
  					u64 *spte);
  
  #ifdef CONFIG_X86_64
++<<<<<<< HEAD
 +void kvm_mmu_init_tdp_mmu(struct kvm *kvm);
++=======
+ int kvm_mmu_init_tdp_mmu(struct kvm *kvm);
++>>>>>>> a1a39128faab (KVM: MMU: propagate alloc_workqueue failure)
  void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm);
 +static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return kvm->arch.tdp_mmu_enabled; }
  static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }
  
  static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
@@@ -112,9 -93,8 +116,13 @@@
  	return sp && is_tdp_mmu_page(sp) && sp->root_count;
  }
  #else
++<<<<<<< HEAD
 +static inline void kvm_mmu_init_tdp_mmu(struct kvm *kvm) {}
++=======
+ static inline int kvm_mmu_init_tdp_mmu(struct kvm *kvm) { return 0; }
++>>>>>>> a1a39128faab (KVM: MMU: propagate alloc_workqueue failure)
  static inline void kvm_mmu_uninit_tdp_mmu(struct kvm *kvm) {}
 +static inline bool is_tdp_mmu_enabled(struct kvm *kvm) { return false; }
  static inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return false; }
  static inline bool is_tdp_mmu(struct kvm_mmu *mmu) { return false; }
  #endif
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 96ecf6f2c1a5..4379e0ed7dcb 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1568,7 +1568,7 @@ void kvm_mmu_module_exit(void);
 
 void kvm_mmu_destroy(struct kvm_vcpu *vcpu);
 int kvm_mmu_create(struct kvm_vcpu *vcpu);
-void kvm_mmu_init_vm(struct kvm *kvm);
+int kvm_mmu_init_vm(struct kvm *kvm);
 void kvm_mmu_uninit_vm(struct kvm *kvm);
 
 void kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu);
* Unmerged path arch/x86/kvm/mmu/mmu.c
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.c
* Unmerged path arch/x86/kvm/mmu/tdp_mmu.h
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index e014b7ea74df..6bb497d32bfb 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -11573,12 +11573,13 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	ret = kvm_page_track_init(kvm);
 	if (ret)
-		return ret;
+		goto out;
+
+	ret = kvm_mmu_init_vm(kvm);
+	if (ret)
+		goto out_page_track;
 
 	INIT_HLIST_HEAD(&kvm->arch.mask_notifier_list);
-	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
-	INIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);
-	INIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);
 	INIT_LIST_HEAD(&kvm->arch.assigned_dev_head);
 	atomic_set(&kvm->arch.noncoherent_dma_count, 0);
 
@@ -11607,10 +11608,14 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm_apicv_init(kvm);
 	kvm_hv_init_vm(kvm);
-	kvm_mmu_init_vm(kvm);
 	kvm_xen_init_vm(kvm);
 
 	return static_call(kvm_x86_vm_init)(kvm);
+
+out_page_track:
+	kvm_page_track_cleanup(kvm);
+out:
+	return ret;
 }
 
 int kvm_arch_post_init_vm(struct kvm *kvm)
