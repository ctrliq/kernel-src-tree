net/smc: Avoid overwriting the copies of clcsock callback functions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Wen Gu <guwen@linux.alibaba.com>
commit 1de9770d121ee9294794cca0e0be8fbfa0134ee8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/1de9770d.failed

The callback functions of clcsock will be saved and replaced during
the fallback. But if the fallback happens more than once, then the
copies of these callback functions will be overwritten incorrectly,
resulting in a loop call issue:

clcsk->sk_error_report
 |- smc_fback_error_report() <------------------------------|
     |- smc_fback_forward_wakeup()                          | (loop)
         |- clcsock_callback()  (incorrectly overwritten)   |
             |- smc->clcsk_error_report() ------------------|

So this patch fixes the issue by saving these function pointers only
once in the fallback and avoiding overwriting.

	Reported-by: syzbot+4de3c0e8a263e1e499bc@syzkaller.appspotmail.com
Fixes: 341adeec9ada ("net/smc: Forward wakeup to smc socket waitqueue after fallback")
Link: https://lore.kernel.org/r/0000000000006d045e05d78776f6@google.com
	Signed-off-by: Wen Gu <guwen@linux.alibaba.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 1de9770d121ee9294794cca0e0be8fbfa0134ee8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/smc/af_smc.c
diff --cc net/smc/af_smc.c
index feb957973c38,306d9e8cd1dd..000000000000
--- a/net/smc/af_smc.c
+++ b/net/smc/af_smc.c
@@@ -563,17 -566,148 +563,135 @@@ static void smc_stat_fallback(struct sm
  	mutex_unlock(&net->smc.mutex_fback_rsn);
  }
  
 -/* must be called under rcu read lock */
 -static void smc_fback_wakeup_waitqueue(struct smc_sock *smc, void *key)
 +static void smc_switch_to_fallback(struct smc_sock *smc, int reason_code)
  {
++<<<<<<< HEAD
++=======
+ 	struct socket_wq *wq;
+ 	__poll_t flags;
+ 
+ 	wq = rcu_dereference(smc->sk.sk_wq);
+ 	if (!skwq_has_sleeper(wq))
+ 		return;
+ 
+ 	/* wake up smc sk->sk_wq */
+ 	if (!key) {
+ 		/* sk_state_change */
+ 		wake_up_interruptible_all(&wq->wait);
+ 	} else {
+ 		flags = key_to_poll(key);
+ 		if (flags & (EPOLLIN | EPOLLOUT))
+ 			/* sk_data_ready or sk_write_space */
+ 			wake_up_interruptible_sync_poll(&wq->wait, flags);
+ 		else if (flags & EPOLLERR)
+ 			/* sk_error_report */
+ 			wake_up_interruptible_poll(&wq->wait, flags);
+ 	}
+ }
+ 
+ static int smc_fback_mark_woken(wait_queue_entry_t *wait,
+ 				unsigned int mode, int sync, void *key)
+ {
+ 	struct smc_mark_woken *mark =
+ 		container_of(wait, struct smc_mark_woken, wait_entry);
+ 
+ 	mark->woken = true;
+ 	mark->key = key;
+ 	return 0;
+ }
+ 
+ static void smc_fback_forward_wakeup(struct smc_sock *smc, struct sock *clcsk,
+ 				     void (*clcsock_callback)(struct sock *sk))
+ {
+ 	struct smc_mark_woken mark = { .woken = false };
+ 	struct socket_wq *wq;
+ 
+ 	init_waitqueue_func_entry(&mark.wait_entry,
+ 				  smc_fback_mark_woken);
+ 	rcu_read_lock();
+ 	wq = rcu_dereference(clcsk->sk_wq);
+ 	if (!wq)
+ 		goto out;
+ 	add_wait_queue(sk_sleep(clcsk), &mark.wait_entry);
+ 	clcsock_callback(clcsk);
+ 	remove_wait_queue(sk_sleep(clcsk), &mark.wait_entry);
+ 
+ 	if (mark.woken)
+ 		smc_fback_wakeup_waitqueue(smc, mark.key);
+ out:
+ 	rcu_read_unlock();
+ }
+ 
+ static void smc_fback_state_change(struct sock *clcsk)
+ {
+ 	struct smc_sock *smc =
+ 		smc_clcsock_user_data(clcsk);
+ 
+ 	if (!smc)
+ 		return;
+ 	smc_fback_forward_wakeup(smc, clcsk, smc->clcsk_state_change);
+ }
+ 
+ static void smc_fback_data_ready(struct sock *clcsk)
+ {
+ 	struct smc_sock *smc =
+ 		smc_clcsock_user_data(clcsk);
+ 
+ 	if (!smc)
+ 		return;
+ 	smc_fback_forward_wakeup(smc, clcsk, smc->clcsk_data_ready);
+ }
+ 
+ static void smc_fback_write_space(struct sock *clcsk)
+ {
+ 	struct smc_sock *smc =
+ 		smc_clcsock_user_data(clcsk);
+ 
+ 	if (!smc)
+ 		return;
+ 	smc_fback_forward_wakeup(smc, clcsk, smc->clcsk_write_space);
+ }
+ 
+ static void smc_fback_error_report(struct sock *clcsk)
+ {
+ 	struct smc_sock *smc =
+ 		smc_clcsock_user_data(clcsk);
+ 
+ 	if (!smc)
+ 		return;
+ 	smc_fback_forward_wakeup(smc, clcsk, smc->clcsk_error_report);
+ }
+ 
+ static int smc_switch_to_fallback(struct smc_sock *smc, int reason_code)
+ {
+ 	struct sock *clcsk;
+ 	int rc = 0;
+ 
+ 	mutex_lock(&smc->clcsock_release_lock);
+ 	if (!smc->clcsock) {
+ 		rc = -EBADF;
+ 		goto out;
+ 	}
+ 	clcsk = smc->clcsock->sk;
+ 
+ 	if (smc->use_fallback)
+ 		goto out;
++>>>>>>> 1de9770d121e (net/smc: Avoid overwriting the copies of clcsock callback functions)
  	smc->use_fallback = true;
  	smc->fallback_rsn = reason_code;
  	smc_stat_fallback(smc);
  	if (smc->sk.sk_socket && smc->sk.sk_socket->file) {
  		smc->clcsock->file = smc->sk.sk_socket->file;
  		smc->clcsock->file->private_data = smc->clcsock;
 -		smc->clcsock->wq.fasync_list =
 -			smc->sk.sk_socket->wq.fasync_list;
 -
 -		/* There might be some wait entries remaining
 -		 * in smc sk->sk_wq and they should be woken up
 -		 * as clcsock's wait queue is woken up.
 -		 */
 -		smc->clcsk_state_change = clcsk->sk_state_change;
 -		smc->clcsk_data_ready = clcsk->sk_data_ready;
 -		smc->clcsk_write_space = clcsk->sk_write_space;
 -		smc->clcsk_error_report = clcsk->sk_error_report;
 -
 -		clcsk->sk_state_change = smc_fback_state_change;
 -		clcsk->sk_data_ready = smc_fback_data_ready;
 -		clcsk->sk_write_space = smc_fback_write_space;
 -		clcsk->sk_error_report = smc_fback_error_report;
 -
 -		smc->clcsock->sk->sk_user_data =
 -			(void *)((uintptr_t)smc | SK_USER_DATA_NOCOPY);
 +		smc->clcsock->wq->fasync_list =
 +			smc->sk.sk_socket->wq->fasync_list;
  	}
++<<<<<<< HEAD
++=======
+ out:
+ 	mutex_unlock(&smc->clcsock_release_lock);
+ 	return rc;
++>>>>>>> 1de9770d121e (net/smc: Avoid overwriting the copies of clcsock callback functions)
  }
  
  /* fall back during connect */
* Unmerged path net/smc/af_smc.c
