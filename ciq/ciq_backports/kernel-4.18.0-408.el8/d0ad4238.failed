dmaengine: idxd: skip irq free when wq type is not kernel

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Dave Jiang <dave.jiang@intel.com>
commit d0ad42388a396813771e9407614f40d128ad62db
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/d0ad4238.failed

Skip wq irq resources freeing when wq type is not kernel since the driver
skips the irq alloction during wq enable. Add check in wq type check in
idxd_wq_free_irq() to mirror idxd_wq_request_irq().

Fixes: 63c14ae6c161 ("dmaengine: idxd: refactor wq driver enable/disable operations")
	Reported-by: Tony Zu <tony.zhu@intel.com>
	Tested-by: Tony Zu <tony.zhu@intel.com>
	Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Link: https://lore.kernel.org/r/165176310726.2112428.7474366910758522079.stgit@djiang5-desk3.ch.intel.com
	Signed-off-by: Vinod Koul <vkoul@kernel.org>
(cherry picked from commit d0ad42388a396813771e9407614f40d128ad62db)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/dma/idxd/device.c
diff --cc drivers/dma/idxd/device.c
index 558d3f457da6,8b1f8591ae83..000000000000
--- a/drivers/dma/idxd/device.c
+++ b/drivers/dma/idxd/device.c
@@@ -1128,3 -1125,345 +1128,348 @@@ int idxd_device_load_config(struct idxd
  
  	return 0;
  }
++<<<<<<< HEAD
++=======
+ 
+ static void idxd_flush_pending_descs(struct idxd_irq_entry *ie)
+ {
+ 	struct idxd_desc *desc, *itr;
+ 	struct llist_node *head;
+ 	LIST_HEAD(flist);
+ 	enum idxd_complete_type ctype;
+ 
+ 	spin_lock(&ie->list_lock);
+ 	head = llist_del_all(&ie->pending_llist);
+ 	if (head) {
+ 		llist_for_each_entry_safe(desc, itr, head, llnode)
+ 			list_add_tail(&desc->list, &ie->work_list);
+ 	}
+ 
+ 	list_for_each_entry_safe(desc, itr, &ie->work_list, list)
+ 		list_move_tail(&desc->list, &flist);
+ 	spin_unlock(&ie->list_lock);
+ 
+ 	list_for_each_entry_safe(desc, itr, &flist, list) {
+ 		list_del(&desc->list);
+ 		ctype = desc->completion->status ? IDXD_COMPLETE_NORMAL : IDXD_COMPLETE_ABORT;
+ 		idxd_dma_complete_txd(desc, ctype, true);
+ 	}
+ }
+ 
+ static void idxd_device_set_perm_entry(struct idxd_device *idxd,
+ 				       struct idxd_irq_entry *ie)
+ {
+ 	union msix_perm mperm;
+ 
+ 	if (ie->pasid == INVALID_IOASID)
+ 		return;
+ 
+ 	mperm.bits = 0;
+ 	mperm.pasid = ie->pasid;
+ 	mperm.pasid_en = 1;
+ 	iowrite32(mperm.bits, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ static void idxd_device_clear_perm_entry(struct idxd_device *idxd,
+ 					 struct idxd_irq_entry *ie)
+ {
+ 	iowrite32(0, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);
+ }
+ 
+ void idxd_wq_free_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct idxd_irq_entry *ie = &wq->ie;
+ 
+ 	if (wq->type != IDXD_WQT_KERNEL)
+ 		return;
+ 
+ 	synchronize_irq(ie->vector);
+ 	free_irq(ie->vector, ie);
+ 	idxd_flush_pending_descs(ie);
+ 	if (idxd->request_int_handles)
+ 		idxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->vector = -1;
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	ie->pasid = INVALID_IOASID;
+ }
+ 
+ int idxd_wq_request_irq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct pci_dev *pdev = idxd->pdev;
+ 	struct device *dev = &pdev->dev;
+ 	struct idxd_irq_entry *ie;
+ 	int rc;
+ 
+ 	if (wq->type != IDXD_WQT_KERNEL)
+ 		return 0;
+ 
+ 	ie = &wq->ie;
+ 	ie->vector = pci_irq_vector(pdev, ie->id);
+ 	ie->pasid = device_pasid_enabled(idxd) ? idxd->pasid : INVALID_IOASID;
+ 	idxd_device_set_perm_entry(idxd, ie);
+ 
+ 	rc = request_threaded_irq(ie->vector, NULL, idxd_wq_thread, 0, "idxd-portal", ie);
+ 	if (rc < 0) {
+ 		dev_err(dev, "Failed to request irq %d.\n", ie->vector);
+ 		goto err_irq;
+ 	}
+ 
+ 	if (idxd->request_int_handles) {
+ 		rc = idxd_device_request_int_handle(idxd, ie->id, &ie->int_handle,
+ 						    IDXD_IRQ_MSIX);
+ 		if (rc < 0)
+ 			goto err_int_handle;
+ 	} else {
+ 		ie->int_handle = ie->id;
+ 	}
+ 
+ 	return 0;
+ 
+ err_int_handle:
+ 	ie->int_handle = INVALID_INT_HANDLE;
+ 	free_irq(ie->vector, ie);
+ err_irq:
+ 	idxd_device_clear_perm_entry(idxd, ie);
+ 	ie->pasid = INVALID_IOASID;
+ 	return rc;
+ }
+ 
+ int drv_enable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 	int rc = -ENXIO;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd->state != IDXD_DEV_ENABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_NOT_ENABLED;
+ 		goto err;
+ 	}
+ 
+ 	if (wq->state != IDXD_WQ_DISABLED) {
+ 		dev_dbg(dev, "wq %d already enabled.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_ENABLED;
+ 		rc = -EBUSY;
+ 		goto err;
+ 	}
+ 
+ 	if (!wq->group) {
+ 		dev_dbg(dev, "wq %d not attached to group.\n", wq->id);
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_GRP;
+ 		goto err;
+ 	}
+ 
+ 	if (strlen(wq->name) == 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_NO_NAME;
+ 		dev_dbg(dev, "wq %d name not set.\n", wq->id);
+ 		goto err;
+ 	}
+ 
+ 	/* Shared WQ checks */
+ 	if (wq_shared(wq)) {
+ 		if (!wq_shared_supported(wq)) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_SVM;
+ 			dev_dbg(dev, "PASID not enabled and shared wq.\n");
+ 			goto err;
+ 		}
+ 		/*
+ 		 * Shared wq with the threshold set to 0 means the user
+ 		 * did not set the threshold or transitioned from a
+ 		 * dedicated wq but did not set threshold. A value
+ 		 * of 0 would effectively disable the shared wq. The
+ 		 * driver does not allow a value of 0 to be set for
+ 		 * threshold via sysfs.
+ 		 */
+ 		if (wq->threshold == 0) {
+ 			idxd->cmd_status = IDXD_SCMD_WQ_NO_THRESH;
+ 			dev_dbg(dev, "Shared wq and threshold 0.\n");
+ 			goto err;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * In the event that the WQ is configurable for pasid and priv bits.
+ 	 * For kernel wq, the driver should setup the pasid, pasid_en, and priv bit.
+ 	 * However, for non-kernel wq, the driver should only set the pasid_en bit for
+ 	 * shared wq. A dedicated wq that is not 'kernel' type will configure pasid and
+ 	 * pasid_en later on so there is no need to setup.
+ 	 */
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags)) {
+ 		int priv = 0;
+ 
+ 		if (wq_pasid_enabled(wq)) {
+ 			if (is_idxd_wq_kernel(wq) || wq_shared(wq)) {
+ 				u32 pasid = wq_dedicated(wq) ? idxd->pasid : 0;
+ 
+ 				__idxd_wq_set_pasid_locked(wq, pasid);
+ 			}
+ 		}
+ 
+ 		if (is_idxd_wq_kernel(wq))
+ 			priv = 1;
+ 		__idxd_wq_set_priv_locked(wq, priv);
+ 	}
+ 
+ 	rc = 0;
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "Writing wq %d config failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_enable(wq);
+ 	if (rc < 0) {
+ 		dev_dbg(dev, "wq %d enabling failed: %d\n", wq->id, rc);
+ 		goto err;
+ 	}
+ 
+ 	rc = idxd_wq_map_portal(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_PORTAL_ERR;
+ 		dev_dbg(dev, "wq %d portal mapping failed: %d\n", wq->id, rc);
+ 		goto err_map_portal;
+ 	}
+ 
+ 	wq->client_count = 0;
+ 
+ 	rc = idxd_wq_request_irq(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_IRQ_ERR;
+ 		dev_dbg(dev, "WQ %d irq setup failed: %d\n", wq->id, rc);
+ 		goto err_irq;
+ 	}
+ 
+ 	rc = idxd_wq_alloc_resources(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_WQ_RES_ALLOC_ERR;
+ 		dev_dbg(dev, "WQ resource alloc failed\n");
+ 		goto err_res_alloc;
+ 	}
+ 
+ 	rc = idxd_wq_init_percpu_ref(wq);
+ 	if (rc < 0) {
+ 		idxd->cmd_status = IDXD_SCMD_PERCPU_ERR;
+ 		dev_dbg(dev, "percpu_ref setup failed\n");
+ 		goto err_ref;
+ 	}
+ 
+ 	return 0;
+ 
+ err_ref:
+ 	idxd_wq_free_resources(wq);
+ err_res_alloc:
+ 	idxd_wq_free_irq(wq);
+ err_irq:
+ 	idxd_wq_unmap_portal(wq);
+ err_map_portal:
+ 	rc = idxd_wq_disable(wq, false);
+ 	if (rc < 0)
+ 		dev_dbg(dev, "wq %s disable failed\n", dev_name(wq_confdev(wq)));
+ err:
+ 	return rc;
+ }
+ 
+ void drv_disable_wq(struct idxd_wq *wq)
+ {
+ 	struct idxd_device *idxd = wq->idxd;
+ 	struct device *dev = &idxd->pdev->dev;
+ 
+ 	lockdep_assert_held(&wq->wq_lock);
+ 
+ 	if (idxd_wq_refcount(wq))
+ 		dev_warn(dev, "Clients has claim on wq %d: %d\n",
+ 			 wq->id, idxd_wq_refcount(wq));
+ 
+ 	idxd_wq_free_resources(wq);
+ 	idxd_wq_unmap_portal(wq);
+ 	idxd_wq_drain(wq);
+ 	idxd_wq_free_irq(wq);
+ 	idxd_wq_reset(wq);
+ 	percpu_ref_exit(&wq->wq_active);
+ 	wq->type = IDXD_WQT_NONE;
+ 	wq->client_count = 0;
+ }
+ 
+ int idxd_device_drv_probe(struct idxd_dev *idxd_dev)
+ {
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int rc = 0;
+ 
+ 	/*
+ 	 * Device should be in disabled state for the idxd_drv to load. If it's in
+ 	 * enabled state, then the device was altered outside of driver's control.
+ 	 * If the state is in halted state, then we don't want to proceed.
+ 	 */
+ 	if (idxd->state != IDXD_DEV_DISABLED) {
+ 		idxd->cmd_status = IDXD_SCMD_DEV_ENABLED;
+ 		return -ENXIO;
+ 	}
+ 
+ 	/* Device configuration */
+ 	spin_lock(&idxd->dev_lock);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		rc = idxd_device_config(idxd);
+ 	spin_unlock(&idxd->dev_lock);
+ 	if (rc < 0)
+ 		return -ENXIO;
+ 
+ 	/* Start device */
+ 	rc = idxd_device_enable(idxd);
+ 	if (rc < 0)
+ 		return rc;
+ 
+ 	/* Setup DMA device without channels */
+ 	rc = idxd_register_dma_device(idxd);
+ 	if (rc < 0) {
+ 		idxd_device_disable(idxd);
+ 		idxd->cmd_status = IDXD_SCMD_DEV_DMA_ERR;
+ 		return rc;
+ 	}
+ 
+ 	idxd->cmd_status = 0;
+ 	return 0;
+ }
+ 
+ void idxd_device_drv_remove(struct idxd_dev *idxd_dev)
+ {
+ 	struct device *dev = &idxd_dev->conf_dev;
+ 	struct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);
+ 	int i;
+ 
+ 	for (i = 0; i < idxd->max_wqs; i++) {
+ 		struct idxd_wq *wq = idxd->wqs[i];
+ 		struct device *wq_dev = wq_confdev(wq);
+ 
+ 		if (wq->state == IDXD_WQ_DISABLED)
+ 			continue;
+ 		dev_warn(dev, "Active wq %d on disable %s.\n", i, dev_name(wq_dev));
+ 		device_release_driver(wq_dev);
+ 	}
+ 
+ 	idxd_unregister_dma_device(idxd);
+ 	idxd_device_disable(idxd);
+ 	if (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))
+ 		idxd_device_reset(idxd);
+ }
+ 
+ static enum idxd_dev_type dev_types[] = {
+ 	IDXD_DEV_DSA,
+ 	IDXD_DEV_IAX,
+ 	IDXD_DEV_NONE,
+ };
+ 
+ struct idxd_device_driver idxd_drv = {
+ 	.type = dev_types,
+ 	.probe = idxd_device_drv_probe,
+ 	.remove = idxd_device_drv_remove,
+ 	.name = "idxd",
+ };
+ EXPORT_SYMBOL_GPL(idxd_drv);
++>>>>>>> d0ad42388a39 (dmaengine: idxd: skip irq free when wq type is not kernel)
* Unmerged path drivers/dma/idxd/device.c
