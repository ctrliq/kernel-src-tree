powerpc/papr_scm: Fix buffer overflow issue with CONFIG_FORTIFY_SOURCE

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-408.el8
commit-author Kajol Jain <kjain@linux.ibm.com>
commit 348c71344111d7a48892e3e52264ff11956fc196
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-408.el8/348c7134.failed

With CONFIG_FORTIFY_SOURCE enabled, string functions will also perform
dynamic checks for string size which can panic the kernel, like incase
of overflow detection.

In papr_scm, papr_scm_pmu_check_events function uses stat->stat_id with
string operations, to populate the nvdimm_events_map array. Since
stat_id variable is not NULL terminated, the kernel panics with
CONFIG_FORTIFY_SOURCE enabled at boot time.

Below are the logs of kernel panic:

  detected buffer overflow in __fortify_strlen
  ------------[ cut here ]------------
  kernel BUG at lib/string_helpers.c:980!
  Oops: Exception in kernel mode, sig: 5 [#1]
  NIP [c00000000077dad0] fortify_panic+0x28/0x38
  LR [c00000000077dacc] fortify_panic+0x24/0x38
  Call Trace:
  [c0000022d77836e0] [c00000000077dacc] fortify_panic+0x24/0x38 (unreliable)
  [c00800000deb2660] papr_scm_pmu_check_events.constprop.0+0x118/0x220 [papr_scm]
  [c00800000deb2cb0] papr_scm_probe+0x288/0x62c [papr_scm]
  [c0000000009b46a8] platform_probe+0x98/0x150

Fix this issue by using kmemdup_nul() to copy the content of
stat->stat_id directly to the nvdimm_events_map array.

mpe: stat->stat_id comes from the hypervisor, not userspace, so there is
no security exposure.

Fixes: 4c08d4bbc089 ("powerpc/papr_scm: Add perf interface support")
	Signed-off-by: Kajol Jain <kjain@linux.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/20220505153451.35503-1-kjain@linux.ibm.com
(cherry picked from commit 348c71344111d7a48892e3e52264ff11956fc196)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/platforms/pseries/papr_scm.c
diff --cc arch/powerpc/platforms/pseries/papr_scm.c
index 8ac8b5513ae6,39962c905542..000000000000
--- a/arch/powerpc/platforms/pseries/papr_scm.c
+++ b/arch/powerpc/platforms/pseries/papr_scm.c
@@@ -301,6 -347,222 +301,225 @@@ static ssize_t drc_pmem_query_stats(str
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_PERF_EVENTS
+ #define to_nvdimm_pmu(_pmu)	container_of(_pmu, struct nvdimm_pmu, pmu)
+ 
+ static int papr_scm_pmu_get_value(struct perf_event *event, struct device *dev, u64 *count)
+ {
+ 	struct papr_scm_perf_stat *stat;
+ 	struct papr_scm_perf_stats *stats;
+ 	struct papr_scm_priv *p = (struct papr_scm_priv *)dev->driver_data;
+ 	int rc, size;
+ 
+ 	/* Allocate request buffer enough to hold single performance stat */
+ 	size = sizeof(struct papr_scm_perf_stats) +
+ 		sizeof(struct papr_scm_perf_stat);
+ 
+ 	if (!p || !p->nvdimm_events_map)
+ 		return -EINVAL;
+ 
+ 	stats = kzalloc(size, GFP_KERNEL);
+ 	if (!stats)
+ 		return -ENOMEM;
+ 
+ 	stat = &stats->scm_statistic[0];
+ 	memcpy(&stat->stat_id,
+ 	       p->nvdimm_events_map[event->attr.config],
+ 		sizeof(stat->stat_id));
+ 	stat->stat_val = 0;
+ 
+ 	rc = drc_pmem_query_stats(p, stats, 1);
+ 	if (rc < 0) {
+ 		kfree(stats);
+ 		return rc;
+ 	}
+ 
+ 	*count = be64_to_cpu(stat->stat_val);
+ 	kfree(stats);
+ 	return 0;
+ }
+ 
+ static int papr_scm_pmu_event_init(struct perf_event *event)
+ {
+ 	struct nvdimm_pmu *nd_pmu = to_nvdimm_pmu(event->pmu);
+ 	struct papr_scm_priv *p;
+ 
+ 	if (!nd_pmu)
+ 		return -EINVAL;
+ 
+ 	/* test the event attr type for PMU enumeration */
+ 	if (event->attr.type != event->pmu->type)
+ 		return -ENOENT;
+ 
+ 	/* it does not support event sampling mode */
+ 	if (is_sampling_event(event))
+ 		return -EOPNOTSUPP;
+ 
+ 	/* no branch sampling */
+ 	if (has_branch_stack(event))
+ 		return -EOPNOTSUPP;
+ 
+ 	p = (struct papr_scm_priv *)nd_pmu->dev->driver_data;
+ 	if (!p)
+ 		return -EINVAL;
+ 
+ 	/* Invalid eventcode */
+ 	if (event->attr.config == 0 || event->attr.config > 16)
+ 		return -EINVAL;
+ 
+ 	return 0;
+ }
+ 
+ static int papr_scm_pmu_add(struct perf_event *event, int flags)
+ {
+ 	u64 count;
+ 	int rc;
+ 	struct nvdimm_pmu *nd_pmu = to_nvdimm_pmu(event->pmu);
+ 
+ 	if (!nd_pmu)
+ 		return -EINVAL;
+ 
+ 	if (flags & PERF_EF_START) {
+ 		rc = papr_scm_pmu_get_value(event, nd_pmu->dev, &count);
+ 		if (rc)
+ 			return rc;
+ 
+ 		local64_set(&event->hw.prev_count, count);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void papr_scm_pmu_read(struct perf_event *event)
+ {
+ 	u64 prev, now;
+ 	int rc;
+ 	struct nvdimm_pmu *nd_pmu = to_nvdimm_pmu(event->pmu);
+ 
+ 	if (!nd_pmu)
+ 		return;
+ 
+ 	rc = papr_scm_pmu_get_value(event, nd_pmu->dev, &now);
+ 	if (rc)
+ 		return;
+ 
+ 	prev = local64_xchg(&event->hw.prev_count, now);
+ 	local64_add(now - prev, &event->count);
+ }
+ 
+ static void papr_scm_pmu_del(struct perf_event *event, int flags)
+ {
+ 	papr_scm_pmu_read(event);
+ }
+ 
+ static int papr_scm_pmu_check_events(struct papr_scm_priv *p, struct nvdimm_pmu *nd_pmu)
+ {
+ 	struct papr_scm_perf_stat *stat;
+ 	struct papr_scm_perf_stats *stats;
+ 	int index, rc, count;
+ 	u32 available_events;
+ 
+ 	if (!p->stat_buffer_len)
+ 		return -ENOENT;
+ 
+ 	available_events = (p->stat_buffer_len  - sizeof(struct papr_scm_perf_stats))
+ 			/ sizeof(struct papr_scm_perf_stat);
+ 
+ 	/* Allocate the buffer for phyp where stats are written */
+ 	stats = kzalloc(p->stat_buffer_len, GFP_KERNEL);
+ 	if (!stats) {
+ 		rc = -ENOMEM;
+ 		return rc;
+ 	}
+ 
+ 	/* Allocate memory to nvdimm_event_map */
+ 	p->nvdimm_events_map = kcalloc(available_events, sizeof(char *), GFP_KERNEL);
+ 	if (!p->nvdimm_events_map) {
+ 		rc = -ENOMEM;
+ 		goto out_stats;
+ 	}
+ 
+ 	/* Called to get list of events supported */
+ 	rc = drc_pmem_query_stats(p, stats, 0);
+ 	if (rc)
+ 		goto out_nvdimm_events_map;
+ 
+ 	for (index = 0, stat = stats->scm_statistic, count = 0;
+ 		     index < available_events; index++, ++stat) {
+ 		p->nvdimm_events_map[count] = kmemdup_nul(stat->stat_id, 8, GFP_KERNEL);
+ 		if (!p->nvdimm_events_map[count]) {
+ 			rc = -ENOMEM;
+ 			goto out_nvdimm_events_map;
+ 		}
+ 
+ 		count++;
+ 	}
+ 	p->nvdimm_events_map[count] = NULL;
+ 	kfree(stats);
+ 	return 0;
+ 
+ out_nvdimm_events_map:
+ 	kfree(p->nvdimm_events_map);
+ out_stats:
+ 	kfree(stats);
+ 	return rc;
+ }
+ 
+ static void papr_scm_pmu_register(struct papr_scm_priv *p)
+ {
+ 	struct nvdimm_pmu *nd_pmu;
+ 	int rc, nodeid;
+ 
+ 	nd_pmu = kzalloc(sizeof(*nd_pmu), GFP_KERNEL);
+ 	if (!nd_pmu) {
+ 		rc = -ENOMEM;
+ 		goto pmu_err_print;
+ 	}
+ 
+ 	rc = papr_scm_pmu_check_events(p, nd_pmu);
+ 	if (rc)
+ 		goto pmu_check_events_err;
+ 
+ 	nd_pmu->pmu.task_ctx_nr = perf_invalid_context;
+ 	nd_pmu->pmu.name = nvdimm_name(p->nvdimm);
+ 	nd_pmu->pmu.event_init = papr_scm_pmu_event_init;
+ 	nd_pmu->pmu.read = papr_scm_pmu_read;
+ 	nd_pmu->pmu.add = papr_scm_pmu_add;
+ 	nd_pmu->pmu.del = papr_scm_pmu_del;
+ 
+ 	nd_pmu->pmu.capabilities = PERF_PMU_CAP_NO_INTERRUPT |
+ 				PERF_PMU_CAP_NO_EXCLUDE;
+ 
+ 	/*updating the cpumask variable */
+ 	nodeid = numa_map_to_online_node(dev_to_node(&p->pdev->dev));
+ 	nd_pmu->arch_cpumask = *cpumask_of_node(nodeid);
+ 
+ 	rc = register_nvdimm_pmu(nd_pmu, p->pdev);
+ 	if (rc)
+ 		goto pmu_register_err;
+ 
+ 	/*
+ 	 * Set archdata.priv value to nvdimm_pmu structure, to handle the
+ 	 * unregistering of pmu device.
+ 	 */
+ 	p->pdev->archdata.priv = nd_pmu;
+ 	return;
+ 
+ pmu_register_err:
+ 	kfree(p->nvdimm_events_map);
+ pmu_check_events_err:
+ 	kfree(nd_pmu);
+ pmu_err_print:
+ 	dev_info(&p->pdev->dev, "nvdimm pmu didn't register rc=%d\n", rc);
+ }
+ 
+ #else
+ static void papr_scm_pmu_register(struct papr_scm_priv *p) { }
+ #endif
+ 
++>>>>>>> 348c71344111 (powerpc/papr_scm: Fix buffer overflow issue with CONFIG_FORTIFY_SOURCE)
  /*
   * Issue hcall to retrieve dimm health info and populate papr_scm_priv with the
   * health information.
* Unmerged path arch/powerpc/platforms/pseries/papr_scm.c
