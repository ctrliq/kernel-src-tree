x86/sev: Guard sev_evict_cache() with CONFIG_AMD_MEM_ENCRYPT

jira KERNEL-660
Rebuild_History Non-Buildable kernel-5.14.0-611.34.1.el9_7
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit 7f830e126dc357fc086905ce9730140fd4528d66
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-611.34.1.el9_7/7f830e12.failed

The sev_evict_cache() is guest-related code and should be guarded by
CONFIG_AMD_MEM_ENCRYPT, not CONFIG_KVM_AMD_SEV.

CONFIG_AMD_MEM_ENCRYPT=y is required for a guest to run properly as an SEV-SNP
guest, but a guest kernel built with CONFIG_KVM_AMD_SEV=n would get the stub
function of sev_evict_cache() instead of the version that performs the actual
eviction. Move the function declarations under the appropriate #ifdef.

Fixes: 7b306dfa326f ("x86/sev: Evict cache lines during SNP memory validation")
	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
	Cc: stable@kernel.org # 6.16.x
Link: https://lore.kernel.org/r/70e38f2c4a549063de54052c9f64929705313526.1757708959.git.thomas.lendacky@amd.com
(cherry picked from commit 7f830e126dc357fc086905ce9730140fd4528d66)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/sev.h
diff --cc arch/x86/include/asm/sev.h
index a171cb51048f,465b19fd1a2d..000000000000
--- a/arch/x86/include/asm/sev.h
+++ b/arch/x86/include/asm/sev.h
@@@ -422,8 -523,63 +422,60 @@@ int prepare_pte_enc(struct pte_enc_des
  void set_pte_enc_mask(pte_t *kpte, unsigned long pfn, pgprot_t new_prot);
  void snp_kexec_finish(void);
  void snp_kexec_begin(void);
 -
 -int snp_msg_init(struct snp_msg_desc *mdesc, int vmpck_id);
 -struct snp_msg_desc *snp_msg_alloc(void);
 -void snp_msg_free(struct snp_msg_desc *mdesc);
 -int snp_send_guest_request(struct snp_msg_desc *mdesc, struct snp_guest_req *req);
 -
  int snp_svsm_vtpm_send_command(u8 *buffer);
  
++<<<<<<< HEAD
++=======
+ void __init snp_secure_tsc_prepare(void);
+ void __init snp_secure_tsc_init(void);
+ 
+ static __always_inline void vc_ghcb_invalidate(struct ghcb *ghcb)
+ {
+ 	ghcb->save.sw_exit_code = 0;
+ 	__builtin_memset(ghcb->save.valid_bitmap, 0, sizeof(ghcb->save.valid_bitmap));
+ }
+ 
+ void vc_forward_exception(struct es_em_ctxt *ctxt);
+ 
+ /* I/O parameters for CPUID-related helpers */
+ struct cpuid_leaf {
+ 	u32 fn;
+ 	u32 subfn;
+ 	u32 eax;
+ 	u32 ebx;
+ 	u32 ecx;
+ 	u32 edx;
+ };
+ 
+ int snp_cpuid(struct ghcb *ghcb, struct es_em_ctxt *ctxt, struct cpuid_leaf *leaf);
+ 
+ void __noreturn sev_es_terminate(unsigned int set, unsigned int reason);
+ enum es_result sev_es_ghcb_hv_call(struct ghcb *ghcb,
+ 				   struct es_em_ctxt *ctxt,
+ 				   u64 exit_code, u64 exit_info_1,
+ 				   u64 exit_info_2);
+ 
+ extern struct ghcb *boot_ghcb;
+ 
+ static inline void sev_evict_cache(void *va, int npages)
+ {
+ 	volatile u8 val __always_unused;
+ 	u8 *bytes = va;
+ 	int page_idx;
+ 
+ 	/*
+ 	 * For SEV guests, a read from the first/last cache-lines of a 4K page
+ 	 * using the guest key is sufficient to cause a flush of all cache-lines
+ 	 * associated with that 4K page without incurring all the overhead of a
+ 	 * full CLFLUSH sequence.
+ 	 */
+ 	for (page_idx = 0; page_idx < npages; page_idx++) {
+ 		val = bytes[page_idx * PAGE_SIZE];
+ 		val = bytes[page_idx * PAGE_SIZE + PAGE_SIZE - 1];
+ 	}
+ }
+ 
++>>>>>>> 7f830e126dc3 (x86/sev: Guard sev_evict_cache() with CONFIG_AMD_MEM_ENCRYPT)
  #else	/* !CONFIG_AMD_MEM_ENCRYPT */
  
  #define snp_vmpl 0
@@@ -463,7 -615,15 +515,13 @@@ static inline int prepare_pte_enc(struc
  static inline void set_pte_enc_mask(pte_t *kpte, unsigned long pfn, pgprot_t new_prot) { }
  static inline void snp_kexec_finish(void) { }
  static inline void snp_kexec_begin(void) { }
 -static inline int snp_msg_init(struct snp_msg_desc *mdesc, int vmpck_id) { return -1; }
 -static inline struct snp_msg_desc *snp_msg_alloc(void) { return NULL; }
 -static inline void snp_msg_free(struct snp_msg_desc *mdesc) { }
 -static inline int snp_send_guest_request(struct snp_msg_desc *mdesc,
 -					 struct snp_guest_req *req) { return -ENODEV; }
  static inline int snp_svsm_vtpm_send_command(u8 *buffer) { return -ENODEV; }
++<<<<<<< HEAD
++=======
+ static inline void __init snp_secure_tsc_prepare(void) { }
+ static inline void __init snp_secure_tsc_init(void) { }
+ static inline void sev_evict_cache(void *va, int npages) {}
++>>>>>>> 7f830e126dc3 (x86/sev: Guard sev_evict_cache() with CONFIG_AMD_MEM_ENCRYPT)
  
  #endif	/* CONFIG_AMD_MEM_ENCRYPT */
  
* Unmerged path arch/x86/include/asm/sev.h
