perf/x86/amd/uncore: Move discovery and registration

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-531.el8
commit-author Sandipan Das <sandipan.das@amd.com>
commit 07888daa056e809de0b6b234116b575c11f9f99d
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-531.el8/07888daa.failed

Uncore PMUs have traditionally been registered in the module init path.
This is fine for the existing DF and L3 PMUs since the CPUID information
does not vary across CPUs but not for the memory controller (UMC) PMUs
since information like active memory channels can vary for each socket
depending on how the DIMMs have been physically populated.

To overcome this, the discovery of PMU information using CPUID is moved
to the startup of UNCORE_STARTING. This cannot be done in the startup of
UNCORE_PREP since the hotplug callback does not run on the CPU that is
being brought online.

Previously, the startup of UNCORE_PREP was used for allocating uncore
contexts following which, the startup of UNCORE_STARTING was used to
find and reuse an existing sibling context, if possible. Any unused
contexts were added to a list for reclaimation later during the startup
of UNCORE_ONLINE.

Since all required CPUID info is now available only after the startup of
UNCORE_STARTING has completed, context allocation has been moved to the
startup of UNCORE_ONLINE. Before allocating contexts, the first CPU that
comes online has to take up the additional responsibility of registering
the PMUs. This is a one-time process though. Since sibling discovery now
happens prior to deciding whether a new context is required, there is no
longer a need to track and free up unused contexts.

The teardown of UNCORE_ONLINE and UNCORE_PREP functionally remain the
same.

Overall, the flow of control described above is achieved using the
following handlers for managing uncore PMUs. It is mandatory to define
them for each type of uncore PMU.

  * scan() runs during startup of UNCORE_STARTING and collects PMU info
    using CPUID.

  * init() runs during startup of UNCORE_ONLINE, registers PMUs and sets
    up uncore contexts.

  * move() runs during teardown of UNCORE_ONLINE and migrates uncore
    contexts to a shared sibling, if possible.

  * free() runs during teardown of UNCORE_PREP and frees up uncore
    contexts.

	Signed-off-by: Sandipan Das <sandipan.das@amd.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lore.kernel.org/r/e6c447e48872fcab8452e0dd81b1c9cb09f39eb4.1696425185.git.sandipan.das@amd.com
(cherry picked from commit 07888daa056e809de0b6b234116b575c11f9f99d)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/amd/uncore.c
diff --cc arch/x86/events/amd/uncore.c
index 63fc97aecaf4,ff1d09cc07ad..000000000000
--- a/arch/x86/events/amd/uncore.c
+++ b/arch/x86/events/amd/uncore.c
@@@ -29,57 -26,62 +29,106 @@@
  #define RDPMC_BASE_LLC		10
  
  #define COUNTER_SHIFT		16
++<<<<<<< HEAD
++=======
+ #define UNCORE_NAME_LEN		16
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  
  #undef pr_fmt
  #define pr_fmt(fmt)	"amd_uncore: " fmt
  
  static int pmu_version;
 +static int num_counters_llc;
 +static int num_counters_nb;
 +static bool l3_mask;
 +
++<<<<<<< HEAD
 +static HLIST_HEAD(uncore_unused_list);
  
 +struct amd_uncore {
 +	int id;
++=======
+ struct amd_uncore_ctx {
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  	int refcnt;
  	int cpu;
 +	int num_counters;
 +	int rdpmc_base;
 +	u32 msr_base;
 +	cpumask_t *active_mask;
 +	struct pmu *pmu;
  	struct perf_event **events;
  	struct hlist_node node;
  };
  
++<<<<<<< HEAD
 +static struct amd_uncore * __percpu *amd_uncore_nb;
 +static struct amd_uncore * __percpu *amd_uncore_llc;
 +
 +static struct pmu amd_nb_pmu;
 +static struct pmu amd_llc_pmu;
++=======
+ struct amd_uncore_pmu {
+ 	char name[UNCORE_NAME_LEN];
+ 	int num_counters;
+ 	int rdpmc_base;
+ 	u32 msr_base;
+ 	cpumask_t active_mask;
+ 	struct pmu pmu;
+ 	struct amd_uncore_ctx * __percpu *ctx;
+ };
+ 
+ enum {
+ 	UNCORE_TYPE_DF,
+ 	UNCORE_TYPE_L3,
+ 
+ 	UNCORE_TYPE_MAX
+ };
+ 
+ union amd_uncore_info {
+ 	struct {
+ 		u64	aux_data:32;	/* auxiliary data */
+ 		u64	num_pmcs:8;	/* number of counters */
+ 		u64	cid:8;		/* context id */
+ 	} split;
+ 	u64		full;
+ };
+ 
+ struct amd_uncore {
+ 	union amd_uncore_info * __percpu info;
+ 	struct amd_uncore_pmu *pmus;
+ 	unsigned int num_pmus;
+ 	bool init_done;
+ 	void (*scan)(struct amd_uncore *uncore, unsigned int cpu);
+ 	int  (*init)(struct amd_uncore *uncore, unsigned int cpu);
+ 	void (*move)(struct amd_uncore *uncore, unsigned int cpu);
+ 	void (*free)(struct amd_uncore *uncore, unsigned int cpu);
+ };
+ 
+ static struct amd_uncore uncores[UNCORE_TYPE_MAX];
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
 +
 +static cpumask_t amd_nb_active_mask;
 +static cpumask_t amd_llc_active_mask;
 +
 +static bool is_nb_event(struct perf_event *event)
 +{
 +	return event->pmu->type == amd_nb_pmu.type;
 +}
  
 -static struct amd_uncore_pmu *event_to_amd_uncore_pmu(struct perf_event *event)
 +static bool is_llc_event(struct perf_event *event)
  {
 -	return container_of(event->pmu, struct amd_uncore_pmu, pmu);
 +	return event->pmu->type == amd_llc_pmu.type;
 +}
 +
 +static struct amd_uncore *event_to_amd_uncore(struct perf_event *event)
 +{
 +	if (is_nb_event(event) && amd_uncore_nb)
 +		return *per_cpu_ptr(amd_uncore_nb, event->cpu);
 +	else if (is_llc_event(event) && amd_uncore_llc)
 +		return *per_cpu_ptr(amd_uncore_llc, event->cpu);
 +
 +	return NULL;
  }
  
  static void amd_uncore_read(struct perf_event *event)
@@@ -399,261 -352,482 +448,734 @@@ static const struct attribute_group *am
  	NULL,
  };
  
++<<<<<<< HEAD
 +static struct pmu amd_nb_pmu = {
 +	.task_ctx_nr	= perf_invalid_context,
 +	.attr_groups	= amd_uncore_df_attr_groups,
 +	.name		= "amd_nb",
 +	.event_init	= amd_uncore_event_init,
 +	.add		= amd_uncore_add,
 +	.del		= amd_uncore_del,
 +	.start		= amd_uncore_start,
 +	.stop		= amd_uncore_stop,
 +	.read		= amd_uncore_read,
 +	.capabilities	= PERF_PMU_CAP_NO_EXCLUDE | PERF_PMU_CAP_NO_INTERRUPT,
 +	.module		= THIS_MODULE,
 +};
 +
 +static struct pmu amd_llc_pmu = {
 +	.task_ctx_nr	= perf_invalid_context,
 +	.attr_groups	= amd_uncore_l3_attr_groups,
 +	.attr_update	= amd_uncore_l3_attr_update,
 +	.name		= "amd_l2",
 +	.event_init	= amd_uncore_event_init,
 +	.add		= amd_uncore_add,
 +	.del		= amd_uncore_del,
 +	.start		= amd_uncore_start,
 +	.stop		= amd_uncore_stop,
 +	.read		= amd_uncore_read,
 +	.capabilities	= PERF_PMU_CAP_NO_EXCLUDE | PERF_PMU_CAP_NO_INTERRUPT,
 +	.module		= THIS_MODULE,
 +};
 +
 +static struct amd_uncore *amd_uncore_alloc(unsigned int cpu)
 +{
 +	return kzalloc_node(sizeof(struct amd_uncore), GFP_KERNEL,
 +			cpu_to_node(cpu));
 +}
 +
 +static inline struct perf_event **
 +amd_uncore_events_alloc(unsigned int num, unsigned int cpu)
 +{
 +	return kzalloc_node(sizeof(struct perf_event *) * num, GFP_KERNEL,
 +			    cpu_to_node(cpu));
 +}
 +
 +static int amd_uncore_cpu_up_prepare(unsigned int cpu)
 +{
 +	struct amd_uncore *uncore_nb = NULL, *uncore_llc = NULL;
 +
 +	if (amd_uncore_nb) {
 +		*per_cpu_ptr(amd_uncore_nb, cpu) = NULL;
 +		uncore_nb = amd_uncore_alloc(cpu);
 +		if (!uncore_nb)
 +			goto fail;
 +		uncore_nb->cpu = cpu;
 +		uncore_nb->num_counters = num_counters_nb;
 +		uncore_nb->rdpmc_base = RDPMC_BASE_NB;
 +		uncore_nb->msr_base = MSR_F15H_NB_PERF_CTL;
 +		uncore_nb->active_mask = &amd_nb_active_mask;
 +		uncore_nb->pmu = &amd_nb_pmu;
 +		uncore_nb->events = amd_uncore_events_alloc(num_counters_nb, cpu);
 +		if (!uncore_nb->events)
 +			goto fail;
 +		uncore_nb->id = -1;
 +		*per_cpu_ptr(amd_uncore_nb, cpu) = uncore_nb;
 +	}
 +
 +	if (amd_uncore_llc) {
 +		*per_cpu_ptr(amd_uncore_llc, cpu) = NULL;
 +		uncore_llc = amd_uncore_alloc(cpu);
 +		if (!uncore_llc)
 +			goto fail;
 +		uncore_llc->cpu = cpu;
 +		uncore_llc->num_counters = num_counters_llc;
 +		uncore_llc->rdpmc_base = RDPMC_BASE_LLC;
 +		uncore_llc->msr_base = MSR_F16H_L2I_PERF_CTL;
 +		uncore_llc->active_mask = &amd_llc_active_mask;
 +		uncore_llc->pmu = &amd_llc_pmu;
 +		uncore_llc->events = amd_uncore_events_alloc(num_counters_llc, cpu);
 +		if (!uncore_llc->events)
 +			goto fail;
 +		uncore_llc->id = -1;
 +		*per_cpu_ptr(amd_uncore_llc, cpu) = uncore_llc;
 +	}
 +
 +	return 0;
 +
 +fail:
 +	if (uncore_nb) {
 +		kfree(uncore_nb->events);
 +		kfree(uncore_nb);
 +	}
 +
 +	if (uncore_llc) {
 +		kfree(uncore_llc->events);
 +		kfree(uncore_llc);
 +	}
++=======
+ static __always_inline
+ int amd_uncore_ctx_cid(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	union amd_uncore_info *info = per_cpu_ptr(uncore->info, cpu);
+ 	return info->split.cid;
+ }
+ 
+ static __always_inline
+ int amd_uncore_ctx_num_pmcs(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	union amd_uncore_info *info = per_cpu_ptr(uncore->info, cpu);
+ 	return info->split.num_pmcs;
+ }
+ 
+ static void amd_uncore_ctx_free(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	struct amd_uncore_pmu *pmu;
+ 	struct amd_uncore_ctx *ctx;
+ 	int i;
+ 
+ 	if (!uncore->init_done)
+ 		return;
+ 
+ 	for (i = 0; i < uncore->num_pmus; i++) {
+ 		pmu = &uncore->pmus[i];
+ 		ctx = *per_cpu_ptr(pmu->ctx, cpu);
+ 		if (!ctx)
+ 			continue;
+ 
+ 		if (cpu == ctx->cpu)
+ 			cpumask_clear_cpu(cpu, &pmu->active_mask);
+ 
+ 		if (!--ctx->refcnt) {
+ 			kfree(ctx->events);
+ 			kfree(ctx);
+ 		}
+ 
+ 		*per_cpu_ptr(pmu->ctx, cpu) = NULL;
+ 	}
+ }
+ 
+ static int amd_uncore_ctx_init(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	struct amd_uncore_ctx *curr, *prev;
+ 	struct amd_uncore_pmu *pmu;
+ 	int node, cid, i, j;
+ 
+ 	if (!uncore->init_done || !uncore->num_pmus)
+ 		return 0;
+ 
+ 	cid = amd_uncore_ctx_cid(uncore, cpu);
+ 
+ 	for (i = 0; i < uncore->num_pmus; i++) {
+ 		pmu = &uncore->pmus[i];
+ 		*per_cpu_ptr(pmu->ctx, cpu) = NULL;
+ 		curr = NULL;
+ 
+ 		/* Find a sibling context */
+ 		for_each_online_cpu(j) {
+ 			if (cpu == j)
+ 				continue;
+ 
+ 			prev = *per_cpu_ptr(pmu->ctx, j);
+ 			if (!prev)
+ 				continue;
+ 
+ 			if (cid == amd_uncore_ctx_cid(uncore, j)) {
+ 				curr = prev;
+ 				break;
+ 			}
+ 		}
+ 
+ 		/* Allocate context if sibling does not exist */
+ 		if (!curr) {
+ 			node = cpu_to_node(cpu);
+ 			curr = kzalloc_node(sizeof(*curr), GFP_KERNEL, node);
+ 			if (!curr)
+ 				goto fail;
+ 
+ 			curr->cpu = cpu;
+ 			curr->events = kzalloc_node(sizeof(*curr->events) *
+ 						    pmu->num_counters,
+ 						    GFP_KERNEL, node);
+ 			if (!curr->events) {
+ 				kfree(curr);
+ 				goto fail;
+ 			}
+ 
+ 			cpumask_set_cpu(cpu, &pmu->active_mask);
+ 		}
+ 
+ 		curr->refcnt++;
+ 		*per_cpu_ptr(pmu->ctx, cpu) = curr;
+ 	}
+ 
+ 	return 0;
+ 
+ fail:
+ 	amd_uncore_ctx_free(uncore, cpu);
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  
  	return -ENOMEM;
  }
  
++<<<<<<< HEAD
 +static struct amd_uncore *
 +amd_uncore_find_online_sibling(struct amd_uncore *this,
 +			       struct amd_uncore * __percpu *uncores)
 +{
 +	unsigned int cpu;
 +	struct amd_uncore *that;
 +
 +	for_each_online_cpu(cpu) {
 +		that = *per_cpu_ptr(uncores, cpu);
 +
 +		if (!that)
 +			continue;
 +
 +		if (this == that)
 +			continue;
 +
 +		if (this->id == that->id) {
 +			hlist_add_head(&this->node, &uncore_unused_list);
 +			this = that;
 +			break;
 +		}
 +	}
 +
 +	this->refcnt++;
 +	return this;
++=======
+ static void amd_uncore_ctx_move(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	struct amd_uncore_ctx *curr, *next;
+ 	struct amd_uncore_pmu *pmu;
+ 	int i, j;
+ 
+ 	if (!uncore->init_done)
+ 		return;
+ 
+ 	for (i = 0; i < uncore->num_pmus; i++) {
+ 		pmu = &uncore->pmus[i];
+ 		curr = *per_cpu_ptr(pmu->ctx, cpu);
+ 		if (!curr)
+ 			continue;
+ 
+ 		/* Migrate to a shared sibling if possible */
+ 		for_each_online_cpu(j) {
+ 			next = *per_cpu_ptr(pmu->ctx, j);
+ 			if (!next || cpu == j)
+ 				continue;
+ 
+ 			if (curr == next) {
+ 				perf_pmu_migrate_context(&pmu->pmu, cpu, j);
+ 				cpumask_clear_cpu(cpu, &pmu->active_mask);
+ 				cpumask_set_cpu(j, &pmu->active_mask);
+ 				next->cpu = j;
+ 				break;
+ 			}
+ 		}
+ 	}
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  }
  
  static int amd_uncore_cpu_starting(unsigned int cpu)
  {
++<<<<<<< HEAD
 +	unsigned int eax, ebx, ecx, edx;
 +	struct amd_uncore *uncore;
 +
 +	if (amd_uncore_nb) {
 +		uncore = *per_cpu_ptr(amd_uncore_nb, cpu);
 +		cpuid(0x8000001e, &eax, &ebx, &ecx, &edx);
 +		uncore->id = ecx & 0xff;
 +
 +		uncore = amd_uncore_find_online_sibling(uncore, amd_uncore_nb);
 +		*per_cpu_ptr(amd_uncore_nb, cpu) = uncore;
 +	}
 +
 +	if (amd_uncore_llc) {
 +		uncore = *per_cpu_ptr(amd_uncore_llc, cpu);
 +		uncore->id = get_llc_id(cpu);
 +
 +		uncore = amd_uncore_find_online_sibling(uncore, amd_uncore_llc);
 +		*per_cpu_ptr(amd_uncore_llc, cpu) = uncore;
 +	}
 +
 +	return 0;
 +}
 +
 +static void uncore_clean_online(void)
 +{
 +	struct amd_uncore *uncore;
 +	struct hlist_node *n;
 +
 +	hlist_for_each_entry_safe(uncore, n, &uncore_unused_list, node) {
 +		hlist_del(&uncore->node);
 +		kfree(uncore->events);
 +		kfree(uncore);
 +	}
 +}
 +
 +static void uncore_online(unsigned int cpu,
 +			  struct amd_uncore * __percpu *uncores)
 +{
 +	struct amd_uncore *uncore = *per_cpu_ptr(uncores, cpu);
 +
 +	uncore_clean_online();
 +
 +	if (cpu == uncore->cpu)
 +		cpumask_set_cpu(cpu, uncore->active_mask);
 +}
 +
 +static int amd_uncore_cpu_online(unsigned int cpu)
 +{
 +	if (amd_uncore_nb)
 +		uncore_online(cpu, amd_uncore_nb);
 +
 +	if (amd_uncore_llc)
 +		uncore_online(cpu, amd_uncore_llc);
 +
 +	return 0;
 +}
 +
 +static void uncore_down_prepare(unsigned int cpu,
 +				struct amd_uncore * __percpu *uncores)
 +{
 +	unsigned int i;
 +	struct amd_uncore *this = *per_cpu_ptr(uncores, cpu);
 +
 +	if (this->cpu != cpu)
 +		return;
 +
 +	/* this cpu is going down, migrate to a shared sibling if possible */
 +	for_each_online_cpu(i) {
 +		struct amd_uncore *that = *per_cpu_ptr(uncores, i);
 +
 +		if (cpu == i)
 +			continue;
 +
 +		if (this == that) {
 +			perf_pmu_migrate_context(this->pmu, cpu, i);
 +			cpumask_clear_cpu(cpu, that->active_mask);
 +			cpumask_set_cpu(i, that->active_mask);
 +			that->cpu = i;
 +			break;
 +		}
 +	}
 +}
 +
 +static int amd_uncore_cpu_down_prepare(unsigned int cpu)
 +{
 +	if (amd_uncore_nb)
 +		uncore_down_prepare(cpu, amd_uncore_nb);
 +
 +	if (amd_uncore_llc)
 +		uncore_down_prepare(cpu, amd_uncore_llc);
 +
 +	return 0;
 +}
 +
 +static void uncore_dead(unsigned int cpu, struct amd_uncore * __percpu *uncores)
 +{
 +	struct amd_uncore *uncore = *per_cpu_ptr(uncores, cpu);
 +
 +	if (cpu == uncore->cpu)
 +		cpumask_clear_cpu(cpu, uncore->active_mask);
 +
 +	if (!--uncore->refcnt) {
 +		kfree(uncore->events);
 +		kfree(uncore);
 +	}
 +
 +	*per_cpu_ptr(uncores, cpu) = NULL;
 +}
 +
 +static int amd_uncore_cpu_dead(unsigned int cpu)
 +{
 +	if (amd_uncore_nb)
 +		uncore_dead(cpu, amd_uncore_nb);
 +
 +	if (amd_uncore_llc)
 +		uncore_dead(cpu, amd_uncore_llc);
++=======
+ 	struct amd_uncore *uncore;
+ 	int i;
+ 
+ 	for (i = 0; i < UNCORE_TYPE_MAX; i++) {
+ 		uncore = &uncores[i];
+ 		uncore->scan(uncore, cpu);
+ 	}
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  
  	return 0;
  }
  
++<<<<<<< HEAD
 +static int __init amd_uncore_init(void)
 +{
 +	struct attribute **df_attr = amd_uncore_df_format_attr;
 +	struct attribute **l3_attr = amd_uncore_l3_format_attr;
 +	union cpuid_0x80000022_ebx ebx;
 +	int ret = -ENODEV;
++=======
+ static int amd_uncore_cpu_online(unsigned int cpu)
+ {
+ 	struct amd_uncore *uncore;
+ 	int i;
+ 
+ 	for (i = 0; i < UNCORE_TYPE_MAX; i++) {
+ 		uncore = &uncores[i];
+ 		if (uncore->init(uncore, cpu))
+ 			break;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int amd_uncore_cpu_down_prepare(unsigned int cpu)
+ {
+ 	struct amd_uncore *uncore;
+ 	int i;
+ 
+ 	for (i = 0; i < UNCORE_TYPE_MAX; i++) {
+ 		uncore = &uncores[i];
+ 		uncore->move(uncore, cpu);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int amd_uncore_cpu_dead(unsigned int cpu)
+ {
+ 	struct amd_uncore *uncore;
+ 	int i;
+ 
+ 	for (i = 0; i < UNCORE_TYPE_MAX; i++) {
+ 		uncore = &uncores[i];
+ 		uncore->free(uncore, cpu);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int amd_uncore_df_event_init(struct perf_event *event)
+ {
+ 	struct hw_perf_event *hwc = &event->hw;
+ 	int ret = amd_uncore_event_init(event);
+ 
+ 	if (ret || pmu_version < 2)
+ 		return ret;
+ 
+ 	hwc->config = event->attr.config &
+ 		      (pmu_version >= 2 ? AMD64_PERFMON_V2_RAW_EVENT_MASK_NB :
+ 					  AMD64_RAW_EVENT_MASK_NB);
+ 
+ 	return 0;
+ }
+ 
+ static int amd_uncore_df_add(struct perf_event *event, int flags)
+ {
+ 	int ret = amd_uncore_add(event, flags & ~PERF_EF_START);
+ 	struct hw_perf_event *hwc = &event->hw;
+ 
+ 	if (ret)
+ 		return ret;
+ 
+ 	/*
+ 	 * The first four DF counters are accessible via RDPMC index 6 to 9
+ 	 * followed by the L3 counters from index 10 to 15. For processors
+ 	 * with more than four DF counters, the DF RDPMC assignments become
+ 	 * discontiguous as the additional counters are accessible starting
+ 	 * from index 16.
+ 	 */
+ 	if (hwc->idx >= NUM_COUNTERS_NB)
+ 		hwc->event_base_rdpmc += NUM_COUNTERS_L3;
+ 
+ 	/* Delayed start after rdpmc base update */
+ 	if (flags & PERF_EF_START)
+ 		amd_uncore_start(event, PERF_EF_RELOAD);
+ 
+ 	return 0;
+ }
+ 
+ static
+ void amd_uncore_df_ctx_scan(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	union cpuid_0x80000022_ebx ebx;
+ 	union amd_uncore_info info;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_PERFCTR_NB))
+ 		return;
+ 
+ 	info.split.aux_data = 0;
+ 	info.split.num_pmcs = NUM_COUNTERS_NB;
+ 	info.split.cid = topology_die_id(cpu);
+ 
+ 	if (pmu_version >= 2) {
+ 		ebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);
+ 		info.split.num_pmcs = ebx.split.num_df_pmc;
+ 	}
+ 
+ 	*per_cpu_ptr(uncore->info, cpu) = info;
+ }
+ 
+ static
+ int amd_uncore_df_ctx_init(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	struct attribute **df_attr = amd_uncore_df_format_attr;
+ 	struct amd_uncore_pmu *pmu;
+ 
+ 	/* Run just once */
+ 	if (uncore->init_done)
+ 		return amd_uncore_ctx_init(uncore, cpu);
+ 
+ 	/* No grouping, single instance for a system */
+ 	uncore->pmus = kzalloc(sizeof(*uncore->pmus), GFP_KERNEL);
+ 	if (!uncore->pmus) {
+ 		uncore->num_pmus = 0;
+ 		goto done;
+ 	}
+ 
+ 	/*
+ 	 * For Family 17h and above, the Northbridge counters are repurposed
+ 	 * as Data Fabric counters. The PMUs are exported based on family as
+ 	 * either NB or DF.
+ 	 */
+ 	pmu = &uncore->pmus[0];
+ 	strscpy(pmu->name, boot_cpu_data.x86 >= 0x17 ? "amd_df" : "amd_nb",
+ 		sizeof(pmu->name));
+ 	pmu->num_counters = amd_uncore_ctx_num_pmcs(uncore, cpu);
+ 	pmu->msr_base = MSR_F15H_NB_PERF_CTL;
+ 	pmu->rdpmc_base = RDPMC_BASE_NB;
+ 
+ 	if (pmu_version >= 2) {
+ 		*df_attr++ = &format_attr_event14v2.attr;
+ 		*df_attr++ = &format_attr_umask12.attr;
+ 	} else if (boot_cpu_data.x86 >= 0x17) {
+ 		*df_attr = &format_attr_event14.attr;
+ 	}
+ 
+ 	pmu->ctx = alloc_percpu(struct amd_uncore_ctx *);
+ 	if (!pmu->ctx)
+ 		goto done;
+ 
+ 	pmu->pmu = (struct pmu) {
+ 		.task_ctx_nr	= perf_invalid_context,
+ 		.attr_groups	= amd_uncore_df_attr_groups,
+ 		.name		= pmu->name,
+ 		.event_init	= amd_uncore_df_event_init,
+ 		.add		= amd_uncore_df_add,
+ 		.del		= amd_uncore_del,
+ 		.start		= amd_uncore_start,
+ 		.stop		= amd_uncore_stop,
+ 		.read		= amd_uncore_read,
+ 		.capabilities	= PERF_PMU_CAP_NO_EXCLUDE | PERF_PMU_CAP_NO_INTERRUPT,
+ 		.module		= THIS_MODULE,
+ 	};
+ 
+ 	if (perf_pmu_register(&pmu->pmu, pmu->pmu.name, -1)) {
+ 		free_percpu(pmu->ctx);
+ 		pmu->ctx = NULL;
+ 		goto done;
+ 	}
+ 
+ 	pr_info("%d %s%s counters detected\n", pmu->num_counters,
+ 		boot_cpu_data.x86_vendor == X86_VENDOR_HYGON ?  "HYGON " : "",
+ 		pmu->pmu.name);
+ 
+ 	uncore->num_pmus = 1;
+ 
+ done:
+ 	uncore->init_done = true;
+ 
+ 	return amd_uncore_ctx_init(uncore, cpu);
+ }
+ 
+ static int amd_uncore_l3_event_init(struct perf_event *event)
+ {
+ 	int ret = amd_uncore_event_init(event);
+ 	struct hw_perf_event *hwc = &event->hw;
+ 	u64 config = event->attr.config;
+ 	u64 mask;
+ 
+ 	hwc->config = config & AMD64_RAW_EVENT_MASK_NB;
+ 
+ 	/*
+ 	 * SliceMask and ThreadMask need to be set for certain L3 events.
+ 	 * For other events, the two fields do not affect the count.
+ 	 */
+ 	if (ret || boot_cpu_data.x86 < 0x17)
+ 		return ret;
+ 
+ 	mask = config & (AMD64_L3_F19H_THREAD_MASK | AMD64_L3_SLICEID_MASK |
+ 			 AMD64_L3_EN_ALL_CORES | AMD64_L3_EN_ALL_SLICES |
+ 			 AMD64_L3_COREID_MASK);
+ 
+ 	if (boot_cpu_data.x86 <= 0x18)
+ 		mask = ((config & AMD64_L3_SLICE_MASK) ? : AMD64_L3_SLICE_MASK) |
+ 		       ((config & AMD64_L3_THREAD_MASK) ? : AMD64_L3_THREAD_MASK);
+ 
+ 	/*
+ 	 * If the user doesn't specify a ThreadMask, they're not trying to
+ 	 * count core 0, so we enable all cores & threads.
+ 	 * We'll also assume that they want to count slice 0 if they specify
+ 	 * a ThreadMask and leave SliceId and EnAllSlices unpopulated.
+ 	 */
+ 	else if (!(config & AMD64_L3_F19H_THREAD_MASK))
+ 		mask = AMD64_L3_F19H_THREAD_MASK | AMD64_L3_EN_ALL_SLICES |
+ 		       AMD64_L3_EN_ALL_CORES;
+ 
+ 	hwc->config |= mask;
+ 
+ 	return 0;
+ }
+ 
+ static
+ void amd_uncore_l3_ctx_scan(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	union amd_uncore_info info;
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_PERFCTR_LLC))
+ 		return;
+ 
+ 	info.split.aux_data = 0;
+ 	info.split.num_pmcs = NUM_COUNTERS_L2;
+ 	info.split.cid = get_llc_id(cpu);
+ 
+ 	if (boot_cpu_data.x86 >= 0x17)
+ 		info.split.num_pmcs = NUM_COUNTERS_L3;
+ 
+ 	*per_cpu_ptr(uncore->info, cpu) = info;
+ }
+ 
+ static
+ int amd_uncore_l3_ctx_init(struct amd_uncore *uncore, unsigned int cpu)
+ {
+ 	struct attribute **l3_attr = amd_uncore_l3_format_attr;
+ 	struct amd_uncore_pmu *pmu;
+ 
+ 	/* Run just once */
+ 	if (uncore->init_done)
+ 		return amd_uncore_ctx_init(uncore, cpu);
+ 
+ 	/* No grouping, single instance for a system */
+ 	uncore->pmus = kzalloc(sizeof(*uncore->pmus), GFP_KERNEL);
+ 	if (!uncore->pmus) {
+ 		uncore->num_pmus = 0;
+ 		goto done;
+ 	}
+ 
+ 	/*
+ 	 * For Family 17h and above, L3 cache counters are available instead
+ 	 * of L2 cache counters. The PMUs are exported based on family as
+ 	 * either L2 or L3.
+ 	 */
+ 	pmu = &uncore->pmus[0];
+ 	strscpy(pmu->name, boot_cpu_data.x86 >= 0x17 ? "amd_l3" : "amd_l2",
+ 		sizeof(pmu->name));
+ 	pmu->num_counters = amd_uncore_ctx_num_pmcs(uncore, cpu);
+ 	pmu->msr_base = MSR_F16H_L2I_PERF_CTL;
+ 	pmu->rdpmc_base = RDPMC_BASE_LLC;
+ 
+ 	if (boot_cpu_data.x86 >= 0x17) {
+ 		*l3_attr++ = &format_attr_event8.attr;
+ 		*l3_attr++ = &format_attr_umask8.attr;
+ 		*l3_attr++ = boot_cpu_data.x86 >= 0x19 ?
+ 			     &format_attr_threadmask2.attr :
+ 			     &format_attr_threadmask8.attr;
+ 	}
+ 
+ 	pmu->ctx = alloc_percpu(struct amd_uncore_ctx *);
+ 	if (!pmu->ctx)
+ 		goto done;
+ 
+ 	pmu->pmu = (struct pmu) {
+ 		.task_ctx_nr	= perf_invalid_context,
+ 		.attr_groups	= amd_uncore_l3_attr_groups,
+ 		.attr_update	= amd_uncore_l3_attr_update,
+ 		.name		= pmu->name,
+ 		.event_init	= amd_uncore_l3_event_init,
+ 		.add		= amd_uncore_add,
+ 		.del		= amd_uncore_del,
+ 		.start		= amd_uncore_start,
+ 		.stop		= amd_uncore_stop,
+ 		.read		= amd_uncore_read,
+ 		.capabilities	= PERF_PMU_CAP_NO_EXCLUDE | PERF_PMU_CAP_NO_INTERRUPT,
+ 		.module		= THIS_MODULE,
+ 	};
+ 
+ 	if (perf_pmu_register(&pmu->pmu, pmu->pmu.name, -1)) {
+ 		free_percpu(pmu->ctx);
+ 		pmu->ctx = NULL;
+ 		goto done;
+ 	}
+ 
+ 	pr_info("%d %s%s counters detected\n", pmu->num_counters,
+ 		boot_cpu_data.x86_vendor == X86_VENDOR_HYGON ?  "HYGON " : "",
+ 		pmu->pmu.name);
+ 
+ 	uncore->num_pmus = 1;
+ 
+ done:
+ 	uncore->init_done = true;
+ 
+ 	return amd_uncore_ctx_init(uncore, cpu);
+ }
+ 
+ static struct amd_uncore uncores[UNCORE_TYPE_MAX] = {
+ 	/* UNCORE_TYPE_DF */
+ 	{
+ 		.scan = amd_uncore_df_ctx_scan,
+ 		.init = amd_uncore_df_ctx_init,
+ 		.move = amd_uncore_ctx_move,
+ 		.free = amd_uncore_ctx_free,
+ 	},
+ 	/* UNCORE_TYPE_L3 */
+ 	{
+ 		.scan = amd_uncore_l3_ctx_scan,
+ 		.init = amd_uncore_l3_ctx_init,
+ 		.move = amd_uncore_ctx_move,
+ 		.free = amd_uncore_ctx_free,
+ 	},
+ };
+ 
+ static int __init amd_uncore_init(void)
+ {
+ 	struct amd_uncore *uncore;
+ 	int ret, i;
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  
 -	if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&
 -	    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)
 +	if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD)
  		return -ENODEV;
  
  	if (!boot_cpu_has(X86_FEATURE_TOPOEXT))
@@@ -662,77 -836,28 +1184,99 @@@
  	if (boot_cpu_has(X86_FEATURE_PERFMON_V2))
  		pmu_version = 2;
  
++<<<<<<< HEAD
 +	num_counters_nb	= NUM_COUNTERS_NB;
 +	num_counters_llc = NUM_COUNTERS_L2;
 +	if (boot_cpu_data.x86 >= 0x17) {
 +		/*
 +		 * For F17h and above, the Northbridge counters are repurposed as Data
 +		 * Fabric counters. Also, L3 counters are supported too. The PMUs
 +		 * are exported based on  family as either L2 or L3 and NB or DF.
 +		 */
 +		num_counters_llc	  = NUM_COUNTERS_L3;
 +		amd_nb_pmu.name		  = "amd_df";
 +		amd_llc_pmu.name	  = "amd_l3";
 +		l3_mask			  = true;
 +	}
 +
 +	if (boot_cpu_has(X86_FEATURE_PERFCTR_NB)) {
 +		if (pmu_version >= 2) {
 +			*df_attr++ = &format_attr_event14v2.attr;
 +			*df_attr++ = &format_attr_umask12.attr;
 +		} else if (boot_cpu_data.x86 >= 0x17) {
 +			*df_attr = &format_attr_event14.attr;
 +		}
 +
 +		amd_uncore_nb = alloc_percpu(struct amd_uncore *);
 +		if (!amd_uncore_nb) {
 +			ret = -ENOMEM;
 +			goto fail_nb;
 +		}
 +		ret = perf_pmu_register(&amd_nb_pmu, amd_nb_pmu.name, -1);
 +		if (ret)
 +			goto fail_nb;
 +
 +		if (pmu_version >= 2) {
 +			ebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);
 +			num_counters_nb = ebx.split.num_df_pmc;
 +		}
 +
 +		pr_info("%d %s counters detected\n", num_counters_nb, amd_nb_pmu.name);
 +		ret = 0;
 +	}
 +
 +	if (boot_cpu_has(X86_FEATURE_PERFCTR_LLC)) {
 +		if (boot_cpu_data.x86 >= 0x19) {
 +			*l3_attr++ = &format_attr_event8.attr;
 +			*l3_attr++ = &format_attr_umask8.attr;
 +			*l3_attr++ = &format_attr_threadmask2.attr;
 +		} else if (boot_cpu_data.x86 >= 0x17) {
 +			*l3_attr++ = &format_attr_event8.attr;
 +			*l3_attr++ = &format_attr_umask8.attr;
 +			*l3_attr++ = &format_attr_threadmask8.attr;
 +		}
 +
 +		amd_uncore_llc = alloc_percpu(struct amd_uncore *);
 +		if (!amd_uncore_llc) {
 +			ret = -ENOMEM;
 +			goto fail_llc;
 +		}
 +		ret = perf_pmu_register(&amd_llc_pmu, amd_llc_pmu.name, -1);
 +		if (ret)
 +			goto fail_llc;
 +
 +		pr_info("%d %s counters detected\n", num_counters_llc, amd_llc_pmu.name);
 +		ret = 0;
 +	}
++=======
+ 	for (i = 0; i < UNCORE_TYPE_MAX; i++) {
+ 		uncore = &uncores[i];
+ 
+ 		BUG_ON(!uncore->scan);
+ 		BUG_ON(!uncore->init);
+ 		BUG_ON(!uncore->move);
+ 		BUG_ON(!uncore->free);
+ 
+ 		uncore->info = alloc_percpu(union amd_uncore_info);
+ 		if (!uncore->info) {
+ 			ret = -ENOMEM;
+ 			goto fail;
+ 		}
+ 	};
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  
  	/*
  	 * Install callbacks. Core will call them for each online cpu.
  	 */
  	if (cpuhp_setup_state(CPUHP_PERF_X86_AMD_UNCORE_PREP,
  			      "perf/x86/amd/uncore:prepare",
++<<<<<<< HEAD
 +			      amd_uncore_cpu_up_prepare, amd_uncore_cpu_dead))
 +		goto fail_llc;
++=======
+ 			      NULL, amd_uncore_cpu_dead))
+ 		goto fail;
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  
  	if (cpuhp_setup_state(CPUHP_AP_PERF_X86_AMD_UNCORE_STARTING,
  			      "perf/x86/amd/uncore:starting",
@@@ -749,12 -874,14 +1293,23 @@@ fail_start
  	cpuhp_remove_state(CPUHP_AP_PERF_X86_AMD_UNCORE_STARTING);
  fail_prep:
  	cpuhp_remove_state(CPUHP_PERF_X86_AMD_UNCORE_PREP);
++<<<<<<< HEAD
 +fail_llc:
 +	if (boot_cpu_has(X86_FEATURE_PERFCTR_NB))
 +		perf_pmu_unregister(&amd_nb_pmu);
 +	free_percpu(amd_uncore_llc);
 +fail_nb:
 +	free_percpu(amd_uncore_nb);
++=======
+ fail:
+ 	for (i = 0; i < UNCORE_TYPE_MAX; i++) {
+ 		uncore = &uncores[i];
+ 		if (uncore->info) {
+ 			free_percpu(uncore->info);
+ 			uncore->info = NULL;
+ 		}
+ 	}
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  
  	return ret;
  }
@@@ -765,16 -896,26 +1324,39 @@@ static void __exit amd_uncore_exit(void
  	cpuhp_remove_state(CPUHP_AP_PERF_X86_AMD_UNCORE_STARTING);
  	cpuhp_remove_state(CPUHP_PERF_X86_AMD_UNCORE_PREP);
  
++<<<<<<< HEAD
 +	if (boot_cpu_has(X86_FEATURE_PERFCTR_LLC)) {
 +		perf_pmu_unregister(&amd_llc_pmu);
 +		free_percpu(amd_uncore_llc);
 +		amd_uncore_llc = NULL;
 +	}
 +
 +	if (boot_cpu_has(X86_FEATURE_PERFCTR_NB)) {
 +		perf_pmu_unregister(&amd_nb_pmu);
 +		free_percpu(amd_uncore_nb);
 +		amd_uncore_nb = NULL;
++=======
+ 	for (i = 0; i < UNCORE_TYPE_MAX; i++) {
+ 		uncore = &uncores[i];
+ 		if (!uncore->info)
+ 			continue;
+ 
+ 		free_percpu(uncore->info);
+ 		uncore->info = NULL;
+ 
+ 		for (j = 0; j < uncore->num_pmus; j++) {
+ 			pmu = &uncore->pmus[j];
+ 			if (!pmu->ctx)
+ 				continue;
+ 
+ 			perf_pmu_unregister(&pmu->pmu);
+ 			free_percpu(pmu->ctx);
+ 			pmu->ctx = NULL;
+ 		}
+ 
+ 		kfree(uncore->pmus);
+ 		uncore->pmus = NULL;
++>>>>>>> 07888daa056e (perf/x86/amd/uncore: Move discovery and registration)
  	}
  }
  
* Unmerged path arch/x86/events/amd/uncore.c
