x86/sev: Harden #VC instruction emulation somewhat

jira LE-1907
cve CVE-2024-25742
cve CVE-2024-25743
Rebuild_History Non-Buildable kernel-4.18.0-551.el8
commit-author Borislav Petkov (AMD) <bp@alien8.de>
commit e3ef461af35a8c74f2f4ce6616491ddb355a208f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-551.el8/e3ef461a.failed

Compare the opcode bytes at rIP for each #VC exit reason to verify the
instruction which raised the #VC exception is actually the right one.

	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
	Acked-by: Tom Lendacky <thomas.lendacky@amd.com>
Link: https://lore.kernel.org/r/20240105101407.11694-1-bp@alien8.de
(cherry picked from commit e3ef461af35a8c74f2f4ce6616491ddb355a208f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/sev-shared.c
diff --cc arch/x86/kernel/sev-shared.c
index 71d8698702ce,5db24d0fc557..000000000000
--- a/arch/x86/kernel/sev-shared.c
+++ b/arch/x86/kernel/sev-shared.c
@@@ -10,8 -10,15 +10,20 @@@
   */
  
  #ifndef __BOOT_COMPRESSED
++<<<<<<< HEAD
 +#define error(v)	pr_err(v)
 +#define has_cpuflag(f)	boot_cpu_has(f)
++=======
+ #define error(v)			pr_err(v)
+ #define has_cpuflag(f)			boot_cpu_has(f)
+ #define sev_printk(fmt, ...)		printk(fmt, ##__VA_ARGS__)
+ #define sev_printk_rtl(fmt, ...)	printk_ratelimited(fmt, ##__VA_ARGS__)
+ #else
+ #undef WARN
+ #define WARN(condition, format...) (!!(condition))
+ #define sev_printk(fmt, ...)
+ #define sev_printk_rtl(fmt, ...)
++>>>>>>> e3ef461af35a (x86/sev: Harden #VC instruction emulation somewhat)
  #endif
  
  /* I/O parameters for CPUID-related helpers */
@@@ -1067,3 -1079,192 +1084,195 @@@ static void __init setup_cpuid_table(co
  			cpuid_ext_range_max = fn->eax;
  	}
  }
++<<<<<<< HEAD
++=======
+ 
+ static void pvalidate_pages(struct snp_psc_desc *desc)
+ {
+ 	struct psc_entry *e;
+ 	unsigned long vaddr;
+ 	unsigned int size;
+ 	unsigned int i;
+ 	bool validate;
+ 	int rc;
+ 
+ 	for (i = 0; i <= desc->hdr.end_entry; i++) {
+ 		e = &desc->entries[i];
+ 
+ 		vaddr = (unsigned long)pfn_to_kaddr(e->gfn);
+ 		size = e->pagesize ? RMP_PG_SIZE_2M : RMP_PG_SIZE_4K;
+ 		validate = e->operation == SNP_PAGE_STATE_PRIVATE;
+ 
+ 		rc = pvalidate(vaddr, size, validate);
+ 		if (rc == PVALIDATE_FAIL_SIZEMISMATCH && size == RMP_PG_SIZE_2M) {
+ 			unsigned long vaddr_end = vaddr + PMD_SIZE;
+ 
+ 			for (; vaddr < vaddr_end; vaddr += PAGE_SIZE) {
+ 				rc = pvalidate(vaddr, RMP_PG_SIZE_4K, validate);
+ 				if (rc)
+ 					break;
+ 			}
+ 		}
+ 
+ 		if (rc) {
+ 			WARN(1, "Failed to validate address 0x%lx ret %d", vaddr, rc);
+ 			sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PVALIDATE);
+ 		}
+ 	}
+ }
+ 
+ static int vmgexit_psc(struct ghcb *ghcb, struct snp_psc_desc *desc)
+ {
+ 	int cur_entry, end_entry, ret = 0;
+ 	struct snp_psc_desc *data;
+ 	struct es_em_ctxt ctxt;
+ 
+ 	vc_ghcb_invalidate(ghcb);
+ 
+ 	/* Copy the input desc into GHCB shared buffer */
+ 	data = (struct snp_psc_desc *)ghcb->shared_buffer;
+ 	memcpy(ghcb->shared_buffer, desc, min_t(int, GHCB_SHARED_BUF_SIZE, sizeof(*desc)));
+ 
+ 	/*
+ 	 * As per the GHCB specification, the hypervisor can resume the guest
+ 	 * before processing all the entries. Check whether all the entries
+ 	 * are processed. If not, then keep retrying. Note, the hypervisor
+ 	 * will update the data memory directly to indicate the status, so
+ 	 * reference the data->hdr everywhere.
+ 	 *
+ 	 * The strategy here is to wait for the hypervisor to change the page
+ 	 * state in the RMP table before guest accesses the memory pages. If the
+ 	 * page state change was not successful, then later memory access will
+ 	 * result in a crash.
+ 	 */
+ 	cur_entry = data->hdr.cur_entry;
+ 	end_entry = data->hdr.end_entry;
+ 
+ 	while (data->hdr.cur_entry <= data->hdr.end_entry) {
+ 		ghcb_set_sw_scratch(ghcb, (u64)__pa(data));
+ 
+ 		/* This will advance the shared buffer data points to. */
+ 		ret = sev_es_ghcb_hv_call(ghcb, &ctxt, SVM_VMGEXIT_PSC, 0, 0);
+ 
+ 		/*
+ 		 * Page State Change VMGEXIT can pass error code through
+ 		 * exit_info_2.
+ 		 */
+ 		if (WARN(ret || ghcb->save.sw_exit_info_2,
+ 			 "SNP: PSC failed ret=%d exit_info_2=%llx\n",
+ 			 ret, ghcb->save.sw_exit_info_2)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/* Verify that reserved bit is not set */
+ 		if (WARN(data->hdr.reserved, "Reserved bit is set in the PSC header\n")) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/*
+ 		 * Sanity check that entry processing is not going backwards.
+ 		 * This will happen only if hypervisor is tricking us.
+ 		 */
+ 		if (WARN(data->hdr.end_entry > end_entry || cur_entry > data->hdr.cur_entry,
+ "SNP: PSC processing going backward, end_entry %d (got %d) cur_entry %d (got %d)\n",
+ 			 end_entry, data->hdr.end_entry, cur_entry, data->hdr.cur_entry)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 	}
+ 
+ out:
+ 	return ret;
+ }
+ 
+ static enum es_result vc_check_opcode_bytes(struct es_em_ctxt *ctxt,
+ 					    unsigned long exit_code)
+ {
+ 	unsigned int opcode = (unsigned int)ctxt->insn.opcode.value;
+ 	u8 modrm = ctxt->insn.modrm.value;
+ 
+ 	switch (exit_code) {
+ 
+ 	case SVM_EXIT_IOIO:
+ 	case SVM_EXIT_NPF:
+ 		/* handled separately */
+ 		return ES_OK;
+ 
+ 	case SVM_EXIT_CPUID:
+ 		if (opcode == 0xa20f)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_INVD:
+ 		if (opcode == 0x080f)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_MONITOR:
+ 		if (opcode == 0x010f && modrm == 0xc8)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_MWAIT:
+ 		if (opcode == 0x010f && modrm == 0xc9)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_MSR:
+ 		/* RDMSR */
+ 		if (opcode == 0x320f ||
+ 		/* WRMSR */
+ 		    opcode == 0x300f)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_RDPMC:
+ 		if (opcode == 0x330f)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_RDTSC:
+ 		if (opcode == 0x310f)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_RDTSCP:
+ 		if (opcode == 0x010f && modrm == 0xf9)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_READ_DR7:
+ 		if (opcode == 0x210f &&
+ 		    X86_MODRM_REG(ctxt->insn.modrm.value) == 7)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_VMMCALL:
+ 		if (opcode == 0x010f && modrm == 0xd9)
+ 			return ES_OK;
+ 
+ 		break;
+ 
+ 	case SVM_EXIT_WRITE_DR7:
+ 		if (opcode == 0x230f &&
+ 		    X86_MODRM_REG(ctxt->insn.modrm.value) == 7)
+ 			return ES_OK;
+ 		break;
+ 
+ 	case SVM_EXIT_WBINVD:
+ 		if (opcode == 0x90f)
+ 			return ES_OK;
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
+ 
+ 	sev_printk(KERN_ERR "Wrong/unhandled opcode bytes: 0x%x, exit_code: 0x%lx, rIP: 0x%lx\n",
+ 		   opcode, exit_code, ctxt->regs->ip);
+ 
+ 	return ES_UNSUPPORTED;
+ }
++>>>>>>> e3ef461af35a (x86/sev: Harden #VC instruction emulation somewhat)
diff --git a/arch/x86/boot/compressed/sev.c b/arch/x86/boot/compressed/sev.c
index 1d834361bb39..3367399d1670 100644
--- a/arch/x86/boot/compressed/sev.c
+++ b/arch/x86/boot/compressed/sev.c
@@ -255,6 +255,10 @@ void do_boot_stage2_vc(struct pt_regs *regs, unsigned long exit_code)
 	if (result != ES_OK)
 		goto finish;
 
+	result = vc_check_opcode_bytes(&ctxt, exit_code);
+	if (result != ES_OK)
+		goto finish;
+
 	switch (exit_code) {
 	case SVM_EXIT_RDTSC:
 	case SVM_EXIT_RDTSCP:
* Unmerged path arch/x86/kernel/sev-shared.c
diff --git a/arch/x86/kernel/sev.c b/arch/x86/kernel/sev.c
index f8ed0ab37051..f03dcb8332ff 100644
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@ -1799,7 +1799,10 @@ static enum es_result vc_handle_exitcode(struct es_em_ctxt *ctxt,
 					 struct ghcb *ghcb,
 					 unsigned long exit_code)
 {
-	enum es_result result;
+	enum es_result result = vc_check_opcode_bytes(ctxt, exit_code);
+
+	if (result != ES_OK)
+		return result;
 
 	switch (exit_code) {
 	case SVM_EXIT_READ_DR7:
