sched/fair: Make per-cpu cpumasks static

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-497.el8
commit-author Bing Huang <huangbing@kylinos.cn>
commit 18c31c9711a90b48a77b78afb65012d9feec444c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-497.el8/18c31c97.failed

The load_balance_mask and select_rq_mask percpu variables are only used in
kernel/sched/fair.c.

Make them static and move their allocation into init_sched_fair_class().

Replace kzalloc_node() with zalloc_cpumask_var_node() to get rid of the
CONFIG_CPUMASK_OFFSTACK #ifdef and to align with per-cpu cpumask
allocation for RT (local_cpu_mask in init_sched_rt_class()) and DL
class (local_cpu_mask_dl in init_sched_dl_class()).

[ mingo: Tidied up changelog & touched up the code. ]

	Signed-off-by: Bing Huang <huangbing@kylinos.cn>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
	Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
	Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
Link: https://lore.kernel.org/r/20220722213609.3901-1-huangbing775@126.com
(cherry picked from commit 18c31c9711a90b48a77b78afb65012d9feec444c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
#	kernel/sched/fair.c
diff --cc kernel/sched/core.c
index dd25d2369100,64c08993221b..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -7243,14 -9563,19 +7243,17 @@@ LIST_HEAD(task_groups)
  static struct kmem_cache *task_group_cache __read_mostly;
  #endif
  
++<<<<<<< HEAD
 +DECLARE_PER_CPU(cpumask_var_t, load_balance_mask);
 +DECLARE_PER_CPU(cpumask_var_t, select_idle_mask);
 +
++=======
++>>>>>>> 18c31c9711a9 (sched/fair: Make per-cpu cpumasks static)
  void __init sched_init(void)
  {
 -	unsigned long ptr = 0;
 +	unsigned long alloc_size = 0, ptr;
  	int i;
  
 -	/* Make sure the linker didn't screw up */
 -	BUG_ON(&idle_sched_class != &fair_sched_class + 1 ||
 -	       &fair_sched_class != &rt_sched_class + 1 ||
 -	       &rt_sched_class   != &dl_sched_class + 1);
 -#ifdef CONFIG_SMP
 -	BUG_ON(&dl_sched_class != &stop_sched_class + 1);
 -#endif
 -
  	wait_bit_init();
  
  #ifdef CONFIG_FAIR_GROUP_SCHED
@@@ -7281,17 -9606,8 +7284,20 @@@
  
  #endif /* CONFIG_RT_GROUP_SCHED */
  	}
++<<<<<<< HEAD
 +#ifdef CONFIG_CPUMASK_OFFSTACK
 +	for_each_possible_cpu(i) {
 +		per_cpu(load_balance_mask, i) = (cpumask_var_t)kzalloc_node(
 +			cpumask_size(), GFP_KERNEL, cpu_to_node(i));
 +		per_cpu(select_idle_mask, i) = (cpumask_var_t)kzalloc_node(
 +			cpumask_size(), GFP_KERNEL, cpu_to_node(i));
 +	}
 +#endif /* CONFIG_CPUMASK_OFFSTACK */
++=======
++>>>>>>> 18c31c9711a9 (sched/fair: Make per-cpu cpumasks static)
  
  	init_rt_bandwidth(&def_rt_bandwidth, global_rt_period(), global_rt_runtime());
 +	init_dl_bandwidth(&def_dl_bandwidth, global_rt_period(), global_rt_runtime());
  
  #ifdef CONFIG_SMP
  	init_defrootdomain();
diff --cc kernel/sched/fair.c
index e2a86c009bfe,da388657d5ac..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -5776,8 -5893,8 +5776,13 @@@ dequeue_throttle
  #ifdef CONFIG_SMP
  
  /* Working cpumask for: load_balance, load_balance_newidle. */
++<<<<<<< HEAD
 +DEFINE_PER_CPU(cpumask_var_t, load_balance_mask);
 +DEFINE_PER_CPU(cpumask_var_t, select_idle_mask);
++=======
+ static DEFINE_PER_CPU(cpumask_var_t, load_balance_mask);
+ static DEFINE_PER_CPU(cpumask_var_t, select_rq_mask);
++>>>>>>> 18c31c9711a9 (sched/fair: Make per-cpu cpumasks static)
  
  #ifdef CONFIG_NO_HZ_COMMON
  
* Unmerged path kernel/sched/core.c
* Unmerged path kernel/sched/fair.c
