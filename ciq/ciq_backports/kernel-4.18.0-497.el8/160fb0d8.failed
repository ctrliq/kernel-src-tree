sched/core: Reorganize ttwu_do_wakeup() and ttwu_do_activate()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-497.el8
commit-author Chengming Zhou <zhouchengming@bytedance.com>
commit 160fb0d83f206b3429fc495864a022110f9e4978
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-497.el8/160fb0d8.failed

ttwu_do_activate() is used for a complete wakeup, in which we will
activate_task() and use ttwu_do_wakeup() to mark the task runnable
and perform wakeup-preemption, also call class->task_woken() callback
and update the rq->idle_stamp.

Since ttwu_runnable() is not a complete wakeup, don't need all those
done in ttwu_do_wakeup(), so we can move those to ttwu_do_activate()
to simplify ttwu_do_wakeup(), making it only mark the task runnable
to be reused in ttwu_runnable() and try_to_wake_up().

This patch should not have any functional changes.

	Suggested-by: Peter Zijlstra <peterz@infradead.org>
	Signed-off-by: Chengming Zhou <zhouchengming@bytedance.com>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20221223103257.4962-2-zhouchengming@bytedance.com
(cherry picked from commit 160fb0d83f206b3429fc495864a022110f9e4978)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 4d4fdb30072f,03b8529db73f..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -2132,271 -2530,394 +2132,298 @@@ out
  }
  
  /*
 - * sched_class::set_cpus_allowed must do the below, but is not required to
 - * actually call this function.
 + * The caller (fork, wakeup) owns p->pi_lock, ->cpus_ptr is stable.
   */
 -void set_cpus_allowed_common(struct task_struct *p, struct affinity_context *ctx)
 +static inline
 +int select_task_rq(struct task_struct *p, int cpu, int wake_flags)
  {
 -	if (ctx->flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE)) {
 -		p->cpus_ptr = ctx->new_mask;
 -		return;
 -	}
 +	lockdep_assert_held(&p->pi_lock);
  
 -	cpumask_copy(&p->cpus_mask, ctx->new_mask);
 -	p->nr_cpus_allowed = cpumask_weight(ctx->new_mask);
 +	if (p->nr_cpus_allowed > 1 && !is_migration_disabled(p))
 +		cpu = p->sched_class->select_task_rq(p, cpu, wake_flags);
 +	else
 +		cpu = cpumask_any(p->cpus_ptr);
  
  	/*
 -	 * Swap in a new user_cpus_ptr if SCA_USER flag set
 +	 * In order not to call set_task_cpu() on a blocking task we need
 +	 * to rely on ttwu() to place the task on a valid ->cpus_ptr
 +	 * CPU.
 +	 *
 +	 * Since this is common to all placement strategies, this lives here.
 +	 *
 +	 * [ this allows ->select_task() to simply return task_cpu(p) and
 +	 *   not worry about this generic constraint ]
  	 */
 -	if (ctx->flags & SCA_USER)
 -		swap(p->user_cpus_ptr, ctx->user_mask);
 +	if (unlikely(!is_cpu_allowed(p, cpu)))
 +		cpu = select_fallback_rq(task_cpu(p), p);
 +
 +	return cpu;
  }
  
 -static void
 -__do_set_cpus_allowed(struct task_struct *p, struct affinity_context *ctx)
 +void sched_set_stop_task(int cpu, struct task_struct *stop)
  {
 -	struct rq *rq = task_rq(p);
 -	bool queued, running;
 +	static struct lock_class_key stop_pi_lock;
 +	struct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };
 +	struct task_struct *old_stop = cpu_rq(cpu)->stop;
  
 -	/*
 -	 * This here violates the locking rules for affinity, since we're only
 -	 * supposed to change these variables while holding both rq->lock and
 -	 * p->pi_lock.
 -	 *
 -	 * HOWEVER, it magically works, because ttwu() is the only code that
 -	 * accesses these variables under p->pi_lock and only does so after
 -	 * smp_cond_load_acquire(&p->on_cpu, !VAL), and we're in __schedule()
 -	 * before finish_task().
 -	 *
 -	 * XXX do further audits, this smells like something putrid.
 -	 */
 -	if (ctx->flags & SCA_MIGRATE_DISABLE)
 -		SCHED_WARN_ON(!p->on_cpu);
 -	else
 -		lockdep_assert_held(&p->pi_lock);
 +	if (stop) {
 +		/*
 +		 * Make it appear like a SCHED_FIFO task, its something
 +		 * userspace knows about and won't get confused about.
 +		 *
 +		 * Also, it will make PI more or less work without too
 +		 * much confusion -- but then, stop work should not
 +		 * rely on PI working anyway.
 +		 */
 +		sched_setscheduler_nocheck(stop, SCHED_FIFO, &param);
  
 -	queued = task_on_rq_queued(p);
 -	running = task_current(rq, p);
 +		stop->sched_class = &stop_sched_class;
  
 -	if (queued) {
  		/*
 -		 * Because __kthread_bind() calls this on blocked tasks without
 -		 * holding rq->lock.
 +		 * The PI code calls rt_mutex_setprio() with ->pi_lock held to
 +		 * adjust the effective priority of a task. As a result,
 +		 * rt_mutex_setprio() can trigger (RT) balancing operations,
 +		 * which can then trigger wakeups of the stop thread to push
 +		 * around the current task.
 +		 *
 +		 * The stop task itself will never be part of the PI-chain, it
 +		 * never blocks, therefore that ->pi_lock recursion is safe.
 +		 * Tell lockdep about this by placing the stop->pi_lock in its
 +		 * own class.
  		 */
 -		lockdep_assert_rq_held(rq);
 -		dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
 +		lockdep_set_class(&stop->pi_lock, &stop_pi_lock);
  	}
 -	if (running)
 -		put_prev_task(rq, p);
  
 -	p->sched_class->set_cpus_allowed(p, ctx);
 +	cpu_rq(cpu)->stop = stop;
  
 -	if (queued)
 -		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
 -	if (running)
 -		set_next_task(rq, p);
 +	if (old_stop) {
 +		/*
 +		 * Reset it back to a normal scheduling class so that
 +		 * it can die in pieces.
 +		 */
 +		old_stop->sched_class = &rt_sched_class;
 +	}
  }
  
 -/*
 - * Used for kthread_bind() and select_fallback_rq(), in both cases the user
 - * affinity (if any) should be destroyed too.
 - */
 -void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 -{
 -	struct affinity_context ac = {
 -		.new_mask  = new_mask,
 -		.user_mask = NULL,
 -		.flags     = SCA_USER,	/* clear the user requested mask */
 -	};
 -
 -	__do_set_cpus_allowed(p, &ac);
 -	kfree(ac.user_mask);
 -}
 +#else /* CONFIG_SMP */
  
 -int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,
 -		      int node)
 +static inline int __set_cpus_allowed_ptr(struct task_struct *p,
 +					 const struct cpumask *new_mask,
 +					 u32 flags)
  {
 -	unsigned long flags;
 -
 -	if (!src->user_cpus_ptr)
 -		return 0;
 +	return set_cpus_allowed_ptr(p, new_mask);
 +}
  
 -	dst->user_cpus_ptr = kmalloc_node(cpumask_size(), GFP_KERNEL, node);
 -	if (!dst->user_cpus_ptr)
 -		return -ENOMEM;
 +static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }
  
 -	/* Use pi_lock to protect content of user_cpus_ptr */
 -	raw_spin_lock_irqsave(&src->pi_lock, flags);
 -	cpumask_copy(dst->user_cpus_ptr, src->user_cpus_ptr);
 -	raw_spin_unlock_irqrestore(&src->pi_lock, flags);
 -	return 0;
 +static inline bool rq_has_pinned_tasks(struct rq *rq)
 +{
 +	return false;
  }
  
 -static inline struct cpumask *clear_user_cpus_ptr(struct task_struct *p)
 -{
 -	struct cpumask *user_mask = NULL;
 +#endif /* !CONFIG_SMP */
  
 -	swap(p->user_cpus_ptr, user_mask);
 +static void
 +ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
 +{
 +	struct rq *rq;
  
 -	return user_mask;
 -}
 +	if (!schedstat_enabled())
 +		return;
  
 -void release_user_cpus_ptr(struct task_struct *p)
 -{
 -	kfree(clear_user_cpus_ptr(p));
 -}
 +	rq = this_rq();
  
 -/*
 - * This function is wildly self concurrent; here be dragons.
 - *
 - *
 - * When given a valid mask, __set_cpus_allowed_ptr() must block until the
 - * designated task is enqueued on an allowed CPU. If that task is currently
 - * running, we have to kick it out using the CPU stopper.
 - *
 - * Migrate-Disable comes along and tramples all over our nice sandcastle.
 - * Consider:
 - *
 - *     Initial conditions: P0->cpus_mask = [0, 1]
 - *
 - *     P0@CPU0                  P1
 - *
 - *     migrate_disable();
 - *     <preempted>
 - *                              set_cpus_allowed_ptr(P0, [1]);
 - *
 - * P1 *cannot* return from this set_cpus_allowed_ptr() call until P0 executes
 - * its outermost migrate_enable() (i.e. it exits its Migrate-Disable region).
 - * This means we need the following scheme:
 - *
 - *     P0@CPU0                  P1
 - *
 - *     migrate_disable();
 - *     <preempted>
 - *                              set_cpus_allowed_ptr(P0, [1]);
 - *                                <blocks>
 - *     <resumes>
 - *     migrate_enable();
 - *       __set_cpus_allowed_ptr();
 - *       <wakes local stopper>
 - *                         `--> <woken on migration completion>
 - *
 - * Now the fun stuff: there may be several P1-like tasks, i.e. multiple
 - * concurrent set_cpus_allowed_ptr(P0, [*]) calls. CPU affinity changes of any
 - * task p are serialized by p->pi_lock, which we can leverage: the one that
 - * should come into effect at the end of the Migrate-Disable region is the last
 - * one. This means we only need to track a single cpumask (i.e. p->cpus_mask),
 - * but we still need to properly signal those waiting tasks at the appropriate
 - * moment.
 - *
 - * This is implemented using struct set_affinity_pending. The first
 - * __set_cpus_allowed_ptr() caller within a given Migrate-Disable region will
 - * setup an instance of that struct and install it on the targeted task_struct.
 - * Any and all further callers will reuse that instance. Those then wait for
 - * a completion signaled at the tail of the CPU stopper callback (1), triggered
 - * on the end of the Migrate-Disable region (i.e. outermost migrate_enable()).
 - *
 - *
 - * (1) In the cases covered above. There is one more where the completion is
 - * signaled within affine_move_task() itself: when a subsequent affinity request
 - * occurs after the stopper bailed out due to the targeted task still being
 - * Migrate-Disable. Consider:
 - *
 - *     Initial conditions: P0->cpus_mask = [0, 1]
 - *
 - *     CPU0		  P1				P2
 - *     <P0>
 - *       migrate_disable();
 - *       <preempted>
 - *                        set_cpus_allowed_ptr(P0, [1]);
 - *                          <blocks>
 - *     <migration/0>
 - *       migration_cpu_stop()
 - *         is_migration_disabled()
 - *           <bails>
 - *                                                       set_cpus_allowed_ptr(P0, [0, 1]);
 - *                                                         <signal completion>
 - *                          <awakes>
 - *
 - * Note that the above is safe vs a concurrent migrate_enable(), as any
 - * pending affinity completion is preceded by an uninstallation of
 - * p->migration_pending done with p->pi_lock held.
 - */
 -static int affine_move_task(struct rq *rq, struct task_struct *p, struct rq_flags *rf,
 -			    int dest_cpu, unsigned int flags)
 -	__releases(rq->lock)
 -	__releases(p->pi_lock)
 -{
 -	struct set_affinity_pending my_pending = { }, *pending = NULL;
 -	bool stop_pending, complete = false;
 -
 -	/* Can the task run on the task's current CPU? If so, we're done */
 -	if (cpumask_test_cpu(task_cpu(p), &p->cpus_mask)) {
 -		struct task_struct *push_task = NULL;
 +#ifdef CONFIG_SMP
 +	if (cpu == rq->cpu) {
 +		__schedstat_inc(rq->ttwu_local);
 +		__schedstat_inc(p->se.statistics.nr_wakeups_local);
 +	} else {
 +		struct sched_domain *sd;
  
 -		if ((flags & SCA_MIGRATE_ENABLE) &&
 -		    (p->migration_flags & MDF_PUSH) && !rq->push_busy) {
 -			rq->push_busy = true;
 -			push_task = get_task_struct(p);
 +		__schedstat_inc(p->se.statistics.nr_wakeups_remote);
 +		rcu_read_lock();
 +		for_each_domain(rq->cpu, sd) {
 +			if (cpumask_test_cpu(cpu, sched_domain_span(sd))) {
 +				__schedstat_inc(sd->ttwu_wake_remote);
 +				break;
 +			}
  		}
 +		rcu_read_unlock();
 +	}
  
 -		/*
 -		 * If there are pending waiters, but no pending stop_work,
 -		 * then complete now.
 -		 */
 -		pending = p->migration_pending;
 -		if (pending && !pending->stop_pending) {
 -			p->migration_pending = NULL;
 -			complete = true;
 -		}
 +	if (wake_flags & WF_MIGRATED)
 +		__schedstat_inc(p->se.statistics.nr_wakeups_migrate);
 +#endif /* CONFIG_SMP */
  
 -		task_rq_unlock(rq, p, rf);
 +	__schedstat_inc(rq->ttwu_count);
 +	__schedstat_inc(p->se.statistics.nr_wakeups);
  
 -		if (push_task) {
 -			stop_one_cpu_nowait(rq->cpu, push_cpu_stop,
 -					    p, &rq->push_work);
 -		}
 +	if (wake_flags & WF_SYNC)
 +		__schedstat_inc(p->se.statistics.nr_wakeups_sync);
 +}
  
 -		if (complete)
 -			complete_all(&pending->done);
 +/*
-  * Mark the task runnable and perform wakeup-preemption.
++ * Mark the task runnable.
 + */
- static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,
- 			   struct rq_flags *rf)
++static inline void ttwu_do_wakeup(struct task_struct *p)
 +{
- 	check_preempt_curr(rq, p, wake_flags);
 +	WRITE_ONCE(p->__state, TASK_RUNNING);
 +	trace_sched_wakeup(p);
++}
+ 
 -		return 0;
 -	}
++static void
++ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,
++		 struct rq_flags *rf)
++{
++	int en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK;
+ 
 -	if (!(flags & SCA_MIGRATE_ENABLE)) {
 -		/* serialized by p->pi_lock */
 -		if (!p->migration_pending) {
 -			/* Install the request */
 -			refcount_set(&my_pending.refs, 1);
 -			init_completion(&my_pending.done);
 -			my_pending.arg = (struct migration_arg) {
 -				.task = p,
 -				.dest_cpu = dest_cpu,
 -				.pending = &my_pending,
 -			};
++	lockdep_assert_rq_held(rq);
+ 
 -			p->migration_pending = &my_pending;
 -		} else {
 -			pending = p->migration_pending;
 -			refcount_inc(&pending->refs);
 -			/*
 -			 * Affinity has changed, but we've already installed a
 -			 * pending. migration_cpu_stop() *must* see this, else
 -			 * we risk a completion of the pending despite having a
 -			 * task on a disallowed CPU.
 -			 *
 -			 * Serialized by p->pi_lock, so this is safe.
 -			 */
 -			pending->arg.dest_cpu = dest_cpu;
 -		}
 -	}
 -	pending = p->migration_pending;
 -	/*
 -	 * - !MIGRATE_ENABLE:
 -	 *   we'll have installed a pending if there wasn't one already.
 -	 *
 -	 * - MIGRATE_ENABLE:
 -	 *   we're here because the current CPU isn't matching anymore,
 -	 *   the only way that can happen is because of a concurrent
 -	 *   set_cpus_allowed_ptr() call, which should then still be
 -	 *   pending completion.
 -	 *
 -	 * Either way, we really should have a @pending here.
 -	 */
 -	if (WARN_ON_ONCE(!pending)) {
 -		task_rq_unlock(rq, p, rf);
 -		return -EINVAL;
++	if (p->sched_contributes_to_load)
++		rq->nr_uninterruptible--;
++
++#ifdef CONFIG_SMP
++	if (wake_flags & WF_MIGRATED)
++		en_flags |= ENQUEUE_MIGRATED;
++	else
++#endif
++	if (p->in_iowait) {
++		delayacct_blkio_end(p);
++		atomic_dec(&task_rq(p)->nr_iowait);
+ 	}
+ 
 -	if (task_on_cpu(rq, p) || READ_ONCE(p->__state) == TASK_WAKING) {
 -		/*
 -		 * MIGRATE_ENABLE gets here because 'p == current', but for
 -		 * anything else we cannot do is_migration_disabled(), punt
 -		 * and have the stopper function handle it all race-free.
 -		 */
 -		stop_pending = pending->stop_pending;
 -		if (!stop_pending)
 -			pending->stop_pending = true;
++	activate_task(rq, p, en_flags);
++	check_preempt_curr(rq, p, wake_flags);
+ 
 -		if (flags & SCA_MIGRATE_ENABLE)
 -			p->migration_flags &= ~MDF_PUSH;
++	ttwu_do_wakeup(p);
  
 -		task_rq_unlock(rq, p, rf);
 +#ifdef CONFIG_SMP
 +	if (p->sched_class->task_woken) {
 +		/*
 +		 * Our task @p is fully woken up and running; so it's safe to
 +		 * drop the rq->lock, hereafter rq is only used for statistics.
 +		 */
 +		rq_unpin_lock(rq, rf);
 +		p->sched_class->task_woken(rq, p);
 +		rq_repin_lock(rq, rf);
 +	}
  
 -		if (!stop_pending) {
 -			stop_one_cpu_nowait(cpu_of(rq), migration_cpu_stop,
 -					    &pending->arg, &pending->stop_work);
 -		}
 +	if (rq->idle_stamp) {
 +		u64 delta = rq_clock(rq) - rq->idle_stamp;
 +		u64 max = 2*rq->max_idle_balance_cost;
  
 -		if (flags & SCA_MIGRATE_ENABLE)
 -			return 0;
 -	} else {
 +		update_avg(&rq->avg_idle, delta);
  
 -		if (!is_migration_disabled(p)) {
 -			if (task_on_rq_queued(p))
 -				rq = move_queued_task(rq, rf, p, dest_cpu);
 +		if (rq->avg_idle > max)
 +			rq->avg_idle = max;
  
 -			if (!pending->stop_pending) {
 -				p->migration_pending = NULL;
 -				complete = true;
 -			}
 -		}
 -		task_rq_unlock(rq, p, rf);
 +		rq->wake_stamp = jiffies;
 +		rq->wake_avg_idle = rq->avg_idle / 2;
  
 -		if (complete)
 -			complete_all(&pending->done);
 +		rq->idle_stamp = 0;
  	}
 +#endif
 +}
  
 -	wait_for_completion(&pending->done);
++<<<<<<< HEAD
 +static void
 +ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,
 +		 struct rq_flags *rf)
 +{
 +	int en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK;
  
 -	if (refcount_dec_and_test(&pending->refs))
 -		wake_up_var(&pending->refs); /* No UaF, just an address */
 +	lockdep_assert_held(&rq->lock);
  
 -	/*
 -	 * Block the original owner of &pending until all subsequent callers
 -	 * have seen the completion and decremented the refcount
 -	 */
 -	wait_var_event(&my_pending.refs, !refcount_read(&my_pending.refs));
 +	if (p->sched_contributes_to_load)
 +		rq->nr_uninterruptible--;
  
 -	/* ARGH */
 -	WARN_ON_ONCE(my_pending.stop_pending);
 +#ifdef CONFIG_SMP
 +	if (wake_flags & WF_MIGRATED)
 +		en_flags |= ENQUEUE_MIGRATED;
 +	else
 +#endif
 +	if (p->in_iowait) {
 +		delayacct_blkio_end(p);
 +		atomic_dec(&task_rq(p)->nr_iowait);
 +	}
  
 -	return 0;
 +	activate_task(rq, p, en_flags);
 +	ttwu_do_wakeup(rq, p, wake_flags, rf);
  }
  
++=======
++>>>>>>> 160fb0d83f20 (sched/core: Reorganize ttwu_do_wakeup() and ttwu_do_activate())
  /*
 - * Called with both p->pi_lock and rq->lock held; drops both before returning.
 + * Consider @p being inside a wait loop:
 + *
 + *   for (;;) {
 + *      set_current_state(TASK_UNINTERRUPTIBLE);
 + *
 + *      if (CONDITION)
 + *         break;
 + *
 + *      schedule();
 + *   }
 + *   __set_current_state(TASK_RUNNING);
 + *
 + * between set_current_state() and schedule(). In this case @p is still
 + * runnable, so all that needs doing is change p->state back to TASK_RUNNING in
 + * an atomic manner.
 + *
 + * By taking task_rq(p)->lock we serialize against schedule(), if @p->on_rq
 + * then schedule() must still happen and p->state can be changed to
 + * TASK_RUNNING. Otherwise we lost the race, schedule() has happened, and we
 + * need to do a full wakeup with enqueue.
 + *
 + * Returns: %true when the wakeup is done,
 + *          %false otherwise.
   */
 -static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 -					 struct affinity_context *ctx,
 -					 struct rq *rq,
 -					 struct rq_flags *rf)
 -	__releases(rq->lock)
 -	__releases(p->pi_lock)
 +static int ttwu_runnable(struct task_struct *p, int wake_flags)
  {
 -	const struct cpumask *cpu_allowed_mask = task_cpu_possible_mask(p);
 -	const struct cpumask *cpu_valid_mask = cpu_active_mask;
 -	bool kthread = p->flags & PF_KTHREAD;
 -	unsigned int dest_cpu;
 +	struct rq_flags rf;
 +	struct rq *rq;
  	int ret = 0;
  
 -	update_rq_clock(rq);
 -
 -	if (kthread || is_migration_disabled(p)) {
 -		/*
 -		 * Kernel threads are allowed on online && !active CPUs,
 -		 * however, during cpu-hot-unplug, even these might get pushed
 -		 * away if not KTHREAD_IS_PER_CPU.
 -		 *
 -		 * Specifically, migration_disabled() tasks must not fail the
 -		 * cpumask_any_and_distribute() pick below, esp. so on
 -		 * SCA_MIGRATE_ENABLE, otherwise we'll not call
 -		 * set_cpus_allowed_common() and actually reset p->cpus_ptr.
 -		 */
 -		cpu_valid_mask = cpu_online_mask;
 +	rq = __task_rq_lock(p, &rf);
 +	if (task_on_rq_queued(p)) {
 +		if (!task_on_cpu(rq, p)) {
 +			/*
 +			 * When on_rq && !on_cpu the task is preempted, see if
 +			 * it should preempt the task that is current now.
 +			 */
 +			update_rq_clock(rq);
 +			check_preempt_curr(rq, p, wake_flags);
 +		}
- 		WRITE_ONCE(p->__state, TASK_RUNNING);
- 		trace_sched_wakeup(p);
++		ttwu_do_wakeup(p);
 +		ret = 1;
  	}
 +	__task_rq_unlock(rq, &rf);
  
 -	if (!kthread && !cpumask_subset(ctx->new_mask, cpu_allowed_mask)) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 +	return ret;
 +}
  
 -	/*
 -	 * Must re-check here, to close a race against __kthread_bind(),
 -	 * sched_setaffinity() is not guaranteed to observe the flag.
 -	 */
 -	if ((ctx->flags & SCA_CHECK) && (p->flags & PF_NO_SETAFFINITY)) {
 -		ret = -EINVAL;
 -		goto out;
 -	}
 +#ifdef CONFIG_SMP
 +void sched_ttwu_pending(void *arg)
 +{
 +	struct llist_node *llist = arg;
 +	struct rq *rq = this_rq();
 +	struct task_struct *p, *t;
 +	struct rq_flags rf;
  
 -	if (!(ctx->flags & SCA_MIGRATE_ENABLE)) {
 -		if (cpumask_equal(&p->cpus_mask, ctx->new_mask))
 -			goto out;
 +	if (!llist)
 +		return;
  
 -		if (WARN_ON_ONCE(p == current &&
 -				 is_migration_disabled(p) &&
 -				 !cpumask_test_cpu(task_cpu(p), ctx->new_mask))) {
 -			ret = -EBUSY;
 -			goto out;
 -		}
 +	rq_lock_irqsave(rq, &rf);
 +	update_rq_clock(rq);
 +
 +	llist_for_each_entry_safe(p, t, llist, wake_entry.llist) {
 +		if (WARN_ON_ONCE(p->on_cpu))
 +			smp_cond_load_acquire(&p->on_cpu, !VAL);
 +
 +		if (WARN_ON_ONCE(task_cpu(p) != cpu_of(rq)))
 +			set_task_cpu(p, cpu_of(rq));
 +
 +		ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
  	}
  
  	/*
@@@ -2542,252 -3229,925 +2569,251 @@@ static void ttwu_queue(struct task_stru
  }
  
  /*
 - * Cross migrate two tasks
 + * Invoked from try_to_wake_up() to check whether the task can be woken up.
 + *
 + * The caller holds p::pi_lock if p != current or has preemption
 + * disabled when p == current.
 + *
 + * The rules of PREEMPT_RT saved_state:
 + *
 + *   The related locking code always holds p::pi_lock when updating
 + *   p::saved_state, which means the code is fully serialized in both cases.
 + *
 + *   The lock wait and lock wakeups happen via TASK_RTLOCK_WAIT. No other
 + *   bits set. This allows to distinguish all wakeup scenarios.
   */
 -int migrate_swap(struct task_struct *cur, struct task_struct *p,
 -		int target_cpu, int curr_cpu)
 +static __always_inline
 +bool ttwu_state_match(struct task_struct *p, unsigned int state, int *success)
  {
 -	struct migration_swap_arg arg;
 -	int ret = -EINVAL;
 -
 -	arg = (struct migration_swap_arg){
 -		.src_task = cur,
 -		.src_cpu = curr_cpu,
 -		.dst_task = p,
 -		.dst_cpu = target_cpu,
 -	};
 +	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT)) {
 +		WARN_ON_ONCE((state & TASK_RTLOCK_WAIT) &&
 +			     state != TASK_RTLOCK_WAIT);
 +	}
  
 -	if (arg.src_cpu == arg.dst_cpu)
 -		goto out;
 +	if (READ_ONCE(p->__state) & state) {
 +		*success = 1;
 +		return true;
 +	}
  
 +#ifdef CONFIG_PREEMPT_RT
  	/*
 -	 * These three tests are all lockless; this is OK since all of them
 -	 * will be re-checked with proper locks held further down the line.
 +	 * Saved state preserves the task state across blocking on
 +	 * an RT lock.  If the state matches, set p::saved_state to
 +	 * TASK_RUNNING, but do not wake the task because it waits
 +	 * for a lock wakeup. Also indicate success because from
 +	 * the regular waker's point of view this has succeeded.
 +	 *
 +	 * After acquiring the lock the task will restore p::__state
 +	 * from p::saved_state which ensures that the regular
 +	 * wakeup is not lost. The restore will also set
 +	 * p::saved_state to TASK_RUNNING so any further tests will
 +	 * not result in false positives vs. @success
  	 */
 -	if (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu))
 -		goto out;
 -
 -	if (!cpumask_test_cpu(arg.dst_cpu, arg.src_task->cpus_ptr))
 -		goto out;
 -
 -	if (!cpumask_test_cpu(arg.src_cpu, arg.dst_task->cpus_ptr))
 -		goto out;
 -
 -	trace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu);
 -	ret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, &arg);
 -
 -out:
 -	return ret;
 +	if (p->saved_state & state) {
 +		p->saved_state = TASK_RUNNING;
 +		*success = 1;
 +	}
 +#endif
 +	return false;
  }
 -#endif /* CONFIG_NUMA_BALANCING */
  
  /*
 - * wait_task_inactive - wait for a thread to unschedule.
 + * Notes on Program-Order guarantees on SMP systems.
   *
 - * Wait for the thread to block in any of the states set in @match_state.
 - * If it changes, i.e. @p might have woken up, then return zero.  When we
 - * succeed in waiting for @p to be off its CPU, we return a positive number
 - * (its total switch count).  If a second call a short while later returns the
 - * same number, the caller can be sure that @p has remained unscheduled the
 - * whole time.
 + *  MIGRATION
   *
 - * The caller must ensure that the task *will* unschedule sometime soon,
 - * else this function might spin for a *long* time. This function can't
 - * be called with interrupts off, or it may introduce deadlock with
 - * smp_call_function() if an IPI is sent by the same process we are
 - * waiting to become inactive.
 - */
 -unsigned long wait_task_inactive(struct task_struct *p, unsigned int match_state)
 -{
 -	int running, queued;
 -	struct rq_flags rf;
 -	unsigned long ncsw;
 -	struct rq *rq;
 -
 -	for (;;) {
 -		/*
 -		 * We do the initial early heuristics without holding
 -		 * any task-queue locks at all. We'll only try to get
 -		 * the runqueue lock when things look like they will
 -		 * work out!
 -		 */
 -		rq = task_rq(p);
 -
 -		/*
 -		 * If the task is actively running on another CPU
 -		 * still, just relax and busy-wait without holding
 -		 * any locks.
 -		 *
 -		 * NOTE! Since we don't hold any locks, it's not
 -		 * even sure that "rq" stays as the right runqueue!
 -		 * But we don't care, since "task_on_cpu()" will
 -		 * return false if the runqueue has changed and p
 -		 * is actually now running somewhere else!
 -		 */
 -		while (task_on_cpu(rq, p)) {
 -			if (!(READ_ONCE(p->__state) & match_state))
 -				return 0;
 -			cpu_relax();
 -		}
 -
 -		/*
 -		 * Ok, time to look more closely! We need the rq
 -		 * lock now, to be *sure*. If we're wrong, we'll
 -		 * just go back and repeat.
 -		 */
 -		rq = task_rq_lock(p, &rf);
 -		trace_sched_wait_task(p);
 -		running = task_on_cpu(rq, p);
 -		queued = task_on_rq_queued(p);
 -		ncsw = 0;
 -		if (READ_ONCE(p->__state) & match_state)
 -			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
 -		task_rq_unlock(rq, p, &rf);
 -
 -		/*
 -		 * If it changed from the expected state, bail out now.
 -		 */
 -		if (unlikely(!ncsw))
 -			break;
 -
 -		/*
 -		 * Was it really running after all now that we
 -		 * checked with the proper locks actually held?
 -		 *
 -		 * Oops. Go back and try again..
 -		 */
 -		if (unlikely(running)) {
 -			cpu_relax();
 -			continue;
 -		}
 -
 -		/*
 -		 * It's not enough that it's not actively running,
 -		 * it must be off the runqueue _entirely_, and not
 -		 * preempted!
 -		 *
 -		 * So if it was still runnable (but just not actively
 -		 * running right now), it's preempted, and we should
 -		 * yield - it could be a while.
 -		 */
 -		if (unlikely(queued)) {
 -			ktime_t to = NSEC_PER_SEC / HZ;
 -
 -			set_current_state(TASK_UNINTERRUPTIBLE);
 -			schedule_hrtimeout(&to, HRTIMER_MODE_REL_HARD);
 -			continue;
 -		}
 -
 -		/*
 -		 * Ahh, all good. It wasn't running, and it wasn't
 -		 * runnable, which means that it will never become
 -		 * running in the future either. We're all done!
 -		 */
 -		break;
 -	}
 -
 -	return ncsw;
 -}
 -
 -/***
 - * kick_process - kick a running thread to enter/exit the kernel
 - * @p: the to-be-kicked thread
 + * The basic program-order guarantee on SMP systems is that when a task [t]
 + * migrates, all its activity on its old CPU [c0] happens-before any subsequent
 + * execution on its new CPU [c1].
   *
 - * Cause a process which is running on another CPU to enter
 - * kernel-mode, without any delay. (to get signals handled.)
 + * For migration (of runnable tasks) this is provided by the following means:
   *
 - * NOTE: this function doesn't have to take the runqueue lock,
 - * because all it wants to ensure is that the remote task enters
 - * the kernel. If the IPI races and the task has been migrated
 - * to another CPU then no harm is done and the purpose has been
 - * achieved as well.
 + *  A) UNLOCK of the rq(c0)->lock scheduling out task t
 + *  B) migration for t is required to synchronize *both* rq(c0)->lock and
 + *     rq(c1)->lock (if not at the same time, then in that order).
 + *  C) LOCK of the rq(c1)->lock scheduling in task
 + *
 + * Release/acquire chaining guarantees that B happens after A and C after B.
 + * Note: the CPU doing B need not be c0 or c1
 + *
 + * Example:
 + *
 + *   CPU0            CPU1            CPU2
 + *
 + *   LOCK rq(0)->lock
 + *   sched-out X
 + *   sched-in Y
 + *   UNLOCK rq(0)->lock
 + *
 + *                                   LOCK rq(0)->lock // orders against CPU0
 + *                                   dequeue X
 + *                                   UNLOCK rq(0)->lock
 + *
 + *                                   LOCK rq(1)->lock
 + *                                   enqueue X
 + *                                   UNLOCK rq(1)->lock
 + *
 + *                   LOCK rq(1)->lock // orders against CPU2
 + *                   sched-out Z
 + *                   sched-in X
 + *                   UNLOCK rq(1)->lock
 + *
 + *
 + *  BLOCKING -- aka. SLEEP + WAKEUP
 + *
 + * For blocking we (obviously) need to provide the same guarantee as for
 + * migration. However the means are completely different as there is no lock
 + * chain to provide order. Instead we do:
 + *
 + *   1) smp_store_release(X->on_cpu, 0)   -- finish_task()
 + *   2) smp_cond_load_acquire(!X->on_cpu) -- try_to_wake_up()
 + *
 + * Example:
 + *
 + *   CPU0 (schedule)  CPU1 (try_to_wake_up) CPU2 (schedule)
 + *
 + *   LOCK rq(0)->lock LOCK X->pi_lock
 + *   dequeue X
 + *   sched-out X
 + *   smp_store_release(X->on_cpu, 0);
 + *
 + *                    smp_cond_load_acquire(&X->on_cpu, !VAL);
 + *                    X->state = WAKING
 + *                    set_task_cpu(X,2)
 + *
 + *                    LOCK rq(2)->lock
 + *                    enqueue X
 + *                    X->state = RUNNING
 + *                    UNLOCK rq(2)->lock
 + *
 + *                                          LOCK rq(2)->lock // orders against CPU1
 + *                                          sched-out Z
 + *                                          sched-in X
 + *                                          UNLOCK rq(2)->lock
 + *
 + *                    UNLOCK X->pi_lock
 + *   UNLOCK rq(0)->lock
 + *
 + *
 + * However, for wakeups there is a second guarantee we must provide, namely we
 + * must ensure that CONDITION=1 done by the caller can not be reordered with
 + * accesses to the task state; see try_to_wake_up() and set_current_state().
   */
 -void kick_process(struct task_struct *p)
 -{
 -	int cpu;
 -
 -	preempt_disable();
 -	cpu = task_cpu(p);
 -	if ((cpu != smp_processor_id()) && task_curr(p))
 -		smp_send_reschedule(cpu);
 -	preempt_enable();
 -}
 -EXPORT_SYMBOL_GPL(kick_process);
  
 -/*
 - * ->cpus_ptr is protected by both rq->lock and p->pi_lock
 +/**
 + * try_to_wake_up - wake up a thread
 + * @p: the thread to be awakened
 + * @state: the mask of task states that can be woken
 + * @wake_flags: wake modifier flags (WF_*)
   *
 - * A few notes on cpu_active vs cpu_online:
 + * Conceptually does:
   *
 - *  - cpu_active must be a subset of cpu_online
 + *   If (@state & @p->state) @p->state = TASK_RUNNING.
   *
 - *  - on CPU-up we allow per-CPU kthreads on the online && !active CPU,
 - *    see __set_cpus_allowed_ptr(). At this point the newly online
 - *    CPU isn't yet part of the sched domains, and balancing will not
 - *    see it.
 + * If the task was not queued/runnable, also place it back on a runqueue.
   *
 - *  - on CPU-down we clear cpu_active() to mask the sched domains and
 - *    avoid the load balancer to place new tasks on the to be removed
 - *    CPU. Existing tasks will remain running there and will be taken
 - *    off.
 + * This function is atomic against schedule() which would dequeue the task.
   *
 - * This means that fallback selection must not select !active CPUs.
 - * And can assume that any active CPU must be online. Conversely
 - * select_task_rq() below may allow selection of !active CPUs in order
 - * to satisfy the above rules.
 + * It issues a full memory barrier before accessing @p->state, see the comment
 + * with set_current_state().
 + *
 + * Uses p->pi_lock to serialize against concurrent wake-ups.
 + *
 + * Relies on p->pi_lock stabilizing:
 + *  - p->sched_class
 + *  - p->cpus_ptr
 + *  - p->sched_task_group
 + * in order to do migration, see its use of select_task_rq()/set_task_cpu().
 + *
 + * Tries really hard to only take one task_rq(p)->lock for performance.
 + * Takes rq->lock in:
 + *  - ttwu_runnable()    -- old rq, unavoidable, see comment there;
 + *  - ttwu_queue()       -- new rq, for enqueue of the task;
 + *  - psi_ttwu_dequeue() -- much sadness :-( accounting will kill us.
 + *
 + * As a consequence we race really badly with just about everything. See the
 + * many memory barriers and their comments for details.
 + *
 + * Return: %true if @p->state changes (an actual wakeup was done),
 + *	   %false otherwise.
   */
 -static int select_fallback_rq(int cpu, struct task_struct *p)
 +static int
 +try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
  {
 -	int nid = cpu_to_node(cpu);
 -	const struct cpumask *nodemask = NULL;
 -	enum { cpuset, possible, fail } state = cpuset;
 -	int dest_cpu;
 +	unsigned long flags;
 +	int cpu, success = 0;
  
 -	/*
 -	 * If the node that the CPU is on has been offlined, cpu_to_node()
 -	 * will return -1. There is no CPU on the node, and we should
 -	 * select the CPU on the other node.
 -	 */
 -	if (nid != -1) {
 -		nodemask = cpumask_of_node(nid);
 +	preempt_disable();
 +	if (p == current) {
 +		/*
 +		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)
 +		 * == smp_processor_id()'. Together this means we can special
 +		 * case the whole 'p->on_rq && ttwu_runnable()' case below
 +		 * without taking any locks.
 +		 *
 +		 * In particular:
 +		 *  - we rely on Program-Order guarantees for all the ordering,
 +		 *  - we're serialized against set_special_state() by virtue of
 +		 *    it disabling IRQs (this allows not taking ->pi_lock).
 +		 */
 +		if (!ttwu_state_match(p, state, &success))
 +			goto out;
  
 -		/* Look for allowed, online CPU in same node. */
 -		for_each_cpu(dest_cpu, nodemask) {
 -			if (is_cpu_allowed(p, dest_cpu))
 -				return dest_cpu;
 -		}
 +		trace_sched_waking(p);
- 		WRITE_ONCE(p->__state, TASK_RUNNING);
- 		trace_sched_wakeup(p);
++		ttwu_do_wakeup(p);
 +		goto out;
  	}
  
 -	for (;;) {
 -		/* Any allowed, online CPU? */
 -		for_each_cpu(dest_cpu, p->cpus_ptr) {
 -			if (!is_cpu_allowed(p, dest_cpu))
 -				continue;
 +	/*
 +	 * If we are going to wake up a thread waiting for CONDITION we
 +	 * need to ensure that CONDITION=1 done by the caller can not be
 +	 * reordered with p->state check below. This pairs with smp_store_mb()
 +	 * in set_current_state() that the waiting thread does.
 +	 */
 +	raw_spin_lock_irqsave(&p->pi_lock, flags);
 +	smp_mb__after_spinlock();
 +	if (!ttwu_state_match(p, state, &success))
 +		goto unlock;
  
 -			goto out;
 -		}
 -
 -		/* No more Mr. Nice Guy. */
 -		switch (state) {
 -		case cpuset:
 -			if (cpuset_cpus_allowed_fallback(p)) {
 -				state = possible;
 -				break;
 -			}
 -			fallthrough;
 -		case possible:
 -			/*
 -			 * XXX When called from select_task_rq() we only
 -			 * hold p->pi_lock and again violate locking order.
 -			 *
 -			 * More yuck to audit.
 -			 */
 -			do_set_cpus_allowed(p, task_cpu_possible_mask(p));
 -			state = fail;
 -			break;
 -		case fail:
 -			BUG();
 -			break;
 -		}
 -	}
 -
 -out:
 -	if (state != cpuset) {
 -		/*
 -		 * Don't tell them about moving exiting tasks or
 -		 * kernel threads (both mm NULL), since they never
 -		 * leave kernel.
 -		 */
 -		if (p->mm && printk_ratelimit()) {
 -			printk_deferred("process %d (%s) no longer affine to cpu%d\n",
 -					task_pid_nr(p), p->comm, cpu);
 -		}
 -	}
 -
 -	return dest_cpu;
 -}
 -
 -/*
 - * The caller (fork, wakeup) owns p->pi_lock, ->cpus_ptr is stable.
 - */
 -static inline
 -int select_task_rq(struct task_struct *p, int cpu, int wake_flags)
 -{
 -	lockdep_assert_held(&p->pi_lock);
 -
 -	if (p->nr_cpus_allowed > 1 && !is_migration_disabled(p))
 -		cpu = p->sched_class->select_task_rq(p, cpu, wake_flags);
 -	else
 -		cpu = cpumask_any(p->cpus_ptr);
 +	trace_sched_waking(p);
  
  	/*
 -	 * In order not to call set_task_cpu() on a blocking task we need
 -	 * to rely on ttwu() to place the task on a valid ->cpus_ptr
 -	 * CPU.
 +	 * Ensure we load p->on_rq _after_ p->state, otherwise it would
 +	 * be possible to, falsely, observe p->on_rq == 0 and get stuck
 +	 * in smp_cond_load_acquire() below.
  	 *
 -	 * Since this is common to all placement strategies, this lives here.
 +	 * sched_ttwu_pending()			try_to_wake_up()
 +	 *   STORE p->on_rq = 1			  LOAD p->state
 +	 *   UNLOCK rq->lock
  	 *
 -	 * [ this allows ->select_task() to simply return task_cpu(p) and
 -	 *   not worry about this generic constraint ]
 +	 * __schedule() (switch to task 'p')
 +	 *   LOCK rq->lock			  smp_rmb();
 +	 *   smp_mb__after_spinlock();
 +	 *   UNLOCK rq->lock
 +	 *
 +	 * [task p]
 +	 *   STORE p->state = UNINTERRUPTIBLE	  LOAD p->on_rq
 +	 *
 +	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in
 +	 * __schedule().  See the comment for smp_mb__after_spinlock().
 +	 *
 +	 * A similar smb_rmb() lives in try_invoke_on_locked_down_task().
  	 */
 -	if (unlikely(!is_cpu_allowed(p, cpu)))
 -		cpu = select_fallback_rq(task_cpu(p), p);
 -
 -	return cpu;
 -}
 -
 -void sched_set_stop_task(int cpu, struct task_struct *stop)
 -{
 -	static struct lock_class_key stop_pi_lock;
 -	struct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };
 -	struct task_struct *old_stop = cpu_rq(cpu)->stop;
 -
 -	if (stop) {
 -		/*
 -		 * Make it appear like a SCHED_FIFO task, its something
 -		 * userspace knows about and won't get confused about.
 -		 *
 -		 * Also, it will make PI more or less work without too
 -		 * much confusion -- but then, stop work should not
 -		 * rely on PI working anyway.
 -		 */
 -		sched_setscheduler_nocheck(stop, SCHED_FIFO, &param);
 -
 -		stop->sched_class = &stop_sched_class;
 -
 -		/*
 -		 * The PI code calls rt_mutex_setprio() with ->pi_lock held to
 -		 * adjust the effective priority of a task. As a result,
 -		 * rt_mutex_setprio() can trigger (RT) balancing operations,
 -		 * which can then trigger wakeups of the stop thread to push
 -		 * around the current task.
 -		 *
 -		 * The stop task itself will never be part of the PI-chain, it
 -		 * never blocks, therefore that ->pi_lock recursion is safe.
 -		 * Tell lockdep about this by placing the stop->pi_lock in its
 -		 * own class.
 -		 */
 -		lockdep_set_class(&stop->pi_lock, &stop_pi_lock);
 -	}
 -
 -	cpu_rq(cpu)->stop = stop;
 -
 -	if (old_stop) {
 -		/*
 -		 * Reset it back to a normal scheduling class so that
 -		 * it can die in pieces.
 -		 */
 -		old_stop->sched_class = &rt_sched_class;
 -	}
 -}
 -
 -#else /* CONFIG_SMP */
 -
 -static inline int __set_cpus_allowed_ptr(struct task_struct *p,
 -					 struct affinity_context *ctx)
 -{
 -	return set_cpus_allowed_ptr(p, ctx->new_mask);
 -}
 -
 -static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }
 -
 -static inline bool rq_has_pinned_tasks(struct rq *rq)
 -{
 -	return false;
 -}
 -
 -#endif /* !CONFIG_SMP */
 -
 -static void
 -ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
 -{
 -	struct rq *rq;
 -
 -	if (!schedstat_enabled())
 -		return;
 -
 -	rq = this_rq();
 -
 -#ifdef CONFIG_SMP
 -	if (cpu == rq->cpu) {
 -		__schedstat_inc(rq->ttwu_local);
 -		__schedstat_inc(p->stats.nr_wakeups_local);
 -	} else {
 -		struct sched_domain *sd;
 -
 -		__schedstat_inc(p->stats.nr_wakeups_remote);
 -		rcu_read_lock();
 -		for_each_domain(rq->cpu, sd) {
 -			if (cpumask_test_cpu(cpu, sched_domain_span(sd))) {
 -				__schedstat_inc(sd->ttwu_wake_remote);
 -				break;
 -			}
 -		}
 -		rcu_read_unlock();
 -	}
 -
 -	if (wake_flags & WF_MIGRATED)
 -		__schedstat_inc(p->stats.nr_wakeups_migrate);
 -#endif /* CONFIG_SMP */
 -
 -	__schedstat_inc(rq->ttwu_count);
 -	__schedstat_inc(p->stats.nr_wakeups);
 -
 -	if (wake_flags & WF_SYNC)
 -		__schedstat_inc(p->stats.nr_wakeups_sync);
 -}
 -
 -/*
 - * Mark the task runnable.
 - */
 -static inline void ttwu_do_wakeup(struct task_struct *p)
 -{
 -	WRITE_ONCE(p->__state, TASK_RUNNING);
 -	trace_sched_wakeup(p);
 -}
 -
 -static void
 -ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,
 -		 struct rq_flags *rf)
 -{
 -	int en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK;
 -
 -	lockdep_assert_rq_held(rq);
 -
 -	if (p->sched_contributes_to_load)
 -		rq->nr_uninterruptible--;
 -
 -#ifdef CONFIG_SMP
 -	if (wake_flags & WF_MIGRATED)
 -		en_flags |= ENQUEUE_MIGRATED;
 -	else
 -#endif
 -	if (p->in_iowait) {
 -		delayacct_blkio_end(p);
 -		atomic_dec(&task_rq(p)->nr_iowait);
 -	}
 -
 -	activate_task(rq, p, en_flags);
 -	check_preempt_curr(rq, p, wake_flags);
 -
 -	ttwu_do_wakeup(p);
 +	smp_rmb();
 +	if (READ_ONCE(p->on_rq) && ttwu_runnable(p, wake_flags))
 +		goto unlock;
  
  #ifdef CONFIG_SMP
 -	if (p->sched_class->task_woken) {
 -		/*
 -		 * Our task @p is fully woken up and running; so it's safe to
 -		 * drop the rq->lock, hereafter rq is only used for statistics.
 -		 */
 -		rq_unpin_lock(rq, rf);
 -		p->sched_class->task_woken(rq, p);
 -		rq_repin_lock(rq, rf);
 -	}
 -
 -	if (rq->idle_stamp) {
 -		u64 delta = rq_clock(rq) - rq->idle_stamp;
 -		u64 max = 2*rq->max_idle_balance_cost;
 -
 -		update_avg(&rq->avg_idle, delta);
 -
 -		if (rq->avg_idle > max)
 -			rq->avg_idle = max;
 -
 -		rq->wake_stamp = jiffies;
 -		rq->wake_avg_idle = rq->avg_idle / 2;
 -
 -		rq->idle_stamp = 0;
 -	}
 -#endif
 -}
 -
 -/*
 - * Consider @p being inside a wait loop:
 - *
 - *   for (;;) {
 - *      set_current_state(TASK_UNINTERRUPTIBLE);
 - *
 - *      if (CONDITION)
 - *         break;
 - *
 - *      schedule();
 - *   }
 - *   __set_current_state(TASK_RUNNING);
 - *
 - * between set_current_state() and schedule(). In this case @p is still
 - * runnable, so all that needs doing is change p->state back to TASK_RUNNING in
 - * an atomic manner.
 - *
 - * By taking task_rq(p)->lock we serialize against schedule(), if @p->on_rq
 - * then schedule() must still happen and p->state can be changed to
 - * TASK_RUNNING. Otherwise we lost the race, schedule() has happened, and we
 - * need to do a full wakeup with enqueue.
 - *
 - * Returns: %true when the wakeup is done,
 - *          %false otherwise.
 - */
 -static int ttwu_runnable(struct task_struct *p, int wake_flags)
 -{
 -	struct rq_flags rf;
 -	struct rq *rq;
 -	int ret = 0;
 -
 -	rq = __task_rq_lock(p, &rf);
 -	if (task_on_rq_queued(p)) {
 -		if (!task_on_cpu(rq, p)) {
 -			/*
 -			 * When on_rq && !on_cpu the task is preempted, see if
 -			 * it should preempt the task that is current now.
 -			 */
 -			update_rq_clock(rq);
 -			check_preempt_curr(rq, p, wake_flags);
 -		}
 -		ttwu_do_wakeup(p);
 -		ret = 1;
 -	}
 -	__task_rq_unlock(rq, &rf);
 -
 -	return ret;
 -}
 -
 -#ifdef CONFIG_SMP
 -void sched_ttwu_pending(void *arg)
 -{
 -	struct llist_node *llist = arg;
 -	struct rq *rq = this_rq();
 -	struct task_struct *p, *t;
 -	struct rq_flags rf;
 -
 -	if (!llist)
 -		return;
 -
 -	rq_lock_irqsave(rq, &rf);
 -	update_rq_clock(rq);
 -
 -	llist_for_each_entry_safe(p, t, llist, wake_entry.llist) {
 -		if (WARN_ON_ONCE(p->on_cpu))
 -			smp_cond_load_acquire(&p->on_cpu, !VAL);
 -
 -		if (WARN_ON_ONCE(task_cpu(p) != cpu_of(rq)))
 -			set_task_cpu(p, cpu_of(rq));
 -
 -		ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
 -	}
 -
  	/*
 -	 * Must be after enqueueing at least once task such that
 -	 * idle_cpu() does not observe a false-negative -- if it does,
 -	 * it is possible for select_idle_siblings() to stack a number
 -	 * of tasks on this CPU during that window.
 +	 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be
 +	 * possible to, falsely, observe p->on_cpu == 0.
  	 *
 -	 * It is ok to clear ttwu_pending when another task pending.
 -	 * We will receive IPI after local irq enabled and then enqueue it.
 -	 * Since now nr_running > 0, idle_cpu() will always get correct result.
 -	 */
 -	WRITE_ONCE(rq->ttwu_pending, 0);
 -	rq_unlock_irqrestore(rq, &rf);
 -}
 -
 -void send_call_function_single_ipi(int cpu)
 -{
 -	struct rq *rq = cpu_rq(cpu);
 -
 -	if (!set_nr_if_polling(rq->idle))
 -		arch_send_call_function_single_ipi(cpu);
 -	else
 -		trace_sched_wake_idle_without_ipi(cpu);
 -}
 -
 -/*
 - * Queue a task on the target CPUs wake_list and wake the CPU via IPI if
 - * necessary. The wakee CPU on receipt of the IPI will queue the task
 - * via sched_ttwu_wakeup() for activation so the wakee incurs the cost
 - * of the wakeup instead of the waker.
 - */
 -static void __ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
 -{
 -	struct rq *rq = cpu_rq(cpu);
 -
 -	p->sched_remote_wakeup = !!(wake_flags & WF_MIGRATED);
 -
 -	WRITE_ONCE(rq->ttwu_pending, 1);
 -	__smp_call_single_queue(cpu, &p->wake_entry.llist);
 -}
 -
 -void wake_up_if_idle(int cpu)
 -{
 -	struct rq *rq = cpu_rq(cpu);
 -	struct rq_flags rf;
 -
 -	rcu_read_lock();
 -
 -	if (!is_idle_task(rcu_dereference(rq->curr)))
 -		goto out;
 -
 -	rq_lock_irqsave(rq, &rf);
 -	if (is_idle_task(rq->curr))
 -		resched_curr(rq);
 -	/* Else CPU is not idle, do nothing here: */
 -	rq_unlock_irqrestore(rq, &rf);
 -
 -out:
 -	rcu_read_unlock();
 -}
 -
 -bool cpus_share_cache(int this_cpu, int that_cpu)
 -{
 -	if (this_cpu == that_cpu)
 -		return true;
 -
 -	return per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);
 -}
 -
 -static inline bool ttwu_queue_cond(struct task_struct *p, int cpu)
 -{
 -	/*
 -	 * Do not complicate things with the async wake_list while the CPU is
 -	 * in hotplug state.
 -	 */
 -	if (!cpu_active(cpu))
 -		return false;
 -
 -	/* Ensure the task will still be allowed to run on the CPU. */
 -	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 -		return false;
 -
 -	/*
 -	 * If the CPU does not share cache, then queue the task on the
 -	 * remote rqs wakelist to avoid accessing remote data.
 -	 */
 -	if (!cpus_share_cache(smp_processor_id(), cpu))
 -		return true;
 -
 -	if (cpu == smp_processor_id())
 -		return false;
 -
 -	/*
 -	 * If the wakee cpu is idle, or the task is descheduling and the
 -	 * only running task on the CPU, then use the wakelist to offload
 -	 * the task activation to the idle (or soon-to-be-idle) CPU as
 -	 * the current CPU is likely busy. nr_running is checked to
 -	 * avoid unnecessary task stacking.
 +	 * One must be running (->on_cpu == 1) in order to remove oneself
 +	 * from the runqueue.
  	 *
 -	 * Note that we can only get here with (wakee) p->on_rq=0,
 -	 * p->on_cpu can be whatever, we've done the dequeue, so
 -	 * the wakee has been accounted out of ->nr_running.
 -	 */
 -	if (!cpu_rq(cpu)->nr_running)
 -		return true;
 -
 -	return false;
 -}
 -
 -static bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
 -{
 -	if (sched_feat(TTWU_QUEUE) && ttwu_queue_cond(p, cpu)) {
 -		sched_clock_cpu(cpu); /* Sync clocks across CPUs */
 -		__ttwu_queue_wakelist(p, cpu, wake_flags);
 -		return true;
 -	}
 -
 -	return false;
 -}
 -
 -#else /* !CONFIG_SMP */
 -
 -static inline bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
 -{
 -	return false;
 -}
 -
 -#endif /* CONFIG_SMP */
 -
 -static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags)
 -{
 -	struct rq *rq = cpu_rq(cpu);
 -	struct rq_flags rf;
 -
 -	if (ttwu_queue_wakelist(p, cpu, wake_flags))
 -		return;
 -
 -	rq_lock(rq, &rf);
 -	update_rq_clock(rq);
 -	ttwu_do_activate(rq, p, wake_flags, &rf);
 -	rq_unlock(rq, &rf);
 -}
 -
 -/*
 - * Invoked from try_to_wake_up() to check whether the task can be woken up.
 - *
 - * The caller holds p::pi_lock if p != current or has preemption
 - * disabled when p == current.
 - *
 - * The rules of PREEMPT_RT saved_state:
 - *
 - *   The related locking code always holds p::pi_lock when updating
 - *   p::saved_state, which means the code is fully serialized in both cases.
 - *
 - *   The lock wait and lock wakeups happen via TASK_RTLOCK_WAIT. No other
 - *   bits set. This allows to distinguish all wakeup scenarios.
 - */
 -static __always_inline
 -bool ttwu_state_match(struct task_struct *p, unsigned int state, int *success)
 -{
 -	if (IS_ENABLED(CONFIG_DEBUG_PREEMPT)) {
 -		WARN_ON_ONCE((state & TASK_RTLOCK_WAIT) &&
 -			     state != TASK_RTLOCK_WAIT);
 -	}
 -
 -	if (READ_ONCE(p->__state) & state) {
 -		*success = 1;
 -		return true;
 -	}
 -
 -#ifdef CONFIG_PREEMPT_RT
 -	/*
 -	 * Saved state preserves the task state across blocking on
 -	 * an RT lock.  If the state matches, set p::saved_state to
 -	 * TASK_RUNNING, but do not wake the task because it waits
 -	 * for a lock wakeup. Also indicate success because from
 -	 * the regular waker's point of view this has succeeded.
 -	 *
 -	 * After acquiring the lock the task will restore p::__state
 -	 * from p::saved_state which ensures that the regular
 -	 * wakeup is not lost. The restore will also set
 -	 * p::saved_state to TASK_RUNNING so any further tests will
 -	 * not result in false positives vs. @success
 -	 */
 -	if (p->saved_state & state) {
 -		p->saved_state = TASK_RUNNING;
 -		*success = 1;
 -	}
 -#endif
 -	return false;
 -}
 -
 -/*
 - * Notes on Program-Order guarantees on SMP systems.
 - *
 - *  MIGRATION
 - *
 - * The basic program-order guarantee on SMP systems is that when a task [t]
 - * migrates, all its activity on its old CPU [c0] happens-before any subsequent
 - * execution on its new CPU [c1].
 - *
 - * For migration (of runnable tasks) this is provided by the following means:
 - *
 - *  A) UNLOCK of the rq(c0)->lock scheduling out task t
 - *  B) migration for t is required to synchronize *both* rq(c0)->lock and
 - *     rq(c1)->lock (if not at the same time, then in that order).
 - *  C) LOCK of the rq(c1)->lock scheduling in task
 - *
 - * Release/acquire chaining guarantees that B happens after A and C after B.
 - * Note: the CPU doing B need not be c0 or c1
 - *
 - * Example:
 - *
 - *   CPU0            CPU1            CPU2
 - *
 - *   LOCK rq(0)->lock
 - *   sched-out X
 - *   sched-in Y
 - *   UNLOCK rq(0)->lock
 - *
 - *                                   LOCK rq(0)->lock // orders against CPU0
 - *                                   dequeue X
 - *                                   UNLOCK rq(0)->lock
 - *
 - *                                   LOCK rq(1)->lock
 - *                                   enqueue X
 - *                                   UNLOCK rq(1)->lock
 - *
 - *                   LOCK rq(1)->lock // orders against CPU2
 - *                   sched-out Z
 - *                   sched-in X
 - *                   UNLOCK rq(1)->lock
 - *
 - *
 - *  BLOCKING -- aka. SLEEP + WAKEUP
 - *
 - * For blocking we (obviously) need to provide the same guarantee as for
 - * migration. However the means are completely different as there is no lock
 - * chain to provide order. Instead we do:
 - *
 - *   1) smp_store_release(X->on_cpu, 0)   -- finish_task()
 - *   2) smp_cond_load_acquire(!X->on_cpu) -- try_to_wake_up()
 - *
 - * Example:
 - *
 - *   CPU0 (schedule)  CPU1 (try_to_wake_up) CPU2 (schedule)
 - *
 - *   LOCK rq(0)->lock LOCK X->pi_lock
 - *   dequeue X
 - *   sched-out X
 - *   smp_store_release(X->on_cpu, 0);
 - *
 - *                    smp_cond_load_acquire(&X->on_cpu, !VAL);
 - *                    X->state = WAKING
 - *                    set_task_cpu(X,2)
 - *
 - *                    LOCK rq(2)->lock
 - *                    enqueue X
 - *                    X->state = RUNNING
 - *                    UNLOCK rq(2)->lock
 - *
 - *                                          LOCK rq(2)->lock // orders against CPU1
 - *                                          sched-out Z
 - *                                          sched-in X
 - *                                          UNLOCK rq(2)->lock
 - *
 - *                    UNLOCK X->pi_lock
 - *   UNLOCK rq(0)->lock
 - *
 - *
 - * However, for wakeups there is a second guarantee we must provide, namely we
 - * must ensure that CONDITION=1 done by the caller can not be reordered with
 - * accesses to the task state; see try_to_wake_up() and set_current_state().
 - */
 -
 -/**
 - * try_to_wake_up - wake up a thread
 - * @p: the thread to be awakened
 - * @state: the mask of task states that can be woken
 - * @wake_flags: wake modifier flags (WF_*)
 - *
 - * Conceptually does:
 - *
 - *   If (@state & @p->state) @p->state = TASK_RUNNING.
 - *
 - * If the task was not queued/runnable, also place it back on a runqueue.
 - *
 - * This function is atomic against schedule() which would dequeue the task.
 - *
 - * It issues a full memory barrier before accessing @p->state, see the comment
 - * with set_current_state().
 - *
 - * Uses p->pi_lock to serialize against concurrent wake-ups.
 - *
 - * Relies on p->pi_lock stabilizing:
 - *  - p->sched_class
 - *  - p->cpus_ptr
 - *  - p->sched_task_group
 - * in order to do migration, see its use of select_task_rq()/set_task_cpu().
 - *
 - * Tries really hard to only take one task_rq(p)->lock for performance.
 - * Takes rq->lock in:
 - *  - ttwu_runnable()    -- old rq, unavoidable, see comment there;
 - *  - ttwu_queue()       -- new rq, for enqueue of the task;
 - *  - psi_ttwu_dequeue() -- much sadness :-( accounting will kill us.
 - *
 - * As a consequence we race really badly with just about everything. See the
 - * many memory barriers and their comments for details.
 - *
 - * Return: %true if @p->state changes (an actual wakeup was done),
 - *	   %false otherwise.
 - */
 -static int
 -try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 -{
 -	unsigned long flags;
 -	int cpu, success = 0;
 -
 -	preempt_disable();
 -	if (p == current) {
 -		/*
 -		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p)
 -		 * == smp_processor_id()'. Together this means we can special
 -		 * case the whole 'p->on_rq && ttwu_runnable()' case below
 -		 * without taking any locks.
 -		 *
 -		 * In particular:
 -		 *  - we rely on Program-Order guarantees for all the ordering,
 -		 *  - we're serialized against set_special_state() by virtue of
 -		 *    it disabling IRQs (this allows not taking ->pi_lock).
 -		 */
 -		if (!ttwu_state_match(p, state, &success))
 -			goto out;
 -
 -		trace_sched_waking(p);
 -		ttwu_do_wakeup(p);
 -		goto out;
 -	}
 -
 -	/*
 -	 * If we are going to wake up a thread waiting for CONDITION we
 -	 * need to ensure that CONDITION=1 done by the caller can not be
 -	 * reordered with p->state check below. This pairs with smp_store_mb()
 -	 * in set_current_state() that the waiting thread does.
 -	 */
 -	raw_spin_lock_irqsave(&p->pi_lock, flags);
 -	smp_mb__after_spinlock();
 -	if (!ttwu_state_match(p, state, &success))
 -		goto unlock;
 -
 -	trace_sched_waking(p);
 -
 -	/*
 -	 * Ensure we load p->on_rq _after_ p->state, otherwise it would
 -	 * be possible to, falsely, observe p->on_rq == 0 and get stuck
 -	 * in smp_cond_load_acquire() below.
 -	 *
 -	 * sched_ttwu_pending()			try_to_wake_up()
 -	 *   STORE p->on_rq = 1			  LOAD p->state
 -	 *   UNLOCK rq->lock
 -	 *
 -	 * __schedule() (switch to task 'p')
 -	 *   LOCK rq->lock			  smp_rmb();
 -	 *   smp_mb__after_spinlock();
 -	 *   UNLOCK rq->lock
 -	 *
 -	 * [task p]
 -	 *   STORE p->state = UNINTERRUPTIBLE	  LOAD p->on_rq
 -	 *
 -	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in
 -	 * __schedule().  See the comment for smp_mb__after_spinlock().
 -	 *
 -	 * A similar smb_rmb() lives in try_invoke_on_locked_down_task().
 -	 */
 -	smp_rmb();
 -	if (READ_ONCE(p->on_rq) && ttwu_runnable(p, wake_flags))
 -		goto unlock;
 -
 -#ifdef CONFIG_SMP
 -	/*
 -	 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be
 -	 * possible to, falsely, observe p->on_cpu == 0.
 -	 *
 -	 * One must be running (->on_cpu == 1) in order to remove oneself
 -	 * from the runqueue.
 -	 *
 -	 * __schedule() (switch to task 'p')	try_to_wake_up()
 -	 *   STORE p->on_cpu = 1		  LOAD p->on_rq
 -	 *   UNLOCK rq->lock
 +	 * __schedule() (switch to task 'p')	try_to_wake_up()
 +	 *   STORE p->on_cpu = 1		  LOAD p->on_rq
 +	 *   UNLOCK rq->lock
  	 *
  	 * __schedule() (put 'p' to sleep)
  	 *   LOCK rq->lock			  smp_rmb();
* Unmerged path kernel/sched/core.c
