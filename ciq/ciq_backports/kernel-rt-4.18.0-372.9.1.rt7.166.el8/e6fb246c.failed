RDMA/mlx5: Consolidate MR destruction to mlx5_ib_dereg_mr()

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-372.9.1.rt7.166.el8
commit-author Jason Gunthorpe <jgg@nvidia.com>
commit e6fb246ccafbdfc86e0750af021628132fdbceac
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-372.9.1.rt7.166.el8/e6fb246c.failed

Now that the SRCU stuff has been removed the entire MR destroy logic can
be made a lot simpler. Currently there are many different ways to destroy a
MR and it makes it really hard to do this task correctly. Route all
destruction through mlx5_ib_dereg_mr() and make it work for all
situations.

Since it turns out all the different MR types do basically the same thing
this removes a lot of knowledge of MR internals from ODP and leaves ODP
just exporting an operation to clean up children.

This fixes a few weird corner cases bugs and firmly uses the correct
ordering of the MR destruction:
 - Stop parallel access to the mkey via the ODP xarray
 - Stop DMA
 - Release the umem
 - Clean up ODP children
 - Free/Recycle the MR

Link: https://lore.kernel.org/r/20210304120745.1090751-4-leon@kernel.org
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit e6fb246ccafbdfc86e0750af021628132fdbceac)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/core/umem_dmabuf.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index ba5066be432c,03fe99deea14..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -1273,7 -1285,7 +1273,11 @@@ struct mlx5_ib_mr *mlx5_ib_alloc_implic
  					     struct ib_udata *udata,
  					     int access_flags);
  void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *mr);
++<<<<<<< HEAD
 +void mlx5_ib_fence_odp_mr(struct mlx5_ib_mr *mr);
++=======
+ void mlx5_ib_free_odp_mr(struct mlx5_ib_mr *mr);
++>>>>>>> e6fb246ccafb (RDMA/mlx5: Consolidate MR destruction to mlx5_ib_dereg_mr())
  struct ib_mr *mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
  				    u64 length, u64 virt_addr, int access_flags,
  				    struct ib_pd *pd, struct ib_udata *udata);
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 4344f79c242a,6304ba54a42d..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -1476,26 -1451,59 +1454,74 @@@ struct ib_mr *mlx5_ib_reg_user_mr(struc
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
  	struct mlx5_ib_mr *mr = NULL;
  	bool xlt_with_umr;
 +	struct ib_umem *umem;
  	int err;
  
++<<<<<<< HEAD
 +	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))
++=======
+ 	xlt_with_umr = mlx5_ib_can_load_pas_with_umr(dev, umem->length);
+ 	if (xlt_with_umr) {
+ 		mr = alloc_cacheable_mr(pd, umem, iova, access_flags);
+ 	} else {
+ 		unsigned int page_size = mlx5_umem_find_best_pgsz(
+ 			umem, mkc, log_page_size, 0, iova);
+ 
+ 		mutex_lock(&dev->slow_path_mutex);
+ 		mr = reg_create(pd, umem, iova, access_flags, page_size, true);
+ 		mutex_unlock(&dev->slow_path_mutex);
+ 	}
+ 	if (IS_ERR(mr)) {
+ 		ib_umem_release(umem);
+ 		return ERR_CAST(mr);
+ 	}
+ 
+ 	mlx5_ib_dbg(dev, "mkey 0x%x\n", mr->mmkey.key);
+ 
+ 	atomic_add(ib_umem_num_pages(umem), &dev->mdev->priv.reg_pages);
+ 
+ 	if (xlt_with_umr) {
+ 		/*
+ 		 * If the MR was created with reg_create then it will be
+ 		 * configured properly but left disabled. It is safe to go ahead
+ 		 * and configure it again via UMR while enabling it.
+ 		 */
+ 		err = mlx5_ib_update_mr_pas(mr, MLX5_IB_UPD_XLT_ENABLE);
+ 		if (err) {
+ 			mlx5_ib_dereg_mr(&mr->ibmr, NULL);
+ 			return ERR_PTR(err);
+ 		}
+ 	}
+ 	return &mr->ibmr;
+ }
+ 
+ static struct ib_mr *create_user_odp_mr(struct ib_pd *pd, u64 start, u64 length,
+ 					u64 iova, int access_flags,
+ 					struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct ib_umem_odp *odp;
+ 	struct mlx5_ib_mr *mr;
+ 	int err;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
++>>>>>>> e6fb246ccafb (RDMA/mlx5: Consolidate MR destruction to mlx5_ib_dereg_mr())
  		return ERR_PTR(-EOPNOTSUPP);
  
 -	if (!start && length == U64_MAX) {
 -		if (iova != 0)
 +	mlx5_ib_dbg(dev, "start 0x%llx, virt_addr 0x%llx, length 0x%llx, access_flags 0x%x\n",
 +		    start, virt_addr, length, access_flags);
 +
 +	xlt_with_umr = mlx5_ib_can_load_pas_with_umr(dev, length);
 +	/* ODP requires xlt update via umr to work. */
 +	if (!xlt_with_umr && (access_flags & IB_ACCESS_ON_DEMAND))
 +		return ERR_PTR(-EINVAL);
 +
 +	if (IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) && !start &&
 +	    length == U64_MAX) {
 +		if (virt_addr != start)
  			return ERR_PTR(-EINVAL);
 -		if (!(dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
 +		if (!(access_flags & IB_ACCESS_ON_DEMAND) ||
 +		    !(dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
  			return ERR_PTR(-EINVAL);
  
  		mr = mlx5_ib_alloc_implicit_mr(to_mpd(pd), udata, access_flags);
@@@ -1504,70 -1512,130 +1530,126 @@@
  		return &mr->ibmr;
  	}
  
++<<<<<<< HEAD
 +	umem = mr_umem_get(dev, udata, start, length, access_flags);
++=======
+ 	/* ODP requires xlt update via umr to work. */
+ 	if (!mlx5_ib_can_load_pas_with_umr(dev, length))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	odp = ib_umem_odp_get(&dev->ib_dev, start, length, access_flags,
+ 			      &mlx5_mn_ops);
+ 	if (IS_ERR(odp))
+ 		return ERR_CAST(odp);
+ 
+ 	mr = alloc_cacheable_mr(pd, &odp->umem, iova, access_flags);
+ 	if (IS_ERR(mr)) {
+ 		ib_umem_release(&odp->umem);
+ 		return ERR_CAST(mr);
+ 	}
+ 
+ 	odp->private = mr;
+ 	err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 
+ 	err = mlx5_ib_init_odp_mr(mr);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 	return &mr->ibmr;
+ 
+ err_dereg_mr:
+ 	mlx5_ib_dereg_mr(&mr->ibmr, NULL);
+ 	return ERR_PTR(err);
+ }
+ 
+ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
+ 				  u64 iova, int access_flags,
+ 				  struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct ib_umem *umem;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	mlx5_ib_dbg(dev, "start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\n",
+ 		    start, iova, length, access_flags);
+ 
+ 	if (access_flags & IB_ACCESS_ON_DEMAND)
+ 		return create_user_odp_mr(pd, start, length, iova, access_flags,
+ 					  udata);
+ 	umem = ib_umem_get(&dev->ib_dev, start, length, access_flags);
++>>>>>>> e6fb246ccafb (RDMA/mlx5: Consolidate MR destruction to mlx5_ib_dereg_mr())
  	if (IS_ERR(umem))
  		return ERR_CAST(umem);
 -	return create_real_mr(pd, umem, iova, access_flags);
 -}
  
 -static void mlx5_ib_dmabuf_invalidate_cb(struct dma_buf_attachment *attach)
 -{
 -	struct ib_umem_dmabuf *umem_dmabuf = attach->importer_priv;
 -	struct mlx5_ib_mr *mr = umem_dmabuf->private;
 -
 -	dma_resv_assert_held(umem_dmabuf->attach->dmabuf->resv);
 -
 -	if (!umem_dmabuf->sgt)
 -		return;
 +	if (xlt_with_umr) {
 +		mr = alloc_mr_from_cache(pd, umem, virt_addr, access_flags);
 +		if (IS_ERR(mr))
 +			mr = NULL;
 +	}
  
 -	mlx5_ib_update_mr_pas(mr, MLX5_IB_UPD_XLT_ZAP);
 -	ib_umem_dmabuf_unmap_pages(umem_dmabuf);
 -}
 +	if (!mr) {
 +		mutex_lock(&dev->slow_path_mutex);
 +		mr = reg_create(NULL, pd, umem, virt_addr, access_flags,
 +				!xlt_with_umr);
 +		mutex_unlock(&dev->slow_path_mutex);
 +	}
  
 -static struct dma_buf_attach_ops mlx5_ib_dmabuf_attach_ops = {
 -	.allow_peer2peer = 1,
 -	.move_notify = mlx5_ib_dmabuf_invalidate_cb,
 -};
 +	if (IS_ERR(mr)) {
 +		err = PTR_ERR(mr);
 +		goto error;
 +	}
  
 -struct ib_mr *mlx5_ib_reg_user_mr_dmabuf(struct ib_pd *pd, u64 offset,
 -					 u64 length, u64 virt_addr,
 -					 int fd, int access_flags,
 -					 struct ib_udata *udata)
 -{
 -	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 -	struct mlx5_ib_mr *mr = NULL;
 -	struct ib_umem_dmabuf *umem_dmabuf;
 -	int err;
 +	mlx5_ib_dbg(dev, "mkey 0x%x\n", mr->mmkey.key);
  
 -	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM) ||
 -	    !IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
 -		return ERR_PTR(-EOPNOTSUPP);
 +	mr->umem = umem;
 +	atomic_add(ib_umem_num_pages(mr->umem), &dev->mdev->priv.reg_pages);
 +	set_mr_fields(dev, mr, length, access_flags);
  
 -	mlx5_ib_dbg(dev,
 -		    "offset 0x%llx, virt_addr 0x%llx, length 0x%llx, fd %d, access_flags 0x%x\n",
 -		    offset, virt_addr, length, fd, access_flags);
 +	if (xlt_with_umr) {
 +		/*
 +		 * If the MR was created with reg_create then it will be
 +		 * configured properly but left disabled. It is safe to go ahead
 +		 * and configure it again via UMR while enabling it.
 +		 */
 +		int update_xlt_flags = MLX5_IB_UPD_XLT_ENABLE;
  
 -	/* dmabuf requires xlt update via umr to work. */
 -	if (!mlx5_ib_can_load_pas_with_umr(dev, length))
 -		return ERR_PTR(-EINVAL);
 +		if (access_flags & IB_ACCESS_ON_DEMAND)
 +			update_xlt_flags |= MLX5_IB_UPD_XLT_ZAP;
  
 -	umem_dmabuf = ib_umem_dmabuf_get(&dev->ib_dev, offset, length, fd,
 -					 access_flags,
 -					 &mlx5_ib_dmabuf_attach_ops);
 -	if (IS_ERR(umem_dmabuf)) {
 -		mlx5_ib_dbg(dev, "umem_dmabuf get failed (%ld)\n",
 -			    PTR_ERR(umem_dmabuf));
 -		return ERR_CAST(umem_dmabuf);
 +		if (access_flags & IB_ACCESS_ON_DEMAND)
 +			err = mlx5_ib_update_xlt(
 +						 mr, 0,
 +						 ib_umem_num_dma_blocks(umem, 1UL << mr->page_shift),
 +						 mr->page_shift, update_xlt_flags);
 +		else
 +			err = mlx5_ib_update_mr_pas(mr, update_xlt_flags);
 +		if (err) {
 +			dereg_mr(dev, mr);
 +			return ERR_PTR(err);
 +		}
  	}
  
 -	mr = alloc_cacheable_mr(pd, &umem_dmabuf->umem, virt_addr,
 -				access_flags);
 -	if (IS_ERR(mr)) {
 -		ib_umem_release(&umem_dmabuf->umem);
 -		return ERR_CAST(mr);
 +	if (is_odp_mr(mr)) {
 +		to_ib_umem_odp(mr->umem)->private = mr;
 +		err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
 +		if (err) {
 +			dereg_mr(dev, mr);
 +			return ERR_PTR(err);
 +		}
  	}
  
 -	mlx5_ib_dbg(dev, "mkey 0x%x\n", mr->mmkey.key);
 -
 -	atomic_add(ib_umem_num_pages(mr->umem), &dev->mdev->priv.reg_pages);
 -	umem_dmabuf->private = mr;
 -	err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
 -	if (err)
 -		goto err_dereg_mr;
 -
 -	err = mlx5_ib_init_dmabuf_mr(mr);
 -	if (err)
 -		goto err_dereg_mr;
  	return &mr->ibmr;
++<<<<<<< HEAD
 +error:
 +	ib_umem_release(umem);
++=======
+ 
+ err_dereg_mr:
+ 	mlx5_ib_dereg_mr(&mr->ibmr, NULL);
++>>>>>>> e6fb246ccafb (RDMA/mlx5: Consolidate MR destruction to mlx5_ib_dereg_mr())
  	return ERR_PTR(err);
  }
  
@@@ -1791,51 -1963,38 +1901,50 @@@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr
  		mr->sig = NULL;
  	}
  
+ 	/* Stop DMA */
+ 	if (mr->cache_ent) {
+ 		if (mlx5_mr_cache_invalidate(mr)) {
+ 			spin_lock_irq(&mr->cache_ent->lock);
+ 			mr->cache_ent->total_mrs--;
+ 			spin_unlock_irq(&mr->cache_ent->lock);
+ 			mr->cache_ent = NULL;
+ 		}
+ 	}
  	if (!mr->cache_ent) {
- 		destroy_mkey(dev, mr);
- 		mlx5_free_priv_descs(mr);
+ 		rc = destroy_mkey(to_mdev(mr->ibmr.device), mr);
+ 		if (rc)
+ 			return rc;
  	}
- }
  
- static void dereg_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
- {
- 	struct ib_umem *umem = mr->umem;
+ 	if (mr->umem) {
+ 		bool is_odp = is_odp_mr(mr);
  
++<<<<<<< HEAD
 +	/* Stop all DMA */
 +	if (is_odp_mr(mr))
 +		mlx5_ib_fence_odp_mr(mr);
 +	else
 +		clean_mr(dev, mr);
 +
 +	if (umem) {
 +		if (!is_odp_mr(mr))
 +			atomic_sub(ib_umem_num_pages(umem),
++=======
+ 		if (!is_odp)
+ 			atomic_sub(ib_umem_num_pages(mr->umem),
++>>>>>>> e6fb246ccafb (RDMA/mlx5: Consolidate MR destruction to mlx5_ib_dereg_mr())
  				   &dev->mdev->priv.reg_pages);
- 		ib_umem_release(umem);
+ 		ib_umem_release(mr->umem);
+ 		if (is_odp)
+ 			mlx5_ib_free_odp_mr(mr);
  	}
  
- 	if (mr->cache_ent)
+ 	if (mr->cache_ent) {
  		mlx5_mr_cache_free(dev, mr);
- 	else
+ 	} else {
+ 		mlx5_free_priv_descs(mr);
  		kfree(mr);
- }
- 
- int mlx5_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
- {
- 	struct mlx5_ib_mr *mmr = to_mmr(ibmr);
- 
- 	if (ibmr->type == IB_MR_TYPE_INTEGRITY) {
- 		dereg_mr(to_mdev(mmr->mtt_mr->ibmr.device), mmr->mtt_mr);
- 		dereg_mr(to_mdev(mmr->klm_mr->ibmr.device), mmr->klm_mr);
- 	}
- 
- 	if (is_odp_mr(mmr) && to_ib_umem_odp(mmr->umem)->is_implicit_odp) {
- 		mlx5_ib_free_implicit_mr(mmr);
- 		return 0;
  	}
- 
- 	dereg_mr(to_mdev(ibmr->device), mmr);
- 
  	return 0;
  }
  
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 4701fc81d8a6,3008d1539ad4..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -554,55 -536,14 +518,59 @@@ void mlx5_ib_free_odp_mr(struct mlx5_ib
  	struct mlx5_ib_mr *mtt;
  	unsigned long idx;
  
- 	xa_erase(&dev->odp_mkeys, mlx5_base_mkey(imr->mmkey.key));
  	/*
- 	 * All work on the prefetch list must be completed, xa_erase() prevented
- 	 * new work from being created.
+ 	 * If this is an implicit MR it is already invalidated so we can just
+ 	 * delete the children mkeys.
  	 */
++<<<<<<< HEAD
 +	mlx5r_deref_wait_odp_mkey(&imr->mmkey);
 +	/*
 +	 * At this point it is forbidden for any other thread to enter
 +	 * pagefault_mr() on this imr. It is already forbidden to call
 +	 * pagefault_mr() on an implicit child. Due to this additions to
 +	 * implicit_children are prevented.
 +	 * In addition, any new call to destroy_unused_implicit_child_mr()
 +	 * may return immediately.
 +	 */
 +
 +	/*
 +	 * Fence the imr before we destroy the children. This allows us to
 +	 * skip updating the XLT of the imr during destroy of the child mkey
 +	 * the imr points to.
 +	 */
 +	mlx5_mr_cache_invalidate(imr);
 +
 +	xa_for_each(&imr->implicit_children, idx, mtt) {
 +		xa_erase(&imr->implicit_children, idx);
 +		free_implicit_child_mr(mtt, false);
 +	}
 +
 +	mlx5_mr_cache_free(dev, imr);
 +	ib_umem_odp_release(odp_imr);
 +}
 +
 +/**
 + * mlx5_ib_fence_odp_mr - Stop all access to the ODP MR
 + * @mr: to fence
 + *
 + * On return no parallel threads will be touching this MR and no DMA will be
 + * active.
 + */
 +void mlx5_ib_fence_odp_mr(struct mlx5_ib_mr *mr)
 +{
 +	/* Prevent new page faults and prefetch requests from succeeding */
 +	xa_erase(&mr_to_mdev(mr)->odp_mkeys, mlx5_base_mkey(mr->mmkey.key));
 +
 +	/* Wait for all running page-fault handlers to finish. */
 +	mlx5r_deref_wait_odp_mkey(&mr->mmkey);
 +
 +	dma_fence_odp_mr(mr);
++=======
+ 	xa_for_each(&mr->implicit_children, idx, mtt) {
+ 		xa_erase(&mr->implicit_children, idx);
+ 		mlx5_ib_dereg_mr(&mtt->ibmr, NULL);
+ 	}
++>>>>>>> e6fb246ccafb (RDMA/mlx5: Consolidate MR destruction to mlx5_ib_dereg_mr())
  }
  
  #define MLX5_PF_FLAGS_DOWNGRADE BIT(1)
* Unmerged path drivers/infiniband/core/umem_dmabuf.c
* Unmerged path drivers/infiniband/core/umem_dmabuf.c
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
