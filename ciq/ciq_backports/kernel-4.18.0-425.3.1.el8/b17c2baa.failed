x86: Prepare inline-asm for straight-line-speculation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit b17c2baa305cccbd16bafa289fd743cc2db77966
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/b17c2baa.failed

Replace all ret/retq instructions with ASM_RET in preparation of
making it more than a single instruction.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lore.kernel.org/r/20211204134907.964635458@infradead.org
(cherry picked from commit b17c2baa305cccbd16bafa289fd743cc2db77966)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/alternative.c
#	arch/x86/kernel/kprobes/core.c
#	arch/x86/kernel/paravirt.c
#	samples/ftrace/ftrace-direct-modify.c
#	samples/ftrace/ftrace-direct-too.c
#	samples/ftrace/ftrace-direct.c
diff --cc arch/x86/kernel/alternative.c
index 0b65a0cb501d,175cde66a1ae..000000000000
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@@ -612,11 -691,98 +612,97 @@@ extern struct paravirt_patch_site __sta
  	__stop_parainstructions[];
  #endif	/* CONFIG_PARAVIRT */
  
++<<<<<<< HEAD
++=======
+ /*
+  * Self-test for the INT3 based CALL emulation code.
+  *
+  * This exercises int3_emulate_call() to make sure INT3 pt_regs are set up
+  * properly and that there is a stack gap between the INT3 frame and the
+  * previous context. Without this gap doing a virtual PUSH on the interrupted
+  * stack would corrupt the INT3 IRET frame.
+  *
+  * See entry_{32,64}.S for more details.
+  */
+ 
+ /*
+  * We define the int3_magic() function in assembly to control the calling
+  * convention such that we can 'call' it from assembly.
+  */
+ 
+ extern void int3_magic(unsigned int *ptr); /* defined in asm */
+ 
+ asm (
+ "	.pushsection	.init.text, \"ax\", @progbits\n"
+ "	.type		int3_magic, @function\n"
+ "int3_magic:\n"
+ "	movl	$1, (%" _ASM_ARG1 ")\n"
+ 	ASM_RET
+ "	.size		int3_magic, .-int3_magic\n"
+ "	.popsection\n"
+ );
+ 
+ extern __initdata unsigned long int3_selftest_ip; /* defined in asm below */
+ 
+ static int __init
+ int3_exception_notify(struct notifier_block *self, unsigned long val, void *data)
+ {
+ 	struct die_args *args = data;
+ 	struct pt_regs *regs = args->regs;
+ 
+ 	if (!regs || user_mode(regs))
+ 		return NOTIFY_DONE;
+ 
+ 	if (val != DIE_INT3)
+ 		return NOTIFY_DONE;
+ 
+ 	if (regs->ip - INT3_INSN_SIZE != int3_selftest_ip)
+ 		return NOTIFY_DONE;
+ 
+ 	int3_emulate_call(regs, (unsigned long)&int3_magic);
+ 	return NOTIFY_STOP;
+ }
+ 
+ static void __init int3_selftest(void)
+ {
+ 	static __initdata struct notifier_block int3_exception_nb = {
+ 		.notifier_call	= int3_exception_notify,
+ 		.priority	= INT_MAX-1, /* last */
+ 	};
+ 	unsigned int val = 0;
+ 
+ 	BUG_ON(register_die_notifier(&int3_exception_nb));
+ 
+ 	/*
+ 	 * Basically: int3_magic(&val); but really complicated :-)
+ 	 *
+ 	 * Stick the address of the INT3 instruction into int3_selftest_ip,
+ 	 * then trigger the INT3, padded with NOPs to match a CALL instruction
+ 	 * length.
+ 	 */
+ 	asm volatile ("1: int3; nop; nop; nop; nop\n\t"
+ 		      ".pushsection .init.data,\"aw\"\n\t"
+ 		      ".align " __ASM_SEL(4, 8) "\n\t"
+ 		      ".type int3_selftest_ip, @object\n\t"
+ 		      ".size int3_selftest_ip, " __ASM_SEL(4, 8) "\n\t"
+ 		      "int3_selftest_ip:\n\t"
+ 		      __ASM_SEL(.long, .quad) " 1b\n\t"
+ 		      ".popsection\n\t"
+ 		      : ASM_CALL_CONSTRAINT
+ 		      : __ASM_SEL_RAW(a, D) (&val)
+ 		      : "memory");
+ 
+ 	BUG_ON(val != 1);
+ 
+ 	unregister_die_notifier(&int3_exception_nb);
+ }
+ 
++>>>>>>> b17c2baa305c (x86: Prepare inline-asm for straight-line-speculation)
  void __init alternative_instructions(void)
  {
 -	int3_selftest();
 -
 -	/*
 -	 * The patching is not fully atomic, so try to avoid local
 -	 * interruptions that might execute the to be patched code.
 -	 * Other CPUs are not running.
 -	 */
 +	/* The patching is not fully atomic, so try to avoid local interruptions
 +	   that might execute the to be patched code.
 +	   Other CPUs are not running. */
  	stop_nmi();
  
  	/*
diff --cc arch/x86/kernel/kprobes/core.c
index 914c7d77ba21,6290712cb36d..000000000000
--- a/arch/x86/kernel/kprobes/core.c
+++ b/arch/x86/kernel/kprobes/core.c
@@@ -761,40 -1046,42 +761,45 @@@ asm
  	SAVE_REGS_STRING
  	"	movl %esp, %eax\n"
  	"	call trampoline_handler\n"
 +	/* Replace saved sp with true return address. */
 +	"	movl %eax, 15*4(%esp)\n"
  	RESTORE_REGS_STRING
 -	/* In trampoline_handler(), 'regs->flags' is copied to 'regs->sp'. */
 -	"	addl $4, %esp\n"
  	"	popfl\n"
  #endif
++<<<<<<< HEAD
 +	"	ret\n"
 +	".size kretprobe_trampoline, .-kretprobe_trampoline\n"
++=======
+ 	ASM_RET
+ 	".size __kretprobe_trampoline, .-__kretprobe_trampoline\n"
++>>>>>>> b17c2baa305c (x86: Prepare inline-asm for straight-line-speculation)
  );
 -NOKPROBE_SYMBOL(__kretprobe_trampoline);
 -/*
 - * __kretprobe_trampoline() skips updating frame pointer. The frame pointer
 - * saved in trampoline_handler() points to the real caller function's
 - * frame pointer. Thus the __kretprobe_trampoline() doesn't have a
 - * standard stack frame with CONFIG_FRAME_POINTER=y.
 - * Let's mark it non-standard function. Anyway, FP unwinder can correctly
 - * unwind without the hint.
 - */
 -STACK_FRAME_NON_STANDARD_FP(__kretprobe_trampoline);
 -
 -/* This is called from kretprobe_trampoline_handler(). */
 -void arch_kretprobe_fixup_return(struct pt_regs *regs,
 -				 kprobe_opcode_t *correct_ret_addr)
 -{
 -	unsigned long *frame_pointer = &regs->sp + 1;
 -
 -	/* Replace fake return address with real one. */
 -	*frame_pointer = (unsigned long)correct_ret_addr;
 -}
 +NOKPROBE_SYMBOL(kretprobe_trampoline);
 +STACK_FRAME_NON_STANDARD(kretprobe_trampoline);
  
  /*
 - * Called from __kretprobe_trampoline
 + * Called from kretprobe_trampoline
   */
 -__used __visible void trampoline_handler(struct pt_regs *regs)
 +__visible __used void *trampoline_handler(struct pt_regs *regs)
  {
 -	unsigned long *frame_pointer;
 +	struct kretprobe_instance *ri = NULL;
 +	struct hlist_head *head, empty_rp;
 +	struct hlist_node *tmp;
 +	unsigned long flags, orig_ret_address = 0;
 +	unsigned long trampoline_address = (unsigned long)&kretprobe_trampoline;
 +	kprobe_opcode_t *correct_ret_addr = NULL;
 +	void *frame_pointer;
 +	bool skipped = false;
  
 +	/*
 +	 * Set a dummy kprobe for avoiding kretprobe recursion.
 +	 * Since kretprobe never run in kprobe handler, kprobe must not
 +	 * be running at this point.
 +	 */
 +	kprobe_busy_begin();
 +
 +	INIT_HLIST_HEAD(&empty_rp);
 +	kretprobe_hash_lock(current, &head, &flags);
  	/* fixup registers */
  	regs->cs = __KERNEL_CS;
  #ifdef CONFIG_X86_32
diff --cc arch/x86/kernel/paravirt.c
index d251a7e52f03,4420499f7bb4..000000000000
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@@ -56,16 -46,16 +56,28 @@@ asm (".pushsection .entry.text, \"ax\"\
       ".type _paravirt_nop, @function\n\t"
       ".popsection");
  
++<<<<<<< HEAD
 +/* identity function, which can be inlined */
 +u32 notrace _paravirt_ident_32(u32 x)
 +{
 +	return x;
 +}
++=======
+ /* stub always returning 0. */
+ asm (".pushsection .entry.text, \"ax\"\n"
+      ".global paravirt_ret0\n"
+      "paravirt_ret0:\n\t"
+      "xor %" _ASM_AX ", %" _ASM_AX ";\n\t"
+      ASM_RET
+      ".size paravirt_ret0, . - paravirt_ret0\n\t"
+      ".type paravirt_ret0, @function\n\t"
+      ".popsection");
++>>>>>>> b17c2baa305c (x86: Prepare inline-asm for straight-line-speculation)
  
 +u64 notrace _paravirt_ident_64(u64 x)
 +{
 +	return x;
 +}
  
  void __init default_banner(void)
  {
* Unmerged path samples/ftrace/ftrace-direct-modify.c
* Unmerged path samples/ftrace/ftrace-direct-too.c
* Unmerged path samples/ftrace/ftrace-direct.c
diff --git a/arch/x86/include/asm/linkage.h b/arch/x86/include/asm/linkage.h
index e07188e8d763..6214e94100b2 100644
--- a/arch/x86/include/asm/linkage.h
+++ b/arch/x86/include/asm/linkage.h
@@ -26,6 +26,10 @@
 #define __ALIGN_STR	__stringify(__ALIGN)
 #endif
 
+#else /* __ASSEMBLY__ */
+
+#define ASM_RET	"ret\n\t"
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* _ASM_X86_LINKAGE_H */
diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h
index e9f118dae8cc..491bcf99875b 100644
--- a/arch/x86/include/asm/paravirt.h
+++ b/arch/x86/include/asm/paravirt.h
@@ -772,7 +772,7 @@ static __always_inline bool pv_vcpu_is_preempted(long cpu)
 	    "call " #func ";"						\
 	    PV_RESTORE_ALL_CALLER_REGS					\
 	    FRAME_END							\
-	    "ret;"							\
+	    ASM_RET							\
 	    ".size " PV_THUNK_NAME(func) ", .-" PV_THUNK_NAME(func) ";"	\
 	    ".popsection")
 
diff --git a/arch/x86/include/asm/qspinlock_paravirt.h b/arch/x86/include/asm/qspinlock_paravirt.h
index 159622ee0674..1474cf96251d 100644
--- a/arch/x86/include/asm/qspinlock_paravirt.h
+++ b/arch/x86/include/asm/qspinlock_paravirt.h
@@ -48,7 +48,7 @@ asm    (".pushsection .text;"
 	"jne   .slowpath;"
 	"pop   %rdx;"
 	FRAME_END
-	"ret;"
+	ASM_RET
 	".slowpath: "
 	"push   %rsi;"
 	"movzbl %al,%esi;"
@@ -56,7 +56,7 @@ asm    (".pushsection .text;"
 	"pop    %rsi;"
 	"pop    %rdx;"
 	FRAME_END
-	"ret;"
+	ASM_RET
 	".size " PV_UNLOCK ", .-" PV_UNLOCK ";"
 	".popsection");
 
* Unmerged path arch/x86/kernel/alternative.c
* Unmerged path arch/x86/kernel/kprobes/core.c
* Unmerged path arch/x86/kernel/paravirt.c
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 728f143c512c..42e28b07dfe7 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -317,7 +317,7 @@ static int fastop(struct x86_emulate_ctxt *ctxt, fastop_t fop);
 	__FOP_FUNC(#name)
 
 #define __FOP_RET(name) \
-	"ret \n\t" \
+	ASM_RET \
 	".size " name ", .-" name "\n\t"
 
 #define FOP_RET(name) \
@@ -452,7 +452,7 @@ static_assert(SETCC_LENGTH <= SETCC_ALIGN);
 	__FOP_RET(#op)
 
 asm(".pushsection .fixup, \"ax\"\n"
-    "kvm_fastop_exception: xor %esi, %esi; ret\n"
+    "kvm_fastop_exception: xor %esi, %esi; " ASM_RET
     ".popsection");
 
 FOP_START(setcc)
diff --git a/arch/x86/lib/error-inject.c b/arch/x86/lib/error-inject.c
index be5b5fb1598b..520897061ee0 100644
--- a/arch/x86/lib/error-inject.c
+++ b/arch/x86/lib/error-inject.c
@@ -1,5 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 
+#include <linux/linkage.h>
 #include <linux/error-injection.h>
 #include <linux/kprobes.h>
 
@@ -10,7 +11,7 @@ asm(
 	".type just_return_func, @function\n"
 	".globl just_return_func\n"
 	"just_return_func:\n"
-	"	ret\n"
+		ASM_RET
 	".size just_return_func, .-just_return_func\n"
 );
 
* Unmerged path samples/ftrace/ftrace-direct-modify.c
* Unmerged path samples/ftrace/ftrace-direct-too.c
* Unmerged path samples/ftrace/ftrace-direct.c
