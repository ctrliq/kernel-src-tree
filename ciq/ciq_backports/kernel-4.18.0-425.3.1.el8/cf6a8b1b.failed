RDMA/mlx5: Remove iova from struct mlx5_core_mkey

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit cf6a8b1b24d675afc35a01cccd081160014a0125
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/cf6a8b1b.failed

iova is already stored in ibmr->iova, no need to store it here.

	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Shay Drory <shayd@nvidia.com>
	Acked-by: Michael S. Tsirkin <mst@redhat.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
(cherry picked from commit cf6a8b1b24d675afc35a01cccd081160014a0125)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 4263bf96852f,6815500cfdaa..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -941,9 -910,31 +941,37 @@@ static struct mlx5_cache_ent *mr_cache_
  	return &cache->ent[order];
  }
  
++<<<<<<< HEAD
 +static struct mlx5_ib_mr *alloc_mr_from_cache(struct ib_pd *pd,
 +					      struct ib_umem *umem, u64 iova,
 +					      int access_flags)
++=======
+ static void set_mr_fields(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,
+ 			  u64 length, int access_flags, u64 iova)
+ {
+ 	mr->ibmr.lkey = mr->mmkey.key;
+ 	mr->ibmr.rkey = mr->mmkey.key;
+ 	mr->ibmr.length = length;
+ 	mr->ibmr.device = &dev->ib_dev;
+ 	mr->ibmr.iova = iova;
+ 	mr->access_flags = access_flags;
+ }
+ 
+ static unsigned int mlx5_umem_dmabuf_default_pgsz(struct ib_umem *umem,
+ 						  u64 iova)
+ {
+ 	/*
+ 	 * The alignment of iova has already been checked upon entering
+ 	 * UVERBS_METHOD_REG_DMABUF_MR
+ 	 */
+ 	umem->iova = iova;
+ 	return PAGE_SIZE;
+ }
+ 
+ static struct mlx5_ib_mr *alloc_cacheable_mr(struct ib_pd *pd,
+ 					     struct ib_umem *umem, u64 iova,
+ 					     int access_flags)
++>>>>>>> cf6a8b1b24d6 (RDMA/mlx5: Remove iova from struct mlx5_core_mkey)
  {
  	struct mlx5_ib_dev *dev = to_mdev(pd->device);
  	struct mlx5_cache_ent *ent;
@@@ -974,7 -978,7 +1002,10 @@@
  	mr->mmkey.size = umem->length;
  	mr->mmkey.pd = to_mpd(pd)->pdn;
  	mr->page_shift = order_base_2(page_size);
++<<<<<<< HEAD
++=======
+ 	set_mr_fields(dev, mr, umem->length, access_flags, iova);
++>>>>>>> cf6a8b1b24d6 (RDMA/mlx5: Remove iova from struct mlx5_core_mkey)
  
  	return mr;
  }
@@@ -1333,6 -1339,9 +1364,12 @@@ static struct mlx5_ib_mr *reg_create(st
  		goto err_2;
  	}
  	mr->mmkey.type = MLX5_MKEY_MR;
++<<<<<<< HEAD
++=======
+ 	mr->desc_size = sizeof(struct mlx5_mtt);
+ 	mr->umem = umem;
+ 	set_mr_fields(dev, mr, umem->length, access_flags, iova);
++>>>>>>> cf6a8b1b24d6 (RDMA/mlx5: Remove iova from struct mlx5_core_mkey)
  	kvfree(in);
  
  	mlx5_ib_dbg(dev, "mkey = 0x%x\n", mr->mmkey.key);
@@@ -1588,27 -1671,116 +1625,103 @@@ static int revoke_mr(struct mlx5_ib_mr 
  	return mlx5_ib_post_send_wait(mr_to_mdev(mr), &umrwr);
  }
  
 -/*
 - * True if the change in access flags can be done via UMR, only some access
 - * flags can be updated.
 - */
 -static bool can_use_umr_rereg_access(struct mlx5_ib_dev *dev,
 -				     unsigned int current_access_flags,
 -				     unsigned int target_access_flags)
 +static int rereg_umr(struct ib_pd *pd, struct mlx5_ib_mr *mr,
 +		     int access_flags, int flags)
  {
 -	unsigned int diffs = current_access_flags ^ target_access_flags;
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct mlx5_umr_wr umrwr = {};
 +	int err;
  
 -	if (diffs & ~(IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE |
 -		      IB_ACCESS_REMOTE_READ | IB_ACCESS_RELAXED_ORDERING))
 -		return false;
 -	return mlx5_ib_can_reconfig_with_umr(dev, current_access_flags,
 -					     target_access_flags);
 -}
 +	umrwr.wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE;
  
 -static int umr_rereg_pd_access(struct mlx5_ib_mr *mr, struct ib_pd *pd,
 -			       int access_flags)
 -{
 -	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 -	struct mlx5_umr_wr umrwr = {
 -		.wr = {
 -			.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE |
 -				      MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS,
 -			.opcode = MLX5_IB_WR_UMR,
 -		},
 -		.mkey = mr->mmkey.key,
 -		.pd = pd,
 -		.access_flags = access_flags,
 -	};
 -	int err;
 +	umrwr.wr.opcode = MLX5_IB_WR_UMR;
 +	umrwr.mkey = mr->mmkey.key;
 +
 +	if (flags & IB_MR_REREG_PD || flags & IB_MR_REREG_ACCESS) {
 +		umrwr.pd = pd;
 +		umrwr.access_flags = access_flags;
 +		umrwr.wr.send_flags |= MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
 +	}
  
  	err = mlx5_ib_post_send_wait(dev, &umrwr);
 -	if (err)
 -		return err;
  
++<<<<<<< HEAD
 +	return err;
++=======
+ 	mr->access_flags = access_flags;
+ 	mr->mmkey.pd = to_mpd(pd)->pdn;
+ 	return 0;
+ }
+ 
+ static bool can_use_umr_rereg_pas(struct mlx5_ib_mr *mr,
+ 				  struct ib_umem *new_umem,
+ 				  int new_access_flags, u64 iova,
+ 				  unsigned long *page_size)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
+ 
+ 	/* We only track the allocated sizes of MRs from the cache */
+ 	if (!mr->cache_ent)
+ 		return false;
+ 	if (!mlx5_ib_can_load_pas_with_umr(dev, new_umem->length))
+ 		return false;
+ 
+ 	*page_size =
+ 		mlx5_umem_find_best_pgsz(new_umem, mkc, log_page_size, 0, iova);
+ 	if (WARN_ON(!*page_size))
+ 		return false;
+ 	return (1ULL << mr->cache_ent->order) >=
+ 	       ib_umem_num_dma_blocks(new_umem, *page_size);
+ }
+ 
+ static int umr_rereg_pas(struct mlx5_ib_mr *mr, struct ib_pd *pd,
+ 			 int access_flags, int flags, struct ib_umem *new_umem,
+ 			 u64 iova, unsigned long page_size)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
+ 	int upd_flags = MLX5_IB_UPD_XLT_ADDR | MLX5_IB_UPD_XLT_ENABLE;
+ 	struct ib_umem *old_umem = mr->umem;
+ 	int err;
+ 
+ 	/*
+ 	 * To keep everything simple the MR is revoked before we start to mess
+ 	 * with it. This ensure the change is atomic relative to any use of the
+ 	 * MR.
+ 	 */
+ 	err = revoke_mr(mr);
+ 	if (err)
+ 		return err;
+ 
+ 	if (flags & IB_MR_REREG_PD) {
+ 		mr->ibmr.pd = pd;
+ 		mr->mmkey.pd = to_mpd(pd)->pdn;
+ 		upd_flags |= MLX5_IB_UPD_XLT_PD;
+ 	}
+ 	if (flags & IB_MR_REREG_ACCESS) {
+ 		mr->access_flags = access_flags;
+ 		upd_flags |= MLX5_IB_UPD_XLT_ACCESS;
+ 	}
+ 
+ 	mr->ibmr.length = new_umem->length;
+ 	mr->ibmr.iova = iova;
+ 	mr->mmkey.size = new_umem->length;
+ 	mr->page_shift = order_base_2(page_size);
+ 	mr->umem = new_umem;
+ 	err = mlx5_ib_update_mr_pas(mr, upd_flags);
+ 	if (err) {
+ 		/*
+ 		 * The MR is revoked at this point so there is no issue to free
+ 		 * new_umem.
+ 		 */
+ 		mr->umem = old_umem;
+ 		return err;
+ 	}
+ 
+ 	atomic_sub(ib_umem_num_pages(old_umem), &dev->mdev->priv.reg_pages);
+ 	ib_umem_release(old_umem);
+ 	atomic_add(ib_umem_num_pages(new_umem), &dev->mdev->priv.reg_pages);
+ 	return 0;
++>>>>>>> cf6a8b1b24d6 (RDMA/mlx5: Remove iova from struct mlx5_core_mkey)
  }
  
  struct ib_mr *mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
@@@ -1618,105 -1790,92 +1731,146 @@@
  {
  	struct mlx5_ib_dev *dev = to_mdev(ib_mr->device);
  	struct mlx5_ib_mr *mr = to_mmr(ib_mr);
 +	struct ib_pd *pd = (flags & IB_MR_REREG_PD) ? new_pd : ib_mr->pd;
 +	int access_flags = flags & IB_MR_REREG_ACCESS ?
 +			    new_access_flags :
 +			    mr->access_flags;
 +	int upd_flags = 0;
 +	u64 addr, len;
  	int err;
  
 -	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))
 -		return ERR_PTR(-EOPNOTSUPP);
 +	mlx5_ib_dbg(dev, "start 0x%llx, virt_addr 0x%llx, length 0x%llx, access_flags 0x%x\n",
 +		    start, virt_addr, length, access_flags);
  
 -	mlx5_ib_dbg(
 -		dev,
 -		"start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\n",
 -		start, iova, length, new_access_flags);
 +	if (!mr->umem)
 +		return ERR_PTR(-EINVAL);
  
 -	if (flags & ~(IB_MR_REREG_TRANS | IB_MR_REREG_PD | IB_MR_REREG_ACCESS))
 +	if (is_odp_mr(mr))
  		return ERR_PTR(-EOPNOTSUPP);
  
 -	if (!(flags & IB_MR_REREG_ACCESS))
 -		new_access_flags = mr->access_flags;
 -	if (!(flags & IB_MR_REREG_PD))
 -		new_pd = ib_mr->pd;
 -
 -	if (!(flags & IB_MR_REREG_TRANS)) {
 -		struct ib_umem *umem;
 -
 -		/* Fast path for PD/access change */
 -		if (can_use_umr_rereg_access(dev, mr->access_flags,
 -					     new_access_flags)) {
 -			err = umr_rereg_pd_access(mr, new_pd, new_access_flags);
 -			if (err)
 -				return ERR_PTR(err);
 -			return NULL;
 -		}
 -		/* DM or ODP MR's don't have a normal umem so we can't re-use it */
 -		if (!mr->umem || is_odp_mr(mr) || is_dmabuf_mr(mr))
 -			goto recreate;
 +	if (flags & IB_MR_REREG_TRANS) {
 +		addr = virt_addr;
 +		len = length;
 +	} else {
 +		addr = mr->umem->address;
 +		len = mr->umem->length;
 +	}
  
 +	if (flags != IB_MR_REREG_PD) {
  		/*
 -		 * Only one active MR can refer to a umem at one time, revoke
 -		 * the old MR before assigning the umem to the new one.
 +		 * Replace umem. This needs to be done whether or not UMR is
 +		 * used.
  		 */
++<<<<<<< HEAD
 +		flags |= IB_MR_REREG_TRANS;
 +		atomic_sub(ib_umem_num_pages(mr->umem),
 +			   &dev->mdev->priv.reg_pages);
 +		ib_umem_release(mr->umem);
 +		mr->umem = mr_umem_get(dev, udata, addr, len, access_flags);
 +		if (IS_ERR(mr->umem)) {
 +			err = PTR_ERR(mr->umem);
 +			mr->umem = NULL;
 +			goto err;
++=======
+ 		err = revoke_mr(mr);
+ 		if (err)
+ 			return ERR_PTR(err);
+ 		umem = mr->umem;
+ 		mr->umem = NULL;
+ 		atomic_sub(ib_umem_num_pages(umem), &dev->mdev->priv.reg_pages);
+ 
+ 		return create_real_mr(new_pd, umem, mr->ibmr.iova,
+ 				      new_access_flags);
+ 	}
+ 
+ 	/*
+ 	 * DM doesn't have a PAS list so we can't re-use it, odp/dmabuf does
+ 	 * but the logic around releasing the umem is different
+ 	 */
+ 	if (!mr->umem || is_odp_mr(mr) || is_dmabuf_mr(mr))
+ 		goto recreate;
+ 
+ 	if (!(new_access_flags & IB_ACCESS_ON_DEMAND) &&
+ 	    can_use_umr_rereg_access(dev, mr->access_flags, new_access_flags)) {
+ 		struct ib_umem *new_umem;
+ 		unsigned long page_size;
+ 
+ 		new_umem = ib_umem_get(&dev->ib_dev, start, length,
+ 				       new_access_flags);
+ 		if (IS_ERR(new_umem))
+ 			return ERR_CAST(new_umem);
+ 
+ 		/* Fast path for PAS change */
+ 		if (can_use_umr_rereg_pas(mr, new_umem, new_access_flags, iova,
+ 					  &page_size)) {
+ 			err = umr_rereg_pas(mr, new_pd, new_access_flags, flags,
+ 					    new_umem, iova, page_size);
+ 			if (err) {
+ 				ib_umem_release(new_umem);
+ 				return ERR_PTR(err);
+ 			}
+ 			return NULL;
++>>>>>>> cf6a8b1b24d6 (RDMA/mlx5: Remove iova from struct mlx5_core_mkey)
  		}
 -		return create_real_mr(new_pd, new_umem, iova, new_access_flags);
 +		atomic_add(ib_umem_num_pages(mr->umem),
 +			   &dev->mdev->priv.reg_pages);
  	}
  
 -	/*
 -	 * Everything else has no state we can preserve, just create a new MR
 -	 * from scratch
 -	 */
 -recreate:
 -	return mlx5_ib_reg_user_mr(new_pd, start, length, iova,
 -				   new_access_flags, udata);
 +	if (!mlx5_ib_can_reconfig_with_umr(dev, mr->access_flags,
 +					   access_flags) ||
 +	    !mlx5_ib_can_load_pas_with_umr(dev, len) ||
 +	    (flags & IB_MR_REREG_TRANS &&
 +	     !mlx5_ib_pas_fits_in_mr(mr, addr, len))) {
 +		/*
 +		 * UMR can't be used - MKey needs to be replaced.
 +		 */
 +		if (mr->cache_ent)
 +			detach_mr_from_cache(mr);
 +		err = destroy_mkey(dev, mr);
 +		if (err)
 +			goto err;
 +
 +		mr = reg_create(ib_mr, pd, mr->umem, addr, access_flags, true);
 +		if (IS_ERR(mr)) {
 +			err = PTR_ERR(mr);
 +			mr = to_mmr(ib_mr);
 +			goto err;
 +		}
 +	} else {
 +		/*
 +		 * Send a UMR WQE
 +		 */
 +		mr->ibmr.pd = pd;
 +		mr->access_flags = access_flags;
 +		mr->mmkey.iova = addr;
 +		mr->mmkey.size = len;
 +		mr->mmkey.pd = to_mpd(pd)->pdn;
 +
 +		if (flags & IB_MR_REREG_TRANS) {
 +			upd_flags = MLX5_IB_UPD_XLT_ADDR;
 +			if (flags & IB_MR_REREG_PD)
 +				upd_flags |= MLX5_IB_UPD_XLT_PD;
 +			if (flags & IB_MR_REREG_ACCESS)
 +				upd_flags |= MLX5_IB_UPD_XLT_ACCESS;
 +			err = mlx5_ib_update_mr_pas(mr, upd_flags);
 +		} else {
 +			err = rereg_umr(pd, mr, access_flags, flags);
 +		}
 +
 +		if (err)
 +			goto err;
 +	}
 +
 +	set_mr_fields(dev, mr, len, access_flags);
 +
 +	return NULL;
 +
 +err:
 +	ib_umem_release(mr->umem);
 +	mr->umem = NULL;
 +
 +	mlx5_ib_dereg_mr(&mr->ibmr, NULL);
 +	return ERR_PTR(err);
  }
  
  static int
diff --git a/drivers/infiniband/hw/mlx5/devx.c b/drivers/infiniband/hw/mlx5/devx.c
index abfde896b9aa..a960d497e1ce 100644
--- a/drivers/infiniband/hw/mlx5/devx.c
+++ b/drivers/infiniband/hw/mlx5/devx.c
@@ -1303,7 +1303,6 @@ static int devx_handle_mkey_indirect(struct devx_obj *obj,
 	mkey->key = mlx5_idx_to_mkey(
 			MLX5_GET(create_mkey_out, out, mkey_index)) | key;
 	mkey->type = MLX5_MKEY_INDIRECT_DEVX;
-	mkey->iova = MLX5_GET64(mkc, mkc, start_addr);
 	mkey->size = MLX5_GET64(mkc, mkc, len);
 	mkey->pd = MLX5_GET(mkc, mkc, pd);
 	devx_mr->ndescs = MLX5_GET(mkc, mkc, translations_octword_size);
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
diff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c
index 677d9275aa78..dd9e8cf085e0 100644
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@ -415,7 +415,7 @@ static struct mlx5_ib_mr *implicit_get_child_mr(struct mlx5_ib_mr *imr,
 	mr->umem = &odp->umem;
 	mr->ibmr.lkey = mr->mmkey.key;
 	mr->ibmr.rkey = mr->mmkey.key;
-	mr->mmkey.iova = idx * MLX5_IMR_MTT_SIZE;
+	mr->ibmr.iova = idx * MLX5_IMR_MTT_SIZE;
 	mr->parent = imr;
 	odp->private = mr;
 
@@ -482,7 +482,7 @@ struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
 	}
 
 	imr->ibmr.pd = &pd->ibpd;
-	imr->mmkey.iova = 0;
+	imr->ibmr.iova = 0;
 	imr->umem = &umem_odp->umem;
 	imr->ibmr.lkey = imr->mmkey.key;
 	imr->ibmr.rkey = imr->mmkey.key;
@@ -704,13 +704,13 @@ static int pagefault_mr(struct mlx5_ib_mr *mr, u64 io_virt, size_t bcnt,
 {
 	struct ib_umem_odp *odp = to_ib_umem_odp(mr->umem);
 
-	if (unlikely(io_virt < mr->mmkey.iova))
+	if (unlikely(io_virt < mr->ibmr.iova))
 		return -EFAULT;
 
 	if (!odp->is_implicit_odp) {
 		u64 user_va;
 
-		if (check_add_overflow(io_virt - mr->mmkey.iova,
+		if (check_add_overflow(io_virt - mr->ibmr.iova,
 				       (u64)odp->umem.address, &user_va))
 			return -EFAULT;
 		if (unlikely(user_va >= ib_umem_end(odp) ||
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/mr.c b/drivers/net/ethernet/mellanox/mlx5/core/mr.c
index 174f71ed5280..d239d559994f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/mr.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mr.c
@@ -52,7 +52,6 @@ int mlx5_core_create_mkey(struct mlx5_core_dev *dev,
 
 	mkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);
 	mkey_index = MLX5_GET(create_mkey_out, lout, mkey_index);
-	mkey->iova = MLX5_GET64(mkc, mkc, start_addr);
 	mkey->size = MLX5_GET64(mkc, mkc, len);
 	mkey->key = (u32)mlx5_mkey_variant(mkey->key) | mlx5_idx_to_mkey(mkey_index);
 	mkey->pd = MLX5_GET(mkc, mkc, pd);
diff --git a/drivers/vdpa/mlx5/core/resources.c b/drivers/vdpa/mlx5/core/resources.c
index 15e266d0e27a..14d4314cdc29 100644
--- a/drivers/vdpa/mlx5/core/resources.c
+++ b/drivers/vdpa/mlx5/core/resources.c
@@ -215,7 +215,6 @@ int mlx5_vdpa_create_mkey(struct mlx5_vdpa_dev *mvdev, struct mlx5_core_mkey *mk
 
 	mkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);
 	mkey_index = MLX5_GET(create_mkey_out, lout, mkey_index);
-	mkey->iova = MLX5_GET64(mkc, mkc, start_addr);
 	mkey->size = MLX5_GET64(mkc, mkc, len);
 	mkey->key |= mlx5_idx_to_mkey(mkey_index);
 	mkey->pd = MLX5_GET(mkc, mkc, pd);
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index 010ada427bed..edbfa5dd9884 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -363,7 +363,6 @@ enum {
 };
 
 struct mlx5_core_mkey {
-	u64			iova;
 	u64			size;
 	u32			key;
 	u32			pd;
