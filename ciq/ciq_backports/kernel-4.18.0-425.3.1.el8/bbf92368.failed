x86/text-patching: Make text_gen_insn() play nice with ANNOTATE_NOENDBR

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit bbf92368b0b1fe472d489e62d3340d7897e9c697
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/bbf92368.failed

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
Link: https://lore.kernel.org/r/20220308154317.638561109@infradead.org
(cherry picked from commit bbf92368b0b1fe472d489e62d3340d7897e9c697)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/text-patching.h
diff --cc arch/x86/include/asm/text-patching.h
index 47a8c6185e2b,1c4cfb1c6e4f..000000000000
--- a/arch/x86/include/asm/text-patching.h
+++ b/arch/x86/include/asm/text-patching.h
@@@ -71,7 -65,79 +71,83 @@@ static inline void int3_emulate_jmp(str
  #define JMP8_INSN_SIZE		2
  #define JMP8_INSN_OPCODE	0xEB
  
++<<<<<<< HEAD
 +static inline void int3_emulate_push(struct pt_regs *regs, unsigned long val)
++=======
+ #define DISP32_SIZE		4
+ 
+ static __always_inline int text_opcode_size(u8 opcode)
+ {
+ 	int size = 0;
+ 
+ #define __CASE(insn)	\
+ 	case insn##_INSN_OPCODE: size = insn##_INSN_SIZE; break
+ 
+ 	switch(opcode) {
+ 	__CASE(INT3);
+ 	__CASE(RET);
+ 	__CASE(CALL);
+ 	__CASE(JMP32);
+ 	__CASE(JMP8);
+ 	}
+ 
+ #undef __CASE
+ 
+ 	return size;
+ }
+ 
+ union text_poke_insn {
+ 	u8 text[POKE_MAX_OPCODE_SIZE];
+ 	struct {
+ 		u8 opcode;
+ 		s32 disp;
+ 	} __attribute__((packed));
+ };
+ 
+ static __always_inline
+ void *text_gen_insn(u8 opcode, const void *addr, const void *dest)
+ {
+ 	static union text_poke_insn insn; /* per instance */
+ 	int size = text_opcode_size(opcode);
+ 
+ 	/*
+ 	 * Hide the addresses to avoid the compiler folding in constants when
+ 	 * referencing code, these can mess up annotations like
+ 	 * ANNOTATE_NOENDBR.
+ 	 */
+ 	OPTIMIZER_HIDE_VAR(addr);
+ 	OPTIMIZER_HIDE_VAR(dest);
+ 
+ 	insn.opcode = opcode;
+ 
+ 	if (size > 1) {
+ 		insn.disp = (long)dest - (long)(addr + size);
+ 		if (size == 2) {
+ 			/*
+ 			 * Ensure that for JMP8 the displacement
+ 			 * actually fits the signed byte.
+ 			 */
+ 			BUG_ON((insn.disp >> 31) != (insn.disp >> 7));
+ 		}
+ 	}
+ 
+ 	return &insn.text;
+ }
+ 
+ extern int after_bootmem;
+ extern __ro_after_init struct mm_struct *poking_mm;
+ extern __ro_after_init unsigned long poking_addr;
+ 
+ #ifndef CONFIG_UML_X86
+ static __always_inline
+ void int3_emulate_jmp(struct pt_regs *regs, unsigned long ip)
+ {
+ 	regs->ip = ip;
+ }
+ 
+ static __always_inline
+ void int3_emulate_push(struct pt_regs *regs, unsigned long val)
++>>>>>>> bbf92368b0b1 (x86/text-patching: Make text_gen_insn() play nice with ANNOTATE_NOENDBR)
  {
  	/*
  	 * The int3 handler in entry_64.S adds a gap between the
* Unmerged path arch/x86/include/asm/text-patching.h
