net/mlx5e: Optimize modulo in mlx5e_select_queue

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Maxim Mikityanskiy <maximmi@nvidia.com>
commit 3a9e5fff2ab0d6f4af701757d35b9453dc563b78
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/3a9e5fff.failed

To improve the performance of the modulo operation (%), it's replaced by
a subtracting the divisor in a loop. The modulo is used to fix up an
out-of-bounds value that might be returned by netdev_pick_tx or to
convert the queue number to the channel number when num_tcs > 1. Both
situations are unlikely, because XPS is configured not to pick higher
queues (qid >= num_channels) by default, so under normal circumstances
the flow won't go inside the loop, and it will be faster than %.

num_tcs == 8 adds at most 7 iterations to the loop. PTP adds at most 1
iteration to the loop. HTB would add at most 256 iterations (when
num_channels == 1), so there is an additional boundary check in the HTB
flow, which falls back to % if more than 7 iterations are expected.

	Signed-off-by: Maxim Mikityanskiy <maximmi@nvidia.com>
	Reviewed-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 3a9e5fff2ab0d6f4af701757d35b9453dc563b78)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en/selq.c
#	drivers/net/ethernet/mellanox/mlx5/core/en/selq.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/selq.c
index 50ea58a3cc94,667bc95a0d44..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/selq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/selq.c
@@@ -93,3 -95,132 +93,135 @@@ void mlx5e_selq_cancel(struct mlx5e_sel
  
  	selq->is_prepared = false;
  }
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_MLX5_CORE_EN_DCB
+ static int mlx5e_get_dscp_up(struct mlx5e_priv *priv, struct sk_buff *skb)
+ {
+ 	int dscp_cp = 0;
+ 
+ 	if (skb->protocol == htons(ETH_P_IP))
+ 		dscp_cp = ipv4_get_dsfield(ip_hdr(skb)) >> 2;
+ 	else if (skb->protocol == htons(ETH_P_IPV6))
+ 		dscp_cp = ipv6_get_dsfield(ipv6_hdr(skb)) >> 2;
+ 
+ 	return priv->dcbx_dp.dscp2prio[dscp_cp];
+ }
+ #endif
+ 
+ static int mlx5e_get_up(struct mlx5e_priv *priv, struct sk_buff *skb)
+ {
+ #ifdef CONFIG_MLX5_CORE_EN_DCB
+ 	if (READ_ONCE(priv->dcbx_dp.trust_state) == MLX5_QPTS_TRUST_DSCP)
+ 		return mlx5e_get_dscp_up(priv, skb);
+ #endif
+ 	if (skb_vlan_tag_present(skb))
+ 		return skb_vlan_tag_get_prio(skb);
+ 	return 0;
+ }
+ 
+ static u16 mlx5e_select_ptpsq(struct net_device *dev, struct sk_buff *skb,
+ 			      struct mlx5e_selq_params *selq)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	int up;
+ 
+ 	up = selq->num_tcs > 1 ? mlx5e_get_up(priv, skb) : 0;
+ 
+ 	return selq->num_regular_queues + up;
+ }
+ 
+ static int mlx5e_select_htb_queue(struct mlx5e_priv *priv, struct sk_buff *skb)
+ {
+ 	u16 classid;
+ 
+ 	/* Order maj_id before defcls - pairs with mlx5e_htb_root_add. */
+ 	if ((TC_H_MAJ(skb->priority) >> 16) == smp_load_acquire(&priv->htb.maj_id))
+ 		classid = TC_H_MIN(skb->priority);
+ 	else
+ 		classid = READ_ONCE(priv->htb.defcls);
+ 
+ 	if (!classid)
+ 		return 0;
+ 
+ 	return mlx5e_get_txq_by_classid(priv, classid);
+ }
+ 
+ u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+ 		       struct net_device *sb_dev)
+ {
+ 	struct mlx5e_priv *priv = netdev_priv(dev);
+ 	struct mlx5e_selq_params *selq;
+ 	int txq_ix, up;
+ 
+ 	selq = rcu_dereference_bh(priv->selq.active);
+ 
+ 	/* This is a workaround needed only for the mlx5e_netdev_change_profile
+ 	 * flow that zeroes out the whole priv without unregistering the netdev
+ 	 * and without preventing ndo_select_queue from being called.
+ 	 */
+ 	if (unlikely(!selq))
+ 		return 0;
+ 
+ 	if (likely(!selq->is_ptp && !selq->is_htb)) {
+ 		/* No special queues, netdev_pick_tx returns one of the regular ones. */
+ 
+ 		txq_ix = netdev_pick_tx(dev, skb, NULL);
+ 
+ 		if (selq->num_tcs <= 1)
+ 			return txq_ix;
+ 
+ 		up = mlx5e_get_up(priv, skb);
+ 
+ 		/* Normalize any picked txq_ix to [0, num_channels),
+ 		 * So we can return a txq_ix that matches the channel and
+ 		 * packet UP.
+ 		 */
+ 		return mlx5e_txq_to_ch_ix(txq_ix, selq->num_channels) +
+ 			up * selq->num_channels;
+ 	}
+ 
+ 	if (unlikely(selq->is_htb)) {
+ 		/* num_tcs == 1, shortcut for PTP */
+ 
+ 		txq_ix = mlx5e_select_htb_queue(priv, skb);
+ 		if (txq_ix > 0)
+ 			return txq_ix;
+ 
+ 		if (unlikely(selq->is_ptp && mlx5e_use_ptpsq(skb)))
+ 			return selq->num_channels;
+ 
+ 		txq_ix = netdev_pick_tx(dev, skb, NULL);
+ 
+ 		/* Fix netdev_pick_tx() not to choose ptp_channel and HTB txqs.
+ 		 * If they are selected, switch to regular queues.
+ 		 * Driver to select these queues only at mlx5e_select_ptpsq()
+ 		 * and mlx5e_select_htb_queue().
+ 		 */
+ 		return mlx5e_txq_to_ch_ix_htb(txq_ix, selq->num_channels);
+ 	}
+ 
+ 	/* PTP is enabled */
+ 
+ 	if (mlx5e_use_ptpsq(skb))
+ 		return mlx5e_select_ptpsq(dev, skb, selq);
+ 
+ 	txq_ix = netdev_pick_tx(dev, skb, NULL);
+ 
+ 	/* Normalize any picked txq_ix to [0, num_channels). Queues in range
+ 	 * [0, num_regular_queues) will be mapped to the corresponding channel
+ 	 * index, so that we can apply the packet's UP (if num_tcs > 1).
+ 	 * If netdev_pick_tx() picks ptp_channel, switch to a regular queue,
+ 	 * because driver should select the PTP only at mlx5e_select_ptpsq().
+ 	 */
+ 	txq_ix = mlx5e_txq_to_ch_ix(txq_ix, selq->num_channels);
+ 
+ 	if (selq->num_tcs <= 1)
+ 		return txq_ix;
+ 
+ 	up = mlx5e_get_up(priv, skb);
+ 
+ 	return txq_ix + up * selq->num_channels;
+ }
++>>>>>>> 3a9e5fff2ab0 (net/mlx5e: Optimize modulo in mlx5e_select_queue)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/selq.h
index 2648c23e8238,6c070141d8f1..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/selq.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/selq.h
@@@ -23,4 -25,27 +23,30 @@@ void mlx5e_selq_prepare(struct mlx5e_se
  void mlx5e_selq_apply(struct mlx5e_selq *selq);
  void mlx5e_selq_cancel(struct mlx5e_selq *selq);
  
++<<<<<<< HEAD
++=======
+ static inline u16 mlx5e_txq_to_ch_ix(u16 txq, u16 num_channels)
+ {
+ 	while (unlikely(txq >= num_channels))
+ 		txq -= num_channels;
+ 	return txq;
+ }
+ 
+ static inline u16 mlx5e_txq_to_ch_ix_htb(u16 txq, u16 num_channels)
+ {
+ 	if (unlikely(txq >= num_channels)) {
+ 		if (unlikely(txq >= num_channels << 3))
+ 			txq %= num_channels;
+ 		else
+ 			do
+ 				txq -= num_channels;
+ 			while (txq >= num_channels);
+ 	}
+ 	return txq;
+ }
+ 
+ u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+ 		       struct net_device *sb_dev);
+ 
++>>>>>>> 3a9e5fff2ab0 (net/mlx5e: Optimize modulo in mlx5e_select_queue)
  #endif /* __MLX5_EN_SELQ_H__ */
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/selq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/selq.h
