net/mlx5: Introduce API for bulk request and release of IRQs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Shay Drory <shayd@nvidia.com>
commit 79b60ca83b6fa63ef307d2edcc77ee6581da8971
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/79b60ca8.failed

Currently IRQs are requested one by one. To balance spreading IRQs
among cpus using such scheme requires remembering cpu mask for the
cpus used for a given device. This complicates the IRQ allocation
scheme in subsequent patch.

Hence, prepare the code for bulk IRQs allocation. This enables
spreading IRQs among cpus in subsequent patch.

	Signed-off-by: Shay Drory <shayd@nvidia.com>
	Reviewed-by: Parav Pandit <parav@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 79b60ca83b6fa63ef307d2edcc77ee6581da8971)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index 5fbef7c95e48,14547b6f2894..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -649,8 -659,8 +645,13 @@@ static int create_async_eqs(struct mlx5
  	mlx5_cmd_allowed_opcode(dev, CMD_ALLOWED_OPCODE_ALL);
  
  	param = (struct mlx5_eq_param) {
++<<<<<<< HEAD
 +		.irq_index = MLX5_IRQ_EQ_CTRL,
 +		.nent = mlx5_devlink_res_size(dev, MLX5_DL_RES_ASYNC_EQ),
++=======
+ 		.irq = table->ctrl_irq,
+ 		.nent = async_eq_depth_devlink_param_get(dev),
++>>>>>>> 79b60ca83b6f (net/mlx5: Introduce API for bulk request and release of IRQs)
  	};
  
  	gather_async_events_mask(dev, param.mask);
@@@ -798,8 -847,24 +838,9 @@@ static void destroy_comp_eqs(struct mlx
  		tasklet_disable(&eq->tasklet_ctx.task);
  		kfree(eq);
  	}
+ 	comp_irqs_release(dev);
  }
  
 -static u16 comp_eq_depth_devlink_param_get(struct mlx5_core_dev *dev)
 -{
 -	struct devlink *devlink = priv_to_devlink(dev);
 -	union devlink_param_value val;
 -	int err;
 -
 -	err = devlink_param_driverinit_value_get(devlink,
 -						 DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE,
 -						 &val);
 -	if (!err)
 -		return val.vu32;
 -	mlx5_core_dbg(dev, "Failed to get param. using default. err = %d\n", err);
 -	return MLX5_COMP_EQ_SIZE;
 -}
 -
  static int create_comp_eqs(struct mlx5_core_dev *dev)
  {
  	struct mlx5_eq_table *table = dev->priv.eq_table;
@@@ -809,12 -874,13 +850,18 @@@
  	int err;
  	int i;
  
+ 	ncomp_eqs = comp_irqs_request(dev);
+ 	if (ncomp_eqs < 0)
+ 		return ncomp_eqs;
  	INIT_LIST_HEAD(&table->comp_eqs_list);
++<<<<<<< HEAD
 +	ncomp_eqs = table->num_comp_eqs;
 +	nent = mlx5_devlink_res_size(dev, MLX5_DL_RES_COMP_EQ);
++=======
+ 	nent = comp_eq_depth_devlink_param_get(dev);
++>>>>>>> 79b60ca83b6f (net/mlx5: Introduce API for bulk request and release of IRQs)
  	for (i = 0; i < ncomp_eqs; i++) {
  		struct mlx5_eq_param param = {};
- 		int vecidx = i;
  
  		eq = kzalloc(sizeof(*eq), GFP_KERNEL);
  		if (!eq) {
diff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c
index 677d9275aa78..04ef64d687cb 100644
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@ -1502,16 +1502,10 @@ int mlx5r_odp_create_eq(struct mlx5_ib_dev *dev, struct mlx5_ib_pf_eq *eq)
 
 	eq->irq_nb.notifier_call = mlx5_ib_eq_pf_int;
 	param = (struct mlx5_eq_param) {
-		.irq_index = MLX5_IRQ_EQ_CTRL,
 		.nent = MLX5_IB_NUM_PF_EQE,
 	};
 	param.mask[0] = 1ull << MLX5_EVENT_TYPE_PAGE_FAULT;
-	if (!zalloc_cpumask_var(&param.affinity, GFP_KERNEL)) {
-		err = -ENOMEM;
-		goto err_wq;
-	}
 	eq->core = mlx5_eq_create_generic(dev->mdev, &param);
-	free_cpumask_var(param.affinity);
 	if (IS_ERR(eq->core)) {
 		err = PTR_ERR(eq->core);
 		goto err_wq;
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
index 7028e4b43837..665b1c372d85 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
@@ -23,9 +23,12 @@ int mlx5_set_msix_vec_count(struct mlx5_core_dev *dev, int devfn,
 int mlx5_get_default_msix_vec_count(struct mlx5_core_dev *dev, int num_vfs);
 
 struct mlx5_irq *mlx5_ctrl_irq_request(struct mlx5_core_dev *dev);
+void mlx5_ctrl_irq_release(struct mlx5_irq *ctrl_irq);
 struct mlx5_irq *mlx5_irq_request(struct mlx5_core_dev *dev, u16 vecidx,
 				  struct cpumask *affinity);
-void mlx5_irq_release(struct mlx5_irq *irq);
+int mlx5_irqs_request_vectors(struct mlx5_core_dev *dev, u16 *cpus, int nirqs,
+			      struct mlx5_irq **irqs);
+void mlx5_irqs_release_vectors(struct mlx5_irq **irqs, int nirqs);
 int mlx5_irq_attach_nb(struct mlx5_irq *irq, struct notifier_block *nb);
 int mlx5_irq_detach_nb(struct mlx5_irq *irq, struct notifier_block *nb);
 struct cpumask *mlx5_irq_get_affinity_mask(struct mlx5_irq *irq);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c b/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
index 4a06fb0b287d..b9be00715c05 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
@@ -404,13 +404,27 @@ static struct mlx5_irq_pool *ctrl_irq_pool_get(struct mlx5_core_dev *dev)
 }
 
 /**
- * mlx5_irq_release - release an IRQ back to the system.
- * @irq: irq to be released.
+ * mlx5_irqs_release - release one or more IRQs back to the system.
+ * @irqs: IRQs to be released.
+ * @nirqs: number of IRQs to be released.
  */
-void mlx5_irq_release(struct mlx5_irq *irq)
+static void mlx5_irqs_release(struct mlx5_irq **irqs, int nirqs)
 {
-	synchronize_irq(irq->irqn);
-	irq_put(irq);
+	int i;
+
+	for (i = 0; i < nirqs; i++) {
+		synchronize_irq(irqs[i]->irqn);
+		irq_put(irqs[i]);
+	}
+}
+
+/**
+ * mlx5_ctrl_irq_release - release a ctrl IRQ back to the system.
+ * @ctrl_irq: ctrl IRQ to be released.
+ */
+void mlx5_ctrl_irq_release(struct mlx5_irq *ctrl_irq)
+{
+	mlx5_irqs_release(&ctrl_irq, 1);
 }
 
 /**
@@ -485,6 +499,51 @@ struct mlx5_irq *mlx5_irq_request(struct mlx5_core_dev *dev, u16 vecidx,
 	return irq;
 }
 
+/**
+ * mlx5_irqs_release_vectors - release one or more IRQs back to the system.
+ * @irqs: IRQs to be released.
+ * @nirqs: number of IRQs to be released.
+ */
+void mlx5_irqs_release_vectors(struct mlx5_irq **irqs, int nirqs)
+{
+	mlx5_irqs_release(irqs, nirqs);
+}
+
+/**
+ * mlx5_irqs_request_vectors - request one or more IRQs for mlx5 device.
+ * @dev: mlx5 device that is requesting the IRQs.
+ * @cpus: CPUs array for binding the IRQs
+ * @nirqs: number of IRQs to request.
+ * @irqs: an output array of IRQs pointers.
+ *
+ * Each IRQ is bound to at most 1 CPU.
+ * This function is requests nirqs IRQs, starting from @vecidx.
+ *
+ * This function returns the number of IRQs requested, (which might be smaller than
+ * @nirqs), if successful, or a negative error code in case of an error.
+ */
+int mlx5_irqs_request_vectors(struct mlx5_core_dev *dev, u16 *cpus, int nirqs,
+			      struct mlx5_irq **irqs)
+{
+	cpumask_var_t req_mask;
+	struct mlx5_irq *irq;
+	int i;
+
+	if (!zalloc_cpumask_var(&req_mask, GFP_KERNEL))
+		return -ENOMEM;
+	for (i = 0; i < nirqs; i++) {
+		cpumask_set_cpu(cpus[i], req_mask);
+		irq = mlx5_irq_request(dev, i, req_mask);
+		if (IS_ERR(irq))
+			break;
+		cpumask_clear(req_mask);
+		irqs[i] = irq;
+	}
+
+	free_cpumask_var(req_mask);
+	return i ? i : PTR_ERR(irq);
+}
+
 static struct mlx5_irq_pool *
 irq_pool_alloc(struct mlx5_core_dev *dev, int start, int size, char *name,
 	       u32 min_threshold, u32 max_threshold)
diff --git a/include/linux/mlx5/eq.h b/include/linux/mlx5/eq.h
index 11161e427608..1b36840e4b1f 100644
--- a/include/linux/mlx5/eq.h
+++ b/include/linux/mlx5/eq.h
@@ -8,13 +8,13 @@
 #define MLX5_NUM_SPARE_EQE (0x80)
 
 struct mlx5_eq;
+struct mlx5_irq;
 struct mlx5_core_dev;
 
 struct mlx5_eq_param {
-	u8             irq_index;
 	int            nent;
 	u64            mask[4];
-	cpumask_var_t  affinity;
+	struct mlx5_irq *irq;
 };
 
 struct mlx5_eq *
