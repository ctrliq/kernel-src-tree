drm/i915/gem: add missing else

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author katrinzhou <katrinzhou@tencent.com>
commit 7482a65664c16cc88eb84d2b545a1fed887378a1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/7482a656.failed

Add missing else in set_proto_ctx_param() to fix coverity issue.

Addresses-Coverity: ("Unused value")
Fixes: d4433c7600f7 ("drm/i915/gem: Use the proto-context to handle create parameters (v5)")
	Suggested-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
	Signed-off-by: katrinzhou <katrinzhou@tencent.com>
[tursulin: fixup alignment]
	Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Link: https://patchwork.freedesktop.org/patch/msgid/20220621124926.615884-1-tvrtko.ursulin@linux.intel.com
(cherry picked from commit 7482a65664c16cc88eb84d2b545a1fed887378a1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/gem/i915_gem_context.c
diff --cc drivers/gpu/drm/i915/gem/i915_gem_context.c
index 7720b8c22c81,dabdfe09f5e5..000000000000
--- a/drivers/gpu/drm/i915/gem/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_context.c
@@@ -167,855 -169,977 +167,1005 @@@ lookup_user_engine(struct i915_gem_cont
  	return i915_gem_context_get_engine(ctx, idx);
  }
  
 -static int validate_priority(struct drm_i915_private *i915,
 -			     const struct drm_i915_gem_context_param *args)
 +static struct i915_address_space *
 +context_get_vm_rcu(struct i915_gem_context *ctx)
  {
 -	s64 priority = args->value;
 +	GEM_BUG_ON(!rcu_access_pointer(ctx->vm));
  
 -	if (args->size)
 +	do {
 +		struct i915_address_space *vm;
 +
 +		/*
 +		 * We do not allow downgrading from full-ppgtt [to a shared
 +		 * global gtt], so ctx->vm cannot become NULL.
 +		 */
 +		vm = rcu_dereference(ctx->vm);
 +		if (!kref_get_unless_zero(&vm->ref))
 +			continue;
 +
 +		/*
 +		 * This ppgtt may have be reallocated between
 +		 * the read and the kref, and reassigned to a third
 +		 * context. In order to avoid inadvertent sharing
 +		 * of this ppgtt with that third context (and not
 +		 * src), we have to confirm that we have the same
 +		 * ppgtt after passing through the strong memory
 +		 * barrier implied by a successful
 +		 * kref_get_unless_zero().
 +		 *
 +		 * Once we have acquired the current ppgtt of ctx,
 +		 * we no longer care if it is released from ctx, as
 +		 * it cannot be reallocated elsewhere.
 +		 */
 +
 +		if (vm == rcu_access_pointer(ctx->vm))
 +			return rcu_pointer_handoff(vm);
 +
 +		i915_vm_put(vm);
 +	} while (1);
 +}
 +
 +static void intel_context_set_gem(struct intel_context *ce,
 +				  struct i915_gem_context *ctx)
 +{
++<<<<<<< HEAD
++=======
++	struct drm_i915_private *i915 = fpriv->dev_priv;
++	struct drm_i915_gem_context_param_sseu user_sseu;
++	struct intel_sseu *sseu;
++	int ret;
++
++	if (args->size < sizeof(user_sseu))
+ 		return -EINVAL;
+ 
 -	if (!(i915->caps.scheduler & I915_SCHEDULER_CAP_PRIORITY))
++	if (GRAPHICS_VER(i915) != 11)
+ 		return -ENODEV;
+ 
 -	if (priority > I915_CONTEXT_MAX_USER_PRIORITY ||
 -	    priority < I915_CONTEXT_MIN_USER_PRIORITY)
++	if (copy_from_user(&user_sseu, u64_to_user_ptr(args->value),
++			   sizeof(user_sseu)))
++		return -EFAULT;
++
++	if (user_sseu.rsvd)
+ 		return -EINVAL;
+ 
 -	if (priority > I915_CONTEXT_DEFAULT_PRIORITY &&
 -	    !capable(CAP_SYS_NICE))
 -		return -EPERM;
++	if (user_sseu.flags & ~(I915_CONTEXT_SSEU_FLAG_ENGINE_INDEX))
++		return -EINVAL;
+ 
 -	return 0;
 -}
++	if (!!(user_sseu.flags & I915_CONTEXT_SSEU_FLAG_ENGINE_INDEX) != (pc->num_user_engines >= 0))
++		return -EINVAL;
+ 
 -static void proto_context_close(struct drm_i915_private *i915,
 -				struct i915_gem_proto_context *pc)
 -{
 -	int i;
++	if (pc->num_user_engines >= 0) {
++		int idx = user_sseu.engine.engine_instance;
++		struct i915_gem_proto_engine *pe;
+ 
 -	if (pc->pxp_wakeref)
 -		intel_runtime_pm_put(&i915->runtime_pm, pc->pxp_wakeref);
 -	if (pc->vm)
 -		i915_vm_put(pc->vm);
 -	if (pc->user_engines) {
 -		for (i = 0; i < pc->num_user_engines; i++)
 -			kfree(pc->user_engines[i].siblings);
 -		kfree(pc->user_engines);
 -	}
 -	kfree(pc);
 -}
++		if (idx >= pc->num_user_engines)
++			return -EINVAL;
+ 
 -static int proto_context_set_persistence(struct drm_i915_private *i915,
 -					 struct i915_gem_proto_context *pc,
 -					 bool persist)
 -{
 -	if (persist) {
 -		/*
 -		 * Only contexts that are short-lived [that will expire or be
 -		 * reset] are allowed to survive past termination. We require
 -		 * hangcheck to ensure that the persistent requests are healthy.
 -		 */
 -		if (!i915->params.enable_hangcheck)
++		pe = &pc->user_engines[idx];
++
++		/* Only render engine supports RPCS configuration. */
++		if (pe->engine->class != RENDER_CLASS)
+ 			return -EINVAL;
+ 
 -		pc->user_flags |= BIT(UCONTEXT_PERSISTENCE);
++		sseu = &pe->sseu;
+ 	} else {
 -		/* To cancel a context we use "preempt-to-idle" */
 -		if (!(i915->caps.scheduler & I915_SCHEDULER_CAP_PREEMPTION))
 -			return -ENODEV;
++		/* Only render engine supports RPCS configuration. */
++		if (user_sseu.engine.engine_class != I915_ENGINE_CLASS_RENDER)
++			return -EINVAL;
+ 
 -		/*
 -		 * If the cancel fails, we then need to reset, cleanly!
 -		 *
 -		 * If the per-engine reset fails, all hope is lost! We resort
 -		 * to a full GPU reset in that unlikely case, but realistically
 -		 * if the engine could not reset, the full reset does not fare
 -		 * much better. The damage has been done.
 -		 *
 -		 * However, if we cannot reset an engine by itself, we cannot
 -		 * cleanup a hanging persistent context without causing
 -		 * colateral damage, and we should not pretend we can by
 -		 * exposing the interface.
 -		 */
 -		if (!intel_has_reset_engine(to_gt(i915)))
 -			return -ENODEV;
++		/* There is only one render engine */
++		if (user_sseu.engine.engine_instance != 0)
++			return -EINVAL;
+ 
 -		pc->user_flags &= ~BIT(UCONTEXT_PERSISTENCE);
++		sseu = &pc->legacy_rcs_sseu;
+ 	}
+ 
++	ret = i915_gem_user_to_context_sseu(to_gt(i915), &user_sseu, sseu);
++	if (ret)
++		return ret;
++
++	args->size = sizeof(user_sseu);
++
+ 	return 0;
+ }
+ 
 -static int proto_context_set_protected(struct drm_i915_private *i915,
 -				       struct i915_gem_proto_context *pc,
 -				       bool protected)
++static int set_proto_ctx_param(struct drm_i915_file_private *fpriv,
++			       struct i915_gem_proto_context *pc,
++			       struct drm_i915_gem_context_param *args)
+ {
+ 	int ret = 0;
+ 
 -	if (!protected) {
 -		pc->uses_protected_content = false;
 -	} else if (!intel_pxp_is_enabled(&to_gt(i915)->pxp)) {
 -		ret = -ENODEV;
 -	} else if ((pc->user_flags & BIT(UCONTEXT_RECOVERABLE)) ||
 -		   !(pc->user_flags & BIT(UCONTEXT_BANNABLE))) {
 -		ret = -EPERM;
 -	} else {
 -		pc->uses_protected_content = true;
++	switch (args->param) {
++	case I915_CONTEXT_PARAM_NO_ERROR_CAPTURE:
++		if (args->size)
++			ret = -EINVAL;
++		else if (args->value)
++			pc->user_flags |= BIT(UCONTEXT_NO_ERROR_CAPTURE);
++		else
++			pc->user_flags &= ~BIT(UCONTEXT_NO_ERROR_CAPTURE);
++		break;
+ 
 -		/*
 -		 * protected context usage requires the PXP session to be up,
 -		 * which in turn requires the device to be active.
 -		 */
 -		pc->pxp_wakeref = intel_runtime_pm_get(&i915->runtime_pm);
++	case I915_CONTEXT_PARAM_BANNABLE:
++		if (args->size)
++			ret = -EINVAL;
++		else if (!capable(CAP_SYS_ADMIN) && !args->value)
++			ret = -EPERM;
++		else if (args->value)
++			pc->user_flags |= BIT(UCONTEXT_BANNABLE);
++		else if (pc->uses_protected_content)
++			ret = -EPERM;
++		else
++			pc->user_flags &= ~BIT(UCONTEXT_BANNABLE);
++		break;
+ 
 -		if (!intel_pxp_is_active(&to_gt(i915)->pxp))
 -			ret = intel_pxp_start(&to_gt(i915)->pxp);
 -	}
++	case I915_CONTEXT_PARAM_RECOVERABLE:
++		if (args->size)
++			ret = -EINVAL;
++		else if (!args->value)
++			pc->user_flags &= ~BIT(UCONTEXT_RECOVERABLE);
++		else if (pc->uses_protected_content)
++			ret = -EPERM;
++		else
++			pc->user_flags |= BIT(UCONTEXT_RECOVERABLE);
++		break;
+ 
 -	return ret;
 -}
++	case I915_CONTEXT_PARAM_PRIORITY:
++		ret = validate_priority(fpriv->dev_priv, args);
++		if (!ret)
++			pc->sched.priority = args->value;
++		break;
+ 
 -static struct i915_gem_proto_context *
 -proto_context_create(struct drm_i915_private *i915, unsigned int flags)
 -{
 -	struct i915_gem_proto_context *pc, *err;
++	case I915_CONTEXT_PARAM_SSEU:
++		ret = set_proto_ctx_sseu(fpriv, pc, args);
++		break;
+ 
 -	pc = kzalloc(sizeof(*pc), GFP_KERNEL);
 -	if (!pc)
 -		return ERR_PTR(-ENOMEM);
++	case I915_CONTEXT_PARAM_VM:
++		ret = set_proto_ctx_vm(fpriv, pc, args);
++		break;
+ 
 -	pc->num_user_engines = -1;
 -	pc->user_engines = NULL;
 -	pc->user_flags = BIT(UCONTEXT_BANNABLE) |
 -			 BIT(UCONTEXT_RECOVERABLE);
 -	if (i915->params.enable_hangcheck)
 -		pc->user_flags |= BIT(UCONTEXT_PERSISTENCE);
 -	pc->sched.priority = I915_PRIORITY_NORMAL;
++	case I915_CONTEXT_PARAM_ENGINES:
++		ret = set_proto_ctx_engines(fpriv, pc, args);
++		break;
+ 
 -	if (flags & I915_CONTEXT_CREATE_FLAGS_SINGLE_TIMELINE) {
 -		if (!HAS_EXECLISTS(i915)) {
 -			err = ERR_PTR(-EINVAL);
 -			goto proto_close;
 -		}
 -		pc->single_timeline = true;
 -	}
++	case I915_CONTEXT_PARAM_PERSISTENCE:
++		if (args->size)
++			ret = -EINVAL;
++		else
++			ret = proto_context_set_persistence(fpriv->dev_priv, pc,
++							    args->value);
++		break;
++
++	case I915_CONTEXT_PARAM_PROTECTED_CONTENT:
++		ret = proto_context_set_protected(fpriv->dev_priv, pc,
++						  args->value);
++		break;
+ 
 -	return pc;
++	case I915_CONTEXT_PARAM_NO_ZEROMAP:
++	case I915_CONTEXT_PARAM_BAN_PERIOD:
++	case I915_CONTEXT_PARAM_RINGSIZE:
++	default:
++		ret = -EINVAL;
++		break;
++	}
+ 
 -proto_close:
 -	proto_context_close(i915, pc);
 -	return err;
++	return ret;
+ }
+ 
 -static int proto_context_register_locked(struct drm_i915_file_private *fpriv,
 -					 struct i915_gem_proto_context *pc,
 -					 u32 *id)
++static int intel_context_set_gem(struct intel_context *ce,
++				 struct i915_gem_context *ctx,
++				 struct intel_sseu sseu)
+ {
 -	int ret;
 -	void *old;
++	int ret = 0;
++
++>>>>>>> 7482a65664c1 (drm/i915/gem: add missing else)
 +	GEM_BUG_ON(rcu_access_pointer(ce->gem_context));
 +	RCU_INIT_POINTER(ce->gem_context, ctx);
  
 -	lockdep_assert_held(&fpriv->proto_context_lock);
 +	if (!test_bit(CONTEXT_ALLOC_BIT, &ce->flags))
 +		ce->ring = __intel_context_ring_size(SZ_16K);
  
 -	ret = xa_alloc(&fpriv->context_xa, id, NULL, xa_limit_32b, GFP_KERNEL);
 -	if (ret)
 -		return ret;
 +	if (rcu_access_pointer(ctx->vm)) {
 +		struct i915_address_space *vm;
 +
 +		rcu_read_lock();
 +		vm = context_get_vm_rcu(ctx); /* hmm */
 +		rcu_read_unlock();
  
 -	old = xa_store(&fpriv->proto_context_xa, *id, pc, GFP_KERNEL);
 -	if (xa_is_err(old)) {
 -		xa_erase(&fpriv->context_xa, *id);
 -		return xa_err(old);
 +		i915_vm_put(ce->vm);
 +		ce->vm = vm;
  	}
 -	WARN_ON(old);
  
 -	return 0;
 +	GEM_BUG_ON(ce->timeline);
 +	if (ctx->timeline)
 +		ce->timeline = intel_timeline_get(ctx->timeline);
 +
 +	if (ctx->sched.priority >= I915_PRIORITY_NORMAL &&
 +	    intel_engine_has_timeslices(ce->engine))
 +		__set_bit(CONTEXT_USE_SEMAPHORES, &ce->flags);
 +
 +	intel_context_set_watchdog_us(ce, ctx->watchdog.timeout_us);
  }
  
 -static int proto_context_register(struct drm_i915_file_private *fpriv,
 -				  struct i915_gem_proto_context *pc,
 -				  u32 *id)
 +static void __free_engines(struct i915_gem_engines *e, unsigned int count)
  {
 -	int ret;
 -
 -	mutex_lock(&fpriv->proto_context_lock);
 -	ret = proto_context_register_locked(fpriv, pc, id);
 -	mutex_unlock(&fpriv->proto_context_lock);
 +	while (count--) {
 +		if (!e->engines[count])
 +			continue;
  
 -	return ret;
 +		intel_context_put(e->engines[count]);
 +	}
 +	kfree(e);
  }
  
 -static struct i915_address_space *
 -i915_gem_vm_lookup(struct drm_i915_file_private *file_priv, u32 id)
 +static void free_engines(struct i915_gem_engines *e)
  {
 -	struct i915_address_space *vm;
 +	__free_engines(e, e->num_engines);
 +}
  
 -	xa_lock(&file_priv->vm_xa);
 -	vm = xa_load(&file_priv->vm_xa, id);
 -	if (vm)
 -		kref_get(&vm->ref);
 -	xa_unlock(&file_priv->vm_xa);
 +static void free_engines_rcu(struct rcu_head *rcu)
 +{
 +	struct i915_gem_engines *engines =
 +		container_of(rcu, struct i915_gem_engines, rcu);
  
 -	return vm;
 +	i915_sw_fence_fini(&engines->fence);
 +	free_engines(engines);
  }
  
 -static int set_proto_ctx_vm(struct drm_i915_file_private *fpriv,
 -			    struct i915_gem_proto_context *pc,
 -			    const struct drm_i915_gem_context_param *args)
 +static int __i915_sw_fence_call
 +engines_notify(struct i915_sw_fence *fence, enum i915_sw_fence_notify state)
  {
 -	struct drm_i915_private *i915 = fpriv->dev_priv;
 -	struct i915_address_space *vm;
 +	struct i915_gem_engines *engines =
 +		container_of(fence, typeof(*engines), fence);
  
 -	if (args->size)
 -		return -EINVAL;
 +	switch (state) {
 +	case FENCE_COMPLETE:
 +		if (!list_empty(&engines->link)) {
 +			struct i915_gem_context *ctx = engines->ctx;
 +			unsigned long flags;
  
 -	if (!HAS_FULL_PPGTT(i915))
 -		return -ENODEV;
 +			spin_lock_irqsave(&ctx->stale.lock, flags);
 +			list_del(&engines->link);
 +			spin_unlock_irqrestore(&ctx->stale.lock, flags);
 +		}
 +		i915_gem_context_put(engines->ctx);
 +		break;
  
 -	if (upper_32_bits(args->value))
 -		return -ENOENT;
 +	case FENCE_FREE:
 +		init_rcu_head(&engines->rcu);
 +		call_rcu(&engines->rcu, free_engines_rcu);
 +		break;
 +	}
  
 -	vm = i915_gem_vm_lookup(fpriv, args->value);
 -	if (!vm)
 -		return -ENOENT;
 +	return NOTIFY_DONE;
 +}
  
 -	if (pc->vm)
 -		i915_vm_put(pc->vm);
 -	pc->vm = vm;
 +static struct i915_gem_engines *alloc_engines(unsigned int count)
 +{
 +	struct i915_gem_engines *e;
  
 -	return 0;
 -}
 +	e = kzalloc(struct_size(e, engines, count), GFP_KERNEL);
 +	if (!e)
 +		return NULL;
  
 -struct set_proto_ctx_engines {
 -	struct drm_i915_private *i915;
 -	unsigned num_engines;
 -	struct i915_gem_proto_engine *engines;
 -};
 +	i915_sw_fence_init(&e->fence, engines_notify);
 +	return e;
 +}
  
 -static int
 -set_proto_ctx_engines_balance(struct i915_user_extension __user *base,
 -			      void *data)
 +static struct i915_gem_engines *default_engines(struct i915_gem_context *ctx)
  {
 -	struct i915_context_engines_load_balance __user *ext =
 -		container_of_user(base, typeof(*ext), base);
 -	const struct set_proto_ctx_engines *set = data;
 -	struct drm_i915_private *i915 = set->i915;
 -	struct intel_engine_cs **siblings;
 -	u16 num_siblings, idx;
 -	unsigned int n;
 -	int err;
 +	const struct intel_gt *gt = &ctx->i915->gt;
 +	struct intel_engine_cs *engine;
 +	struct i915_gem_engines *e;
 +	enum intel_engine_id id;
  
 -	if (!HAS_EXECLISTS(i915))
 -		return -ENODEV;
 +	e = alloc_engines(I915_NUM_ENGINES);
 +	if (!e)
 +		return ERR_PTR(-ENOMEM);
  
 -	if (get_user(idx, &ext->engine_index))
 -		return -EFAULT;
 +	for_each_engine(engine, gt, id) {
 +		struct intel_context *ce;
  
 -	if (idx >= set->num_engines) {
 -		drm_dbg(&i915->drm, "Invalid placement value, %d >= %d\n",
 -			idx, set->num_engines);
 -		return -EINVAL;
 -	}
 +		if (engine->legacy_idx == INVALID_ENGINE)
 +			continue;
  
 -	idx = array_index_nospec(idx, set->num_engines);
 -	if (set->engines[idx].type != I915_GEM_ENGINE_TYPE_INVALID) {
 -		drm_dbg(&i915->drm,
 -			"Invalid placement[%d], already occupied\n", idx);
 -		return -EEXIST;
 -	}
 +		GEM_BUG_ON(engine->legacy_idx >= I915_NUM_ENGINES);
 +		GEM_BUG_ON(e->engines[engine->legacy_idx]);
  
 -	if (get_user(num_siblings, &ext->num_siblings))
 -		return -EFAULT;
 +		ce = intel_context_create(engine);
 +		if (IS_ERR(ce)) {
 +			__free_engines(e, e->num_engines + 1);
 +			return ERR_CAST(ce);
 +		}
  
 -	err = check_user_mbz(&ext->flags);
 -	if (err)
 -		return err;
 +		intel_context_set_gem(ce, ctx);
  
 -	err = check_user_mbz(&ext->mbz64);
 -	if (err)
 -		return err;
 +		e->engines[engine->legacy_idx] = ce;
 +		e->num_engines = max(e->num_engines, engine->legacy_idx);
 +	}
 +	e->num_engines++;
  
 -	if (num_siblings == 0)
 -		return 0;
 +	return e;
 +}
  
 -	siblings = kmalloc_array(num_siblings, sizeof(*siblings), GFP_KERNEL);
 -	if (!siblings)
 -		return -ENOMEM;
 +void i915_gem_context_release(struct kref *ref)
 +{
 +	struct i915_gem_context *ctx = container_of(ref, typeof(*ctx), ref);
  
 -	for (n = 0; n < num_siblings; n++) {
 -		struct i915_engine_class_instance ci;
 +	trace_i915_context_free(ctx);
 +	GEM_BUG_ON(!i915_gem_context_is_closed(ctx));
  
 -		if (copy_from_user(&ci, &ext->engines[n], sizeof(ci))) {
 -			err = -EFAULT;
 -			goto err_siblings;
 -		}
 +	mutex_destroy(&ctx->engines_mutex);
 +	mutex_destroy(&ctx->lut_mutex);
  
 -		siblings[n] = intel_engine_lookup_user(i915,
 -						       ci.engine_class,
 -						       ci.engine_instance);
 -		if (!siblings[n]) {
 -			drm_dbg(&i915->drm,
 -				"Invalid sibling[%d]: { class:%d, inst:%d }\n",
 -				n, ci.engine_class, ci.engine_instance);
 -			err = -EINVAL;
 -			goto err_siblings;
 -		}
 -	}
 +	if (ctx->timeline)
 +		intel_timeline_put(ctx->timeline);
  
 -	if (num_siblings == 1) {
 -		set->engines[idx].type = I915_GEM_ENGINE_TYPE_PHYSICAL;
 -		set->engines[idx].engine = siblings[0];
 -		kfree(siblings);
 -	} else {
 -		set->engines[idx].type = I915_GEM_ENGINE_TYPE_BALANCED;
 -		set->engines[idx].num_siblings = num_siblings;
 -		set->engines[idx].siblings = siblings;
 -	}
 +	put_pid(ctx->pid);
 +	mutex_destroy(&ctx->mutex);
  
 -	return 0;
 +	kfree_rcu(ctx, rcu);
 +}
  
 -err_siblings:
 -	kfree(siblings);
 +static inline struct i915_gem_engines *
 +__context_engines_static(const struct i915_gem_context *ctx)
 +{
 +	return rcu_dereference_protected(ctx->engines, true);
 +}
  
 -	return err;
 +static void __reset_context(struct i915_gem_context *ctx,
 +			    struct intel_engine_cs *engine)
 +{
 +	intel_gt_handle_error(engine->gt, engine->mask, 0,
 +			      "context closure in %s", ctx->name);
  }
  
 -static int
 -set_proto_ctx_engines_bond(struct i915_user_extension __user *base, void *data)
 +static bool __cancel_engine(struct intel_engine_cs *engine)
  {
 -	struct i915_context_engines_bond __user *ext =
 -		container_of_user(base, typeof(*ext), base);
 -	const struct set_proto_ctx_engines *set = data;
 -	struct drm_i915_private *i915 = set->i915;
 -	struct i915_engine_class_instance ci;
 -	struct intel_engine_cs *master;
 -	u16 idx, num_bonds;
 -	int err, n;
 +	/*
 +	 * Send a "high priority pulse" down the engine to cause the
 +	 * current request to be momentarily preempted. (If it fails to
 +	 * be preempted, it will be reset). As we have marked our context
 +	 * as banned, any incomplete request, including any running, will
 +	 * be skipped following the preemption.
 +	 *
 +	 * If there is no hangchecking (one of the reasons why we try to
 +	 * cancel the context) and no forced preemption, there may be no
 +	 * means by which we reset the GPU and evict the persistent hog.
 +	 * Ergo if we are unable to inject a preemptive pulse that can
 +	 * kill the banned context, we fallback to doing a local reset
 +	 * instead.
 +	 */
 +	return intel_engine_pulse(engine) == 0;
 +}
  
 -	if (GRAPHICS_VER(i915) >= 12 && !IS_TIGERLAKE(i915) &&
 -	    !IS_ROCKETLAKE(i915) && !IS_ALDERLAKE_S(i915)) {
 -		drm_dbg(&i915->drm,
 -			"Bonding not supported on this platform\n");
 -		return -ENODEV;
 -	}
 +static struct intel_engine_cs *active_engine(struct intel_context *ce)
 +{
 +	struct intel_engine_cs *engine = NULL;
 +	struct i915_request *rq;
  
 -	if (get_user(idx, &ext->virtual_index))
 -		return -EFAULT;
 +	if (intel_context_has_inflight(ce))
 +		return intel_context_inflight(ce);
  
 -	if (idx >= set->num_engines) {
 -		drm_dbg(&i915->drm,
 -			"Invalid index for virtual engine: %d >= %d\n",
 -			idx, set->num_engines);
 -		return -EINVAL;
 -	}
 +	if (!ce->timeline)
 +		return NULL;
  
 -	idx = array_index_nospec(idx, set->num_engines);
 -	if (set->engines[idx].type == I915_GEM_ENGINE_TYPE_INVALID) {
 -		drm_dbg(&i915->drm, "Invalid engine at %d\n", idx);
 -		return -EINVAL;
 -	}
 +	/*
 +	 * rq->link is only SLAB_TYPESAFE_BY_RCU, we need to hold a reference
 +	 * to the request to prevent it being transferred to a new timeline
 +	 * (and onto a new timeline->requests list).
 +	 */
 +	rcu_read_lock();
 +	list_for_each_entry_reverse(rq, &ce->timeline->requests, link) {
 +		bool found;
  
 -	if (set->engines[idx].type != I915_GEM_ENGINE_TYPE_PHYSICAL) {
 -		drm_dbg(&i915->drm,
 -			"Bonding with virtual engines not allowed\n");
 -		return -EINVAL;
 -	}
 +		/* timeline is already completed upto this point? */
 +		if (!i915_request_get_rcu(rq))
 +			break;
  
 -	err = check_user_mbz(&ext->flags);
 -	if (err)
 -		return err;
 +		/* Check with the backend if the request is inflight */
 +		found = true;
 +		if (likely(rcu_access_pointer(rq->timeline) == ce->timeline))
 +			found = i915_request_active_engine(rq, &engine);
  
 -	for (n = 0; n < ARRAY_SIZE(ext->mbz64); n++) {
 -		err = check_user_mbz(&ext->mbz64[n]);
 -		if (err)
 -			return err;
 +		i915_request_put(rq);
 +		if (found)
 +			break;
  	}
 +	rcu_read_unlock();
  
 -	if (copy_from_user(&ci, &ext->master, sizeof(ci)))
 -		return -EFAULT;
 -
 -	master = intel_engine_lookup_user(i915,
 -					  ci.engine_class,
 -					  ci.engine_instance);
 -	if (!master) {
 -		drm_dbg(&i915->drm,
 -			"Unrecognised master engine: { class:%u, instance:%u }\n",
 -			ci.engine_class, ci.engine_instance);
 -		return -EINVAL;
 -	}
 +	return engine;
 +}
  
 -	if (intel_engine_uses_guc(master)) {
 -		DRM_DEBUG("bonding extension not supported with GuC submission");
 -		return -ENODEV;
 -	}
 +static void kill_engines(struct i915_gem_engines *engines, bool ban)
 +{
 +	struct i915_gem_engines_iter it;
 +	struct intel_context *ce;
  
 -	if (get_user(num_bonds, &ext->num_bonds))
 -		return -EFAULT;
 +	/*
 +	 * Map the user's engine back to the actual engines; one virtual
 +	 * engine will be mapped to multiple engines, and using ctx->engine[]
 +	 * the same engine may be have multiple instances in the user's map.
 +	 * However, we only care about pending requests, so only include
 +	 * engines on which there are incomplete requests.
 +	 */
 +	for_each_gem_engine(ce, engines, it) {
 +		struct intel_engine_cs *engine;
  
 -	for (n = 0; n < num_bonds; n++) {
 -		struct intel_engine_cs *bond;
 +		if (ban && intel_context_set_banned(ce))
 +			continue;
  
 -		if (copy_from_user(&ci, &ext->engines[n], sizeof(ci)))
 -			return -EFAULT;
 +		/*
 +		 * Check the current active state of this context; if we
 +		 * are currently executing on the GPU we need to evict
 +		 * ourselves. On the other hand, if we haven't yet been
 +		 * submitted to the GPU or if everything is complete,
 +		 * we have nothing to do.
 +		 */
 +		engine = active_engine(ce);
  
 -		bond = intel_engine_lookup_user(i915,
 -						ci.engine_class,
 -						ci.engine_instance);
 -		if (!bond) {
 -			drm_dbg(&i915->drm,
 -				"Unrecognised engine[%d] for bonding: { class:%d, instance: %d }\n",
 -				n, ci.engine_class, ci.engine_instance);
 -			return -EINVAL;
 -		}
 +		/* First attempt to gracefully cancel the context */
 +		if (engine && !__cancel_engine(engine) && ban)
 +			/*
 +			 * If we are unable to send a preemptive pulse to bump
 +			 * the context from the GPU, we have to resort to a full
 +			 * reset. We hope the collateral damage is worth it.
 +			 */
 +			__reset_context(engines->ctx, engine);
  	}
 -
 -	return 0;
  }
  
 -static int
 -set_proto_ctx_engines_parallel_submit(struct i915_user_extension __user *base,
 -				      void *data)
 +static void kill_context(struct i915_gem_context *ctx)
  {
 -	struct i915_context_engines_parallel_submit __user *ext =
 -		container_of_user(base, typeof(*ext), base);
 -	const struct set_proto_ctx_engines *set = data;
 -	struct drm_i915_private *i915 = set->i915;
 -	struct i915_engine_class_instance prev_engine;
 -	u64 flags;
 -	int err = 0, n, i, j;
 -	u16 slot, width, num_siblings;
 -	struct intel_engine_cs **siblings = NULL;
 -	intel_engine_mask_t prev_mask;
 -
 -	if (get_user(slot, &ext->engine_index))
 -		return -EFAULT;
 +	bool ban = (!i915_gem_context_is_persistent(ctx) ||
 +		    !ctx->i915->params.enable_hangcheck);
 +	struct i915_gem_engines *pos, *next;
  
 -	if (get_user(width, &ext->width))
 -		return -EFAULT;
 +	spin_lock_irq(&ctx->stale.lock);
 +	GEM_BUG_ON(!i915_gem_context_is_closed(ctx));
 +	list_for_each_entry_safe(pos, next, &ctx->stale.engines, link) {
 +		if (!i915_sw_fence_await(&pos->fence)) {
 +			list_del_init(&pos->link);
 +			continue;
 +		}
  
 -	if (get_user(num_siblings, &ext->num_siblings))
 -		return -EFAULT;
 +		spin_unlock_irq(&ctx->stale.lock);
  
 -	if (!intel_uc_uses_guc_submission(&to_gt(i915)->uc) &&
 -	    num_siblings != 1) {
 -		drm_dbg(&i915->drm, "Only 1 sibling (%d) supported in non-GuC mode\n",
 -			num_siblings);
 -		return -EINVAL;
 -	}
 +		kill_engines(pos, ban);
  
 -	if (slot >= set->num_engines) {
 -		drm_dbg(&i915->drm, "Invalid placement value, %d >= %d\n",
 -			slot, set->num_engines);
 -		return -EINVAL;
 -	}
 +		spin_lock_irq(&ctx->stale.lock);
 +		GEM_BUG_ON(i915_sw_fence_signaled(&pos->fence));
 +		list_safe_reset_next(pos, next, link);
 +		list_del_init(&pos->link); /* decouple from FENCE_COMPLETE */
  
 -	if (set->engines[slot].type != I915_GEM_ENGINE_TYPE_INVALID) {
 -		drm_dbg(&i915->drm,
 -			"Invalid placement[%d], already occupied\n", slot);
 -		return -EINVAL;
 +		i915_sw_fence_complete(&pos->fence);
  	}
 +	spin_unlock_irq(&ctx->stale.lock);
 +}
  
 -	if (get_user(flags, &ext->flags))
 -		return -EFAULT;
 +static void engines_idle_release(struct i915_gem_context *ctx,
 +				 struct i915_gem_engines *engines)
 +{
 +	struct i915_gem_engines_iter it;
 +	struct intel_context *ce;
  
 -	if (flags) {
 -		drm_dbg(&i915->drm, "Unknown flags 0x%02llx", flags);
 -		return -EINVAL;
 -	}
 +	INIT_LIST_HEAD(&engines->link);
  
 -	for (n = 0; n < ARRAY_SIZE(ext->mbz64); n++) {
 -		err = check_user_mbz(&ext->mbz64[n]);
 -		if (err)
 -			return err;
 -	}
 +	engines->ctx = i915_gem_context_get(ctx);
  
 -	if (width < 2) {
 -		drm_dbg(&i915->drm, "Width (%d) < 2\n", width);
 -		return -EINVAL;
 -	}
 +	for_each_gem_engine(ce, engines, it) {
 +		int err;
  
 -	if (num_siblings < 1) {
 -		drm_dbg(&i915->drm, "Number siblings (%d) < 1\n",
 -			num_siblings);
 -		return -EINVAL;
 +		/* serialises with execbuf */
 +		set_bit(CONTEXT_CLOSED_BIT, &ce->flags);
 +		if (!intel_context_pin_if_active(ce))
 +			continue;
 +
 +		/* Wait until context is finally scheduled out and retired */
 +		err = i915_sw_fence_await_active(&engines->fence,
 +						 &ce->active,
 +						 I915_ACTIVE_AWAIT_BARRIER);
 +		intel_context_unpin(ce);
 +		if (err)
 +			goto kill;
  	}
  
 -	siblings = kmalloc_array(num_siblings * width,
 -				 sizeof(*siblings),
 -				 GFP_KERNEL);
 -	if (!siblings)
 -		return -ENOMEM;
 +	spin_lock_irq(&ctx->stale.lock);
 +	if (!i915_gem_context_is_closed(ctx))
 +		list_add_tail(&engines->link, &ctx->stale.engines);
 +	spin_unlock_irq(&ctx->stale.lock);
  
 -	/* Create contexts / engines */
 -	for (i = 0; i < width; ++i) {
 -		intel_engine_mask_t current_mask = 0;
 -
 -		for (j = 0; j < num_siblings; ++j) {
 -			struct i915_engine_class_instance ci;
 -
 -			n = i * num_siblings + j;
 -			if (copy_from_user(&ci, &ext->engines[n], sizeof(ci))) {
 -				err = -EFAULT;
 -				goto out_err;
 -			}
 -
 -			siblings[n] =
 -				intel_engine_lookup_user(i915, ci.engine_class,
 -							 ci.engine_instance);
 -			if (!siblings[n]) {
 -				drm_dbg(&i915->drm,
 -					"Invalid sibling[%d]: { class:%d, inst:%d }\n",
 -					n, ci.engine_class, ci.engine_instance);
 -				err = -EINVAL;
 -				goto out_err;
 -			}
 +kill:
 +	if (list_empty(&engines->link)) /* raced, already closed */
 +		kill_engines(engines, true);
  
 -			/*
 -			 * We don't support breadcrumb handshake on these
 -			 * classes
 -			 */
 -			if (siblings[n]->class == RENDER_CLASS ||
 -			    siblings[n]->class == COMPUTE_CLASS) {
 -				err = -EINVAL;
 -				goto out_err;
 -			}
 -
 -			if (n) {
 -				if (prev_engine.engine_class !=
 -				    ci.engine_class) {
 -					drm_dbg(&i915->drm,
 -						"Mismatched class %d, %d\n",
 -						prev_engine.engine_class,
 -						ci.engine_class);
 -					err = -EINVAL;
 -					goto out_err;
 -				}
 -			}
 -
 -			prev_engine = ci;
 -			current_mask |= siblings[n]->logical_mask;
 -		}
 +	i915_sw_fence_commit(&engines->fence);
 +}
  
 -		if (i > 0) {
 -			if (current_mask != prev_mask << 1) {
 -				drm_dbg(&i915->drm,
 -					"Non contiguous logical mask 0x%x, 0x%x\n",
 -					prev_mask, current_mask);
 -				err = -EINVAL;
 -				goto out_err;
 -			}
 -		}
 -		prev_mask = current_mask;
 -	}
 +static void set_closed_name(struct i915_gem_context *ctx)
 +{
 +	char *s;
  
 -	set->engines[slot].type = I915_GEM_ENGINE_TYPE_PARALLEL;
 -	set->engines[slot].num_siblings = num_siblings;
 -	set->engines[slot].width = width;
 -	set->engines[slot].siblings = siblings;
 +	/* Replace '[]' with '<>' to indicate closed in debug prints */
  
 -	return 0;
 +	s = strrchr(ctx->name, '[');
 +	if (!s)
 +		return;
  
 -out_err:
 -	kfree(siblings);
 +	*s = '<';
  
 -	return err;
 +	s = strchr(s + 1, ']');
 +	if (s)
 +		*s = '>';
  }
  
 -static const i915_user_extension_fn set_proto_ctx_engines_extensions[] = {
 -	[I915_CONTEXT_ENGINES_EXT_LOAD_BALANCE] = set_proto_ctx_engines_balance,
 -	[I915_CONTEXT_ENGINES_EXT_BOND] = set_proto_ctx_engines_bond,
 -	[I915_CONTEXT_ENGINES_EXT_PARALLEL_SUBMIT] =
 -		set_proto_ctx_engines_parallel_submit,
 -};
 +static void context_close(struct i915_gem_context *ctx)
 +{
 +	struct i915_address_space *vm;
 +
 +	/* Flush any concurrent set_engines() */
 +	mutex_lock(&ctx->engines_mutex);
 +	engines_idle_release(ctx, rcu_replace_pointer(ctx->engines, NULL, 1));
 +	i915_gem_context_set_closed(ctx);
 +	mutex_unlock(&ctx->engines_mutex);
  
 -static int set_proto_ctx_engines(struct drm_i915_file_private *fpriv,
 -			         struct i915_gem_proto_context *pc,
 -			         const struct drm_i915_gem_context_param *args)
 +	mutex_lock(&ctx->mutex);
 +
 +	set_closed_name(ctx);
 +
 +	vm = i915_gem_context_vm(ctx);
 +	if (vm)
 +		i915_vm_close(vm);
 +
 +	ctx->file_priv = ERR_PTR(-EBADF);
 +
 +	/*
 +	 * The LUT uses the VMA as a backpointer to unref the object,
 +	 * so we need to clear the LUT before we close all the VMA (inside
 +	 * the ppgtt).
 +	 */
 +	lut_close(ctx);
 +
 +	spin_lock(&ctx->i915->gem.contexts.lock);
 +	list_del(&ctx->link);
 +	spin_unlock(&ctx->i915->gem.contexts.lock);
 +
 +	mutex_unlock(&ctx->mutex);
 +
 +	/*
 +	 * If the user has disabled hangchecking, we can not be sure that
 +	 * the batches will ever complete after the context is closed,
 +	 * keeping the context and all resources pinned forever. So in this
 +	 * case we opt to forcibly kill off all remaining requests on
 +	 * context close.
 +	 */
 +	kill_context(ctx);
 +
 +	i915_gem_context_put(ctx);
 +}
 +
 +static int __context_set_persistence(struct i915_gem_context *ctx, bool state)
  {
 -	struct drm_i915_private *i915 = fpriv->dev_priv;
 -	struct set_proto_ctx_engines set = { .i915 = i915 };
 -	struct i915_context_param_engines __user *user =
 -		u64_to_user_ptr(args->value);
 -	unsigned int n;
 -	u64 extensions;
 -	int err;
 +	if (i915_gem_context_is_persistent(ctx) == state)
 +		return 0;
  
 -	if (pc->num_user_engines >= 0) {
 -		drm_dbg(&i915->drm, "Cannot set engines twice");
 -		return -EINVAL;
 +	if (state) {
 +		/*
 +		 * Only contexts that are short-lived [that will expire or be
 +		 * reset] are allowed to survive past termination. We require
 +		 * hangcheck to ensure that the persistent requests are healthy.
 +		 */
 +		if (!ctx->i915->params.enable_hangcheck)
 +			return -EINVAL;
 +
 +		i915_gem_context_set_persistence(ctx);
 +	} else {
 +		/* To cancel a context we use "preempt-to-idle" */
 +		if (!(ctx->i915->caps.scheduler & I915_SCHEDULER_CAP_PREEMPTION))
 +			return -ENODEV;
 +
 +		/*
 +		 * If the cancel fails, we then need to reset, cleanly!
 +		 *
 +		 * If the per-engine reset fails, all hope is lost! We resort
 +		 * to a full GPU reset in that unlikely case, but realistically
 +		 * if the engine could not reset, the full reset does not fare
 +		 * much better. The damage has been done.
 +		 *
 +		 * However, if we cannot reset an engine by itself, we cannot
 +		 * cleanup a hanging persistent context without causing
 +		 * colateral damage, and we should not pretend we can by
 +		 * exposing the interface.
 +		 */
 +		if (!intel_has_reset_engine(&ctx->i915->gt))
 +			return -ENODEV;
 +
 +		i915_gem_context_clear_persistence(ctx);
  	}
  
 -	if (args->size < sizeof(*user) ||
 -	    !IS_ALIGNED(args->size - sizeof(*user), sizeof(*user->engines))) {
 -		drm_dbg(&i915->drm, "Invalid size for engine array: %d\n",
 -			args->size);
 -		return -EINVAL;
 -	}
 +	return 0;
 +}
  
 -	set.num_engines = (args->size - sizeof(*user)) / sizeof(*user->engines);
 -	/* RING_MASK has no shift so we can use it directly here */
 -	if (set.num_engines > I915_EXEC_RING_MASK + 1)
 -		return -EINVAL;
 +static struct i915_gem_context *
 +__create_context(struct drm_i915_private *i915)
 +{
 +	struct i915_gem_context *ctx;
 +	struct i915_gem_engines *e;
 +	int err;
 +	int i;
  
 -	set.engines = kmalloc_array(set.num_engines, sizeof(*set.engines), GFP_KERNEL);
 -	if (!set.engines)
 -		return -ENOMEM;
 +	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
 +	if (!ctx)
 +		return ERR_PTR(-ENOMEM);
  
 -	for (n = 0; n < set.num_engines; n++) {
 -		struct i915_engine_class_instance ci;
 -		struct intel_engine_cs *engine;
 +	kref_init(&ctx->ref);
 +	ctx->i915 = i915;
 +	ctx->sched.priority = I915_PRIORITY_NORMAL;
 +	mutex_init(&ctx->mutex);
 +	INIT_LIST_HEAD(&ctx->link);
  
 -		if (copy_from_user(&ci, &user->engines[n], sizeof(ci))) {
 -			kfree(set.engines);
 -			return -EFAULT;
 -		}
 +	spin_lock_init(&ctx->stale.lock);
 +	INIT_LIST_HEAD(&ctx->stale.engines);
  
 -		memset(&set.engines[n], 0, sizeof(set.engines[n]));
 +	mutex_init(&ctx->engines_mutex);
 +	e = default_engines(ctx);
 +	if (IS_ERR(e)) {
 +		err = PTR_ERR(e);
 +		goto err_free;
 +	}
 +	RCU_INIT_POINTER(ctx->engines, e);
  
 -		if (ci.engine_class == (u16)I915_ENGINE_CLASS_INVALID &&
 -		    ci.engine_instance == (u16)I915_ENGINE_CLASS_INVALID_NONE)
 -			continue;
 +	INIT_RADIX_TREE(&ctx->handles_vma, GFP_KERNEL);
 +	mutex_init(&ctx->lut_mutex);
  
 -		engine = intel_engine_lookup_user(i915,
 -						  ci.engine_class,
 -						  ci.engine_instance);
 -		if (!engine) {
 -			drm_dbg(&i915->drm,
 -				"Invalid engine[%d]: { class:%d, instance:%d }\n",
 -				n, ci.engine_class, ci.engine_instance);
 -			kfree(set.engines);
 -			return -ENOENT;
 -		}
 +	/* NB: Mark all slices as needing a remap so that when the context first
 +	 * loads it will restore whatever remap state already exists. If there
 +	 * is no remap info, it will be a NOP. */
 +	ctx->remap_slice = ALL_L3_SLICES(i915);
  
 -		set.engines[n].type = I915_GEM_ENGINE_TYPE_PHYSICAL;
 -		set.engines[n].engine = engine;
 -	}
 +	i915_gem_context_set_bannable(ctx);
 +	i915_gem_context_set_recoverable(ctx);
 +	__context_set_persistence(ctx, true /* cgroup hook? */);
  
 -	err = -EFAULT;
 -	if (!get_user(extensions, &user->extensions))
 -		err = i915_user_extensions(u64_to_user_ptr(extensions),
 -					   set_proto_ctx_engines_extensions,
 -					   ARRAY_SIZE(set_proto_ctx_engines_extensions),
 -					   &set);
 -	if (err) {
 -		kfree(set.engines);
 -		return err;
 -	}
 +	for (i = 0; i < ARRAY_SIZE(ctx->hang_timestamp); i++)
 +		ctx->hang_timestamp[i] = jiffies - CONTEXT_FAST_HANG_JIFFIES;
  
 -	pc->num_user_engines = set.num_engines;
 -	pc->user_engines = set.engines;
 +	return ctx;
  
 -	return 0;
 +err_free:
 +	kfree(ctx);
 +	return ERR_PTR(err);
  }
  
 -static int set_proto_ctx_sseu(struct drm_i915_file_private *fpriv,
 -			      struct i915_gem_proto_context *pc,
 -			      struct drm_i915_gem_context_param *args)
 +static inline struct i915_gem_engines *
 +__context_engines_await(const struct i915_gem_context *ctx,
 +			bool *user_engines)
  {
 -	struct drm_i915_private *i915 = fpriv->dev_priv;
 -	struct drm_i915_gem_context_param_sseu user_sseu;
 -	struct intel_sseu *sseu;
 -	int ret;
 +	struct i915_gem_engines *engines;
  
 -	if (args->size < sizeof(user_sseu))
 -		return -EINVAL;
 +	rcu_read_lock();
 +	do {
 +		engines = rcu_dereference(ctx->engines);
 +		GEM_BUG_ON(!engines);
  
 -	if (GRAPHICS_VER(i915) != 11)
 -		return -ENODEV;
 +		if (user_engines)
 +			*user_engines = i915_gem_context_user_engines(ctx);
  
 -	if (copy_from_user(&user_sseu, u64_to_user_ptr(args->value),
 -			   sizeof(user_sseu)))
 -		return -EFAULT;
 +		/* successful await => strong mb */
 +		if (unlikely(!i915_sw_fence_await(&engines->fence)))
 +			continue;
  
 -	if (user_sseu.rsvd)
 -		return -EINVAL;
 +		if (likely(engines == rcu_access_pointer(ctx->engines)))
 +			break;
  
 -	if (user_sseu.flags & ~(I915_CONTEXT_SSEU_FLAG_ENGINE_INDEX))
 -		return -EINVAL;
 +		i915_sw_fence_complete(&engines->fence);
 +	} while (1);
 +	rcu_read_unlock();
  
 -	if (!!(user_sseu.flags & I915_CONTEXT_SSEU_FLAG_ENGINE_INDEX) != (pc->num_user_engines >= 0))
 -		return -EINVAL;
 +	return engines;
 +}
  
 -	if (pc->num_user_engines >= 0) {
 -		int idx = user_sseu.engine.engine_instance;
 -		struct i915_gem_proto_engine *pe;
 +static int
 +context_apply_all(struct i915_gem_context *ctx,
 +		  int (*fn)(struct intel_context *ce, void *data),
 +		  void *data)
 +{
 +	struct i915_gem_engines_iter it;
 +	struct i915_gem_engines *e;
 +	struct intel_context *ce;
 +	int err = 0;
  
 -		if (idx >= pc->num_user_engines)
 -			return -EINVAL;
 +	e = __context_engines_await(ctx, NULL);
 +	for_each_gem_engine(ce, e, it) {
 +		err = fn(ce, data);
 +		if (err)
 +			break;
 +	}
 +	i915_sw_fence_complete(&e->fence);
  
 -		pe = &pc->user_engines[idx];
 +	return err;
 +}
  
 -		/* Only render engine supports RPCS configuration. */
 -		if (pe->engine->class != RENDER_CLASS)
 -			return -EINVAL;
 +static int __apply_ppgtt(struct intel_context *ce, void *vm)
 +{
 +	i915_vm_put(ce->vm);
 +	ce->vm = i915_vm_get(vm);
 +	return 0;
 +}
  
 -		sseu = &pe->sseu;
 -	} else {
 -		/* Only render engine supports RPCS configuration. */
 -		if (user_sseu.engine.engine_class != I915_ENGINE_CLASS_RENDER)
 -			return -EINVAL;
 +static struct i915_address_space *
 +__set_ppgtt(struct i915_gem_context *ctx, struct i915_address_space *vm)
 +{
 +	struct i915_address_space *old;
  
 -		/* There is only one render engine */
 -		if (user_sseu.engine.engine_instance != 0)
 -			return -EINVAL;
 +	old = rcu_replace_pointer(ctx->vm,
 +				  i915_vm_open(vm),
 +				  lockdep_is_held(&ctx->mutex));
 +	GEM_BUG_ON(old && i915_vm_is_4lvl(vm) != i915_vm_is_4lvl(old));
  
 -		sseu = &pc->legacy_rcs_sseu;
 -	}
 +	context_apply_all(ctx, __apply_ppgtt, vm);
  
 -	ret = i915_gem_user_to_context_sseu(to_gt(i915), &user_sseu, sseu);
 -	if (ret)
 -		return ret;
 +	return old;
 +}
  
 -	args->size = sizeof(user_sseu);
 +static void __assign_ppgtt(struct i915_gem_context *ctx,
 +			   struct i915_address_space *vm)
 +{
 +	if (vm == rcu_access_pointer(ctx->vm))
 +		return;
  
 -	return 0;
 +	vm = __set_ppgtt(ctx, vm);
 +	if (vm)
 +		i915_vm_close(vm);
  }
  
 -static int set_proto_ctx_param(struct drm_i915_file_private *fpriv,
 -			       struct i915_gem_proto_context *pc,
 -			       struct drm_i915_gem_context_param *args)
 +static void __set_timeline(struct intel_timeline **dst,
 +			   struct intel_timeline *src)
  {
 -	int ret = 0;
 +	struct intel_timeline *old = *dst;
  
 -	switch (args->param) {
 -	case I915_CONTEXT_PARAM_NO_ERROR_CAPTURE:
 -		if (args->size)
 -			ret = -EINVAL;
 -		else if (args->value)
 -			pc->user_flags |= BIT(UCONTEXT_NO_ERROR_CAPTURE);
 -		else
 -			pc->user_flags &= ~BIT(UCONTEXT_NO_ERROR_CAPTURE);
 -		break;
 +	*dst = src ? intel_timeline_get(src) : NULL;
  
 -	case I915_CONTEXT_PARAM_BANNABLE:
 -		if (args->size)
 -			ret = -EINVAL;
 -		else if (!capable(CAP_SYS_ADMIN) && !args->value)
 -			ret = -EPERM;
 -		else if (args->value)
 -			pc->user_flags |= BIT(UCONTEXT_BANNABLE);
 -		else if (pc->uses_protected_content)
 -			ret = -EPERM;
 -		else
 -			pc->user_flags &= ~BIT(UCONTEXT_BANNABLE);
 -		break;
 +	if (old)
 +		intel_timeline_put(old);
 +}
  
 -	case I915_CONTEXT_PARAM_RECOVERABLE:
 -		if (args->size)
 -			ret = -EINVAL;
 -		else if (!args->value)
 -			pc->user_flags &= ~BIT(UCONTEXT_RECOVERABLE);
 -		else if (pc->uses_protected_content)
 -			ret = -EPERM;
 -		else
 -			pc->user_flags |= BIT(UCONTEXT_RECOVERABLE);
 -		break;
 +static int __apply_timeline(struct intel_context *ce, void *timeline)
 +{
 +	__set_timeline(&ce->timeline, timeline);
 +	return 0;
 +}
  
 -	case I915_CONTEXT_PARAM_PRIORITY:
 -		ret = validate_priority(fpriv->dev_priv, args);
 -		if (!ret)
 -			pc->sched.priority = args->value;
 -		break;
 +static void __assign_timeline(struct i915_gem_context *ctx,
 +			      struct intel_timeline *timeline)
 +{
 +	__set_timeline(&ctx->timeline, timeline);
 +	context_apply_all(ctx, __apply_timeline, timeline);
 +}
  
 -	case I915_CONTEXT_PARAM_SSEU:
 -		ret = set_proto_ctx_sseu(fpriv, pc, args);
 -		break;
 +static int __apply_watchdog(struct intel_context *ce, void *timeout_us)
 +{
 +	return intel_context_set_watchdog_us(ce, (uintptr_t)timeout_us);
 +}
  
 -	case I915_CONTEXT_PARAM_VM:
 -		ret = set_proto_ctx_vm(fpriv, pc, args);
 -		break;
 +static int
 +__set_watchdog(struct i915_gem_context *ctx, unsigned long timeout_us)
 +{
 +	int ret;
  
 -	case I915_CONTEXT_PARAM_ENGINES:
 -		ret = set_proto_ctx_engines(fpriv, pc, args);
 -		break;
 +	ret = context_apply_all(ctx, __apply_watchdog,
 +				(void *)(uintptr_t)timeout_us);
 +	if (!ret)
 +		ctx->watchdog.timeout_us = timeout_us;
  
 -	case I915_CONTEXT_PARAM_PERSISTENCE:
 -		if (args->size)
 -			ret = -EINVAL;
 -		else
 -			ret = proto_context_set_persistence(fpriv->dev_priv, pc,
 -							    args->value);
 -		break;
 +	return ret;
 +}
  
 -	case I915_CONTEXT_PARAM_PROTECTED_CONTENT:
 -		ret = proto_context_set_protected(fpriv->dev_priv, pc,
 -						  args->value);
 -		break;
 +static void __set_default_fence_expiry(struct i915_gem_context *ctx)
 +{
 +	struct drm_i915_private *i915 = ctx->i915;
 +	int ret;
  
 -	case I915_CONTEXT_PARAM_NO_ZEROMAP:
 -	case I915_CONTEXT_PARAM_BAN_PERIOD:
 -	case I915_CONTEXT_PARAM_RINGSIZE:
 -	default:
 -		ret = -EINVAL;
 -		break;
 -	}
 +	if (!IS_ACTIVE(CONFIG_DRM_I915_REQUEST_TIMEOUT) ||
 +	    !i915->params.request_timeout_ms)
 +		return;
  
 -	return ret;
 +	/* Default expiry for user fences. */
 +	ret = __set_watchdog(ctx, i915->params.request_timeout_ms * 1000);
 +	if (ret)
 +		drm_notice(&i915->drm,
 +			   "Failed to configure default fence expiry! (%d)",
 +			   ret);
  }
  
 -static int intel_context_set_gem(struct intel_context *ce,
 -				 struct i915_gem_context *ctx,
 -				 struct intel_sseu sseu)
 +static struct i915_gem_context *
 +i915_gem_create_context(struct drm_i915_private *i915, unsigned int flags)
  {
 -	int ret = 0;
 +	struct i915_gem_context *ctx;
  
 -	GEM_BUG_ON(rcu_access_pointer(ce->gem_context));
 -	RCU_INIT_POINTER(ce->gem_context, ctx);
 +	if (flags & I915_CONTEXT_CREATE_FLAGS_SINGLE_TIMELINE &&
 +	    !HAS_EXECLISTS(i915))
 +		return ERR_PTR(-EINVAL);
  
 -	GEM_BUG_ON(intel_context_is_pinned(ce));
 -	ce->ring_size = SZ_16K;
 +	ctx = __create_context(i915);
 +	if (IS_ERR(ctx))
 +		return ctx;
  
 -	i915_vm_put(ce->vm);
 -	ce->vm = i915_gem_context_get_eb_vm(ctx);
 +	if (HAS_FULL_PPGTT(i915)) {
 +		struct i915_ppgtt *ppgtt;
  
 -	if (ctx->sched.priority >= I915_PRIORITY_NORMAL &&
 -	    intel_engine_has_timeslices(ce->engine) &&
 -	    intel_engine_has_semaphores(ce->engine))
 -		__set_bit(CONTEXT_USE_SEMAPHORES, &ce->flags);
 +		ppgtt = i915_ppgtt_create(&i915->gt);
 +		if (IS_ERR(ppgtt)) {
 +			drm_dbg(&i915->drm, "PPGTT setup failed (%ld)\n",
 +				PTR_ERR(ppgtt));
 +			context_close(ctx);
 +			return ERR_CAST(ppgtt);
 +		}
  
 -	if (CONFIG_DRM_I915_REQUEST_TIMEOUT &&
 -	    ctx->i915->params.request_timeout_ms) {
 -		unsigned int timeout_ms = ctx->i915->params.request_timeout_ms;
 +		mutex_lock(&ctx->mutex);
 +		__assign_ppgtt(ctx, &ppgtt->vm);
 +		mutex_unlock(&ctx->mutex);
  
 -		intel_context_set_watchdog_us(ce, (u64)timeout_ms * 1000);
 +		i915_vm_put(&ppgtt->vm);
  	}
  
 -	/* A valid SSEU has no zero fields */
 -	if (sseu.slice_mask && !WARN_ON(ce->engine->class != RENDER_CLASS))
 -		ret = intel_context_reconfigure_sseu(ce, sseu);
 -
 -	return ret;
 -}
 -
 -static void __unpin_engines(struct i915_gem_engines *e, unsigned int count)
 -{
 -	while (count--) {
 -		struct intel_context *ce = e->engines[count], *child;
 +	if (flags & I915_CONTEXT_CREATE_FLAGS_SINGLE_TIMELINE) {
 +		struct intel_timeline *timeline;
  
 -		if (!ce || !test_bit(CONTEXT_PERMA_PIN, &ce->flags))
 -			continue;
 +		timeline = intel_timeline_create(&i915->gt);
 +		if (IS_ERR(timeline)) {
 +			context_close(ctx);
 +			return ERR_CAST(timeline);
 +		}
  
 -		for_each_child(ce, child)
 -			intel_context_unpin(child);
 -		intel_context_unpin(ce);
 +		__assign_timeline(ctx, timeline);
 +		intel_timeline_put(timeline);
  	}
 -}
  
 -static void unpin_engines(struct i915_gem_engines *e)
 -{
 -	__unpin_engines(e, e->num_engines);
 -}
 +	__set_default_fence_expiry(ctx);
  
 -static void __free_engines(struct i915_gem_engines *e, unsigned int count)
 -{
 -	while (count--) {
 -		if (!e->engines[count])
 -			continue;
 +	trace_i915_context_create(ctx);
  
 -		intel_context_put(e->engines[count]);
 -	}
 -	kfree(e);
 +	return ctx;
  }
  
 -static void free_engines(struct i915_gem_engines *e)
 +static void init_contexts(struct i915_gem_contexts *gc)
  {
 -	__free_engines(e, e->num_engines);
 +	spin_lock_init(&gc->lock);
 +	INIT_LIST_HEAD(&gc->list);
  }
  
 -static void free_engines_rcu(struct rcu_head *rcu)
 +void i915_gem_init__contexts(struct drm_i915_private *i915)
  {
 -	struct i915_gem_engines *engines =
 -		container_of(rcu, struct i915_gem_engines, rcu);
 -
 -	i915_sw_fence_fini(&engines->fence);
 -	free_engines(engines);
 +	init_contexts(&i915->gem.contexts);
  }
  
 -static void accumulate_runtime(struct i915_drm_client *client,
 -			       struct i915_gem_engines *engines)
 +static int gem_context_register(struct i915_gem_context *ctx,
 +				struct drm_i915_file_private *fpriv,
 +				u32 *id)
  {
 -	struct i915_gem_engines_iter it;
 -	struct intel_context *ce;
 +	struct drm_i915_private *i915 = ctx->i915;
 +	struct i915_address_space *vm;
 +	int ret;
  
 -	if (!client)
 -		return;
 +	ctx->file_priv = fpriv;
  
 -	/* Transfer accumulated runtime to the parent GEM context. */
 -	for_each_gem_engine(ce, engines, it) {
 -		unsigned int class = ce->engine->uabi_class;
 +	mutex_lock(&ctx->mutex);
 +	vm = i915_gem_context_vm(ctx);
 +	if (vm)
 +		WRITE_ONCE(vm->file, fpriv); /* XXX */
 +	mutex_unlock(&ctx->mutex);
  
 -		GEM_BUG_ON(class >= ARRAY_SIZE(client->past_runtime));
 -		atomic64_add(intel_context_get_total_runtime_ns(ce),
 -			     &client->past_runtime[class]);
 -	}
 +	ctx->pid = get_task_pid(current, PIDTYPE_PID);
 +	snprintf(ctx->name, sizeof(ctx->name), "%s[%d]",
 +		 current->comm, pid_nr(ctx->pid));
 +
 +	/* And finally expose ourselves to userspace via the idr */
 +	ret = xa_alloc(&fpriv->context_xa, id, ctx, xa_limit_32b, GFP_KERNEL);
 +	if (ret)
 +		goto err_pid;
 +
 +	spin_lock(&i915->gem.contexts.lock);
 +	list_add_tail(&ctx->link, &i915->gem.contexts.list);
 +	spin_unlock(&i915->gem.contexts.lock);
 +
 +	return 0;
 +
 +err_pid:
 +	put_pid(fetch_and_zero(&ctx->pid));
 +	return ret;
  }
  
 -static int
 -engines_notify(struct i915_sw_fence *fence, enum i915_sw_fence_notify state)
 +int i915_gem_context_open(struct drm_i915_private *i915,
 +			  struct drm_file *file)
  {
 -	struct i915_gem_engines *engines =
 -		container_of(fence, typeof(*engines), fence);
 -	struct i915_gem_context *ctx = engines->ctx;
 -
 -	switch (state) {
 -	case FENCE_COMPLETE:
 -		if (!list_empty(&engines->link)) {
 -			unsigned long flags;
 +	struct drm_i915_file_private *file_priv = file->driver_priv;
 +	struct i915_gem_context *ctx;
 +	int err;
 +	u32 id;
  
 -			spin_lock_irqsave(&ctx->stale.lock, flags);
 -			list_del(&engines->link);
 -			spin_unlock_irqrestore(&ctx->stale.lock, flags);
 -		}
 -		accumulate_runtime(ctx->client, engines);
 -		i915_gem_context_put(ctx);
 +	xa_init_flags(&file_priv->context_xa, XA_FLAGS_ALLOC);
  
 -		break;
 +	/* 0 reserved for invalid/unassigned ppgtt */
 +	xa_init_flags(&file_priv->vm_xa, XA_FLAGS_ALLOC1);
  
 -	case FENCE_FREE:
 -		init_rcu_head(&engines->rcu);
 -		call_rcu(&engines->rcu, free_engines_rcu);
 -		break;
 +	ctx = i915_gem_create_context(i915, 0);
 +	if (IS_ERR(ctx)) {
 +		err = PTR_ERR(ctx);
 +		goto err;
  	}
  
 -	return NOTIFY_DONE;
 -}
 -
 -static struct i915_gem_engines *alloc_engines(unsigned int count)
 -{
 -	struct i915_gem_engines *e;
 +	err = gem_context_register(ctx, file_priv, &id);
 +	if (err < 0)
 +		goto err_ctx;
  
 -	e = kzalloc(struct_size(e, engines, count), GFP_KERNEL);
 -	if (!e)
 -		return NULL;
 +	GEM_BUG_ON(id);
 +	return 0;
  
 -	i915_sw_fence_init(&e->fence, engines_notify);
 -	return e;
 +err_ctx:
 +	context_close(ctx);
 +err:
 +	xa_destroy(&file_priv->vm_xa);
 +	xa_destroy(&file_priv->context_xa);
 +	return err;
  }
  
 -static struct i915_gem_engines *default_engines(struct i915_gem_context *ctx,
 -						struct intel_sseu rcs_sseu)
 +void i915_gem_context_close(struct drm_file *file)
  {
 -	const struct intel_gt *gt = to_gt(ctx->i915);
 -	struct intel_engine_cs *engine;
 -	struct i915_gem_engines *e, *err;
 -	enum intel_engine_id id;
 -
 -	e = alloc_engines(I915_NUM_ENGINES);
 -	if (!e)
 -		return ERR_PTR(-ENOMEM);
 +	struct drm_i915_file_private *file_priv = file->driver_priv;
 +	struct i915_address_space *vm;
 +	struct i915_gem_context *ctx;
 +	unsigned long idx;
  
 -	for_each_engine(engine, gt, id) {
 -		struct intel_context *ce;
 -		struct intel_sseu sseu = {};
 -		int ret;
 +	xa_for_each(&file_priv->context_xa, idx, ctx)
 +		context_close(ctx);
 +	xa_destroy(&file_priv->context_xa);
  
 -		if (engine->legacy_idx == INVALID_ENGINE)
 -			continue;
 +	xa_for_each(&file_priv->vm_xa, idx, vm)
 +		i915_vm_put(vm);
 +	xa_destroy(&file_priv->vm_xa);
 +}
  
 -		GEM_BUG_ON(engine->legacy_idx >= I915_NUM_ENGINES);
 -		GEM_BUG_ON(e->engines[engine->legacy_idx]);
 +int i915_gem_vm_create_ioctl(struct drm_device *dev, void *data,
 +			     struct drm_file *file)
 +{
 +	struct drm_i915_private *i915 = to_i915(dev);
 +	struct drm_i915_gem_vm_control *args = data;
 +	struct drm_i915_file_private *file_priv = file->driver_priv;
 +	struct i915_ppgtt *ppgtt;
 +	u32 id;
 +	int err;
  
 -		ce = intel_context_create(engine);
 -		if (IS_ERR(ce)) {
 -			err = ERR_CAST(ce);
 -			goto free_engines;
 -		}
 +	if (!HAS_FULL_PPGTT(i915))
 +		return -ENODEV;
  
 -		e->engines[engine->legacy_idx] = ce;
 -		e->num_engines = max(e->num_engines, engine->legacy_idx + 1);
 +	if (args->flags)
 +		return -EINVAL;
  
 -		if (engine->class == RENDER_CLASS)
 -			sseu = rcs_sseu;
 +	ppgtt = i915_ppgtt_create(&i915->gt);
 +	if (IS_ERR(ppgtt))
 +		return PTR_ERR(ppgtt);
  
 -		ret = intel_context_set_gem(ce, ctx, sseu);
 -		if (ret) {
 -			err = ERR_PTR(ret);
 -			goto free_engines;
 -		}
 +	ppgtt->vm.file = file_priv;
  
 +	if (args->extensions) {
 +		err = i915_user_extensions(u64_to_user_ptr(args->extensions),
 +					   NULL, 0,
 +					   ppgtt);
 +		if (err)
 +			goto err_put;
  	}
  
 -	return e;
 +	err = xa_alloc(&file_priv->vm_xa, &id, &ppgtt->vm,
 +		       xa_limit_32b, GFP_KERNEL);
 +	if (err)
 +		goto err_put;
 +
 +	GEM_BUG_ON(id == 0); /* reserved for invalid/unassigned ppgtt */
 +	args->vm_id = id;
 +	return 0;
  
 -free_engines:
 -	free_engines(e);
 +err_put:
 +	i915_vm_put(&ppgtt->vm);
  	return err;
  }
  
* Unmerged path drivers/gpu/drm/i915/gem/i915_gem_context.c
