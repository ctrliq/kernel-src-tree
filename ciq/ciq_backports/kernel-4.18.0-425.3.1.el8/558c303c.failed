arm64: Mitigate spectre style branch history side channels

jira LE-1907
cve CVE-2022-23960
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author James Morse <james.morse@arm.com>
commit 558c303c9734af5a813739cd284879227f7297d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/558c303c.failed

Speculation attacks against some high-performance processors can
make use of branch history to influence future speculation.
When taking an exception from user-space, a sequence of branches
or a firmware call overwrites or invalidates the branch history.

The sequence of branches is added to the vectors, and should appear
before the first indirect branch. For systems using KPTI the sequence
is added to the kpti trampoline where it has a free register as the exit
from the trampoline is via a 'ret'. For systems not using KPTI, the same
register tricks are used to free up a register in the vectors.

For the firmware call, arch-workaround-3 clobbers 4 registers, so
there is no choice but to save them to the EL1 stack. This only happens
for entry from EL0, so if we take an exception due to the stack access,
it will not become re-entrant.

For KVM, the existing branch-predictor-hardening vectors are used.
When a spectre version of these vectors is in use, the firmware call
is sufficient to mitigate against Spectre-BHB. For the non-spectre
versions, the sequence of branches is added to the indirect vector.

	Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
	Signed-off-by: James Morse <james.morse@arm.com>
(cherry picked from commit 558c303c9734af5a813739cd284879227f7297d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/Kconfig
#	arch/arm64/include/asm/assembler.h
#	arch/arm64/include/asm/cpufeature.h
#	arch/arm64/include/asm/cputype.h
#	arch/arm64/include/asm/spectre.h
#	arch/arm64/include/asm/sysreg.h
#	arch/arm64/include/asm/vectors.h
#	arch/arm64/kernel/image-vars.h
#	arch/arm64/kernel/proton-pack.c
#	arch/arm64/kvm/hyp/hyp-entry.S
#	arch/arm64/tools/cpucaps
diff --cc arch/arm64/Kconfig
index 445f789e8a49,c9631626c0b3..000000000000
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@@ -1160,21 -1382,14 +1160,32 @@@ config UNMAP_KERNEL_AT_EL
  
  	  If unsure, say Y.
  
++<<<<<<< HEAD
 +config HARDEN_EL2_VECTORS
 +	bool "Harden EL2 vector mapping against system register leak" if EXPERT
 +	default y
 +	help
 +	  Speculation attacks against some high-performance processors can
 +	  be used to leak privileged information such as the vector base
 +	  register, resulting in a potential defeat of the EL2 layout
 +	  randomization.
 +
 +	  This config option will map the vectors to a fixed location,
 +	  independent of the EL2 code mapping, so that revealing VBAR_EL2
 +	  to an attacker does not give away any extra information. This
 +	  only gets enabled on affected CPUs.
 +
 +	  If unsure, say Y.
++=======
+ config MITIGATE_SPECTRE_BRANCH_HISTORY
+ 	bool "Mitigate Spectre style attacks against branch history" if EXPERT
+ 	default y
+ 	help
+ 	  Speculation attacks against some high-performance processors can
+ 	  make use of branch history to influence future speculation.
+ 	  When taking an exception from user-space, a sequence of branches
+ 	  or a firmware call overwrites the branch history.
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  
  config RODATA_FULL_DEFAULT_ENABLED
  	bool "Apply r/o permissions of VM areas also to their linear aliases"
diff --cc arch/arm64/include/asm/assembler.h
index 0e63a57c6a05,5cc97130843f..000000000000
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@@ -678,73 -737,153 +678,116 @@@ USER(\label, ic	ivau, \tmp2)			// inval
  	.endm
  
  /*
 - * Set SCTLR_ELx to the @reg value, and invalidate the local icache
 - * in the process. This is called when setting the MMU on.
 + * Check whether to yield to another runnable task from kernel mode NEON code
 + * (which runs with preemption disabled).
 + *
 + * if_will_cond_yield_neon
 + *        // pre-yield patchup code
 + * do_cond_yield_neon
 + *        // post-yield patchup code
 + * endif_yield_neon    <label>
 + *
 + * where <label> is optional, and marks the point where execution will resume
 + * after a yield has been performed. If omitted, execution resumes right after
 + * the endif_yield_neon invocation. Note that the entire sequence, including
 + * the provided patchup code, will be omitted from the image if CONFIG_PREEMPT
 + * is not defined.
 + *
 + * As a convenience, in the case where no patchup code is required, the above
 + * sequence may be abbreviated to
 + *
 + * cond_yield_neon <label>
 + *
 + * Note that the patchup code does not support assembler directives that change
 + * the output section, any use of such directives is undefined.
 + *
 + * The yield itself consists of the following:
 + * - Check whether the preempt count is exactly 1 and a reschedule is also
 + *   needed. If so, calling of preempt_enable() in kernel_neon_end() will
 + *   trigger a reschedule. If it is not the case, yielding is pointless.
 + * - Disable and re-enable kernel mode NEON, and branch to the yield fixup
 + *   code.
 + *
 + * This macro sequence may clobber all CPU state that is not guaranteed by the
 + * AAPCS to be preserved across an ordinary function call.
   */
 -.macro set_sctlr, sreg, reg
 -	msr	\sreg, \reg
 -	isb
 -	/*
 -	 * Invalidate the local I-cache so that any instructions fetched
 -	 * speculatively from the PoC are discarded, since they may have
 -	 * been dynamically patched at the PoU.
 -	 */
 -	ic	iallu
 -	dsb	nsh
 -	isb
 -.endm
  
 -.macro set_sctlr_el1, reg
 -	set_sctlr sctlr_el1, \reg
 -.endm
 -
 -.macro set_sctlr_el2, reg
 -	set_sctlr sctlr_el2, \reg
 -.endm
 -
 -	/*
 -	 * Check whether preempt/bh-disabled asm code should yield as soon as
 -	 * it is able. This is the case if we are currently running in task
 -	 * context, and either a softirq is pending, or the TIF_NEED_RESCHED
 -	 * flag is set and re-enabling preemption a single time would result in
 -	 * a preempt count of zero. (Note that the TIF_NEED_RESCHED flag is
 -	 * stored negated in the top word of the thread_info::preempt_count
 -	 * field)
 -	 */
 -	.macro		cond_yield, lbl:req, tmp:req, tmp2:req
 -	get_current_task \tmp
 -	ldr		\tmp, [\tmp, #TSK_TI_PREEMPT]
 -	/*
 -	 * If we are serving a softirq, there is no point in yielding: the
 -	 * softirq will not be preempted no matter what we do, so we should
 -	 * run to completion as quickly as we can.
 -	 */
 -	tbnz		\tmp, #SOFTIRQ_SHIFT, .Lnoyield_\@
 -#ifdef CONFIG_PREEMPTION
 -	sub		\tmp, \tmp, #PREEMPT_DISABLE_OFFSET
 -	cbz		\tmp, \lbl
 -#endif
 -	adr_l		\tmp, irq_stat + IRQ_CPUSTAT_SOFTIRQ_PENDING
 -	get_this_cpu_offset	\tmp2
 -	ldr		w\tmp, [\tmp, \tmp2]
 -	cbnz		w\tmp, \lbl	// yield on pending softirq in task context
 -.Lnoyield_\@:
 -	.endm
 -
 -/*
 - * Branch Target Identifier (BTI)
 - */
 -	.macro  bti, targets
 -	.equ	.L__bti_targets_c, 34
 -	.equ	.L__bti_targets_j, 36
 -	.equ	.L__bti_targets_jc,38
 -	hint	#.L__bti_targets_\targets
 +	.macro		cond_yield_neon, lbl
 +	if_will_cond_yield_neon
 +	do_cond_yield_neon
 +	endif_yield_neon	\lbl
  	.endm
  
 -/*
 - * This macro emits a program property note section identifying
 - * architecture features which require special handling, mainly for
 - * use in assembly files included in the VDSO.
 - */
 -
 -#define NT_GNU_PROPERTY_TYPE_0  5
 -#define GNU_PROPERTY_AARCH64_FEATURE_1_AND      0xc0000000
 -
 -#define GNU_PROPERTY_AARCH64_FEATURE_1_BTI      (1U << 0)
 -#define GNU_PROPERTY_AARCH64_FEATURE_1_PAC      (1U << 1)
 -
 -#ifdef CONFIG_ARM64_BTI_KERNEL
 -#define GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT		\
 -		((GNU_PROPERTY_AARCH64_FEATURE_1_BTI |	\
 -		  GNU_PROPERTY_AARCH64_FEATURE_1_PAC))
 +	.macro		if_will_cond_yield_neon
 +#ifdef CONFIG_PREEMPT
 +	get_current_task	x0
 +	ldr		x0, [x0, #TSK_TI_PREEMPT]
 +	sub		x0, x0, #PREEMPT_DISABLE_OFFSET
 +	cbz		x0, .Lyield_\@
 +	/* fall through to endif_yield_neon */
 +	.subsection	1
 +.Lyield_\@ :
 +#else
++<<<<<<< HEAD
 +	.section	".discard.cond_yield_neon", "ax"
  #endif
 +	.endm
  
 -#ifdef GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT
 -.macro emit_aarch64_feature_1_and, feat=GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT
 -	.pushsection .note.gnu.property, "a"
 -	.align  3
 -	.long   2f - 1f
 -	.long   6f - 3f
 -	.long   NT_GNU_PROPERTY_TYPE_0
 -1:      .string "GNU"
 -2:
 -	.align  3
 -3:      .long   GNU_PROPERTY_AARCH64_FEATURE_1_AND
 -	.long   5f - 4f
 -4:
 -	/*
 -	 * This is described with an array of char in the Linux API
 -	 * spec but the text and all other usage (including binutils,
 -	 * clang and GCC) treat this as a 32 bit value so no swizzling
 -	 * is required for big endian.
 -	 */
 -	.long   \feat
 -5:
 -	.align  3
 -6:
 -	.popsection
 -.endm
 -
 -#else
 +	.macro		do_cond_yield_neon
 +	bl		kernel_neon_end
 +	bl		kernel_neon_begin
++=======
+ .macro emit_aarch64_feature_1_and, feat=0
+ .endm
+ 
+ #endif /* GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT */
+ 
+ 	.macro __mitigate_spectre_bhb_loop      tmp
+ #ifdef CONFIG_MITIGATE_SPECTRE_BRANCH_HISTORY
+ alternative_cb  spectre_bhb_patch_loop_iter
+ 	mov	\tmp, #32		// Patched to correct the immediate
+ alternative_cb_end
+ .Lspectre_bhb_loop\@:
+ 	b	. + 4
+ 	subs	\tmp, \tmp, #1
+ 	b.ne	.Lspectre_bhb_loop\@
+ 	sb
+ #endif /* CONFIG_MITIGATE_SPECTRE_BRANCH_HISTORY */
+ 	.endm
+ 
+ 	.macro mitigate_spectre_bhb_loop	tmp
+ #ifdef CONFIG_MITIGATE_SPECTRE_BRANCH_HISTORY
+ alternative_cb	spectre_bhb_patch_loop_mitigation_enable
+ 	b	.L_spectre_bhb_loop_done\@	// Patched to NOP
+ alternative_cb_end
+ 	__mitigate_spectre_bhb_loop	\tmp
+ .L_spectre_bhb_loop_done\@:
+ #endif /* CONFIG_MITIGATE_SPECTRE_BRANCH_HISTORY */
+ 	.endm
+ 
+ 	/* Save/restores x0-x3 to the stack */
+ 	.macro __mitigate_spectre_bhb_fw
+ #ifdef CONFIG_MITIGATE_SPECTRE_BRANCH_HISTORY
+ 	stp	x0, x1, [sp, #-16]!
+ 	stp	x2, x3, [sp, #-16]!
+ 	mov	w0, #ARM_SMCCC_ARCH_WORKAROUND_3
+ alternative_cb	smccc_patch_fw_mitigation_conduit
+ 	nop					// Patched to SMC/HVC #0
+ alternative_cb_end
+ 	ldp	x2, x3, [sp], #16
+ 	ldp	x0, x1, [sp], #16
+ #endif /* CONFIG_MITIGATE_SPECTRE_BRANCH_HISTORY */
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  	.endm
 +
 +	.macro		endif_yield_neon, lbl
 +	.ifnb		\lbl
 +	b		\lbl
 +	.else
 +	b		.Lyield_out_\@
 +	.endif
 +	.previous
 +.Lyield_out_\@ :
 +	.endm
 +
  #endif	/* __ASM_ASSEMBLER_H */
diff --cc arch/arm64/include/asm/cpufeature.h
index 9935e9521383,b158fd447f3a..000000000000
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@@ -609,9 -637,31 +609,31 @@@ static inline bool cpu_supports_mixed_e
  	return id_aa64mmfr0_mixed_endian_el0(read_cpuid(ID_AA64MMFR0_EL1));
  }
  
++<<<<<<< HEAD
++=======
+ 
+ static inline bool supports_csv2p3(int scope)
+ {
+ 	u64 pfr0;
+ 	u8 csv2_val;
+ 
+ 	if (scope == SCOPE_LOCAL_CPU)
+ 		pfr0 = read_sysreg_s(SYS_ID_AA64PFR0_EL1);
+ 	else
+ 		pfr0 = read_sanitised_ftr_reg(SYS_ID_AA64PFR0_EL1);
+ 
+ 	csv2_val = cpuid_feature_extract_unsigned_field(pfr0,
+ 							ID_AA64PFR0_CSV2_SHIFT);
+ 	return csv2_val == 3;
+ }
+ 
+ const struct cpumask *system_32bit_el0_cpumask(void);
+ DECLARE_STATIC_KEY_FALSE(arm64_mismatched_32bit_el0);
+ 
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  static inline bool system_supports_32bit_el0(void)
  {
 -	u64 pfr0 = read_sanitised_ftr_reg(SYS_ID_AA64PFR0_EL1);
 -
 -	return static_branch_unlikely(&arm64_mismatched_32bit_el0) ||
 -	       id_aa64pfr0_32bit_el0(pfr0);
 +	return cpus_have_const_cap(ARM64_HAS_32BIT_EL0);
  }
  
  static inline bool system_supports_4kb_granule(void)
diff --cc arch/arm64/include/asm/cputype.h
index a9e98b04b606,bfbf0c4c7c5e..000000000000
--- a/arch/arm64/include/asm/cputype.h
+++ b/arch/arm64/include/asm/cputype.h
@@@ -83,6 -73,10 +83,13 @@@
  #define ARM_CPU_PART_CORTEX_A76		0xD0B
  #define ARM_CPU_PART_NEOVERSE_N1	0xD0C
  #define ARM_CPU_PART_CORTEX_A77		0xD0D
++<<<<<<< HEAD
++=======
+ #define ARM_CPU_PART_NEOVERSE_V1	0xD40
+ #define ARM_CPU_PART_CORTEX_A78		0xD41
+ #define ARM_CPU_PART_CORTEX_X1		0xD44
+ #define ARM_CPU_PART_CORTEX_A510	0xD46
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  #define ARM_CPU_PART_CORTEX_A710	0xD47
  #define ARM_CPU_PART_CORTEX_X2		0xD48
  #define ARM_CPU_PART_NEOVERSE_N2	0xD49
@@@ -123,6 -121,10 +131,13 @@@
  #define MIDR_CORTEX_A76	MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A76)
  #define MIDR_NEOVERSE_N1 MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_NEOVERSE_N1)
  #define MIDR_CORTEX_A77	MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A77)
++<<<<<<< HEAD
++=======
+ #define MIDR_NEOVERSE_V1	MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_NEOVERSE_V1)
+ #define MIDR_CORTEX_A78	MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A78)
+ #define MIDR_CORTEX_X1	MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_X1)
+ #define MIDR_CORTEX_A510 MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A510)
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  #define MIDR_CORTEX_A710 MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A710)
  #define MIDR_CORTEX_X2 MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_X2)
  #define MIDR_NEOVERSE_N2 MIDR_CPU_MODEL(ARM_CPU_IMP_ARM, ARM_CPU_PART_NEOVERSE_N2)
diff --cc arch/arm64/include/asm/sysreg.h
index b1809fe256e9,238b9f6a28fc..000000000000
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@@ -810,6 -904,8 +810,11 @@@
  #endif
  
  /* id_aa64mmfr1 */
++<<<<<<< HEAD
++=======
+ #define ID_AA64MMFR1_ECBHB_SHIFT	60
+ #define ID_AA64MMFR1_AFP_SHIFT		44
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  #define ID_AA64MMFR1_ETS_SHIFT		36
  #define ID_AA64MMFR1_TWED_SHIFT		32
  #define ID_AA64MMFR1_XNX_SHIFT		28
diff --cc arch/arm64/kernel/image-vars.h
index 6b3f280bb1b2,56db964a4f0c..000000000000
--- a/arch/arm64/kernel/image-vars.h
+++ b/arch/arm64/kernel/image-vars.h
@@@ -58,24 -61,20 +58,32 @@@ __efistub_screen_info		= screen_info
   * memory mappings.
   */
  
 +#define KVM_NVHE_ALIAS(sym) __kvm_nvhe_##sym = sym;
 +
  /* Alternative callbacks for init-time patching of nVHE hyp code. */
 +KVM_NVHE_ALIAS(arm64_enable_wa2_handling);
  KVM_NVHE_ALIAS(kvm_patch_vector_branch);
  KVM_NVHE_ALIAS(kvm_update_va_mask);
++<<<<<<< HEAD
++=======
+ KVM_NVHE_ALIAS(kvm_get_kimage_voffset);
+ KVM_NVHE_ALIAS(kvm_compute_final_ctr_el0);
+ KVM_NVHE_ALIAS(spectre_bhb_patch_loop_iter);
+ KVM_NVHE_ALIAS(spectre_bhb_patch_loop_mitigation_enable);
+ KVM_NVHE_ALIAS(spectre_bhb_patch_wa3);
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  
  /* Global kernel state accessed by nVHE hyp code. */
 +KVM_NVHE_ALIAS(arm64_ssbd_callback_required);
 +KVM_NVHE_ALIAS(kvm_host_data);
  KVM_NVHE_ALIAS(kvm_vgic_global_state);
  
 +/* Kernel constant needed to compute idmap addresses. */
 +KVM_NVHE_ALIAS(kimage_voffset);
 +
  /* Kernel symbols used to call panic() from nVHE hyp code (via ERET). */
 -KVM_NVHE_ALIAS(nvhe_hyp_panic_handler);
 +KVM_NVHE_ALIAS(__hyp_panic_string);
 +KVM_NVHE_ALIAS(panic);
  
  /* Vectors installed by hyp-init on reset HVC. */
  KVM_NVHE_ALIAS(__hyp_stub_vectors);
diff --cc arch/arm64/kvm/hyp/hyp-entry.S
index 678b9b6d6a8f,12aa49324955..000000000000
--- a/arch/arm64/kvm/hyp/hyp-entry.S
+++ b/arch/arm64/kvm/hyp/hyp-entry.S
@@@ -125,35 -62,12 +125,39 @@@ el1_hvc_guest
  	/* ARM_SMCCC_ARCH_WORKAROUND_2 handling */
  	eor	w1, w1, #(ARM_SMCCC_ARCH_WORKAROUND_1 ^ \
  			  ARM_SMCCC_ARCH_WORKAROUND_2)
+ 	cbz	w1, wa_epilogue
+ 
+ 	eor	w1, w1, #(ARM_SMCCC_ARCH_WORKAROUND_2 ^ \
+ 			  ARM_SMCCC_ARCH_WORKAROUND_3)
  	cbnz	w1, el1_trap
  
 +alternative_cb	arm64_enable_wa2_handling
 +	b	wa2_end
 +alternative_cb_end
 +	get_vcpu_ptr	x2, x0
 +	ldr	x0, [x2, #VCPU_WORKAROUND_FLAGS]
 +
 +	// Sanitize the argument and update the guest flags
 +	ldr	x1, [sp, #8]			// Guest's x1
 +	clz	w1, w1				// Murphy's device:
 +	lsr	w1, w1, #5			// w1 = !!w1 without using
 +	eor	w1, w1, #1			// the flags...
 +	bfi	x0, x1, #VCPU_WORKAROUND_2_FLAG_SHIFT, #1
 +	str	x0, [x2, #VCPU_WORKAROUND_FLAGS]
 +
 +	/* Check that we actually need to perform the call */
 +	hyp_ldr_this_cpu x0, arm64_ssbd_callback_required, x2
 +	cbz	x0, wa2_end
 +
 +	mov	w0, #ARM_SMCCC_ARCH_WORKAROUND_2
 +	smc	#0
 +
 +	/* Don't leak data from the SMC call */
 +	mov	x3, xzr
 +wa2_end:
 +	mov	x2, xzr
 +	mov	x1, xzr
 +
  wa_epilogue:
  	mov	x0, xzr
  	add	sp, sp, #16
@@@ -297,44 -192,55 +301,87 @@@ SYM_CODE_START(__kvm_hyp_vector
  	valid_vect	el1_error		// Error 32-bit EL1
  SYM_CODE_END(__kvm_hyp_vector)
  
++<<<<<<< HEAD
 +#ifdef CONFIG_KVM_INDIRECT_VECTORS
 +.macro hyp_ventry
 +	.align 7
++=======
+ .macro spectrev2_smccc_wa1_smc
+ 	sub	sp, sp, #(8 * 4)
+ 	stp	x2, x3, [sp, #(8 * 0)]
+ 	stp	x0, x1, [sp, #(8 * 2)]
+ 	alternative_cb spectre_bhb_patch_wa3
+ 	/* Patched to mov WA3 when supported */
+ 	mov	w0, #ARM_SMCCC_ARCH_WORKAROUND_1
+ 	alternative_cb_end
+ 	smc	#0
+ 	ldp	x2, x3, [sp, #(8 * 0)]
+ 	add	sp, sp, #(8 * 2)
+ .endm
+ 
+ .macro hyp_ventry	indirect, spectrev2
+ 	.align	7
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  1:	esb
 -	.if \spectrev2 != 0
 -	spectrev2_smccc_wa1_smc
 -	.else
 +	.rept 26
 +	nop
 +	.endr
 +/*
 + * The default sequence is to directly branch to the KVM vectors,
 + * using the computed offset. This applies for VHE as well as
 + * !ARM64_HARDEN_EL2_VECTORS. The first vector must always run the preamble.
 + *
 + * For ARM64_HARDEN_EL2_VECTORS configurations, this gets replaced
 + * with:
 + *
 + * stp	x0, x1, [sp, #-16]!
 + * movz	x0, #(addr & 0xffff)
 + * movk	x0, #((addr >> 16) & 0xffff), lsl #16
 + * movk	x0, #((addr >> 32) & 0xffff), lsl #32
 + * br	x0
 + *
 + * Where:
 + * addr = kern_hyp_va(__kvm_hyp_vector) + vector-offset + KVM_VECTOR_PREAMBLE.
 + * See kvm_patch_vector_branch for details.
 + */
 +alternative_cb	kvm_patch_vector_branch
  	stp	x0, x1, [sp, #-16]!
++<<<<<<< HEAD
++=======
+ 	mitigate_spectre_bhb_loop	x0
+ 	.endif
+ 	.if \indirect != 0
+ 	alternative_cb  kvm_patch_vector_branch
+ 	/*
+ 	 * For ARM64_SPECTRE_V3A configurations, these NOPs get replaced with:
+ 	 *
+ 	 * movz	x0, #(addr & 0xffff)
+ 	 * movk	x0, #((addr >> 16) & 0xffff), lsl #16
+ 	 * movk	x0, #((addr >> 32) & 0xffff), lsl #32
+ 	 * br	x0
+ 	 *
+ 	 * Where:
+ 	 * addr = kern_hyp_va(__kvm_hyp_vector) + vector-offset + KVM_VECTOR_PREAMBLE.
+ 	 * See kvm_patch_vector_branch for details.
+ 	 */
+ 	nop
+ 	nop
+ 	nop
+ 	nop
+ 	alternative_cb_end
+ 	.endif
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  	b	__kvm_hyp_vector + (1b - 0b + KVM_VECTOR_PREAMBLE)
 +	nop
 +	nop
 +	nop
 +alternative_cb_end
  .endm
  
 -.macro generate_vectors	indirect, spectrev2
 +.macro generate_vectors
  0:
  	.rept 16
 -	hyp_ventry	\indirect, \spectrev2
 +	hyp_ventry
  	.endr
  	.org 0b + SZ_2K		// Safety measure
  .endm
diff --cc arch/arm64/tools/cpucaps
index 171a0a6976ae,cea7533cb304..000000000000
--- a/arch/arm64/tools/cpucaps
+++ b/arch/arm64/tools/cpucaps
@@@ -35,12 -35,16 +35,20 @@@ HAS_S
  HAS_STAGE2_FWB
  HAS_SYSREG_GIC_CPUIF
  HAS_TLB_RANGE
 +HAS_UAO
  HAS_VIRT_HOST_EXTN
  HW_DBM
 -KVM_PROTECTED_MODE
  MISMATCHED_CACHE_TYPE
  MTE
++<<<<<<< HEAD
 +SSBD
++=======
+ MTE_ASYMM
+ SPECTRE_V2
+ SPECTRE_V3A
+ SPECTRE_V4
+ SPECTRE_BHB
++>>>>>>> 558c303c9734 (arm64: Mitigate spectre style branch history side channels)
  SSBS
  SVE
  UNMAP_KERNEL_AT_EL0
* Unmerged path arch/arm64/include/asm/spectre.h
* Unmerged path arch/arm64/include/asm/vectors.h
* Unmerged path arch/arm64/kernel/proton-pack.c
* Unmerged path arch/arm64/Kconfig
* Unmerged path arch/arm64/include/asm/assembler.h
* Unmerged path arch/arm64/include/asm/cpufeature.h
* Unmerged path arch/arm64/include/asm/cputype.h
* Unmerged path arch/arm64/include/asm/spectre.h
* Unmerged path arch/arm64/include/asm/sysreg.h
* Unmerged path arch/arm64/include/asm/vectors.h
diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index da606a914429..e879cb1ea612 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -923,6 +923,13 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.cpu_enable = cpu_enable_ssbd_mitigation,
 		.midr_range_list = arm64_ssb_cpus,
 	},
+	{
+		.desc = "Spectre-BHB",
+		.capability = ARM64_SPECTRE_BHB,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = is_spectre_bhb_affected,
+		.cpu_enable = spectre_bhb_enable_mitigation,
+	},
 #ifdef CONFIG_ARM64_ERRATUM_1418040
 	{
 		.desc = "ARM erratum 1418040",
* Unmerged path arch/arm64/kernel/image-vars.h
* Unmerged path arch/arm64/kernel/proton-pack.c
* Unmerged path arch/arm64/kvm/hyp/hyp-entry.S
* Unmerged path arch/arm64/tools/cpucaps
