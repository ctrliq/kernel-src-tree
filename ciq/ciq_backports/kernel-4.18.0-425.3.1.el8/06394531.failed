KVM: arm64: Generalise VM features into a set of flags

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Marc Zyngier <maz@kernel.org>
commit 06394531b425794dc56f3d525b7994d25b8072f7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/06394531.failed

We currently deal with a set of booleans for VM features,
while they could be better represented as set of flags
contained in an unsigned long, similarily to what we are
doing on the CPU side.

	Signed-off-by: Marc Zyngier <maz@kernel.org>
[Oliver: Flag-ify the 'ran_once' boolean]
	Signed-off-by: Oliver Upton <oupton@google.com>
	Signed-off-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20220311174001.605719-2-oupton@google.com
(cherry picked from commit 06394531b425794dc56f3d525b7994d25b8072f7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/kvm_host.h
#	arch/arm64/kvm/arm.c
#	arch/arm64/kvm/pmu-emul.c
diff --cc arch/arm64/include/asm/kvm_host.h
index 904c61e9d332,0e96087885fe..000000000000
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@@ -105,7 -122,24 +105,28 @@@ struct kvm_arch 
  	 * should) opt in to this feature if KVM_CAP_ARM_NISV_TO_USER is
  	 * supported.
  	 */
++<<<<<<< HEAD
 +	bool return_nisv_io_abort_to_user;
++=======
+ #define KVM_ARCH_FLAG_RETURN_NISV_IO_ABORT_TO_USER	0
+ 	/* Memory Tagging Extension enabled for the guest */
+ #define KVM_ARCH_FLAG_MTE_ENABLED			1
+ 	/* At least one vCPU has ran in the VM */
+ #define KVM_ARCH_FLAG_HAS_RAN_ONCE			2
+ 	unsigned long flags;
+ 
+ 	/*
+ 	 * VM-wide PMU filter, implemented as a bitmap and big enough for
+ 	 * up to 2^10 events (ARMv8.0) or 2^16 events (ARMv8.1+).
+ 	 */
+ 	unsigned long *pmu_filter;
+ 	struct arm_pmu *arm_pmu;
+ 
+ 	cpumask_var_t supported_cpus;
+ 
+ 	u8 pfr0_csv2;
+ 	u8 pfr0_csv3;
++>>>>>>> 06394531b425 (KVM: arm64: Generalise VM features into a set of flags)
  };
  
  struct kvm_vcpu_fault_info {
@@@ -686,6 -811,9 +707,12 @@@ bool kvm_arm_vcpu_is_finalized(struct k
  #define kvm_arm_vcpu_sve_finalized(vcpu) \
  	((vcpu)->arch.flags & KVM_ARM64_VCPU_SVE_FINALIZED)
  
++<<<<<<< HEAD
++=======
+ #define kvm_has_mte(kvm)					\
+ 	(system_supports_mte() &&				\
+ 	 test_bit(KVM_ARCH_FLAG_MTE_ENABLED, &(kvm)->arch.flags))
++>>>>>>> 06394531b425 (KVM: arm64: Generalise VM features into a set of flags)
  #define kvm_vcpu_has_pmu(vcpu)					\
  	(test_bit(KVM_ARM_VCPU_PMU_V3, (vcpu)->arch.features))
  
diff --cc arch/arm64/kvm/arm.c
index 60a64e83f8da,17021bc8ee2c..000000000000
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@@ -97,8 -84,19 +97,22 @@@ int kvm_vm_ioctl_enable_cap(struct kvm 
  	switch (cap->cap) {
  	case KVM_CAP_ARM_NISV_TO_USER:
  		r = 0;
- 		kvm->arch.return_nisv_io_abort_to_user = true;
+ 		set_bit(KVM_ARCH_FLAG_RETURN_NISV_IO_ABORT_TO_USER,
+ 			&kvm->arch.flags);
  		break;
++<<<<<<< HEAD
++=======
+ 	case KVM_CAP_ARM_MTE:
+ 		mutex_lock(&kvm->lock);
+ 		if (!system_supports_mte() || kvm->created_vcpus) {
+ 			r = -EINVAL;
+ 		} else {
+ 			r = 0;
+ 			set_bit(KVM_ARCH_FLAG_MTE_ENABLED, &kvm->arch.flags);
+ 		}
+ 		mutex_unlock(&kvm->lock);
+ 		break;
++>>>>>>> 06394531b425 (KVM: arm64: Generalise VM features into a set of flags)
  	default:
  		r = -EINVAL;
  		break;
@@@ -566,7 -551,17 +580,21 @@@ static int kvm_vcpu_first_run_init(stru
  		static_branch_inc(&userspace_irqchip_in_use);
  	}
  
++<<<<<<< HEAD
 +	vcpu->arch.has_run_once = true;
++=======
+ 	/*
+ 	 * Initialize traps for protected VMs.
+ 	 * NOTE: Move to run in EL2 directly, rather than via a hypercall, once
+ 	 * the code is in place for first run initialization at EL2.
+ 	 */
+ 	if (kvm_vm_is_protected(kvm))
+ 		kvm_call_hyp_nvhe(__pkvm_vcpu_init_traps, vcpu);
+ 
+ 	mutex_lock(&kvm->lock);
+ 	set_bit(KVM_ARCH_FLAG_HAS_RAN_ONCE, &kvm->arch.flags);
+ 	mutex_unlock(&kvm->lock);
++>>>>>>> 06394531b425 (KVM: arm64: Generalise VM features into a set of flags)
  
  	return ret;
  }
diff --cc arch/arm64/kvm/pmu-emul.c
index a4e7294c3e8c,78fdc443adc7..000000000000
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@@ -824,9 -948,42 +824,43 @@@ static bool pmu_irq_is_valid(struct kv
  	return true;
  }
  
++<<<<<<< HEAD
++=======
+ static int kvm_arm_pmu_v3_set_pmu(struct kvm_vcpu *vcpu, int pmu_id)
+ {
+ 	struct kvm *kvm = vcpu->kvm;
+ 	struct arm_pmu_entry *entry;
+ 	struct arm_pmu *arm_pmu;
+ 	int ret = -ENXIO;
+ 
+ 	mutex_lock(&kvm->lock);
+ 	mutex_lock(&arm_pmus_lock);
+ 
+ 	list_for_each_entry(entry, &arm_pmus, entry) {
+ 		arm_pmu = entry->arm_pmu;
+ 		if (arm_pmu->pmu.type == pmu_id) {
+ 			if (test_bit(KVM_ARCH_FLAG_HAS_RAN_ONCE, &kvm->arch.flags) ||
+ 			    (kvm->arch.pmu_filter && kvm->arch.arm_pmu != arm_pmu)) {
+ 				ret = -EBUSY;
+ 				break;
+ 			}
+ 
+ 			kvm->arch.arm_pmu = arm_pmu;
+ 			cpumask_copy(kvm->arch.supported_cpus, &arm_pmu->supported_cpus);
+ 			ret = 0;
+ 			break;
+ 		}
+ 	}
+ 
+ 	mutex_unlock(&arm_pmus_lock);
+ 	mutex_unlock(&kvm->lock);
+ 	return ret;
+ }
+ 
++>>>>>>> 06394531b425 (KVM: arm64: Generalise VM features into a set of flags)
  int kvm_arm_pmu_v3_set_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
  {
 -	struct kvm *kvm = vcpu->kvm;
 -
 -	if (!kvm_vcpu_has_pmu(vcpu))
 +	if (!kvm_arm_support_pmu_v3() || !kvm_vcpu_has_pmu(vcpu))
  		return -ENODEV;
  
  	if (vcpu->arch.pmu.created)
@@@ -857,6 -1025,67 +891,70 @@@
  		vcpu->arch.pmu.irq_num = irq;
  		return 0;
  	}
++<<<<<<< HEAD
++=======
+ 	case KVM_ARM_VCPU_PMU_V3_FILTER: {
+ 		struct kvm_pmu_event_filter __user *uaddr;
+ 		struct kvm_pmu_event_filter filter;
+ 		int nr_events;
+ 
+ 		nr_events = kvm_pmu_event_mask(kvm) + 1;
+ 
+ 		uaddr = (struct kvm_pmu_event_filter __user *)(long)attr->addr;
+ 
+ 		if (copy_from_user(&filter, uaddr, sizeof(filter)))
+ 			return -EFAULT;
+ 
+ 		if (((u32)filter.base_event + filter.nevents) > nr_events ||
+ 		    (filter.action != KVM_PMU_EVENT_ALLOW &&
+ 		     filter.action != KVM_PMU_EVENT_DENY))
+ 			return -EINVAL;
+ 
+ 		mutex_lock(&kvm->lock);
+ 
+ 		if (test_bit(KVM_ARCH_FLAG_HAS_RAN_ONCE, &kvm->arch.flags)) {
+ 			mutex_unlock(&kvm->lock);
+ 			return -EBUSY;
+ 		}
+ 
+ 		if (!kvm->arch.pmu_filter) {
+ 			kvm->arch.pmu_filter = bitmap_alloc(nr_events, GFP_KERNEL_ACCOUNT);
+ 			if (!kvm->arch.pmu_filter) {
+ 				mutex_unlock(&kvm->lock);
+ 				return -ENOMEM;
+ 			}
+ 
+ 			/*
+ 			 * The default depends on the first applied filter.
+ 			 * If it allows events, the default is to deny.
+ 			 * Conversely, if the first filter denies a set of
+ 			 * events, the default is to allow.
+ 			 */
+ 			if (filter.action == KVM_PMU_EVENT_ALLOW)
+ 				bitmap_zero(kvm->arch.pmu_filter, nr_events);
+ 			else
+ 				bitmap_fill(kvm->arch.pmu_filter, nr_events);
+ 		}
+ 
+ 		if (filter.action == KVM_PMU_EVENT_ALLOW)
+ 			bitmap_set(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+ 		else
+ 			bitmap_clear(kvm->arch.pmu_filter, filter.base_event, filter.nevents);
+ 
+ 		mutex_unlock(&kvm->lock);
+ 
+ 		return 0;
+ 	}
+ 	case KVM_ARM_VCPU_PMU_V3_SET_PMU: {
+ 		int __user *uaddr = (int __user *)(long)attr->addr;
+ 		int pmu_id;
+ 
+ 		if (get_user(pmu_id, uaddr))
+ 			return -EFAULT;
+ 
+ 		return kvm_arm_pmu_v3_set_pmu(vcpu, pmu_id);
+ 	}
++>>>>>>> 06394531b425 (KVM: arm64: Generalise VM features into a set of flags)
  	case KVM_ARM_VCPU_PMU_V3_INIT:
  		return kvm_arm_pmu_v3_init(vcpu);
  	}
* Unmerged path arch/arm64/include/asm/kvm_host.h
* Unmerged path arch/arm64/kvm/arm.c
diff --git a/arch/arm64/kvm/mmio.c b/arch/arm64/kvm/mmio.c
index cef5ee70c616..5875e3002495 100644
--- a/arch/arm64/kvm/mmio.c
+++ b/arch/arm64/kvm/mmio.c
@@ -147,7 +147,8 @@ int io_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 	 * volunteered to do so, and bail out otherwise.
 	 */
 	if (!kvm_vcpu_dabt_isvalid(vcpu)) {
-		if (vcpu->kvm->arch.return_nisv_io_abort_to_user) {
+		if (test_bit(KVM_ARCH_FLAG_RETURN_NISV_IO_ABORT_TO_USER,
+			     &vcpu->kvm->arch.flags)) {
 			run->exit_reason = KVM_EXIT_ARM_NISV;
 			run->arm_nisv.esr_iss = kvm_vcpu_dabt_iss_nisv_sanitized(vcpu);
 			run->arm_nisv.fault_ipa = fault_ipa;
* Unmerged path arch/arm64/kvm/pmu-emul.c
