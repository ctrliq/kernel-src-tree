mm: x86: Invoke hypercall when page encryption status is changed

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Brijesh Singh <brijesh.singh@amd.com>
commit 064ce6c550a0630789978bfec7a13ab2bd1bdcdf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/064ce6c5.failed

Invoke a hypercall when a memory region is changed from encrypted ->
decrypted and vice versa. Hypervisor needs to know the page encryption
status during the guest migration.

	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Paolo Bonzini <pbonzini@redhat.com>
	Cc: Joerg Roedel <joro@8bytes.org>
	Cc: Borislav Petkov <bp@suse.de>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: x86@kernel.org
	Cc: kvm@vger.kernel.org
	Cc: linux-kernel@vger.kernel.org
	Reviewed-by: Steve Rutherford <srutherford@google.com>
	Reviewed-by: Venu Busireddy <venu.busireddy@oracle.com>
	Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
	Signed-off-by: Ashish Kalra <ashish.kalra@amd.com>
	Reviewed-by: Borislav Petkov <bp@suse.de>
Message-Id: <0a237d5bb08793916c7790a3e653a2cbe7485761.1629726117.git.ashish.kalra@amd.com>
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 064ce6c550a0630789978bfec7a13ab2bd1bdcdf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/paravirt.h
#	arch/x86/include/asm/paravirt_types.h
#	arch/x86/kernel/paravirt.c
diff --cc arch/x86/include/asm/paravirt.h
index e9f118dae8cc,540bf8cb37db..000000000000
--- a/arch/x86/include/asm/paravirt.h
+++ b/arch/x86/include/asm/paravirt.h
@@@ -15,11 -15,98 +15,100 @@@
  #include <linux/bug.h>
  #include <linux/types.h>
  #include <linux/cpumask.h>
 -#include <linux/static_call_types.h>
  #include <asm/frame.h>
  
++<<<<<<< HEAD
++=======
+ u64 dummy_steal_clock(int cpu);
+ u64 dummy_sched_clock(void);
+ 
+ DECLARE_STATIC_CALL(pv_steal_clock, dummy_steal_clock);
+ DECLARE_STATIC_CALL(pv_sched_clock, dummy_sched_clock);
+ 
+ void paravirt_set_sched_clock(u64 (*func)(void));
+ 
+ static inline u64 paravirt_sched_clock(void)
+ {
+ 	return static_call(pv_sched_clock)();
+ }
+ 
+ struct static_key;
+ extern struct static_key paravirt_steal_enabled;
+ extern struct static_key paravirt_steal_rq_enabled;
+ 
+ __visible void __native_queued_spin_unlock(struct qspinlock *lock);
+ bool pv_is_native_spin_unlock(void);
+ __visible bool __native_vcpu_is_preempted(long cpu);
+ bool pv_is_native_vcpu_is_preempted(void);
+ 
+ static inline u64 paravirt_steal_clock(int cpu)
+ {
+ 	return static_call(pv_steal_clock)(cpu);
+ }
+ 
+ #ifdef CONFIG_PARAVIRT_SPINLOCKS
+ void __init paravirt_set_cap(void);
+ #endif
+ 
+ /* The paravirtualized I/O functions */
+ static inline void slow_down_io(void)
+ {
+ 	pv_ops.cpu.io_delay();
+ #ifdef REALLY_SLOW_IO
+ 	pv_ops.cpu.io_delay();
+ 	pv_ops.cpu.io_delay();
+ 	pv_ops.cpu.io_delay();
+ #endif
+ }
+ 
+ void native_flush_tlb_local(void);
+ void native_flush_tlb_global(void);
+ void native_flush_tlb_one_user(unsigned long addr);
+ void native_flush_tlb_multi(const struct cpumask *cpumask,
+ 			     const struct flush_tlb_info *info);
+ 
+ static inline void __flush_tlb_local(void)
+ {
+ 	PVOP_VCALL0(mmu.flush_tlb_user);
+ }
+ 
+ static inline void __flush_tlb_global(void)
+ {
+ 	PVOP_VCALL0(mmu.flush_tlb_kernel);
+ }
+ 
+ static inline void __flush_tlb_one_user(unsigned long addr)
+ {
+ 	PVOP_VCALL1(mmu.flush_tlb_one_user, addr);
+ }
+ 
+ static inline void __flush_tlb_multi(const struct cpumask *cpumask,
+ 				      const struct flush_tlb_info *info)
+ {
+ 	PVOP_VCALL2(mmu.flush_tlb_multi, cpumask, info);
+ }
+ 
+ static inline void paravirt_tlb_remove_table(struct mmu_gather *tlb, void *table)
+ {
+ 	PVOP_VCALL2(mmu.tlb_remove_table, tlb, table);
+ }
+ 
+ static inline void paravirt_arch_exit_mmap(struct mm_struct *mm)
+ {
+ 	PVOP_VCALL1(mmu.exit_mmap, mm);
+ }
+ 
+ static inline void notify_page_enc_status_changed(unsigned long pfn,
+ 						  int npages, bool enc)
+ {
+ 	PVOP_VCALL3(mmu.notify_page_enc_status_changed, pfn, npages, enc);
+ }
+ 
+ #ifdef CONFIG_PARAVIRT_XXL
++>>>>>>> 064ce6c550a0 (mm: x86: Invoke hypercall when page encryption status is changed)
  static inline void load_sp0(unsigned long sp0)
  {
 -	PVOP_VCALL1(cpu.load_sp0, sp0);
 +	PVOP_VCALL1(pv_cpu_ops.load_sp0, sp0);
  }
  
  /* The paravirtualized CPUID instruction. */
diff --cc arch/x86/include/asm/paravirt_types.h
index f6865098cca5,664199820239..000000000000
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@@ -203,7 -157,21 +203,25 @@@ struct pv_irq_ops 
  } __no_randomize_layout;
  
  struct pv_mmu_ops {
++<<<<<<< HEAD
 +	unsigned long (*read_cr2)(void);
++=======
+ 	/* TLB operations */
+ 	void (*flush_tlb_user)(void);
+ 	void (*flush_tlb_kernel)(void);
+ 	void (*flush_tlb_one_user)(unsigned long addr);
+ 	void (*flush_tlb_multi)(const struct cpumask *cpus,
+ 				const struct flush_tlb_info *info);
+ 
+ 	void (*tlb_remove_table)(struct mmu_gather *tlb, void *table);
+ 
+ 	/* Hook for intercepting the destruction of an mm_struct. */
+ 	void (*exit_mmap)(struct mm_struct *mm);
+ 	void (*notify_page_enc_status_changed)(unsigned long pfn, int npages, bool enc);
+ 
+ #ifdef CONFIG_PARAVIRT_XXL
+ 	struct paravirt_callee_save read_cr2;
++>>>>>>> 064ce6c550a0 (mm: x86: Invoke hypercall when page encryption status is changed)
  	void (*write_cr2)(unsigned long);
  
  	unsigned long (*read_cr3)(void);
diff --cc arch/x86/kernel/paravirt.c
index e7adefb01b85,1cc20ac9a54f..000000000000
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@@ -320,72 -235,143 +320,209 @@@ struct pv_info pv_info = 
  #endif
  };
  
++<<<<<<< HEAD
 +struct pv_init_ops pv_init_ops = {
 +	.patch = native_patch,
 +};
 +
 +struct pv_time_ops pv_time_ops = {
 +	.sched_clock = native_sched_clock,
 +	.steal_clock = native_steal_clock,
 +};
 +
 +__visible struct pv_irq_ops pv_irq_ops = {
 +	.save_fl = __PV_IS_CALLEE_SAVE(native_save_fl),
 +	.restore_fl = __PV_IS_CALLEE_SAVE(native_restore_fl),
 +	.irq_disable = __PV_IS_CALLEE_SAVE(native_irq_disable),
 +	.irq_enable = __PV_IS_CALLEE_SAVE(native_irq_enable),
 +	.safe_halt = native_safe_halt,
 +	.halt = native_halt,
 +};
 +
 +__visible struct pv_cpu_ops pv_cpu_ops = {
 +	.cpuid = native_cpuid,
 +	.get_debugreg = native_get_debugreg,
 +	.set_debugreg = native_set_debugreg,
 +	.read_cr0 = native_read_cr0,
 +	.write_cr0 = native_write_cr0,
 +	.write_cr4 = native_write_cr4,
 +#ifdef CONFIG_X86_64
 +	.read_cr8 = native_read_cr8,
 +	.write_cr8 = native_write_cr8,
 +#endif
 +	.wbinvd = native_wbinvd,
 +	.read_msr = native_read_msr,
 +	.write_msr = native_write_msr,
 +	.read_msr_safe = native_read_msr_safe,
 +	.write_msr_safe = native_write_msr_safe,
 +	.read_pmc = native_read_pmc,
 +	.load_tr_desc = native_load_tr_desc,
 +	.set_ldt = native_set_ldt,
 +	.load_gdt = native_load_gdt,
 +	.load_idt = native_load_idt,
 +	.store_tr = native_store_tr,
 +	.load_tls = native_load_tls,
 +#ifdef CONFIG_X86_64
 +	.load_gs_index = native_load_gs_index,
 +#endif
 +	.write_ldt_entry = native_write_ldt_entry,
 +	.write_gdt_entry = native_write_gdt_entry,
 +	.write_idt_entry = native_write_idt_entry,
 +
 +	.alloc_ldt = paravirt_nop,
 +	.free_ldt = paravirt_nop,
 +
 +	.load_sp0 = native_load_sp0,
 +
 +#ifdef CONFIG_X86_64
 +	.usergs_sysret64 = native_usergs_sysret64,
 +#endif
 +	.iret = native_iret,
 +	.swapgs = native_swapgs,
 +
 +	.set_iopl_mask = native_set_iopl_mask,
 +	.io_delay = native_io_delay,
 +
 +	.start_context_switch = paravirt_nop,
 +	.end_context_switch = paravirt_nop,
++=======
+ /* 64-bit pagetable entries */
+ #define PTE_IDENT	__PV_IS_CALLEE_SAVE(_paravirt_ident_64)
+ 
+ struct paravirt_patch_template pv_ops = {
+ 	/* Cpu ops. */
+ 	.cpu.io_delay		= native_io_delay,
+ 
+ #ifdef CONFIG_PARAVIRT_XXL
+ 	.cpu.cpuid		= native_cpuid,
+ 	.cpu.get_debugreg	= native_get_debugreg,
+ 	.cpu.set_debugreg	= native_set_debugreg,
+ 	.cpu.read_cr0		= native_read_cr0,
+ 	.cpu.write_cr0		= native_write_cr0,
+ 	.cpu.write_cr4		= native_write_cr4,
+ 	.cpu.wbinvd		= native_wbinvd,
+ 	.cpu.read_msr		= native_read_msr,
+ 	.cpu.write_msr		= native_write_msr,
+ 	.cpu.read_msr_safe	= native_read_msr_safe,
+ 	.cpu.write_msr_safe	= native_write_msr_safe,
+ 	.cpu.read_pmc		= native_read_pmc,
+ 	.cpu.load_tr_desc	= native_load_tr_desc,
+ 	.cpu.set_ldt		= native_set_ldt,
+ 	.cpu.load_gdt		= native_load_gdt,
+ 	.cpu.load_idt		= native_load_idt,
+ 	.cpu.store_tr		= native_store_tr,
+ 	.cpu.load_tls		= native_load_tls,
+ 	.cpu.load_gs_index	= native_load_gs_index,
+ 	.cpu.write_ldt_entry	= native_write_ldt_entry,
+ 	.cpu.write_gdt_entry	= native_write_gdt_entry,
+ 	.cpu.write_idt_entry	= native_write_idt_entry,
+ 
+ 	.cpu.alloc_ldt		= paravirt_nop,
+ 	.cpu.free_ldt		= paravirt_nop,
+ 
+ 	.cpu.load_sp0		= native_load_sp0,
+ 
+ #ifdef CONFIG_X86_IOPL_IOPERM
+ 	.cpu.invalidate_io_bitmap	= native_tss_invalidate_io_bitmap,
+ 	.cpu.update_io_bitmap		= native_tss_update_io_bitmap,
+ #endif
+ 
+ 	.cpu.start_context_switch	= paravirt_nop,
+ 	.cpu.end_context_switch		= paravirt_nop,
+ 
+ 	/* Irq ops. */
+ 	.irq.save_fl		= __PV_IS_CALLEE_SAVE(native_save_fl),
+ 	.irq.irq_disable	= __PV_IS_CALLEE_SAVE(native_irq_disable),
+ 	.irq.irq_enable		= __PV_IS_CALLEE_SAVE(native_irq_enable),
+ 	.irq.safe_halt		= native_safe_halt,
+ 	.irq.halt		= native_halt,
+ #endif /* CONFIG_PARAVIRT_XXL */
+ 
+ 	/* Mmu ops. */
+ 	.mmu.flush_tlb_user	= native_flush_tlb_local,
+ 	.mmu.flush_tlb_kernel	= native_flush_tlb_global,
+ 	.mmu.flush_tlb_one_user	= native_flush_tlb_one_user,
+ 	.mmu.flush_tlb_multi	= native_flush_tlb_multi,
+ 	.mmu.tlb_remove_table	=
+ 			(void (*)(struct mmu_gather *, void *))tlb_remove_page,
+ 
+ 	.mmu.exit_mmap		= paravirt_nop,
+ 	.mmu.notify_page_enc_status_changed	= paravirt_nop,
+ 
+ #ifdef CONFIG_PARAVIRT_XXL
+ 	.mmu.read_cr2		= __PV_IS_CALLEE_SAVE(native_read_cr2),
+ 	.mmu.write_cr2		= native_write_cr2,
+ 	.mmu.read_cr3		= __native_read_cr3,
+ 	.mmu.write_cr3		= native_write_cr3,
+ 
+ 	.mmu.pgd_alloc		= __paravirt_pgd_alloc,
+ 	.mmu.pgd_free		= paravirt_nop,
+ 
+ 	.mmu.alloc_pte		= paravirt_nop,
+ 	.mmu.alloc_pmd		= paravirt_nop,
+ 	.mmu.alloc_pud		= paravirt_nop,
+ 	.mmu.alloc_p4d		= paravirt_nop,
+ 	.mmu.release_pte	= paravirt_nop,
+ 	.mmu.release_pmd	= paravirt_nop,
+ 	.mmu.release_pud	= paravirt_nop,
+ 	.mmu.release_p4d	= paravirt_nop,
+ 
+ 	.mmu.set_pte		= native_set_pte,
+ 	.mmu.set_pmd		= native_set_pmd,
+ 
+ 	.mmu.ptep_modify_prot_start	= __ptep_modify_prot_start,
+ 	.mmu.ptep_modify_prot_commit	= __ptep_modify_prot_commit,
+ 
+ 	.mmu.set_pud		= native_set_pud,
+ 
+ 	.mmu.pmd_val		= PTE_IDENT,
+ 	.mmu.make_pmd		= PTE_IDENT,
+ 
+ 	.mmu.pud_val		= PTE_IDENT,
+ 	.mmu.make_pud		= PTE_IDENT,
+ 
+ 	.mmu.set_p4d		= native_set_p4d,
+ 
+ #if CONFIG_PGTABLE_LEVELS >= 5
+ 	.mmu.p4d_val		= PTE_IDENT,
+ 	.mmu.make_p4d		= PTE_IDENT,
+ 
+ 	.mmu.set_pgd		= native_set_pgd,
+ #endif /* CONFIG_PGTABLE_LEVELS >= 5 */
+ 
+ 	.mmu.pte_val		= PTE_IDENT,
+ 	.mmu.pgd_val		= PTE_IDENT,
+ 
+ 	.mmu.make_pte		= PTE_IDENT,
+ 	.mmu.make_pgd		= PTE_IDENT,
+ 
+ 	.mmu.dup_mmap		= paravirt_nop,
+ 	.mmu.activate_mm	= paravirt_nop,
+ 
+ 	.mmu.lazy_mode = {
+ 		.enter		= paravirt_nop,
+ 		.leave		= paravirt_nop,
+ 		.flush		= paravirt_nop,
+ 	},
+ 
+ 	.mmu.set_fixmap		= native_set_fixmap,
+ #endif /* CONFIG_PARAVIRT_XXL */
+ 
+ #if defined(CONFIG_PARAVIRT_SPINLOCKS)
+ 	/* Lock ops. */
+ #ifdef CONFIG_SMP
+ 	.lock.queued_spin_lock_slowpath	= native_queued_spin_lock_slowpath,
+ 	.lock.queued_spin_unlock	=
+ 				PV_CALLEE_SAVE(__native_queued_spin_unlock),
+ 	.lock.wait			= paravirt_nop,
+ 	.lock.kick			= paravirt_nop,
+ 	.lock.vcpu_is_preempted		=
+ 				PV_CALLEE_SAVE(__native_vcpu_is_preempted),
+ #endif /* SMP */
+ #endif
++>>>>>>> 064ce6c550a0 (mm: x86: Invoke hypercall when page encryption status is changed)
  };
  
 -#ifdef CONFIG_PARAVIRT_XXL
  /* At this point, native_get/set_debugreg has real function entries */
  NOKPROBE_SYMBOL(native_get_debugreg);
  NOKPROBE_SYMBOL(native_set_debugreg);
* Unmerged path arch/x86/include/asm/paravirt.h
* Unmerged path arch/x86/include/asm/paravirt_types.h
diff --git a/arch/x86/include/asm/set_memory.h b/arch/x86/include/asm/set_memory.h
index f9d48d550bb1..5b7d97a7f32f 100644
--- a/arch/x86/include/asm/set_memory.h
+++ b/arch/x86/include/asm/set_memory.h
@@ -81,6 +81,7 @@ int set_pages_rw(struct page *page, int numpages);
 int set_direct_map_invalid_noflush(struct page *page);
 int set_direct_map_default_noflush(struct page *page);
 bool kernel_page_present(struct page *page);
+void notify_range_enc_status_changed(unsigned long vaddr, int npages, bool enc);
 
 extern int kernel_set_to_readonly;
 void set_kernel_text_rw(void);
* Unmerged path arch/x86/kernel/paravirt.c
diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c
index a1e274118a1f..af429afbdb2b 100644
--- a/arch/x86/mm/mem_encrypt.c
+++ b/arch/x86/mm/mem_encrypt.c
@@ -229,29 +229,76 @@ void __init sev_setup_arch(void)
 	swiotlb_adjust_size(size);
 }
 
-static void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)
+static unsigned long pg_level_to_pfn(int level, pte_t *kpte, pgprot_t *ret_prot)
 {
-	pgprot_t old_prot, new_prot;
-	unsigned long pfn, pa, size;
-	pte_t new_pte;
+	unsigned long pfn = 0;
+	pgprot_t prot;
 
 	switch (level) {
 	case PG_LEVEL_4K:
 		pfn = pte_pfn(*kpte);
-		old_prot = pte_pgprot(*kpte);
+		prot = pte_pgprot(*kpte);
 		break;
 	case PG_LEVEL_2M:
 		pfn = pmd_pfn(*(pmd_t *)kpte);
-		old_prot = pmd_pgprot(*(pmd_t *)kpte);
+		prot = pmd_pgprot(*(pmd_t *)kpte);
 		break;
 	case PG_LEVEL_1G:
 		pfn = pud_pfn(*(pud_t *)kpte);
-		old_prot = pud_pgprot(*(pud_t *)kpte);
+		prot = pud_pgprot(*(pud_t *)kpte);
 		break;
 	default:
-		return;
+		WARN_ONCE(1, "Invalid level for kpte\n");
+		return 0;
 	}
 
+	if (ret_prot)
+		*ret_prot = prot;
+
+	return pfn;
+}
+
+void notify_range_enc_status_changed(unsigned long vaddr, int npages, bool enc)
+{
+#ifdef CONFIG_PARAVIRT
+	unsigned long sz = npages << PAGE_SHIFT;
+	unsigned long vaddr_end = vaddr + sz;
+
+	while (vaddr < vaddr_end) {
+		int psize, pmask, level;
+		unsigned long pfn;
+		pte_t *kpte;
+
+		kpte = lookup_address(vaddr, &level);
+		if (!kpte || pte_none(*kpte)) {
+			WARN_ONCE(1, "kpte lookup for vaddr\n");
+			return;
+		}
+
+		pfn = pg_level_to_pfn(level, kpte, NULL);
+		if (!pfn)
+			continue;
+
+		psize = page_level_size(level);
+		pmask = page_level_mask(level);
+
+		notify_page_enc_status_changed(pfn, psize >> PAGE_SHIFT, enc);
+
+		vaddr = (vaddr & pmask) + psize;
+	}
+#endif
+}
+
+static void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)
+{
+	pgprot_t old_prot, new_prot;
+	unsigned long pfn, pa, size;
+	pte_t new_pte;
+
+	pfn = pg_level_to_pfn(level, kpte, &old_prot);
+	if (!pfn)
+		return;
+
 	new_prot = old_prot;
 	if (enc)
 		pgprot_val(new_prot) |= _PAGE_ENC;
@@ -286,12 +333,13 @@ static void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)
 static int __init early_set_memory_enc_dec(unsigned long vaddr,
 					   unsigned long size, bool enc)
 {
-	unsigned long vaddr_end, vaddr_next;
+	unsigned long vaddr_end, vaddr_next, start;
 	unsigned long psize, pmask;
 	int split_page_size_mask;
 	int level, ret;
 	pte_t *kpte;
 
+	start = vaddr;
 	vaddr_next = vaddr;
 	vaddr_end = vaddr + size;
 
@@ -346,6 +394,7 @@ static int __init early_set_memory_enc_dec(unsigned long vaddr,
 
 	ret = 0;
 
+	notify_range_enc_status_changed(start, PAGE_ALIGN(size) >> PAGE_SHIFT, enc);
 out:
 	__flush_tlb_all();
 	return ret;
diff --git a/arch/x86/mm/pat/set_memory.c b/arch/x86/mm/pat/set_memory.c
index c0225f75913c..e039445a7cfc 100644
--- a/arch/x86/mm/pat/set_memory.c
+++ b/arch/x86/mm/pat/set_memory.c
@@ -1986,6 +1986,12 @@ static int __set_memory_enc_pgtable(unsigned long addr, int numpages, bool enc)
 	 */
 	cpa_flush(&cpa, 0);
 
+	/*
+	 * Notify hypervisor that a given memory range is mapped encrypted
+	 * or decrypted.
+	 */
+	notify_range_enc_status_changed(addr, numpages, enc);
+
 	return ret;
 }
 
