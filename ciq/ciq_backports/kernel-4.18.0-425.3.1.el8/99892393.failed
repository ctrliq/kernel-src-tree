net/mlx5e: Drop cqe_bcnt32 from mlx5e_skb_from_cqe_mpwrq_linear

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Maxim Mikityanskiy <maximmi@nvidia.com>
commit 998923932f13d5694a0b6e73f9a350cc6949ac2f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/99892393.failed

The packet size in mlx5e_skb_from_cqe_mpwrq_linear can't overflow u16,
since the maximum packet size in linear striding RQ is 2^13 bytes. Drop
the unneeded u32 variable.

	Signed-off-by: Maxim Mikityanskiy <maximmi@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 998923932f13d5694a0b6e73f9a350cc6949ac2f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index b3f823cc73ba,4b8699f39200..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -1457,12 -1848,11 +1457,16 @@@ mlx5e_skb_from_cqe_mpwrq_linear(struct 
  {
  	struct mlx5e_dma_info *di = &wi->umr.dma_info[page_idx];
  	u16 rx_headroom = rq->buff.headroom;
++<<<<<<< HEAD
 +	u32 cqe_bcnt32 = cqe_bcnt;
 +	struct xdp_buff xdp;
++=======
+ 	struct bpf_prog *prog;
++>>>>>>> 998923932f13 (net/mlx5e: Drop cqe_bcnt32 from mlx5e_skb_from_cqe_mpwrq_linear)
  	struct sk_buff *skb;
 -	u32 metasize = 0;
  	void *va, *data;
  	u32 frag_size;
 +	u32 metasize;
  
  	/* Check packet size. Note LRO doesn't use linear SKB */
  	if (unlikely(cqe_bcnt > rq->hw_mtu)) {
@@@ -1476,20 -1866,26 +1480,41 @@@
  
  	dma_sync_single_range_for_cpu(rq->pdev, di->addr, head_offset,
  				      frag_size, DMA_FROM_DEVICE);
 +	net_prefetchw(va); /* xdp_frame data area */
  	net_prefetch(data);
  
++<<<<<<< HEAD
 +	mlx5e_fill_xdp_buff(rq, va, rx_headroom, cqe_bcnt32, &xdp);
 +	if (mlx5e_xdp_handle(rq, di, &cqe_bcnt32, &xdp)) {
 +		if (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags))
 +			__set_bit(page_idx, wi->xdp_xmit_bitmap); /* non-atomic */
 +		return NULL; /* page/packet was consumed by XDP */
 +	}
 +
 +	rx_headroom = xdp.data - xdp.data_hard_start;
 +	frag_size = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt32);
 +	metasize = xdp.data - xdp.data_meta;
 +	skb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt32, metasize);
++=======
+ 	prog = rcu_dereference(rq->xdp_prog);
+ 	if (prog) {
+ 		struct xdp_buff xdp;
+ 
+ 		net_prefetchw(va); /* xdp_frame data area */
+ 		mlx5e_fill_xdp_buff(rq, va, rx_headroom, cqe_bcnt, &xdp);
+ 		if (mlx5e_xdp_handle(rq, di, prog, &xdp)) {
+ 			if (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags))
+ 				__set_bit(page_idx, wi->xdp_xmit_bitmap); /* non-atomic */
+ 			return NULL; /* page/packet was consumed by XDP */
+ 		}
+ 
+ 		rx_headroom = xdp.data - xdp.data_hard_start;
+ 		metasize = xdp.data - xdp.data_meta;
+ 		cqe_bcnt = xdp.data_end - xdp.data;
+ 	}
+ 	frag_size = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);
+ 	skb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt, metasize);
++>>>>>>> 998923932f13 (net/mlx5e: Drop cqe_bcnt32 from mlx5e_skb_from_cqe_mpwrq_linear)
  	if (unlikely(!skb))
  		return NULL;
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
