net/mlx5e: SHAMPO, Fix constant expression result

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.3.1.el8
commit-author Ben Ben-Ishay <benishay@nvidia.com>
commit 8c8cf0382257b28378eeff535150c087a653ca19
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.3.1.el8/8c8cf038.failed

mlx5e_build_shampo_hd_umr uses counters i and index incorrectly
as unsigned, thus the err state err_unmap could stuck in endless loop.
Change i to int to solve the first issue.
Reduce index check to solve the second issue, the caller function
validates that index could not rotate.

Fixes: 64509b052525 ("net/mlx5e: Add data path for SHAMPO feature")
	Signed-off-by: Ben Ben-Ishay <benishay@nvidia.com>
	Reviewed-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 8c8cf0382257b28378eeff535150c087a653ca19)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 66ea1cbed1c1,793511d5ee4c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -499,6 -501,155 +499,158 @@@ static void mlx5e_post_rx_mpwqe(struct 
  	mlx5_wq_ll_update_db_record(wq);
  }
  
++<<<<<<< HEAD
++=======
+ /* This function returns the size of the continuous free space inside a bitmap
+  * that starts from first and no longer than len including circular ones.
+  */
+ static int bitmap_find_window(unsigned long *bitmap, int len,
+ 			      int bitmap_size, int first)
+ {
+ 	int next_one, count;
+ 
+ 	next_one = find_next_bit(bitmap, bitmap_size, first);
+ 	if (next_one == bitmap_size) {
+ 		if (bitmap_size - first >= len)
+ 			return len;
+ 		next_one = find_next_bit(bitmap, bitmap_size, 0);
+ 		count = next_one + bitmap_size - first;
+ 	} else {
+ 		count = next_one - first;
+ 	}
+ 
+ 	return min(len, count);
+ }
+ 
+ static void build_klm_umr(struct mlx5e_icosq *sq, struct mlx5e_umr_wqe *umr_wqe,
+ 			  __be32 key, u16 offset, u16 klm_len, u16 wqe_bbs)
+ {
+ 	memset(umr_wqe, 0, offsetof(struct mlx5e_umr_wqe, inline_klms));
+ 	umr_wqe->ctrl.opmod_idx_opcode =
+ 		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
+ 			     MLX5_OPCODE_UMR);
+ 	umr_wqe->ctrl.umr_mkey = key;
+ 	umr_wqe->ctrl.qpn_ds = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT)
+ 					    | MLX5E_KLM_UMR_DS_CNT(klm_len));
+ 	umr_wqe->uctrl.flags = MLX5_UMR_TRANSLATION_OFFSET_EN | MLX5_UMR_INLINE;
+ 	umr_wqe->uctrl.xlt_offset = cpu_to_be16(offset);
+ 	umr_wqe->uctrl.xlt_octowords = cpu_to_be16(klm_len);
+ 	umr_wqe->uctrl.mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ }
+ 
+ static int mlx5e_build_shampo_hd_umr(struct mlx5e_rq *rq,
+ 				     struct mlx5e_icosq *sq,
+ 				     u16 klm_entries, u16 index)
+ {
+ 	struct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;
+ 	u16 entries, pi, header_offset, err, wqe_bbs, new_entries;
+ 	u32 lkey = rq->mdev->mlx5e_res.hw_objs.mkey;
+ 	struct page *page = shampo->last_page;
+ 	u64 addr = shampo->last_addr;
+ 	struct mlx5e_dma_info *dma_info;
+ 	struct mlx5e_umr_wqe *umr_wqe;
+ 	int headroom, i;
+ 
+ 	headroom = rq->buff.headroom;
+ 	new_entries = klm_entries - (shampo->pi & (MLX5_UMR_KLM_ALIGNMENT - 1));
+ 	entries = ALIGN(klm_entries, MLX5_UMR_KLM_ALIGNMENT);
+ 	wqe_bbs = MLX5E_KLM_UMR_WQEBBS(entries);
+ 	pi = mlx5e_icosq_get_next_pi(sq, wqe_bbs);
+ 	umr_wqe = mlx5_wq_cyc_get_wqe(&sq->wq, pi);
+ 	build_klm_umr(sq, umr_wqe, shampo->key, index, entries, wqe_bbs);
+ 
+ 	for (i = 0; i < entries; i++, index++) {
+ 		dma_info = &shampo->info[index];
+ 		if (i >= klm_entries || (index < shampo->pi && shampo->pi - index <
+ 					 MLX5_UMR_KLM_ALIGNMENT))
+ 			goto update_klm;
+ 		header_offset = (index & (MLX5E_SHAMPO_WQ_HEADER_PER_PAGE - 1)) <<
+ 			MLX5E_SHAMPO_LOG_MAX_HEADER_ENTRY_SIZE;
+ 		if (!(header_offset & (PAGE_SIZE - 1))) {
+ 			err = mlx5e_page_alloc(rq, dma_info);
+ 			if (unlikely(err))
+ 				goto err_unmap;
+ 			addr = dma_info->addr;
+ 			page = dma_info->page;
+ 		} else {
+ 			dma_info->addr = addr + header_offset;
+ 			dma_info->page = page;
+ 		}
+ 
+ update_klm:
+ 		umr_wqe->inline_klms[i].bcount =
+ 			cpu_to_be32(MLX5E_RX_MAX_HEAD);
+ 		umr_wqe->inline_klms[i].key    = cpu_to_be32(lkey);
+ 		umr_wqe->inline_klms[i].va     =
+ 			cpu_to_be64(dma_info->addr + headroom);
+ 	}
+ 
+ 	sq->db.wqe_info[pi] = (struct mlx5e_icosq_wqe_info) {
+ 		.wqe_type	= MLX5E_ICOSQ_WQE_SHAMPO_HD_UMR,
+ 		.num_wqebbs	= wqe_bbs,
+ 		.shampo.len	= new_entries,
+ 	};
+ 
+ 	shampo->pi = (shampo->pi + new_entries) & (shampo->hd_per_wq - 1);
+ 	shampo->last_page = page;
+ 	shampo->last_addr = addr;
+ 	sq->pc += wqe_bbs;
+ 	sq->doorbell_cseg = &umr_wqe->ctrl;
+ 
+ 	return 0;
+ 
+ err_unmap:
+ 	while (--i >= 0) {
+ 		dma_info = &shampo->info[--index];
+ 		if (!(i & (MLX5E_SHAMPO_WQ_HEADER_PER_PAGE - 1))) {
+ 			dma_info->addr = ALIGN_DOWN(dma_info->addr, PAGE_SIZE);
+ 			mlx5e_page_release(rq, dma_info, true);
+ 		}
+ 	}
+ 	rq->stats->buff_alloc_err++;
+ 	return err;
+ }
+ 
+ static int mlx5e_alloc_rx_hd_mpwqe(struct mlx5e_rq *rq)
+ {
+ 	struct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;
+ 	u16 klm_entries, num_wqe, index, entries_before;
+ 	struct mlx5e_icosq *sq = rq->icosq;
+ 	int i, err, max_klm_entries, len;
+ 
+ 	max_klm_entries = MLX5E_MAX_KLM_PER_WQE(rq->mdev);
+ 	klm_entries = bitmap_find_window(shampo->bitmap,
+ 					 shampo->hd_per_wqe,
+ 					 shampo->hd_per_wq, shampo->pi);
+ 	if (!klm_entries)
+ 		return 0;
+ 
+ 	klm_entries += (shampo->pi & (MLX5_UMR_KLM_ALIGNMENT - 1));
+ 	index = ALIGN_DOWN(shampo->pi, MLX5_UMR_KLM_ALIGNMENT);
+ 	entries_before = shampo->hd_per_wq - index;
+ 
+ 	if (unlikely(entries_before < klm_entries))
+ 		num_wqe = DIV_ROUND_UP(entries_before, max_klm_entries) +
+ 			  DIV_ROUND_UP(klm_entries - entries_before, max_klm_entries);
+ 	else
+ 		num_wqe = DIV_ROUND_UP(klm_entries, max_klm_entries);
+ 
+ 	for (i = 0; i < num_wqe; i++) {
+ 		len = (klm_entries > max_klm_entries) ? max_klm_entries :
+ 							klm_entries;
+ 		if (unlikely(index + len > shampo->hd_per_wq))
+ 			len = shampo->hd_per_wq - index;
+ 		err = mlx5e_build_shampo_hd_umr(rq, sq, len, index);
+ 		if (unlikely(err))
+ 			return err;
+ 		index = (index + len) & (rq->mpwqe.shampo->hd_per_wq - 1);
+ 		klm_entries -= len;
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 8c8cf0382257 (net/mlx5e: SHAMPO, Fix constant expression result)
  static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
  {
  	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
