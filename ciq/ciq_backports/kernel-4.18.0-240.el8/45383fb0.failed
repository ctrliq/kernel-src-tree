virtio: support VIRTIO_F_ORDER_PLATFORM

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-240.el8
commit-author Tiwei Bie <tiwei.bie@intel.com>
commit 45383fb0f42db3945ac6cc658704706cdae19528
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-240.el8/45383fb0.failed

This patch introduces the support for VIRTIO_F_ORDER_PLATFORM.
If this feature is negotiated, the driver must use the barriers
suitable for hardware devices. Otherwise, the device and driver
are assumed to be implemented in software, that is they can be
assumed to run on identical CPUs in an SMP configuration. Thus
a weaker form of memory barriers is sufficient to yield better
performance.

It is recommended that an add-in card based PCI device offers
this feature for portability. The device will fail to operate
further or will operate in a slower emulation mode if this
feature is offered but not accepted.

	Signed-off-by: Tiwei Bie <tiwei.bie@intel.com>
	Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
(cherry picked from commit 45383fb0f42db3945ac6cc658704706cdae19528)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/virtio/virtio_ring.c
diff --cc drivers/virtio/virtio_ring.c
index c503baeff4ee,27d3f057493e..000000000000
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@@ -445,6 -579,1125 +445,1128 @@@ unmap_release
  	return -EIO;
  }
  
++<<<<<<< HEAD
++=======
+ static bool virtqueue_kick_prepare_split(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	u16 new, old;
+ 	bool needs_kick;
+ 
+ 	START_USE(vq);
+ 	/* We need to expose available array entries before checking avail
+ 	 * event. */
+ 	virtio_mb(vq->weak_barriers);
+ 
+ 	old = vq->split.avail_idx_shadow - vq->num_added;
+ 	new = vq->split.avail_idx_shadow;
+ 	vq->num_added = 0;
+ 
+ 	LAST_ADD_TIME_CHECK(vq);
+ 	LAST_ADD_TIME_INVALID(vq);
+ 
+ 	if (vq->event) {
+ 		needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev,
+ 					vring_avail_event(&vq->split.vring)),
+ 					      new, old);
+ 	} else {
+ 		needs_kick = !(vq->split.vring.used->flags &
+ 					cpu_to_virtio16(_vq->vdev,
+ 						VRING_USED_F_NO_NOTIFY));
+ 	}
+ 	END_USE(vq);
+ 	return needs_kick;
+ }
+ 
+ static void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,
+ 			     void **ctx)
+ {
+ 	unsigned int i, j;
+ 	__virtio16 nextflag = cpu_to_virtio16(vq->vq.vdev, VRING_DESC_F_NEXT);
+ 
+ 	/* Clear data ptr. */
+ 	vq->split.desc_state[head].data = NULL;
+ 
+ 	/* Put back on free list: unmap first-level descriptors and find end */
+ 	i = head;
+ 
+ 	while (vq->split.vring.desc[i].flags & nextflag) {
+ 		vring_unmap_one_split(vq, &vq->split.vring.desc[i]);
+ 		i = virtio16_to_cpu(vq->vq.vdev, vq->split.vring.desc[i].next);
+ 		vq->vq.num_free++;
+ 	}
+ 
+ 	vring_unmap_one_split(vq, &vq->split.vring.desc[i]);
+ 	vq->split.vring.desc[i].next = cpu_to_virtio16(vq->vq.vdev,
+ 						vq->free_head);
+ 	vq->free_head = head;
+ 
+ 	/* Plus final descriptor */
+ 	vq->vq.num_free++;
+ 
+ 	if (vq->indirect) {
+ 		struct vring_desc *indir_desc =
+ 				vq->split.desc_state[head].indir_desc;
+ 		u32 len;
+ 
+ 		/* Free the indirect table, if any, now that it's unmapped. */
+ 		if (!indir_desc)
+ 			return;
+ 
+ 		len = virtio32_to_cpu(vq->vq.vdev,
+ 				vq->split.vring.desc[head].len);
+ 
+ 		BUG_ON(!(vq->split.vring.desc[head].flags &
+ 			 cpu_to_virtio16(vq->vq.vdev, VRING_DESC_F_INDIRECT)));
+ 		BUG_ON(len == 0 || len % sizeof(struct vring_desc));
+ 
+ 		for (j = 0; j < len / sizeof(struct vring_desc); j++)
+ 			vring_unmap_one_split(vq, &indir_desc[j]);
+ 
+ 		kfree(indir_desc);
+ 		vq->split.desc_state[head].indir_desc = NULL;
+ 	} else if (ctx) {
+ 		*ctx = vq->split.desc_state[head].indir_desc;
+ 	}
+ }
+ 
+ static inline bool more_used_split(const struct vring_virtqueue *vq)
+ {
+ 	return vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev,
+ 			vq->split.vring.used->idx);
+ }
+ 
+ static void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,
+ 					 unsigned int *len,
+ 					 void **ctx)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	void *ret;
+ 	unsigned int i;
+ 	u16 last_used;
+ 
+ 	START_USE(vq);
+ 
+ 	if (unlikely(vq->broken)) {
+ 		END_USE(vq);
+ 		return NULL;
+ 	}
+ 
+ 	if (!more_used_split(vq)) {
+ 		pr_debug("No more buffers in queue\n");
+ 		END_USE(vq);
+ 		return NULL;
+ 	}
+ 
+ 	/* Only get used array entries after they have been exposed by host. */
+ 	virtio_rmb(vq->weak_barriers);
+ 
+ 	last_used = (vq->last_used_idx & (vq->split.vring.num - 1));
+ 	i = virtio32_to_cpu(_vq->vdev,
+ 			vq->split.vring.used->ring[last_used].id);
+ 	*len = virtio32_to_cpu(_vq->vdev,
+ 			vq->split.vring.used->ring[last_used].len);
+ 
+ 	if (unlikely(i >= vq->split.vring.num)) {
+ 		BAD_RING(vq, "id %u out of range\n", i);
+ 		return NULL;
+ 	}
+ 	if (unlikely(!vq->split.desc_state[i].data)) {
+ 		BAD_RING(vq, "id %u is not a head!\n", i);
+ 		return NULL;
+ 	}
+ 
+ 	/* detach_buf_split clears data, so grab it now. */
+ 	ret = vq->split.desc_state[i].data;
+ 	detach_buf_split(vq, i, ctx);
+ 	vq->last_used_idx++;
+ 	/* If we expect an interrupt for the next entry, tell host
+ 	 * by writing event index and flush out the write before
+ 	 * the read in the next get_buf call. */
+ 	if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))
+ 		virtio_store_mb(vq->weak_barriers,
+ 				&vring_used_event(&vq->split.vring),
+ 				cpu_to_virtio16(_vq->vdev, vq->last_used_idx));
+ 
+ 	LAST_ADD_TIME_INVALID(vq);
+ 
+ 	END_USE(vq);
+ 	return ret;
+ }
+ 
+ static void virtqueue_disable_cb_split(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 
+ 	if (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {
+ 		vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
+ 		if (!vq->event)
+ 			vq->split.vring.avail->flags =
+ 				cpu_to_virtio16(_vq->vdev,
+ 						vq->split.avail_flags_shadow);
+ 	}
+ }
+ 
+ static unsigned virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	u16 last_used_idx;
+ 
+ 	START_USE(vq);
+ 
+ 	/* We optimistically turn back on interrupts, then check if there was
+ 	 * more to do. */
+ 	/* Depending on the VIRTIO_RING_F_EVENT_IDX feature, we need to
+ 	 * either clear the flags bit or point the event index at the next
+ 	 * entry. Always do both to keep code simple. */
+ 	if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+ 		vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+ 		if (!vq->event)
+ 			vq->split.vring.avail->flags =
+ 				cpu_to_virtio16(_vq->vdev,
+ 						vq->split.avail_flags_shadow);
+ 	}
+ 	vring_used_event(&vq->split.vring) = cpu_to_virtio16(_vq->vdev,
+ 			last_used_idx = vq->last_used_idx);
+ 	END_USE(vq);
+ 	return last_used_idx;
+ }
+ 
+ static bool virtqueue_poll_split(struct virtqueue *_vq, unsigned last_used_idx)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 
+ 	return (u16)last_used_idx != virtio16_to_cpu(_vq->vdev,
+ 			vq->split.vring.used->idx);
+ }
+ 
+ static bool virtqueue_enable_cb_delayed_split(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	u16 bufs;
+ 
+ 	START_USE(vq);
+ 
+ 	/* We optimistically turn back on interrupts, then check if there was
+ 	 * more to do. */
+ 	/* Depending on the VIRTIO_RING_F_USED_EVENT_IDX feature, we need to
+ 	 * either clear the flags bit or point the event index at the next
+ 	 * entry. Always update the event index to keep code simple. */
+ 	if (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {
+ 		vq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;
+ 		if (!vq->event)
+ 			vq->split.vring.avail->flags =
+ 				cpu_to_virtio16(_vq->vdev,
+ 						vq->split.avail_flags_shadow);
+ 	}
+ 	/* TODO: tune this threshold */
+ 	bufs = (u16)(vq->split.avail_idx_shadow - vq->last_used_idx) * 3 / 4;
+ 
+ 	virtio_store_mb(vq->weak_barriers,
+ 			&vring_used_event(&vq->split.vring),
+ 			cpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));
+ 
+ 	if (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->split.vring.used->idx)
+ 					- vq->last_used_idx) > bufs)) {
+ 		END_USE(vq);
+ 		return false;
+ 	}
+ 
+ 	END_USE(vq);
+ 	return true;
+ }
+ 
+ static void *virtqueue_detach_unused_buf_split(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	unsigned int i;
+ 	void *buf;
+ 
+ 	START_USE(vq);
+ 
+ 	for (i = 0; i < vq->split.vring.num; i++) {
+ 		if (!vq->split.desc_state[i].data)
+ 			continue;
+ 		/* detach_buf_split clears data, so grab it now. */
+ 		buf = vq->split.desc_state[i].data;
+ 		detach_buf_split(vq, i, NULL);
+ 		vq->split.avail_idx_shadow--;
+ 		vq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev,
+ 				vq->split.avail_idx_shadow);
+ 		END_USE(vq);
+ 		return buf;
+ 	}
+ 	/* That should have freed everything. */
+ 	BUG_ON(vq->vq.num_free != vq->split.vring.num);
+ 
+ 	END_USE(vq);
+ 	return NULL;
+ }
+ 
+ static struct virtqueue *vring_create_virtqueue_split(
+ 	unsigned int index,
+ 	unsigned int num,
+ 	unsigned int vring_align,
+ 	struct virtio_device *vdev,
+ 	bool weak_barriers,
+ 	bool may_reduce_num,
+ 	bool context,
+ 	bool (*notify)(struct virtqueue *),
+ 	void (*callback)(struct virtqueue *),
+ 	const char *name)
+ {
+ 	struct virtqueue *vq;
+ 	void *queue = NULL;
+ 	dma_addr_t dma_addr;
+ 	size_t queue_size_in_bytes;
+ 	struct vring vring;
+ 
+ 	/* We assume num is a power of 2. */
+ 	if (num & (num - 1)) {
+ 		dev_warn(&vdev->dev, "Bad virtqueue length %u\n", num);
+ 		return NULL;
+ 	}
+ 
+ 	/* TODO: allocate each queue chunk individually */
+ 	for (; num && vring_size(num, vring_align) > PAGE_SIZE; num /= 2) {
+ 		queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ 					  &dma_addr,
+ 					  GFP_KERNEL|__GFP_NOWARN|__GFP_ZERO);
+ 		if (queue)
+ 			break;
+ 	}
+ 
+ 	if (!num)
+ 		return NULL;
+ 
+ 	if (!queue) {
+ 		/* Try to get a single page. You are my only hope! */
+ 		queue = vring_alloc_queue(vdev, vring_size(num, vring_align),
+ 					  &dma_addr, GFP_KERNEL|__GFP_ZERO);
+ 	}
+ 	if (!queue)
+ 		return NULL;
+ 
+ 	queue_size_in_bytes = vring_size(num, vring_align);
+ 	vring_init(&vring, num, queue, vring_align);
+ 
+ 	vq = __vring_new_virtqueue(index, vring, vdev, weak_barriers, context,
+ 				   notify, callback, name);
+ 	if (!vq) {
+ 		vring_free_queue(vdev, queue_size_in_bytes, queue,
+ 				 dma_addr);
+ 		return NULL;
+ 	}
+ 
+ 	to_vvq(vq)->split.queue_dma_addr = dma_addr;
+ 	to_vvq(vq)->split.queue_size_in_bytes = queue_size_in_bytes;
+ 	to_vvq(vq)->we_own_ring = true;
+ 
+ 	return vq;
+ }
+ 
+ 
+ /*
+  * Packed ring specific functions - *_packed().
+  */
+ 
+ static void vring_unmap_state_packed(const struct vring_virtqueue *vq,
+ 				     struct vring_desc_extra_packed *state)
+ {
+ 	u16 flags;
+ 
+ 	if (!vq->use_dma_api)
+ 		return;
+ 
+ 	flags = state->flags;
+ 
+ 	if (flags & VRING_DESC_F_INDIRECT) {
+ 		dma_unmap_single(vring_dma_dev(vq),
+ 				 state->addr, state->len,
+ 				 (flags & VRING_DESC_F_WRITE) ?
+ 				 DMA_FROM_DEVICE : DMA_TO_DEVICE);
+ 	} else {
+ 		dma_unmap_page(vring_dma_dev(vq),
+ 			       state->addr, state->len,
+ 			       (flags & VRING_DESC_F_WRITE) ?
+ 			       DMA_FROM_DEVICE : DMA_TO_DEVICE);
+ 	}
+ }
+ 
+ static void vring_unmap_desc_packed(const struct vring_virtqueue *vq,
+ 				   struct vring_packed_desc *desc)
+ {
+ 	u16 flags;
+ 
+ 	if (!vq->use_dma_api)
+ 		return;
+ 
+ 	flags = le16_to_cpu(desc->flags);
+ 
+ 	if (flags & VRING_DESC_F_INDIRECT) {
+ 		dma_unmap_single(vring_dma_dev(vq),
+ 				 le64_to_cpu(desc->addr),
+ 				 le32_to_cpu(desc->len),
+ 				 (flags & VRING_DESC_F_WRITE) ?
+ 				 DMA_FROM_DEVICE : DMA_TO_DEVICE);
+ 	} else {
+ 		dma_unmap_page(vring_dma_dev(vq),
+ 			       le64_to_cpu(desc->addr),
+ 			       le32_to_cpu(desc->len),
+ 			       (flags & VRING_DESC_F_WRITE) ?
+ 			       DMA_FROM_DEVICE : DMA_TO_DEVICE);
+ 	}
+ }
+ 
+ static struct vring_packed_desc *alloc_indirect_packed(unsigned int total_sg,
+ 						       gfp_t gfp)
+ {
+ 	struct vring_packed_desc *desc;
+ 
+ 	/*
+ 	 * We require lowmem mappings for the descriptors because
+ 	 * otherwise virt_to_phys will give us bogus addresses in the
+ 	 * virtqueue.
+ 	 */
+ 	gfp &= ~__GFP_HIGHMEM;
+ 
+ 	desc = kmalloc_array(total_sg, sizeof(struct vring_packed_desc), gfp);
+ 
+ 	return desc;
+ }
+ 
+ static int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,
+ 				       struct scatterlist *sgs[],
+ 				       unsigned int total_sg,
+ 				       unsigned int out_sgs,
+ 				       unsigned int in_sgs,
+ 				       void *data,
+ 				       gfp_t gfp)
+ {
+ 	struct vring_packed_desc *desc;
+ 	struct scatterlist *sg;
+ 	unsigned int i, n, err_idx;
+ 	u16 head, id;
+ 	dma_addr_t addr;
+ 
+ 	head = vq->packed.next_avail_idx;
+ 	desc = alloc_indirect_packed(total_sg, gfp);
+ 
+ 	if (unlikely(vq->vq.num_free < 1)) {
+ 		pr_debug("Can't add buf len 1 - avail = 0\n");
+ 		END_USE(vq);
+ 		return -ENOSPC;
+ 	}
+ 
+ 	i = 0;
+ 	id = vq->free_head;
+ 	BUG_ON(id == vq->packed.vring.num);
+ 
+ 	for (n = 0; n < out_sgs + in_sgs; n++) {
+ 		for (sg = sgs[n]; sg; sg = sg_next(sg)) {
+ 			addr = vring_map_one_sg(vq, sg, n < out_sgs ?
+ 					DMA_TO_DEVICE : DMA_FROM_DEVICE);
+ 			if (vring_mapping_error(vq, addr))
+ 				goto unmap_release;
+ 
+ 			desc[i].flags = cpu_to_le16(n < out_sgs ?
+ 						0 : VRING_DESC_F_WRITE);
+ 			desc[i].addr = cpu_to_le64(addr);
+ 			desc[i].len = cpu_to_le32(sg->length);
+ 			i++;
+ 		}
+ 	}
+ 
+ 	/* Now that the indirect table is filled in, map it. */
+ 	addr = vring_map_single(vq, desc,
+ 			total_sg * sizeof(struct vring_packed_desc),
+ 			DMA_TO_DEVICE);
+ 	if (vring_mapping_error(vq, addr))
+ 		goto unmap_release;
+ 
+ 	vq->packed.vring.desc[head].addr = cpu_to_le64(addr);
+ 	vq->packed.vring.desc[head].len = cpu_to_le32(total_sg *
+ 				sizeof(struct vring_packed_desc));
+ 	vq->packed.vring.desc[head].id = cpu_to_le16(id);
+ 
+ 	if (vq->use_dma_api) {
+ 		vq->packed.desc_extra[id].addr = addr;
+ 		vq->packed.desc_extra[id].len = total_sg *
+ 				sizeof(struct vring_packed_desc);
+ 		vq->packed.desc_extra[id].flags = VRING_DESC_F_INDIRECT |
+ 						  vq->packed.avail_used_flags;
+ 	}
+ 
+ 	/*
+ 	 * A driver MUST NOT make the first descriptor in the list
+ 	 * available before all subsequent descriptors comprising
+ 	 * the list are made available.
+ 	 */
+ 	virtio_wmb(vq->weak_barriers);
+ 	vq->packed.vring.desc[head].flags = cpu_to_le16(VRING_DESC_F_INDIRECT |
+ 						vq->packed.avail_used_flags);
+ 
+ 	/* We're using some buffers from the free list. */
+ 	vq->vq.num_free -= 1;
+ 
+ 	/* Update free pointer */
+ 	n = head + 1;
+ 	if (n >= vq->packed.vring.num) {
+ 		n = 0;
+ 		vq->packed.avail_wrap_counter ^= 1;
+ 		vq->packed.avail_used_flags ^=
+ 				1 << VRING_PACKED_DESC_F_AVAIL |
+ 				1 << VRING_PACKED_DESC_F_USED;
+ 	}
+ 	vq->packed.next_avail_idx = n;
+ 	vq->free_head = vq->packed.desc_state[id].next;
+ 
+ 	/* Store token and indirect buffer state. */
+ 	vq->packed.desc_state[id].num = 1;
+ 	vq->packed.desc_state[id].data = data;
+ 	vq->packed.desc_state[id].indir_desc = desc;
+ 	vq->packed.desc_state[id].last = id;
+ 
+ 	vq->num_added += 1;
+ 
+ 	pr_debug("Added buffer head %i to %p\n", head, vq);
+ 	END_USE(vq);
+ 
+ 	return 0;
+ 
+ unmap_release:
+ 	err_idx = i;
+ 
+ 	for (i = 0; i < err_idx; i++)
+ 		vring_unmap_desc_packed(vq, &desc[i]);
+ 
+ 	kfree(desc);
+ 
+ 	END_USE(vq);
+ 	return -EIO;
+ }
+ 
+ static inline int virtqueue_add_packed(struct virtqueue *_vq,
+ 				       struct scatterlist *sgs[],
+ 				       unsigned int total_sg,
+ 				       unsigned int out_sgs,
+ 				       unsigned int in_sgs,
+ 				       void *data,
+ 				       void *ctx,
+ 				       gfp_t gfp)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	struct vring_packed_desc *desc;
+ 	struct scatterlist *sg;
+ 	unsigned int i, n, c, descs_used, err_idx;
+ 	__le16 uninitialized_var(head_flags), flags;
+ 	u16 head, id, uninitialized_var(prev), curr, avail_used_flags;
+ 
+ 	START_USE(vq);
+ 
+ 	BUG_ON(data == NULL);
+ 	BUG_ON(ctx && vq->indirect);
+ 
+ 	if (unlikely(vq->broken)) {
+ 		END_USE(vq);
+ 		return -EIO;
+ 	}
+ 
+ 	LAST_ADD_TIME_UPDATE(vq);
+ 
+ 	BUG_ON(total_sg == 0);
+ 
+ 	if (virtqueue_use_indirect(_vq, total_sg))
+ 		return virtqueue_add_indirect_packed(vq, sgs, total_sg,
+ 				out_sgs, in_sgs, data, gfp);
+ 
+ 	head = vq->packed.next_avail_idx;
+ 	avail_used_flags = vq->packed.avail_used_flags;
+ 
+ 	WARN_ON_ONCE(total_sg > vq->packed.vring.num && !vq->indirect);
+ 
+ 	desc = vq->packed.vring.desc;
+ 	i = head;
+ 	descs_used = total_sg;
+ 
+ 	if (unlikely(vq->vq.num_free < descs_used)) {
+ 		pr_debug("Can't add buf len %i - avail = %i\n",
+ 			 descs_used, vq->vq.num_free);
+ 		END_USE(vq);
+ 		return -ENOSPC;
+ 	}
+ 
+ 	id = vq->free_head;
+ 	BUG_ON(id == vq->packed.vring.num);
+ 
+ 	curr = id;
+ 	c = 0;
+ 	for (n = 0; n < out_sgs + in_sgs; n++) {
+ 		for (sg = sgs[n]; sg; sg = sg_next(sg)) {
+ 			dma_addr_t addr = vring_map_one_sg(vq, sg, n < out_sgs ?
+ 					DMA_TO_DEVICE : DMA_FROM_DEVICE);
+ 			if (vring_mapping_error(vq, addr))
+ 				goto unmap_release;
+ 
+ 			flags = cpu_to_le16(vq->packed.avail_used_flags |
+ 				    (++c == total_sg ? 0 : VRING_DESC_F_NEXT) |
+ 				    (n < out_sgs ? 0 : VRING_DESC_F_WRITE));
+ 			if (i == head)
+ 				head_flags = flags;
+ 			else
+ 				desc[i].flags = flags;
+ 
+ 			desc[i].addr = cpu_to_le64(addr);
+ 			desc[i].len = cpu_to_le32(sg->length);
+ 			desc[i].id = cpu_to_le16(id);
+ 
+ 			if (unlikely(vq->use_dma_api)) {
+ 				vq->packed.desc_extra[curr].addr = addr;
+ 				vq->packed.desc_extra[curr].len = sg->length;
+ 				vq->packed.desc_extra[curr].flags =
+ 					le16_to_cpu(flags);
+ 			}
+ 			prev = curr;
+ 			curr = vq->packed.desc_state[curr].next;
+ 
+ 			if ((unlikely(++i >= vq->packed.vring.num))) {
+ 				i = 0;
+ 				vq->packed.avail_used_flags ^=
+ 					1 << VRING_PACKED_DESC_F_AVAIL |
+ 					1 << VRING_PACKED_DESC_F_USED;
+ 			}
+ 		}
+ 	}
+ 
+ 	if (i < head)
+ 		vq->packed.avail_wrap_counter ^= 1;
+ 
+ 	/* We're using some buffers from the free list. */
+ 	vq->vq.num_free -= descs_used;
+ 
+ 	/* Update free pointer */
+ 	vq->packed.next_avail_idx = i;
+ 	vq->free_head = curr;
+ 
+ 	/* Store token. */
+ 	vq->packed.desc_state[id].num = descs_used;
+ 	vq->packed.desc_state[id].data = data;
+ 	vq->packed.desc_state[id].indir_desc = ctx;
+ 	vq->packed.desc_state[id].last = prev;
+ 
+ 	/*
+ 	 * A driver MUST NOT make the first descriptor in the list
+ 	 * available before all subsequent descriptors comprising
+ 	 * the list are made available.
+ 	 */
+ 	virtio_wmb(vq->weak_barriers);
+ 	vq->packed.vring.desc[head].flags = head_flags;
+ 	vq->num_added += descs_used;
+ 
+ 	pr_debug("Added buffer head %i to %p\n", head, vq);
+ 	END_USE(vq);
+ 
+ 	return 0;
+ 
+ unmap_release:
+ 	err_idx = i;
+ 	i = head;
+ 
+ 	vq->packed.avail_used_flags = avail_used_flags;
+ 
+ 	for (n = 0; n < total_sg; n++) {
+ 		if (i == err_idx)
+ 			break;
+ 		vring_unmap_desc_packed(vq, &desc[i]);
+ 		i++;
+ 		if (i >= vq->packed.vring.num)
+ 			i = 0;
+ 	}
+ 
+ 	END_USE(vq);
+ 	return -EIO;
+ }
+ 
+ static bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	u16 new, old, off_wrap, flags, wrap_counter, event_idx;
+ 	bool needs_kick;
+ 	union {
+ 		struct {
+ 			__le16 off_wrap;
+ 			__le16 flags;
+ 		};
+ 		u32 u32;
+ 	} snapshot;
+ 
+ 	START_USE(vq);
+ 
+ 	/*
+ 	 * We need to expose the new flags value before checking notification
+ 	 * suppressions.
+ 	 */
+ 	virtio_mb(vq->weak_barriers);
+ 
+ 	old = vq->packed.next_avail_idx - vq->num_added;
+ 	new = vq->packed.next_avail_idx;
+ 	vq->num_added = 0;
+ 
+ 	snapshot.u32 = *(u32 *)vq->packed.vring.device;
+ 	flags = le16_to_cpu(snapshot.flags);
+ 
+ 	LAST_ADD_TIME_CHECK(vq);
+ 	LAST_ADD_TIME_INVALID(vq);
+ 
+ 	if (flags != VRING_PACKED_EVENT_FLAG_DESC) {
+ 		needs_kick = (flags != VRING_PACKED_EVENT_FLAG_DISABLE);
+ 		goto out;
+ 	}
+ 
+ 	off_wrap = le16_to_cpu(snapshot.off_wrap);
+ 
+ 	wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ 	event_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ 	if (wrap_counter != vq->packed.avail_wrap_counter)
+ 		event_idx -= vq->packed.vring.num;
+ 
+ 	needs_kick = vring_need_event(event_idx, new, old);
+ out:
+ 	END_USE(vq);
+ 	return needs_kick;
+ }
+ 
+ static void detach_buf_packed(struct vring_virtqueue *vq,
+ 			      unsigned int id, void **ctx)
+ {
+ 	struct vring_desc_state_packed *state = NULL;
+ 	struct vring_packed_desc *desc;
+ 	unsigned int i, curr;
+ 
+ 	state = &vq->packed.desc_state[id];
+ 
+ 	/* Clear data ptr. */
+ 	state->data = NULL;
+ 
+ 	vq->packed.desc_state[state->last].next = vq->free_head;
+ 	vq->free_head = id;
+ 	vq->vq.num_free += state->num;
+ 
+ 	if (unlikely(vq->use_dma_api)) {
+ 		curr = id;
+ 		for (i = 0; i < state->num; i++) {
+ 			vring_unmap_state_packed(vq,
+ 				&vq->packed.desc_extra[curr]);
+ 			curr = vq->packed.desc_state[curr].next;
+ 		}
+ 	}
+ 
+ 	if (vq->indirect) {
+ 		u32 len;
+ 
+ 		/* Free the indirect table, if any, now that it's unmapped. */
+ 		desc = state->indir_desc;
+ 		if (!desc)
+ 			return;
+ 
+ 		if (vq->use_dma_api) {
+ 			len = vq->packed.desc_extra[id].len;
+ 			for (i = 0; i < len / sizeof(struct vring_packed_desc);
+ 					i++)
+ 				vring_unmap_desc_packed(vq, &desc[i]);
+ 		}
+ 		kfree(desc);
+ 		state->indir_desc = NULL;
+ 	} else if (ctx) {
+ 		*ctx = state->indir_desc;
+ 	}
+ }
+ 
+ static inline bool is_used_desc_packed(const struct vring_virtqueue *vq,
+ 				       u16 idx, bool used_wrap_counter)
+ {
+ 	bool avail, used;
+ 	u16 flags;
+ 
+ 	flags = le16_to_cpu(vq->packed.vring.desc[idx].flags);
+ 	avail = !!(flags & (1 << VRING_PACKED_DESC_F_AVAIL));
+ 	used = !!(flags & (1 << VRING_PACKED_DESC_F_USED));
+ 
+ 	return avail == used && used == used_wrap_counter;
+ }
+ 
+ static inline bool more_used_packed(const struct vring_virtqueue *vq)
+ {
+ 	return is_used_desc_packed(vq, vq->last_used_idx,
+ 			vq->packed.used_wrap_counter);
+ }
+ 
+ static void *virtqueue_get_buf_ctx_packed(struct virtqueue *_vq,
+ 					  unsigned int *len,
+ 					  void **ctx)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	u16 last_used, id;
+ 	void *ret;
+ 
+ 	START_USE(vq);
+ 
+ 	if (unlikely(vq->broken)) {
+ 		END_USE(vq);
+ 		return NULL;
+ 	}
+ 
+ 	if (!more_used_packed(vq)) {
+ 		pr_debug("No more buffers in queue\n");
+ 		END_USE(vq);
+ 		return NULL;
+ 	}
+ 
+ 	/* Only get used elements after they have been exposed by host. */
+ 	virtio_rmb(vq->weak_barriers);
+ 
+ 	last_used = vq->last_used_idx;
+ 	id = le16_to_cpu(vq->packed.vring.desc[last_used].id);
+ 	*len = le32_to_cpu(vq->packed.vring.desc[last_used].len);
+ 
+ 	if (unlikely(id >= vq->packed.vring.num)) {
+ 		BAD_RING(vq, "id %u out of range\n", id);
+ 		return NULL;
+ 	}
+ 	if (unlikely(!vq->packed.desc_state[id].data)) {
+ 		BAD_RING(vq, "id %u is not a head!\n", id);
+ 		return NULL;
+ 	}
+ 
+ 	/* detach_buf_packed clears data, so grab it now. */
+ 	ret = vq->packed.desc_state[id].data;
+ 	detach_buf_packed(vq, id, ctx);
+ 
+ 	vq->last_used_idx += vq->packed.desc_state[id].num;
+ 	if (unlikely(vq->last_used_idx >= vq->packed.vring.num)) {
+ 		vq->last_used_idx -= vq->packed.vring.num;
+ 		vq->packed.used_wrap_counter ^= 1;
+ 	}
+ 
+ 	/*
+ 	 * If we expect an interrupt for the next entry, tell host
+ 	 * by writing event index and flush out the write before
+ 	 * the read in the next get_buf call.
+ 	 */
+ 	if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)
+ 		virtio_store_mb(vq->weak_barriers,
+ 				&vq->packed.vring.driver->off_wrap,
+ 				cpu_to_le16(vq->last_used_idx |
+ 					(vq->packed.used_wrap_counter <<
+ 					 VRING_PACKED_EVENT_F_WRAP_CTR)));
+ 
+ 	LAST_ADD_TIME_INVALID(vq);
+ 
+ 	END_USE(vq);
+ 	return ret;
+ }
+ 
+ static void virtqueue_disable_cb_packed(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 
+ 	if (vq->packed.event_flags_shadow != VRING_PACKED_EVENT_FLAG_DISABLE) {
+ 		vq->packed.event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+ 		vq->packed.vring.driver->flags =
+ 			cpu_to_le16(vq->packed.event_flags_shadow);
+ 	}
+ }
+ 
+ static unsigned virtqueue_enable_cb_prepare_packed(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 
+ 	START_USE(vq);
+ 
+ 	/*
+ 	 * We optimistically turn back on interrupts, then check if there was
+ 	 * more to do.
+ 	 */
+ 
+ 	if (vq->event) {
+ 		vq->packed.vring.driver->off_wrap =
+ 			cpu_to_le16(vq->last_used_idx |
+ 				(vq->packed.used_wrap_counter <<
+ 				 VRING_PACKED_EVENT_F_WRAP_CTR));
+ 		/*
+ 		 * We need to update event offset and event wrap
+ 		 * counter first before updating event flags.
+ 		 */
+ 		virtio_wmb(vq->weak_barriers);
+ 	}
+ 
+ 	if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+ 		vq->packed.event_flags_shadow = vq->event ?
+ 				VRING_PACKED_EVENT_FLAG_DESC :
+ 				VRING_PACKED_EVENT_FLAG_ENABLE;
+ 		vq->packed.vring.driver->flags =
+ 				cpu_to_le16(vq->packed.event_flags_shadow);
+ 	}
+ 
+ 	END_USE(vq);
+ 	return vq->last_used_idx | ((u16)vq->packed.used_wrap_counter <<
+ 			VRING_PACKED_EVENT_F_WRAP_CTR);
+ }
+ 
+ static bool virtqueue_poll_packed(struct virtqueue *_vq, u16 off_wrap)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	bool wrap_counter;
+ 	u16 used_idx;
+ 
+ 	wrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;
+ 	used_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);
+ 
+ 	return is_used_desc_packed(vq, used_idx, wrap_counter);
+ }
+ 
+ static bool virtqueue_enable_cb_delayed_packed(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	u16 used_idx, wrap_counter;
+ 	u16 bufs;
+ 
+ 	START_USE(vq);
+ 
+ 	/*
+ 	 * We optimistically turn back on interrupts, then check if there was
+ 	 * more to do.
+ 	 */
+ 
+ 	if (vq->event) {
+ 		/* TODO: tune this threshold */
+ 		bufs = (vq->packed.vring.num - vq->vq.num_free) * 3 / 4;
+ 		wrap_counter = vq->packed.used_wrap_counter;
+ 
+ 		used_idx = vq->last_used_idx + bufs;
+ 		if (used_idx >= vq->packed.vring.num) {
+ 			used_idx -= vq->packed.vring.num;
+ 			wrap_counter ^= 1;
+ 		}
+ 
+ 		vq->packed.vring.driver->off_wrap = cpu_to_le16(used_idx |
+ 			(wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));
+ 
+ 		/*
+ 		 * We need to update event offset and event wrap
+ 		 * counter first before updating event flags.
+ 		 */
+ 		virtio_wmb(vq->weak_barriers);
+ 	} else {
+ 		used_idx = vq->last_used_idx;
+ 		wrap_counter = vq->packed.used_wrap_counter;
+ 	}
+ 
+ 	if (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {
+ 		vq->packed.event_flags_shadow = vq->event ?
+ 				VRING_PACKED_EVENT_FLAG_DESC :
+ 				VRING_PACKED_EVENT_FLAG_ENABLE;
+ 		vq->packed.vring.driver->flags =
+ 				cpu_to_le16(vq->packed.event_flags_shadow);
+ 	}
+ 
+ 	/*
+ 	 * We need to update event suppression structure first
+ 	 * before re-checking for more used buffers.
+ 	 */
+ 	virtio_mb(vq->weak_barriers);
+ 
+ 	if (is_used_desc_packed(vq, used_idx, wrap_counter)) {
+ 		END_USE(vq);
+ 		return false;
+ 	}
+ 
+ 	END_USE(vq);
+ 	return true;
+ }
+ 
+ static void *virtqueue_detach_unused_buf_packed(struct virtqueue *_vq)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 	unsigned int i;
+ 	void *buf;
+ 
+ 	START_USE(vq);
+ 
+ 	for (i = 0; i < vq->packed.vring.num; i++) {
+ 		if (!vq->packed.desc_state[i].data)
+ 			continue;
+ 		/* detach_buf clears data, so grab it now. */
+ 		buf = vq->packed.desc_state[i].data;
+ 		detach_buf_packed(vq, i, NULL);
+ 		END_USE(vq);
+ 		return buf;
+ 	}
+ 	/* That should have freed everything. */
+ 	BUG_ON(vq->vq.num_free != vq->packed.vring.num);
+ 
+ 	END_USE(vq);
+ 	return NULL;
+ }
+ 
+ static struct virtqueue *vring_create_virtqueue_packed(
+ 	unsigned int index,
+ 	unsigned int num,
+ 	unsigned int vring_align,
+ 	struct virtio_device *vdev,
+ 	bool weak_barriers,
+ 	bool may_reduce_num,
+ 	bool context,
+ 	bool (*notify)(struct virtqueue *),
+ 	void (*callback)(struct virtqueue *),
+ 	const char *name)
+ {
+ 	struct vring_virtqueue *vq;
+ 	struct vring_packed_desc *ring;
+ 	struct vring_packed_desc_event *driver, *device;
+ 	dma_addr_t ring_dma_addr, driver_event_dma_addr, device_event_dma_addr;
+ 	size_t ring_size_in_bytes, event_size_in_bytes;
+ 	unsigned int i;
+ 
+ 	ring_size_in_bytes = num * sizeof(struct vring_packed_desc);
+ 
+ 	ring = vring_alloc_queue(vdev, ring_size_in_bytes,
+ 				 &ring_dma_addr,
+ 				 GFP_KERNEL|__GFP_NOWARN|__GFP_ZERO);
+ 	if (!ring)
+ 		goto err_ring;
+ 
+ 	event_size_in_bytes = sizeof(struct vring_packed_desc_event);
+ 
+ 	driver = vring_alloc_queue(vdev, event_size_in_bytes,
+ 				   &driver_event_dma_addr,
+ 				   GFP_KERNEL|__GFP_NOWARN|__GFP_ZERO);
+ 	if (!driver)
+ 		goto err_driver;
+ 
+ 	device = vring_alloc_queue(vdev, event_size_in_bytes,
+ 				   &device_event_dma_addr,
+ 				   GFP_KERNEL|__GFP_NOWARN|__GFP_ZERO);
+ 	if (!device)
+ 		goto err_device;
+ 
+ 	vq = kmalloc(sizeof(*vq), GFP_KERNEL);
+ 	if (!vq)
+ 		goto err_vq;
+ 
+ 	vq->vq.callback = callback;
+ 	vq->vq.vdev = vdev;
+ 	vq->vq.name = name;
+ 	vq->vq.num_free = num;
+ 	vq->vq.index = index;
+ 	vq->we_own_ring = true;
+ 	vq->notify = notify;
+ 	vq->weak_barriers = weak_barriers;
+ 	vq->broken = false;
+ 	vq->last_used_idx = 0;
+ 	vq->num_added = 0;
+ 	vq->packed_ring = true;
+ 	vq->use_dma_api = vring_use_dma_api(vdev);
+ 	list_add_tail(&vq->vq.list, &vdev->vqs);
+ #ifdef DEBUG
+ 	vq->in_use = false;
+ 	vq->last_add_time_valid = false;
+ #endif
+ 
+ 	vq->indirect = virtio_has_feature(vdev, VIRTIO_RING_F_INDIRECT_DESC) &&
+ 		!context;
+ 	vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
+ 
+ 	if (virtio_has_feature(vdev, VIRTIO_F_ORDER_PLATFORM))
+ 		vq->weak_barriers = false;
+ 
+ 	vq->packed.ring_dma_addr = ring_dma_addr;
+ 	vq->packed.driver_event_dma_addr = driver_event_dma_addr;
+ 	vq->packed.device_event_dma_addr = device_event_dma_addr;
+ 
+ 	vq->packed.ring_size_in_bytes = ring_size_in_bytes;
+ 	vq->packed.event_size_in_bytes = event_size_in_bytes;
+ 
+ 	vq->packed.vring.num = num;
+ 	vq->packed.vring.desc = ring;
+ 	vq->packed.vring.driver = driver;
+ 	vq->packed.vring.device = device;
+ 
+ 	vq->packed.next_avail_idx = 0;
+ 	vq->packed.avail_wrap_counter = 1;
+ 	vq->packed.used_wrap_counter = 1;
+ 	vq->packed.event_flags_shadow = 0;
+ 	vq->packed.avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;
+ 
+ 	vq->packed.desc_state = kmalloc_array(num,
+ 			sizeof(struct vring_desc_state_packed),
+ 			GFP_KERNEL);
+ 	if (!vq->packed.desc_state)
+ 		goto err_desc_state;
+ 
+ 	memset(vq->packed.desc_state, 0,
+ 		num * sizeof(struct vring_desc_state_packed));
+ 
+ 	/* Put everything in free lists. */
+ 	vq->free_head = 0;
+ 	for (i = 0; i < num-1; i++)
+ 		vq->packed.desc_state[i].next = i + 1;
+ 
+ 	vq->packed.desc_extra = kmalloc_array(num,
+ 			sizeof(struct vring_desc_extra_packed),
+ 			GFP_KERNEL);
+ 	if (!vq->packed.desc_extra)
+ 		goto err_desc_extra;
+ 
+ 	memset(vq->packed.desc_extra, 0,
+ 		num * sizeof(struct vring_desc_extra_packed));
+ 
+ 	/* No callback?  Tell other side not to bother us. */
+ 	if (!callback) {
+ 		vq->packed.event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;
+ 		vq->packed.vring.driver->flags =
+ 			cpu_to_le16(vq->packed.event_flags_shadow);
+ 	}
+ 
+ 	return &vq->vq;
+ 
+ err_desc_extra:
+ 	kfree(vq->packed.desc_state);
+ err_desc_state:
+ 	kfree(vq);
+ err_vq:
+ 	vring_free_queue(vdev, event_size_in_bytes, device, ring_dma_addr);
+ err_device:
+ 	vring_free_queue(vdev, event_size_in_bytes, driver, ring_dma_addr);
+ err_driver:
+ 	vring_free_queue(vdev, ring_size_in_bytes, ring, ring_dma_addr);
+ err_ring:
+ 	return NULL;
+ }
+ 
+ 
+ /*
+  * Generic functions and exported symbols.
+  */
+ 
+ static inline int virtqueue_add(struct virtqueue *_vq,
+ 				struct scatterlist *sgs[],
+ 				unsigned int total_sg,
+ 				unsigned int out_sgs,
+ 				unsigned int in_sgs,
+ 				void *data,
+ 				void *ctx,
+ 				gfp_t gfp)
+ {
+ 	struct vring_virtqueue *vq = to_vvq(_vq);
+ 
+ 	return vq->packed_ring ? virtqueue_add_packed(_vq, sgs, total_sg,
+ 					out_sgs, in_sgs, data, ctx, gfp) :
+ 				 virtqueue_add_split(_vq, sgs, total_sg,
+ 					out_sgs, in_sgs, data, ctx, gfp);
+ }
+ 
++>>>>>>> 45383fb0f42d (virtio: support VIRTIO_F_ORDER_PLATFORM)
  /**
   * virtqueue_add_sgs - expose buffers to other end
   * @vq: the struct virtqueue we're talking about.
@@@ -1007,11 -2082,29 +2129,24 @@@ struct virtqueue *__vring_new_virtqueue
  		!context;
  	vq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);
  
++<<<<<<< HEAD
++=======
+ 	if (virtio_has_feature(vdev, VIRTIO_F_ORDER_PLATFORM))
+ 		vq->weak_barriers = false;
+ 
+ 	vq->split.queue_dma_addr = 0;
+ 	vq->split.queue_size_in_bytes = 0;
+ 
+ 	vq->split.vring = vring;
+ 	vq->split.avail_flags_shadow = 0;
+ 	vq->split.avail_idx_shadow = 0;
+ 
++>>>>>>> 45383fb0f42d (virtio: support VIRTIO_F_ORDER_PLATFORM)
  	/* No callback?  Tell other side not to bother us. */
  	if (!callback) {
 -		vq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
 +		vq->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;
  		if (!vq->event)
 -			vq->split.vring.avail->flags = cpu_to_virtio16(vdev,
 -					vq->split.avail_flags_shadow);
 -	}
 -
 -	vq->split.desc_state = kmalloc_array(vring.num,
 -			sizeof(struct vring_desc_state_split), GFP_KERNEL);
 -	if (!vq->split.desc_state) {
 -		kfree(vq);
 -		return NULL;
 +			vq->vring.avail->flags = cpu_to_virtio16(vdev, vq->avail_flags_shadow);
  	}
  
  	/* Put everything in free lists. */
@@@ -1177,6 -2217,10 +2312,13 @@@ void vring_transport_features(struct vi
  			break;
  		case VIRTIO_F_IOMMU_PLATFORM:
  			break;
++<<<<<<< HEAD
++=======
+ 		case VIRTIO_F_RING_PACKED:
+ 			break;
+ 		case VIRTIO_F_ORDER_PLATFORM:
+ 			break;
++>>>>>>> 45383fb0f42d (virtio: support VIRTIO_F_ORDER_PLATFORM)
  		default:
  			/* We don't understand this bit. */
  			__virtio_clear_bit(vdev, i);
* Unmerged path drivers/virtio/virtio_ring.c
diff --git a/include/uapi/linux/virtio_config.h b/include/uapi/linux/virtio_config.h
index 449132c76b1c..f75a06182a73 100644
--- a/include/uapi/linux/virtio_config.h
+++ b/include/uapi/linux/virtio_config.h
@@ -75,6 +75,12 @@
  */
 #define VIRTIO_F_IOMMU_PLATFORM		33
 
+/*
+ * This feature indicates that memory accesses by the driver and the
+ * device are ordered in a way described by the platform.
+ */
+#define VIRTIO_F_ORDER_PLATFORM		36
+
 /*
  * Does the device support Single Root I/O Virtualization?
  */
