vfio: Add vfio_file_is_group()

jira LE-1907
Rebuild_History Non-Buildable kernel-5.14.0-284.30.1.el9_2
commit-author Jason Gunthorpe <jgg@ziepe.ca>
commit 4b22ef042d6f54a6e5899555f2db71749133eca8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-284.30.1.el9_2/4b22ef04.failed

This replaces uses of vfio_file_iommu_group() which were only detecting if
the file is a VFIO file with no interest in the actual group.

The only remaning user of vfio_file_iommu_group() is in KVM for the SPAPR
stuff. It passes the iommu_group into the arch code through kvm for some
reason.

	Tested-by: Matthew Rosato <mjrosato@linux.ibm.com>
	Tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
	Tested-by: Eric Farman <farman@linux.ibm.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
Link: https://lore.kernel.org/r/1-v2-15417f29324e+1c-vfio_group_disassociate_jgg@nvidia.com
	Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
(cherry picked from commit 4b22ef042d6f54a6e5899555f2db71749133eca8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/vfio/pci/vfio_pci_core.c
diff --cc drivers/vfio/pci/vfio_pci_core.c
index c43bf4624041,badc9d828cac..000000000000
--- a/drivers/vfio/pci/vfio_pci_core.c
+++ b/drivers/vfio/pci/vfio_pci_core.c
@@@ -669,503 -872,527 +669,511 @@@ int vfio_pci_register_dev_region(struc
  
  	return 0;
  }
 -EXPORT_SYMBOL_GPL(vfio_pci_core_register_dev_region);
 +EXPORT_SYMBOL_GPL(vfio_pci_register_dev_region);
  
 -static int vfio_pci_ioctl_get_info(struct vfio_pci_core_device *vdev,
 -				   struct vfio_device_info __user *arg)
 +long vfio_pci_core_ioctl(struct vfio_device *core_vdev, unsigned int cmd,
 +		unsigned long arg)
  {
 -	unsigned long minsz = offsetofend(struct vfio_device_info, num_irqs);
 -	struct vfio_device_info info;
 -	struct vfio_info_cap caps = { .buf = NULL, .size = 0 };
 -	unsigned long capsz;
 -	int ret;
 +	struct vfio_pci_core_device *vdev =
 +		container_of(core_vdev, struct vfio_pci_core_device, vdev);
 +	unsigned long minsz;
  
 -	/* For backward compatibility, cannot require this */
 -	capsz = offsetofend(struct vfio_iommu_type1_info, cap_offset);
 +	if (cmd == VFIO_DEVICE_GET_INFO) {
 +		struct vfio_device_info info;
 +		struct vfio_info_cap caps = { .buf = NULL, .size = 0 };
 +		unsigned long capsz;
 +		int ret;
  
 -	if (copy_from_user(&info, arg, minsz))
 -		return -EFAULT;
 +		minsz = offsetofend(struct vfio_device_info, num_irqs);
  
 -	if (info.argsz < minsz)
 -		return -EINVAL;
 +		/* For backward compatibility, cannot require this */
 +		capsz = offsetofend(struct vfio_iommu_type1_info, cap_offset);
  
 -	if (info.argsz >= capsz) {
 -		minsz = capsz;
 -		info.cap_offset = 0;
 -	}
 +		if (copy_from_user(&info, (void __user *)arg, minsz))
 +			return -EFAULT;
  
 -	info.flags = VFIO_DEVICE_FLAGS_PCI;
 +		if (info.argsz < minsz)
 +			return -EINVAL;
  
 -	if (vdev->reset_works)
 -		info.flags |= VFIO_DEVICE_FLAGS_RESET;
 +		if (info.argsz >= capsz) {
 +			minsz = capsz;
 +			info.cap_offset = 0;
 +		}
  
 -	info.num_regions = VFIO_PCI_NUM_REGIONS + vdev->num_regions;
 -	info.num_irqs = VFIO_PCI_NUM_IRQS;
 +		info.flags = VFIO_DEVICE_FLAGS_PCI;
  
 -	ret = vfio_pci_info_zdev_add_caps(vdev, &caps);
 -	if (ret && ret != -ENODEV) {
 -		pci_warn(vdev->pdev,
 -			 "Failed to setup zPCI info capabilities\n");
 -		return ret;
 -	}
 +		if (vdev->reset_works)
 +			info.flags |= VFIO_DEVICE_FLAGS_RESET;
  
 -	if (caps.size) {
 -		info.flags |= VFIO_DEVICE_FLAGS_CAPS;
 -		if (info.argsz < sizeof(info) + caps.size) {
 -			info.argsz = sizeof(info) + caps.size;
 -		} else {
 -			vfio_info_cap_shift(&caps, sizeof(info));
 -			if (copy_to_user(arg + 1, caps.buf, caps.size)) {
 -				kfree(caps.buf);
 -				return -EFAULT;
 +		info.num_regions = VFIO_PCI_NUM_REGIONS + vdev->num_regions;
 +		info.num_irqs = VFIO_PCI_NUM_IRQS;
 +
 +		ret = vfio_pci_info_zdev_add_caps(vdev, &caps);
 +		if (ret && ret != -ENODEV) {
 +			pci_warn(vdev->pdev, "Failed to setup zPCI info capabilities\n");
 +			return ret;
 +		}
 +
 +		if (caps.size) {
 +			info.flags |= VFIO_DEVICE_FLAGS_CAPS;
 +			if (info.argsz < sizeof(info) + caps.size) {
 +				info.argsz = sizeof(info) + caps.size;
 +			} else {
 +				vfio_info_cap_shift(&caps, sizeof(info));
 +				if (copy_to_user((void __user *)arg +
 +						  sizeof(info), caps.buf,
 +						  caps.size)) {
 +					kfree(caps.buf);
 +					return -EFAULT;
 +				}
 +				info.cap_offset = sizeof(info);
  			}
 -			info.cap_offset = sizeof(*arg);
 +
 +			kfree(caps.buf);
  		}
  
 -		kfree(caps.buf);
 -	}
 +		return copy_to_user((void __user *)arg, &info, minsz) ?
 +			-EFAULT : 0;
  
 -	return copy_to_user(arg, &info, minsz) ? -EFAULT : 0;
 -}
 +	} else if (cmd == VFIO_DEVICE_GET_REGION_INFO) {
 +		struct pci_dev *pdev = vdev->pdev;
 +		struct vfio_region_info info;
 +		struct vfio_info_cap caps = { .buf = NULL, .size = 0 };
 +		int i, ret;
  
 -static int vfio_pci_ioctl_get_region_info(struct vfio_pci_core_device *vdev,
 -					  struct vfio_region_info __user *arg)
 -{
 -	unsigned long minsz = offsetofend(struct vfio_region_info, offset);
 -	struct pci_dev *pdev = vdev->pdev;
 -	struct vfio_region_info info;
 -	struct vfio_info_cap caps = { .buf = NULL, .size = 0 };
 -	int i, ret;
 +		minsz = offsetofend(struct vfio_region_info, offset);
  
 -	if (copy_from_user(&info, arg, minsz))
 -		return -EFAULT;
 +		if (copy_from_user(&info, (void __user *)arg, minsz))
 +			return -EFAULT;
  
 -	if (info.argsz < minsz)
 -		return -EINVAL;
 +		if (info.argsz < minsz)
 +			return -EINVAL;
  
 -	switch (info.index) {
 -	case VFIO_PCI_CONFIG_REGION_INDEX:
 -		info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 -		info.size = pdev->cfg_size;
 -		info.flags = VFIO_REGION_INFO_FLAG_READ |
 -			     VFIO_REGION_INFO_FLAG_WRITE;
 -		break;
 -	case VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:
 -		info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 -		info.size = pci_resource_len(pdev, info.index);
 -		if (!info.size) {
 -			info.flags = 0;
 +		switch (info.index) {
 +		case VFIO_PCI_CONFIG_REGION_INDEX:
 +			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 +			info.size = pdev->cfg_size;
 +			info.flags = VFIO_REGION_INFO_FLAG_READ |
 +				     VFIO_REGION_INFO_FLAG_WRITE;
  			break;
 -		}
 +		case VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:
 +			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 +			info.size = pci_resource_len(pdev, info.index);
 +			if (!info.size) {
 +				info.flags = 0;
 +				break;
 +			}
  
 -		info.flags = VFIO_REGION_INFO_FLAG_READ |
 -			     VFIO_REGION_INFO_FLAG_WRITE;
 -		if (vdev->bar_mmap_supported[info.index]) {
 -			info.flags |= VFIO_REGION_INFO_FLAG_MMAP;
 -			if (info.index == vdev->msix_bar) {
 -				ret = msix_mmappable_cap(vdev, &caps);
 -				if (ret)
 -					return ret;
 +			info.flags = VFIO_REGION_INFO_FLAG_READ |
 +				     VFIO_REGION_INFO_FLAG_WRITE;
 +			if (vdev->bar_mmap_supported[info.index]) {
 +				info.flags |= VFIO_REGION_INFO_FLAG_MMAP;
 +				if (info.index == vdev->msix_bar) {
 +					ret = msix_mmappable_cap(vdev, &caps);
 +					if (ret)
 +						return ret;
 +				}
  			}
 -		}
  
 -		break;
 -	case VFIO_PCI_ROM_REGION_INDEX: {
 -		void __iomem *io;
 -		size_t size;
 -		u16 cmd;
 -
 -		info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 -		info.flags = 0;
 -
 -		/* Report the BAR size, not the ROM size */
 -		info.size = pci_resource_len(pdev, info.index);
 -		if (!info.size) {
 -			/* Shadow ROMs appear as PCI option ROMs */
 -			if (pdev->resource[PCI_ROM_RESOURCE].flags &
 -			    IORESOURCE_ROM_SHADOW)
 -				info.size = 0x20000;
 -			else
 -				break;
 -		}
 +			break;
 +		case VFIO_PCI_ROM_REGION_INDEX:
 +		{
 +			void __iomem *io;
 +			size_t size;
 +			u16 cmd;
  
 -		/*
 -		 * Is it really there?  Enable memory decode for implicit access
 -		 * in pci_map_rom().
 -		 */
 -		cmd = vfio_pci_memory_lock_and_enable(vdev);
 -		io = pci_map_rom(pdev, &size);
 -		if (io) {
 -			info.flags = VFIO_REGION_INFO_FLAG_READ;
 -			pci_unmap_rom(pdev, io);
 -		} else {
 -			info.size = 0;
 -		}
 -		vfio_pci_memory_unlock_and_restore(vdev, cmd);
 +			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 +			info.flags = 0;
  
 -		break;
 -	}
 -	case VFIO_PCI_VGA_REGION_INDEX:
 -		if (!vdev->has_vga)
 -			return -EINVAL;
 +			/* Report the BAR size, not the ROM size */
 +			info.size = pci_resource_len(pdev, info.index);
 +			if (!info.size) {
 +				/* Shadow ROMs appear as PCI option ROMs */
 +				if (pdev->resource[PCI_ROM_RESOURCE].flags &
 +							IORESOURCE_ROM_SHADOW)
 +					info.size = 0x20000;
 +				else
 +					break;
 +			}
 +
 +			/*
 +			 * Is it really there?  Enable memory decode for
 +			 * implicit access in pci_map_rom().
 +			 */
 +			cmd = vfio_pci_memory_lock_and_enable(vdev);
 +			io = pci_map_rom(pdev, &size);
 +			if (io) {
 +				info.flags = VFIO_REGION_INFO_FLAG_READ;
 +				pci_unmap_rom(pdev, io);
 +			} else {
 +				info.size = 0;
 +			}
 +			vfio_pci_memory_unlock_and_restore(vdev, cmd);
  
 -		info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 -		info.size = 0xc0000;
 -		info.flags = VFIO_REGION_INFO_FLAG_READ |
 -			     VFIO_REGION_INFO_FLAG_WRITE;
 +			break;
 +		}
 +		case VFIO_PCI_VGA_REGION_INDEX:
 +			if (!vdev->has_vga)
 +				return -EINVAL;
  
 -		break;
 -	default: {
 -		struct vfio_region_info_cap_type cap_type = {
 -			.header.id = VFIO_REGION_INFO_CAP_TYPE,
 -			.header.version = 1
 -		};
 +			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 +			info.size = 0xc0000;
 +			info.flags = VFIO_REGION_INFO_FLAG_READ |
 +				     VFIO_REGION_INFO_FLAG_WRITE;
  
 -		if (info.index >= VFIO_PCI_NUM_REGIONS + vdev->num_regions)
 -			return -EINVAL;
 -		info.index = array_index_nospec(
 -			info.index, VFIO_PCI_NUM_REGIONS + vdev->num_regions);
 +			break;
 +		default:
 +		{
 +			struct vfio_region_info_cap_type cap_type = {
 +					.header.id = VFIO_REGION_INFO_CAP_TYPE,
 +					.header.version = 1 };
  
 -		i = info.index - VFIO_PCI_NUM_REGIONS;
 +			if (info.index >=
 +			    VFIO_PCI_NUM_REGIONS + vdev->num_regions)
 +				return -EINVAL;
 +			info.index = array_index_nospec(info.index,
 +							VFIO_PCI_NUM_REGIONS +
 +							vdev->num_regions);
  
 -		info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 -		info.size = vdev->region[i].size;
 -		info.flags = vdev->region[i].flags;
 +			i = info.index - VFIO_PCI_NUM_REGIONS;
  
 -		cap_type.type = vdev->region[i].type;
 -		cap_type.subtype = vdev->region[i].subtype;
 +			info.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);
 +			info.size = vdev->region[i].size;
 +			info.flags = vdev->region[i].flags;
  
 -		ret = vfio_info_add_capability(&caps, &cap_type.header,
 -					       sizeof(cap_type));
 -		if (ret)
 -			return ret;
 +			cap_type.type = vdev->region[i].type;
 +			cap_type.subtype = vdev->region[i].subtype;
  
 -		if (vdev->region[i].ops->add_capability) {
 -			ret = vdev->region[i].ops->add_capability(
 -				vdev, &vdev->region[i], &caps);
 +			ret = vfio_info_add_capability(&caps, &cap_type.header,
 +						       sizeof(cap_type));
  			if (ret)
  				return ret;
 +
 +			if (vdev->region[i].ops->add_capability) {
 +				ret = vdev->region[i].ops->add_capability(vdev,
 +						&vdev->region[i], &caps);
 +				if (ret)
 +					return ret;
 +			}
 +		}
  		}
 -	}
 -	}
  
 -	if (caps.size) {
 -		info.flags |= VFIO_REGION_INFO_FLAG_CAPS;
 -		if (info.argsz < sizeof(info) + caps.size) {
 -			info.argsz = sizeof(info) + caps.size;
 -			info.cap_offset = 0;
 -		} else {
 -			vfio_info_cap_shift(&caps, sizeof(info));
 -			if (copy_to_user(arg + 1, caps.buf, caps.size)) {
 -				kfree(caps.buf);
 -				return -EFAULT;
 +		if (caps.size) {
 +			info.flags |= VFIO_REGION_INFO_FLAG_CAPS;
 +			if (info.argsz < sizeof(info) + caps.size) {
 +				info.argsz = sizeof(info) + caps.size;
 +				info.cap_offset = 0;
 +			} else {
 +				vfio_info_cap_shift(&caps, sizeof(info));
 +				if (copy_to_user((void __user *)arg +
 +						  sizeof(info), caps.buf,
 +						  caps.size)) {
 +					kfree(caps.buf);
 +					return -EFAULT;
 +				}
 +				info.cap_offset = sizeof(info);
  			}
 -			info.cap_offset = sizeof(*arg);
 +
 +			kfree(caps.buf);
  		}
  
 -		kfree(caps.buf);
 -	}
 +		return copy_to_user((void __user *)arg, &info, minsz) ?
 +			-EFAULT : 0;
  
 -	return copy_to_user(arg, &info, minsz) ? -EFAULT : 0;
 -}
 +	} else if (cmd == VFIO_DEVICE_GET_IRQ_INFO) {
 +		struct vfio_irq_info info;
  
 -static int vfio_pci_ioctl_get_irq_info(struct vfio_pci_core_device *vdev,
 -				       struct vfio_irq_info __user *arg)
 -{
 -	unsigned long minsz = offsetofend(struct vfio_irq_info, count);
 -	struct vfio_irq_info info;
 +		minsz = offsetofend(struct vfio_irq_info, count);
  
 -	if (copy_from_user(&info, arg, minsz))
 -		return -EFAULT;
 +		if (copy_from_user(&info, (void __user *)arg, minsz))
 +			return -EFAULT;
  
 -	if (info.argsz < minsz || info.index >= VFIO_PCI_NUM_IRQS)
 -		return -EINVAL;
 +		if (info.argsz < minsz || info.index >= VFIO_PCI_NUM_IRQS)
 +			return -EINVAL;
  
 -	switch (info.index) {
 -	case VFIO_PCI_INTX_IRQ_INDEX ... VFIO_PCI_MSIX_IRQ_INDEX:
 -	case VFIO_PCI_REQ_IRQ_INDEX:
 -		break;
 -	case VFIO_PCI_ERR_IRQ_INDEX:
 -		if (pci_is_pcie(vdev->pdev))
 +		switch (info.index) {
 +		case VFIO_PCI_INTX_IRQ_INDEX ... VFIO_PCI_MSIX_IRQ_INDEX:
 +		case VFIO_PCI_REQ_IRQ_INDEX:
  			break;
 -		fallthrough;
 -	default:
 -		return -EINVAL;
 -	}
 +		case VFIO_PCI_ERR_IRQ_INDEX:
 +			if (pci_is_pcie(vdev->pdev))
 +				break;
 +			fallthrough;
 +		default:
 +			return -EINVAL;
 +		}
  
 -	info.flags = VFIO_IRQ_INFO_EVENTFD;
 +		info.flags = VFIO_IRQ_INFO_EVENTFD;
  
 -	info.count = vfio_pci_get_irq_count(vdev, info.index);
 +		info.count = vfio_pci_get_irq_count(vdev, info.index);
  
 -	if (info.index == VFIO_PCI_INTX_IRQ_INDEX)
 -		info.flags |=
 -			(VFIO_IRQ_INFO_MASKABLE | VFIO_IRQ_INFO_AUTOMASKED);
 -	else
 -		info.flags |= VFIO_IRQ_INFO_NORESIZE;
 +		if (info.index == VFIO_PCI_INTX_IRQ_INDEX)
 +			info.flags |= (VFIO_IRQ_INFO_MASKABLE |
 +				       VFIO_IRQ_INFO_AUTOMASKED);
 +		else
 +			info.flags |= VFIO_IRQ_INFO_NORESIZE;
  
 -	return copy_to_user(arg, &info, minsz) ? -EFAULT : 0;
 -}
 +		return copy_to_user((void __user *)arg, &info, minsz) ?
 +			-EFAULT : 0;
  
 -static int vfio_pci_ioctl_set_irqs(struct vfio_pci_core_device *vdev,
 -				   struct vfio_irq_set __user *arg)
 -{
 -	unsigned long minsz = offsetofend(struct vfio_irq_set, count);
 -	struct vfio_irq_set hdr;
 -	u8 *data = NULL;
 -	int max, ret = 0;
 -	size_t data_size = 0;
 +	} else if (cmd == VFIO_DEVICE_SET_IRQS) {
 +		struct vfio_irq_set hdr;
 +		u8 *data = NULL;
 +		int max, ret = 0;
 +		size_t data_size = 0;
  
 -	if (copy_from_user(&hdr, arg, minsz))
 -		return -EFAULT;
 +		minsz = offsetofend(struct vfio_irq_set, count);
  
 -	max = vfio_pci_get_irq_count(vdev, hdr.index);
 +		if (copy_from_user(&hdr, (void __user *)arg, minsz))
 +			return -EFAULT;
  
 -	ret = vfio_set_irqs_validate_and_prepare(&hdr, max, VFIO_PCI_NUM_IRQS,
 -						 &data_size);
 -	if (ret)
 -		return ret;
 +		max = vfio_pci_get_irq_count(vdev, hdr.index);
  
 -	if (data_size) {
 -		data = memdup_user(&arg->data, data_size);
 -		if (IS_ERR(data))
 -			return PTR_ERR(data);
 -	}
 +		ret = vfio_set_irqs_validate_and_prepare(&hdr, max,
 +						 VFIO_PCI_NUM_IRQS, &data_size);
 +		if (ret)
 +			return ret;
  
 -	mutex_lock(&vdev->igate);
 +		if (data_size) {
 +			data = memdup_user((void __user *)(arg + minsz),
 +					    data_size);
 +			if (IS_ERR(data))
 +				return PTR_ERR(data);
 +		}
  
 -	ret = vfio_pci_set_irqs_ioctl(vdev, hdr.flags, hdr.index, hdr.start,
 -				      hdr.count, data);
 +		mutex_lock(&vdev->igate);
  
 -	mutex_unlock(&vdev->igate);
 -	kfree(data);
 +		ret = vfio_pci_set_irqs_ioctl(vdev, hdr.flags, hdr.index,
 +					      hdr.start, hdr.count, data);
  
 -	return ret;
 -}
 +		mutex_unlock(&vdev->igate);
 +		kfree(data);
  
 -static int vfio_pci_ioctl_reset(struct vfio_pci_core_device *vdev,
 -				void __user *arg)
 -{
 -	int ret;
 +		return ret;
  
 -	if (!vdev->reset_works)
 -		return -EINVAL;
 +	} else if (cmd == VFIO_DEVICE_RESET) {
 +		int ret;
  
 -	vfio_pci_zap_and_down_write_memory_lock(vdev);
 +		if (!vdev->reset_works)
 +			return -EINVAL;
  
 -	/*
 -	 * This function can be invoked while the power state is non-D0. If
 -	 * pci_try_reset_function() has been called while the power state is
 -	 * non-D0, then pci_try_reset_function() will internally set the power
 -	 * state to D0 without vfio driver involvement. For the devices which
 -	 * have NoSoftRst-, the reset function can cause the PCI config space
 -	 * reset without restoring the original state (saved locally in
 -	 * 'vdev->pm_save').
 -	 */
 -	vfio_pci_set_power_state(vdev, PCI_D0);
 +		vfio_pci_zap_and_down_write_memory_lock(vdev);
  
 -	ret = pci_try_reset_function(vdev->pdev);
 -	up_write(&vdev->memory_lock);
 +		/*
 +		 * This function can be invoked while the power state is non-D0.
 +		 * If pci_try_reset_function() has been called while the power
 +		 * state is non-D0, then pci_try_reset_function() will
 +		 * internally set the power state to D0 without vfio driver
 +		 * involvement. For the devices which have NoSoftRst-, the
 +		 * reset function can cause the PCI config space reset without
 +		 * restoring the original state (saved locally in
 +		 * 'vdev->pm_save').
 +		 */
 +		vfio_pci_set_power_state(vdev, PCI_D0);
  
 -	return ret;
 -}
 +		ret = pci_try_reset_function(vdev->pdev);
 +		up_write(&vdev->memory_lock);
  
 -static int vfio_pci_ioctl_get_pci_hot_reset_info(
 -	struct vfio_pci_core_device *vdev,
 -	struct vfio_pci_hot_reset_info __user *arg)
 -{
 -	unsigned long minsz =
 -		offsetofend(struct vfio_pci_hot_reset_info, count);
 -	struct vfio_pci_hot_reset_info hdr;
 -	struct vfio_pci_fill_info fill = { 0 };
 -	struct vfio_pci_dependent_device *devices = NULL;
 -	bool slot = false;
 -	int ret = 0;
 +		return ret;
  
 -	if (copy_from_user(&hdr, arg, minsz))
 -		return -EFAULT;
 +	} else if (cmd == VFIO_DEVICE_GET_PCI_HOT_RESET_INFO) {
 +		struct vfio_pci_hot_reset_info hdr;
 +		struct vfio_pci_fill_info fill = { 0 };
 +		struct vfio_pci_dependent_device *devices = NULL;
 +		bool slot = false;
 +		int ret = 0;
  
 -	if (hdr.argsz < minsz)
 -		return -EINVAL;
 +		minsz = offsetofend(struct vfio_pci_hot_reset_info, count);
  
 -	hdr.flags = 0;
 +		if (copy_from_user(&hdr, (void __user *)arg, minsz))
 +			return -EFAULT;
  
 -	/* Can we do a slot or bus reset or neither? */
 -	if (!pci_probe_reset_slot(vdev->pdev->slot))
 -		slot = true;
 -	else if (pci_probe_reset_bus(vdev->pdev->bus))
 -		return -ENODEV;
 +		if (hdr.argsz < minsz)
 +			return -EINVAL;
  
 -	/* How many devices are affected? */
 -	ret = vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_count_devs,
 -					    &fill.max, slot);
 -	if (ret)
 -		return ret;
 +		hdr.flags = 0;
  
 -	WARN_ON(!fill.max); /* Should always be at least one */
 +		/* Can we do a slot or bus reset or neither? */
 +		if (!pci_probe_reset_slot(vdev->pdev->slot))
 +			slot = true;
 +		else if (pci_probe_reset_bus(vdev->pdev->bus))
 +			return -ENODEV;
  
 -	/*
 -	 * If there's enough space, fill it now, otherwise return -ENOSPC and
 -	 * the number of devices affected.
 -	 */
 -	if (hdr.argsz < sizeof(hdr) + (fill.max * sizeof(*devices))) {
 -		ret = -ENOSPC;
 -		hdr.count = fill.max;
 -		goto reset_info_exit;
 -	}
 +		/* How many devices are affected? */
 +		ret = vfio_pci_for_each_slot_or_bus(vdev->pdev,
 +						    vfio_pci_count_devs,
 +						    &fill.max, slot);
 +		if (ret)
 +			return ret;
  
 -	devices = kcalloc(fill.max, sizeof(*devices), GFP_KERNEL);
 -	if (!devices)
 -		return -ENOMEM;
 +		WARN_ON(!fill.max); /* Should always be at least one */
  
 -	fill.devices = devices;
 +		/*
 +		 * If there's enough space, fill it now, otherwise return
 +		 * -ENOSPC and the number of devices affected.
 +		 */
 +		if (hdr.argsz < sizeof(hdr) + (fill.max * sizeof(*devices))) {
 +			ret = -ENOSPC;
 +			hdr.count = fill.max;
 +			goto reset_info_exit;
 +		}
  
 -	ret = vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_fill_devs,
 -					    &fill, slot);
 +		devices = kcalloc(fill.max, sizeof(*devices), GFP_KERNEL);
 +		if (!devices)
 +			return -ENOMEM;
  
 -	/*
 -	 * If a device was removed between counting and filling, we may come up
 -	 * short of fill.max.  If a device was added, we'll have a return of
 -	 * -EAGAIN above.
 -	 */
 -	if (!ret)
 -		hdr.count = fill.cur;
 +		fill.devices = devices;
  
 -reset_info_exit:
 -	if (copy_to_user(arg, &hdr, minsz))
 -		ret = -EFAULT;
 +		ret = vfio_pci_for_each_slot_or_bus(vdev->pdev,
 +						    vfio_pci_fill_devs,
 +						    &fill, slot);
  
 -	if (!ret) {
 -		if (copy_to_user(&arg->devices, devices,
 -				 hdr.count * sizeof(*devices)))
 -			ret = -EFAULT;
 -	}
 +		/*
 +		 * If a device was removed between counting and filling,
 +		 * we may come up short of fill.max.  If a device was
 +		 * added, we'll have a return of -EAGAIN above.
 +		 */
 +		if (!ret)
 +			hdr.count = fill.cur;
  
 -	kfree(devices);
 -	return ret;
 -}
 +reset_info_exit:
 +		if (copy_to_user((void __user *)arg, &hdr, minsz))
 +			ret = -EFAULT;
  
 -static int vfio_pci_ioctl_pci_hot_reset(struct vfio_pci_core_device *vdev,
 -					struct vfio_pci_hot_reset __user *arg)
 -{
 -	unsigned long minsz = offsetofend(struct vfio_pci_hot_reset, count);
 -	struct vfio_pci_hot_reset hdr;
 -	int32_t *group_fds;
 -	struct file **files;
 -	struct vfio_pci_group_info info;
 -	bool slot = false;
 -	int file_idx, count = 0, ret = 0;
 +		if (!ret) {
 +			if (copy_to_user((void __user *)(arg + minsz), devices,
 +					 hdr.count * sizeof(*devices)))
 +				ret = -EFAULT;
 +		}
  
 -	if (copy_from_user(&hdr, arg, minsz))
 -		return -EFAULT;
 +		kfree(devices);
 +		return ret;
  
 -	if (hdr.argsz < minsz || hdr.flags)
 -		return -EINVAL;
 +	} else if (cmd == VFIO_DEVICE_PCI_HOT_RESET) {
 +		struct vfio_pci_hot_reset hdr;
 +		int32_t *group_fds;
 +		struct file **files;
 +		struct vfio_pci_group_info info;
 +		bool slot = false;
 +		int file_idx, count = 0, ret = 0;
  
 -	/* Can we do a slot or bus reset or neither? */
 -	if (!pci_probe_reset_slot(vdev->pdev->slot))
 -		slot = true;
 -	else if (pci_probe_reset_bus(vdev->pdev->bus))
 -		return -ENODEV;
 +		minsz = offsetofend(struct vfio_pci_hot_reset, count);
  
 -	/*
 -	 * We can't let userspace give us an arbitrarily large buffer to copy,
 -	 * so verify how many we think there could be.  Note groups can have
 -	 * multiple devices so one group per device is the max.
 -	 */
 -	ret = vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_count_devs,
 -					    &count, slot);
 -	if (ret)
 -		return ret;
 +		if (copy_from_user(&hdr, (void __user *)arg, minsz))
 +			return -EFAULT;
  
 -	/* Somewhere between 1 and count is OK */
 -	if (!hdr.count || hdr.count > count)
 -		return -EINVAL;
 +		if (hdr.argsz < minsz || hdr.flags)
 +			return -EINVAL;
  
 -	group_fds = kcalloc(hdr.count, sizeof(*group_fds), GFP_KERNEL);
 -	files = kcalloc(hdr.count, sizeof(*files), GFP_KERNEL);
 -	if (!group_fds || !files) {
 -		kfree(group_fds);
 -		kfree(files);
 -		return -ENOMEM;
 -	}
 +		/* Can we do a slot or bus reset or neither? */
 +		if (!pci_probe_reset_slot(vdev->pdev->slot))
 +			slot = true;
 +		else if (pci_probe_reset_bus(vdev->pdev->bus))
 +			return -ENODEV;
  
 -	if (copy_from_user(group_fds, arg->group_fds,
 -			   hdr.count * sizeof(*group_fds))) {
 -		kfree(group_fds);
 -		kfree(files);
 -		return -EFAULT;
 -	}
 +		/*
 +		 * We can't let userspace give us an arbitrarily large
 +		 * buffer to copy, so verify how many we think there
 +		 * could be.  Note groups can have multiple devices so
 +		 * one group per device is the max.
 +		 */
 +		ret = vfio_pci_for_each_slot_or_bus(vdev->pdev,
 +						    vfio_pci_count_devs,
 +						    &count, slot);
 +		if (ret)
 +			return ret;
  
 -	/*
 -	 * For each group_fd, get the group through the vfio external user
 -	 * interface and store the group and iommu ID.  This ensures the group
 -	 * is held across the reset.
 -	 */
 -	for (file_idx = 0; file_idx < hdr.count; file_idx++) {
 -		struct file *file = fget(group_fds[file_idx]);
 +		/* Somewhere between 1 and count is OK */
 +		if (!hdr.count || hdr.count > count)
 +			return -EINVAL;
  
 -		if (!file) {
 -			ret = -EBADF;
 -			break;
 +		group_fds = kcalloc(hdr.count, sizeof(*group_fds), GFP_KERNEL);
 +		files = kcalloc(hdr.count, sizeof(*files), GFP_KERNEL);
 +		if (!group_fds || !files) {
 +			kfree(group_fds);
 +			kfree(files);
 +			return -ENOMEM;
  		}
  
++<<<<<<< HEAD
 +		if (copy_from_user(group_fds, (void __user *)(arg + minsz),
 +				   hdr.count * sizeof(*group_fds))) {
 +			kfree(group_fds);
 +			kfree(files);
 +			return -EFAULT;
++=======
+ 		/* Ensure the FD is a vfio group FD.*/
+ 		if (!vfio_file_is_group(file)) {
+ 			fput(file);
+ 			ret = -EINVAL;
+ 			break;
++>>>>>>> 4b22ef042d6f (vfio: Add vfio_file_is_group())
  		}
  
 -		files[file_idx] = file;
 -	}
 +		/*
 +		 * For each group_fd, get the group through the vfio external
 +		 * user interface and store the group and iommu ID.  This
 +		 * ensures the group is held across the reset.
 +		 */
 +		for (file_idx = 0; file_idx < hdr.count; file_idx++) {
 +			struct file *file = fget(group_fds[file_idx]);
  
 -	kfree(group_fds);
 +			if (!file) {
 +				ret = -EBADF;
 +				break;
 +			}
  
 -	/* release reference to groups on error */
 -	if (ret)
 -		goto hot_reset_release;
 +			/* Ensure the FD is a vfio group FD.*/
 +			if (!vfio_file_iommu_group(file)) {
 +				fput(file);
 +				ret = -EINVAL;
 +				break;
 +			}
  
 -	info.count = hdr.count;
 -	info.files = files;
 +			files[file_idx] = file;
 +		}
  
 -	ret = vfio_pci_dev_set_hot_reset(vdev->vdev.dev_set, &info);
 +		kfree(group_fds);
  
 -hot_reset_release:
 -	for (file_idx--; file_idx >= 0; file_idx--)
 -		fput(files[file_idx]);
 +		/* release reference to groups on error */
 +		if (ret)
 +			goto hot_reset_release;
  
 -	kfree(files);
 -	return ret;
 -}
 +		info.count = hdr.count;
 +		info.files = files;
  
 -static int vfio_pci_ioctl_ioeventfd(struct vfio_pci_core_device *vdev,
 -				    struct vfio_device_ioeventfd __user *arg)
 -{
 -	unsigned long minsz = offsetofend(struct vfio_device_ioeventfd, fd);
 -	struct vfio_device_ioeventfd ioeventfd;
 -	int count;
 +		ret = vfio_pci_dev_set_hot_reset(vdev->vdev.dev_set, &info);
  
 -	if (copy_from_user(&ioeventfd, arg, minsz))
 -		return -EFAULT;
 +hot_reset_release:
 +		for (file_idx--; file_idx >= 0; file_idx--)
 +			fput(files[file_idx]);
  
 -	if (ioeventfd.argsz < minsz)
 -		return -EINVAL;
 +		kfree(files);
 +		return ret;
 +	} else if (cmd == VFIO_DEVICE_IOEVENTFD) {
 +		struct vfio_device_ioeventfd ioeventfd;
 +		int count;
  
 -	if (ioeventfd.flags & ~VFIO_DEVICE_IOEVENTFD_SIZE_MASK)
 -		return -EINVAL;
 +		minsz = offsetofend(struct vfio_device_ioeventfd, fd);
  
 -	count = ioeventfd.flags & VFIO_DEVICE_IOEVENTFD_SIZE_MASK;
 +		if (copy_from_user(&ioeventfd, (void __user *)arg, minsz))
 +			return -EFAULT;
  
 -	if (hweight8(count) != 1 || ioeventfd.fd < -1)
 -		return -EINVAL;
 +		if (ioeventfd.argsz < minsz)
 +			return -EINVAL;
  
 -	return vfio_pci_ioeventfd(vdev, ioeventfd.offset, ioeventfd.data, count,
 -				  ioeventfd.fd);
 -}
 +		if (ioeventfd.flags & ~VFIO_DEVICE_IOEVENTFD_SIZE_MASK)
 +			return -EINVAL;
  
 -long vfio_pci_core_ioctl(struct vfio_device *core_vdev, unsigned int cmd,
 -			 unsigned long arg)
 -{
 -	struct vfio_pci_core_device *vdev =
 -		container_of(core_vdev, struct vfio_pci_core_device, vdev);
 -	void __user *uarg = (void __user *)arg;
 -
 -	switch (cmd) {
 -	case VFIO_DEVICE_GET_INFO:
 -		return vfio_pci_ioctl_get_info(vdev, uarg);
 -	case VFIO_DEVICE_GET_IRQ_INFO:
 -		return vfio_pci_ioctl_get_irq_info(vdev, uarg);
 -	case VFIO_DEVICE_GET_PCI_HOT_RESET_INFO:
 -		return vfio_pci_ioctl_get_pci_hot_reset_info(vdev, uarg);
 -	case VFIO_DEVICE_GET_REGION_INFO:
 -		return vfio_pci_ioctl_get_region_info(vdev, uarg);
 -	case VFIO_DEVICE_IOEVENTFD:
 -		return vfio_pci_ioctl_ioeventfd(vdev, uarg);
 -	case VFIO_DEVICE_PCI_HOT_RESET:
 -		return vfio_pci_ioctl_pci_hot_reset(vdev, uarg);
 -	case VFIO_DEVICE_RESET:
 -		return vfio_pci_ioctl_reset(vdev, uarg);
 -	case VFIO_DEVICE_SET_IRQS:
 -		return vfio_pci_ioctl_set_irqs(vdev, uarg);
 -	default:
 -		return -ENOTTY;
 +		count = ioeventfd.flags & VFIO_DEVICE_IOEVENTFD_SIZE_MASK;
 +
 +		if (hweight8(count) != 1 || ioeventfd.fd < -1)
 +			return -EINVAL;
 +
 +		return vfio_pci_ioeventfd(vdev, ioeventfd.offset,
 +					  ioeventfd.data, count, ioeventfd.fd);
  	}
 +	return -ENOTTY;
  }
  EXPORT_SYMBOL_GPL(vfio_pci_core_ioctl);
  
* Unmerged path drivers/vfio/pci/vfio_pci_core.c
diff --git a/drivers/vfio/vfio_main.c b/drivers/vfio/vfio_main.c
index eb849d5b81b0..ebaa8b9d2c3c 100644
--- a/drivers/vfio/vfio_main.c
+++ b/drivers/vfio/vfio_main.c
@@ -2033,17 +2033,31 @@ static const struct file_operations vfio_device_fops = {
  * @file: VFIO group file
  *
  * The returned iommu_group is valid as long as a ref is held on the file.
+ * This function is deprecated, only the SPAPR path in kvm should call it.
  */
 struct iommu_group *vfio_file_iommu_group(struct file *file)
 {
 	struct vfio_group *group = file->private_data;
 
-	if (file->f_op != &vfio_group_fops)
+	if (!IS_ENABLED(CONFIG_SPAPR_TCE_IOMMU))
+		return NULL;
+
+	if (!vfio_file_is_group(file))
 		return NULL;
 	return group->iommu_group;
 }
 EXPORT_SYMBOL_GPL(vfio_file_iommu_group);
 
+/**
+ * vfio_file_is_group - True if the file is usable with VFIO aPIS
+ * @file: VFIO group file
+ */
+bool vfio_file_is_group(struct file *file)
+{
+	return file->f_op == &vfio_group_fops;
+}
+EXPORT_SYMBOL_GPL(vfio_file_is_group);
+
 /**
  * vfio_file_enforced_coherent - True if the DMA associated with the VFIO file
  *        is always CPU cache coherent
diff --git a/include/linux/vfio.h b/include/linux/vfio.h
index ff6794bdea7f..dc99d484ea19 100644
--- a/include/linux/vfio.h
+++ b/include/linux/vfio.h
@@ -185,6 +185,7 @@ int vfio_mig_get_next_state(struct vfio_device *device,
  * External user API
  */
 struct iommu_group *vfio_file_iommu_group(struct file *file);
+bool vfio_file_is_group(struct file *file);
 bool vfio_file_enforced_coherent(struct file *file);
 void vfio_file_set_kvm(struct file *file, struct kvm *kvm);
 bool vfio_file_has_dev(struct file *file, struct vfio_device *device);
diff --git a/virt/kvm/vfio.c b/virt/kvm/vfio.c
index ce1b01d02c51..54aec3b0559c 100644
--- a/virt/kvm/vfio.c
+++ b/virt/kvm/vfio.c
@@ -61,6 +61,23 @@ static bool kvm_vfio_file_enforced_coherent(struct file *file)
 	return ret;
 }
 
+static bool kvm_vfio_file_is_group(struct file *file)
+{
+	bool (*fn)(struct file *file);
+	bool ret;
+
+	fn = symbol_get(vfio_file_is_group);
+	if (!fn)
+		return false;
+
+	ret = fn(file);
+
+	symbol_put(vfio_file_is_group);
+
+	return ret;
+}
+
+#ifdef CONFIG_SPAPR_TCE_IOMMU
 static struct iommu_group *kvm_vfio_file_iommu_group(struct file *file)
 {
 	struct iommu_group *(*fn)(struct file *file);
@@ -77,7 +94,6 @@ static struct iommu_group *kvm_vfio_file_iommu_group(struct file *file)
 	return ret;
 }
 
-#ifdef CONFIG_SPAPR_TCE_IOMMU
 static void kvm_spapr_tce_release_vfio_group(struct kvm *kvm,
 					     struct kvm_vfio_group *kvg)
 {
@@ -136,7 +152,7 @@ static int kvm_vfio_group_add(struct kvm_device *dev, unsigned int fd)
 		return -EBADF;
 
 	/* Ensure the FD is a vfio group FD.*/
-	if (!kvm_vfio_file_iommu_group(filp)) {
+	if (!kvm_vfio_file_is_group(filp)) {
 		ret = -EINVAL;
 		goto err_fput;
 	}
