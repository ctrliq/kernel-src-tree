workqueue: Move the code of waking a worker up in unbind_workers()

jira LE-1907
Rebuild_History Non-Buildable kernel-5.14.0-284.30.1.el9_2
commit-author Lai Jiangshan <laijs@linux.alibaba.com>
commit 989442d73757868118a73b92732b549a73c9ce35
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-284.30.1.el9_2/989442d7.failed

In unbind_workers(), there are two pool->lock held sections separated
by the code of zapping nr_running.  wake_up_worker() needs to be in
pool->lock held section and after zapping nr_running.  And zapping
nr_running had to be after schedule() when the local wake up
functionality was in use.  Now, the call to schedule() has been removed
along with the local wake up functionality, so the code can be merged
into the same pool->lock held section.

The diffstat shows that it is other code moved down because the diff
tools can not know the meaning of merging lock sections by swapping
two code blocks.

	Signed-off-by: Lai Jiangshan <laijs@linux.alibaba.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 989442d73757868118a73b92732b549a73c9ce35)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/workqueue.c
diff --cc kernel/workqueue.c
index 813beee1a27a,403387e9a924..000000000000
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@@ -5016,24 -4982,12 +5010,27 @@@ static void unbind_workers(int cpu
  
  		pool->flags |= POOL_DISASSOCIATED;
  
++<<<<<<< HEAD
 +		raw_spin_unlock_irq(&pool->lock);
 +
 +		for_each_pool_worker(worker, pool) {
 +			kthread_set_per_cpu(worker->task, -1);
 +			if (cpumask_intersects(wq_unbound_cpumask, cpu_active_mask))
 +				WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, wq_unbound_cpumask) < 0);
 +			else
 +				WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_possible_mask) < 0);
 +		}
 +
 +		mutex_unlock(&wq_pool_attach_mutex);
 +
++=======
++>>>>>>> 989442d73757 (workqueue: Move the code of waking a worker up in unbind_workers())
  		/*
- 		 * Sched callbacks are disabled now.  Zap nr_running.
- 		 * After this, nr_running stays zero and need_more_worker()
- 		 * and keep_working() are always true as long as the
- 		 * worklist is not empty.  This pool now behaves as an
- 		 * unbound (in terms of concurrency management) pool which
+ 		 * The handling of nr_running in sched callbacks are disabled
+ 		 * now.  Zap nr_running.  After this, nr_running stays zero and
+ 		 * need_more_worker() and keep_working() are always true as
+ 		 * long as the worklist is not empty.  This pool now behaves as
+ 		 * an unbound (in terms of concurrency management) pool which
  		 * are served by workers tied to the pool.
  		 */
  		atomic_set(&pool->nr_running, 0);
* Unmerged path kernel/workqueue.c
