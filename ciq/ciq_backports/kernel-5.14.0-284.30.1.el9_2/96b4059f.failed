rxrpc: Remove call->state_lock

jira LE-1907
Rebuild_History Non-Buildable kernel-5.14.0-284.30.1.el9_2
commit-author David Howells <dhowells@redhat.com>
commit 96b4059f43ce69e9c590f77d6ce3e99888d5cfe6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-284.30.1.el9_2/96b4059f.failed

All the setters of call->state are now in the I/O thread and thus the state
lock is now unnecessary.

	Signed-off-by: David Howells <dhowells@redhat.com>
cc: Marc Dionne <marc.dionne@auristor.com>
cc: linux-afs@lists.infradead.org
(cherry picked from commit 96b4059f43ce69e9c590f77d6ce3e99888d5cfe6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/rxrpc/ar-internal.h
#	net/rxrpc/call_accept.c
#	net/rxrpc/call_event.c
#	net/rxrpc/call_object.c
#	net/rxrpc/call_state.c
#	net/rxrpc/conn_client.c
#	net/rxrpc/conn_event.c
#	net/rxrpc/input.c
#	net/rxrpc/output.c
#	net/rxrpc/rxkad.c
#	net/rxrpc/sendmsg.c
diff --cc net/rxrpc/ar-internal.h
index 46ce41afb431,751b1903fd6e..000000000000
--- a/net/rxrpc/ar-internal.h
+++ b/net/rxrpc/ar-internal.h
@@@ -601,13 -631,15 +601,21 @@@ struct rxrpc_call 
  	unsigned long		flags;
  	unsigned long		events;
  	spinlock_t		notify_lock;	/* Kernel notification lock */
++<<<<<<< HEAD
 +	rwlock_t		state_lock;	/* lock for state transition */
 +	u32			abort_code;	/* Local/remote abort code */
++=======
+ 	unsigned int		send_abort_why; /* Why the abort [enum rxrpc_abort_reason] */
+ 	s32			send_abort;	/* Abort code to be sent */
+ 	short			send_abort_err;	/* Error to be associated with the abort */
+ 	rxrpc_seq_t		send_abort_seq;	/* DATA packet that incurred the abort (or 0) */
+ 	s32			abort_code;	/* Local/remote abort code */
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  	int			error;		/* Local error incurred */
- 	enum rxrpc_call_state	state;		/* current state of call */
+ 	enum rxrpc_call_state	_state;		/* Current state of call (needs barrier) */
  	enum rxrpc_call_completion completion;	/* Call completion condition */
  	refcount_t		ref;
 +	u16			service_id;	/* service ID */
  	u8			security_ix;	/* Security type */
  	enum rxrpc_interruptibility interruptibility; /* At what point call may be interrupted */
  	u32			call_id;	/* call ID on connection  */
@@@ -855,6 -886,52 +863,55 @@@ static inline bool rxrpc_is_client_call
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * call_state.c
+  */
+ bool rxrpc_set_call_completion(struct rxrpc_call *call,
+ 			       enum rxrpc_call_completion compl,
+ 			       u32 abort_code,
+ 			       int error);
+ bool rxrpc_call_completed(struct rxrpc_call *call);
+ bool rxrpc_abort_call(struct rxrpc_call *call, rxrpc_seq_t seq,
+ 		      u32 abort_code, int error, enum rxrpc_abort_reason why);
+ void rxrpc_prefail_call(struct rxrpc_call *call, enum rxrpc_call_completion compl,
+ 			int error);
+ 
+ static inline void rxrpc_set_call_state(struct rxrpc_call *call,
+ 					enum rxrpc_call_state state)
+ {
+ 	/* Order write of completion info before write of ->state. */
+ 	smp_store_release(&call->_state, state);
+ }
+ 
+ static inline enum rxrpc_call_state __rxrpc_call_state(const struct rxrpc_call *call)
+ {
+ 	return call->_state; /* Only inside I/O thread */
+ }
+ 
+ static inline bool __rxrpc_call_is_complete(const struct rxrpc_call *call)
+ {
+ 	return __rxrpc_call_state(call) == RXRPC_CALL_COMPLETE;
+ }
+ 
+ static inline enum rxrpc_call_state rxrpc_call_state(const struct rxrpc_call *call)
+ {
+ 	/* Order read ->state before read of completion info. */
+ 	return smp_load_acquire(&call->_state);
+ }
+ 
+ static inline bool rxrpc_call_is_complete(const struct rxrpc_call *call)
+ {
+ 	return rxrpc_call_state(call) == RXRPC_CALL_COMPLETE;
+ }
+ 
+ static inline bool rxrpc_call_has_failed(const struct rxrpc_call *call)
+ {
+ 	return rxrpc_call_is_complete(call) && call->completion != RXRPC_CALL_SUCCEEDED;
+ }
+ 
+ /*
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
   * conn_client.c
   */
  extern unsigned int rxrpc_reap_client_connections;
diff --cc net/rxrpc/call_accept.c
index afe1f587aaf0,3fbf2fcaaf9e..000000000000
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@@ -100,7 -99,8 +100,12 @@@ static int rxrpc_service_prealloc_one(s
  	if (!call)
  		return -ENOMEM;
  	call->flags |= (1 << RXRPC_CALL_IS_SERVICE);
++<<<<<<< HEAD
 +	call->state = RXRPC_CALL_SERVER_PREALLOC;
++=======
+ 	rxrpc_set_call_state(call, RXRPC_CALL_SERVER_PREALLOC);
+ 	__set_bit(RXRPC_CALL_EV_INITIAL_PING, &call->events);
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  
  	trace_rxrpc_call(call->debug_id, refcount_read(&call->ref),
  			 user_call_ID, rxrpc_call_new_prealloc_service);
diff --cc net/rxrpc/call_event.c
index a95f4604cb29,1abdef15debc..000000000000
--- a/net/rxrpc/call_event.c
+++ b/net/rxrpc/call_event.c
@@@ -292,42 -252,153 +292,167 @@@ out
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Start transmitting the reply to a service.  This cancels the need to ACK the
+  * request if we haven't yet done so.
+  */
+ static void rxrpc_begin_service_reply(struct rxrpc_call *call)
+ {
+ 	unsigned long now = jiffies;
+ 
+ 	rxrpc_set_call_state(call, RXRPC_CALL_SERVER_SEND_REPLY);
+ 	WRITE_ONCE(call->delay_ack_at, now + MAX_JIFFY_OFFSET);
+ 	if (call->ackr_reason == RXRPC_ACK_DELAY)
+ 		call->ackr_reason = 0;
+ 	trace_rxrpc_timer(call, rxrpc_timer_init_for_send_reply, now);
+ }
+ 
+ /*
+  * Close the transmission phase.  After this point there is no more data to be
+  * transmitted in the call.
+  */
+ static void rxrpc_close_tx_phase(struct rxrpc_call *call)
+ {
+ 	_debug("________awaiting reply/ACK__________");
+ 
+ 	switch (__rxrpc_call_state(call)) {
+ 	case RXRPC_CALL_CLIENT_SEND_REQUEST:
+ 		rxrpc_set_call_state(call, RXRPC_CALL_CLIENT_AWAIT_REPLY);
+ 		break;
+ 	case RXRPC_CALL_SERVER_SEND_REPLY:
+ 		rxrpc_set_call_state(call, RXRPC_CALL_SERVER_AWAIT_ACK);
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ }
+ 
+ static bool rxrpc_tx_window_has_space(struct rxrpc_call *call)
+ {
+ 	unsigned int winsize = min_t(unsigned int, call->tx_winsize,
+ 				     call->cong_cwnd + call->cong_extra);
+ 	rxrpc_seq_t window = call->acks_hard_ack, wtop = window + winsize;
+ 	rxrpc_seq_t tx_top = call->tx_top;
+ 	int space;
+ 
+ 	space = wtop - tx_top;
+ 	return space > 0;
+ }
+ 
+ /*
+  * Decant some if the sendmsg prepared queue into the transmission buffer.
+  */
+ static void rxrpc_decant_prepared_tx(struct rxrpc_call *call)
+ {
+ 	struct rxrpc_txbuf *txb;
+ 
+ 	if (!test_bit(RXRPC_CALL_EXPOSED, &call->flags)) {
+ 		if (list_empty(&call->tx_sendmsg))
+ 			return;
+ 		rxrpc_expose_client_call(call);
+ 	}
+ 
+ 	while ((txb = list_first_entry_or_null(&call->tx_sendmsg,
+ 					       struct rxrpc_txbuf, call_link))) {
+ 		spin_lock(&call->tx_lock);
+ 		list_del(&txb->call_link);
+ 		spin_unlock(&call->tx_lock);
+ 
+ 		call->tx_top = txb->seq;
+ 		list_add_tail(&txb->call_link, &call->tx_buffer);
+ 
+ 		if (txb->wire.flags & RXRPC_LAST_PACKET)
+ 			rxrpc_close_tx_phase(call);
+ 
+ 		rxrpc_transmit_one(call, txb);
+ 
+ 		if (!rxrpc_tx_window_has_space(call))
+ 			break;
+ 	}
+ }
+ 
+ static void rxrpc_transmit_some_data(struct rxrpc_call *call)
+ {
+ 	switch (__rxrpc_call_state(call)) {
+ 	case RXRPC_CALL_SERVER_ACK_REQUEST:
+ 		if (list_empty(&call->tx_sendmsg))
+ 			return;
+ 		rxrpc_begin_service_reply(call);
+ 		fallthrough;
+ 
+ 	case RXRPC_CALL_SERVER_SEND_REPLY:
+ 	case RXRPC_CALL_CLIENT_SEND_REQUEST:
+ 		if (!rxrpc_tx_window_has_space(call))
+ 			return;
+ 		if (list_empty(&call->tx_sendmsg)) {
+ 			rxrpc_inc_stat(call->rxnet, stat_tx_data_underflow);
+ 			return;
+ 		}
+ 		rxrpc_decant_prepared_tx(call);
+ 		break;
+ 	default:
+ 		return;
+ 	}
+ }
+ 
+ /*
+  * Ping the other end to fill our RTT cache and to retrieve the rwind
+  * and MTU parameters.
+  */
+ static void rxrpc_send_initial_ping(struct rxrpc_call *call)
+ {
+ 	if (call->peer->rtt_count < 3 ||
+ 	    ktime_before(ktime_add_ms(call->peer->rtt_last_req, 1000),
+ 			 ktime_get_real()))
+ 		rxrpc_send_ACK(call, RXRPC_ACK_PING, 0,
+ 			       rxrpc_propose_ack_ping_for_params);
+ }
+ 
+ /*
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
   * Handle retransmission and deferred ACK/abort generation.
   */
 -bool rxrpc_input_call_event(struct rxrpc_call *call, struct sk_buff *skb)
 +void rxrpc_process_call(struct work_struct *work)
  {
 +	struct rxrpc_call *call =
 +		container_of(work, struct rxrpc_call, processor);
  	unsigned long now, next, t;
 +	unsigned int iterations = 0;
  	rxrpc_serial_t ackr_serial;
 -	bool resend = false, expired = false;
 -	s32 abort_code;
  
  	rxrpc_see_call(call, rxrpc_call_see_input);
  
  	//printk("\n--------------------\n");
  	_enter("{%d,%s,%lx}",
- 	       call->debug_id, rxrpc_call_states[call->state], call->events);
+ 	       call->debug_id, rxrpc_call_states[__rxrpc_call_state(call)],
+ 	       call->events);
  
++<<<<<<< HEAD
 +recheck_state:
 +	/* Limit the number of times we do this before returning to the manager */
 +	iterations++;
 +	if (iterations > 5)
 +		goto requeue;
++=======
+ 	if (__rxrpc_call_is_complete(call))
+ 		goto out;
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  
 -	/* Handle abort request locklessly, vs rxrpc_propose_abort(). */
 -	abort_code = smp_load_acquire(&call->send_abort);
 -	if (abort_code) {
 -		rxrpc_abort_call(call, 0, call->send_abort, call->send_abort_err,
 -				 call->send_abort_why);
 -		goto out;
 +	if (test_and_clear_bit(RXRPC_CALL_EV_ABORT, &call->events)) {
 +		rxrpc_send_abort_packet(call);
 +		goto recheck_state;
  	}
  
 -	if (skb && skb->mark == RXRPC_SKB_MARK_ERROR)
 -		goto out;
 +	if (READ_ONCE(call->acks_hard_ack) != call->tx_bottom)
 +		rxrpc_shrink_call_tx_buffer(call);
  
 -	/* If we see our async-event poke, check for timeout trippage. */
 +	if (call->state == RXRPC_CALL_COMPLETE) {
 +		rxrpc_delete_call_timer(call);
 +		goto out_put;
 +	}
 +
 +	/* Work out if any timeouts tripped */
  	now = jiffies;
  	t = READ_ONCE(call->expect_rx_by);
  	if (time_after_eq(now, t)) {
@@@ -336,10 -407,10 +461,10 @@@
  	}
  
  	t = READ_ONCE(call->expect_req_by);
- 	if (call->state == RXRPC_CALL_SERVER_RECV_REQUEST &&
+ 	if (__rxrpc_call_state(call) == RXRPC_CALL_SERVER_RECV_REQUEST &&
  	    time_after_eq(now, t)) {
  		trace_rxrpc_timer(call, rxrpc_timer_exp_idle, now);
 -		expired = true;
 +		set_bit(RXRPC_CALL_EV_EXPIRED, &call->events);
  	}
  
  	t = READ_ONCE(call->expect_term_by);
@@@ -392,56 -478,61 +517,85 @@@
  		if (test_bit(RXRPC_CALL_RX_HEARD, &call->flags) &&
  		    (int)call->conn->hi_serial - (int)call->rx_serial > 0) {
  			trace_rxrpc_call_reset(call);
 -			rxrpc_abort_call(call, 0, RX_CALL_DEAD, -ECONNRESET,
 -					 rxrpc_abort_call_reset);
 +			rxrpc_abort_call("EXP", call, 0, RX_CALL_DEAD, -ECONNRESET);
  		} else {
 -			rxrpc_abort_call(call, 0, RX_CALL_TIMEOUT, -ETIME,
 -					 rxrpc_abort_call_timeout);
 +			rxrpc_abort_call("EXP", call, 0, RX_CALL_TIMEOUT, -ETIME);
  		}
 -		goto out;
 +		set_bit(RXRPC_CALL_EV_ABORT, &call->events);
 +		goto recheck_state;
  	}
  
 -	if (test_and_clear_bit(RXRPC_CALL_EV_ACK_LOST, &call->events))
 +	if (test_and_clear_bit(RXRPC_CALL_EV_ACK_LOST, &call->events)) {
 +		call->acks_lost_top = call->tx_top;
  		rxrpc_send_ACK(call, RXRPC_ACK_PING, 0,
  			       rxrpc_propose_ack_ping_for_lost_ack);
 +	}
 +
++<<<<<<< HEAD
 +	if (test_and_clear_bit(RXRPC_CALL_EV_RESEND, &call->events) &&
 +	    call->state != RXRPC_CALL_CLIENT_RECV_REPLY) {
 +		rxrpc_resend(call, now);
 +		goto recheck_state;
 +	}
  
 +	/* Make sure the timer is restarted */
 +	next = call->expect_rx_by;
++=======
+ 	if (resend && __rxrpc_call_state(call) != RXRPC_CALL_CLIENT_RECV_REPLY)
+ 		rxrpc_resend(call, NULL);
+ 
+ 	if (test_and_clear_bit(RXRPC_CALL_RX_IS_IDLE, &call->flags))
+ 		rxrpc_send_ACK(call, RXRPC_ACK_IDLE, 0,
+ 			       rxrpc_propose_ack_rx_idle);
+ 
+ 	if (atomic_read(&call->ackr_nr_unacked) > 2)
+ 		rxrpc_send_ACK(call, RXRPC_ACK_IDLE, 0,
+ 			       rxrpc_propose_ack_input_data);
+ 
+ 	/* Make sure the timer is restarted */
+ 	if (!__rxrpc_call_is_complete(call)) {
+ 		next = call->expect_rx_by;
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  
  #define set(T) { t = READ_ONCE(T); if (time_before(t, next)) next = t; }
  
 -		set(call->expect_req_by);
 -		set(call->expect_term_by);
 -		set(call->delay_ack_at);
 -		set(call->ack_lost_at);
 -		set(call->resend_at);
 -		set(call->keepalive_at);
 -		set(call->ping_at);
 +	set(call->expect_req_by);
 +	set(call->expect_term_by);
 +	set(call->delay_ack_at);
 +	set(call->ack_lost_at);
 +	set(call->resend_at);
 +	set(call->keepalive_at);
 +	set(call->ping_at);
  
 -		now = jiffies;
 -		if (time_after_eq(now, next))
 -			rxrpc_poke_call(call, rxrpc_call_poke_timer_now);
 +	now = jiffies;
 +	if (time_after_eq(now, next))
 +		goto recheck_state;
  
 -		rxrpc_reduce_call_timer(call, next, now, rxrpc_timer_restart);
 -	}
 +	rxrpc_reduce_call_timer(call, next, now, rxrpc_timer_restart);
  
 +	/* other events may have been raised since we started checking */
 +	if (call->events && call->state < RXRPC_CALL_COMPLETE)
 +		goto requeue;
 +
 +out_put:
 +	rxrpc_put_call(call, rxrpc_call_put_work);
  out:
++<<<<<<< HEAD
++=======
+ 	if (__rxrpc_call_is_complete(call)) {
+ 		del_timer_sync(&call->timer);
+ 		if (!test_bit(RXRPC_CALL_DISCONNECTED, &call->flags))
+ 			rxrpc_disconnect_call(call);
+ 		if (call->security)
+ 			call->security->free_call_crypto(call);
+ 	}
+ 	if (call->acks_hard_ack != call->tx_bottom)
+ 		rxrpc_shrink_call_tx_buffer(call);
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  	_leave("");
 -	return true;
 +	return;
 +
 +requeue:
 +	__rxrpc_queue_call(call, rxrpc_call_queue_requeue);
 +	goto out;
  }
diff --cc net/rxrpc/call_object.c
index ad495d0d21a8,c94161acf3c4..000000000000
--- a/net/rxrpc/call_object.c
+++ b/net/rxrpc/call_object.c
@@@ -51,11 -69,9 +51,11 @@@ static void rxrpc_call_timer_expired(st
  
  	_enter("%d", call->debug_id);
  
- 	if (call->state < RXRPC_CALL_COMPLETE) {
+ 	if (!__rxrpc_call_is_complete(call)) {
  		trace_rxrpc_timer_expired(call, jiffies);
 -		rxrpc_poke_call(call, rxrpc_call_poke_timer);
 +		__rxrpc_queue_call(call, rxrpc_call_queue_timer);
 +	} else {
 +		rxrpc_put_call(call, rxrpc_call_put_already_queued);
  	}
  }
  
@@@ -151,9 -162,6 +151,12 @@@ struct rxrpc_call *rxrpc_alloc_call(str
  	init_waitqueue_head(&call->waitq);
  	spin_lock_init(&call->notify_lock);
  	spin_lock_init(&call->tx_lock);
++<<<<<<< HEAD
 +	spin_lock_init(&call->input_lock);
 +	spin_lock_init(&call->acks_ack_lock);
 +	rwlock_init(&call->state_lock);
++=======
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  	refcount_set(&call->ref, 1);
  	call->debug_id = debug_id;
  	call->tx_total_len = -1;
@@@ -196,11 -207,33 +199,40 @@@ static struct rxrpc_call *rxrpc_alloc_c
  	call = rxrpc_alloc_call(rx, gfp, debug_id);
  	if (!call)
  		return ERR_PTR(-ENOMEM);
 +	call->state = RXRPC_CALL_CLIENT_AWAIT_CONN;
 +	call->service_id = srx->srx_service;
  	now = ktime_get_real();
++<<<<<<< HEAD
 +	call->acks_latest_ts = now;
 +	call->cong_tstamp = now;
++=======
+ 	call->acks_latest_ts	= now;
+ 	call->cong_tstamp	= now;
+ 	call->dest_srx		= *srx;
+ 	call->interruptibility	= p->interruptibility;
+ 	call->tx_total_len	= p->tx_total_len;
+ 	call->key		= key_get(cp->key);
+ 	call->local		= rxrpc_get_local(cp->local, rxrpc_local_get_call);
+ 	call->security_level	= cp->security_level;
+ 	if (p->kernel)
+ 		__set_bit(RXRPC_CALL_KERNEL, &call->flags);
+ 	if (cp->upgrade)
+ 		__set_bit(RXRPC_CALL_UPGRADE, &call->flags);
+ 	if (cp->exclusive)
+ 		__set_bit(RXRPC_CALL_EXCLUSIVE, &call->flags);
+ 
+ 	ret = rxrpc_init_client_call_security(call);
+ 	if (ret < 0) {
+ 		rxrpc_prefail_call(call, RXRPC_CALL_LOCAL_ERROR, ret);
+ 		rxrpc_put_call(call, rxrpc_call_put_discard_error);
+ 		return ERR_PTR(ret);
+ 	}
+ 
+ 	rxrpc_set_call_state(call, RXRPC_CALL_CLIENT_AWAIT_CONN);
+ 
+ 	trace_rxrpc_call(call->debug_id, refcount_read(&call->ref),
+ 			 p->user_call_ID, rxrpc_call_new_client);
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  
  	_leave(" = %p", call);
  	return call;
@@@ -400,11 -423,34 +430,37 @@@ void rxrpc_incoming_call(struct rxrpc_s
  
  	rcu_assign_pointer(call->socket, rx);
  	call->call_id		= sp->hdr.callNumber;
 -	call->dest_srx.srx_service = sp->hdr.serviceId;
 +	call->service_id	= sp->hdr.serviceId;
  	call->cid		= sp->hdr.cid;
- 	call->state		= RXRPC_CALL_SERVER_SECURING;
  	call->cong_tstamp	= skb->tstamp;
  
++<<<<<<< HEAD
++=======
+ 	__set_bit(RXRPC_CALL_EXPOSED, &call->flags);
+ 	rxrpc_set_call_state(call, RXRPC_CALL_SERVER_SECURING);
+ 
+ 	spin_lock(&conn->state_lock);
+ 
+ 	switch (conn->state) {
+ 	case RXRPC_CONN_SERVICE_UNSECURED:
+ 	case RXRPC_CONN_SERVICE_CHALLENGING:
+ 		rxrpc_set_call_state(call, RXRPC_CALL_SERVER_SECURING);
+ 		break;
+ 	case RXRPC_CONN_SERVICE:
+ 		rxrpc_set_call_state(call, RXRPC_CALL_SERVER_RECV_REQUEST);
+ 		break;
+ 
+ 	case RXRPC_CONN_ABORTED:
+ 		rxrpc_set_call_completion(call, conn->completion,
+ 					  conn->abort_code, conn->error);
+ 		break;
+ 	default:
+ 		BUG();
+ 	}
+ 
+ 	rxrpc_get_call(call, rxrpc_call_get_io_thread);
+ 
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  	/* Set the channel for this call.  We don't get channel_lock as we're
  	 * only defending against the data_ready handler (which we're called
  	 * from) and the RESPONSE packet parser (which is only really
@@@ -603,12 -612,12 +659,12 @@@ void rxrpc_put_call(struct rxrpc_call *
  	dead = __refcount_dec_and_test(&call->ref, &r);
  	trace_rxrpc_call(debug_id, r - 1, 0, why);
  	if (dead) {
- 		ASSERTCMP(call->state, ==, RXRPC_CALL_COMPLETE);
+ 		ASSERTCMP(__rxrpc_call_state(call), ==, RXRPC_CALL_COMPLETE);
  
  		if (!list_empty(&call->link)) {
 -			spin_lock(&rxnet->call_lock);
 +			spin_lock_bh(&rxnet->call_lock);
  			list_del_init(&call->link);
 -			spin_unlock(&rxnet->call_lock);
 +			spin_unlock_bh(&rxnet->call_lock);
  		}
  
  		rxrpc_cleanup_call(call);
@@@ -653,25 -673,20 +709,25 @@@ static void rxrpc_rcu_destroy_call(stru
   */
  void rxrpc_cleanup_call(struct rxrpc_call *call)
  {
 +	struct rxrpc_txbuf *txb;
 +
 +	_net("DESTROY CALL %d", call->debug_id);
 +
  	memset(&call->sock_node, 0xcd, sizeof(call->sock_node));
  
- 	ASSERTCMP(call->state, ==, RXRPC_CALL_COMPLETE);
+ 	ASSERTCMP(__rxrpc_call_state(call), ==, RXRPC_CALL_COMPLETE);
  	ASSERT(test_bit(RXRPC_CALL_RELEASED, &call->flags));
  
 -	del_timer(&call->timer);
 +	rxrpc_cleanup_ring(call);
 +	while ((txb = list_first_entry_or_null(&call->tx_buffer,
 +					       struct rxrpc_txbuf, call_link))) {
 +		list_del(&txb->call_link);
 +		rxrpc_put_txbuf(txb, rxrpc_txbuf_put_cleaned);
 +	}
 +	rxrpc_put_txbuf(call->tx_pending, rxrpc_txbuf_put_cleaned);
 +	rxrpc_free_skb(call->acks_soft_tbl, rxrpc_skb_cleaned);
  
 -	if (rcu_read_lock_held())
 -		/* Can't use the rxrpc workqueue as we need to cancel/flush
 -		 * something that may be running/waiting there.
 -		 */
 -		schedule_work(&call->destroyer);
 -	else
 -		rxrpc_destroy_call(&call->destroyer);
 +	call_rcu(&call->rcu, rxrpc_rcu_destroy_call);
  }
  
  /*
@@@ -698,15 -713,15 +754,15 @@@ void rxrpc_destroy_all_calls(struct rxr
  
  			pr_err("Call %p still in use (%d,%s,%lx,%lx)!\n",
  			       call, refcount_read(&call->ref),
- 			       rxrpc_call_states[call->state],
+ 			       rxrpc_call_states[__rxrpc_call_state(call)],
  			       call->flags, call->events);
  
 -			spin_unlock(&rxnet->call_lock);
 +			spin_unlock_bh(&rxnet->call_lock);
  			cond_resched();
 -			spin_lock(&rxnet->call_lock);
 +			spin_lock_bh(&rxnet->call_lock);
  		}
  
 -		spin_unlock(&rxnet->call_lock);
 +		spin_unlock_bh(&rxnet->call_lock);
  	}
  
  	atomic_dec(&rxnet->nr_calls);
diff --cc net/rxrpc/conn_client.c
index 827c1308297c,8b5ea68dc47e..000000000000
--- a/net/rxrpc/conn_client.c
+++ b/net/rxrpc/conn_client.c
@@@ -532,21 -546,14 +532,25 @@@ static void rxrpc_activate_one_channel(
  
  	rxrpc_see_call(call, rxrpc_call_see_activate_client);
  	list_del_init(&call->chan_wait_link);
 -	call->conn	= rxrpc_get_connection(conn, rxrpc_conn_get_activate_call);
 +	call->peer	= rxrpc_get_peer(conn->params.peer);
 +	call->conn	= rxrpc_get_connection(conn);
  	call->cid	= conn->proto.cid | channel;
  	call->call_id	= call_id;
 -	call->dest_srx.srx_service = conn->service_id;
 +	call->security	= conn->security;
 +	call->security_ix = conn->security_ix;
 +	call->service_id = conn->service_id;
  
  	trace_rxrpc_connect_call(call);
 -
 +	_net("CONNECT call %08x:%08x as call %d on conn %d",
 +	     call->cid, call->call_id, call->debug_id, conn->debug_id);
 +
++<<<<<<< HEAD
 +	write_lock_bh(&call->state_lock);
 +	call->state = RXRPC_CALL_CLIENT_SEND_REQUEST;
 +	write_unlock_bh(&call->state_lock);
++=======
+ 	rxrpc_set_call_state(call, RXRPC_CALL_CLIENT_SEND_REQUEST);
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  
  	/* Paired with the read barrier in rxrpc_connect_call().  This orders
  	 * cid and epoch in the connection wrt to call_id without the need to
diff --cc net/rxrpc/conn_event.c
index abf03a5b1d31,8d0b9ff0a5e1..000000000000
--- a/net/rxrpc/conn_event.c
+++ b/net/rxrpc/conn_event.c
@@@ -263,14 -230,9 +263,20 @@@ static int rxrpc_abort_connection(struc
   */
  static void rxrpc_call_is_secure(struct rxrpc_call *call)
  {
++<<<<<<< HEAD
 +	_enter("%p", call);
 +	if (call) {
 +		write_lock_bh(&call->state_lock);
 +		if (call->state == RXRPC_CALL_SERVER_SECURING) {
 +			call->state = RXRPC_CALL_SERVER_RECV_REQUEST;
 +			rxrpc_notify_socket(call);
 +		}
 +		write_unlock_bh(&call->state_lock);
++=======
+ 	if (call && __rxrpc_call_state(call) == RXRPC_CALL_SERVER_SECURING) {
+ 		rxrpc_set_call_state(call, RXRPC_CALL_SERVER_RECV_REQUEST);
+ 		rxrpc_notify_socket(call);
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  	}
  }
  
diff --cc net/rxrpc/input.c
index b5326e160685,367927a99881..000000000000
--- a/net/rxrpc/input.c
+++ b/net/rxrpc/input.c
@@@ -197,6 -175,33 +197,36 @@@ send_extra_data
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * Degrade the congestion window if we haven't transmitted a packet for >1RTT.
+  */
+ void rxrpc_congestion_degrade(struct rxrpc_call *call)
+ {
+ 	ktime_t rtt, now;
+ 
+ 	if (call->cong_mode != RXRPC_CALL_SLOW_START &&
+ 	    call->cong_mode != RXRPC_CALL_CONGEST_AVOIDANCE)
+ 		return;
+ 	if (__rxrpc_call_state(call) == RXRPC_CALL_CLIENT_AWAIT_REPLY)
+ 		return;
+ 
+ 	rtt = ns_to_ktime(call->peer->srtt_us * (1000 / 8));
+ 	now = ktime_get_real();
+ 	if (!ktime_before(ktime_add(call->tx_last_sent, rtt), now))
+ 		return;
+ 
+ 	trace_rxrpc_reset_cwnd(call, now);
+ 	rxrpc_inc_stat(call->rxnet, stat_tx_data_cwnd_reset);
+ 	call->tx_last_sent = now;
+ 	call->cong_mode = RXRPC_CALL_SLOW_START;
+ 	call->cong_ssthresh = max_t(unsigned int, call->cong_ssthresh,
+ 				    call->cong_cwnd * 3 / 4);
+ 	call->cong_cwnd = max_t(unsigned int, call->cong_cwnd / 2, RXRPC_MIN_CWND);
+ }
+ 
+ /*
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
   * Apply a hard ACK by advancing the Tx window.
   */
  static bool rxrpc_rotate_tx_window(struct rxrpc_call *call, rxrpc_seq_t to,
@@@ -244,23 -249,22 +274,22 @@@
   * This occurs when we get an ACKALL packet, the first DATA packet of a reply,
   * or a final ACK packet.
   */
 -static void rxrpc_end_tx_phase(struct rxrpc_call *call, bool reply_begun,
 -			       enum rxrpc_abort_reason abort_why)
 +static bool rxrpc_end_tx_phase(struct rxrpc_call *call, bool reply_begun,
 +			       const char *abort_why)
  {
- 	unsigned int state;
- 
  	ASSERT(test_bit(RXRPC_CALL_TX_LAST, &call->flags));
  
- 	write_lock(&call->state_lock);
- 
- 	state = call->state;
- 	switch (state) {
+ 	switch (__rxrpc_call_state(call)) {
  	case RXRPC_CALL_CLIENT_SEND_REQUEST:
  	case RXRPC_CALL_CLIENT_AWAIT_REPLY:
- 		if (reply_begun)
- 			call->state = state = RXRPC_CALL_CLIENT_RECV_REPLY;
- 		else
- 			call->state = state = RXRPC_CALL_CLIENT_AWAIT_REPLY;
+ 		if (reply_begun) {
+ 			rxrpc_set_call_state(call, RXRPC_CALL_CLIENT_RECV_REPLY);
+ 			trace_rxrpc_txqueue(call, rxrpc_txqueue_end);
+ 			break;
+ 		}
+ 
+ 		rxrpc_set_call_state(call, RXRPC_CALL_CLIENT_AWAIT_REPLY);
+ 		trace_rxrpc_txqueue(call, rxrpc_txqueue_await_reply);
  		break;
  
  	case RXRPC_CALL_SERVER_AWAIT_ACK:
@@@ -269,22 -273,10 +298,27 @@@
  		break;
  
  	default:
- 		goto bad_state;
+ 		kdebug("end_tx %s", rxrpc_call_states[__rxrpc_call_state(call)]);
+ 		rxrpc_proto_abort(call, call->tx_top, abort_why);
+ 		break;
  	}
++<<<<<<< HEAD
 +
 +	write_unlock(&call->state_lock);
 +	if (state == RXRPC_CALL_CLIENT_AWAIT_REPLY)
 +		trace_rxrpc_txqueue(call, rxrpc_txqueue_await_reply);
 +	else
 +		trace_rxrpc_txqueue(call, rxrpc_txqueue_end);
 +	_leave(" = ok");
 +	return true;
 +
 +bad_state:
 +	write_unlock(&call->state_lock);
 +	kdebug("end_tx %s", rxrpc_call_states[call->state]);
 +	rxrpc_proto_abort(abort_why, call, call->tx_top);
 +	return false;
++=======
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  }
  
  /*
@@@ -310,7 -302,37 +344,41 @@@ static bool rxrpc_receiving_reply(struc
  			return false;
  		}
  	}
++<<<<<<< HEAD
 +	return rxrpc_end_tx_phase(call, true, "ETD");
++=======
+ 
+ 	rxrpc_end_tx_phase(call, true, rxrpc_eproto_unexpected_reply);
+ 	return true;
+ }
+ 
+ /*
+  * End the packet reception phase.
+  */
+ static void rxrpc_end_rx_phase(struct rxrpc_call *call, rxrpc_serial_t serial)
+ {
+ 	rxrpc_seq_t whigh = READ_ONCE(call->rx_highest_seq);
+ 
+ 	_enter("%d,%s", call->debug_id, rxrpc_call_states[__rxrpc_call_state(call)]);
+ 
+ 	trace_rxrpc_receive(call, rxrpc_receive_end, 0, whigh);
+ 
+ 	switch (__rxrpc_call_state(call)) {
+ 	case RXRPC_CALL_CLIENT_RECV_REPLY:
+ 		rxrpc_propose_delay_ACK(call, serial, rxrpc_propose_ack_terminal_ack);
+ 		rxrpc_call_completed(call);
+ 		break;
+ 
+ 	case RXRPC_CALL_SERVER_RECV_REQUEST:
+ 		rxrpc_set_call_state(call, RXRPC_CALL_SERVER_ACK_REQUEST);
+ 		call->expect_req_by = jiffies + MAX_JIFFY_OFFSET;
+ 		rxrpc_propose_delay_ACK(call, serial, rxrpc_propose_ack_processing_op);
+ 		break;
+ 
+ 	default:
+ 		break;
+ 	}
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  }
  
  static void rxrpc_input_update_ack_window(struct rxrpc_call *call,
@@@ -551,31 -571,20 +618,45 @@@ static void rxrpc_input_data(struct rxr
  	       atomic64_read(&call->ackr_window), call->rx_highest_seq,
  	       skb->len, seq0);
  
++<<<<<<< HEAD
 +	state = READ_ONCE(call->state);
 +	if (state >= RXRPC_CALL_COMPLETE) {
 +		rxrpc_free_skb(skb, rxrpc_skb_freed);
++=======
+ 	if (__rxrpc_call_is_complete(call))
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  		return;
 +	}
 +
 +	/* Unshare the packet so that it can be modified for in-place
 +	 * decryption.
 +	 */
 +	if (sp->hdr.securityIndex != 0) {
 +		struct sk_buff *nskb = skb_unshare(skb, GFP_ATOMIC);
 +		if (!nskb) {
 +			rxrpc_eaten_skb(skb, rxrpc_skb_unshared_nomem);
 +			return;
 +		}
 +
 +		if (nskb != skb) {
 +			rxrpc_eaten_skb(skb, rxrpc_skb_received);
 +			skb = nskb;
 +			rxrpc_new_skb(skb, rxrpc_skb_unshared);
 +			sp = rxrpc_skb(skb);
 +		}
 +	}
  
- 	if (state == RXRPC_CALL_SERVER_RECV_REQUEST) {
+ 	switch (__rxrpc_call_state(call)) {
+ 	case RXRPC_CALL_CLIENT_SEND_REQUEST:
+ 	case RXRPC_CALL_CLIENT_AWAIT_REPLY:
+ 		/* Received data implicitly ACKs all of the request
+ 		 * packets we sent when we're acting as a client.
+ 		 */
+ 		if (!rxrpc_receiving_reply(call))
+ 			goto out_notify;
+ 		break;
+ 
+ 	case RXRPC_CALL_SERVER_RECV_REQUEST: {
  		unsigned long timo = READ_ONCE(call->next_req_timo);
  		unsigned long now, expect_req_by;
  
@@@ -586,21 -595,16 +667,28 @@@
  			rxrpc_reduce_call_timer(call, expect_req_by, now,
  						rxrpc_timer_set_for_idle);
  		}
+ 		break;
  	}
  
++<<<<<<< HEAD
 +	spin_lock(&call->input_lock);
 +
 +	/* Received data implicitly ACKs all of the request packets we sent
 +	 * when we're acting as a client.
 +	 */
 +	if ((state == RXRPC_CALL_CLIENT_SEND_REQUEST ||
 +	     state == RXRPC_CALL_CLIENT_AWAIT_REPLY) &&
 +	    !rxrpc_receiving_reply(call))
 +		goto out;
++=======
+ 	default:
+ 		break;
+ 	}
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  
  	if (!rxrpc_input_split_jumbo(call, skb)) {
 -		rxrpc_proto_abort(call, sp->hdr.seq, rxrpc_badmsg_bad_jumbo);
 -		goto out_notify;
 +		rxrpc_proto_abort("VLD", call, sp->hdr.seq);
 +		goto out;
  	}
  	skb = NULL;
  
@@@ -931,13 -886,11 +1019,13 @@@ static void rxrpc_input_ack(struct rxrp
  	if (info.rxMTU)
  		rxrpc_input_ackinfo(call, skb, &info);
  
 -	if (first_soft_ack == 0)
 -		return rxrpc_proto_abort(call, 0, rxrpc_eproto_ackr_zero);
 +	if (first_soft_ack == 0) {
 +		rxrpc_proto_abort("AK0", call, 0);
 +		goto out;
 +	}
  
  	/* Ignore ACKs unless we are or have just been transmitting. */
- 	switch (READ_ONCE(call->state)) {
+ 	switch (__rxrpc_call_state(call)) {
  	case RXRPC_CALL_CLIENT_SEND_REQUEST:
  	case RXRPC_CALL_CLIENT_AWAIT_REPLY:
  	case RXRPC_CALL_SERVER_SEND_REPLY:
@@@ -1087,11 -1011,9 +1175,11 @@@ no_free
   *
   * TODO: If callNumber > call_id + 1, renegotiate security.
   */
 -void rxrpc_implicit_end_call(struct rxrpc_call *call, struct sk_buff *skb)
 +static void rxrpc_input_implicit_end_call(struct rxrpc_sock *rx,
 +					  struct rxrpc_connection *conn,
 +					  struct rxrpc_call *call)
  {
- 	switch (READ_ONCE(call->state)) {
+ 	switch (__rxrpc_call_state(call)) {
  	case RXRPC_CALL_SERVER_AWAIT_ACK:
  		rxrpc_call_completed(call);
  		fallthrough;
diff --cc net/rxrpc/output.c
index 71b6fea4598b,a9746be29634..000000000000
--- a/net/rxrpc/output.c
+++ b/net/rxrpc/output.c
@@@ -713,3 -716,43 +713,46 @@@ void rxrpc_send_keepalive(struct rxrpc_
  	peer->last_tx_at = ktime_get_seconds();
  	_leave("");
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Schedule an instant Tx resend.
+  */
+ static inline void rxrpc_instant_resend(struct rxrpc_call *call,
+ 					struct rxrpc_txbuf *txb)
+ {
+ 	if (!__rxrpc_call_is_complete(call))
+ 		kdebug("resend");
+ }
+ 
+ /*
+  * Transmit one packet.
+  */
+ void rxrpc_transmit_one(struct rxrpc_call *call, struct rxrpc_txbuf *txb)
+ {
+ 	int ret;
+ 
+ 	ret = rxrpc_send_data_packet(call, txb);
+ 	if (ret < 0) {
+ 		switch (ret) {
+ 		case -ENETUNREACH:
+ 		case -EHOSTUNREACH:
+ 		case -ECONNREFUSED:
+ 			rxrpc_set_call_completion(call, RXRPC_CALL_LOCAL_ERROR,
+ 						  0, ret);
+ 			break;
+ 		default:
+ 			_debug("need instant resend %d", ret);
+ 			rxrpc_instant_resend(call, txb);
+ 		}
+ 	} else {
+ 		unsigned long now = jiffies;
+ 		unsigned long resend_at = now + call->peer->rto_j;
+ 
+ 		WRITE_ONCE(call->resend_at, resend_at);
+ 		rxrpc_reduce_call_timer(call, resend_at, now,
+ 					rxrpc_timer_set_for_send);
+ 	}
+ }
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
diff --cc net/rxrpc/rxkad.c
index 36cf40442a7e,dfb01e7b90fb..000000000000
--- a/net/rxrpc/rxkad.c
+++ b/net/rxrpc/rxkad.c
@@@ -1209,8 -1143,11 +1209,14 @@@ static int rxkad_verify_response(struc
  			call = rcu_dereference_protected(
  				conn->channels[i].call,
  				lockdep_is_held(&conn->bundle->channel_lock));
++<<<<<<< HEAD
 +			if (call && call->state < RXRPC_CALL_COMPLETE)
++=======
+ 			if (call && !__rxrpc_call_is_complete(call)) {
+ 				rxrpc_abort_conn(conn, skb, RXKADSEALEDINCON, -EPROTO,
+ 						 rxkad_abort_resp_call_state);
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  				goto protocol_error_unlock;
 -			}
  			conn->channels[i].call_counter = call_id;
  		}
  	}
diff --cc net/rxrpc/sendmsg.c
index 45c09f0de6fe,a5d0005b7ce5..000000000000
--- a/net/rxrpc/sendmsg.c
+++ b/net/rxrpc/sendmsg.c
@@@ -773,27 -702,10 +773,33 @@@ int rxrpc_kernel_send_data(struct socke
  
  	mutex_lock(&call->user_mutex);
  
++<<<<<<< HEAD
 +	_debug("CALL %d USR %lx ST %d on CONN %p",
 +	       call->debug_id, call->user_call_ID, call->state, call->conn);
 +
 +	switch (READ_ONCE(call->state)) {
 +	case RXRPC_CALL_CLIENT_SEND_REQUEST:
 +	case RXRPC_CALL_SERVER_ACK_REQUEST:
 +	case RXRPC_CALL_SERVER_SEND_REPLY:
 +		ret = rxrpc_send_data(rxrpc_sk(sock->sk), call, msg, len,
 +				      notify_end_tx, &dropped_lock);
 +		break;
 +	case RXRPC_CALL_COMPLETE:
 +		read_lock_bh(&call->state_lock);
++=======
+ 	ret = rxrpc_send_data(rxrpc_sk(sock->sk), call, msg, len,
+ 			      notify_end_tx, &dropped_lock);
+ 	if (ret == -ESHUTDOWN)
++>>>>>>> 96b4059f43ce (rxrpc: Remove call->state_lock)
  		ret = call->error;
 +		read_unlock_bh(&call->state_lock);
 +		break;
 +	default:
 +		/* Request phase complete for this client call */
 +		trace_rxrpc_rx_eproto(call, 0, tracepoint_string("late_send"));
 +		ret = -EPROTO;
 +		break;
 +	}
  
  	if (!dropped_lock)
  		mutex_unlock(&call->user_mutex);
* Unmerged path net/rxrpc/call_state.c
* Unmerged path net/rxrpc/ar-internal.h
* Unmerged path net/rxrpc/call_accept.c
* Unmerged path net/rxrpc/call_event.c
* Unmerged path net/rxrpc/call_object.c
* Unmerged path net/rxrpc/call_state.c
* Unmerged path net/rxrpc/conn_client.c
* Unmerged path net/rxrpc/conn_event.c
* Unmerged path net/rxrpc/input.c
* Unmerged path net/rxrpc/output.c
diff --git a/net/rxrpc/proc.c b/net/rxrpc/proc.c
index fae22a8b38d6..687405142b16 100644
--- a/net/rxrpc/proc.c
+++ b/net/rxrpc/proc.c
@@ -53,6 +53,7 @@ static int rxrpc_call_seq_show(struct seq_file *seq, void *v)
 	struct rxrpc_peer *peer;
 	struct rxrpc_call *call;
 	struct rxrpc_net *rxnet = rxrpc_net(seq_file_net(seq));
+	enum rxrpc_call_state state;
 	unsigned long timeout = 0;
 	rxrpc_seq_t acks_hard_ack;
 	char lbuff[50], rbuff[50];
@@ -86,7 +87,8 @@ static int rxrpc_call_seq_show(struct seq_file *seq, void *v)
 	else
 		strcpy(rbuff, "no_connection");
 
-	if (call->state != RXRPC_CALL_SERVER_PREALLOC) {
+	state = rxrpc_call_state(call);
+	if (state != RXRPC_CALL_SERVER_PREALLOC) {
 		timeout = READ_ONCE(call->expect_rx_by);
 		timeout -= jiffies;
 	}
@@ -103,7 +105,7 @@ static int rxrpc_call_seq_show(struct seq_file *seq, void *v)
 		   call->call_id,
 		   rxrpc_is_service_call(call) ? "Svc" : "Clt",
 		   refcount_read(&call->ref),
-		   rxrpc_call_states[call->state],
+		   rxrpc_call_states[state],
 		   call->abort_code,
 		   call->debug_id,
 		   acks_hard_ack, READ_ONCE(call->tx_top) - acks_hard_ack,
* Unmerged path net/rxrpc/rxkad.c
* Unmerged path net/rxrpc/sendmsg.c
