locking/rtmutex: Add a lockdep assert to catch potential nested blocking

jira LE-1907
Rebuild_History Non-Buildable kernel-5.14.0-284.30.1.el9_2
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 45f67f30a22f264bc7a0a61255c2ee1a838e9403
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-284.30.1.el9_2/45f67f30.failed

There used to be a BUG_ON(current->pi_blocked_on) in the lock acquisition
functions, but that vanished in one of the rtmutex overhauls.

Bring it back in form of a lockdep assert to catch code paths which take
rtmutex based locks with current::pi_blocked_on != NULL.

	Reported-by: Crystal Wood <swood@redhat.com>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Signed-off-by: "Peter Zijlstra (Intel)" <peterz@infradead.org>
	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20230908162254.999499-7-bigeasy@linutronix.de
(cherry picked from commit 45f67f30a22f264bc7a0a61255c2ee1a838e9403)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/rtmutex.c
diff --cc kernel/locking/rtmutex.c
index 852bdea96bbf,4a10e8c16fd2..000000000000
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@@ -1690,7 -1784,9 +1690,13 @@@ static int __sched rt_mutex_slowlock(st
  static __always_inline int __rt_mutex_lock(struct rt_mutex_base *lock,
  					   unsigned int state)
  {
++<<<<<<< HEAD
 +	if (likely(rt_mutex_cmpxchg_acquire(lock, NULL, current)))
++=======
+ 	lockdep_assert(!current->pi_blocked_on);
+ 
+ 	if (likely(rt_mutex_try_acquire(lock)))
++>>>>>>> 45f67f30a22f (locking/rtmutex: Add a lockdep assert to catch potential nested blocking)
  		return 0;
  
  	return rt_mutex_slowlock(lock, NULL, state);
* Unmerged path kernel/locking/rtmutex.c
diff --git a/kernel/locking/rwbase_rt.c b/kernel/locking/rwbase_rt.c
index 356b247521d8..e83654da67f0 100644
--- a/kernel/locking/rwbase_rt.c
+++ b/kernel/locking/rwbase_rt.c
@@ -127,6 +127,8 @@ static int __sched __rwbase_read_lock(struct rwbase_rt *rwb,
 static __always_inline int rwbase_read_lock(struct rwbase_rt *rwb,
 					    unsigned int state)
 {
+	lockdep_assert(!current->pi_blocked_on);
+
 	if (rwbase_read_trylock(rwb))
 		return 0;
 
diff --git a/kernel/locking/spinlock_rt.c b/kernel/locking/spinlock_rt.c
index 48a19ed8486d..e90cb2216a7b 100644
--- a/kernel/locking/spinlock_rt.c
+++ b/kernel/locking/spinlock_rt.c
@@ -37,6 +37,8 @@
 
 static __always_inline void rtlock_lock(struct rt_mutex_base *rtm)
 {
+	lockdep_assert(!current->pi_blocked_on);
+
 	if (unlikely(!rt_mutex_cmpxchg_acquire(rtm, NULL, current)))
 		rtlock_slowlock(rtm);
 }
