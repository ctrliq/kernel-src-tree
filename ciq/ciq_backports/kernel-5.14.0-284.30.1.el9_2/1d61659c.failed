locking/rwsem: Disable preemption in all down_write*() and up_write() code paths

jira LE-1907
Rebuild_History Non-Buildable kernel-5.14.0-284.30.1.el9_2
commit-author Waiman Long <longman@redhat.com>
commit 1d61659ced6bd8881cf2fb5cbcb28f9541fc7430
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-284.30.1.el9_2/1d61659c.failed

The previous patch has disabled preemption in all the down_read() and
up_read() code paths. For symmetry, this patch extends commit:

  48dfb5d2560d ("locking/rwsem: Disable preemption while trying for rwsem lock")

... to have preemption disabled in all the down_write() and up_write()
code paths, including downgrade_write().

	Suggested-by: Peter Zijlstra <peterz@infradead.org>
	Signed-off-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20230126003628.365092-4-longman@redhat.com
(cherry picked from commit 1d61659ced6bd8881cf2fb5cbcb28f9541fc7430)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/locking/rwsem.c
diff --cc kernel/locking/rwsem.c
index 6e9b6fe29f61,acb5a50309a1..000000000000
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@@ -1093,51 -1167,17 +1085,54 @@@ rwsem_down_write_slowpath(struct rw_sem
  		 * In this case, we attempt to acquire the lock again
  		 * without sleeping.
  		 */
 -		if (waiter.handoff_set) {
 +		if (wstate == WRITER_HANDOFF) {
  			enum owner_state owner_state;
  
- 			preempt_disable();
  			owner_state = rwsem_spin_on_owner(sem);
- 			preempt_enable();
- 
  			if (owner_state == OWNER_NULL)
  				goto trylock_again;
  		}
  
++<<<<<<< HEAD
 +		/* Block until there are no active lockers. */
 +		for (;;) {
 +			if (signal_pending_state(state, current))
 +				goto out_nolock;
 +
 +			schedule();
 +			lockevent_inc(rwsem_sleep_writer);
 +			set_current_state(state);
 +			/*
 +			 * If HANDOFF bit is set, unconditionally do
 +			 * a trylock.
 +			 */
 +			if (wstate == WRITER_HANDOFF)
 +				break;
 +
 +			if ((wstate == WRITER_NOT_FIRST) &&
 +			    (rwsem_first_waiter(sem) == &waiter))
 +				wstate = WRITER_FIRST;
 +
 +			count = atomic_long_read(&sem->count);
 +			if (!(count & RWSEM_LOCK_MASK))
 +				break;
 +
 +			/*
 +			 * The setting of the handoff bit is deferred
 +			 * until rwsem_try_write_lock() is called.
 +			 */
 +			if ((wstate == WRITER_FIRST) && (rt_task(current) ||
 +			    time_after(jiffies, waiter.timeout))) {
 +				wstate = WRITER_HANDOFF;
 +				lockevent_inc(rwsem_wlock_handoff);
 +				break;
 +			}
 +		}
++=======
+ 		schedule_preempt_disabled();
+ 		lockevent_inc(rwsem_sleep_writer);
+ 		set_current_state(state);
++>>>>>>> 1d61659ced6b (locking/rwsem: Disable preemption in all down_write*() and up_write() code paths)
  trylock_again:
  		raw_spin_lock_irq(&sem->wait_lock);
  	}
* Unmerged path kernel/locking/rwsem.c
