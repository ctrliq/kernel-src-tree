KVM: x86: Refactor tsc synchronization code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.26.1.el8_6
commit-author Oliver Upton <oupton@google.com>
commit 58d4277be9b66d8048054c8e57214a86b6b15da9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.26.1.el8_6/58d4277b.failed

Refactor kvm_synchronize_tsc to make a new function that allows callers
to specify TSC parameters (offset, value, nanoseconds, etc.) explicitly
for the sake of participating in TSC synchronization.

	Signed-off-by: Oliver Upton <oupton@google.com>
Message-Id: <20210916181538.968978-7-oupton@google.com>
[Make sure kvm->arch.cur_tsc_generation and vcpu->arch.this_tsc_generation are
 equal at the end of __kvm_synchronize_tsc, if matched is false. Reported by
 Maxim Levitsky. - Paolo]
	Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
(cherry picked from commit 58d4277be9b66d8048054c8e57214a86b6b15da9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/x86.c
diff --cc arch/x86/kvm/x86.c
index 3e40344015c8,c74a44f2a38c..000000000000
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@@ -2560,51 -2537,10 +2610,40 @@@ static void kvm_synchronize_tsc(struct 
  			offset = kvm_compute_l1_tsc_offset(vcpu, data);
  		}
  		matched = true;
- 		already_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);
- 	} else {
- 		/*
- 		 * We split periods of matched TSC writes into generations.
- 		 * For each generation, we track the original measured
- 		 * nanosecond time, offset, and write, so if TSCs are in
- 		 * sync, we can match exact offset, and if not, we can match
- 		 * exact software computation in compute_guest_tsc()
- 		 *
- 		 * These values are tracked in kvm->arch.cur_xxx variables.
- 		 */
- 		kvm->arch.cur_tsc_generation++;
- 		kvm->arch.cur_tsc_nsec = ns;
- 		kvm->arch.cur_tsc_write = data;
- 		kvm->arch.cur_tsc_offset = offset;
- 		matched = false;
  	}
  
++<<<<<<< HEAD
 +	/*
 +	 * We also track th most recent recorded KHZ, write and time to
 +	 * allow the matching interval to be extended at each write.
 +	 */
 +	kvm->arch.last_tsc_nsec = ns;
 +	kvm->arch.last_tsc_write = data;
 +	kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
 +
 +	vcpu->arch.last_guest_tsc = data;
 +
 +	/* Keep track of which generation this VCPU has synchronized to */
 +	vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
 +	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
 +	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 +
 +	kvm_vcpu_write_tsc_offset(vcpu, offset);
 +	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 +
 +	raw_spin_lock_irqsave(&kvm->arch.pvclock_gtod_sync_lock, flags);
 +	if (!matched) {
 +		kvm->arch.nr_vcpus_matched_tsc = 0;
 +	} else if (!already_matched) {
 +		kvm->arch.nr_vcpus_matched_tsc++;
 +	}
 +
 +	kvm_track_tsc_matching(vcpu);
 +	raw_spin_unlock_irqrestore(&kvm->arch.pvclock_gtod_sync_lock, flags);
++=======
+ 	__kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
+ 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
++>>>>>>> 58d4277be9b6 (KVM: x86: Refactor tsc synchronization code)
  }
  
  static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
* Unmerged path arch/x86/kvm/x86.c
