ice: cleanup long lines in ice_sriov.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.26.1.el8_6
commit-author Jacob Keller <jacob.e.keller@intel.com>
commit 8cf52bec5ca0231133fdc3c6ee2f8d8ec4da9517
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.26.1.el8_6/8cf52bec.failed

Before we move the virtchnl message handling from ice_sriov.c into
ice_virtchnl.c, cleanup some long line warnings to avoid checkpatch.pl
complaints.

	Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
	Tested-by: Konrad Jankowski <konrad0.jankowski@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 8cf52bec5ca0231133fdc3c6ee2f8d8ec4da9517)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_sriov.c
diff --cc drivers/net/ethernet/intel/ice/ice_sriov.c
index 52c6bac41bf7,4f3d25ed68c9..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_sriov.c
+++ b/drivers/net/ethernet/intel/ice/ice_sriov.c
@@@ -40,355 -250,5217 +40,1958 @@@ ice_aq_send_msg_to_vf(struct ice_hw *hw
  }
  
  /**
 - * ice_vc_notify_vf_link_state - Inform a VF of link status
 - * @vf: pointer to the VF structure
 + * ice_conv_link_speed_to_virtchnl
 + * @adv_link_support: determines the format of the returned link speed
 + * @link_speed: variable containing the link_speed to be converted
   *
 - * send a link status message to a single VF
 + * Convert link speed supported by HW to link speed supported by virtchnl.
 + * If adv_link_support is true, then return link speed in Mbps. Else return
 + * link speed as a VIRTCHNL_LINK_SPEED_* casted to a u32. Note that the caller
 + * needs to cast back to an enum virtchnl_link_speed in the case where
 + * adv_link_support is false, but when adv_link_support is true the caller can
 + * expect the speed in Mbps.
   */
 -void ice_vc_notify_vf_link_state(struct ice_vf *vf)
 +u32 ice_conv_link_speed_to_virtchnl(bool adv_link_support, u16 link_speed)
  {
 -	struct virtchnl_pf_event pfe = { 0 };
 -	struct ice_hw *hw = &vf->pf->hw;
 +	u32 speed;
  
 -	pfe.event = VIRTCHNL_EVENT_LINK_CHANGE;
 -	pfe.severity = PF_EVENT_SEVERITY_INFO;
 -
 -	if (ice_is_vf_link_up(vf))
 -		ice_set_pfe_link(vf, &pfe,
 -				 hw->port_info->phy.link_info.link_speed, true);
 +	if (adv_link_support)
 +		switch (link_speed) {
 +		case ICE_AQ_LINK_SPEED_10MB:
 +			speed = ICE_LINK_SPEED_10MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_100MB:
 +			speed = ICE_LINK_SPEED_100MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_1000MB:
 +			speed = ICE_LINK_SPEED_1000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_2500MB:
 +			speed = ICE_LINK_SPEED_2500MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_5GB:
 +			speed = ICE_LINK_SPEED_5000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_10GB:
 +			speed = ICE_LINK_SPEED_10000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_20GB:
 +			speed = ICE_LINK_SPEED_20000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_25GB:
 +			speed = ICE_LINK_SPEED_25000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_40GB:
 +			speed = ICE_LINK_SPEED_40000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_50GB:
 +			speed = ICE_LINK_SPEED_50000MBPS;
 +			break;
 +		case ICE_AQ_LINK_SPEED_100GB:
 +			speed = ICE_LINK_SPEED_100000MBPS;
 +			break;
 +		default:
 +			speed = ICE_LINK_SPEED_UNKNOWN;
 +			break;
 +		}
  	else
 -		ice_set_pfe_link(vf, &pfe, ICE_AQ_LINK_SPEED_UNKNOWN, false);
 -
 -	ice_aq_send_msg_to_vf(hw, vf->vf_id, VIRTCHNL_OP_EVENT,
 -			      VIRTCHNL_STATUS_SUCCESS, (u8 *)&pfe,
 -			      sizeof(pfe), NULL);
 -}
++<<<<<<< HEAD
 +		/* Virtchnl speeds are not defined for every speed supported in
 +		 * the hardware. To maintain compatibility with older AVF
 +		 * drivers, while reporting the speed the new speed values are
 +		 * resolved to the closest known virtchnl speeds
++=======
++		vf->spoofchk = ena;
+ 
 -/**
 - * ice_vf_vsi_release - invalidate the VF's VSI after freeing it
 - * @vf: invalidate this VF's VSI after freeing it
 - */
 -static void ice_vf_vsi_release(struct ice_vf *vf)
 -{
 -	ice_vsi_release(ice_get_vf_vsi(vf));
 -	ice_vf_invalidate_vsi(vf);
++out_put_vf:
++	ice_put_vf(vf);
++	return ret;
+ }
+ 
+ /**
 - * ice_free_vf_res - Free a VF's resources
++ * ice_vc_cfg_promiscuous_mode_msg
+  * @vf: pointer to the VF info
++ * @msg: pointer to the msg buffer
++ *
++ * called from the VF to configure VF VSIs promiscuous mode
+  */
 -static void ice_free_vf_res(struct ice_vf *vf)
++static int ice_vc_cfg_promiscuous_mode_msg(struct ice_vf *vf, u8 *msg)
+ {
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	bool rm_promisc, alluni = false, allmulti = false;
++	struct virtchnl_promisc_info *info =
++	    (struct virtchnl_promisc_info *)msg;
++	struct ice_vsi_vlan_ops *vlan_ops;
++	int mcast_err = 0, ucast_err = 0;
+ 	struct ice_pf *pf = vf->pf;
 -	int i, last_vector_idx;
++	struct ice_vsi *vsi;
++	struct device *dev;
++	int ret = 0;
+ 
 -	/* First, disable VF's configuration API to prevent OS from
 -	 * accessing the VF's VSI after it's freed or invalidated.
 -	 */
 -	clear_bit(ICE_VF_STATE_INIT, vf->vf_states);
 -	ice_vf_fdir_exit(vf);
 -	/* free VF control VSI */
 -	if (vf->ctrl_vsi_idx != ICE_NO_VSI)
 -		ice_vf_ctrl_vsi_release(vf);
 -
 -	/* free VSI and disconnect it from the parent uplink */
 -	if (vf->lan_vsi_idx != ICE_NO_VSI) {
 -		ice_vf_vsi_release(vf);
 -		vf->num_mac = 0;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	last_vector_idx = vf->first_vector_idx + pf->vfs.num_msix_per - 1;
 -
 -	/* clear VF MDD event information */
 -	memset(&vf->mdd_tx_events, 0, sizeof(vf->mdd_tx_events));
 -	memset(&vf->mdd_rx_events, 0, sizeof(vf->mdd_rx_events));
 -
 -	/* Disable interrupts so that VF starts in a known state */
 -	for (i = vf->first_vector_idx; i <= last_vector_idx; i++) {
 -		wr32(&pf->hw, GLINT_DYN_CTL(i), GLINT_DYN_CTL_CLEARPBA_M);
 -		ice_flush(&pf->hw);
++	if (!ice_vc_isvalid_vsi_id(vf, info->vsi_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
 -	/* reset some of the state variables keeping track of the resources */
 -	clear_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states);
 -	clear_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states);
 -}
 -
 -/**
 - * ice_dis_vf_mappings
 - * @vf: pointer to the VF structure
 - */
 -static void ice_dis_vf_mappings(struct ice_vf *vf)
 -{
 -	struct ice_pf *pf = vf->pf;
 -	struct ice_vsi *vsi;
 -	struct device *dev;
 -	int first, last, v;
 -	struct ice_hw *hw;
+ 
 -	hw = &pf->hw;
+ 	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
+ 
+ 	dev = ice_pf_to_dev(pf);
 -	wr32(hw, VPINT_ALLOC(vf->vf_id), 0);
 -	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), 0);
 -
 -	first = vf->first_vector_idx;
 -	last = first + pf->vfs.num_msix_per - 1;
 -	for (v = first; v <= last; v++) {
 -		u32 reg;
 -
 -		reg = (((1 << GLINT_VECT2FUNC_IS_PF_S) &
 -			GLINT_VECT2FUNC_IS_PF_M) |
 -		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
 -			GLINT_VECT2FUNC_PF_NUM_M));
 -		wr32(hw, GLINT_VECT2FUNC(v), reg);
++	if (!ice_is_vf_trusted(vf)) {
++		dev_err(dev, "Unprivileged VF %d is attempting to configure promiscuous mode\n",
++			vf->vf_id);
++		/* Leave v_ret alone, lie to the VF on purpose. */
++		goto error_param;
+ 	}
+ 
 -	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG)
 -		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), 0);
 -	else
 -		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
 -
 -	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG)
 -		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), 0);
 -	else
 -		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
 -}
 -
 -/**
 - * ice_sriov_free_msix_res - Reset/free any used MSIX resources
 - * @pf: pointer to the PF structure
 - *
 - * Since no MSIX entries are taken from the pf->irq_tracker then just clear
 - * the pf->sriov_base_vector.
 - *
 - * Returns 0 on success, and -EINVAL on error.
 - */
 -static int ice_sriov_free_msix_res(struct ice_pf *pf)
 -{
 -	struct ice_res_tracker *res;
 -
 -	if (!pf)
 -		return -EINVAL;
 -
 -	res = pf->irq_tracker;
 -	if (!res)
 -		return -EINVAL;
 -
 -	/* give back irq_tracker resources used */
 -	WARN_ON(pf->sriov_base_vector < res->num_entries);
 -
 -	pf->sriov_base_vector = 0;
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_free_vfs - Free all VFs
 - * @pf: pointer to the PF structure
 - */
 -void ice_free_vfs(struct ice_pf *pf)
 -{
 -	struct device *dev = ice_pf_to_dev(pf);
 -	struct ice_vfs *vfs = &pf->vfs;
 -	struct ice_hw *hw = &pf->hw;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
++	if (info->flags & FLAG_VF_UNICAST_PROMISC)
++		alluni = true;
+ 
 -	if (!ice_has_vfs(pf))
 -		return;
++	if (info->flags & FLAG_VF_MULTICAST_PROMISC)
++		allmulti = true;
+ 
 -	while (test_and_set_bit(ICE_VF_DIS, pf->state))
 -		usleep_range(1000, 2000);
++	rm_promisc = !allmulti && !alluni;
+ 
 -	/* Disable IOV before freeing resources. This lets any VF drivers
 -	 * running in the host get themselves cleaned up before we yank
 -	 * the carpet out from underneath their feet.
 -	 */
 -	if (!pci_vfs_assigned(pf->pdev))
 -		pci_disable_sriov(pf->pdev);
++	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
++	if (rm_promisc)
++		ret = vlan_ops->ena_rx_filtering(vsi);
+ 	else
 -		dev_warn(dev, "VFs are assigned - not disabling SR-IOV\n");
 -
 -	mutex_lock(&vfs->table_lock);
 -
 -	ice_eswitch_release(pf);
++		ret = vlan_ops->dis_rx_filtering(vsi);
++	if (ret) {
++		dev_err(dev, "Failed to configure VLAN pruning in promiscuous mode\n");
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
+ 
 -	ice_for_each_vf(pf, bkt, vf) {
 -		mutex_lock(&vf->cfg_lock);
++	if (!test_bit(ICE_FLAG_VF_TRUE_PROMISC_ENA, pf->flags)) {
++		bool set_dflt_vsi = alluni || allmulti;
+ 
 -		ice_dis_vf_qs(vf);
++		if (set_dflt_vsi && !ice_is_dflt_vsi_in_use(pf->first_sw))
++			/* only attempt to set the default forwarding VSI if
++			 * it's not currently set
++			 */
++			ret = ice_set_dflt_vsi(pf->first_sw, vsi);
++		else if (!set_dflt_vsi &&
++			 ice_is_vsi_dflt_vsi(pf->first_sw, vsi))
++			/* only attempt to free the default forwarding VSI if we
++			 * are the owner
++			 */
++			ret = ice_clear_dflt_vsi(pf->first_sw);
+ 
 -		if (test_bit(ICE_VF_STATE_INIT, vf->vf_states)) {
 -			/* disable VF qp mappings and set VF disable state */
 -			ice_dis_vf_mappings(vf);
 -			set_bit(ICE_VF_STATE_DIS, vf->vf_states);
 -			ice_free_vf_res(vf);
++		if (ret) {
++			dev_err(dev, "%sable VF %d as the default VSI failed, error %d\n",
++				set_dflt_vsi ? "en" : "dis", vf->vf_id, ret);
++			v_ret = VIRTCHNL_STATUS_ERR_ADMIN_QUEUE_ERROR;
++			goto error_param;
+ 		}
++	} else {
++		u8 mcast_m, ucast_m;
+ 
 -		if (!pci_vfs_assigned(pf->pdev)) {
 -			u32 reg_idx, bit_idx;
 -
 -			reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
 -			bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
 -			wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
++		if (ice_vf_is_port_vlan_ena(vf) ||
++		    ice_vsi_has_non_zero_vlans(vsi)) {
++			mcast_m = ICE_MCAST_VLAN_PROMISC_BITS;
++			ucast_m = ICE_UCAST_VLAN_PROMISC_BITS;
++		} else {
++			mcast_m = ICE_MCAST_PROMISC_BITS;
++			ucast_m = ICE_UCAST_PROMISC_BITS;
+ 		}
+ 
 -		/* clear malicious info since the VF is getting released */
 -		if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
 -					ICE_MAX_SRIOV_VFS, vf->vf_id))
 -			dev_dbg(dev, "failed to clear malicious VF state for VF %u\n",
 -				vf->vf_id);
++		if (alluni)
++			ucast_err = ice_vf_set_vsi_promisc(vf, vsi, ucast_m);
++		else
++			ucast_err = ice_vf_clear_vsi_promisc(vf, vsi, ucast_m);
+ 
 -		mutex_unlock(&vf->cfg_lock);
 -	}
++		if (allmulti)
++			mcast_err = ice_vf_set_vsi_promisc(vf, vsi, mcast_m);
++		else
++			mcast_err = ice_vf_clear_vsi_promisc(vf, vsi, mcast_m);
+ 
 -	if (ice_sriov_free_msix_res(pf))
 -		dev_err(dev, "Failed to free MSIX resources used by SR-IOV\n");
++		if (ucast_err || mcast_err)
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++	}
+ 
 -	vfs->num_qps_per = 0;
 -	ice_free_vf_entries(pf);
++	if (!mcast_err) {
++		if (allmulti &&
++		    !test_and_set_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states))
++			dev_info(dev, "VF %u successfully set multicast promiscuous mode\n",
++				 vf->vf_id);
++		else if (!allmulti &&
++			 test_and_clear_bit(ICE_VF_STATE_MC_PROMISC,
++					    vf->vf_states))
++			dev_info(dev, "VF %u successfully unset multicast promiscuous mode\n",
++				 vf->vf_id);
++	}
+ 
 -	mutex_unlock(&vfs->table_lock);
++	if (!ucast_err) {
++		if (alluni &&
++		    !test_and_set_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states))
++			dev_info(dev, "VF %u successfully set unicast promiscuous mode\n",
++				 vf->vf_id);
++		else if (!alluni &&
++			 test_and_clear_bit(ICE_VF_STATE_UC_PROMISC,
++					    vf->vf_states))
++			dev_info(dev, "VF %u successfully unset unicast promiscuous mode\n",
++				 vf->vf_id);
++	}
+ 
 -	clear_bit(ICE_VF_DIS, pf->state);
 -	clear_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
++error_param:
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE,
++				     v_ret, NULL, 0);
+ }
+ 
+ /**
 - * ice_vf_vsi_setup - Set up a VF VSI
 - * @vf: VF to setup VSI for
++ * ice_vc_get_stats_msg
++ * @vf: pointer to the VF info
++ * @msg: pointer to the msg buffer
+  *
 - * Returns pointer to the successfully allocated VSI struct on success,
 - * otherwise returns NULL on failure.
++ * called from the VF to get VSI stats
+  */
 -static struct ice_vsi *ice_vf_vsi_setup(struct ice_vf *vf)
++static int ice_vc_get_stats_msg(struct ice_vf *vf, u8 *msg)
+ {
 -	struct ice_port_info *pi = ice_vf_get_port_info(vf);
 -	struct ice_pf *pf = vf->pf;
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_queue_select *vqs =
++		(struct virtchnl_queue_select *)msg;
++	struct ice_eth_stats stats = { 0 };
+ 	struct ice_vsi *vsi;
+ 
 -	vsi = ice_vsi_setup(pf, pi, ICE_VSI_VF, vf, NULL);
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
++
++	if (!ice_vc_isvalid_vsi_id(vf, vqs->vsi_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
+ 
++	vsi = ice_get_vf_vsi(vf);
+ 	if (!vsi) {
 -		dev_err(ice_pf_to_dev(pf), "Failed to create VF VSI\n");
 -		ice_vf_invalidate_vsi(vf);
 -		return NULL;
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	vf->lan_vsi_idx = vsi->idx;
 -	vf->lan_vsi_num = vsi->vsi_num;
++	ice_update_eth_stats(vsi);
+ 
 -	return vsi;
 -}
++	stats = vsi->eth_stats;
+ 
 -/**
 - * ice_calc_vf_first_vector_idx - Calculate MSIX vector index in the PF space
 - * @pf: pointer to PF structure
 - * @vf: pointer to VF that the first MSIX vector index is being calculated for
 - *
 - * This returns the first MSIX vector index in PF space that is used by this VF.
 - * This index is used when accessing PF relative registers such as
 - * GLINT_VECT2FUNC and GLINT_DYN_CTL.
 - * This will always be the OICR index in the AVF driver so any functionality
 - * using vf->first_vector_idx for queue configuration will have to increment by
 - * 1 to avoid meddling with the OICR index.
 - */
 -static int ice_calc_vf_first_vector_idx(struct ice_pf *pf, struct ice_vf *vf)
 -{
 -	return pf->sriov_base_vector + vf->vf_id * pf->vfs.num_msix_per;
++error_param:
++	/* send the response to the VF */
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_GET_STATS, v_ret,
++				     (u8 *)&stats, sizeof(stats));
+ }
+ 
+ /**
 - * ice_ena_vf_msix_mappings - enable VF MSIX mappings in hardware
 - * @vf: VF to enable MSIX mappings for
++ * ice_vc_validate_vqs_bitmaps - validate Rx/Tx queue bitmaps from VIRTCHNL
++ * @vqs: virtchnl_queue_select structure containing bitmaps to validate
+  *
 - * Some of the registers need to be indexed/configured using hardware global
 - * device values and other registers need 0-based values, which represent PF
 - * based values.
++ * Return true on successful validation, else false
+  */
 -static void ice_ena_vf_msix_mappings(struct ice_vf *vf)
++static bool ice_vc_validate_vqs_bitmaps(struct virtchnl_queue_select *vqs)
+ {
 -	int device_based_first_msix, device_based_last_msix;
 -	int pf_based_first_msix, pf_based_last_msix, v;
 -	struct ice_pf *pf = vf->pf;
 -	int device_based_vf_id;
 -	struct ice_hw *hw;
 -	u32 reg;
 -
 -	hw = &pf->hw;
 -	pf_based_first_msix = vf->first_vector_idx;
 -	pf_based_last_msix = (pf_based_first_msix + pf->vfs.num_msix_per) - 1;
 -
 -	device_based_first_msix = pf_based_first_msix +
 -		pf->hw.func_caps.common_cap.msix_vector_first_id;
 -	device_based_last_msix =
 -		(device_based_first_msix + pf->vfs.num_msix_per) - 1;
 -	device_based_vf_id = vf->vf_id + hw->func_caps.vf_base_id;
 -
 -	reg = (((device_based_first_msix << VPINT_ALLOC_FIRST_S) &
 -		VPINT_ALLOC_FIRST_M) |
 -	       ((device_based_last_msix << VPINT_ALLOC_LAST_S) &
 -		VPINT_ALLOC_LAST_M) | VPINT_ALLOC_VALID_M);
 -	wr32(hw, VPINT_ALLOC(vf->vf_id), reg);
 -
 -	reg = (((device_based_first_msix << VPINT_ALLOC_PCI_FIRST_S)
 -		 & VPINT_ALLOC_PCI_FIRST_M) |
 -	       ((device_based_last_msix << VPINT_ALLOC_PCI_LAST_S) &
 -		VPINT_ALLOC_PCI_LAST_M) | VPINT_ALLOC_PCI_VALID_M);
 -	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), reg);
 -
 -	/* map the interrupts to its functions */
 -	for (v = pf_based_first_msix; v <= pf_based_last_msix; v++) {
 -		reg = (((device_based_vf_id << GLINT_VECT2FUNC_VF_NUM_S) &
 -			GLINT_VECT2FUNC_VF_NUM_M) |
 -		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
 -			GLINT_VECT2FUNC_PF_NUM_M));
 -		wr32(hw, GLINT_VECT2FUNC(v), reg);
 -	}
++	if ((!vqs->rx_queues && !vqs->tx_queues) ||
++	    vqs->rx_queues >= BIT(ICE_MAX_RSS_QS_PER_VF) ||
++	    vqs->tx_queues >= BIT(ICE_MAX_RSS_QS_PER_VF))
++		return false;
+ 
 -	/* Map mailbox interrupt to VF MSI-X vector 0 */
 -	wr32(hw, VPINT_MBX_CTL(device_based_vf_id), VPINT_MBX_CTL_CAUSE_ENA_M);
++	return true;
+ }
+ 
+ /**
 - * ice_ena_vf_q_mappings - enable Rx/Tx queue mappings for a VF
 - * @vf: VF to enable the mappings for
 - * @max_txq: max Tx queues allowed on the VF's VSI
 - * @max_rxq: max Rx queues allowed on the VF's VSI
++ * ice_vf_ena_txq_interrupt - enable Tx queue interrupt via QINT_TQCTL
++ * @vsi: VSI of the VF to configure
++ * @q_idx: VF queue index used to determine the queue in the PF's space
+  */
 -static void ice_ena_vf_q_mappings(struct ice_vf *vf, u16 max_txq, u16 max_rxq)
++static void ice_vf_ena_txq_interrupt(struct ice_vsi *vsi, u32 q_idx)
+ {
 -	struct device *dev = ice_pf_to_dev(vf->pf);
 -	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
 -	struct ice_hw *hw = &vf->pf->hw;
++	struct ice_hw *hw = &vsi->back->hw;
++	u32 pfq = vsi->txq_map[q_idx];
+ 	u32 reg;
+ 
 -	/* set regardless of mapping mode */
 -	wr32(hw, VPLAN_TXQ_MAPENA(vf->vf_id), VPLAN_TXQ_MAPENA_TX_ENA_M);
++	reg = rd32(hw, QINT_TQCTL(pfq));
+ 
 -	/* VF Tx queues allocation */
 -	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG) {
 -		/* set the VF PF Tx queue range
 -		 * VFNUMQ value should be set to (number of queues - 1). A value
 -		 * of 0 means 1 queue and a value of 255 means 256 queues
 -		 */
 -		reg = (((vsi->txq_map[0] << VPLAN_TX_QBASE_VFFIRSTQ_S) &
 -			VPLAN_TX_QBASE_VFFIRSTQ_M) |
 -		       (((max_txq - 1) << VPLAN_TX_QBASE_VFNUMQ_S) &
 -			VPLAN_TX_QBASE_VFNUMQ_M));
 -		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), reg);
 -	} else {
 -		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
 -	}
 -
 -	/* set regardless of mapping mode */
 -	wr32(hw, VPLAN_RXQ_MAPENA(vf->vf_id), VPLAN_RXQ_MAPENA_RX_ENA_M);
 -
 -	/* VF Rx queues allocation */
 -	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG) {
 -		/* set the VF PF Rx queue range
 -		 * VFNUMQ value should be set to (number of queues - 1). A value
 -		 * of 0 means 1 queue and a value of 255 means 256 queues
 -		 */
 -		reg = (((vsi->rxq_map[0] << VPLAN_RX_QBASE_VFFIRSTQ_S) &
 -			VPLAN_RX_QBASE_VFFIRSTQ_M) |
 -		       (((max_rxq - 1) << VPLAN_RX_QBASE_VFNUMQ_S) &
 -			VPLAN_RX_QBASE_VFNUMQ_M));
 -		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), reg);
 -	} else {
 -		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
 -	}
 -}
 -
 -/**
 - * ice_ena_vf_mappings - enable VF MSIX and queue mapping
 - * @vf: pointer to the VF structure
 - */
 -static void ice_ena_vf_mappings(struct ice_vf *vf)
 -{
 -	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
 -
 -	ice_ena_vf_msix_mappings(vf);
 -	ice_ena_vf_q_mappings(vf, vsi->alloc_txq, vsi->alloc_rxq);
 -}
 -
 -/**
 - * ice_calc_vf_reg_idx - Calculate the VF's register index in the PF space
 - * @vf: VF to calculate the register index for
 - * @q_vector: a q_vector associated to the VF
 - */
 -int ice_calc_vf_reg_idx(struct ice_vf *vf, struct ice_q_vector *q_vector)
 -{
 -	struct ice_pf *pf;
 -
 -	if (!vf || !q_vector)
 -		return -EINVAL;
 -
 -	pf = vf->pf;
 -
 -	/* always add one to account for the OICR being the first MSIX */
 -	return pf->sriov_base_vector + pf->vfs.num_msix_per * vf->vf_id +
 -		q_vector->v_idx + 1;
 -}
 -
 -/**
 - * ice_get_max_valid_res_idx - Get the max valid resource index
 - * @res: pointer to the resource to find the max valid index for
 - *
 - * Start from the end of the ice_res_tracker and return right when we find the
 - * first res->list entry with the ICE_RES_VALID_BIT set. This function is only
 - * valid for SR-IOV because it is the only consumer that manipulates the
 - * res->end and this is always called when res->end is set to res->num_entries.
 - */
 -static int ice_get_max_valid_res_idx(struct ice_res_tracker *res)
 -{
 -	int i;
 -
 -	if (!res)
 -		return -EINVAL;
 -
 -	for (i = res->num_entries - 1; i >= 0; i--)
 -		if (res->list[i] & ICE_RES_VALID_BIT)
 -			return i;
++	/* MSI-X index 0 in the VF's space is always for the OICR, which means
++	 * this is most likely a poll mode VF driver, so don't enable an
++	 * interrupt that was never configured via VIRTCHNL_OP_CONFIG_IRQ_MAP
++	 */
++	if (!(reg & QINT_TQCTL_MSIX_INDX_M))
++		return;
+ 
 -	return 0;
++	wr32(hw, QINT_TQCTL(pfq), reg | QINT_TQCTL_CAUSE_ENA_M);
+ }
+ 
+ /**
 - * ice_sriov_set_msix_res - Set any used MSIX resources
 - * @pf: pointer to PF structure
 - * @num_msix_needed: number of MSIX vectors needed for all SR-IOV VFs
 - *
 - * This function allows SR-IOV resources to be taken from the end of the PF's
 - * allowed HW MSIX vectors so that the irq_tracker will not be affected. We
 - * just set the pf->sriov_base_vector and return success.
 - *
 - * If there are not enough resources available, return an error. This should
 - * always be caught by ice_set_per_vf_res().
 - *
 - * Return 0 on success, and -EINVAL when there are not enough MSIX vectors
 - * in the PF's space available for SR-IOV.
++ * ice_vf_ena_rxq_interrupt - enable Tx queue interrupt via QINT_RQCTL
++ * @vsi: VSI of the VF to configure
++ * @q_idx: VF queue index used to determine the queue in the PF's space
+  */
 -static int ice_sriov_set_msix_res(struct ice_pf *pf, u16 num_msix_needed)
++static void ice_vf_ena_rxq_interrupt(struct ice_vsi *vsi, u32 q_idx)
+ {
 -	u16 total_vectors = pf->hw.func_caps.common_cap.num_msix_vectors;
 -	int vectors_used = pf->irq_tracker->num_entries;
 -	int sriov_base_vector;
++	struct ice_hw *hw = &vsi->back->hw;
++	u32 pfq = vsi->rxq_map[q_idx];
++	u32 reg;
+ 
 -	sriov_base_vector = total_vectors - num_msix_needed;
++	reg = rd32(hw, QINT_RQCTL(pfq));
+ 
 -	/* make sure we only grab irq_tracker entries from the list end and
 -	 * that we have enough available MSIX vectors
++	/* MSI-X index 0 in the VF's space is always for the OICR, which means
++	 * this is most likely a poll mode VF driver, so don't enable an
++	 * interrupt that was never configured via VIRTCHNL_OP_CONFIG_IRQ_MAP
+ 	 */
 -	if (sriov_base_vector < vectors_used)
 -		return -EINVAL;
 -
 -	pf->sriov_base_vector = sriov_base_vector;
++	if (!(reg & QINT_RQCTL_MSIX_INDX_M))
++		return;
+ 
 -	return 0;
++	wr32(hw, QINT_RQCTL(pfq), reg | QINT_RQCTL_CAUSE_ENA_M);
+ }
+ 
+ /**
 - * ice_set_per_vf_res - check if vectors and queues are available
 - * @pf: pointer to the PF structure
 - * @num_vfs: the number of SR-IOV VFs being configured
 - *
 - * First, determine HW interrupts from common pool. If we allocate fewer VFs, we
 - * get more vectors and can enable more queues per VF. Note that this does not
 - * grab any vectors from the SW pool already allocated. Also note, that all
 - * vector counts include one for each VF's miscellaneous interrupt vector
 - * (i.e. OICR).
 - *
 - * Minimum VFs - 2 vectors, 1 queue pair
 - * Small VFs - 5 vectors, 4 queue pairs
 - * Medium VFs - 17 vectors, 16 queue pairs
 - *
 - * Second, determine number of queue pairs per VF by starting with a pre-defined
 - * maximum each VF supports. If this is not possible, then we adjust based on
 - * queue pairs available on the device.
++ * ice_vc_ena_qs_msg
++ * @vf: pointer to the VF info
++ * @msg: pointer to the msg buffer
+  *
 - * Lastly, set queue and MSI-X VF variables tracked by the PF so it can be used
 - * by each VF during VF initialization and reset.
++ * called from the VF to enable all or specific queue(s)
+  */
 -static int ice_set_per_vf_res(struct ice_pf *pf, u16 num_vfs)
++static int ice_vc_ena_qs_msg(struct ice_vf *vf, u8 *msg)
+ {
 -	int max_valid_res_idx = ice_get_max_valid_res_idx(pf->irq_tracker);
 -	u16 num_msix_per_vf, num_txq, num_rxq, avail_qs;
 -	int msix_avail_per_vf, msix_avail_for_sriov;
 -	struct device *dev = ice_pf_to_dev(pf);
 -	int err;
 -
 -	lockdep_assert_held(&pf->vfs.table_lock);
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_queue_select *vqs =
++	    (struct virtchnl_queue_select *)msg;
++	struct ice_vsi *vsi;
++	unsigned long q_map;
++	u16 vf_q_id;
+ 
 -	if (!num_vfs)
 -		return -EINVAL;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
+ 
 -	if (max_valid_res_idx < 0)
 -		return -ENOSPC;
 -
 -	/* determine MSI-X resources per VF */
 -	msix_avail_for_sriov = pf->hw.func_caps.common_cap.num_msix_vectors -
 -		pf->irq_tracker->num_entries;
 -	msix_avail_per_vf = msix_avail_for_sriov / num_vfs;
 -	if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MED) {
 -		num_msix_per_vf = ICE_NUM_VF_MSIX_MED;
 -	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_SMALL) {
 -		num_msix_per_vf = ICE_NUM_VF_MSIX_SMALL;
 -	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MULTIQ_MIN) {
 -		num_msix_per_vf = ICE_NUM_VF_MSIX_MULTIQ_MIN;
 -	} else if (msix_avail_per_vf >= ICE_MIN_INTR_PER_VF) {
 -		num_msix_per_vf = ICE_MIN_INTR_PER_VF;
 -	} else {
 -		dev_err(dev, "Only %d MSI-X interrupts available for SR-IOV. Not enough to support minimum of %d MSI-X interrupts per VF for %d VFs\n",
 -			msix_avail_for_sriov, ICE_MIN_INTR_PER_VF,
 -			num_vfs);
 -		return -ENOSPC;
++	if (!ice_vc_isvalid_vsi_id(vf, vqs->vsi_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	num_txq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
 -			ICE_MAX_RSS_QS_PER_VF);
 -	avail_qs = ice_get_avail_txq_count(pf) / num_vfs;
 -	if (!avail_qs)
 -		num_txq = 0;
 -	else if (num_txq > avail_qs)
 -		num_txq = rounddown_pow_of_two(avail_qs);
 -
 -	num_rxq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
 -			ICE_MAX_RSS_QS_PER_VF);
 -	avail_qs = ice_get_avail_rxq_count(pf) / num_vfs;
 -	if (!avail_qs)
 -		num_rxq = 0;
 -	else if (num_rxq > avail_qs)
 -		num_rxq = rounddown_pow_of_two(avail_qs);
 -
 -	if (num_txq < ICE_MIN_QS_PER_VF || num_rxq < ICE_MIN_QS_PER_VF) {
 -		dev_err(dev, "Not enough queues to support minimum of %d queue pairs per VF for %d VFs\n",
 -			ICE_MIN_QS_PER_VF, num_vfs);
 -		return -ENOSPC;
++	if (!ice_vc_validate_vqs_bitmaps(vqs)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	err = ice_sriov_set_msix_res(pf, num_msix_per_vf * num_vfs);
 -	if (err) {
 -		dev_err(dev, "Unable to set MSI-X resources for %d VFs, err %d\n",
 -			num_vfs, err);
 -		return err;
++	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	/* only allow equal Tx/Rx queue count (i.e. queue pairs) */
 -	pf->vfs.num_qps_per = min_t(int, num_txq, num_rxq);
 -	pf->vfs.num_msix_per = num_msix_per_vf;
 -	dev_info(dev, "Enabling %d VFs with %d vectors and %d queues per VF\n",
 -		 num_vfs, pf->vfs.num_msix_per, pf->vfs.num_qps_per);
++	/* Enable only Rx rings, Tx rings were enabled by the FW when the
++	 * Tx queue group list was configured and the context bits were
++	 * programmed using ice_vsi_cfg_txqs
++	 */
++	q_map = vqs->rx_queues;
++	for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
++		if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++			goto error_param;
++		}
+ 
 -	return 0;
 -}
++		/* Skip queue if enabled */
++		if (test_bit(vf_q_id, vf->rxq_ena))
++			continue;
+ 
 -/**
 - * ice_vc_notify_link_state - Inform all VFs on a PF of link status
 - * @pf: pointer to the PF structure
 - */
 -void ice_vc_notify_link_state(struct ice_pf *pf)
 -{
 -	struct ice_vf *vf;
 -	unsigned int bkt;
++		if (ice_vsi_ctrl_one_rx_ring(vsi, true, vf_q_id, true)) {
++			dev_err(ice_pf_to_dev(vsi->back), "Failed to enable Rx ring %d on VSI %d\n",
++				vf_q_id, vsi->vsi_num);
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++			goto error_param;
++		}
+ 
 -	mutex_lock(&pf->vfs.table_lock);
 -	ice_for_each_vf(pf, bkt, vf)
 -		ice_vc_notify_vf_link_state(vf);
 -	mutex_unlock(&pf->vfs.table_lock);
 -}
++		ice_vf_ena_rxq_interrupt(vsi, vf_q_id);
++		set_bit(vf_q_id, vf->rxq_ena);
++	}
+ 
 -/**
 - * ice_vc_notify_reset - Send pending reset message to all VFs
 - * @pf: pointer to the PF structure
 - *
 - * indicate a pending reset to all VFs on a given PF
 - */
 -void ice_vc_notify_reset(struct ice_pf *pf)
 -{
 -	struct virtchnl_pf_event pfe;
++	q_map = vqs->tx_queues;
++	for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
++		if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++			goto error_param;
++		}
+ 
 -	if (!ice_has_vfs(pf))
 -		return;
++		/* Skip queue if enabled */
++		if (test_bit(vf_q_id, vf->txq_ena))
++			continue;
++
++		ice_vf_ena_txq_interrupt(vsi, vf_q_id);
++		set_bit(vf_q_id, vf->txq_ena);
++	}
++
++	/* Set flag to indicate that queues are enabled */
++	if (v_ret == VIRTCHNL_STATUS_SUCCESS)
++		set_bit(ICE_VF_STATE_QS_ENA, vf->vf_states);
+ 
 -	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
 -	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
 -	ice_vc_vf_broadcast(pf, VIRTCHNL_OP_EVENT, VIRTCHNL_STATUS_SUCCESS,
 -			    (u8 *)&pfe, sizeof(struct virtchnl_pf_event));
++error_param:
++	/* send the response to the VF */
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_QUEUES, v_ret,
++				     NULL, 0);
+ }
+ 
+ /**
 - * ice_init_vf_vsi_res - initialize/setup VF VSI resources
 - * @vf: VF to initialize/setup the VSI for
++ * ice_vc_dis_qs_msg
++ * @vf: pointer to the VF info
++ * @msg: pointer to the msg buffer
+  *
 - * This function creates a VSI for the VF, adds a VLAN 0 filter, and sets up the
 - * VF VSI's broadcast filter and is only used during initial VF creation.
++ * called from the VF to disable all or specific
++ * queue(s)
+  */
 -static int ice_init_vf_vsi_res(struct ice_vf *vf)
++static int ice_vc_dis_qs_msg(struct ice_vf *vf, u8 *msg)
+ {
 -	struct ice_vsi_vlan_ops *vlan_ops;
 -	struct ice_pf *pf = vf->pf;
 -	u8 broadcast[ETH_ALEN];
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_queue_select *vqs =
++	    (struct virtchnl_queue_select *)msg;
+ 	struct ice_vsi *vsi;
 -	struct device *dev;
 -	int err;
 -
 -	vf->first_vector_idx = ice_calc_vf_first_vector_idx(pf, vf);
 -
 -	dev = ice_pf_to_dev(pf);
 -	vsi = ice_vf_vsi_setup(vf);
 -	if (!vsi)
 -		return -ENOMEM;
++	unsigned long q_map;
++	u16 vf_q_id;
+ 
 -	err = ice_vsi_add_vlan_zero(vsi);
 -	if (err) {
 -		dev_warn(dev, "Failed to add VLAN 0 filter for VF %d\n",
 -			 vf->vf_id);
 -		goto release_vsi;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states) &&
++	    !test_bit(ICE_VF_STATE_QS_ENA, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
 -	err = vlan_ops->ena_rx_filtering(vsi);
 -	if (err) {
 -		dev_warn(dev, "Failed to enable Rx VLAN filtering for VF %d\n",
 -			 vf->vf_id);
 -		goto release_vsi;
++	if (!ice_vc_isvalid_vsi_id(vf, vqs->vsi_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	eth_broadcast_addr(broadcast);
 -	err = ice_fltr_add_mac(vsi, broadcast, ICE_FWD_TO_VSI);
 -	if (err) {
 -		dev_err(dev, "Failed to add broadcast MAC filter for VF %d, error %d\n",
 -			vf->vf_id, err);
 -		goto release_vsi;
++	if (!ice_vc_validate_vqs_bitmaps(vqs)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	err = ice_vsi_apply_spoofchk(vsi, vf->spoofchk);
 -	if (err) {
 -		dev_warn(dev, "Failed to initialize spoofchk setting for VF %d\n",
 -			 vf->vf_id);
 -		goto release_vsi;
++	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	vf->num_mac = 1;
++	if (vqs->tx_queues) {
++		q_map = vqs->tx_queues;
+ 
 -	return 0;
++		for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
++			struct ice_tx_ring *ring = vsi->tx_rings[vf_q_id];
++			struct ice_txq_meta txq_meta = { 0 };
+ 
 -release_vsi:
 -	ice_vf_vsi_release(vf);
 -	return err;
 -}
++			if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
+ 
 -/**
 - * ice_start_vfs - start VFs so they are ready to be used by SR-IOV
 - * @pf: PF the VFs are associated with
 - */
 -static int ice_start_vfs(struct ice_pf *pf)
 -{
 -	struct ice_hw *hw = &pf->hw;
 -	unsigned int bkt, it_cnt;
 -	struct ice_vf *vf;
 -	int retval;
 -
 -	lockdep_assert_held(&pf->vfs.table_lock);
 -
 -	it_cnt = 0;
 -	ice_for_each_vf(pf, bkt, vf) {
 -		vf->vf_ops->clear_reset_trigger(vf);
 -
 -		retval = ice_init_vf_vsi_res(vf);
 -		if (retval) {
 -			dev_err(ice_pf_to_dev(pf), "Failed to initialize VSI resources for VF %d, error %d\n",
 -				vf->vf_id, retval);
 -			goto teardown;
 -		}
++			/* Skip queue if not enabled */
++			if (!test_bit(vf_q_id, vf->txq_ena))
++				continue;
++
++			ice_fill_txq_meta(vsi, ring, &txq_meta);
++
++			if (ice_vsi_stop_tx_ring(vsi, ICE_NO_RESET, vf->vf_id,
++						 ring, &txq_meta)) {
++				dev_err(ice_pf_to_dev(vsi->back), "Failed to stop Tx ring %d on VSI %d\n",
++					vf_q_id, vsi->vsi_num);
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
+ 
 -		set_bit(ICE_VF_STATE_INIT, vf->vf_states);
 -		ice_ena_vf_mappings(vf);
 -		wr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
 -		it_cnt++;
++			/* Clear enabled queues flag */
++			clear_bit(vf_q_id, vf->txq_ena);
++		}
+ 	}
+ 
 -	ice_flush(hw);
 -	return 0;
++	q_map = vqs->rx_queues;
++	/* speed up Rx queue disable by batching them if possible */
++	if (q_map &&
++	    bitmap_equal(&q_map, vf->rxq_ena, ICE_MAX_RSS_QS_PER_VF)) {
++		if (ice_vsi_stop_all_rx_rings(vsi)) {
++			dev_err(ice_pf_to_dev(vsi->back), "Failed to stop all Rx rings on VSI %d\n",
++				vsi->vsi_num);
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++			goto error_param;
++		}
+ 
 -teardown:
 -	ice_for_each_vf(pf, bkt, vf) {
 -		if (it_cnt == 0)
 -			break;
++		bitmap_zero(vf->rxq_ena, ICE_MAX_RSS_QS_PER_VF);
++	} else if (q_map) {
++		for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
++			if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
++
++			/* Skip queue if not enabled */
++			if (!test_bit(vf_q_id, vf->rxq_ena))
++				continue;
++
++			if (ice_vsi_ctrl_one_rx_ring(vsi, false, vf_q_id,
++						     true)) {
++				dev_err(ice_pf_to_dev(vsi->back), "Failed to stop Rx ring %d on VSI %d\n",
++					vf_q_id, vsi->vsi_num);
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
+ 
 -		ice_dis_vf_mappings(vf);
 -		ice_vf_vsi_release(vf);
 -		it_cnt--;
++			/* Clear enabled queues flag */
++			clear_bit(vf_q_id, vf->rxq_ena);
++		}
+ 	}
+ 
 -	return retval;
++	/* Clear enabled queues flag */
++	if (v_ret == VIRTCHNL_STATUS_SUCCESS && ice_vf_has_no_qs_ena(vf))
++		clear_bit(ICE_VF_STATE_QS_ENA, vf->vf_states);
++
++error_param:
++	/* send the response to the VF */
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_QUEUES, v_ret,
++				     NULL, 0);
+ }
+ 
+ /**
 - * ice_sriov_free_vf - Free VF memory after all references are dropped
 - * @vf: pointer to VF to free
 - *
 - * Called by ice_put_vf through ice_release_vf once the last reference to a VF
 - * structure has been dropped.
++ * ice_cfg_interrupt
++ * @vf: pointer to the VF info
++ * @vsi: the VSI being configured
++ * @vector_id: vector ID
++ * @map: vector map for mapping vectors to queues
++ * @q_vector: structure for interrupt vector
++ * configure the IRQ to queue map
+  */
 -static void ice_sriov_free_vf(struct ice_vf *vf)
++static int
++ice_cfg_interrupt(struct ice_vf *vf, struct ice_vsi *vsi, u16 vector_id,
++		  struct virtchnl_vector_map *map,
++		  struct ice_q_vector *q_vector)
+ {
 -	mutex_destroy(&vf->cfg_lock);
++	u16 vsi_q_id, vsi_q_id_idx;
++	unsigned long qmap;
+ 
 -	kfree_rcu(vf, rcu);
 -}
++	q_vector->num_ring_rx = 0;
++	q_vector->num_ring_tx = 0;
+ 
 -/**
 - * ice_sriov_clear_mbx_register - clears SRIOV VF's mailbox registers
 - * @vf: the vf to configure
 - */
 -static void ice_sriov_clear_mbx_register(struct ice_vf *vf)
 -{
 -	struct ice_pf *pf = vf->pf;
++	qmap = map->rxq_map;
++	for_each_set_bit(vsi_q_id_idx, &qmap, ICE_MAX_RSS_QS_PER_VF) {
++		vsi_q_id = vsi_q_id_idx;
++
++		if (!ice_vc_isvalid_q_id(vf, vsi->vsi_num, vsi_q_id))
++			return VIRTCHNL_STATUS_ERR_PARAM;
++
++		q_vector->num_ring_rx++;
++		q_vector->rx.itr_idx = map->rxitr_idx;
++		vsi->rx_rings[vsi_q_id]->q_vector = q_vector;
++		ice_cfg_rxq_interrupt(vsi, vsi_q_id, vector_id,
++				      q_vector->rx.itr_idx);
++	}
++
++	qmap = map->txq_map;
++	for_each_set_bit(vsi_q_id_idx, &qmap, ICE_MAX_RSS_QS_PER_VF) {
++		vsi_q_id = vsi_q_id_idx;
++
++		if (!ice_vc_isvalid_q_id(vf, vsi->vsi_num, vsi_q_id))
++			return VIRTCHNL_STATUS_ERR_PARAM;
+ 
 -	wr32(&pf->hw, VF_MBX_ARQLEN(vf->vf_id), 0);
 -	wr32(&pf->hw, VF_MBX_ATQLEN(vf->vf_id), 0);
++		q_vector->num_ring_tx++;
++		q_vector->tx.itr_idx = map->txitr_idx;
++		vsi->tx_rings[vsi_q_id]->q_vector = q_vector;
++		ice_cfg_txq_interrupt(vsi, vsi_q_id, vector_id,
++				      q_vector->tx.itr_idx);
++	}
++
++	return VIRTCHNL_STATUS_SUCCESS;
+ }
+ 
+ /**
 - * ice_sriov_trigger_reset_register - trigger VF reset for SRIOV VF
 - * @vf: pointer to VF structure
 - * @is_vflr: true if reset occurred due to VFLR
++ * ice_vc_cfg_irq_map_msg
++ * @vf: pointer to the VF info
++ * @msg: pointer to the msg buffer
+  *
 - * Trigger and cleanup after a VF reset for a SR-IOV VF.
++ * called from the VF to configure the IRQ to queue map
+  */
 -static void ice_sriov_trigger_reset_register(struct ice_vf *vf, bool is_vflr)
++static int ice_vc_cfg_irq_map_msg(struct ice_vf *vf, u8 *msg)
+ {
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	u16 num_q_vectors_mapped, vsi_id, vector_id;
++	struct virtchnl_irq_map_info *irqmap_info;
++	struct virtchnl_vector_map *map;
+ 	struct ice_pf *pf = vf->pf;
 -	u32 reg, reg_idx, bit_idx;
 -	unsigned int vf_abs_id, i;
 -	struct device *dev;
 -	struct ice_hw *hw;
++	struct ice_vsi *vsi;
++	int i;
+ 
 -	dev = ice_pf_to_dev(pf);
 -	hw = &pf->hw;
 -	vf_abs_id = vf->vf_id + hw->func_caps.vf_base_id;
++	irqmap_info = (struct virtchnl_irq_map_info *)msg;
++	num_q_vectors_mapped = irqmap_info->num_vectors;
+ 
 -	/* In the case of a VFLR, HW has already reset the VF and we just need
 -	 * to clean up. Otherwise we must first trigger the reset using the
 -	 * VFRTRIG register.
++	/* Check to make sure number of VF vectors mapped is not greater than
++	 * number of VF vectors originally allocated, and check that
++	 * there is actually at least a single VF queue vector mapped
+ 	 */
 -	if (!is_vflr) {
 -		reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
 -		reg |= VPGEN_VFRTRIG_VFSWR_M;
 -		wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states) ||
++	    pf->vfs.num_msix_per < num_q_vectors_mapped ||
++	    !num_q_vectors_mapped) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
++
++	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
+ 
 -	/* clear the VFLR bit in GLGEN_VFLRSTAT */
 -	reg_idx = (vf_abs_id) / 32;
 -	bit_idx = (vf_abs_id) % 32;
 -	wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
 -	ice_flush(hw);
 -
 -	wr32(hw, PF_PCI_CIAA,
 -	     VF_DEVICE_STATUS | (vf_abs_id << PF_PCI_CIAA_VF_NUM_S));
 -	for (i = 0; i < ICE_PCI_CIAD_WAIT_COUNT; i++) {
 -		reg = rd32(hw, PF_PCI_CIAD);
 -		/* no transactions pending so stop polling */
 -		if ((reg & VF_TRANS_PENDING_M) == 0)
++	for (i = 0; i < num_q_vectors_mapped; i++) {
++		struct ice_q_vector *q_vector;
++
++		map = &irqmap_info->vecmap[i];
++
++		vector_id = map->vector_id;
++		vsi_id = map->vsi_id;
++		/* vector_id is always 0-based for each VF, and can never be
++		 * larger than or equal to the max allowed interrupts per VF
++>>>>>>> 8cf52bec5ca0 (ice: cleanup long lines in ice_sriov.c)
 +		 */
 +		switch (link_speed) {
 +		case ICE_AQ_LINK_SPEED_10MB:
 +		case ICE_AQ_LINK_SPEED_100MB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_100MB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_1000MB:
 +		case ICE_AQ_LINK_SPEED_2500MB:
 +		case ICE_AQ_LINK_SPEED_5GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_1GB;
  			break;
 +		case ICE_AQ_LINK_SPEED_10GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_10GB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_20GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_20GB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_25GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_25GB;
 +			break;
 +		case ICE_AQ_LINK_SPEED_40GB:
 +		case ICE_AQ_LINK_SPEED_50GB:
 +		case ICE_AQ_LINK_SPEED_100GB:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_40GB;
 +			break;
 +		default:
 +			speed = (u32)VIRTCHNL_LINK_SPEED_UNKNOWN;
 +			break;
 +		}
  
 -		dev_err(dev, "VF %u PCI transactions stuck\n", vf->vf_id);
 -		udelay(ICE_PCI_CIAD_WAIT_DELAY_US);
 -	}
 +	return speed;
  }
  
 -/**
 - * ice_sriov_poll_reset_status - poll SRIOV VF reset status
 - * @vf: pointer to VF structure
 +/* The mailbox overflow detection algorithm helps to check if there
 + * is a possibility of a malicious VF transmitting too many MBX messages to the
 + * PF.
 + * 1. The mailbox snapshot structure, ice_mbx_snapshot, is initialized during
 + * driver initialization in ice_init_hw() using ice_mbx_init_snapshot().
 + * The struct ice_mbx_snapshot helps to track and traverse a static window of
 + * messages within the mailbox queue while looking for a malicious VF.
   *
 - * Returns true when reset is successful, else returns false
++<<<<<<< HEAD
 + * 2. When the caller starts processing its mailbox queue in response to an
 + * interrupt, the structure ice_mbx_snapshot is expected to be cleared before
 + * the algorithm can be run for the first time for that interrupt. This can be
 + * done via ice_mbx_reset_snapshot().
++=======
++ * called from the VF to configure the Rx/Tx queues
+  */
 -static bool ice_sriov_poll_reset_status(struct ice_vf *vf)
++static int ice_vc_cfg_qs_msg(struct ice_vf *vf, u8 *msg)
+ {
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_vsi_queue_config_info *qci =
++	    (struct virtchnl_vsi_queue_config_info *)msg;
++	struct virtchnl_queue_pair_info *qpi;
+ 	struct ice_pf *pf = vf->pf;
 -	unsigned int i;
 -	u32 reg;
++	struct ice_vsi *vsi;
++	int i, q_idx;
+ 
 -	for (i = 0; i < 10; i++) {
 -		/* VF reset requires driver to first reset the VF and then
 -		 * poll the status register to make sure that the reset
 -		 * completed successfully.
 -		 */
 -		reg = rd32(&pf->hw, VPGEN_VFRSTAT(vf->vf_id));
 -		if (reg & VPGEN_VFRSTAT_VFRD_M)
 -			return true;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
+ 
 -		/* only sleep if the reset is not done */
 -		usleep_range(10, 20);
++	if (!ice_vc_isvalid_vsi_id(vf, qci->vsi_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
+ 	}
 -	return false;
 -}
+ 
 -/**
 - * ice_sriov_clear_reset_trigger - enable VF to access hardware
 - * @vf: VF to enabled hardware access for
 - */
 -static void ice_sriov_clear_reset_trigger(struct ice_vf *vf)
 -{
 -	struct ice_hw *hw = &vf->pf->hw;
 -	u32 reg;
++	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
+ 
 -	reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
 -	reg &= ~VPGEN_VFRTRIG_VFSWR_M;
 -	wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
 -	ice_flush(hw);
 -}
++	if (qci->num_queue_pairs > ICE_MAX_RSS_QS_PER_VF ||
++	    qci->num_queue_pairs > min_t(u16, vsi->alloc_txq, vsi->alloc_rxq)) {
++		dev_err(ice_pf_to_dev(pf), "VF-%d requesting more than supported number of queues: %d\n",
++			vf->vf_id, min_t(u16, vsi->alloc_txq, vsi->alloc_rxq));
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto error_param;
++	}
+ 
 -/**
 - * ice_sriov_vsi_rebuild - release and rebuild VF's VSI
 - * @vf: VF to release and setup the VSI for
 - *
 - * This is only called when a single VF is being reset (i.e. VFR, VFLR, host VF
 - * configuration change, etc.).
 - */
 -static int ice_sriov_vsi_rebuild(struct ice_vf *vf)
 -{
 -	struct ice_pf *pf = vf->pf;
++	for (i = 0; i < qci->num_queue_pairs; i++) {
++		qpi = &qci->qpair[i];
++		if (qpi->txq.vsi_id != qci->vsi_id ||
++		    qpi->rxq.vsi_id != qci->vsi_id ||
++		    qpi->rxq.queue_id != qpi->txq.queue_id ||
++		    qpi->txq.headwb_enabled ||
++		    !ice_vc_isvalid_ring_len(qpi->txq.ring_len) ||
++		    !ice_vc_isvalid_ring_len(qpi->rxq.ring_len) ||
++		    !ice_vc_isvalid_q_id(vf, qci->vsi_id, qpi->txq.queue_id)) {
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++			goto error_param;
++		}
+ 
 -	ice_vf_vsi_release(vf);
 -	if (!ice_vf_vsi_setup(vf)) {
 -		dev_err(ice_pf_to_dev(pf),
 -			"Failed to release and setup the VF%u's VSI\n",
 -			vf->vf_id);
 -		return -ENOMEM;
++		q_idx = qpi->rxq.queue_id;
++
++		/* make sure selected "q_idx" is in valid range of queues
++		 * for selected "vsi"
++		 */
++		if (q_idx >= vsi->alloc_txq || q_idx >= vsi->alloc_rxq) {
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++			goto error_param;
++		}
++
++		/* copy Tx queue info from VF into VSI */
++		if (qpi->txq.ring_len > 0) {
++			vsi->tx_rings[i]->dma = qpi->txq.dma_ring_addr;
++			vsi->tx_rings[i]->count = qpi->txq.ring_len;
++			if (ice_vsi_cfg_single_txq(vsi, vsi->tx_rings, q_idx)) {
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
++		}
++
++		/* copy Rx queue info from VF into VSI */
++		if (qpi->rxq.ring_len > 0) {
++			u16 max_frame_size = ice_vc_get_max_frame_size(vf);
++
++			vsi->rx_rings[i]->dma = qpi->rxq.dma_ring_addr;
++			vsi->rx_rings[i]->count = qpi->rxq.ring_len;
++
++			if (qpi->rxq.databuffer_size != 0 &&
++			    (qpi->rxq.databuffer_size > ((16 * 1024) - 128) ||
++			     qpi->rxq.databuffer_size < 1024)) {
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
++			vsi->rx_buf_len = qpi->rxq.databuffer_size;
++			vsi->rx_rings[i]->rx_buf_len = vsi->rx_buf_len;
++			if (qpi->rxq.max_pkt_size > max_frame_size ||
++			    qpi->rxq.max_pkt_size < 64) {
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
++
++			vsi->max_frame = qpi->rxq.max_pkt_size;
++			/* add space for the port VLAN since the VF driver is
++			 * not expected to account for it in the MTU
++			 * calculation
++			 */
++			if (ice_vf_is_port_vlan_ena(vf))
++				vsi->max_frame += VLAN_HLEN;
++
++			if (ice_vsi_cfg_single_rxq(vsi, q_idx)) {
++				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++				goto error_param;
++			}
++		}
+ 	}
+ 
 -	return 0;
++error_param:
++	/* send the response to the VF */
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_VSI_QUEUES, v_ret,
++				     NULL, 0);
+ }
+ 
+ /**
 - * ice_sriov_post_vsi_rebuild - tasks to do after the VF's VSI have been rebuilt
 - * @vf: VF to perform tasks on
++ * ice_can_vf_change_mac
++ * @vf: pointer to the VF info
++>>>>>>> 8cf52bec5ca0 (ice: cleanup long lines in ice_sriov.c)
 + *
 + * 3. For every message read by the caller from the MBX Queue, the caller must
 + * call the detection algorithm's entry function ice_mbx_vf_state_handler().
 + * Before every call to ice_mbx_vf_state_handler() the struct ice_mbx_data is
 + * filled as it is required to be passed to the algorithm.
 + *
 + * 4. Every time a message is read from the MBX queue, a VFId is received which
 + * is passed to the state handler. The boolean output is_malvf of the state
 + * handler ice_mbx_vf_state_handler() serves as an indicator to the caller
 + * whether this VF is malicious or not.
 + *
 + * 5. When a VF is identified to be malicious, the caller can send a message
 + * to the system administrator. The caller can invoke ice_mbx_report_malvf()
 + * to help determine if a malicious VF is to be reported or not. This function
 + * requires the caller to maintain a global bitmap to track all malicious VFs
 + * and pass that to ice_mbx_report_malvf() along with the VFID which was identified
 + * to be malicious by ice_mbx_vf_state_handler().
 + *
 + * 6. The global bitmap maintained by PF can be cleared completely if PF is in
 + * reset or the bit corresponding to a VF can be cleared if that VF is in reset.
 + * When a VF is shut down and brought back up, we assume that the new VF
 + * brought up is not malicious and hence report it if found malicious.
 + *
 + * 7. The function ice_mbx_reset_snapshot() is called to reset the information
 + * in ice_mbx_snapshot for every new mailbox interrupt handled.
 + *
 + * 8. The memory allocated for variables in ice_mbx_snapshot is de-allocated
 + * when driver is unloaded.
   */
 -static void ice_sriov_post_vsi_rebuild(struct ice_vf *vf)
 -{
 -	ice_vf_rebuild_host_cfg(vf);
 -	ice_vf_set_initialized(vf);
 -	ice_ena_vf_mappings(vf);
 -	wr32(&vf->pf->hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
 -}
 -
 -static const struct ice_vf_ops ice_sriov_vf_ops = {
 -	.reset_type = ICE_VF_RESET,
 -	.free = ice_sriov_free_vf,
 -	.clear_mbx_register = ice_sriov_clear_mbx_register,
 -	.trigger_reset_register = ice_sriov_trigger_reset_register,
 -	.poll_reset_status = ice_sriov_poll_reset_status,
 -	.clear_reset_trigger = ice_sriov_clear_reset_trigger,
 -	.vsi_rebuild = ice_sriov_vsi_rebuild,
 -	.post_vsi_rebuild = ice_sriov_post_vsi_rebuild,
 -};
 +#define ICE_RQ_DATA_MASK(rq_data) ((rq_data) & PF_MBX_ARQH_ARQH_M)
 +/* Using the highest value for an unsigned 16-bit value 0xFFFF to indicate that
 + * the max messages check must be ignored in the algorithm
 + */
 +#define ICE_IGNORE_MAX_MSG_CNT	0xFFFF
  
  /**
 - * ice_create_vf_entries - Allocate and insert VF entries
 - * @pf: pointer to the PF structure
 - * @num_vfs: the number of VFs to allocate
 + * ice_mbx_traverse - Pass through mailbox snapshot
 + * @hw: pointer to the HW struct
 + * @new_state: new algorithm state
   *
 - * Allocate new VF entries and insert them into the hash table. Set some
 - * basic default fields for initializing the new VFs.
 - *
 - * After this function exits, the hash table will have num_vfs entries
 - * inserted.
 - *
 - * Returns 0 on success or an integer error code on failure.
 + * Traversing the mailbox static snapshot without checking
 + * for malicious VFs.
   */
 -static int ice_create_vf_entries(struct ice_pf *pf, u16 num_vfs)
 +static void
 +ice_mbx_traverse(struct ice_hw *hw,
 +		 enum ice_mbx_snapshot_state *new_state)
  {
 -	struct ice_vfs *vfs = &pf->vfs;
 -	struct ice_vf *vf;
 -	u16 vf_id;
 -	int err;
 +	struct ice_mbx_snap_buffer_data *snap_buf;
 +	u32 num_iterations;
  
 -	lockdep_assert_held(&vfs->table_lock);
 +	snap_buf = &hw->mbx_snapshot.mbx_buf;
  
 -	for (vf_id = 0; vf_id < num_vfs; vf_id++) {
 -		vf = kzalloc(sizeof(*vf), GFP_KERNEL);
 -		if (!vf) {
 -			err = -ENOMEM;
 -			goto err_free_entries;
 -		}
 -		kref_init(&vf->refcnt);
 -
 -		vf->pf = pf;
 -		vf->vf_id = vf_id;
 -
 -		/* set sriov vf ops for VFs created during SRIOV flow */
 -		vf->vf_ops = &ice_sriov_vf_ops;
 +	/* As mailbox buffer is circular, applying a mask
 +	 * on the incremented iteration count.
 +	 */
 +	num_iterations = ICE_RQ_DATA_MASK(++snap_buf->num_iterations);
 +
 +	/* Checking either of the below conditions to exit snapshot traversal:
 +	 * Condition-1: If the number of iterations in the mailbox is equal to
 +	 * the mailbox head which would indicate that we have reached the end
 +	 * of the static snapshot.
 +	 * Condition-2: If the maximum messages serviced in the mailbox for a
 +	 * given interrupt is the highest possible value then there is no need
 +	 * to check if the number of messages processed is equal to it. If not
 +	 * check if the number of messages processed is greater than or equal
 +	 * to the maximum number of mailbox entries serviced in current work item.
 +	 */
 +	if (num_iterations == snap_buf->head ||
 +	    (snap_buf->max_num_msgs_mbx < ICE_IGNORE_MAX_MSG_CNT &&
 +	     ++snap_buf->num_msg_proc >= snap_buf->max_num_msgs_mbx))
 +		*new_state = ICE_MAL_VF_DETECT_STATE_NEW_SNAPSHOT;
 +}
  
 -		vf->vf_sw_id = pf->first_sw;
 -		/* assign default capabilities */
 -		vf->spoofchk = true;
 -		vf->num_vf_qs = pf->vfs.num_qps_per;
 -		ice_vc_set_default_allowlist(vf);
 +/**
 + * ice_mbx_detect_malvf - Detect malicious VF in snapshot
 + * @hw: pointer to the HW struct
 + * @vf_id: relative virtual function ID
 + * @new_state: new algorithm state
 + * @is_malvf: boolean output to indicate if VF is malicious
 + *
 + * This function tracks the number of asynchronous messages
 + * sent per VF and marks the VF as malicious if it exceeds
 + * the permissible number of messages to send.
 + */
 +static int
 +ice_mbx_detect_malvf(struct ice_hw *hw, u16 vf_id,
 +		     enum ice_mbx_snapshot_state *new_state,
 +		     bool *is_malvf)
 +{
 +	struct ice_mbx_snapshot *snap = &hw->mbx_snapshot;
  
 -		/* ctrl_vsi_idx will be set to a valid value only when VF
 -		 * creates its first fdir rule.
 -		 */
 -		ice_vf_ctrl_invalidate_vsi(vf);
 -		ice_vf_fdir_init(vf);
 +	if (vf_id >= snap->mbx_vf.vfcntr_len)
 +		return -EIO;
  
 -		ice_virtchnl_set_dflt_ops(vf);
 +	/* increment the message count in the VF array */
 +	snap->mbx_vf.vf_cntr[vf_id]++;
  
 -		mutex_init(&vf->cfg_lock);
 +	if (snap->mbx_vf.vf_cntr[vf_id] >= ICE_ASYNC_VF_MSG_THRESHOLD)
 +		*is_malvf = true;
  
 -		hash_add_rcu(vfs->table, &vf->entry, vf_id);
 -	}
 +	/* continue to iterate through the mailbox snapshot */
 +	ice_mbx_traverse(hw, new_state);
  
  	return 0;
 -
 -err_free_entries:
 -	ice_free_vf_entries(pf);
 -	return err;
  }
  
  /**
 - * ice_ena_vfs - enable VFs so they are ready to be used
 - * @pf: pointer to the PF structure
 - * @num_vfs: number of VFs to enable
 + * ice_mbx_reset_snapshot - Reset mailbox snapshot structure
 + * @snap: pointer to mailbox snapshot structure in the ice_hw struct
 + *
 + * Reset the mailbox snapshot structure and clear VF counter array.
   */
 -static int ice_ena_vfs(struct ice_pf *pf, u16 num_vfs)
 +static void ice_mbx_reset_snapshot(struct ice_mbx_snapshot *snap)
  {
 -	struct device *dev = ice_pf_to_dev(pf);
 -	struct ice_hw *hw = &pf->hw;
 -	int ret;
 -
 -	/* Disable global interrupt 0 so we don't try to handle the VFLR. */
 -	wr32(hw, GLINT_DYN_CTL(pf->oicr_idx),
 -	     ICE_ITR_NONE << GLINT_DYN_CTL_ITR_INDX_S);
 -	set_bit(ICE_OICR_INTR_DIS, pf->state);
 -	ice_flush(hw);
 +	u32 vfcntr_len;
  
 -	ret = pci_enable_sriov(pf->pdev, num_vfs);
 -	if (ret)
 -		goto err_unroll_intr;
 -
 -	mutex_lock(&pf->vfs.table_lock);
 +	if (!snap || !snap->mbx_vf.vf_cntr)
 +		return;
  
 -	ret = ice_set_per_vf_res(pf, num_vfs);
 -	if (ret) {
 -		dev_err(dev, "Not enough resources for %d VFs, err %d. Try with fewer number of VFs\n",
 -			num_vfs, ret);
 -		goto err_unroll_sriov;
 -	}
 +	/* Clear VF counters. */
 +	vfcntr_len = snap->mbx_vf.vfcntr_len;
 +	if (vfcntr_len)
 +		memset(snap->mbx_vf.vf_cntr, 0,
 +		       (vfcntr_len * sizeof(*snap->mbx_vf.vf_cntr)));
 +
 +	/* Reset mailbox snapshot for a new capture. */
 +	memset(&snap->mbx_buf, 0, sizeof(snap->mbx_buf));
 +	snap->mbx_buf.state = ICE_MAL_VF_DETECT_STATE_NEW_SNAPSHOT;
 +}
 +
 +/**
 + * ice_mbx_vf_state_handler - Handle states of the overflow algorithm
 + * @hw: pointer to the HW struct
 + * @mbx_data: pointer to structure containing mailbox data
 + * @vf_id: relative virtual function (VF) ID
 + * @is_malvf: boolean output to indicate if VF is malicious
 + *
 + * The function serves as an entry point for the malicious VF
 + * detection algorithm by handling the different states and state
 + * transitions of the algorithm:
 + * New snapshot: This state is entered when creating a new static
 + * snapshot. The data from any previous mailbox snapshot is
 + * cleared and a new capture of the mailbox head and tail is
 + * logged. This will be the new static snapshot to detect
 + * asynchronous messages sent by VFs. On capturing the snapshot
 + * and depending on whether the number of pending messages in that
 + * snapshot exceed the watermark value, the state machine enters
 + * traverse or detect states.
 + * Traverse: If pending message count is below watermark then iterate
 + * through the snapshot without any action on VF.
 + * Detect: If pending message count exceeds watermark traverse
 + * the static snapshot and look for a malicious VF.
 + */
 +int
 +ice_mbx_vf_state_handler(struct ice_hw *hw,
 +			 struct ice_mbx_data *mbx_data, u16 vf_id,
 +			 bool *is_malvf)
 +{
 +	struct ice_mbx_snapshot *snap = &hw->mbx_snapshot;
 +	struct ice_mbx_snap_buffer_data *snap_buf;
 +	struct ice_ctl_q_info *cq = &hw->mailboxq;
 +	enum ice_mbx_snapshot_state new_state;
 +	int status = 0;
  
 -	ret = ice_create_vf_entries(pf, num_vfs);
 -	if (ret) {
 -		dev_err(dev, "Failed to allocate VF entries for %d VFs\n",
 -			num_vfs);
 -		goto err_unroll_sriov;
 -	}
 +	if (!is_malvf || !mbx_data)
 +		return -EINVAL;
  
 -	ret = ice_start_vfs(pf);
 -	if (ret) {
 -		dev_err(dev, "Failed to start %d VFs, err %d\n", num_vfs, ret);
 -		ret = -EAGAIN;
 -		goto err_unroll_vf_entries;
 -	}
 -
 -	clear_bit(ICE_VF_DIS, pf->state);
 -
 -	ret = ice_eswitch_configure(pf);
 -	if (ret) {
 -		dev_err(dev, "Failed to configure eswitch, err %d\n", ret);
 -		goto err_unroll_sriov;
 -	}
 -
 -	/* rearm global interrupts */
 -	if (test_and_clear_bit(ICE_OICR_INTR_DIS, pf->state))
 -		ice_irq_dynamic_ena(hw, NULL, NULL);
 +	/* When entering the mailbox state machine assume that the VF
 +	 * is not malicious until detected.
 +	 */
 +	*is_malvf = false;
 +
++<<<<<<< HEAD
 +	 /* Checking if max messages allowed to be processed while servicing current
 +	  * interrupt is not less than the defined AVF message threshold.
 +	  */
 +	if (mbx_data->max_num_msgs_mbx <= ICE_ASYNC_VF_MSG_THRESHOLD)
++=======
++	if (ice_vf_vlan_offload_ena(vf->driver_caps))
++		return vsi->inner_vlan_ops.ena_stripping(vsi, ETH_P_8021Q);
++	else
++		return vsi->inner_vlan_ops.dis_stripping(vsi);
++}
+ 
 -	mutex_unlock(&pf->vfs.table_lock);
++static u16 ice_vc_get_max_vlan_fltrs(struct ice_vf *vf)
++{
++	if (vf->trusted)
++		return VLAN_N_VID;
++	else
++		return ICE_MAX_VLAN_PER_VF;
++}
+ 
 -	return 0;
++/**
++ * ice_vf_outer_vlan_not_allowed - check if outer VLAN can be used
++ * @vf: VF that being checked for
++ *
++ * When the device is in double VLAN mode, check whether or not the outer VLAN
++ * is allowed.
++ */
++static bool ice_vf_outer_vlan_not_allowed(struct ice_vf *vf)
++{
++	if (ice_vf_is_port_vlan_ena(vf))
++		return true;
+ 
 -err_unroll_vf_entries:
 -	ice_free_vf_entries(pf);
 -err_unroll_sriov:
 -	mutex_unlock(&pf->vfs.table_lock);
 -	pci_disable_sriov(pf->pdev);
 -err_unroll_intr:
 -	/* rearm interrupts here */
 -	ice_irq_dynamic_ena(hw, NULL, NULL);
 -	clear_bit(ICE_OICR_INTR_DIS, pf->state);
 -	return ret;
++	return false;
+ }
+ 
+ /**
 - * ice_pci_sriov_ena - Enable or change number of VFs
 - * @pf: pointer to the PF structure
 - * @num_vfs: number of VFs to allocate
++ * ice_vc_set_dvm_caps - set VLAN capabilities when the device is in DVM
++ * @vf: VF that capabilities are being set for
++ * @caps: VLAN capabilities to populate
+  *
 - * Returns 0 on success and negative on failure
++ * Determine VLAN capabilities support based on whether a port VLAN is
++ * configured. If a port VLAN is configured then the VF should use the inner
++ * filtering/offload capabilities since the port VLAN is using the outer VLAN
++ * capabilies.
+  */
 -static int ice_pci_sriov_ena(struct ice_pf *pf, int num_vfs)
++static void
++ice_vc_set_dvm_caps(struct ice_vf *vf, struct virtchnl_vlan_caps *caps)
+ {
 -	int pre_existing_vfs = pci_num_vf(pf->pdev);
 -	struct device *dev = ice_pf_to_dev(pf);
 -	int err;
++	struct virtchnl_vlan_supported_caps *supported_caps;
+ 
 -	if (pre_existing_vfs && pre_existing_vfs != num_vfs)
 -		ice_free_vfs(pf);
 -	else if (pre_existing_vfs && pre_existing_vfs == num_vfs)
 -		return 0;
++	if (ice_vf_outer_vlan_not_allowed(vf)) {
++		/* until support for inner VLAN filtering is added when a port
++		 * VLAN is configured, only support software offloaded inner
++		 * VLANs when a port VLAN is confgured in DVM
++		 */
++		supported_caps = &caps->filtering.filtering_support;
++		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
+ 
 -	if (num_vfs > pf->vfs.num_supported) {
 -		dev_err(dev, "Can't enable %d VFs, max VFs supported is %d\n",
 -			num_vfs, pf->vfs.num_supported);
 -		return -EOPNOTSUPP;
 -	}
++		supported_caps = &caps->offloads.stripping_support;
++		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
+ 
 -	dev_info(dev, "Enabling %d VFs\n", num_vfs);
 -	err = ice_ena_vfs(pf, num_vfs);
 -	if (err) {
 -		dev_err(dev, "Failed to enable SR-IOV: %d\n", err);
 -		return err;
 -	}
++		supported_caps = &caps->offloads.insertion_support;
++		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
+ 
 -	set_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
 -	return 0;
 -}
++		caps->offloads.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
++		caps->offloads.ethertype_match =
++			VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION;
++	} else {
++		supported_caps = &caps->filtering.filtering_support;
++		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
++		supported_caps->outer = VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_ETHERTYPE_88A8 |
++					VIRTCHNL_VLAN_ETHERTYPE_9100 |
++					VIRTCHNL_VLAN_ETHERTYPE_AND;
++		caps->filtering.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100 |
++						 VIRTCHNL_VLAN_ETHERTYPE_88A8 |
++						 VIRTCHNL_VLAN_ETHERTYPE_9100;
+ 
 -/**
 - * ice_check_sriov_allowed - check if SR-IOV is allowed based on various checks
 - * @pf: PF to enabled SR-IOV on
 - */
 -static int ice_check_sriov_allowed(struct ice_pf *pf)
 -{
 -	struct device *dev = ice_pf_to_dev(pf);
++		supported_caps = &caps->offloads.stripping_support;
++		supported_caps->inner = VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
++		supported_caps->outer = VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_ETHERTYPE_88A8 |
++					VIRTCHNL_VLAN_ETHERTYPE_9100 |
++					VIRTCHNL_VLAN_ETHERTYPE_XOR |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2_2;
+ 
 -	if (!test_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags)) {
 -		dev_err(dev, "This device is not capable of SR-IOV\n");
 -		return -EOPNOTSUPP;
 -	}
++		supported_caps = &caps->offloads.insertion_support;
++		supported_caps->inner = VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
++		supported_caps->outer = VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_ETHERTYPE_88A8 |
++					VIRTCHNL_VLAN_ETHERTYPE_9100 |
++					VIRTCHNL_VLAN_ETHERTYPE_XOR |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2;
+ 
 -	if (ice_is_safe_mode(pf)) {
 -		dev_err(dev, "SR-IOV cannot be configured - Device is in Safe Mode\n");
 -		return -EOPNOTSUPP;
 -	}
++		caps->offloads.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
+ 
 -	if (!ice_pf_state_is_nominal(pf)) {
 -		dev_err(dev, "Cannot enable SR-IOV, device not ready\n");
 -		return -EBUSY;
++		caps->offloads.ethertype_match =
++			VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION;
+ 	}
+ 
 -	return 0;
++	caps->filtering.max_filters = ice_vc_get_max_vlan_fltrs(vf);
+ }
+ 
+ /**
 - * ice_sriov_configure - Enable or change number of VFs via sysfs
 - * @pdev: pointer to a pci_dev structure
 - * @num_vfs: number of VFs to allocate or 0 to free VFs
++ * ice_vc_set_svm_caps - set VLAN capabilities when the device is in SVM
++ * @vf: VF that capabilities are being set for
++ * @caps: VLAN capabilities to populate
+  *
 - * This function is called when the user updates the number of VFs in sysfs. On
 - * success return whatever num_vfs was set to by the caller. Return negative on
 - * failure.
++ * Determine VLAN capabilities support based on whether a port VLAN is
++ * configured. If a port VLAN is configured then the VF does not have any VLAN
++ * filtering or offload capabilities since the port VLAN is using the inner VLAN
++ * capabilities in single VLAN mode (SVM). Otherwise allow the VF to use inner
++ * VLAN fitlering and offload capabilities.
+  */
 -int ice_sriov_configure(struct pci_dev *pdev, int num_vfs)
++static void
++ice_vc_set_svm_caps(struct ice_vf *vf, struct virtchnl_vlan_caps *caps)
+ {
 -	struct ice_pf *pf = pci_get_drvdata(pdev);
 -	struct device *dev = ice_pf_to_dev(pf);
 -	int err;
++	struct virtchnl_vlan_supported_caps *supported_caps;
+ 
 -	err = ice_check_sriov_allowed(pf);
 -	if (err)
 -		return err;
++	if (ice_vf_is_port_vlan_ena(vf)) {
++		supported_caps = &caps->filtering.filtering_support;
++		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
+ 
 -	if (!num_vfs) {
 -		if (!pci_vfs_assigned(pdev)) {
 -			ice_mbx_deinit_snapshot(&pf->hw);
 -			ice_free_vfs(pf);
 -			if (pf->lag)
 -				ice_enable_lag(pf->lag);
 -			return 0;
 -		}
++		supported_caps = &caps->offloads.stripping_support;
++		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
+ 
 -		dev_err(dev, "can't free VFs because some are assigned to VMs.\n");
 -		return -EBUSY;
 -	}
++		supported_caps = &caps->offloads.insertion_support;
++		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
+ 
 -	err = ice_mbx_init_snapshot(&pf->hw, num_vfs);
 -	if (err)
 -		return err;
++		caps->offloads.ethertype_init = VIRTCHNL_VLAN_UNSUPPORTED;
++		caps->offloads.ethertype_match = VIRTCHNL_VLAN_UNSUPPORTED;
++		caps->filtering.max_filters = 0;
++	} else {
++		supported_caps = &caps->filtering.filtering_support;
++		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
++		caps->filtering.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
+ 
 -	err = ice_pci_sriov_ena(pf, num_vfs);
 -	if (err) {
 -		ice_mbx_deinit_snapshot(&pf->hw);
 -		return err;
 -	}
++		supported_caps = &caps->offloads.stripping_support;
++		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
++
++		supported_caps = &caps->offloads.insertion_support;
++		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
++					VIRTCHNL_VLAN_TOGGLE |
++					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
++		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
+ 
 -	if (pf->lag)
 -		ice_disable_lag(pf->lag);
 -	return num_vfs;
++		caps->offloads.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
++		caps->offloads.ethertype_match =
++			VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION;
++		caps->filtering.max_filters = ice_vc_get_max_vlan_fltrs(vf);
++	}
+ }
+ 
+ /**
 - * ice_process_vflr_event - Free VF resources via IRQ calls
 - * @pf: pointer to the PF structure
++ * ice_vc_get_offload_vlan_v2_caps - determine VF's VLAN capabilities
++ * @vf: VF to determine VLAN capabilities for
++ *
++ * This will only be called if the VF and PF successfully negotiated
++ * VIRTCHNL_VF_OFFLOAD_VLAN_V2.
+  *
 - * called from the VFLR IRQ handler to
 - * free up VF resources and state variables
++ * Set VLAN capabilities based on the current VLAN mode and whether a port VLAN
++ * is configured or not.
+  */
 -void ice_process_vflr_event(struct ice_pf *pf)
++static int ice_vc_get_offload_vlan_v2_caps(struct ice_vf *vf)
+ {
 -	struct ice_hw *hw = &pf->hw;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -	u32 reg;
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_vlan_caps *caps = NULL;
++	int err, len = 0;
+ 
 -	if (!test_and_clear_bit(ICE_VFLR_EVENT_PENDING, pf->state) ||
 -	    !ice_has_vfs(pf))
 -		return;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -	mutex_lock(&pf->vfs.table_lock);
 -	ice_for_each_vf(pf, bkt, vf) {
 -		u32 reg_idx, bit_idx;
 -
 -		reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
 -		bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
 -		/* read GLGEN_VFLRSTAT register to find out the flr VFs */
 -		reg = rd32(hw, GLGEN_VFLRSTAT(reg_idx));
 -		if (reg & BIT(bit_idx))
 -			/* GLGEN_VFLRSTAT bit will be cleared in ice_reset_vf */
 -			ice_reset_vf(vf, ICE_VF_RESET_VFLR | ICE_VF_RESET_LOCK);
++	caps = kzalloc(sizeof(*caps), GFP_KERNEL);
++	if (!caps) {
++		v_ret = VIRTCHNL_STATUS_ERR_NO_MEMORY;
++		goto out;
+ 	}
 -	mutex_unlock(&pf->vfs.table_lock);
++	len = sizeof(*caps);
++
++	if (ice_is_dvm_ena(&vf->pf->hw))
++		ice_vc_set_dvm_caps(vf, caps);
++	else
++		ice_vc_set_svm_caps(vf, caps);
++
++	/* store negotiated caps to prevent invalid VF messages */
++	memcpy(&vf->vlan_v2_caps, caps, sizeof(*caps));
++
++out:
++	err = ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS,
++				    v_ret, (u8 *)caps, len);
++	kfree(caps);
++	return err;
+ }
+ 
+ /**
 - * ice_get_vf_from_pfq - get the VF who owns the PF space queue passed in
 - * @pf: PF used to index all VFs
 - * @pfq: queue index relative to the PF's function space
 - *
 - * If no VF is found who owns the pfq then return NULL, otherwise return a
 - * pointer to the VF who owns the pfq
++ * ice_vc_validate_vlan_tpid - validate VLAN TPID
++ * @filtering_caps: negotiated/supported VLAN filtering capabilities
++ * @tpid: VLAN TPID used for validation
+  *
 - * If this function returns non-NULL, it acquires a reference count of the VF
 - * structure. The caller is responsible for calling ice_put_vf() to drop this
 - * reference.
++ * Convert the VLAN TPID to a VIRTCHNL_VLAN_ETHERTYPE_* and then compare against
++ * the negotiated/supported filtering caps to see if the VLAN TPID is valid.
+  */
 -static struct ice_vf *ice_get_vf_from_pfq(struct ice_pf *pf, u16 pfq)
++static bool ice_vc_validate_vlan_tpid(u16 filtering_caps, u16 tpid)
+ {
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	rcu_read_lock();
 -	ice_for_each_vf_rcu(pf, bkt, vf) {
 -		struct ice_vsi *vsi;
 -		u16 rxq_idx;
 -
 -		vsi = ice_get_vf_vsi(vf);
 -
 -		ice_for_each_rxq(vsi, rxq_idx)
 -			if (vsi->rxq_map[rxq_idx] == pfq) {
 -				struct ice_vf *found;
 -
 -				if (kref_get_unless_zero(&vf->refcnt))
 -					found = vf;
 -				else
 -					found = NULL;
 -				rcu_read_unlock();
 -				return found;
 -			}
++	enum virtchnl_vlan_support vlan_ethertype = VIRTCHNL_VLAN_UNSUPPORTED;
++
++	switch (tpid) {
++	case ETH_P_8021Q:
++		vlan_ethertype = VIRTCHNL_VLAN_ETHERTYPE_8100;
++		break;
++	case ETH_P_8021AD:
++		vlan_ethertype = VIRTCHNL_VLAN_ETHERTYPE_88A8;
++		break;
++	case ETH_P_QINQ1:
++		vlan_ethertype = VIRTCHNL_VLAN_ETHERTYPE_9100;
++		break;
+ 	}
 -	rcu_read_unlock();
+ 
 -	return NULL;
 -}
++	if (!(filtering_caps & vlan_ethertype))
++		return false;
+ 
 -/**
 - * ice_globalq_to_pfq - convert from global queue index to PF space queue index
 - * @pf: PF used for conversion
 - * @globalq: global queue index used to convert to PF space queue index
 - */
 -static u32 ice_globalq_to_pfq(struct ice_pf *pf, u32 globalq)
 -{
 -	return globalq - pf->hw.func_caps.common_cap.rxq_first_id;
++	return true;
+ }
+ 
+ /**
 - * ice_vf_lan_overflow_event - handle LAN overflow event for a VF
 - * @pf: PF that the LAN overflow event happened on
 - * @event: structure holding the event information for the LAN overflow event
++ * ice_vc_is_valid_vlan - validate the virtchnl_vlan
++ * @vc_vlan: virtchnl_vlan to validate
+  *
 - * Determine if the LAN overflow event was caused by a VF queue. If it was not
 - * caused by a VF, do nothing. If a VF caused this LAN overflow event trigger a
 - * reset on the offending VF.
++ * If the VLAN TCI and VLAN TPID are 0, then this filter is invalid, so return
++ * false. Otherwise return true.
+  */
 -void
 -ice_vf_lan_overflow_event(struct ice_pf *pf, struct ice_rq_event_info *event)
++static bool ice_vc_is_valid_vlan(struct virtchnl_vlan *vc_vlan)
+ {
 -	u32 gldcb_rtctq, queue;
 -	struct ice_vf *vf;
 -
 -	gldcb_rtctq = le32_to_cpu(event->desc.params.lan_overflow.prtdcb_ruptq);
 -	dev_dbg(ice_pf_to_dev(pf), "GLDCB_RTCTQ: 0x%08x\n", gldcb_rtctq);
 -
 -	/* event returns device global Rx queue number */
 -	queue = (gldcb_rtctq & GLDCB_RTCTQ_RXQNUM_M) >>
 -		GLDCB_RTCTQ_RXQNUM_S;
 -
 -	vf = ice_get_vf_from_pfq(pf, ice_globalq_to_pfq(pf, queue));
 -	if (!vf)
 -		return;
++	if (!vc_vlan->tci || !vc_vlan->tpid)
++		return false;
+ 
 -	ice_reset_vf(vf, ICE_VF_RESET_NOTIFY | ICE_VF_RESET_LOCK);
 -	ice_put_vf(vf);
++	return true;
+ }
+ 
+ /**
 - * ice_vc_send_msg_to_vf - Send message to VF
 - * @vf: pointer to the VF info
 - * @v_opcode: virtual channel opcode
 - * @v_retval: virtual channel return value
 - * @msg: pointer to the msg buffer
 - * @msglen: msg length
++ * ice_vc_validate_vlan_filter_list - validate the filter list from the VF
++ * @vfc: negotiated/supported VLAN filtering capabilities
++ * @vfl: VLAN filter list from VF to validate
+  *
 - * send msg to VF
++ * Validate all of the filters in the VLAN filter list from the VF. If any of
++ * the checks fail then return false. Otherwise return true.
+  */
 -int
 -ice_vc_send_msg_to_vf(struct ice_vf *vf, u32 v_opcode,
 -		      enum virtchnl_status_code v_retval, u8 *msg, u16 msglen)
++static bool
++ice_vc_validate_vlan_filter_list(struct virtchnl_vlan_filtering_caps *vfc,
++				 struct virtchnl_vlan_filter_list_v2 *vfl)
+ {
 -	struct device *dev;
 -	struct ice_pf *pf;
 -	int aq_ret;
++	u16 i;
+ 
 -	pf = vf->pf;
 -	dev = ice_pf_to_dev(pf);
 -
 -	aq_ret = ice_aq_send_msg_to_vf(&pf->hw, vf->vf_id, v_opcode, v_retval,
 -				       msg, msglen, NULL);
 -	if (aq_ret && pf->hw.mailboxq.sq_last_status != ICE_AQ_RC_ENOSYS) {
 -		dev_info(dev, "Unable to send the message to VF %d ret %d aq_err %s\n",
 -			 vf->vf_id, aq_ret,
 -			 ice_aq_str(pf->hw.mailboxq.sq_last_status));
 -		return -EIO;
 -	}
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vc_get_ver_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to request the API version used by the PF
 - */
 -static int ice_vc_get_ver_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	struct virtchnl_version_info info = {
 -		VIRTCHNL_VERSION_MAJOR, VIRTCHNL_VERSION_MINOR
 -	};
 -
 -	vf->vf_ver = *(struct virtchnl_version_info *)msg;
 -	/* VFs running the 1.0 API expect to get 1.0 back or they will cry. */
 -	if (VF_IS_V10(&vf->vf_ver))
 -		info.minor = VIRTCHNL_VERSION_MINOR_NO_VF_CAPS;
 -
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_VERSION,
 -				     VIRTCHNL_STATUS_SUCCESS, (u8 *)&info,
 -				     sizeof(struct virtchnl_version_info));
 -}
 -
 -/**
 - * ice_vc_get_max_frame_size - get max frame size allowed for VF
 - * @vf: VF used to determine max frame size
 - *
 - * Max frame size is determined based on the current port's max frame size and
 - * whether a port VLAN is configured on this VF. The VF is not aware whether
 - * it's in a port VLAN so the PF needs to account for this in max frame size
 - * checks and sending the max frame size to the VF.
 - */
 -static u16 ice_vc_get_max_frame_size(struct ice_vf *vf)
 -{
 -	struct ice_port_info *pi = ice_vf_get_port_info(vf);
 -	u16 max_frame_size;
 -
 -	max_frame_size = pi->phy.link_info.max_frame_size;
 -
 -	if (ice_vf_is_port_vlan_ena(vf))
 -		max_frame_size -= VLAN_HLEN;
 -
 -	return max_frame_size;
 -}
 -
 -/**
 - * ice_vc_get_vf_res_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to request its resources
 - */
 -static int ice_vc_get_vf_res_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vf_resource *vfres = NULL;
 -	struct ice_pf *pf = vf->pf;
 -	struct ice_vsi *vsi;
 -	int len = 0;
 -	int ret;
 -
 -	if (ice_check_vf_init(pf, vf)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto err;
 -	}
++	if (!vfl->num_elements)
++		return false;
+ 
 -	len = sizeof(struct virtchnl_vf_resource);
++	for (i = 0; i < vfl->num_elements; i++) {
++		struct virtchnl_vlan_supported_caps *filtering_support =
++			&vfc->filtering_support;
++		struct virtchnl_vlan_filter *vlan_fltr = &vfl->filters[i];
++		struct virtchnl_vlan *outer = &vlan_fltr->outer;
++		struct virtchnl_vlan *inner = &vlan_fltr->inner;
+ 
 -	vfres = kzalloc(len, GFP_KERNEL);
 -	if (!vfres) {
 -		v_ret = VIRTCHNL_STATUS_ERR_NO_MEMORY;
 -		len = 0;
 -		goto err;
 -	}
 -	if (VF_IS_V11(&vf->vf_ver))
 -		vf->driver_caps = *(u32 *)msg;
 -	else
 -		vf->driver_caps = VIRTCHNL_VF_OFFLOAD_L2 |
 -				  VIRTCHNL_VF_OFFLOAD_RSS_REG |
 -				  VIRTCHNL_VF_OFFLOAD_VLAN;
++		if ((ice_vc_is_valid_vlan(outer) &&
++		     filtering_support->outer == VIRTCHNL_VLAN_UNSUPPORTED) ||
++		    (ice_vc_is_valid_vlan(inner) &&
++		     filtering_support->inner == VIRTCHNL_VLAN_UNSUPPORTED))
++			return false;
+ 
 -	vfres->vf_cap_flags = VIRTCHNL_VF_OFFLOAD_L2;
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto err;
 -	}
++		if ((outer->tci_mask &&
++		     !(filtering_support->outer & VIRTCHNL_VLAN_FILTER_MASK)) ||
++		    (inner->tci_mask &&
++		     !(filtering_support->inner & VIRTCHNL_VLAN_FILTER_MASK)))
++			return false;
+ 
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_VLAN_V2) {
 -		/* VLAN offloads based on current device configuration */
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_VLAN_V2;
 -	} else if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_VLAN) {
 -		/* allow VF to negotiate VIRTCHNL_VF_OFFLOAD explicitly for
 -		 * these two conditions, which amounts to guest VLAN filtering
 -		 * and offloads being based on the inner VLAN or the
 -		 * inner/single VLAN respectively and don't allow VF to
 -		 * negotiate VIRTCHNL_VF_OFFLOAD in any other cases
 -		 */
 -		if (ice_is_dvm_ena(&pf->hw) && ice_vf_is_port_vlan_ena(vf)) {
 -			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_VLAN;
 -		} else if (!ice_is_dvm_ena(&pf->hw) &&
 -			   !ice_vf_is_port_vlan_ena(vf)) {
 -			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_VLAN;
 -			/* configure backward compatible support for VFs that
 -			 * only support VIRTCHNL_VF_OFFLOAD_VLAN, the PF is
 -			 * configured in SVM, and no port VLAN is configured
 -			 */
 -			ice_vf_vsi_cfg_svm_legacy_vlan_mode(vsi);
 -		} else if (ice_is_dvm_ena(&pf->hw)) {
 -			/* configure software offloaded VLAN support when DVM
 -			 * is enabled, but no port VLAN is enabled
 -			 */
 -			ice_vf_vsi_cfg_dvm_legacy_vlan_mode(vsi);
 -		}
 -	}
++		if (((outer->tci & VLAN_PRIO_MASK) &&
++		     !(filtering_support->outer & VIRTCHNL_VLAN_PRIO)) ||
++		    ((inner->tci & VLAN_PRIO_MASK) &&
++		     !(filtering_support->inner & VIRTCHNL_VLAN_PRIO)))
++			return false;
+ 
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_PF) {
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_PF;
 -	} else {
 -		if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_AQ)
 -			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_AQ;
 -		else
 -			vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_REG;
++		if ((ice_vc_is_valid_vlan(outer) &&
++		     !ice_vc_validate_vlan_tpid(filtering_support->outer,
++						outer->tpid)) ||
++		    (ice_vc_is_valid_vlan(inner) &&
++		     !ice_vc_validate_vlan_tpid(filtering_support->inner,
++						inner->tpid)))
++			return false;
+ 	}
+ 
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_FDIR_PF)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_FDIR_PF;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RSS_PCTYPE_V2)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RSS_PCTYPE_V2;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_ENCAP)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_ENCAP;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_ENCAP_CSUM)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_ENCAP_CSUM;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_RX_POLLING)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_RX_POLLING;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_WB_ON_ITR)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_WB_ON_ITR;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_REQ_QUEUES)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_REQ_QUEUES;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_CAP_ADV_LINK_SPEED)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_CAP_ADV_LINK_SPEED;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_ADV_RSS_PF)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_ADV_RSS_PF;
 -
 -	if (vf->driver_caps & VIRTCHNL_VF_OFFLOAD_USO)
 -		vfres->vf_cap_flags |= VIRTCHNL_VF_OFFLOAD_USO;
 -
 -	vfres->num_vsis = 1;
 -	/* Tx and Rx queue are equal for VF */
 -	vfres->num_queue_pairs = vsi->num_txq;
 -	vfres->max_vectors = pf->vfs.num_msix_per;
 -	vfres->rss_key_size = ICE_VSIQF_HKEY_ARRAY_SIZE;
 -	vfres->rss_lut_size = ICE_VSIQF_HLUT_ARRAY_SIZE;
 -	vfres->max_mtu = ice_vc_get_max_frame_size(vf);
 -
 -	vfres->vsi_res[0].vsi_id = vf->lan_vsi_num;
 -	vfres->vsi_res[0].vsi_type = VIRTCHNL_VSI_SRIOV;
 -	vfres->vsi_res[0].num_queue_pairs = vsi->num_txq;
 -	ether_addr_copy(vfres->vsi_res[0].default_mac_addr,
 -			vf->hw_lan_addr.addr);
 -
 -	/* match guest capabilities */
 -	vf->driver_caps = vfres->vf_cap_flags;
 -
 -	ice_vc_set_caps_allowlist(vf);
 -	ice_vc_set_working_allowlist(vf);
 -
 -	set_bit(ICE_VF_STATE_ACTIVE, vf->vf_states);
 -
 -err:
 -	/* send the response back to the VF */
 -	ret = ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_GET_VF_RESOURCES, v_ret,
 -				    (u8 *)vfres, len);
 -
 -	kfree(vfres);
 -	return ret;
 -}
 -
 -/**
 - * ice_vc_reset_vf_msg
 - * @vf: pointer to the VF info
 - *
 - * called from the VF to reset itself,
 - * unlike other virtchnl messages, PF driver
 - * doesn't send the response back to the VF
 - */
 -static void ice_vc_reset_vf_msg(struct ice_vf *vf)
 -{
 -	if (test_bit(ICE_VF_STATE_INIT, vf->vf_states))
 -		ice_reset_vf(vf, 0);
++	return true;
+ }
+ 
+ /**
 - * ice_find_vsi_from_id
 - * @pf: the PF structure to search for the VSI
 - * @id: ID of the VSI it is searching for
 - *
 - * searches for the VSI with the given ID
++ * ice_vc_to_vlan - transform from struct virtchnl_vlan to struct ice_vlan
++ * @vc_vlan: struct virtchnl_vlan to transform
+  */
 -static struct ice_vsi *ice_find_vsi_from_id(struct ice_pf *pf, u16 id)
++static struct ice_vlan ice_vc_to_vlan(struct virtchnl_vlan *vc_vlan)
+ {
 -	int i;
++	struct ice_vlan vlan = { 0 };
+ 
 -	ice_for_each_vsi(pf, i)
 -		if (pf->vsi[i] && pf->vsi[i]->vsi_num == id)
 -			return pf->vsi[i];
++	vlan.prio = (vc_vlan->tci & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
++	vlan.vid = vc_vlan->tci & VLAN_VID_MASK;
++	vlan.tpid = vc_vlan->tpid;
+ 
 -	return NULL;
++	return vlan;
+ }
+ 
+ /**
 - * ice_vc_isvalid_vsi_id
 - * @vf: pointer to the VF info
 - * @vsi_id: VF relative VSI ID
 - *
 - * check for the valid VSI ID
++ * ice_vc_vlan_action - action to perform on the virthcnl_vlan
++ * @vsi: VF's VSI used to perform the action
++ * @vlan_action: function to perform the action with (i.e. add/del)
++ * @vlan: VLAN filter to perform the action with
+  */
 -bool ice_vc_isvalid_vsi_id(struct ice_vf *vf, u16 vsi_id)
++static int
++ice_vc_vlan_action(struct ice_vsi *vsi,
++		   int (*vlan_action)(struct ice_vsi *, struct ice_vlan *),
++		   struct ice_vlan *vlan)
+ {
 -	struct ice_pf *pf = vf->pf;
 -	struct ice_vsi *vsi;
++	int err;
+ 
 -	vsi = ice_find_vsi_from_id(pf, vsi_id);
++	err = vlan_action(vsi, vlan);
++	if (err)
++		return err;
+ 
 -	return (vsi && (vsi->vf == vf));
++	return 0;
+ }
+ 
+ /**
 - * ice_vc_isvalid_q_id
 - * @vf: pointer to the VF info
 - * @vsi_id: VSI ID
 - * @qid: VSI relative queue ID
 - *
 - * check for the valid queue ID
++ * ice_vc_del_vlans - delete VLAN(s) from the virtchnl filter list
++ * @vf: VF used to delete the VLAN(s)
++ * @vsi: VF's VSI used to delete the VLAN(s)
++ * @vfl: virthchnl filter list used to delete the filters
+  */
 -static bool ice_vc_isvalid_q_id(struct ice_vf *vf, u16 vsi_id, u8 qid)
++static int
++ice_vc_del_vlans(struct ice_vf *vf, struct ice_vsi *vsi,
++		 struct virtchnl_vlan_filter_list_v2 *vfl)
+ {
 -	struct ice_vsi *vsi = ice_find_vsi_from_id(vf->pf, vsi_id);
 -	/* allocated Tx and Rx queues should be always equal for VF VSI */
 -	return (vsi && (qid < vsi->alloc_txq));
 -}
++	bool vlan_promisc = ice_is_vlan_promisc_allowed(vf);
++	int err;
++	u16 i;
+ 
 -/**
 - * ice_vc_isvalid_ring_len
 - * @ring_len: length of ring
 - *
 - * check for the valid ring count, should be multiple of ICE_REQ_DESC_MULTIPLE
 - * or zero
 - */
 -static bool ice_vc_isvalid_ring_len(u16 ring_len)
 -{
 -	return ring_len == 0 ||
 -	       (ring_len >= ICE_MIN_NUM_DESC &&
 -		ring_len <= ICE_MAX_NUM_DESC &&
 -		!(ring_len % ICE_REQ_DESC_MULTIPLE));
 -}
++	for (i = 0; i < vfl->num_elements; i++) {
++		struct virtchnl_vlan_filter *vlan_fltr = &vfl->filters[i];
++		struct virtchnl_vlan *vc_vlan;
+ 
 -/**
 - * ice_vc_validate_pattern
 - * @vf: pointer to the VF info
 - * @proto: virtchnl protocol headers
 - *
 - * validate the pattern is supported or not.
 - *
 - * Return: true on success, false on error.
 - */
 -bool
 -ice_vc_validate_pattern(struct ice_vf *vf, struct virtchnl_proto_hdrs *proto)
 -{
 -	bool is_ipv4 = false;
 -	bool is_ipv6 = false;
 -	bool is_udp = false;
 -	u16 ptype = -1;
 -	int i = 0;
 -
 -	while (i < proto->count &&
 -	       proto->proto_hdr[i].type != VIRTCHNL_PROTO_HDR_NONE) {
 -		switch (proto->proto_hdr[i].type) {
 -		case VIRTCHNL_PROTO_HDR_ETH:
 -			ptype = ICE_PTYPE_MAC_PAY;
 -			break;
 -		case VIRTCHNL_PROTO_HDR_IPV4:
 -			ptype = ICE_PTYPE_IPV4_PAY;
 -			is_ipv4 = true;
 -			break;
 -		case VIRTCHNL_PROTO_HDR_IPV6:
 -			ptype = ICE_PTYPE_IPV6_PAY;
 -			is_ipv6 = true;
 -			break;
 -		case VIRTCHNL_PROTO_HDR_UDP:
 -			if (is_ipv4)
 -				ptype = ICE_PTYPE_IPV4_UDP_PAY;
 -			else if (is_ipv6)
 -				ptype = ICE_PTYPE_IPV6_UDP_PAY;
 -			is_udp = true;
 -			break;
 -		case VIRTCHNL_PROTO_HDR_TCP:
 -			if (is_ipv4)
 -				ptype = ICE_PTYPE_IPV4_TCP_PAY;
 -			else if (is_ipv6)
 -				ptype = ICE_PTYPE_IPV6_TCP_PAY;
 -			break;
 -		case VIRTCHNL_PROTO_HDR_SCTP:
 -			if (is_ipv4)
 -				ptype = ICE_PTYPE_IPV4_SCTP_PAY;
 -			else if (is_ipv6)
 -				ptype = ICE_PTYPE_IPV6_SCTP_PAY;
 -			break;
 -		case VIRTCHNL_PROTO_HDR_GTPU_IP:
 -		case VIRTCHNL_PROTO_HDR_GTPU_EH:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_GTPU;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_GTPU;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_L2TPV3:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_L2TPV3;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_L2TPV3;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_ESP:
 -			if (is_ipv4)
 -				ptype = is_udp ? ICE_MAC_IPV4_NAT_T_ESP :
 -						ICE_MAC_IPV4_ESP;
 -			else if (is_ipv6)
 -				ptype = is_udp ? ICE_MAC_IPV6_NAT_T_ESP :
 -						ICE_MAC_IPV6_ESP;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_AH:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_AH;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_AH;
 -			goto out;
 -		case VIRTCHNL_PROTO_HDR_PFCP:
 -			if (is_ipv4)
 -				ptype = ICE_MAC_IPV4_PFCP_SESSION;
 -			else if (is_ipv6)
 -				ptype = ICE_MAC_IPV6_PFCP_SESSION;
 -			goto out;
 -		default:
 -			break;
 -		}
 -		i++;
 -	}
++		vc_vlan = &vlan_fltr->outer;
++		if (ice_vc_is_valid_vlan(vc_vlan)) {
++			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
+ 
 -out:
 -	return ice_hw_ptype_ena(&vf->pf->hw, ptype);
 -}
++			err = ice_vc_vlan_action(vsi,
++						 vsi->outer_vlan_ops.del_vlan,
++						 &vlan);
++			if (err)
++				return err;
+ 
 -/**
 - * ice_vc_parse_rss_cfg - parses hash fields and headers from
 - * a specific virtchnl RSS cfg
 - * @hw: pointer to the hardware
 - * @rss_cfg: pointer to the virtchnl RSS cfg
 - * @addl_hdrs: pointer to the protocol header fields (ICE_FLOW_SEG_HDR_*)
 - * to configure
 - * @hash_flds: pointer to the hash bit fields (ICE_FLOW_HASH_*) to configure
 - *
 - * Return true if all the protocol header and hash fields in the RSS cfg could
 - * be parsed, else return false
 - *
 - * This function parses the virtchnl RSS cfg to be the intended
 - * hash fields and the intended header for RSS configuration
 - */
 -static bool
 -ice_vc_parse_rss_cfg(struct ice_hw *hw, struct virtchnl_rss_cfg *rss_cfg,
 -		     u32 *addl_hdrs, u64 *hash_flds)
 -{
 -	const struct ice_vc_hash_field_match_type *hf_list;
 -	const struct ice_vc_hdr_match_type *hdr_list;
 -	int i, hf_list_len, hdr_list_len;
 -
 -	hf_list = ice_vc_hash_field_list;
 -	hf_list_len = ARRAY_SIZE(ice_vc_hash_field_list);
 -	hdr_list = ice_vc_hdr_list;
 -	hdr_list_len = ARRAY_SIZE(ice_vc_hdr_list);
 -
 -	for (i = 0; i < rss_cfg->proto_hdrs.count; i++) {
 -		struct virtchnl_proto_hdr *proto_hdr =
 -					&rss_cfg->proto_hdrs.proto_hdr[i];
 -		bool hdr_found = false;
 -		int j;
 -
 -		/* Find matched ice headers according to virtchnl headers. */
 -		for (j = 0; j < hdr_list_len; j++) {
 -			struct ice_vc_hdr_match_type hdr_map = hdr_list[j];
 -
 -			if (proto_hdr->type == hdr_map.vc_hdr) {
 -				*addl_hdrs |= hdr_map.ice_hdr;
 -				hdr_found = true;
 -			}
++			if (vlan_promisc)
++				ice_vf_dis_vlan_promisc(vsi, &vlan);
+ 		}
+ 
 -		if (!hdr_found)
 -			return false;
++		vc_vlan = &vlan_fltr->inner;
++		if (ice_vc_is_valid_vlan(vc_vlan)) {
++			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
+ 
 -		/* Find matched ice hash fields according to
 -		 * virtchnl hash fields.
 -		 */
 -		for (j = 0; j < hf_list_len; j++) {
 -			struct ice_vc_hash_field_match_type hf_map = hf_list[j];
++			err = ice_vc_vlan_action(vsi,
++						 vsi->inner_vlan_ops.del_vlan,
++						 &vlan);
++			if (err)
++				return err;
+ 
 -			if (proto_hdr->type == hf_map.vc_hdr &&
 -			    proto_hdr->field_selector == hf_map.vc_hash_field) {
 -				*hash_flds |= hf_map.ice_hash_field;
 -				break;
 -			}
++			/* no support for VLAN promiscuous on inner VLAN unless
++			 * we are in Single VLAN Mode (SVM)
++			 */
++			if (!ice_is_dvm_ena(&vsi->back->hw) && vlan_promisc)
++				ice_vf_dis_vlan_promisc(vsi, &vlan);
+ 		}
+ 	}
+ 
 -	return true;
 -}
 -
 -/**
 - * ice_vf_adv_rss_offload_ena - determine if capabilities support advanced
 - * RSS offloads
 - * @caps: VF driver negotiated capabilities
 - *
 - * Return true if VIRTCHNL_VF_OFFLOAD_ADV_RSS_PF capability is set,
 - * else return false
 - */
 -static bool ice_vf_adv_rss_offload_ena(u32 caps)
 -{
 -	return !!(caps & VIRTCHNL_VF_OFFLOAD_ADV_RSS_PF);
++	return 0;
+ }
+ 
+ /**
 - * ice_vc_handle_rss_cfg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the message buffer
 - * @add: add a RSS config if true, otherwise delete a RSS config
 - *
 - * This function adds/deletes a RSS config
++ * ice_vc_remove_vlan_v2_msg - virtchnl handler for VIRTCHNL_OP_DEL_VLAN_V2
++ * @vf: VF the message was received from
++ * @msg: message received from the VF
+  */
 -static int ice_vc_handle_rss_cfg(struct ice_vf *vf, u8 *msg, bool add)
++static int ice_vc_remove_vlan_v2_msg(struct ice_vf *vf, u8 *msg)
+ {
 -	u32 v_opcode = add ? VIRTCHNL_OP_ADD_RSS_CFG : VIRTCHNL_OP_DEL_RSS_CFG;
 -	struct virtchnl_rss_cfg *rss_cfg = (struct virtchnl_rss_cfg *)msg;
++	struct virtchnl_vlan_filter_list_v2 *vfl =
++		(struct virtchnl_vlan_filter_list_v2 *)msg;
+ 	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct device *dev = ice_pf_to_dev(vf->pf);
 -	struct ice_hw *hw = &vf->pf->hw;
+ 	struct ice_vsi *vsi;
+ 
 -	if (!test_bit(ICE_FLAG_RSS_ENA, vf->pf->flags)) {
 -		dev_dbg(dev, "VF %d attempting to configure RSS, but RSS is not supported by the PF\n",
 -			vf->vf_id);
 -		v_ret = VIRTCHNL_STATUS_ERR_NOT_SUPPORTED;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vf_adv_rss_offload_ena(vf->driver_caps)) {
 -		dev_dbg(dev, "VF %d attempting to configure RSS, but Advanced RSS offload is not supported\n",
 -			vf->vf_id);
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++	if (!ice_vc_validate_vlan_filter_list(&vf->vlan_v2_caps.filtering,
++					      vfl)) {
+ 		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
++		goto out;
+ 	}
+ 
 -	if (rss_cfg->proto_hdrs.count > VIRTCHNL_MAX_NUM_PROTO_HDRS ||
 -	    rss_cfg->rss_algorithm < VIRTCHNL_RSS_ALG_TOEPLITZ_ASYMMETRIC ||
 -	    rss_cfg->rss_algorithm > VIRTCHNL_RSS_ALG_XOR_SYMMETRIC) {
 -		dev_dbg(dev, "VF %d attempting to configure RSS, but RSS configuration is not valid\n",
 -			vf->vf_id);
++	if (!ice_vc_isvalid_vsi_id(vf, vfl->vport_id)) {
+ 		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
++		goto out;
+ 	}
+ 
+ 	vsi = ice_get_vf_vsi(vf);
+ 	if (!vsi) {
+ 		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
++		goto out;
+ 	}
+ 
 -	if (!ice_vc_validate_pattern(vf, &rss_cfg->proto_hdrs)) {
++	if (ice_vc_del_vlans(vf, vsi, vfl))
+ 		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
+ 
 -	if (rss_cfg->rss_algorithm == VIRTCHNL_RSS_ALG_R_ASYMMETRIC) {
 -		struct ice_vsi_ctx *ctx;
 -		u8 lut_type, hash_type;
 -		int status;
 -
 -		lut_type = ICE_AQ_VSI_Q_OPT_RSS_LUT_VSI;
 -		hash_type = add ? ICE_AQ_VSI_Q_OPT_RSS_XOR :
 -				ICE_AQ_VSI_Q_OPT_RSS_TPLZ;
 -
 -		ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
 -		if (!ctx) {
 -			v_ret = VIRTCHNL_STATUS_ERR_NO_MEMORY;
 -			goto error_param;
 -		}
 -
 -		ctx->info.q_opt_rss = ((lut_type <<
 -					ICE_AQ_VSI_Q_OPT_RSS_LUT_S) &
 -				       ICE_AQ_VSI_Q_OPT_RSS_LUT_M) |
 -				       (hash_type &
 -					ICE_AQ_VSI_Q_OPT_RSS_HASH_M);
 -
 -		/* Preserve existing queueing option setting */
 -		ctx->info.q_opt_rss |= (vsi->info.q_opt_rss &
 -					  ICE_AQ_VSI_Q_OPT_RSS_GBL_LUT_M);
 -		ctx->info.q_opt_tc = vsi->info.q_opt_tc;
 -		ctx->info.q_opt_flags = vsi->info.q_opt_rss;
 -
 -		ctx->info.valid_sections =
 -				cpu_to_le16(ICE_AQ_VSI_PROP_Q_OPT_VALID);
 -
 -		status = ice_update_vsi(hw, vsi->idx, ctx, NULL);
 -		if (status) {
 -			dev_err(dev, "update VSI for RSS failed, err %d aq_err %s\n",
 -				status, ice_aq_str(hw->adminq.sq_last_status));
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		} else {
 -			vsi->info.q_opt_rss = ctx->info.q_opt_rss;
 -		}
 -
 -		kfree(ctx);
 -	} else {
 -		u32 addl_hdrs = ICE_FLOW_SEG_HDR_NONE;
 -		u64 hash_flds = ICE_HASH_INVALID;
 -
 -		if (!ice_vc_parse_rss_cfg(hw, rss_cfg, &addl_hdrs,
 -					  &hash_flds)) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		if (add) {
 -			if (ice_add_rss_cfg(hw, vsi->idx, hash_flds,
 -					    addl_hdrs)) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				dev_err(dev, "ice_add_rss_cfg failed for vsi = %d, v_ret = %d\n",
 -					vsi->vsi_num, v_ret);
 -			}
 -		} else {
 -			int status;
 -
 -			status = ice_rem_rss_cfg(hw, vsi->idx, hash_flds,
 -						 addl_hdrs);
 -			/* We just ignore -ENOENT, because if two configurations
 -			 * share the same profile remove one of them actually
 -			 * removes both, since the profile is deleted.
 -			 */
 -			if (status && status != -ENOENT) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				dev_err(dev, "ice_rem_rss_cfg failed for VF ID:%d, error:%d\n",
 -					vf->vf_id, status);
 -			}
 -		}
 -	}
 -
 -error_param:
 -	return ice_vc_send_msg_to_vf(vf, v_opcode, v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_config_rss_key
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * Configure the VF's RSS key
 - */
 -static int ice_vc_config_rss_key(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_rss_key *vrk =
 -		(struct virtchnl_rss_key *)msg;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vrk->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (vrk->key_len != ICE_VSIQF_HKEY_ARRAY_SIZE) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!test_bit(ICE_FLAG_RSS_ENA, vf->pf->flags)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (ice_set_rss_key(vsi, vrk->key))
 -		v_ret = VIRTCHNL_STATUS_ERR_ADMIN_QUEUE_ERROR;
 -error_param:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_RSS_KEY, v_ret,
 -				     NULL, 0);
 -}
 -
 -/**
 - * ice_vc_config_rss_lut
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * Configure the VF's RSS LUT
 - */
 -static int ice_vc_config_rss_lut(struct ice_vf *vf, u8 *msg)
 -{
 -	struct virtchnl_rss_lut *vrl = (struct virtchnl_rss_lut *)msg;
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vrl->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (vrl->lut_entries != ICE_VSIQF_HLUT_ARRAY_SIZE) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!test_bit(ICE_FLAG_RSS_ENA, vf->pf->flags)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (ice_set_rss_lut(vsi, vrl->lut, ICE_VSIQF_HLUT_ARRAY_SIZE))
 -		v_ret = VIRTCHNL_STATUS_ERR_ADMIN_QUEUE_ERROR;
 -error_param:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_RSS_LUT, v_ret,
 -				     NULL, 0);
 -}
 -
 -/**
 - * ice_set_vf_spoofchk
 - * @netdev: network interface device structure
 - * @vf_id: VF identifier
 - * @ena: flag to enable or disable feature
 - *
 - * Enable or disable VF spoof checking
 - */
 -int ice_set_vf_spoofchk(struct net_device *netdev, int vf_id, bool ena)
 -{
 -	struct ice_netdev_priv *np = netdev_priv(netdev);
 -	struct ice_pf *pf = np->vsi->back;
 -	struct ice_vsi *vf_vsi;
 -	struct device *dev;
 -	struct ice_vf *vf;
 -	int ret;
 -
 -	dev = ice_pf_to_dev(pf);
 -
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf)
 -		return -EINVAL;
 -
 -	ret = ice_check_vf_ready_for_cfg(vf);
 -	if (ret)
 -		goto out_put_vf;
 -
 -	vf_vsi = ice_get_vf_vsi(vf);
 -	if (!vf_vsi) {
 -		netdev_err(netdev, "VSI %d for VF %d is null\n",
 -			   vf->lan_vsi_idx, vf->vf_id);
 -		ret = -EINVAL;
 -		goto out_put_vf;
 -	}
 -
 -	if (vf_vsi->type != ICE_VSI_VF) {
 -		netdev_err(netdev, "Type %d of VSI %d for VF %d is no ICE_VSI_VF\n",
 -			   vf_vsi->type, vf_vsi->vsi_num, vf->vf_id);
 -		ret = -ENODEV;
 -		goto out_put_vf;
 -	}
 -
 -	if (ena == vf->spoofchk) {
 -		dev_dbg(dev, "VF spoofchk already %s\n", ena ? "ON" : "OFF");
 -		ret = 0;
 -		goto out_put_vf;
 -	}
 -
 -	ret = ice_vsi_apply_spoofchk(vf_vsi, ena);
 -	if (ret)
 -		dev_err(dev, "Failed to set spoofchk %s for VF %d VSI %d\n error %d\n",
 -			ena ? "ON" : "OFF", vf->vf_id, vf_vsi->vsi_num, ret);
 -	else
 -		vf->spoofchk = ena;
 -
 -out_put_vf:
 -	ice_put_vf(vf);
 -	return ret;
 -}
 -
 -/**
 - * ice_vc_cfg_promiscuous_mode_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to configure VF VSIs promiscuous mode
 - */
 -static int ice_vc_cfg_promiscuous_mode_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	bool rm_promisc, alluni = false, allmulti = false;
 -	struct virtchnl_promisc_info *info =
 -	    (struct virtchnl_promisc_info *)msg;
 -	struct ice_vsi_vlan_ops *vlan_ops;
 -	int mcast_err = 0, ucast_err = 0;
 -	struct ice_pf *pf = vf->pf;
 -	struct ice_vsi *vsi;
 -	struct device *dev;
 -	int ret = 0;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, info->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	dev = ice_pf_to_dev(pf);
 -	if (!ice_is_vf_trusted(vf)) {
 -		dev_err(dev, "Unprivileged VF %d is attempting to configure promiscuous mode\n",
 -			vf->vf_id);
 -		/* Leave v_ret alone, lie to the VF on purpose. */
 -		goto error_param;
 -	}
 -
 -	if (info->flags & FLAG_VF_UNICAST_PROMISC)
 -		alluni = true;
 -
 -	if (info->flags & FLAG_VF_MULTICAST_PROMISC)
 -		allmulti = true;
 -
 -	rm_promisc = !allmulti && !alluni;
 -
 -	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
 -	if (rm_promisc)
 -		ret = vlan_ops->ena_rx_filtering(vsi);
 -	else
 -		ret = vlan_ops->dis_rx_filtering(vsi);
 -	if (ret) {
 -		dev_err(dev, "Failed to configure VLAN pruning in promiscuous mode\n");
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!test_bit(ICE_FLAG_VF_TRUE_PROMISC_ENA, pf->flags)) {
 -		bool set_dflt_vsi = alluni || allmulti;
 -
 -		if (set_dflt_vsi && !ice_is_dflt_vsi_in_use(pf->first_sw))
 -			/* only attempt to set the default forwarding VSI if
 -			 * it's not currently set
 -			 */
 -			ret = ice_set_dflt_vsi(pf->first_sw, vsi);
 -		else if (!set_dflt_vsi &&
 -			 ice_is_vsi_dflt_vsi(pf->first_sw, vsi))
 -			/* only attempt to free the default forwarding VSI if we
 -			 * are the owner
 -			 */
 -			ret = ice_clear_dflt_vsi(pf->first_sw);
 -
 -		if (ret) {
 -			dev_err(dev, "%sable VF %d as the default VSI failed, error %d\n",
 -				set_dflt_vsi ? "en" : "dis", vf->vf_id, ret);
 -			v_ret = VIRTCHNL_STATUS_ERR_ADMIN_QUEUE_ERROR;
 -			goto error_param;
 -		}
 -	} else {
 -		u8 mcast_m, ucast_m;
 -
 -		if (ice_vf_is_port_vlan_ena(vf) ||
 -		    ice_vsi_has_non_zero_vlans(vsi)) {
 -			mcast_m = ICE_MCAST_VLAN_PROMISC_BITS;
 -			ucast_m = ICE_UCAST_VLAN_PROMISC_BITS;
 -		} else {
 -			mcast_m = ICE_MCAST_PROMISC_BITS;
 -			ucast_m = ICE_UCAST_PROMISC_BITS;
 -		}
 -
 -		if (alluni)
 -			ucast_err = ice_vf_set_vsi_promisc(vf, vsi, ucast_m);
 -		else
 -			ucast_err = ice_vf_clear_vsi_promisc(vf, vsi, ucast_m);
 -
 -		if (allmulti)
 -			mcast_err = ice_vf_set_vsi_promisc(vf, vsi, mcast_m);
 -		else
 -			mcast_err = ice_vf_clear_vsi_promisc(vf, vsi, mcast_m);
 -
 -		if (ucast_err || mcast_err)
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -	}
 -
 -	if (!mcast_err) {
 -		if (allmulti &&
 -		    !test_and_set_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states))
 -			dev_info(dev, "VF %u successfully set multicast promiscuous mode\n",
 -				 vf->vf_id);
 -		else if (!allmulti &&
 -			 test_and_clear_bit(ICE_VF_STATE_MC_PROMISC,
 -					    vf->vf_states))
 -			dev_info(dev, "VF %u successfully unset multicast promiscuous mode\n",
 -				 vf->vf_id);
 -	}
 -
 -	if (!ucast_err) {
 -		if (alluni &&
 -		    !test_and_set_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states))
 -			dev_info(dev, "VF %u successfully set unicast promiscuous mode\n",
 -				 vf->vf_id);
 -		else if (!alluni &&
 -			 test_and_clear_bit(ICE_VF_STATE_UC_PROMISC,
 -					    vf->vf_states))
 -			dev_info(dev, "VF %u successfully unset unicast promiscuous mode\n",
 -				 vf->vf_id);
 -	}
 -
 -error_param:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE,
 -				     v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_get_stats_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to get VSI stats
 - */
 -static int ice_vc_get_stats_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_queue_select *vqs =
 -		(struct virtchnl_queue_select *)msg;
 -	struct ice_eth_stats stats = { 0 };
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vqs->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	ice_update_eth_stats(vsi);
 -
 -	stats = vsi->eth_stats;
 -
 -error_param:
 -	/* send the response to the VF */
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_GET_STATS, v_ret,
 -				     (u8 *)&stats, sizeof(stats));
 -}
 -
 -/**
 - * ice_vc_validate_vqs_bitmaps - validate Rx/Tx queue bitmaps from VIRTCHNL
 - * @vqs: virtchnl_queue_select structure containing bitmaps to validate
 - *
 - * Return true on successful validation, else false
 - */
 -static bool ice_vc_validate_vqs_bitmaps(struct virtchnl_queue_select *vqs)
 -{
 -	if ((!vqs->rx_queues && !vqs->tx_queues) ||
 -	    vqs->rx_queues >= BIT(ICE_MAX_RSS_QS_PER_VF) ||
 -	    vqs->tx_queues >= BIT(ICE_MAX_RSS_QS_PER_VF))
 -		return false;
 -
 -	return true;
 -}
 -
 -/**
 - * ice_vf_ena_txq_interrupt - enable Tx queue interrupt via QINT_TQCTL
 - * @vsi: VSI of the VF to configure
 - * @q_idx: VF queue index used to determine the queue in the PF's space
 - */
 -static void ice_vf_ena_txq_interrupt(struct ice_vsi *vsi, u32 q_idx)
 -{
 -	struct ice_hw *hw = &vsi->back->hw;
 -	u32 pfq = vsi->txq_map[q_idx];
 -	u32 reg;
 -
 -	reg = rd32(hw, QINT_TQCTL(pfq));
 -
 -	/* MSI-X index 0 in the VF's space is always for the OICR, which means
 -	 * this is most likely a poll mode VF driver, so don't enable an
 -	 * interrupt that was never configured via VIRTCHNL_OP_CONFIG_IRQ_MAP
 -	 */
 -	if (!(reg & QINT_TQCTL_MSIX_INDX_M))
 -		return;
 -
 -	wr32(hw, QINT_TQCTL(pfq), reg | QINT_TQCTL_CAUSE_ENA_M);
 -}
 -
 -/**
 - * ice_vf_ena_rxq_interrupt - enable Tx queue interrupt via QINT_RQCTL
 - * @vsi: VSI of the VF to configure
 - * @q_idx: VF queue index used to determine the queue in the PF's space
 - */
 -static void ice_vf_ena_rxq_interrupt(struct ice_vsi *vsi, u32 q_idx)
 -{
 -	struct ice_hw *hw = &vsi->back->hw;
 -	u32 pfq = vsi->rxq_map[q_idx];
 -	u32 reg;
 -
 -	reg = rd32(hw, QINT_RQCTL(pfq));
 -
 -	/* MSI-X index 0 in the VF's space is always for the OICR, which means
 -	 * this is most likely a poll mode VF driver, so don't enable an
 -	 * interrupt that was never configured via VIRTCHNL_OP_CONFIG_IRQ_MAP
 -	 */
 -	if (!(reg & QINT_RQCTL_MSIX_INDX_M))
 -		return;
 -
 -	wr32(hw, QINT_RQCTL(pfq), reg | QINT_RQCTL_CAUSE_ENA_M);
 -}
 -
 -/**
 - * ice_vc_ena_qs_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to enable all or specific queue(s)
 - */
 -static int ice_vc_ena_qs_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_queue_select *vqs =
 -	    (struct virtchnl_queue_select *)msg;
 -	struct ice_vsi *vsi;
 -	unsigned long q_map;
 -	u16 vf_q_id;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vqs->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_validate_vqs_bitmaps(vqs)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	/* Enable only Rx rings, Tx rings were enabled by the FW when the
 -	 * Tx queue group list was configured and the context bits were
 -	 * programmed using ice_vsi_cfg_txqs
 -	 */
 -	q_map = vqs->rx_queues;
 -	for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
 -		if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		/* Skip queue if enabled */
 -		if (test_bit(vf_q_id, vf->rxq_ena))
 -			continue;
 -
 -		if (ice_vsi_ctrl_one_rx_ring(vsi, true, vf_q_id, true)) {
 -			dev_err(ice_pf_to_dev(vsi->back), "Failed to enable Rx ring %d on VSI %d\n",
 -				vf_q_id, vsi->vsi_num);
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		ice_vf_ena_rxq_interrupt(vsi, vf_q_id);
 -		set_bit(vf_q_id, vf->rxq_ena);
 -	}
 -
 -	q_map = vqs->tx_queues;
 -	for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
 -		if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		/* Skip queue if enabled */
 -		if (test_bit(vf_q_id, vf->txq_ena))
 -			continue;
 -
 -		ice_vf_ena_txq_interrupt(vsi, vf_q_id);
 -		set_bit(vf_q_id, vf->txq_ena);
 -	}
 -
 -	/* Set flag to indicate that queues are enabled */
 -	if (v_ret == VIRTCHNL_STATUS_SUCCESS)
 -		set_bit(ICE_VF_STATE_QS_ENA, vf->vf_states);
 -
 -error_param:
 -	/* send the response to the VF */
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_QUEUES, v_ret,
 -				     NULL, 0);
 -}
 -
 -/**
 - * ice_vc_dis_qs_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to disable all or specific
 - * queue(s)
 - */
 -static int ice_vc_dis_qs_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_queue_select *vqs =
 -	    (struct virtchnl_queue_select *)msg;
 -	struct ice_vsi *vsi;
 -	unsigned long q_map;
 -	u16 vf_q_id;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states) &&
 -	    !test_bit(ICE_VF_STATE_QS_ENA, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vqs->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_validate_vqs_bitmaps(vqs)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (vqs->tx_queues) {
 -		q_map = vqs->tx_queues;
 -
 -		for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
 -			struct ice_tx_ring *ring = vsi->tx_rings[vf_q_id];
 -			struct ice_txq_meta txq_meta = { 0 };
 -
 -			if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -
 -			/* Skip queue if not enabled */
 -			if (!test_bit(vf_q_id, vf->txq_ena))
 -				continue;
 -
 -			ice_fill_txq_meta(vsi, ring, &txq_meta);
 -
 -			if (ice_vsi_stop_tx_ring(vsi, ICE_NO_RESET, vf->vf_id,
 -						 ring, &txq_meta)) {
 -				dev_err(ice_pf_to_dev(vsi->back), "Failed to stop Tx ring %d on VSI %d\n",
 -					vf_q_id, vsi->vsi_num);
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -
 -			/* Clear enabled queues flag */
 -			clear_bit(vf_q_id, vf->txq_ena);
 -		}
 -	}
 -
 -	q_map = vqs->rx_queues;
 -	/* speed up Rx queue disable by batching them if possible */
 -	if (q_map &&
 -	    bitmap_equal(&q_map, vf->rxq_ena, ICE_MAX_RSS_QS_PER_VF)) {
 -		if (ice_vsi_stop_all_rx_rings(vsi)) {
 -			dev_err(ice_pf_to_dev(vsi->back), "Failed to stop all Rx rings on VSI %d\n",
 -				vsi->vsi_num);
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		bitmap_zero(vf->rxq_ena, ICE_MAX_RSS_QS_PER_VF);
 -	} else if (q_map) {
 -		for_each_set_bit(vf_q_id, &q_map, ICE_MAX_RSS_QS_PER_VF) {
 -			if (!ice_vc_isvalid_q_id(vf, vqs->vsi_id, vf_q_id)) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -
 -			/* Skip queue if not enabled */
 -			if (!test_bit(vf_q_id, vf->rxq_ena))
 -				continue;
 -
 -			if (ice_vsi_ctrl_one_rx_ring(vsi, false, vf_q_id,
 -						     true)) {
 -				dev_err(ice_pf_to_dev(vsi->back), "Failed to stop Rx ring %d on VSI %d\n",
 -					vf_q_id, vsi->vsi_num);
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -
 -			/* Clear enabled queues flag */
 -			clear_bit(vf_q_id, vf->rxq_ena);
 -		}
 -	}
 -
 -	/* Clear enabled queues flag */
 -	if (v_ret == VIRTCHNL_STATUS_SUCCESS && ice_vf_has_no_qs_ena(vf))
 -		clear_bit(ICE_VF_STATE_QS_ENA, vf->vf_states);
 -
 -error_param:
 -	/* send the response to the VF */
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_QUEUES, v_ret,
 -				     NULL, 0);
 -}
 -
 -/**
 - * ice_cfg_interrupt
 - * @vf: pointer to the VF info
 - * @vsi: the VSI being configured
 - * @vector_id: vector ID
 - * @map: vector map for mapping vectors to queues
 - * @q_vector: structure for interrupt vector
 - * configure the IRQ to queue map
 - */
 -static int
 -ice_cfg_interrupt(struct ice_vf *vf, struct ice_vsi *vsi, u16 vector_id,
 -		  struct virtchnl_vector_map *map,
 -		  struct ice_q_vector *q_vector)
 -{
 -	u16 vsi_q_id, vsi_q_id_idx;
 -	unsigned long qmap;
 -
 -	q_vector->num_ring_rx = 0;
 -	q_vector->num_ring_tx = 0;
 -
 -	qmap = map->rxq_map;
 -	for_each_set_bit(vsi_q_id_idx, &qmap, ICE_MAX_RSS_QS_PER_VF) {
 -		vsi_q_id = vsi_q_id_idx;
 -
 -		if (!ice_vc_isvalid_q_id(vf, vsi->vsi_num, vsi_q_id))
 -			return VIRTCHNL_STATUS_ERR_PARAM;
 -
 -		q_vector->num_ring_rx++;
 -		q_vector->rx.itr_idx = map->rxitr_idx;
 -		vsi->rx_rings[vsi_q_id]->q_vector = q_vector;
 -		ice_cfg_rxq_interrupt(vsi, vsi_q_id, vector_id,
 -				      q_vector->rx.itr_idx);
 -	}
 -
 -	qmap = map->txq_map;
 -	for_each_set_bit(vsi_q_id_idx, &qmap, ICE_MAX_RSS_QS_PER_VF) {
 -		vsi_q_id = vsi_q_id_idx;
 -
 -		if (!ice_vc_isvalid_q_id(vf, vsi->vsi_num, vsi_q_id))
 -			return VIRTCHNL_STATUS_ERR_PARAM;
 -
 -		q_vector->num_ring_tx++;
 -		q_vector->tx.itr_idx = map->txitr_idx;
 -		vsi->tx_rings[vsi_q_id]->q_vector = q_vector;
 -		ice_cfg_txq_interrupt(vsi, vsi_q_id, vector_id,
 -				      q_vector->tx.itr_idx);
 -	}
 -
 -	return VIRTCHNL_STATUS_SUCCESS;
 -}
 -
 -/**
 - * ice_vc_cfg_irq_map_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to configure the IRQ to queue map
 - */
 -static int ice_vc_cfg_irq_map_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	u16 num_q_vectors_mapped, vsi_id, vector_id;
 -	struct virtchnl_irq_map_info *irqmap_info;
 -	struct virtchnl_vector_map *map;
 -	struct ice_pf *pf = vf->pf;
 -	struct ice_vsi *vsi;
 -	int i;
 -
 -	irqmap_info = (struct virtchnl_irq_map_info *)msg;
 -	num_q_vectors_mapped = irqmap_info->num_vectors;
 -
 -	/* Check to make sure number of VF vectors mapped is not greater than
 -	 * number of VF vectors originally allocated, and check that
 -	 * there is actually at least a single VF queue vector mapped
 -	 */
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states) ||
 -	    pf->vfs.num_msix_per < num_q_vectors_mapped ||
 -	    !num_q_vectors_mapped) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	for (i = 0; i < num_q_vectors_mapped; i++) {
 -		struct ice_q_vector *q_vector;
 -
 -		map = &irqmap_info->vecmap[i];
 -
 -		vector_id = map->vector_id;
 -		vsi_id = map->vsi_id;
 -		/* vector_id is always 0-based for each VF, and can never be
 -		 * larger than or equal to the max allowed interrupts per VF
 -		 */
 -		if (!(vector_id < pf->vfs.num_msix_per) ||
 -		    !ice_vc_isvalid_vsi_id(vf, vsi_id) ||
 -		    (!vector_id && (map->rxq_map || map->txq_map))) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		/* No need to map VF miscellaneous or rogue vector */
 -		if (!vector_id)
 -			continue;
 -
 -		/* Subtract non queue vector from vector_id passed by VF
 -		 * to get actual number of VSI queue vector array index
 -		 */
 -		q_vector = vsi->q_vectors[vector_id - ICE_NONQ_VECS_VF];
 -		if (!q_vector) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		/* lookout for the invalid queue index */
 -		v_ret = (enum virtchnl_status_code)
 -			ice_cfg_interrupt(vf, vsi, vector_id, map, q_vector);
 -		if (v_ret)
 -			goto error_param;
 -	}
 -
 -error_param:
 -	/* send the response to the VF */
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_IRQ_MAP, v_ret,
 -				     NULL, 0);
 -}
 -
 -/**
 - * ice_vc_cfg_qs_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * called from the VF to configure the Rx/Tx queues
 - */
 -static int ice_vc_cfg_qs_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vsi_queue_config_info *qci =
 -	    (struct virtchnl_vsi_queue_config_info *)msg;
 -	struct virtchnl_queue_pair_info *qpi;
 -	struct ice_pf *pf = vf->pf;
 -	struct ice_vsi *vsi;
 -	int i, q_idx;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, qci->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (qci->num_queue_pairs > ICE_MAX_RSS_QS_PER_VF ||
 -	    qci->num_queue_pairs > min_t(u16, vsi->alloc_txq, vsi->alloc_rxq)) {
 -		dev_err(ice_pf_to_dev(pf), "VF-%d requesting more than supported number of queues: %d\n",
 -			vf->vf_id, min_t(u16, vsi->alloc_txq, vsi->alloc_rxq));
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	for (i = 0; i < qci->num_queue_pairs; i++) {
 -		qpi = &qci->qpair[i];
 -		if (qpi->txq.vsi_id != qci->vsi_id ||
 -		    qpi->rxq.vsi_id != qci->vsi_id ||
 -		    qpi->rxq.queue_id != qpi->txq.queue_id ||
 -		    qpi->txq.headwb_enabled ||
 -		    !ice_vc_isvalid_ring_len(qpi->txq.ring_len) ||
 -		    !ice_vc_isvalid_ring_len(qpi->rxq.ring_len) ||
 -		    !ice_vc_isvalid_q_id(vf, qci->vsi_id, qpi->txq.queue_id)) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		q_idx = qpi->rxq.queue_id;
 -
 -		/* make sure selected "q_idx" is in valid range of queues
 -		 * for selected "vsi"
 -		 */
 -		if (q_idx >= vsi->alloc_txq || q_idx >= vsi->alloc_rxq) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto error_param;
 -		}
 -
 -		/* copy Tx queue info from VF into VSI */
 -		if (qpi->txq.ring_len > 0) {
 -			vsi->tx_rings[i]->dma = qpi->txq.dma_ring_addr;
 -			vsi->tx_rings[i]->count = qpi->txq.ring_len;
 -			if (ice_vsi_cfg_single_txq(vsi, vsi->tx_rings, q_idx)) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -		}
 -
 -		/* copy Rx queue info from VF into VSI */
 -		if (qpi->rxq.ring_len > 0) {
 -			u16 max_frame_size = ice_vc_get_max_frame_size(vf);
 -
 -			vsi->rx_rings[i]->dma = qpi->rxq.dma_ring_addr;
 -			vsi->rx_rings[i]->count = qpi->rxq.ring_len;
 -
 -			if (qpi->rxq.databuffer_size != 0 &&
 -			    (qpi->rxq.databuffer_size > ((16 * 1024) - 128) ||
 -			     qpi->rxq.databuffer_size < 1024)) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -			vsi->rx_buf_len = qpi->rxq.databuffer_size;
 -			vsi->rx_rings[i]->rx_buf_len = vsi->rx_buf_len;
 -			if (qpi->rxq.max_pkt_size > max_frame_size ||
 -			    qpi->rxq.max_pkt_size < 64) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -
 -			vsi->max_frame = qpi->rxq.max_pkt_size;
 -			/* add space for the port VLAN since the VF driver is
 -			 * not expected to account for it in the MTU
 -			 * calculation
 -			 */
 -			if (ice_vf_is_port_vlan_ena(vf))
 -				vsi->max_frame += VLAN_HLEN;
 -
 -			if (ice_vsi_cfg_single_rxq(vsi, q_idx)) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -		}
 -	}
 -
 -error_param:
 -	/* send the response to the VF */
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_VSI_QUEUES, v_ret,
 -				     NULL, 0);
 -}
 -
 -/**
 - * ice_can_vf_change_mac
 - * @vf: pointer to the VF info
 - *
 - * Return true if the VF is allowed to change its MAC filters, false otherwise
 - */
 -static bool ice_can_vf_change_mac(struct ice_vf *vf)
 -{
 -	/* If the VF MAC address has been set administratively (via the
 -	 * ndo_set_vf_mac command), then deny permission to the VF to
 -	 * add/delete unicast MAC addresses, unless the VF is trusted
 -	 */
 -	if (vf->pf_set_mac && !ice_is_vf_trusted(vf))
 -		return false;
 -
 -	return true;
 -}
 -
 -/**
 - * ice_vc_ether_addr_type - get type of virtchnl_ether_addr
 - * @vc_ether_addr: used to extract the type
 - */
 -static u8
 -ice_vc_ether_addr_type(struct virtchnl_ether_addr *vc_ether_addr)
 -{
 -	return (vc_ether_addr->type & VIRTCHNL_ETHER_ADDR_TYPE_MASK);
 -}
 -
 -/**
 - * ice_is_vc_addr_legacy - check if the MAC address is from an older VF
 - * @vc_ether_addr: VIRTCHNL structure that contains MAC and type
 - */
 -static bool
 -ice_is_vc_addr_legacy(struct virtchnl_ether_addr *vc_ether_addr)
 -{
 -	u8 type = ice_vc_ether_addr_type(vc_ether_addr);
 -
 -	return (type == VIRTCHNL_ETHER_ADDR_LEGACY);
 -}
 -
 -/**
 - * ice_is_vc_addr_primary - check if the MAC address is the VF's primary MAC
 - * @vc_ether_addr: VIRTCHNL structure that contains MAC and type
 - *
 - * This function should only be called when the MAC address in
 - * virtchnl_ether_addr is a valid unicast MAC
 - */
 -static bool
 -ice_is_vc_addr_primary(struct virtchnl_ether_addr __maybe_unused *vc_ether_addr)
 -{
 -	u8 type = ice_vc_ether_addr_type(vc_ether_addr);
 -
 -	return (type == VIRTCHNL_ETHER_ADDR_PRIMARY);
 -}
 -
 -/**
 - * ice_vfhw_mac_add - update the VF's cached hardware MAC if allowed
 - * @vf: VF to update
 - * @vc_ether_addr: structure from VIRTCHNL with MAC to add
 - */
 -static void
 -ice_vfhw_mac_add(struct ice_vf *vf, struct virtchnl_ether_addr *vc_ether_addr)
 -{
 -	u8 *mac_addr = vc_ether_addr->addr;
 -
 -	if (!is_valid_ether_addr(mac_addr))
 -		return;
 -
 -	/* only allow legacy VF drivers to set the device and hardware MAC if it
 -	 * is zero and allow new VF drivers to set the hardware MAC if the type
 -	 * was correctly specified over VIRTCHNL
 -	 */
 -	if ((ice_is_vc_addr_legacy(vc_ether_addr) &&
 -	     is_zero_ether_addr(vf->hw_lan_addr.addr)) ||
 -	    ice_is_vc_addr_primary(vc_ether_addr)) {
 -		ether_addr_copy(vf->dev_lan_addr.addr, mac_addr);
 -		ether_addr_copy(vf->hw_lan_addr.addr, mac_addr);
 -	}
 -
 -	/* hardware and device MACs are already set, but its possible that the
 -	 * VF driver sent the VIRTCHNL_OP_ADD_ETH_ADDR message before the
 -	 * VIRTCHNL_OP_DEL_ETH_ADDR when trying to update its MAC, so save it
 -	 * away for the legacy VF driver case as it will be updated in the
 -	 * delete flow for this case
 -	 */
 -	if (ice_is_vc_addr_legacy(vc_ether_addr)) {
 -		ether_addr_copy(vf->legacy_last_added_umac.addr,
 -				mac_addr);
 -		vf->legacy_last_added_umac.time_modified = jiffies;
 -	}
 -}
 -
 -/**
 - * ice_vc_add_mac_addr - attempt to add the MAC address passed in
 - * @vf: pointer to the VF info
 - * @vsi: pointer to the VF's VSI
 - * @vc_ether_addr: VIRTCHNL MAC address structure used to add MAC
 - */
 -static int
 -ice_vc_add_mac_addr(struct ice_vf *vf, struct ice_vsi *vsi,
 -		    struct virtchnl_ether_addr *vc_ether_addr)
 -{
 -	struct device *dev = ice_pf_to_dev(vf->pf);
 -	u8 *mac_addr = vc_ether_addr->addr;
 -	int ret;
 -
 -	/* device MAC already added */
 -	if (ether_addr_equal(mac_addr, vf->dev_lan_addr.addr))
 -		return 0;
 -
 -	if (is_unicast_ether_addr(mac_addr) && !ice_can_vf_change_mac(vf)) {
 -		dev_err(dev, "VF attempting to override administratively set MAC address, bring down and up the VF interface to resume normal operation\n");
 -		return -EPERM;
 -	}
 -
 -	ret = ice_fltr_add_mac(vsi, mac_addr, ICE_FWD_TO_VSI);
 -	if (ret == -EEXIST) {
 -		dev_dbg(dev, "MAC %pM already exists for VF %d\n", mac_addr,
 -			vf->vf_id);
 -		/* don't return since we might need to update
 -		 * the primary MAC in ice_vfhw_mac_add() below
 -		 */
 -	} else if (ret) {
 -		dev_err(dev, "Failed to add MAC %pM for VF %d\n, error %d\n",
 -			mac_addr, vf->vf_id, ret);
 -		return ret;
 -	} else {
 -		vf->num_mac++;
 -	}
 -
 -	ice_vfhw_mac_add(vf, vc_ether_addr);
 -
 -	return ret;
 -}
 -
 -/**
 - * ice_is_legacy_umac_expired - check if last added legacy unicast MAC expired
 - * @last_added_umac: structure used to check expiration
 - */
 -static bool ice_is_legacy_umac_expired(struct ice_time_mac *last_added_umac)
 -{
 -#define ICE_LEGACY_VF_MAC_CHANGE_EXPIRE_TIME	msecs_to_jiffies(3000)
 -	return time_is_before_jiffies(last_added_umac->time_modified +
 -				      ICE_LEGACY_VF_MAC_CHANGE_EXPIRE_TIME);
 -}
 -
 -/**
 - * ice_update_legacy_cached_mac - update cached hardware MAC for legacy VF
 - * @vf: VF to update
 - * @vc_ether_addr: structure from VIRTCHNL with MAC to check
 - *
 - * only update cached hardware MAC for legacy VF drivers on delete
 - * because we cannot guarantee order/type of MAC from the VF driver
 - */
 -static void
 -ice_update_legacy_cached_mac(struct ice_vf *vf,
 -			     struct virtchnl_ether_addr *vc_ether_addr)
 -{
 -	if (!ice_is_vc_addr_legacy(vc_ether_addr) ||
 -	    ice_is_legacy_umac_expired(&vf->legacy_last_added_umac))
 -		return;
 -
 -	ether_addr_copy(vf->dev_lan_addr.addr, vf->legacy_last_added_umac.addr);
 -	ether_addr_copy(vf->hw_lan_addr.addr, vf->legacy_last_added_umac.addr);
 -}
 -
 -/**
 - * ice_vfhw_mac_del - update the VF's cached hardware MAC if allowed
 - * @vf: VF to update
 - * @vc_ether_addr: structure from VIRTCHNL with MAC to delete
 - */
 -static void
 -ice_vfhw_mac_del(struct ice_vf *vf, struct virtchnl_ether_addr *vc_ether_addr)
 -{
 -	u8 *mac_addr = vc_ether_addr->addr;
 -
 -	if (!is_valid_ether_addr(mac_addr) ||
 -	    !ether_addr_equal(vf->dev_lan_addr.addr, mac_addr))
 -		return;
 -
 -	/* allow the device MAC to be repopulated in the add flow and don't
 -	 * clear the hardware MAC (i.e. hw_lan_addr.addr) here as that is meant
 -	 * to be persistent on VM reboot and across driver unload/load, which
 -	 * won't work if we clear the hardware MAC here
 -	 */
 -	eth_zero_addr(vf->dev_lan_addr.addr);
 -
 -	ice_update_legacy_cached_mac(vf, vc_ether_addr);
 -}
 -
 -/**
 - * ice_vc_del_mac_addr - attempt to delete the MAC address passed in
 - * @vf: pointer to the VF info
 - * @vsi: pointer to the VF's VSI
 - * @vc_ether_addr: VIRTCHNL MAC address structure used to delete MAC
 - */
 -static int
 -ice_vc_del_mac_addr(struct ice_vf *vf, struct ice_vsi *vsi,
 -		    struct virtchnl_ether_addr *vc_ether_addr)
 -{
 -	struct device *dev = ice_pf_to_dev(vf->pf);
 -	u8 *mac_addr = vc_ether_addr->addr;
 -	int status;
 -
 -	if (!ice_can_vf_change_mac(vf) &&
 -	    ether_addr_equal(vf->dev_lan_addr.addr, mac_addr))
 -		return 0;
 -
 -	status = ice_fltr_remove_mac(vsi, mac_addr, ICE_FWD_TO_VSI);
 -	if (status == -ENOENT) {
 -		dev_err(dev, "MAC %pM does not exist for VF %d\n", mac_addr,
 -			vf->vf_id);
 -		return -ENOENT;
 -	} else if (status) {
 -		dev_err(dev, "Failed to delete MAC %pM for VF %d, error %d\n",
 -			mac_addr, vf->vf_id, status);
 -		return -EIO;
 -	}
 -
 -	ice_vfhw_mac_del(vf, vc_ether_addr);
 -
 -	vf->num_mac--;
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vc_handle_mac_addr_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - * @set: true if MAC filters are being set, false otherwise
 - *
 - * add guest MAC address filter
 - */
 -static int
 -ice_vc_handle_mac_addr_msg(struct ice_vf *vf, u8 *msg, bool set)
 -{
 -	int (*ice_vc_cfg_mac)
 -		(struct ice_vf *vf, struct ice_vsi *vsi,
 -		 struct virtchnl_ether_addr *virtchnl_ether_addr);
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_ether_addr_list *al =
 -	    (struct virtchnl_ether_addr_list *)msg;
 -	struct ice_pf *pf = vf->pf;
 -	enum virtchnl_ops vc_op;
 -	struct ice_vsi *vsi;
 -	int i;
 -
 -	if (set) {
 -		vc_op = VIRTCHNL_OP_ADD_ETH_ADDR;
 -		ice_vc_cfg_mac = ice_vc_add_mac_addr;
 -	} else {
 -		vc_op = VIRTCHNL_OP_DEL_ETH_ADDR;
 -		ice_vc_cfg_mac = ice_vc_del_mac_addr;
 -	}
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states) ||
 -	    !ice_vc_isvalid_vsi_id(vf, al->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto handle_mac_exit;
 -	}
 -
 -	/* If this VF is not privileged, then we can't add more than a
 -	 * limited number of addresses. Check to make sure that the
 -	 * additions do not push us over the limit.
 -	 */
 -	if (set && !ice_is_vf_trusted(vf) &&
 -	    (vf->num_mac + al->num_elements) > ICE_MAX_MACADDR_PER_VF) {
 -		dev_err(ice_pf_to_dev(pf), "Can't add more MAC addresses, because VF-%d is not trusted, switch the VF to trusted mode in order to add more functionalities\n",
 -			vf->vf_id);
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto handle_mac_exit;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto handle_mac_exit;
 -	}
 -
 -	for (i = 0; i < al->num_elements; i++) {
 -		u8 *mac_addr = al->list[i].addr;
 -		int result;
 -
 -		if (is_broadcast_ether_addr(mac_addr) ||
 -		    is_zero_ether_addr(mac_addr))
 -			continue;
 -
 -		result = ice_vc_cfg_mac(vf, vsi, &al->list[i]);
 -		if (result == -EEXIST || result == -ENOENT) {
 -			continue;
 -		} else if (result) {
 -			v_ret = VIRTCHNL_STATUS_ERR_ADMIN_QUEUE_ERROR;
 -			goto handle_mac_exit;
 -		}
 -	}
 -
 -handle_mac_exit:
 -	/* send the response to the VF */
 -	return ice_vc_send_msg_to_vf(vf, vc_op, v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_add_mac_addr_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * add guest MAC address filter
 - */
 -static int ice_vc_add_mac_addr_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	return ice_vc_handle_mac_addr_msg(vf, msg, true);
 -}
 -
 -/**
 - * ice_vc_del_mac_addr_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * remove guest MAC address filter
 - */
 -static int ice_vc_del_mac_addr_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	return ice_vc_handle_mac_addr_msg(vf, msg, false);
 -}
 -
 -/**
 - * ice_vc_request_qs_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * VFs get a default number of queues but can use this message to request a
 - * different number. If the request is successful, PF will reset the VF and
 - * return 0. If unsuccessful, PF will send message informing VF of number of
 - * available queue pairs via virtchnl message response to VF.
 - */
 -static int ice_vc_request_qs_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vf_res_request *vfres =
 -		(struct virtchnl_vf_res_request *)msg;
 -	u16 req_queues = vfres->num_queue_pairs;
 -	struct ice_pf *pf = vf->pf;
 -	u16 max_allowed_vf_queues;
 -	u16 tx_rx_queue_left;
 -	struct device *dev;
 -	u16 cur_queues;
 -
 -	dev = ice_pf_to_dev(pf);
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	cur_queues = vf->num_vf_qs;
 -	tx_rx_queue_left = min_t(u16, ice_get_avail_txq_count(pf),
 -				 ice_get_avail_rxq_count(pf));
 -	max_allowed_vf_queues = tx_rx_queue_left + cur_queues;
 -	if (!req_queues) {
 -		dev_err(dev, "VF %d tried to request 0 queues. Ignoring.\n",
 -			vf->vf_id);
 -	} else if (req_queues > ICE_MAX_RSS_QS_PER_VF) {
 -		dev_err(dev, "VF %d tried to request more than %d queues.\n",
 -			vf->vf_id, ICE_MAX_RSS_QS_PER_VF);
 -		vfres->num_queue_pairs = ICE_MAX_RSS_QS_PER_VF;
 -	} else if (req_queues > cur_queues &&
 -		   req_queues - cur_queues > tx_rx_queue_left) {
 -		dev_warn(dev, "VF %d requested %u more queues, but only %u left.\n",
 -			 vf->vf_id, req_queues - cur_queues, tx_rx_queue_left);
 -		vfres->num_queue_pairs = min_t(u16, max_allowed_vf_queues,
 -					       ICE_MAX_RSS_QS_PER_VF);
 -	} else {
 -		/* request is successful, then reset VF */
 -		vf->num_req_qs = req_queues;
 -		ice_reset_vf(vf, ICE_VF_RESET_NOTIFY);
 -		dev_info(dev, "VF %d granted request of %u queues.\n",
 -			 vf->vf_id, req_queues);
 -		return 0;
 -	}
 -
 -error_param:
 -	/* send the response to the VF */
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_REQUEST_QUEUES,
 -				     v_ret, (u8 *)vfres, sizeof(*vfres));
 -}
 -
 -/**
 - * ice_vf_vlan_offload_ena - determine if capabilities support VLAN offloads
 - * @caps: VF driver negotiated capabilities
 - *
 - * Return true if VIRTCHNL_VF_OFFLOAD_VLAN capability is set, else return false
 - */
 -static bool ice_vf_vlan_offload_ena(u32 caps)
 -{
 -	return !!(caps & VIRTCHNL_VF_OFFLOAD_VLAN);
 -}
 -
 -/**
 - * ice_is_vlan_promisc_allowed - check if VLAN promiscuous config is allowed
 - * @vf: VF used to determine if VLAN promiscuous config is allowed
 - */
 -static bool ice_is_vlan_promisc_allowed(struct ice_vf *vf)
 -{
 -	if ((test_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states) ||
 -	     test_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states)) &&
 -	    test_bit(ICE_FLAG_VF_TRUE_PROMISC_ENA, vf->pf->flags))
 -		return true;
 -
 -	return false;
 -}
 -
 -/**
 - * ice_vf_ena_vlan_promisc - Enable Tx/Rx VLAN promiscuous for the VLAN
 - * @vsi: VF's VSI used to enable VLAN promiscuous mode
 - * @vlan: VLAN used to enable VLAN promiscuous
 - *
 - * This function should only be called if VLAN promiscuous mode is allowed,
 - * which can be determined via ice_is_vlan_promisc_allowed().
 - */
 -static int ice_vf_ena_vlan_promisc(struct ice_vsi *vsi, struct ice_vlan *vlan)
 -{
 -	u8 promisc_m = ICE_PROMISC_VLAN_TX | ICE_PROMISC_VLAN_RX;
 -	int status;
 -
 -	status = ice_fltr_set_vsi_promisc(&vsi->back->hw, vsi->idx, promisc_m,
 -					  vlan->vid);
 -	if (status && status != -EEXIST)
 -		return status;
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vf_dis_vlan_promisc - Disable Tx/Rx VLAN promiscuous for the VLAN
 - * @vsi: VF's VSI used to disable VLAN promiscuous mode for
 - * @vlan: VLAN used to disable VLAN promiscuous
 - *
 - * This function should only be called if VLAN promiscuous mode is allowed,
 - * which can be determined via ice_is_vlan_promisc_allowed().
 - */
 -static int ice_vf_dis_vlan_promisc(struct ice_vsi *vsi, struct ice_vlan *vlan)
 -{
 -	u8 promisc_m = ICE_PROMISC_VLAN_TX | ICE_PROMISC_VLAN_RX;
 -	int status;
 -
 -	status = ice_fltr_clear_vsi_promisc(&vsi->back->hw, vsi->idx, promisc_m,
 -					    vlan->vid);
 -	if (status && status != -ENOENT)
 -		return status;
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vf_has_max_vlans - check if VF already has the max allowed VLAN filters
 - * @vf: VF to check against
 - * @vsi: VF's VSI
 - *
 - * If the VF is trusted then the VF is allowed to add as many VLANs as it
 - * wants to, so return false.
 - *
 - * When the VF is untrusted compare the number of non-zero VLANs + 1 to the max
 - * allowed VLANs for an untrusted VF. Return the result of this comparison.
 - */
 -static bool ice_vf_has_max_vlans(struct ice_vf *vf, struct ice_vsi *vsi)
 -{
 -	if (ice_is_vf_trusted(vf))
 -		return false;
 -
 -#define ICE_VF_ADDED_VLAN_ZERO_FLTRS	1
 -	return ((ice_vsi_num_non_zero_vlans(vsi) +
 -		ICE_VF_ADDED_VLAN_ZERO_FLTRS) >= ICE_MAX_VLAN_PER_VF);
 -}
 -
 -/**
 - * ice_vc_process_vlan_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - * @add_v: Add VLAN if true, otherwise delete VLAN
 - *
 - * Process virtchnl op to add or remove programmed guest VLAN ID
 - */
 -static int ice_vc_process_vlan_msg(struct ice_vf *vf, u8 *msg, bool add_v)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vlan_filter_list *vfl =
 -	    (struct virtchnl_vlan_filter_list *)msg;
 -	struct ice_pf *pf = vf->pf;
 -	bool vlan_promisc = false;
 -	struct ice_vsi *vsi;
 -	struct device *dev;
 -	int status = 0;
 -	int i;
 -
 -	dev = ice_pf_to_dev(pf);
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vf_vlan_offload_ena(vf->driver_caps)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vfl->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	for (i = 0; i < vfl->num_elements; i++) {
 -		if (vfl->vlan_id[i] >= VLAN_N_VID) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			dev_err(dev, "invalid VF VLAN id %d\n",
 -				vfl->vlan_id[i]);
 -			goto error_param;
 -		}
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (add_v && ice_vf_has_max_vlans(vf, vsi)) {
 -		dev_info(dev, "VF-%d is not trusted, switch the VF to trusted mode, in order to add more VLAN addresses\n",
 -			 vf->vf_id);
 -		/* There is no need to let VF know about being not trusted,
 -		 * so we can just return success message here
 -		 */
 -		goto error_param;
 -	}
 -
 -	/* in DVM a VF can add/delete inner VLAN filters when
 -	 * VIRTCHNL_VF_OFFLOAD_VLAN is negotiated, so only reject in SVM
 -	 */
 -	if (ice_vf_is_port_vlan_ena(vf) && !ice_is_dvm_ena(&pf->hw)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	/* in DVM VLAN promiscuous is based on the outer VLAN, which would be
 -	 * the port VLAN if VIRTCHNL_VF_OFFLOAD_VLAN was negotiated, so only
 -	 * allow vlan_promisc = true in SVM and if no port VLAN is configured
 -	 */
 -	vlan_promisc = ice_is_vlan_promisc_allowed(vf) &&
 -		!ice_is_dvm_ena(&pf->hw) &&
 -		!ice_vf_is_port_vlan_ena(vf);
 -
 -	if (add_v) {
 -		for (i = 0; i < vfl->num_elements; i++) {
 -			u16 vid = vfl->vlan_id[i];
 -			struct ice_vlan vlan;
 -
 -			if (ice_vf_has_max_vlans(vf, vsi)) {
 -				dev_info(dev, "VF-%d is not trusted, switch the VF to trusted mode, in order to add more VLAN addresses\n",
 -					 vf->vf_id);
 -				/* There is no need to let VF know about being
 -				 * not trusted, so we can just return success
 -				 * message here as well.
 -				 */
 -				goto error_param;
 -			}
 -
 -			/* we add VLAN 0 by default for each VF so we can enable
 -			 * Tx VLAN anti-spoof without triggering MDD events so
 -			 * we don't need to add it again here
 -			 */
 -			if (!vid)
 -				continue;
 -
 -			vlan = ICE_VLAN(ETH_P_8021Q, vid, 0);
 -			status = vsi->inner_vlan_ops.add_vlan(vsi, &vlan);
 -			if (status) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -
 -			/* Enable VLAN filtering on first non-zero VLAN */
 -			if (!vlan_promisc && vid && !ice_is_dvm_ena(&pf->hw)) {
 -				if (vsi->inner_vlan_ops.ena_rx_filtering(vsi)) {
 -					v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -					dev_err(dev, "Enable VLAN pruning on VLAN ID: %d failed error-%d\n",
 -						vid, status);
 -					goto error_param;
 -				}
 -			} else if (vlan_promisc) {
 -				status = ice_vf_ena_vlan_promisc(vsi, &vlan);
 -				if (status) {
 -					v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -					dev_err(dev, "Enable Unicast/multicast promiscuous mode on VLAN ID:%d failed error-%d\n",
 -						vid, status);
 -				}
 -			}
 -		}
 -	} else {
 -		/* In case of non_trusted VF, number of VLAN elements passed
 -		 * to PF for removal might be greater than number of VLANs
 -		 * filter programmed for that VF - So, use actual number of
 -		 * VLANS added earlier with add VLAN opcode. In order to avoid
 -		 * removing VLAN that doesn't exist, which result to sending
 -		 * erroneous failed message back to the VF
 -		 */
 -		int num_vf_vlan;
 -
 -		num_vf_vlan = vsi->num_vlan;
 -		for (i = 0; i < vfl->num_elements && i < num_vf_vlan; i++) {
 -			u16 vid = vfl->vlan_id[i];
 -			struct ice_vlan vlan;
 -
 -			/* we add VLAN 0 by default for each VF so we can enable
 -			 * Tx VLAN anti-spoof without triggering MDD events so
 -			 * we don't want a VIRTCHNL request to remove it
 -			 */
 -			if (!vid)
 -				continue;
 -
 -			vlan = ICE_VLAN(ETH_P_8021Q, vid, 0);
 -			status = vsi->inner_vlan_ops.del_vlan(vsi, &vlan);
 -			if (status) {
 -				v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -				goto error_param;
 -			}
 -
 -			/* Disable VLAN filtering when only VLAN 0 is left */
 -			if (!ice_vsi_has_non_zero_vlans(vsi))
 -				vsi->inner_vlan_ops.dis_rx_filtering(vsi);
 -
 -			if (vlan_promisc)
 -				ice_vf_dis_vlan_promisc(vsi, &vlan);
 -		}
 -	}
 -
 -error_param:
 -	/* send the response to the VF */
 -	if (add_v)
 -		return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ADD_VLAN, v_ret,
 -					     NULL, 0);
 -	else
 -		return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DEL_VLAN, v_ret,
 -					     NULL, 0);
 -}
 -
 -/**
 - * ice_vc_add_vlan_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * Add and program guest VLAN ID
 - */
 -static int ice_vc_add_vlan_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	return ice_vc_process_vlan_msg(vf, msg, true);
 -}
 -
 -/**
 - * ice_vc_remove_vlan_msg
 - * @vf: pointer to the VF info
 - * @msg: pointer to the msg buffer
 - *
 - * remove programmed guest VLAN ID
 - */
 -static int ice_vc_remove_vlan_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	return ice_vc_process_vlan_msg(vf, msg, false);
 -}
 -
 -/**
 - * ice_vc_ena_vlan_stripping
 - * @vf: pointer to the VF info
 - *
 - * Enable VLAN header stripping for a given VF
 - */
 -static int ice_vc_ena_vlan_stripping(struct ice_vf *vf)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vf_vlan_offload_ena(vf->driver_caps)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (vsi->inner_vlan_ops.ena_stripping(vsi, ETH_P_8021Q))
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -
 -error_param:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_STRIPPING,
 -				     v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_dis_vlan_stripping
 - * @vf: pointer to the VF info
 - *
 - * Disable VLAN header stripping for a given VF
 - */
 -static int ice_vc_dis_vlan_stripping(struct ice_vf *vf)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (!ice_vf_vlan_offload_ena(vf->driver_caps)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto error_param;
 -	}
 -
 -	if (vsi->inner_vlan_ops.dis_stripping(vsi))
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -
 -error_param:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_STRIPPING,
 -				     v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vf_init_vlan_stripping - enable/disable VLAN stripping on initialization
 - * @vf: VF to enable/disable VLAN stripping for on initialization
 - *
 - * Set the default for VLAN stripping based on whether a port VLAN is configured
 - * and the current VLAN mode of the device.
 - */
 -static int ice_vf_init_vlan_stripping(struct ice_vf *vf)
 -{
 -	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
 -
 -	if (!vsi)
 -		return -EINVAL;
 -
 -	/* don't modify stripping if port VLAN is configured in SVM since the
 -	 * port VLAN is based on the inner/single VLAN in SVM
 -	 */
 -	if (ice_vf_is_port_vlan_ena(vf) && !ice_is_dvm_ena(&vsi->back->hw))
 -		return 0;
 -
 -	if (ice_vf_vlan_offload_ena(vf->driver_caps))
 -		return vsi->inner_vlan_ops.ena_stripping(vsi, ETH_P_8021Q);
 -	else
 -		return vsi->inner_vlan_ops.dis_stripping(vsi);
 -}
 -
 -static u16 ice_vc_get_max_vlan_fltrs(struct ice_vf *vf)
 -{
 -	if (vf->trusted)
 -		return VLAN_N_VID;
 -	else
 -		return ICE_MAX_VLAN_PER_VF;
 -}
 -
 -/**
 - * ice_vf_outer_vlan_not_allowed - check if outer VLAN can be used
 - * @vf: VF that being checked for
 - *
 - * When the device is in double VLAN mode, check whether or not the outer VLAN
 - * is allowed.
 - */
 -static bool ice_vf_outer_vlan_not_allowed(struct ice_vf *vf)
 -{
 -	if (ice_vf_is_port_vlan_ena(vf))
 -		return true;
 -
 -	return false;
 -}
 -
 -/**
 - * ice_vc_set_dvm_caps - set VLAN capabilities when the device is in DVM
 - * @vf: VF that capabilities are being set for
 - * @caps: VLAN capabilities to populate
 - *
 - * Determine VLAN capabilities support based on whether a port VLAN is
 - * configured. If a port VLAN is configured then the VF should use the inner
 - * filtering/offload capabilities since the port VLAN is using the outer VLAN
 - * capabilies.
 - */
 -static void
 -ice_vc_set_dvm_caps(struct ice_vf *vf, struct virtchnl_vlan_caps *caps)
 -{
 -	struct virtchnl_vlan_supported_caps *supported_caps;
 -
 -	if (ice_vf_outer_vlan_not_allowed(vf)) {
 -		/* until support for inner VLAN filtering is added when a port
 -		 * VLAN is configured, only support software offloaded inner
 -		 * VLANs when a port VLAN is confgured in DVM
 -		 */
 -		supported_caps = &caps->filtering.filtering_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		supported_caps = &caps->offloads.stripping_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		supported_caps = &caps->offloads.insertion_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		caps->offloads.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
 -		caps->offloads.ethertype_match =
 -			VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION;
 -	} else {
 -		supported_caps = &caps->filtering.filtering_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
 -		supported_caps->outer = VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_ETHERTYPE_88A8 |
 -					VIRTCHNL_VLAN_ETHERTYPE_9100 |
 -					VIRTCHNL_VLAN_ETHERTYPE_AND;
 -		caps->filtering.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -						 VIRTCHNL_VLAN_ETHERTYPE_88A8 |
 -						 VIRTCHNL_VLAN_ETHERTYPE_9100;
 -
 -		supported_caps = &caps->offloads.stripping_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
 -		supported_caps->outer = VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_ETHERTYPE_88A8 |
 -					VIRTCHNL_VLAN_ETHERTYPE_9100 |
 -					VIRTCHNL_VLAN_ETHERTYPE_XOR |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2_2;
 -
 -		supported_caps = &caps->offloads.insertion_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
 -		supported_caps->outer = VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_ETHERTYPE_88A8 |
 -					VIRTCHNL_VLAN_ETHERTYPE_9100 |
 -					VIRTCHNL_VLAN_ETHERTYPE_XOR |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2;
 -
 -		caps->offloads.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
 -
 -		caps->offloads.ethertype_match =
 -			VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION;
 -	}
 -
 -	caps->filtering.max_filters = ice_vc_get_max_vlan_fltrs(vf);
 -}
 -
 -/**
 - * ice_vc_set_svm_caps - set VLAN capabilities when the device is in SVM
 - * @vf: VF that capabilities are being set for
 - * @caps: VLAN capabilities to populate
 - *
 - * Determine VLAN capabilities support based on whether a port VLAN is
 - * configured. If a port VLAN is configured then the VF does not have any VLAN
 - * filtering or offload capabilities since the port VLAN is using the inner VLAN
 - * capabilities in single VLAN mode (SVM). Otherwise allow the VF to use inner
 - * VLAN fitlering and offload capabilities.
 - */
 -static void
 -ice_vc_set_svm_caps(struct ice_vf *vf, struct virtchnl_vlan_caps *caps)
 -{
 -	struct virtchnl_vlan_supported_caps *supported_caps;
 -
 -	if (ice_vf_is_port_vlan_ena(vf)) {
 -		supported_caps = &caps->filtering.filtering_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		supported_caps = &caps->offloads.stripping_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		supported_caps = &caps->offloads.insertion_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_UNSUPPORTED;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		caps->offloads.ethertype_init = VIRTCHNL_VLAN_UNSUPPORTED;
 -		caps->offloads.ethertype_match = VIRTCHNL_VLAN_UNSUPPORTED;
 -		caps->filtering.max_filters = 0;
 -	} else {
 -		supported_caps = &caps->filtering.filtering_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -		caps->filtering.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
 -
 -		supported_caps = &caps->offloads.stripping_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		supported_caps = &caps->offloads.insertion_support;
 -		supported_caps->inner = VIRTCHNL_VLAN_ETHERTYPE_8100 |
 -					VIRTCHNL_VLAN_TOGGLE |
 -					VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1;
 -		supported_caps->outer = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -		caps->offloads.ethertype_init = VIRTCHNL_VLAN_ETHERTYPE_8100;
 -		caps->offloads.ethertype_match =
 -			VIRTCHNL_ETHERTYPE_STRIPPING_MATCHES_INSERTION;
 -		caps->filtering.max_filters = ice_vc_get_max_vlan_fltrs(vf);
 -	}
 -}
 -
 -/**
 - * ice_vc_get_offload_vlan_v2_caps - determine VF's VLAN capabilities
 - * @vf: VF to determine VLAN capabilities for
 - *
 - * This will only be called if the VF and PF successfully negotiated
 - * VIRTCHNL_VF_OFFLOAD_VLAN_V2.
 - *
 - * Set VLAN capabilities based on the current VLAN mode and whether a port VLAN
 - * is configured or not.
 - */
 -static int ice_vc_get_offload_vlan_v2_caps(struct ice_vf *vf)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vlan_caps *caps = NULL;
 -	int err, len = 0;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	caps = kzalloc(sizeof(*caps), GFP_KERNEL);
 -	if (!caps) {
 -		v_ret = VIRTCHNL_STATUS_ERR_NO_MEMORY;
 -		goto out;
 -	}
 -	len = sizeof(*caps);
 -
 -	if (ice_is_dvm_ena(&vf->pf->hw))
 -		ice_vc_set_dvm_caps(vf, caps);
 -	else
 -		ice_vc_set_svm_caps(vf, caps);
 -
 -	/* store negotiated caps to prevent invalid VF messages */
 -	memcpy(&vf->vlan_v2_caps, caps, sizeof(*caps));
 -
 -out:
 -	err = ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS,
 -				    v_ret, (u8 *)caps, len);
 -	kfree(caps);
 -	return err;
 -}
 -
 -/**
 - * ice_vc_validate_vlan_tpid - validate VLAN TPID
 - * @filtering_caps: negotiated/supported VLAN filtering capabilities
 - * @tpid: VLAN TPID used for validation
 - *
 - * Convert the VLAN TPID to a VIRTCHNL_VLAN_ETHERTYPE_* and then compare against
 - * the negotiated/supported filtering caps to see if the VLAN TPID is valid.
 - */
 -static bool ice_vc_validate_vlan_tpid(u16 filtering_caps, u16 tpid)
 -{
 -	enum virtchnl_vlan_support vlan_ethertype = VIRTCHNL_VLAN_UNSUPPORTED;
 -
 -	switch (tpid) {
 -	case ETH_P_8021Q:
 -		vlan_ethertype = VIRTCHNL_VLAN_ETHERTYPE_8100;
 -		break;
 -	case ETH_P_8021AD:
 -		vlan_ethertype = VIRTCHNL_VLAN_ETHERTYPE_88A8;
 -		break;
 -	case ETH_P_QINQ1:
 -		vlan_ethertype = VIRTCHNL_VLAN_ETHERTYPE_9100;
 -		break;
 -	}
 -
 -	if (!(filtering_caps & vlan_ethertype))
 -		return false;
 -
 -	return true;
 -}
 -
 -/**
 - * ice_vc_is_valid_vlan - validate the virtchnl_vlan
 - * @vc_vlan: virtchnl_vlan to validate
 - *
 - * If the VLAN TCI and VLAN TPID are 0, then this filter is invalid, so return
 - * false. Otherwise return true.
 - */
 -static bool ice_vc_is_valid_vlan(struct virtchnl_vlan *vc_vlan)
 -{
 -	if (!vc_vlan->tci || !vc_vlan->tpid)
 -		return false;
 -
 -	return true;
 -}
 -
 -/**
 - * ice_vc_validate_vlan_filter_list - validate the filter list from the VF
 - * @vfc: negotiated/supported VLAN filtering capabilities
 - * @vfl: VLAN filter list from VF to validate
 - *
 - * Validate all of the filters in the VLAN filter list from the VF. If any of
 - * the checks fail then return false. Otherwise return true.
 - */
 -static bool
 -ice_vc_validate_vlan_filter_list(struct virtchnl_vlan_filtering_caps *vfc,
 -				 struct virtchnl_vlan_filter_list_v2 *vfl)
 -{
 -	u16 i;
 -
 -	if (!vfl->num_elements)
 -		return false;
 -
 -	for (i = 0; i < vfl->num_elements; i++) {
 -		struct virtchnl_vlan_supported_caps *filtering_support =
 -			&vfc->filtering_support;
 -		struct virtchnl_vlan_filter *vlan_fltr = &vfl->filters[i];
 -		struct virtchnl_vlan *outer = &vlan_fltr->outer;
 -		struct virtchnl_vlan *inner = &vlan_fltr->inner;
 -
 -		if ((ice_vc_is_valid_vlan(outer) &&
 -		     filtering_support->outer == VIRTCHNL_VLAN_UNSUPPORTED) ||
 -		    (ice_vc_is_valid_vlan(inner) &&
 -		     filtering_support->inner == VIRTCHNL_VLAN_UNSUPPORTED))
 -			return false;
 -
 -		if ((outer->tci_mask &&
 -		     !(filtering_support->outer & VIRTCHNL_VLAN_FILTER_MASK)) ||
 -		    (inner->tci_mask &&
 -		     !(filtering_support->inner & VIRTCHNL_VLAN_FILTER_MASK)))
 -			return false;
 -
 -		if (((outer->tci & VLAN_PRIO_MASK) &&
 -		     !(filtering_support->outer & VIRTCHNL_VLAN_PRIO)) ||
 -		    ((inner->tci & VLAN_PRIO_MASK) &&
 -		     !(filtering_support->inner & VIRTCHNL_VLAN_PRIO)))
 -			return false;
 -
 -		if ((ice_vc_is_valid_vlan(outer) &&
 -		     !ice_vc_validate_vlan_tpid(filtering_support->outer,
 -						outer->tpid)) ||
 -		    (ice_vc_is_valid_vlan(inner) &&
 -		     !ice_vc_validate_vlan_tpid(filtering_support->inner,
 -						inner->tpid)))
 -			return false;
 -	}
 -
 -	return true;
 -}
 -
 -/**
 - * ice_vc_to_vlan - transform from struct virtchnl_vlan to struct ice_vlan
 - * @vc_vlan: struct virtchnl_vlan to transform
 - */
 -static struct ice_vlan ice_vc_to_vlan(struct virtchnl_vlan *vc_vlan)
 -{
 -	struct ice_vlan vlan = { 0 };
 -
 -	vlan.prio = (vc_vlan->tci & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
 -	vlan.vid = vc_vlan->tci & VLAN_VID_MASK;
 -	vlan.tpid = vc_vlan->tpid;
 -
 -	return vlan;
 -}
 -
 -/**
 - * ice_vc_vlan_action - action to perform on the virthcnl_vlan
 - * @vsi: VF's VSI used to perform the action
 - * @vlan_action: function to perform the action with (i.e. add/del)
 - * @vlan: VLAN filter to perform the action with
 - */
 -static int
 -ice_vc_vlan_action(struct ice_vsi *vsi,
 -		   int (*vlan_action)(struct ice_vsi *, struct ice_vlan *),
 -		   struct ice_vlan *vlan)
 -{
 -	int err;
 -
 -	err = vlan_action(vsi, vlan);
 -	if (err)
 -		return err;
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vc_del_vlans - delete VLAN(s) from the virtchnl filter list
 - * @vf: VF used to delete the VLAN(s)
 - * @vsi: VF's VSI used to delete the VLAN(s)
 - * @vfl: virthchnl filter list used to delete the filters
 - */
 -static int
 -ice_vc_del_vlans(struct ice_vf *vf, struct ice_vsi *vsi,
 -		 struct virtchnl_vlan_filter_list_v2 *vfl)
 -{
 -	bool vlan_promisc = ice_is_vlan_promisc_allowed(vf);
 -	int err;
 -	u16 i;
 -
 -	for (i = 0; i < vfl->num_elements; i++) {
 -		struct virtchnl_vlan_filter *vlan_fltr = &vfl->filters[i];
 -		struct virtchnl_vlan *vc_vlan;
 -
 -		vc_vlan = &vlan_fltr->outer;
 -		if (ice_vc_is_valid_vlan(vc_vlan)) {
 -			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
 -
 -			err = ice_vc_vlan_action(vsi,
 -						 vsi->outer_vlan_ops.del_vlan,
 -						 &vlan);
 -			if (err)
 -				return err;
 -
 -			if (vlan_promisc)
 -				ice_vf_dis_vlan_promisc(vsi, &vlan);
 -		}
 -
 -		vc_vlan = &vlan_fltr->inner;
 -		if (ice_vc_is_valid_vlan(vc_vlan)) {
 -			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
 -
 -			err = ice_vc_vlan_action(vsi,
 -						 vsi->inner_vlan_ops.del_vlan,
 -						 &vlan);
 -			if (err)
 -				return err;
 -
 -			/* no support for VLAN promiscuous on inner VLAN unless
 -			 * we are in Single VLAN Mode (SVM)
 -			 */
 -			if (!ice_is_dvm_ena(&vsi->back->hw) && vlan_promisc)
 -				ice_vf_dis_vlan_promisc(vsi, &vlan);
 -		}
 -	}
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vc_remove_vlan_v2_msg - virtchnl handler for VIRTCHNL_OP_DEL_VLAN_V2
 - * @vf: VF the message was received from
 - * @msg: message received from the VF
 - */
 -static int ice_vc_remove_vlan_v2_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	struct virtchnl_vlan_filter_list_v2 *vfl =
 -		(struct virtchnl_vlan_filter_list_v2 *)msg;
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct ice_vsi *vsi;
 -
 -	if (!ice_vc_validate_vlan_filter_list(&vf->vlan_v2_caps.filtering,
 -					      vfl)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vfl->vport_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (ice_vc_del_vlans(vf, vsi, vfl))
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -
 -out:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DEL_VLAN_V2, v_ret, NULL,
 -				     0);
 -}
 -
 -/**
 - * ice_vc_add_vlans - add VLAN(s) from the virtchnl filter list
 - * @vf: VF used to add the VLAN(s)
 - * @vsi: VF's VSI used to add the VLAN(s)
 - * @vfl: virthchnl filter list used to add the filters
 - */
 -static int
 -ice_vc_add_vlans(struct ice_vf *vf, struct ice_vsi *vsi,
 -		 struct virtchnl_vlan_filter_list_v2 *vfl)
 -{
 -	bool vlan_promisc = ice_is_vlan_promisc_allowed(vf);
 -	int err;
 -	u16 i;
 -
 -	for (i = 0; i < vfl->num_elements; i++) {
 -		struct virtchnl_vlan_filter *vlan_fltr = &vfl->filters[i];
 -		struct virtchnl_vlan *vc_vlan;
 -
 -		vc_vlan = &vlan_fltr->outer;
 -		if (ice_vc_is_valid_vlan(vc_vlan)) {
 -			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
 -
 -			err = ice_vc_vlan_action(vsi,
 -						 vsi->outer_vlan_ops.add_vlan,
 -						 &vlan);
 -			if (err)
 -				return err;
 -
 -			if (vlan_promisc) {
 -				err = ice_vf_ena_vlan_promisc(vsi, &vlan);
 -				if (err)
 -					return err;
 -			}
 -		}
 -
 -		vc_vlan = &vlan_fltr->inner;
 -		if (ice_vc_is_valid_vlan(vc_vlan)) {
 -			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
 -
 -			err = ice_vc_vlan_action(vsi,
 -						 vsi->inner_vlan_ops.add_vlan,
 -						 &vlan);
 -			if (err)
 -				return err;
 -
 -			/* no support for VLAN promiscuous on inner VLAN unless
 -			 * we are in Single VLAN Mode (SVM)
 -			 */
 -			if (!ice_is_dvm_ena(&vsi->back->hw) && vlan_promisc) {
 -				err = ice_vf_ena_vlan_promisc(vsi, &vlan);
 -				if (err)
 -					return err;
 -			}
 -		}
 -	}
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vc_validate_add_vlan_filter_list - validate add filter list from the VF
 - * @vsi: VF VSI used to get number of existing VLAN filters
 - * @vfc: negotiated/supported VLAN filtering capabilities
 - * @vfl: VLAN filter list from VF to validate
 - *
 - * Validate all of the filters in the VLAN filter list from the VF during the
 - * VIRTCHNL_OP_ADD_VLAN_V2 opcode. If any of the checks fail then return false.
 - * Otherwise return true.
 - */
 -static bool
 -ice_vc_validate_add_vlan_filter_list(struct ice_vsi *vsi,
 -				     struct virtchnl_vlan_filtering_caps *vfc,
 -				     struct virtchnl_vlan_filter_list_v2 *vfl)
 -{
 -	u16 num_requested_filters = vsi->num_vlan + vfl->num_elements;
 -
 -	if (num_requested_filters > vfc->max_filters)
 -		return false;
 -
 -	return ice_vc_validate_vlan_filter_list(vfc, vfl);
 -}
 -
 -/**
 - * ice_vc_add_vlan_v2_msg - virtchnl handler for VIRTCHNL_OP_ADD_VLAN_V2
 - * @vf: VF the message was received from
 - * @msg: message received from the VF
 - */
 -static int ice_vc_add_vlan_v2_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vlan_filter_list_v2 *vfl =
 -		(struct virtchnl_vlan_filter_list_v2 *)msg;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, vfl->vport_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (!ice_vc_validate_add_vlan_filter_list(vsi,
 -						  &vf->vlan_v2_caps.filtering,
 -						  vfl)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (ice_vc_add_vlans(vf, vsi, vfl))
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -
 -out:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ADD_VLAN_V2, v_ret, NULL,
 -				     0);
 -}
 -
 -/**
 - * ice_vc_valid_vlan_setting - validate VLAN setting
 - * @negotiated_settings: negotiated VLAN settings during VF init
 - * @ethertype_setting: ethertype(s) requested for the VLAN setting
 - */
 -static bool
 -ice_vc_valid_vlan_setting(u32 negotiated_settings, u32 ethertype_setting)
 -{
 -	if (ethertype_setting && !(negotiated_settings & ethertype_setting))
 -		return false;
 -
 -	/* only allow a single VIRTCHNL_VLAN_ETHERTYPE if
 -	 * VIRTHCNL_VLAN_ETHERTYPE_AND is not negotiated/supported
 -	 */
 -	if (!(negotiated_settings & VIRTCHNL_VLAN_ETHERTYPE_AND) &&
 -	    hweight32(ethertype_setting) > 1)
 -		return false;
 -
 -	/* ability to modify the VLAN setting was not negotiated */
 -	if (!(negotiated_settings & VIRTCHNL_VLAN_TOGGLE))
 -		return false;
 -
 -	return true;
 -}
 -
 -/**
 - * ice_vc_valid_vlan_setting_msg - validate the VLAN setting message
 - * @caps: negotiated VLAN settings during VF init
 - * @msg: message to validate
 - *
 - * Used to validate any VLAN virtchnl message sent as a
 - * virtchnl_vlan_setting structure. Validates the message against the
 - * negotiated/supported caps during VF driver init.
 - */
 -static bool
 -ice_vc_valid_vlan_setting_msg(struct virtchnl_vlan_supported_caps *caps,
 -			      struct virtchnl_vlan_setting *msg)
 -{
 -	if ((!msg->outer_ethertype_setting &&
 -	     !msg->inner_ethertype_setting) ||
 -	    (!caps->outer && !caps->inner))
 -		return false;
 -
 -	if (msg->outer_ethertype_setting &&
 -	    !ice_vc_valid_vlan_setting(caps->outer,
 -				       msg->outer_ethertype_setting))
 -		return false;
 -
 -	if (msg->inner_ethertype_setting &&
 -	    !ice_vc_valid_vlan_setting(caps->inner,
 -				       msg->inner_ethertype_setting))
 -		return false;
 -
 -	return true;
 -}
 -
 -/**
 - * ice_vc_get_tpid - transform from VIRTCHNL_VLAN_ETHERTYPE_* to VLAN TPID
 - * @ethertype_setting: VIRTCHNL_VLAN_ETHERTYPE_* used to get VLAN TPID
 - * @tpid: VLAN TPID to populate
 - */
 -static int ice_vc_get_tpid(u32 ethertype_setting, u16 *tpid)
 -{
 -	switch (ethertype_setting) {
 -	case VIRTCHNL_VLAN_ETHERTYPE_8100:
 -		*tpid = ETH_P_8021Q;
 -		break;
 -	case VIRTCHNL_VLAN_ETHERTYPE_88A8:
 -		*tpid = ETH_P_8021AD;
 -		break;
 -	case VIRTCHNL_VLAN_ETHERTYPE_9100:
 -		*tpid = ETH_P_QINQ1;
 -		break;
 -	default:
 -		*tpid = 0;
 -		return -EINVAL;
 -	}
 -
 -	return 0;
 -}
 -
 -/**
 - * ice_vc_ena_vlan_offload - enable VLAN offload based on the ethertype_setting
 - * @vsi: VF's VSI used to enable the VLAN offload
 - * @ena_offload: function used to enable the VLAN offload
 - * @ethertype_setting: VIRTCHNL_VLAN_ETHERTYPE_* to enable offloads for
 - */
 -static int
 -ice_vc_ena_vlan_offload(struct ice_vsi *vsi,
 -			int (*ena_offload)(struct ice_vsi *vsi, u16 tpid),
 -			u32 ethertype_setting)
 -{
 -	u16 tpid;
 -	int err;
 -
 -	err = ice_vc_get_tpid(ethertype_setting, &tpid);
 -	if (err)
 -		return err;
 -
 -	err = ena_offload(vsi, tpid);
 -	if (err)
 -		return err;
 -
 -	return 0;
 -}
 -
 -#define ICE_L2TSEL_QRX_CONTEXT_REG_IDX	3
 -#define ICE_L2TSEL_BIT_OFFSET		23
 -enum ice_l2tsel {
 -	ICE_L2TSEL_EXTRACT_FIRST_TAG_L2TAG2_2ND,
 -	ICE_L2TSEL_EXTRACT_FIRST_TAG_L2TAG1,
 -};
 -
 -/**
 - * ice_vsi_update_l2tsel - update l2tsel field for all Rx rings on this VSI
 - * @vsi: VSI used to update l2tsel on
 - * @l2tsel: l2tsel setting requested
 - *
 - * Use the l2tsel setting to update all of the Rx queue context bits for l2tsel.
 - * This will modify which descriptor field the first offloaded VLAN will be
 - * stripped into.
 - */
 -static void ice_vsi_update_l2tsel(struct ice_vsi *vsi, enum ice_l2tsel l2tsel)
 -{
 -	struct ice_hw *hw = &vsi->back->hw;
 -	u32 l2tsel_bit;
 -	int i;
 -
 -	if (l2tsel == ICE_L2TSEL_EXTRACT_FIRST_TAG_L2TAG2_2ND)
 -		l2tsel_bit = 0;
 -	else
 -		l2tsel_bit = BIT(ICE_L2TSEL_BIT_OFFSET);
 -
 -	for (i = 0; i < vsi->alloc_rxq; i++) {
 -		u16 pfq = vsi->rxq_map[i];
 -		u32 qrx_context_offset;
 -		u32 regval;
 -
 -		qrx_context_offset =
 -			QRX_CONTEXT(ICE_L2TSEL_QRX_CONTEXT_REG_IDX, pfq);
 -
 -		regval = rd32(hw, qrx_context_offset);
 -		regval &= ~BIT(ICE_L2TSEL_BIT_OFFSET);
 -		regval |= l2tsel_bit;
 -		wr32(hw, qrx_context_offset, regval);
 -	}
 -}
 -
 -/**
 - * ice_vc_ena_vlan_stripping_v2_msg
 - * @vf: VF the message was received from
 - * @msg: message received from the VF
 - *
 - * virthcnl handler for VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2
 - */
 -static int ice_vc_ena_vlan_stripping_v2_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vlan_supported_caps *stripping_support;
 -	struct virtchnl_vlan_setting *strip_msg =
 -		(struct virtchnl_vlan_setting *)msg;
 -	u32 ethertype_setting;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, strip_msg->vport_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	stripping_support = &vf->vlan_v2_caps.offloads.stripping_support;
 -	if (!ice_vc_valid_vlan_setting_msg(stripping_support, strip_msg)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	ethertype_setting = strip_msg->outer_ethertype_setting;
 -	if (ethertype_setting) {
 -		if (ice_vc_ena_vlan_offload(vsi,
 -					    vsi->outer_vlan_ops.ena_stripping,
 -					    ethertype_setting)) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto out;
 -		} else {
 -			enum ice_l2tsel l2tsel =
 -				ICE_L2TSEL_EXTRACT_FIRST_TAG_L2TAG2_2ND;
 -
 -			/* PF tells the VF that the outer VLAN tag is always
 -			 * extracted to VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2_2 and
 -			 * inner is always extracted to
 -			 * VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1. This is needed to
 -			 * support outer stripping so the first tag always ends
 -			 * up in L2TAG2_2ND and the second/inner tag, if
 -			 * enabled, is extracted in L2TAG1.
 -			 */
 -			ice_vsi_update_l2tsel(vsi, l2tsel);
 -		}
 -	}
 -
 -	ethertype_setting = strip_msg->inner_ethertype_setting;
 -	if (ethertype_setting &&
 -	    ice_vc_ena_vlan_offload(vsi, vsi->inner_vlan_ops.ena_stripping,
 -				    ethertype_setting)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -out:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2,
 -				     v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_dis_vlan_stripping_v2_msg
 - * @vf: VF the message was received from
 - * @msg: message received from the VF
 - *
 - * virthcnl handler for VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2
 - */
 -static int ice_vc_dis_vlan_stripping_v2_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vlan_supported_caps *stripping_support;
 -	struct virtchnl_vlan_setting *strip_msg =
 -		(struct virtchnl_vlan_setting *)msg;
 -	u32 ethertype_setting;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, strip_msg->vport_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	stripping_support = &vf->vlan_v2_caps.offloads.stripping_support;
 -	if (!ice_vc_valid_vlan_setting_msg(stripping_support, strip_msg)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	ethertype_setting = strip_msg->outer_ethertype_setting;
 -	if (ethertype_setting) {
 -		if (vsi->outer_vlan_ops.dis_stripping(vsi)) {
 -			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -			goto out;
 -		} else {
 -			enum ice_l2tsel l2tsel =
 -				ICE_L2TSEL_EXTRACT_FIRST_TAG_L2TAG1;
 -
 -			/* PF tells the VF that the outer VLAN tag is always
 -			 * extracted to VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2_2 and
 -			 * inner is always extracted to
 -			 * VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1. This is needed to
 -			 * support inner stripping while outer stripping is
 -			 * disabled so that the first and only tag is extracted
 -			 * in L2TAG1.
 -			 */
 -			ice_vsi_update_l2tsel(vsi, l2tsel);
 -		}
 -	}
 -
 -	ethertype_setting = strip_msg->inner_ethertype_setting;
 -	if (ethertype_setting && vsi->inner_vlan_ops.dis_stripping(vsi)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -out:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2,
 -				     v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_ena_vlan_insertion_v2_msg
 - * @vf: VF the message was received from
 - * @msg: message received from the VF
 - *
 - * virthcnl handler for VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2
 - */
 -static int ice_vc_ena_vlan_insertion_v2_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vlan_supported_caps *insertion_support;
 -	struct virtchnl_vlan_setting *insertion_msg =
 -		(struct virtchnl_vlan_setting *)msg;
 -	u32 ethertype_setting;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, insertion_msg->vport_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	insertion_support = &vf->vlan_v2_caps.offloads.insertion_support;
 -	if (!ice_vc_valid_vlan_setting_msg(insertion_support, insertion_msg)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	ethertype_setting = insertion_msg->outer_ethertype_setting;
 -	if (ethertype_setting &&
 -	    ice_vc_ena_vlan_offload(vsi, vsi->outer_vlan_ops.ena_insertion,
 -				    ethertype_setting)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	ethertype_setting = insertion_msg->inner_ethertype_setting;
 -	if (ethertype_setting &&
 -	    ice_vc_ena_vlan_offload(vsi, vsi->inner_vlan_ops.ena_insertion,
 -				    ethertype_setting)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -out:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2,
 -				     v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_dis_vlan_insertion_v2_msg
 - * @vf: VF the message was received from
 - * @msg: message received from the VF
 - *
 - * virthcnl handler for VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2
 - */
 -static int ice_vc_dis_vlan_insertion_v2_msg(struct ice_vf *vf, u8 *msg)
 -{
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_vlan_supported_caps *insertion_support;
 -	struct virtchnl_vlan_setting *insertion_msg =
 -		(struct virtchnl_vlan_setting *)msg;
 -	u32 ethertype_setting;
 -	struct ice_vsi *vsi;
 -
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	if (!ice_vc_isvalid_vsi_id(vf, insertion_msg->vport_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	insertion_support = &vf->vlan_v2_caps.offloads.insertion_support;
 -	if (!ice_vc_valid_vlan_setting_msg(insertion_support, insertion_msg)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	ethertype_setting = insertion_msg->outer_ethertype_setting;
 -	if (ethertype_setting && vsi->outer_vlan_ops.dis_insertion(vsi)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -	ethertype_setting = insertion_msg->inner_ethertype_setting;
 -	if (ethertype_setting && vsi->inner_vlan_ops.dis_insertion(vsi)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto out;
 -	}
 -
 -out:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2,
 -				     v_ret, NULL, 0);
 -}
 -
 -static const struct ice_virtchnl_ops ice_virtchnl_dflt_ops = {
 -	.get_ver_msg = ice_vc_get_ver_msg,
 -	.get_vf_res_msg = ice_vc_get_vf_res_msg,
 -	.reset_vf = ice_vc_reset_vf_msg,
 -	.add_mac_addr_msg = ice_vc_add_mac_addr_msg,
 -	.del_mac_addr_msg = ice_vc_del_mac_addr_msg,
 -	.cfg_qs_msg = ice_vc_cfg_qs_msg,
 -	.ena_qs_msg = ice_vc_ena_qs_msg,
 -	.dis_qs_msg = ice_vc_dis_qs_msg,
 -	.request_qs_msg = ice_vc_request_qs_msg,
 -	.cfg_irq_map_msg = ice_vc_cfg_irq_map_msg,
 -	.config_rss_key = ice_vc_config_rss_key,
 -	.config_rss_lut = ice_vc_config_rss_lut,
 -	.get_stats_msg = ice_vc_get_stats_msg,
 -	.cfg_promiscuous_mode_msg = ice_vc_cfg_promiscuous_mode_msg,
 -	.add_vlan_msg = ice_vc_add_vlan_msg,
 -	.remove_vlan_msg = ice_vc_remove_vlan_msg,
 -	.ena_vlan_stripping = ice_vc_ena_vlan_stripping,
 -	.dis_vlan_stripping = ice_vc_dis_vlan_stripping,
 -	.handle_rss_cfg_msg = ice_vc_handle_rss_cfg,
 -	.add_fdir_fltr_msg = ice_vc_add_fdir_fltr,
 -	.del_fdir_fltr_msg = ice_vc_del_fdir_fltr,
 -	.get_offload_vlan_v2_caps = ice_vc_get_offload_vlan_v2_caps,
 -	.add_vlan_v2_msg = ice_vc_add_vlan_v2_msg,
 -	.remove_vlan_v2_msg = ice_vc_remove_vlan_v2_msg,
 -	.ena_vlan_stripping_v2_msg = ice_vc_ena_vlan_stripping_v2_msg,
 -	.dis_vlan_stripping_v2_msg = ice_vc_dis_vlan_stripping_v2_msg,
 -	.ena_vlan_insertion_v2_msg = ice_vc_ena_vlan_insertion_v2_msg,
 -	.dis_vlan_insertion_v2_msg = ice_vc_dis_vlan_insertion_v2_msg,
 -};
 -
 -/**
 - * ice_virtchnl_set_dflt_ops - Switch to default virtchnl ops
 - * @vf: the VF to switch ops
 - */
 -void ice_virtchnl_set_dflt_ops(struct ice_vf *vf)
 -{
 -	vf->virtchnl_ops = &ice_virtchnl_dflt_ops;
 -}
++out:
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DEL_VLAN_V2, v_ret, NULL,
++				     0);
++}
+ 
+ /**
 - * ice_vc_repr_add_mac
 - * @vf: pointer to VF
 - * @msg: virtchannel message
 - *
 - * When port representors are created, we do not add MAC rule
 - * to firmware, we store it so that PF could report same
 - * MAC as VF.
++ * ice_vc_add_vlans - add VLAN(s) from the virtchnl filter list
++ * @vf: VF used to add the VLAN(s)
++ * @vsi: VF's VSI used to add the VLAN(s)
++ * @vfl: virthchnl filter list used to add the filters
+  */
 -static int ice_vc_repr_add_mac(struct ice_vf *vf, u8 *msg)
++static int
++ice_vc_add_vlans(struct ice_vf *vf, struct ice_vsi *vsi,
++		 struct virtchnl_vlan_filter_list_v2 *vfl)
+ {
 -	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
 -	struct virtchnl_ether_addr_list *al =
 -	    (struct virtchnl_ether_addr_list *)msg;
 -	struct ice_vsi *vsi;
 -	struct ice_pf *pf;
 -	int i;
++	bool vlan_promisc = ice_is_vlan_promisc_allowed(vf);
++	int err;
++	u16 i;
+ 
 -	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states) ||
 -	    !ice_vc_isvalid_vsi_id(vf, al->vsi_id)) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto handle_mac_exit;
 -	}
++	for (i = 0; i < vfl->num_elements; i++) {
++		struct virtchnl_vlan_filter *vlan_fltr = &vfl->filters[i];
++		struct virtchnl_vlan *vc_vlan;
+ 
 -	pf = vf->pf;
++		vc_vlan = &vlan_fltr->outer;
++		if (ice_vc_is_valid_vlan(vc_vlan)) {
++			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
+ 
 -	vsi = ice_get_vf_vsi(vf);
 -	if (!vsi) {
 -		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
 -		goto handle_mac_exit;
 -	}
++			err = ice_vc_vlan_action(vsi,
++						 vsi->outer_vlan_ops.add_vlan,
++						 &vlan);
++			if (err)
++				return err;
+ 
 -	for (i = 0; i < al->num_elements; i++) {
 -		u8 *mac_addr = al->list[i].addr;
 -		int result;
++			if (vlan_promisc) {
++				err = ice_vf_ena_vlan_promisc(vsi, &vlan);
++				if (err)
++					return err;
++			}
++		}
+ 
 -		if (!is_unicast_ether_addr(mac_addr) ||
 -		    ether_addr_equal(mac_addr, vf->hw_lan_addr.addr))
 -			continue;
++		vc_vlan = &vlan_fltr->inner;
++		if (ice_vc_is_valid_vlan(vc_vlan)) {
++			struct ice_vlan vlan = ice_vc_to_vlan(vc_vlan);
+ 
 -		if (vf->pf_set_mac) {
 -			dev_err(ice_pf_to_dev(pf), "VF attempting to override administratively set MAC address\n");
 -			v_ret = VIRTCHNL_STATUS_ERR_NOT_SUPPORTED;
 -			goto handle_mac_exit;
 -		}
++			err = ice_vc_vlan_action(vsi,
++						 vsi->inner_vlan_ops.add_vlan,
++						 &vlan);
++			if (err)
++				return err;
+ 
 -		result = ice_eswitch_add_vf_mac_rule(pf, vf, mac_addr);
 -		if (result) {
 -			dev_err(ice_pf_to_dev(pf), "Failed to add MAC %pM for VF %d\n, error %d\n",
 -				mac_addr, vf->vf_id, result);
 -			goto handle_mac_exit;
++			/* no support for VLAN promiscuous on inner VLAN unless
++			 * we are in Single VLAN Mode (SVM)
++			 */
++			if (!ice_is_dvm_ena(&vsi->back->hw) && vlan_promisc) {
++				err = ice_vf_ena_vlan_promisc(vsi, &vlan);
++				if (err)
++					return err;
++			}
+ 		}
 -
 -		ice_vfhw_mac_add(vf, &al->list[i]);
 -		vf->num_mac++;
 -		break;
+ 	}
+ 
 -handle_mac_exit:
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ADD_ETH_ADDR,
 -				     v_ret, NULL, 0);
 -}
 -
 -/**
 - * ice_vc_repr_del_mac - response with success for deleting MAC
 - * @vf: pointer to VF
 - * @msg: virtchannel message
 - *
 - * Respond with success to not break normal VF flow.
 - * For legacy VF driver try to update cached MAC address.
 - */
 -static int
 -ice_vc_repr_del_mac(struct ice_vf __always_unused *vf, u8 __always_unused *msg)
 -{
 -	struct virtchnl_ether_addr_list *al =
 -		(struct virtchnl_ether_addr_list *)msg;
 -
 -	ice_update_legacy_cached_mac(vf, &al->list[0]);
 -
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DEL_ETH_ADDR,
 -				     VIRTCHNL_STATUS_SUCCESS, NULL, 0);
 -}
 -
 -static int ice_vc_repr_add_vlan(struct ice_vf *vf, u8 __always_unused *msg)
 -{
 -	dev_dbg(ice_pf_to_dev(vf->pf),
 -		"Can't add VLAN in switchdev mode for VF %d\n", vf->vf_id);
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ADD_VLAN,
 -				     VIRTCHNL_STATUS_SUCCESS, NULL, 0);
 -}
 -
 -static int ice_vc_repr_del_vlan(struct ice_vf *vf, u8 __always_unused *msg)
 -{
 -	dev_dbg(ice_pf_to_dev(vf->pf),
 -		"Can't delete VLAN in switchdev mode for VF %d\n", vf->vf_id);
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DEL_VLAN,
 -				     VIRTCHNL_STATUS_SUCCESS, NULL, 0);
 -}
 -
 -static int ice_vc_repr_ena_vlan_stripping(struct ice_vf *vf)
 -{
 -	dev_dbg(ice_pf_to_dev(vf->pf),
 -		"Can't enable VLAN stripping in switchdev mode for VF %d\n",
 -		vf->vf_id);
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_STRIPPING,
 -				     VIRTCHNL_STATUS_ERR_NOT_SUPPORTED,
 -				     NULL, 0);
 -}
 -
 -static int ice_vc_repr_dis_vlan_stripping(struct ice_vf *vf)
 -{
 -	dev_dbg(ice_pf_to_dev(vf->pf),
 -		"Can't disable VLAN stripping in switchdev mode for VF %d\n",
 -		vf->vf_id);
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_STRIPPING,
 -				     VIRTCHNL_STATUS_ERR_NOT_SUPPORTED,
 -				     NULL, 0);
 -}
 -
 -static int
 -ice_vc_repr_cfg_promiscuous_mode(struct ice_vf *vf, u8 __always_unused *msg)
 -{
 -	dev_dbg(ice_pf_to_dev(vf->pf),
 -		"Can't config promiscuous mode in switchdev mode for VF %d\n",
 -		vf->vf_id);
 -	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE,
 -				     VIRTCHNL_STATUS_ERR_NOT_SUPPORTED,
 -				     NULL, 0);
 -}
 -
 -static const struct ice_virtchnl_ops ice_virtchnl_repr_ops = {
 -	.get_ver_msg = ice_vc_get_ver_msg,
 -	.get_vf_res_msg = ice_vc_get_vf_res_msg,
 -	.reset_vf = ice_vc_reset_vf_msg,
 -	.add_mac_addr_msg = ice_vc_repr_add_mac,
 -	.del_mac_addr_msg = ice_vc_repr_del_mac,
 -	.cfg_qs_msg = ice_vc_cfg_qs_msg,
 -	.ena_qs_msg = ice_vc_ena_qs_msg,
 -	.dis_qs_msg = ice_vc_dis_qs_msg,
 -	.request_qs_msg = ice_vc_request_qs_msg,
 -	.cfg_irq_map_msg = ice_vc_cfg_irq_map_msg,
 -	.config_rss_key = ice_vc_config_rss_key,
 -	.config_rss_lut = ice_vc_config_rss_lut,
 -	.get_stats_msg = ice_vc_get_stats_msg,
 -	.cfg_promiscuous_mode_msg = ice_vc_repr_cfg_promiscuous_mode,
 -	.add_vlan_msg = ice_vc_repr_add_vlan,
 -	.remove_vlan_msg = ice_vc_repr_del_vlan,
 -	.ena_vlan_stripping = ice_vc_repr_ena_vlan_stripping,
 -	.dis_vlan_stripping = ice_vc_repr_dis_vlan_stripping,
 -	.handle_rss_cfg_msg = ice_vc_handle_rss_cfg,
 -	.add_fdir_fltr_msg = ice_vc_add_fdir_fltr,
 -	.del_fdir_fltr_msg = ice_vc_del_fdir_fltr,
 -	.get_offload_vlan_v2_caps = ice_vc_get_offload_vlan_v2_caps,
 -	.add_vlan_v2_msg = ice_vc_add_vlan_v2_msg,
 -	.remove_vlan_v2_msg = ice_vc_remove_vlan_v2_msg,
 -	.ena_vlan_stripping_v2_msg = ice_vc_ena_vlan_stripping_v2_msg,
 -	.dis_vlan_stripping_v2_msg = ice_vc_dis_vlan_stripping_v2_msg,
 -	.ena_vlan_insertion_v2_msg = ice_vc_ena_vlan_insertion_v2_msg,
 -	.dis_vlan_insertion_v2_msg = ice_vc_dis_vlan_insertion_v2_msg,
 -};
 -
 -/**
 - * ice_virtchnl_set_repr_ops - Switch to representor virtchnl ops
 - * @vf: the VF to switch ops
 - */
 -void ice_virtchnl_set_repr_ops(struct ice_vf *vf)
 -{
 -	vf->virtchnl_ops = &ice_virtchnl_repr_ops;
++	return 0;
+ }
+ 
+ /**
 - * ice_vc_process_vf_msg - Process request from VF
 - * @pf: pointer to the PF structure
 - * @event: pointer to the AQ event
++ * ice_vc_validate_add_vlan_filter_list - validate add filter list from the VF
++ * @vsi: VF VSI used to get number of existing VLAN filters
++ * @vfc: negotiated/supported VLAN filtering capabilities
++ * @vfl: VLAN filter list from VF to validate
+  *
 - * called from the common asq/arq handler to
 - * process request from VF
++ * Validate all of the filters in the VLAN filter list from the VF during the
++ * VIRTCHNL_OP_ADD_VLAN_V2 opcode. If any of the checks fail then return false.
++ * Otherwise return true.
+  */
 -void ice_vc_process_vf_msg(struct ice_pf *pf, struct ice_rq_event_info *event)
++static bool
++ice_vc_validate_add_vlan_filter_list(struct ice_vsi *vsi,
++				     struct virtchnl_vlan_filtering_caps *vfc,
++				     struct virtchnl_vlan_filter_list_v2 *vfl)
+ {
 -	u32 v_opcode = le32_to_cpu(event->desc.cookie_high);
 -	s16 vf_id = le16_to_cpu(event->desc.retval);
 -	const struct ice_virtchnl_ops *ops;
 -	u16 msglen = event->msg_len;
 -	u8 *msg = event->msg_buf;
 -	struct ice_vf *vf = NULL;
 -	struct device *dev;
 -	int err = 0;
 -
 -	dev = ice_pf_to_dev(pf);
++	u16 num_requested_filters = vsi->num_vlan + vfl->num_elements;
+ 
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf) {
 -		dev_err(dev, "Unable to locate VF for message from VF ID %d, opcode %d, len %d\n",
 -			vf_id, v_opcode, msglen);
 -		return;
 -	}
++	if (num_requested_filters > vfc->max_filters)
++		return false;
+ 
 -	/* Check if VF is disabled. */
 -	if (test_bit(ICE_VF_STATE_DIS, vf->vf_states)) {
 -		err = -EPERM;
 -		goto error_handler;
 -	}
++	return ice_vc_validate_vlan_filter_list(vfc, vfl);
++}
+ 
 -	ops = vf->virtchnl_ops;
++/**
++ * ice_vc_add_vlan_v2_msg - virtchnl handler for VIRTCHNL_OP_ADD_VLAN_V2
++ * @vf: VF the message was received from
++ * @msg: message received from the VF
++ */
++static int ice_vc_add_vlan_v2_msg(struct ice_vf *vf, u8 *msg)
++{
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_vlan_filter_list_v2 *vfl =
++		(struct virtchnl_vlan_filter_list_v2 *)msg;
++	struct ice_vsi *vsi;
+ 
 -	/* Perform basic checks on the msg */
 -	err = virtchnl_vc_validate_vf_msg(&vf->vf_ver, v_opcode, msg, msglen);
 -	if (err) {
 -		if (err == VIRTCHNL_STATUS_ERR_PARAM)
 -			err = -EPERM;
 -		else
 -			err = -EINVAL;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	if (!ice_vc_is_opcode_allowed(vf, v_opcode)) {
 -		ice_vc_send_msg_to_vf(vf, v_opcode,
 -				      VIRTCHNL_STATUS_ERR_NOT_SUPPORTED, NULL,
 -				      0);
 -		ice_put_vf(vf);
 -		return;
++	if (!ice_vc_isvalid_vsi_id(vf, vfl->vport_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -error_handler:
 -	if (err) {
 -		ice_vc_send_msg_to_vf(vf, v_opcode, VIRTCHNL_STATUS_ERR_PARAM,
 -				      NULL, 0);
 -		dev_err(dev, "Invalid message from VF %d, opcode %d, len %d, error %d\n",
 -			vf_id, v_opcode, msglen, err);
 -		ice_put_vf(vf);
 -		return;
++	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	/* VF is being configured in another context that triggers a VFR, so no
 -	 * need to process this message
 -	 */
 -	if (!mutex_trylock(&vf->cfg_lock)) {
 -		dev_info(dev, "VF %u is being configured in another context that will trigger a VFR, so there is no need to handle this message\n",
 -			 vf->vf_id);
 -		ice_put_vf(vf);
 -		return;
++	if (!ice_vc_validate_add_vlan_filter_list(vsi,
++						  &vf->vlan_v2_caps.filtering,
++						  vfl)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	switch (v_opcode) {
 -	case VIRTCHNL_OP_VERSION:
 -		err = ops->get_ver_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_GET_VF_RESOURCES:
 -		err = ops->get_vf_res_msg(vf, msg);
 -		if (ice_vf_init_vlan_stripping(vf))
 -			dev_dbg(dev, "Failed to initialize VLAN stripping for VF %d\n",
 -				vf->vf_id);
 -		ice_vc_notify_vf_link_state(vf);
 -		break;
 -	case VIRTCHNL_OP_RESET_VF:
 -		ops->reset_vf(vf);
 -		break;
 -	case VIRTCHNL_OP_ADD_ETH_ADDR:
 -		err = ops->add_mac_addr_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_DEL_ETH_ADDR:
 -		err = ops->del_mac_addr_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_CONFIG_VSI_QUEUES:
 -		err = ops->cfg_qs_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_ENABLE_QUEUES:
 -		err = ops->ena_qs_msg(vf, msg);
 -		ice_vc_notify_vf_link_state(vf);
 -		break;
 -	case VIRTCHNL_OP_DISABLE_QUEUES:
 -		err = ops->dis_qs_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_REQUEST_QUEUES:
 -		err = ops->request_qs_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_CONFIG_IRQ_MAP:
 -		err = ops->cfg_irq_map_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_CONFIG_RSS_KEY:
 -		err = ops->config_rss_key(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_CONFIG_RSS_LUT:
 -		err = ops->config_rss_lut(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_GET_STATS:
 -		err = ops->get_stats_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_CONFIG_PROMISCUOUS_MODE:
 -		err = ops->cfg_promiscuous_mode_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_ADD_VLAN:
 -		err = ops->add_vlan_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_DEL_VLAN:
 -		err = ops->remove_vlan_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_ENABLE_VLAN_STRIPPING:
 -		err = ops->ena_vlan_stripping(vf);
 -		break;
 -	case VIRTCHNL_OP_DISABLE_VLAN_STRIPPING:
 -		err = ops->dis_vlan_stripping(vf);
 -		break;
 -	case VIRTCHNL_OP_ADD_FDIR_FILTER:
 -		err = ops->add_fdir_fltr_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_DEL_FDIR_FILTER:
 -		err = ops->del_fdir_fltr_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_ADD_RSS_CFG:
 -		err = ops->handle_rss_cfg_msg(vf, msg, true);
 -		break;
 -	case VIRTCHNL_OP_DEL_RSS_CFG:
 -		err = ops->handle_rss_cfg_msg(vf, msg, false);
 -		break;
 -	case VIRTCHNL_OP_GET_OFFLOAD_VLAN_V2_CAPS:
 -		err = ops->get_offload_vlan_v2_caps(vf);
 -		break;
 -	case VIRTCHNL_OP_ADD_VLAN_V2:
 -		err = ops->add_vlan_v2_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_DEL_VLAN_V2:
 -		err = ops->remove_vlan_v2_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2:
 -		err = ops->ena_vlan_stripping_v2_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2:
 -		err = ops->dis_vlan_stripping_v2_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2:
 -		err = ops->ena_vlan_insertion_v2_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2:
 -		err = ops->dis_vlan_insertion_v2_msg(vf, msg);
 -		break;
 -	case VIRTCHNL_OP_UNKNOWN:
 -	default:
 -		dev_err(dev, "Unsupported opcode %d from VF %d\n", v_opcode,
 -			vf_id);
 -		err = ice_vc_send_msg_to_vf(vf, v_opcode,
 -					    VIRTCHNL_STATUS_ERR_NOT_SUPPORTED,
 -					    NULL, 0);
 -		break;
 -	}
 -	if (err) {
 -		/* Helper function cares less about error return values here
 -		 * as it is busy with pending work.
 -		 */
 -		dev_info(dev, "PF failed to honor VF %d, opcode %d, error %d\n",
 -			 vf_id, v_opcode, err);
 -	}
++	if (ice_vc_add_vlans(vf, vsi, vfl))
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
+ 
 -	mutex_unlock(&vf->cfg_lock);
 -	ice_put_vf(vf);
++out:
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ADD_VLAN_V2, v_ret, NULL,
++				     0);
+ }
+ 
+ /**
 - * ice_get_vf_cfg
 - * @netdev: network interface device structure
 - * @vf_id: VF identifier
 - * @ivi: VF configuration structure
 - *
 - * return VF configuration
++ * ice_vc_valid_vlan_setting - validate VLAN setting
++ * @negotiated_settings: negotiated VLAN settings during VF init
++ * @ethertype_setting: ethertype(s) requested for the VLAN setting
+  */
 -int
 -ice_get_vf_cfg(struct net_device *netdev, int vf_id, struct ifla_vf_info *ivi)
++static bool
++ice_vc_valid_vlan_setting(u32 negotiated_settings, u32 ethertype_setting)
+ {
 -	struct ice_pf *pf = ice_netdev_to_pf(netdev);
 -	struct ice_vf *vf;
 -	int ret;
 -
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf)
 -		return -EINVAL;
 -
 -	ret = ice_check_vf_ready_for_cfg(vf);
 -	if (ret)
 -		goto out_put_vf;
++	if (ethertype_setting && !(negotiated_settings & ethertype_setting))
++		return false;
+ 
 -	ivi->vf = vf_id;
 -	ether_addr_copy(ivi->mac, vf->hw_lan_addr.addr);
++	/* only allow a single VIRTCHNL_VLAN_ETHERTYPE if
++	 * VIRTHCNL_VLAN_ETHERTYPE_AND is not negotiated/supported
++	 */
++	if (!(negotiated_settings & VIRTCHNL_VLAN_ETHERTYPE_AND) &&
++	    hweight32(ethertype_setting) > 1)
++		return false;
+ 
 -	/* VF configuration for VLAN and applicable QoS */
 -	ivi->vlan = ice_vf_get_port_vlan_id(vf);
 -	ivi->qos = ice_vf_get_port_vlan_prio(vf);
 -	if (ice_vf_is_port_vlan_ena(vf))
 -		ivi->vlan_proto = cpu_to_be16(ice_vf_get_port_vlan_tpid(vf));
 -
 -	ivi->trusted = vf->trusted;
 -	ivi->spoofchk = vf->spoofchk;
 -	if (!vf->link_forced)
 -		ivi->linkstate = IFLA_VF_LINK_STATE_AUTO;
 -	else if (vf->link_up)
 -		ivi->linkstate = IFLA_VF_LINK_STATE_ENABLE;
 -	else
 -		ivi->linkstate = IFLA_VF_LINK_STATE_DISABLE;
 -	ivi->max_tx_rate = vf->max_tx_rate;
 -	ivi->min_tx_rate = vf->min_tx_rate;
++	/* ability to modify the VLAN setting was not negotiated */
++	if (!(negotiated_settings & VIRTCHNL_VLAN_TOGGLE))
++		return false;
+ 
 -out_put_vf:
 -	ice_put_vf(vf);
 -	return ret;
++	return true;
+ }
+ 
+ /**
 - * ice_unicast_mac_exists - check if the unicast MAC exists on the PF's switch
 - * @pf: PF used to reference the switch's rules
 - * @umac: unicast MAC to compare against existing switch rules
++ * ice_vc_valid_vlan_setting_msg - validate the VLAN setting message
++ * @caps: negotiated VLAN settings during VF init
++ * @msg: message to validate
+  *
 - * Return true on the first/any match, else return false
++ * Used to validate any VLAN virtchnl message sent as a
++ * virtchnl_vlan_setting structure. Validates the message against the
++ * negotiated/supported caps during VF driver init.
+  */
 -static bool ice_unicast_mac_exists(struct ice_pf *pf, u8 *umac)
++static bool
++ice_vc_valid_vlan_setting_msg(struct virtchnl_vlan_supported_caps *caps,
++			      struct virtchnl_vlan_setting *msg)
+ {
 -	struct ice_sw_recipe *mac_recipe_list =
 -		&pf->hw.switch_info->recp_list[ICE_SW_LKUP_MAC];
 -	struct ice_fltr_mgmt_list_entry *list_itr;
 -	struct list_head *rule_head;
 -	struct mutex *rule_lock; /* protect MAC filter list access */
 -
 -	rule_head = &mac_recipe_list->filt_rules;
 -	rule_lock = &mac_recipe_list->filt_rule_lock;
 -
 -	mutex_lock(rule_lock);
 -	list_for_each_entry(list_itr, rule_head, list_entry) {
 -		u8 *existing_mac = &list_itr->fltr_info.l_data.mac.mac_addr[0];
 -
 -		if (ether_addr_equal(existing_mac, umac)) {
 -			mutex_unlock(rule_lock);
 -			return true;
 -		}
 -	}
++	if ((!msg->outer_ethertype_setting &&
++	     !msg->inner_ethertype_setting) ||
++	    (!caps->outer && !caps->inner))
++		return false;
+ 
 -	mutex_unlock(rule_lock);
++	if (msg->outer_ethertype_setting &&
++	    !ice_vc_valid_vlan_setting(caps->outer,
++				       msg->outer_ethertype_setting))
++		return false;
+ 
 -	return false;
++	if (msg->inner_ethertype_setting &&
++	    !ice_vc_valid_vlan_setting(caps->inner,
++				       msg->inner_ethertype_setting))
++		return false;
++
++	return true;
+ }
+ 
+ /**
 - * ice_set_vf_mac
 - * @netdev: network interface device structure
 - * @vf_id: VF identifier
 - * @mac: MAC address
 - *
 - * program VF MAC address
++ * ice_vc_get_tpid - transform from VIRTCHNL_VLAN_ETHERTYPE_* to VLAN TPID
++ * @ethertype_setting: VIRTCHNL_VLAN_ETHERTYPE_* used to get VLAN TPID
++ * @tpid: VLAN TPID to populate
+  */
 -int ice_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac)
++static int ice_vc_get_tpid(u32 ethertype_setting, u16 *tpid)
+ {
 -	struct ice_pf *pf = ice_netdev_to_pf(netdev);
 -	struct ice_vf *vf;
 -	int ret;
 -
 -	if (is_multicast_ether_addr(mac)) {
 -		netdev_err(netdev, "%pM not a valid unicast address\n", mac);
++	switch (ethertype_setting) {
++	case VIRTCHNL_VLAN_ETHERTYPE_8100:
++		*tpid = ETH_P_8021Q;
++		break;
++	case VIRTCHNL_VLAN_ETHERTYPE_88A8:
++		*tpid = ETH_P_8021AD;
++		break;
++	case VIRTCHNL_VLAN_ETHERTYPE_9100:
++		*tpid = ETH_P_QINQ1;
++		break;
++	default:
++		*tpid = 0;
++>>>>>>> 8cf52bec5ca0 (ice: cleanup long lines in ice_sriov.c)
  		return -EINVAL;
 -	}
  
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf)
 +	/* The watermark value should not be lesser than the threshold limit
 +	 * set for the number of asynchronous messages a VF can send to mailbox
 +	 * nor should it be greater than the maximum number of messages in the
 +	 * mailbox serviced in current interrupt.
 +	 */
 +	if (mbx_data->async_watermark_val < ICE_ASYNC_VF_MSG_THRESHOLD ||
 +	    mbx_data->async_watermark_val > mbx_data->max_num_msgs_mbx)
  		return -EINVAL;
  
 -	/* nothing left to do, unicast MAC already set */
 -	if (ether_addr_equal(vf->dev_lan_addr.addr, mac) &&
 -	    ether_addr_equal(vf->hw_lan_addr.addr, mac)) {
 -		ret = 0;
 -		goto out_put_vf;
 -	}
 -
 -	ret = ice_check_vf_ready_for_cfg(vf);
 -	if (ret)
 -		goto out_put_vf;
 +	new_state = ICE_MAL_VF_DETECT_STATE_INVALID;
 +	snap_buf = &snap->mbx_buf;
  
 -	if (ice_unicast_mac_exists(pf, mac)) {
 -		netdev_err(netdev, "Unicast MAC %pM already exists on this PF. Preventing setting VF %u unicast MAC address to %pM\n",
 -			   mac, vf_id, mac);
 -		ret = -EINVAL;
 -		goto out_put_vf;
 -	}
 +	switch (snap_buf->state) {
 +	case ICE_MAL_VF_DETECT_STATE_NEW_SNAPSHOT:
 +		/* Clear any previously held data in mailbox snapshot structure. */
 +		ice_mbx_reset_snapshot(snap);
  
 -	mutex_lock(&vf->cfg_lock);
 +		/* Collect the pending ARQ count, number of messages processed and
 +		 * the maximum number of messages allowed to be processed from the
 +		 * Mailbox for current interrupt.
 +		 */
 +		snap_buf->num_pending_arq = mbx_data->num_pending_arq;
 +		snap_buf->num_msg_proc = mbx_data->num_msg_proc;
 +		snap_buf->max_num_msgs_mbx = mbx_data->max_num_msgs_mbx;
  
 -	/* VF is notified of its new MAC via the PF's response to the
 -	 * VIRTCHNL_OP_GET_VF_RESOURCES message after the VF has been reset
 -	 */
 -	ether_addr_copy(vf->dev_lan_addr.addr, mac);
 -	ether_addr_copy(vf->hw_lan_addr.addr, mac);
 -	if (is_zero_ether_addr(mac)) {
 -		/* VF will send VIRTCHNL_OP_ADD_ETH_ADDR message with its MAC */
 -		vf->pf_set_mac = false;
 -		netdev_info(netdev, "Removing MAC on VF %d. VF driver will be reinitialized\n",
 -			    vf->vf_id);
 -	} else {
 -		/* PF will add MAC rule for the VF */
 -		vf->pf_set_mac = true;
 -		netdev_info(netdev, "Setting MAC %pM on VF %d. VF driver will be reinitialized\n",
 -			    mac, vf_id);
 +		/* Capture a new static snapshot of the mailbox by logging the
 +		 * head and tail of snapshot and set num_iterations to the tail
 +		 * value to mark the start of the iteration through the snapshot.
 +		 */
 +		snap_buf->head = ICE_RQ_DATA_MASK(cq->rq.next_to_clean +
 +						  mbx_data->num_pending_arq);
 +		snap_buf->tail = ICE_RQ_DATA_MASK(cq->rq.next_to_clean - 1);
 +		snap_buf->num_iterations = snap_buf->tail;
 +
 +		/* Pending ARQ messages returned by ice_clean_rq_elem
 +		 * is the difference between the head and tail of the
 +		 * mailbox queue. Comparing this value against the watermark
 +		 * helps to check if we potentially have malicious VFs.
 +		 */
 +		if (snap_buf->num_pending_arq >=
 +		    mbx_data->async_watermark_val) {
 +			new_state = ICE_MAL_VF_DETECT_STATE_DETECT;
 +			status = ice_mbx_detect_malvf(hw, vf_id, &new_state, is_malvf);
 +		} else {
 +			new_state = ICE_MAL_VF_DETECT_STATE_TRAVERSE;
 +			ice_mbx_traverse(hw, &new_state);
 +		}
++<<<<<<< HEAD
++=======
+ 	}
+ 
 -	ice_reset_vf(vf, ICE_VF_RESET_NOTIFY);
 -	mutex_unlock(&vf->cfg_lock);
++	ethertype_setting = strip_msg->inner_ethertype_setting;
++	if (ethertype_setting &&
++	    ice_vc_ena_vlan_offload(vsi, vsi->inner_vlan_ops.ena_stripping,
++				    ethertype_setting)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -out_put_vf:
 -	ice_put_vf(vf);
 -	return ret;
++out:
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_STRIPPING_V2,
++				     v_ret, NULL, 0);
+ }
+ 
+ /**
 - * ice_set_vf_trust
 - * @netdev: network interface device structure
 - * @vf_id: VF identifier
 - * @trusted: Boolean value to enable/disable trusted VF
++ * ice_vc_dis_vlan_stripping_v2_msg
++ * @vf: VF the message was received from
++ * @msg: message received from the VF
+  *
 - * Enable or disable a given VF as trusted
++ * virthcnl handler for VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2
+  */
 -int ice_set_vf_trust(struct net_device *netdev, int vf_id, bool trusted)
++static int ice_vc_dis_vlan_stripping_v2_msg(struct ice_vf *vf, u8 *msg)
+ {
 -	struct ice_pf *pf = ice_netdev_to_pf(netdev);
 -	struct ice_vf *vf;
 -	int ret;
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_vlan_supported_caps *stripping_support;
++	struct virtchnl_vlan_setting *strip_msg =
++		(struct virtchnl_vlan_setting *)msg;
++	u32 ethertype_setting;
++	struct ice_vsi *vsi;
+ 
 -	if (ice_is_eswitch_mode_switchdev(pf)) {
 -		dev_info(ice_pf_to_dev(pf), "Trusted VF is forbidden in switchdev mode\n");
 -		return -EOPNOTSUPP;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf)
 -		return -EINVAL;
 -
 -	ret = ice_check_vf_ready_for_cfg(vf);
 -	if (ret)
 -		goto out_put_vf;
 -
 -	/* Check if already trusted */
 -	if (trusted == vf->trusted) {
 -		ret = 0;
 -		goto out_put_vf;
++	if (!ice_vc_isvalid_vsi_id(vf, strip_msg->vport_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	mutex_lock(&vf->cfg_lock);
 -
 -	vf->trusted = trusted;
 -	ice_reset_vf(vf, ICE_VF_RESET_NOTIFY);
 -	dev_info(ice_pf_to_dev(pf), "VF %u is now %strusted\n",
 -		 vf_id, trusted ? "" : "un");
 -
 -	mutex_unlock(&vf->cfg_lock);
 -
 -out_put_vf:
 -	ice_put_vf(vf);
 -	return ret;
 -}
 -
 -/**
 - * ice_set_vf_link_state
 - * @netdev: network interface device structure
 - * @vf_id: VF identifier
 - * @link_state: required link state
 - *
 - * Set VF's link state, irrespective of physical link state status
 - */
 -int ice_set_vf_link_state(struct net_device *netdev, int vf_id, int link_state)
 -{
 -	struct ice_pf *pf = ice_netdev_to_pf(netdev);
 -	struct ice_vf *vf;
 -	int ret;
++	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf)
 -		return -EINVAL;
++	stripping_support = &vf->vlan_v2_caps.offloads.stripping_support;
++	if (!ice_vc_valid_vlan_setting_msg(stripping_support, strip_msg)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -	ret = ice_check_vf_ready_for_cfg(vf);
 -	if (ret)
 -		goto out_put_vf;
++	ethertype_setting = strip_msg->outer_ethertype_setting;
++	if (ethertype_setting) {
++		if (vsi->outer_vlan_ops.dis_stripping(vsi)) {
++			v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++			goto out;
++		} else {
++			enum ice_l2tsel l2tsel =
++				ICE_L2TSEL_EXTRACT_FIRST_TAG_L2TAG1;
+ 
 -	switch (link_state) {
 -	case IFLA_VF_LINK_STATE_AUTO:
 -		vf->link_forced = false;
 -		break;
 -	case IFLA_VF_LINK_STATE_ENABLE:
 -		vf->link_forced = true;
 -		vf->link_up = true;
 -		break;
 -	case IFLA_VF_LINK_STATE_DISABLE:
 -		vf->link_forced = true;
 -		vf->link_up = false;
 -		break;
 -	default:
 -		ret = -EINVAL;
 -		goto out_put_vf;
++			/* PF tells the VF that the outer VLAN tag is always
++			 * extracted to VIRTCHNL_VLAN_TAG_LOCATION_L2TAG2_2 and
++			 * inner is always extracted to
++			 * VIRTCHNL_VLAN_TAG_LOCATION_L2TAG1. This is needed to
++			 * support inner stripping while outer stripping is
++			 * disabled so that the first and only tag is extracted
++			 * in L2TAG1.
++			 */
++			ice_vsi_update_l2tsel(vsi, l2tsel);
++		}
+ 	}
+ 
 -	ice_vc_notify_vf_link_state(vf);
++	ethertype_setting = strip_msg->inner_ethertype_setting;
++	if (ethertype_setting && vsi->inner_vlan_ops.dis_stripping(vsi)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -out_put_vf:
 -	ice_put_vf(vf);
 -	return ret;
++out:
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_STRIPPING_V2,
++				     v_ret, NULL, 0);
+ }
+ 
+ /**
 - * ice_calc_all_vfs_min_tx_rate - calculate cumulative min Tx rate on all VFs
 - * @pf: PF associated with VFs
++ * ice_vc_ena_vlan_insertion_v2_msg
++ * @vf: VF the message was received from
++ * @msg: message received from the VF
++ *
++ * virthcnl handler for VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2
+  */
 -static int ice_calc_all_vfs_min_tx_rate(struct ice_pf *pf)
++static int ice_vc_ena_vlan_insertion_v2_msg(struct ice_vf *vf, u8 *msg)
+ {
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -	int rate = 0;
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_vlan_supported_caps *insertion_support;
++	struct virtchnl_vlan_setting *insertion_msg =
++		(struct virtchnl_vlan_setting *)msg;
++	u32 ethertype_setting;
++	struct ice_vsi *vsi;
+ 
 -	rcu_read_lock();
 -	ice_for_each_vf_rcu(pf, bkt, vf)
 -		rate += vf->min_tx_rate;
 -	rcu_read_unlock();
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -	return rate;
 -}
++	if (!ice_vc_isvalid_vsi_id(vf, insertion_msg->vport_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -/**
 - * ice_min_tx_rate_oversubscribed - check if min Tx rate causes oversubscription
 - * @vf: VF trying to configure min_tx_rate
 - * @min_tx_rate: min Tx rate in Mbps
 - *
 - * Check if the min_tx_rate being passed in will cause oversubscription of total
 - * min_tx_rate based on the current link speed and all other VFs configured
 - * min_tx_rate
 - *
 - * Return true if the passed min_tx_rate would cause oversubscription, else
 - * return false
 - */
 -static bool
 -ice_min_tx_rate_oversubscribed(struct ice_vf *vf, int min_tx_rate)
 -{
 -	int link_speed_mbps = ice_get_link_speed_mbps(ice_get_vf_vsi(vf));
 -	int all_vfs_min_tx_rate = ice_calc_all_vfs_min_tx_rate(vf->pf);
++	vsi = ice_get_vf_vsi(vf);
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -	/* this VF's previous rate is being overwritten */
 -	all_vfs_min_tx_rate -= vf->min_tx_rate;
++	insertion_support = &vf->vlan_v2_caps.offloads.insertion_support;
++	if (!ice_vc_valid_vlan_setting_msg(insertion_support, insertion_msg)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -	if (all_vfs_min_tx_rate + min_tx_rate > link_speed_mbps) {
 -		dev_err(ice_pf_to_dev(vf->pf), "min_tx_rate of %d Mbps on VF %u would cause oversubscription of %d Mbps based on the current link speed %d Mbps\n",
 -			min_tx_rate, vf->vf_id,
 -			all_vfs_min_tx_rate + min_tx_rate - link_speed_mbps,
 -			link_speed_mbps);
 -		return true;
++	ethertype_setting = insertion_msg->outer_ethertype_setting;
++	if (ethertype_setting &&
++	    ice_vc_ena_vlan_offload(vsi, vsi->outer_vlan_ops.ena_insertion,
++				    ethertype_setting)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	return false;
++	ethertype_setting = insertion_msg->inner_ethertype_setting;
++	if (ethertype_setting &&
++	    ice_vc_ena_vlan_offload(vsi, vsi->inner_vlan_ops.ena_insertion,
++				    ethertype_setting)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
++
++out:
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_ENABLE_VLAN_INSERTION_V2,
++				     v_ret, NULL, 0);
+ }
+ 
+ /**
 - * ice_set_vf_bw - set min/max VF bandwidth
 - * @netdev: network interface device structure
 - * @vf_id: VF identifier
 - * @min_tx_rate: Minimum Tx rate in Mbps
 - * @max_tx_rate: Maximum Tx rate in Mbps
++ * ice_vc_dis_vlan_insertion_v2_msg
++ * @vf: VF the message was received from
++ * @msg: message received from the VF
++ *
++ * virthcnl handler for VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2
+  */
 -int
 -ice_set_vf_bw(struct net_device *netdev, int vf_id, int min_tx_rate,
 -	      int max_tx_rate)
++static int ice_vc_dis_vlan_insertion_v2_msg(struct ice_vf *vf, u8 *msg)
+ {
 -	struct ice_pf *pf = ice_netdev_to_pf(netdev);
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_vlan_supported_caps *insertion_support;
++	struct virtchnl_vlan_setting *insertion_msg =
++		(struct virtchnl_vlan_setting *)msg;
++	u32 ethertype_setting;
+ 	struct ice_vsi *vsi;
 -	struct device *dev;
 -	struct ice_vf *vf;
 -	int ret;
 -
 -	dev = ice_pf_to_dev(pf);
+ 
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf)
 -		return -EINVAL;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
 -	ret = ice_check_vf_ready_for_cfg(vf);
 -	if (ret)
 -		goto out_put_vf;
++	if (!ice_vc_isvalid_vsi_id(vf, insertion_msg->vport_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
++	}
+ 
+ 	vsi = ice_get_vf_vsi(vf);
 -
 -	/* when max_tx_rate is zero that means no max Tx rate limiting, so only
 -	 * check if max_tx_rate is non-zero
 -	 */
 -	if (max_tx_rate && min_tx_rate > max_tx_rate) {
 -		dev_err(dev, "Cannot set min Tx rate %d Mbps greater than max Tx rate %d Mbps\n",
 -			min_tx_rate, max_tx_rate);
 -		ret = -EINVAL;
 -		goto out_put_vf;
++	if (!vsi) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	if (min_tx_rate && ice_is_dcb_active(pf)) {
 -		dev_err(dev, "DCB on PF is currently enabled. VF min Tx rate limiting not allowed on this PF.\n");
 -		ret = -EOPNOTSUPP;
 -		goto out_put_vf;
++	insertion_support = &vf->vlan_v2_caps.offloads.insertion_support;
++	if (!ice_vc_valid_vlan_setting_msg(insertion_support, insertion_msg)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	if (ice_min_tx_rate_oversubscribed(vf, min_tx_rate)) {
 -		ret = -EINVAL;
 -		goto out_put_vf;
++	ethertype_setting = insertion_msg->outer_ethertype_setting;
++	if (ethertype_setting && vsi->outer_vlan_ops.dis_insertion(vsi)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	if (vf->min_tx_rate != (unsigned int)min_tx_rate) {
 -		ret = ice_set_min_bw_limit(vsi, (u64)min_tx_rate * 1000);
 -		if (ret) {
 -			dev_err(dev, "Unable to set min-tx-rate for VF %d\n",
 -				vf->vf_id);
 -			goto out_put_vf;
 -		}
 -
 -		vf->min_tx_rate = min_tx_rate;
++	ethertype_setting = insertion_msg->inner_ethertype_setting;
++	if (ethertype_setting && vsi->inner_vlan_ops.dis_insertion(vsi)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto out;
+ 	}
+ 
 -	if (vf->max_tx_rate != (unsigned int)max_tx_rate) {
 -		ret = ice_set_max_bw_limit(vsi, (u64)max_tx_rate * 1000);
 -		if (ret) {
 -			dev_err(dev, "Unable to set max-tx-rate for VF %d\n",
 -				vf->vf_id);
 -			goto out_put_vf;
 -		}
++out:
++	return ice_vc_send_msg_to_vf(vf, VIRTCHNL_OP_DISABLE_VLAN_INSERTION_V2,
++				     v_ret, NULL, 0);
++}
+ 
 -		vf->max_tx_rate = max_tx_rate;
 -	}
++static const struct ice_virtchnl_ops ice_virtchnl_dflt_ops = {
++	.get_ver_msg = ice_vc_get_ver_msg,
++	.get_vf_res_msg = ice_vc_get_vf_res_msg,
++	.reset_vf = ice_vc_reset_vf_msg,
++	.add_mac_addr_msg = ice_vc_add_mac_addr_msg,
++	.del_mac_addr_msg = ice_vc_del_mac_addr_msg,
++	.cfg_qs_msg = ice_vc_cfg_qs_msg,
++	.ena_qs_msg = ice_vc_ena_qs_msg,
++	.dis_qs_msg = ice_vc_dis_qs_msg,
++	.request_qs_msg = ice_vc_request_qs_msg,
++	.cfg_irq_map_msg = ice_vc_cfg_irq_map_msg,
++	.config_rss_key = ice_vc_config_rss_key,
++	.config_rss_lut = ice_vc_config_rss_lut,
++	.get_stats_msg = ice_vc_get_stats_msg,
++	.cfg_promiscuous_mode_msg = ice_vc_cfg_promiscuous_mode_msg,
++	.add_vlan_msg = ice_vc_add_vlan_msg,
++	.remove_vlan_msg = ice_vc_remove_vlan_msg,
++	.ena_vlan_stripping = ice_vc_ena_vlan_stripping,
++	.dis_vlan_stripping = ice_vc_dis_vlan_stripping,
++	.handle_rss_cfg_msg = ice_vc_handle_rss_cfg,
++	.add_fdir_fltr_msg = ice_vc_add_fdir_fltr,
++	.del_fdir_fltr_msg = ice_vc_del_fdir_fltr,
++	.get_offload_vlan_v2_caps = ice_vc_get_offload_vlan_v2_caps,
++	.add_vlan_v2_msg = ice_vc_add_vlan_v2_msg,
++	.remove_vlan_v2_msg = ice_vc_remove_vlan_v2_msg,
++	.ena_vlan_stripping_v2_msg = ice_vc_ena_vlan_stripping_v2_msg,
++	.dis_vlan_stripping_v2_msg = ice_vc_dis_vlan_stripping_v2_msg,
++	.ena_vlan_insertion_v2_msg = ice_vc_ena_vlan_insertion_v2_msg,
++	.dis_vlan_insertion_v2_msg = ice_vc_dis_vlan_insertion_v2_msg,
++};
+ 
 -out_put_vf:
 -	ice_put_vf(vf);
 -	return ret;
++/**
++ * ice_virtchnl_set_dflt_ops - Switch to default virtchnl ops
++ * @vf: the VF to switch ops
++ */
++void ice_virtchnl_set_dflt_ops(struct ice_vf *vf)
++{
++	vf->virtchnl_ops = &ice_virtchnl_dflt_ops;
+ }
+ 
+ /**
 - * ice_get_vf_stats - populate some stats for the VF
 - * @netdev: the netdev of the PF
 - * @vf_id: the host OS identifier (0-255)
 - * @vf_stats: pointer to the OS memory to be initialized
++ * ice_vc_repr_add_mac
++ * @vf: pointer to VF
++ * @msg: virtchannel message
++ *
++ * When port representors are created, we do not add MAC rule
++ * to firmware, we store it so that PF could report same
++ * MAC as VF.
+  */
 -int ice_get_vf_stats(struct net_device *netdev, int vf_id,
 -		     struct ifla_vf_stats *vf_stats)
++static int ice_vc_repr_add_mac(struct ice_vf *vf, u8 *msg)
+ {
 -	struct ice_pf *pf = ice_netdev_to_pf(netdev);
 -	struct ice_eth_stats *stats;
++	enum virtchnl_status_code v_ret = VIRTCHNL_STATUS_SUCCESS;
++	struct virtchnl_ether_addr_list *al =
++	    (struct virtchnl_ether_addr_list *)msg;
+ 	struct ice_vsi *vsi;
 -	struct ice_vf *vf;
 -	int ret;
++	struct ice_pf *pf;
++	int i;
+ 
 -	vf = ice_get_vf_by_id(pf, vf_id);
 -	if (!vf)
 -		return -EINVAL;
++	if (!test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states) ||
++	    !ice_vc_isvalid_vsi_id(vf, al->vsi_id)) {
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto handle_mac_exit;
++	}
+ 
 -	ret = ice_check_vf_ready_for_cfg(vf);
 -	if (ret)
 -		goto out_put_vf;
++	pf = vf->pf;
+ 
+ 	vsi = ice_get_vf_vsi(vf);
+ 	if (!vsi) {
 -		ret = -EINVAL;
 -		goto out_put_vf;
++		v_ret = VIRTCHNL_STATUS_ERR_PARAM;
++		goto handle_mac_exit;
+ 	}
+ 
 -	ice_update_eth_stats(vsi);
 -	stats = &vsi->eth_stats;
++	for (i = 0; i < al->num_elements; i++) {
++		u8 *mac_addr = al->list[i].addr;
++		int result;
+ 
 -	memset(vf_stats, 0, sizeof(*vf_stats));
++		if (!is_unicast_ether_addr(mac_addr) ||
++		    ether_addr_equal(mac_addr, vf->hw_lan_addr.addr))
++			continue;
+ 
 -	vf_stats->rx_packets = stats->rx_unicast + stats->rx_broadcast +
 -		stats->rx_multicast;
 -	vf_stats->tx_packets = stats->tx_unicast + stats->tx_broadcast +
 -		stats->tx_multicast;
 -	vf_stats->rx_bytes   = stats->rx_bytes;
 -	vf_stats->tx_bytes   = stats->tx_bytes;
 -	vf_stats->broadcast  = stats->rx_broadcast;
 -	vf_stats->multicast  = stats->rx_multicast;
 -	vf_stats->rx_dropped = stats->rx_discards;
 -	vf_stats->tx_dropped = stats->tx_discards;
++		if (vf->pf_set_mac) {
++			dev_err(ice_pf_to_dev(pf), "VF attempting to override administratively set MAC address\n");
++			v_ret = VIRTCHNL_STATUS_ERR_NOT_SUPPORTED;
++			goto handle_mac_exit;
++		}
+ 
 -out_put_vf:
 -	ice_put_vf(vf);
 -	return ret;
 -}
++		result = ice_eswitch_add_vf_mac_rule(pf, vf, mac_addr);
++		if (result) {
++			dev_err(ice_pf_to_dev(pf), "Failed to add MAC %pM for VF %d\n, error %d\n",
++				mac_addr, vf->vf_id, result);
++			goto handle_mac_exit;
++		}
+ 
 -/**
 - * ice_is_supported_port_vlan_proto - make sure the vlan_proto is supported
 - * @hw: hardware structure used to check the VLAN mode
 - * @vlan_proto: VLAN TPID being checked
 - *
 - * If the device is configured in Double VLAN Mode (DVM), then both ETH_P_8021Q
 - * and ETH_P_8021AD are supported. If the device is configured in Single VLAN
 - * Mode (SVM), then only ETH_P_8021Q is supported.
 - */
 -static bool
 -ice_is_supported_port_vlan_proto(struct ice_hw *hw, u16 vlan_proto)
 -{
 -	bool is_supported = false;
++		ice_vfhw_mac_add(vf, &al->list[i]);
++		vf->num_mac++;
++>>>>>>> 8cf52bec5ca0 (ice: cleanup long lines in ice_sriov.c)
 +		break;
  
 -	switch (vlan_proto) {
 -	case ETH_P_8021Q:
 -		is_supported = true;
 +	case ICE_MAL_VF_DETECT_STATE_TRAVERSE:
 +		new_state = ICE_MAL_VF_DETECT_STATE_TRAVERSE;
 +		ice_mbx_traverse(hw, &new_state);
  		break;
 -	case ETH_P_8021AD:
 -		if (ice_is_dvm_ena(hw))
 -			is_supported = true;
 +
 +	case ICE_MAL_VF_DETECT_STATE_DETECT:
 +		new_state = ICE_MAL_VF_DETECT_STATE_DETECT;
 +		status = ice_mbx_detect_malvf(hw, vf_id, &new_state, is_malvf);
  		break;
 +
 +	default:
 +		new_state = ICE_MAL_VF_DETECT_STATE_INVALID;
 +		status = -EIO;
  	}
  
 -	return is_supported;
 +	snap_buf->state = new_state;
 +
 +	return status;
  }
  
  /**
* Unmerged path drivers/net/ethernet/intel/ice/ice_sriov.c
