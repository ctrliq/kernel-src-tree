ice: introduce VF operations structure for reset flows

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.26.1.el8_6
commit-author Jacob Keller <jacob.e.keller@intel.com>
commit 9c6f787897f6863b16659fb8807ff5965eb894f2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.26.1.el8_6/9c6f7878.failed

The ice driver currently supports virtualization using Single Root IOV,
with code in the ice_sriov.c file. In the future, we plan to also
implement support for Scalable IOV, which uses slightly different
hardware implementations for some functionality.

To eventually allow this, we introduce a new ice_vf_ops structure which
will contain the basic operations that are different between the two IOV
implementations. This primarily includes logic for how to handle the VF
reset registers, as well as what to do before and after rebuilding the
VF's VSI.

Implement these ops structures and call the ops table instead of
directly calling the SR-IOV specific function. This will allow us to
easily add the Scalable IOV implementation in the future. Additionally,
it helps separate the generalized VF logic from SR-IOV specifics. This
change allows us to move the reset logic out of ice_sriov.c and into
ice_vf_lib.c without placing any Single Root specific details into the
generic file.

	Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
	Tested-by: Konrad Jankowski <konrad0.jankowski@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 9c6f787897f6863b16659fb8807ff5965eb894f2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_sriov.c
#	drivers/net/ethernet/intel/ice/ice_vf_lib.c
#	drivers/net/ethernet/intel/ice/ice_vf_lib.h
diff --cc drivers/net/ethernet/intel/ice/ice_sriov.c
index 52c6bac41bf7,be6ec42f97c1..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_sriov.c
+++ b/drivers/net/ethernet/intel/ice/ice_sriov.c
@@@ -1,42 -1,2004 +1,1769 @@@
  // SPDX-License-Identifier: GPL-2.0
  /* Copyright (c) 2018, Intel Corporation. */
  
 -#include "ice.h"
 -#include "ice_vf_lib_private.h"
 -#include "ice_base.h"
 -#include "ice_lib.h"
 -#include "ice_fltr.h"
 -#include "ice_dcb_lib.h"
 -#include "ice_flow.h"
 -#include "ice_eswitch.h"
 -#include "ice_virtchnl_allowlist.h"
 -#include "ice_flex_pipe.h"
 -#include "ice_vf_vsi_vlan_ops.h"
 -#include "ice_vlan.h"
 -
 -#define FIELD_SELECTOR(proto_hdr_field) \
 -		BIT((proto_hdr_field) & PROTO_HDR_FIELD_MASK)
 -
 -struct ice_vc_hdr_match_type {
 -	u32 vc_hdr;	/* virtchnl headers (VIRTCHNL_PROTO_HDR_XXX) */
 -	u32 ice_hdr;	/* ice headers (ICE_FLOW_SEG_HDR_XXX) */
 -};
 -
 -static const struct ice_vc_hdr_match_type ice_vc_hdr_list[] = {
 -	{VIRTCHNL_PROTO_HDR_NONE,	ICE_FLOW_SEG_HDR_NONE},
 -	{VIRTCHNL_PROTO_HDR_ETH,	ICE_FLOW_SEG_HDR_ETH},
 -	{VIRTCHNL_PROTO_HDR_S_VLAN,	ICE_FLOW_SEG_HDR_VLAN},
 -	{VIRTCHNL_PROTO_HDR_C_VLAN,	ICE_FLOW_SEG_HDR_VLAN},
 -	{VIRTCHNL_PROTO_HDR_IPV4,	ICE_FLOW_SEG_HDR_IPV4 |
 -					ICE_FLOW_SEG_HDR_IPV_OTHER},
 -	{VIRTCHNL_PROTO_HDR_IPV6,	ICE_FLOW_SEG_HDR_IPV6 |
 -					ICE_FLOW_SEG_HDR_IPV_OTHER},
 -	{VIRTCHNL_PROTO_HDR_TCP,	ICE_FLOW_SEG_HDR_TCP},
 -	{VIRTCHNL_PROTO_HDR_UDP,	ICE_FLOW_SEG_HDR_UDP},
 -	{VIRTCHNL_PROTO_HDR_SCTP,	ICE_FLOW_SEG_HDR_SCTP},
 -	{VIRTCHNL_PROTO_HDR_PPPOE,	ICE_FLOW_SEG_HDR_PPPOE},
 -	{VIRTCHNL_PROTO_HDR_GTPU_IP,	ICE_FLOW_SEG_HDR_GTPU_IP},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH,	ICE_FLOW_SEG_HDR_GTPU_EH},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_DWN,
 -					ICE_FLOW_SEG_HDR_GTPU_DWN},
 -	{VIRTCHNL_PROTO_HDR_GTPU_EH_PDU_UP,
 -					ICE_FLOW_SEG_HDR_GTPU_UP},
 -	{VIRTCHNL_PROTO_HDR_L2TPV3,	ICE_FLOW_SEG_HDR_L2TPV3},
 -	{VIRTCHNL_PROTO_HDR_ESP,	ICE_FLOW_SEG_HDR_ESP},
 -	{VIRTCHNL_PROTO_HDR_AH,		ICE_FLOW_SEG_HDR_AH},
 -	{VIRTCHNL_PROTO_HDR_PFCP,	ICE_FLOW_SEG_HDR_PFCP_SESSION},
 -};
 -
 -struct ice_vc_hash_field_match_type {
 -	u32 vc_hdr;		/* virtchnl headers
 -				 * (VIRTCHNL_PROTO_HDR_XXX)
 -				 */
 -	u32 vc_hash_field;	/* virtchnl hash fields selector
 -				 * FIELD_SELECTOR((VIRTCHNL_PROTO_HDR_ETH_XXX))
 -				 */
 -	u64 ice_hash_field;	/* ice hash fields
 -				 * (BIT_ULL(ICE_FLOW_FIELD_IDX_XXX))
 -				 */
 -};
 -
 -static const struct
 -ice_vc_hash_field_match_type ice_vc_hash_field_list[] = {
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_SA)},
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_DA)},
 -	{VIRTCHNL_PROTO_HDR_ETH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_DST),
 -		ICE_FLOW_HASH_ETH},
 -	{VIRTCHNL_PROTO_HDR_ETH,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ETH_ETHERTYPE),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ETH_TYPE)},
 -	{VIRTCHNL_PROTO_HDR_S_VLAN,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_S_VLAN_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_S_VLAN)},
 -	{VIRTCHNL_PROTO_HDR_C_VLAN,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_C_VLAN_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_C_VLAN)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_SA)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_DA)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST),
 -		ICE_FLOW_HASH_IPV4},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_SA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_DA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		ICE_FLOW_HASH_IPV4 | BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV4, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV4_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV4_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_SA)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_DA)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST),
 -		ICE_FLOW_HASH_IPV6},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_SA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_DA) |
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_SRC) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_DST) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		ICE_FLOW_HASH_IPV6 | BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_IPV6, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_IPV6_PROT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_IPV6_PROT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_TCP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_TCP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_TCP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_TCP_DST_PORT),
 -		ICE_FLOW_HASH_TCP_PORT},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_UDP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_UDP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_UDP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_UDP_DST_PORT),
 -		ICE_FLOW_HASH_UDP_PORT},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_SRC_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_SCTP_SRC_PORT)},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_DST_PORT),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_SCTP_DST_PORT)},
 -	{VIRTCHNL_PROTO_HDR_SCTP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_SRC_PORT) |
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_SCTP_DST_PORT),
 -		ICE_FLOW_HASH_SCTP_PORT},
 -	{VIRTCHNL_PROTO_HDR_PPPOE,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_PPPOE_SESS_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_PPPOE_SESS_ID)},
 -	{VIRTCHNL_PROTO_HDR_GTPU_IP,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_GTPU_IP_TEID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_GTPU_IP_TEID)},
 -	{VIRTCHNL_PROTO_HDR_L2TPV3,
 -		FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_L2TPV3_SESS_ID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_L2TPV3_SESS_ID)},
 -	{VIRTCHNL_PROTO_HDR_ESP, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_ESP_SPI),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_ESP_SPI)},
 -	{VIRTCHNL_PROTO_HDR_AH, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_AH_SPI),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_AH_SPI)},
 -	{VIRTCHNL_PROTO_HDR_PFCP, FIELD_SELECTOR(VIRTCHNL_PROTO_HDR_PFCP_SEID),
 -		BIT_ULL(ICE_FLOW_FIELD_IDX_PFCP_SEID)},
 -};
 -
 -/**
 - * ice_free_vf_entries - Free all VF entries from the hash table
 - * @pf: pointer to the PF structure
 - *
 - * Iterate over the VF hash table, removing and releasing all VF entries.
 - * Called during VF teardown or as cleanup during failed VF initialization.
 - */
 -static void ice_free_vf_entries(struct ice_pf *pf)
 -{
 -	struct ice_vfs *vfs = &pf->vfs;
 -	struct hlist_node *tmp;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	/* Remove all VFs from the hash table and release their main
 -	 * reference. Once all references to the VF are dropped, ice_put_vf()
 -	 * will call ice_release_vf which will remove the VF memory.
 -	 */
 -	lockdep_assert_held(&vfs->table_lock);
 -
 -	hash_for_each_safe(vfs->table, bkt, tmp, vf, entry) {
 -		hash_del_rcu(&vf->entry);
 -		ice_put_vf(vf);
 -	}
 -}
 +#include "ice_common.h"
 +#include "ice_sriov.h"
  
  /**
 - * ice_vc_vf_broadcast - Broadcast a message to all VFs on PF
 - * @pf: pointer to the PF structure
 - * @v_opcode: operation code
 - * @v_retval: return value
 + * ice_aq_send_msg_to_vf
 + * @hw: pointer to the hardware structure
 + * @vfid: VF ID to send msg
 + * @v_opcode: opcodes for VF-PF communication
 + * @v_retval: return error code
   * @msg: pointer to the msg buffer
   * @msglen: msg length
 - */
 -static void
 -ice_vc_vf_broadcast(struct ice_pf *pf, enum virtchnl_ops v_opcode,
 -		    enum virtchnl_status_code v_retval, u8 *msg, u16 msglen)
 -{
 -	struct ice_hw *hw = &pf->hw;
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	mutex_lock(&pf->vfs.table_lock);
 -	ice_for_each_vf(pf, bkt, vf) {
 -		/* Not all vfs are enabled so skip the ones that are not */
 -		if (!test_bit(ICE_VF_STATE_INIT, vf->vf_states) &&
 -		    !test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states))
 -			continue;
 -
 -		/* Ignore return value on purpose - a given VF may fail, but
 -		 * we need to keep going and send to all of them
 -		 */
 -		ice_aq_send_msg_to_vf(hw, vf->vf_id, v_opcode, v_retval, msg,
 -				      msglen, NULL);
 -	}
 -	mutex_unlock(&pf->vfs.table_lock);
 -}
 -
 -/**
 - * ice_set_pfe_link - Set the link speed/status of the virtchnl_pf_event
 - * @vf: pointer to the VF structure
 - * @pfe: pointer to the virtchnl_pf_event to set link speed/status for
 - * @ice_link_speed: link speed specified by ICE_AQ_LINK_SPEED_*
 - * @link_up: whether or not to set the link up/down
 - */
 -static void
 -ice_set_pfe_link(struct ice_vf *vf, struct virtchnl_pf_event *pfe,
 -		 int ice_link_speed, bool link_up)
 -{
 -	if (vf->driver_caps & VIRTCHNL_VF_CAP_ADV_LINK_SPEED) {
 -		pfe->event_data.link_event_adv.link_status = link_up;
 -		/* Speed in Mbps */
 -		pfe->event_data.link_event_adv.link_speed =
 -			ice_conv_link_speed_to_virtchnl(true, ice_link_speed);
 -	} else {
 -		pfe->event_data.link_event.link_status = link_up;
 -		/* Legacy method for virtchnl link speeds */
 -		pfe->event_data.link_event.link_speed =
 -			(enum virtchnl_link_speed)
 -			ice_conv_link_speed_to_virtchnl(false, ice_link_speed);
 -	}
 -}
 -
 -/**
 - * ice_vc_notify_vf_link_state - Inform a VF of link status
 - * @vf: pointer to the VF structure
 + * @cd: pointer to command details
   *
++<<<<<<< HEAD
 + * Send message to VF driver (0x0802) using mailbox
 + * queue and asynchronously sending message via
 + * ice_sq_send_cmd() function
++=======
+  * send a link status message to a single VF
+  */
+ void ice_vc_notify_vf_link_state(struct ice_vf *vf)
+ {
+ 	struct virtchnl_pf_event pfe = { 0 };
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_LINK_CHANGE;
+ 	pfe.severity = PF_EVENT_SEVERITY_INFO;
+ 
+ 	if (ice_is_vf_link_up(vf))
+ 		ice_set_pfe_link(vf, &pfe,
+ 				 hw->port_info->phy.link_info.link_speed, true);
+ 	else
+ 		ice_set_pfe_link(vf, &pfe, ICE_AQ_LINK_SPEED_UNKNOWN, false);
+ 
+ 	ice_aq_send_msg_to_vf(hw, vf->vf_id, VIRTCHNL_OP_EVENT,
+ 			      VIRTCHNL_STATUS_SUCCESS, (u8 *)&pfe,
+ 			      sizeof(pfe), NULL);
+ }
+ 
+ /**
+  * ice_vf_vsi_release - invalidate the VF's VSI after freeing it
+  * @vf: invalidate this VF's VSI after freeing it
+  */
+ static void ice_vf_vsi_release(struct ice_vf *vf)
+ {
+ 	ice_vsi_release(ice_get_vf_vsi(vf));
+ 	ice_vf_invalidate_vsi(vf);
+ }
+ 
+ /**
+  * ice_free_vf_res - Free a VF's resources
+  * @vf: pointer to the VF info
+  */
+ static void ice_free_vf_res(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	int i, last_vector_idx;
+ 
+ 	/* First, disable VF's configuration API to prevent OS from
+ 	 * accessing the VF's VSI after it's freed or invalidated.
+ 	 */
+ 	clear_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 	ice_vf_fdir_exit(vf);
+ 	/* free VF control VSI */
+ 	if (vf->ctrl_vsi_idx != ICE_NO_VSI)
+ 		ice_vf_ctrl_vsi_release(vf);
+ 
+ 	/* free VSI and disconnect it from the parent uplink */
+ 	if (vf->lan_vsi_idx != ICE_NO_VSI) {
+ 		ice_vf_vsi_release(vf);
+ 		vf->num_mac = 0;
+ 	}
+ 
+ 	last_vector_idx = vf->first_vector_idx + pf->vfs.num_msix_per - 1;
+ 
+ 	/* clear VF MDD event information */
+ 	memset(&vf->mdd_tx_events, 0, sizeof(vf->mdd_tx_events));
+ 	memset(&vf->mdd_rx_events, 0, sizeof(vf->mdd_rx_events));
+ 
+ 	/* Disable interrupts so that VF starts in a known state */
+ 	for (i = vf->first_vector_idx; i <= last_vector_idx; i++) {
+ 		wr32(&pf->hw, GLINT_DYN_CTL(i), GLINT_DYN_CTL_CLEARPBA_M);
+ 		ice_flush(&pf->hw);
+ 	}
+ 	/* reset some of the state variables keeping track of the resources */
+ 	clear_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states);
+ 	clear_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states);
+ }
+ 
+ /**
+  * ice_dis_vf_mappings
+  * @vf: pointer to the VF structure
+  */
+ static void ice_dis_vf_mappings(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	int first, last, v;
+ 	struct ice_hw *hw;
+ 
+ 	hw = &pf->hw;
+ 	vsi = ice_get_vf_vsi(vf);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	wr32(hw, VPINT_ALLOC(vf->vf_id), 0);
+ 	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), 0);
+ 
+ 	first = vf->first_vector_idx;
+ 	last = first + pf->vfs.num_msix_per - 1;
+ 	for (v = first; v <= last; v++) {
+ 		u32 reg;
+ 
+ 		reg = (((1 << GLINT_VECT2FUNC_IS_PF_S) &
+ 			GLINT_VECT2FUNC_IS_PF_M) |
+ 		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
+ 			GLINT_VECT2FUNC_PF_NUM_M));
+ 		wr32(hw, GLINT_VECT2FUNC(v), reg);
+ 	}
+ 
+ 	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG)
+ 		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), 0);
+ 	else
+ 		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
+ 
+ 	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG)
+ 		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), 0);
+ 	else
+ 		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
+ }
+ 
+ /**
+  * ice_sriov_free_msix_res - Reset/free any used MSIX resources
+  * @pf: pointer to the PF structure
+  *
+  * Since no MSIX entries are taken from the pf->irq_tracker then just clear
+  * the pf->sriov_base_vector.
+  *
+  * Returns 0 on success, and -EINVAL on error.
+  */
+ static int ice_sriov_free_msix_res(struct ice_pf *pf)
+ {
+ 	struct ice_res_tracker *res;
+ 
+ 	if (!pf)
+ 		return -EINVAL;
+ 
+ 	res = pf->irq_tracker;
+ 	if (!res)
+ 		return -EINVAL;
+ 
+ 	/* give back irq_tracker resources used */
+ 	WARN_ON(pf->sriov_base_vector < res->num_entries);
+ 
+ 	pf->sriov_base_vector = 0;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_free_vfs - Free all VFs
+  * @pf: pointer to the PF structure
+  */
+ void ice_free_vfs(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vfs *vfs = &pf->vfs;
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	if (!ice_has_vfs(pf))
+ 		return;
+ 
+ 	while (test_and_set_bit(ICE_VF_DIS, pf->state))
+ 		usleep_range(1000, 2000);
+ 
+ 	/* Disable IOV before freeing resources. This lets any VF drivers
+ 	 * running in the host get themselves cleaned up before we yank
+ 	 * the carpet out from underneath their feet.
+ 	 */
+ 	if (!pci_vfs_assigned(pf->pdev))
+ 		pci_disable_sriov(pf->pdev);
+ 	else
+ 		dev_warn(dev, "VFs are assigned - not disabling SR-IOV\n");
+ 
+ 	mutex_lock(&vfs->table_lock);
+ 
+ 	ice_eswitch_release(pf);
+ 
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		mutex_lock(&vf->cfg_lock);
+ 
+ 		ice_dis_vf_qs(vf);
+ 
+ 		if (test_bit(ICE_VF_STATE_INIT, vf->vf_states)) {
+ 			/* disable VF qp mappings and set VF disable state */
+ 			ice_dis_vf_mappings(vf);
+ 			set_bit(ICE_VF_STATE_DIS, vf->vf_states);
+ 			ice_free_vf_res(vf);
+ 		}
+ 
+ 		if (!pci_vfs_assigned(pf->pdev)) {
+ 			u32 reg_idx, bit_idx;
+ 
+ 			reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
+ 			bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
+ 			wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
+ 		}
+ 
+ 		/* clear malicious info since the VF is getting released */
+ 		if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
+ 					ICE_MAX_SRIOV_VFS, vf->vf_id))
+ 			dev_dbg(dev, "failed to clear malicious VF state for VF %u\n",
+ 				vf->vf_id);
+ 
+ 		mutex_unlock(&vf->cfg_lock);
+ 	}
+ 
+ 	if (ice_sriov_free_msix_res(pf))
+ 		dev_err(dev, "Failed to free MSIX resources used by SR-IOV\n");
+ 
+ 	vfs->num_qps_per = 0;
+ 	ice_free_vf_entries(pf);
+ 
+ 	mutex_unlock(&vfs->table_lock);
+ 
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 	clear_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
+ }
+ 
+ /**
+  * ice_trigger_vf_reset - Reset a VF on HW
+  * @vf: pointer to the VF structure
+  * @is_vflr: true if VFLR was issued, false if not
+  * @is_pfr: true if the reset was triggered due to a previous PFR
+  *
+  * Trigger hardware to start a reset for a particular VF. Expects the caller
+  * to wait the proper amount of time to allow hardware to reset the VF before
+  * it cleans up and restores VF functionality.
+  */
+ static void ice_trigger_vf_reset(struct ice_vf *vf, bool is_vflr, bool is_pfr)
+ {
+ 	/* Inform VF that it is no longer active, as a warning */
+ 	clear_bit(ICE_VF_STATE_ACTIVE, vf->vf_states);
+ 
+ 	/* Disable VF's configuration API during reset. The flag is re-enabled
+ 	 * when it's safe again to access VF's VSI.
+ 	 */
+ 	clear_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 
+ 	/* VF_MBX_ARQLEN and VF_MBX_ATQLEN are cleared by PFR, so the driver
+ 	 * needs to clear them in the case of VFR/VFLR. If this is done for
+ 	 * PFR, it can mess up VF resets because the VF driver may already
+ 	 * have started cleanup by the time we get here.
+ 	 */
+ 	if (!is_pfr)
+ 		vf->vf_ops->clear_mbx_register(vf);
+ 
+ 	vf->vf_ops->trigger_reset_register(vf, is_vflr);
+ }
+ 
+ /**
+  * ice_vf_vsi_setup - Set up a VF VSI
+  * @vf: VF to setup VSI for
+  *
+  * Returns pointer to the successfully allocated VSI struct on success,
+  * otherwise returns NULL on failure.
+  */
+ static struct ice_vsi *ice_vf_vsi_setup(struct ice_vf *vf)
+ {
+ 	struct ice_port_info *pi = ice_vf_get_port_info(vf);
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 
+ 	vsi = ice_vsi_setup(pf, pi, ICE_VSI_VF, vf, NULL);
+ 
+ 	if (!vsi) {
+ 		dev_err(ice_pf_to_dev(pf), "Failed to create VF VSI\n");
+ 		ice_vf_invalidate_vsi(vf);
+ 		return NULL;
+ 	}
+ 
+ 	vf->lan_vsi_idx = vsi->idx;
+ 	vf->lan_vsi_num = vsi->vsi_num;
+ 
+ 	return vsi;
+ }
+ 
+ /**
+  * ice_calc_vf_first_vector_idx - Calculate MSIX vector index in the PF space
+  * @pf: pointer to PF structure
+  * @vf: pointer to VF that the first MSIX vector index is being calculated for
+  *
+  * This returns the first MSIX vector index in PF space that is used by this VF.
+  * This index is used when accessing PF relative registers such as
+  * GLINT_VECT2FUNC and GLINT_DYN_CTL.
+  * This will always be the OICR index in the AVF driver so any functionality
+  * using vf->first_vector_idx for queue configuration will have to increment by
+  * 1 to avoid meddling with the OICR index.
+  */
+ static int ice_calc_vf_first_vector_idx(struct ice_pf *pf, struct ice_vf *vf)
+ {
+ 	return pf->sriov_base_vector + vf->vf_id * pf->vfs.num_msix_per;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_tx_rate_cfg - re-apply the Tx rate limiting configuration
+  * @vf: VF to re-apply the configuration for
+  *
+  * Called after a VF VSI has been re-added/rebuild during reset. The PF driver
+  * needs to re-apply the host configured Tx rate limiting configuration.
+  */
+ static int ice_vf_rebuild_host_tx_rate_cfg(struct ice_vf *vf)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	int err;
+ 
+ 	if (vf->min_tx_rate) {
+ 		err = ice_set_min_bw_limit(vsi, (u64)vf->min_tx_rate * 1000);
+ 		if (err) {
+ 			dev_err(dev, "failed to set min Tx rate to %d Mbps for VF %u, error %d\n",
+ 				vf->min_tx_rate, vf->vf_id, err);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	if (vf->max_tx_rate) {
+ 		err = ice_set_max_bw_limit(vsi, (u64)vf->max_tx_rate * 1000);
+ 		if (err) {
+ 			dev_err(dev, "failed to set max Tx rate to %d Mbps for VF %u, error %d\n",
+ 				vf->max_tx_rate, vf->vf_id, err);
+ 			return err;
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_vlan_cfg - add VLAN 0 filter or rebuild the Port VLAN
+  * @vf: VF to add MAC filters for
+  * @vsi: Pointer to VSI
+  *
+  * Called after a VF VSI has been re-added/rebuilt during reset. The PF driver
+  * always re-adds either a VLAN 0 or port VLAN based filter after reset.
+  */
+ static int ice_vf_rebuild_host_vlan_cfg(struct ice_vf *vf, struct ice_vsi *vsi)
+ {
+ 	struct ice_vsi_vlan_ops *vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	int err;
+ 
+ 	if (ice_vf_is_port_vlan_ena(vf)) {
+ 		err = vlan_ops->set_port_vlan(vsi, &vf->port_vlan_info);
+ 		if (err) {
+ 			dev_err(dev, "failed to configure port VLAN via VSI parameters for VF %u, error %d\n",
+ 				vf->vf_id, err);
+ 			return err;
+ 		}
+ 
+ 		err = vlan_ops->add_vlan(vsi, &vf->port_vlan_info);
+ 	} else {
+ 		err = ice_vsi_add_vlan_zero(vsi);
+ 	}
+ 
+ 	if (err) {
+ 		dev_err(dev, "failed to add VLAN %u filter for VF %u during VF rebuild, error %d\n",
+ 			ice_vf_is_port_vlan_ena(vf) ?
+ 			ice_vf_get_port_vlan_id(vf) : 0, vf->vf_id, err);
+ 		return err;
+ 	}
+ 
+ 	err = vlan_ops->ena_rx_filtering(vsi);
+ 	if (err)
+ 		dev_warn(dev, "failed to enable Rx VLAN filtering for VF %d VSI %d during VF rebuild, error %d\n",
+ 			 vf->vf_id, vsi->idx, err);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_mac_cfg - add broadcast and the VF's perm_addr/LAA
+  * @vf: VF to add MAC filters for
+  *
+  * Called after a VF VSI has been re-added/rebuilt during reset. The PF driver
+  * always re-adds a broadcast filter and the VF's perm_addr/LAA after reset.
+  */
+ static int ice_vf_rebuild_host_mac_cfg(struct ice_vf *vf)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	u8 broadcast[ETH_ALEN];
+ 	int status;
+ 
+ 	if (ice_is_eswitch_mode_switchdev(vf->pf))
+ 		return 0;
+ 
+ 	eth_broadcast_addr(broadcast);
+ 	status = ice_fltr_add_mac(vsi, broadcast, ICE_FWD_TO_VSI);
+ 	if (status) {
+ 		dev_err(dev, "failed to add broadcast MAC filter for VF %u, error %d\n",
+ 			vf->vf_id, status);
+ 		return status;
+ 	}
+ 
+ 	vf->num_mac++;
+ 
+ 	if (is_valid_ether_addr(vf->hw_lan_addr.addr)) {
+ 		status = ice_fltr_add_mac(vsi, vf->hw_lan_addr.addr,
+ 					  ICE_FWD_TO_VSI);
+ 		if (status) {
+ 			dev_err(dev, "failed to add default unicast MAC filter %pM for VF %u, error %d\n",
+ 				&vf->hw_lan_addr.addr[0], vf->vf_id,
+ 				status);
+ 			return status;
+ 		}
+ 		vf->num_mac++;
+ 
+ 		ether_addr_copy(vf->dev_lan_addr.addr, vf->hw_lan_addr.addr);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_vf_set_host_trust_cfg - set trust setting based on pre-reset value
+  * @vf: VF to configure trust setting for
+  */
+ static void ice_vf_set_host_trust_cfg(struct ice_vf *vf)
+ {
+ 	if (vf->trusted)
+ 		set_bit(ICE_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps);
+ 	else
+ 		clear_bit(ICE_VIRTCHNL_VF_CAP_PRIVILEGE, &vf->vf_caps);
+ }
+ 
+ /**
+  * ice_ena_vf_msix_mappings - enable VF MSIX mappings in hardware
+  * @vf: VF to enable MSIX mappings for
+  *
+  * Some of the registers need to be indexed/configured using hardware global
+  * device values and other registers need 0-based values, which represent PF
+  * based values.
+  */
+ static void ice_ena_vf_msix_mappings(struct ice_vf *vf)
+ {
+ 	int device_based_first_msix, device_based_last_msix;
+ 	int pf_based_first_msix, pf_based_last_msix, v;
+ 	struct ice_pf *pf = vf->pf;
+ 	int device_based_vf_id;
+ 	struct ice_hw *hw;
+ 	u32 reg;
+ 
+ 	hw = &pf->hw;
+ 	pf_based_first_msix = vf->first_vector_idx;
+ 	pf_based_last_msix = (pf_based_first_msix + pf->vfs.num_msix_per) - 1;
+ 
+ 	device_based_first_msix = pf_based_first_msix +
+ 		pf->hw.func_caps.common_cap.msix_vector_first_id;
+ 	device_based_last_msix =
+ 		(device_based_first_msix + pf->vfs.num_msix_per) - 1;
+ 	device_based_vf_id = vf->vf_id + hw->func_caps.vf_base_id;
+ 
+ 	reg = (((device_based_first_msix << VPINT_ALLOC_FIRST_S) &
+ 		VPINT_ALLOC_FIRST_M) |
+ 	       ((device_based_last_msix << VPINT_ALLOC_LAST_S) &
+ 		VPINT_ALLOC_LAST_M) | VPINT_ALLOC_VALID_M);
+ 	wr32(hw, VPINT_ALLOC(vf->vf_id), reg);
+ 
+ 	reg = (((device_based_first_msix << VPINT_ALLOC_PCI_FIRST_S)
+ 		 & VPINT_ALLOC_PCI_FIRST_M) |
+ 	       ((device_based_last_msix << VPINT_ALLOC_PCI_LAST_S) &
+ 		VPINT_ALLOC_PCI_LAST_M) | VPINT_ALLOC_PCI_VALID_M);
+ 	wr32(hw, VPINT_ALLOC_PCI(vf->vf_id), reg);
+ 
+ 	/* map the interrupts to its functions */
+ 	for (v = pf_based_first_msix; v <= pf_based_last_msix; v++) {
+ 		reg = (((device_based_vf_id << GLINT_VECT2FUNC_VF_NUM_S) &
+ 			GLINT_VECT2FUNC_VF_NUM_M) |
+ 		       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &
+ 			GLINT_VECT2FUNC_PF_NUM_M));
+ 		wr32(hw, GLINT_VECT2FUNC(v), reg);
+ 	}
+ 
+ 	/* Map mailbox interrupt to VF MSI-X vector 0 */
+ 	wr32(hw, VPINT_MBX_CTL(device_based_vf_id), VPINT_MBX_CTL_CAUSE_ENA_M);
+ }
+ 
+ /**
+  * ice_ena_vf_q_mappings - enable Rx/Tx queue mappings for a VF
+  * @vf: VF to enable the mappings for
+  * @max_txq: max Tx queues allowed on the VF's VSI
+  * @max_rxq: max Rx queues allowed on the VF's VSI
+  */
+ static void ice_ena_vf_q_mappings(struct ice_vf *vf, u16 max_txq, u16 max_rxq)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 	u32 reg;
+ 
+ 	/* set regardless of mapping mode */
+ 	wr32(hw, VPLAN_TXQ_MAPENA(vf->vf_id), VPLAN_TXQ_MAPENA_TX_ENA_M);
+ 
+ 	/* VF Tx queues allocation */
+ 	if (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG) {
+ 		/* set the VF PF Tx queue range
+ 		 * VFNUMQ value should be set to (number of queues - 1). A value
+ 		 * of 0 means 1 queue and a value of 255 means 256 queues
+ 		 */
+ 		reg = (((vsi->txq_map[0] << VPLAN_TX_QBASE_VFFIRSTQ_S) &
+ 			VPLAN_TX_QBASE_VFFIRSTQ_M) |
+ 		       (((max_txq - 1) << VPLAN_TX_QBASE_VFNUMQ_S) &
+ 			VPLAN_TX_QBASE_VFNUMQ_M));
+ 		wr32(hw, VPLAN_TX_QBASE(vf->vf_id), reg);
+ 	} else {
+ 		dev_err(dev, "Scattered mode for VF Tx queues is not yet implemented\n");
+ 	}
+ 
+ 	/* set regardless of mapping mode */
+ 	wr32(hw, VPLAN_RXQ_MAPENA(vf->vf_id), VPLAN_RXQ_MAPENA_RX_ENA_M);
+ 
+ 	/* VF Rx queues allocation */
+ 	if (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG) {
+ 		/* set the VF PF Rx queue range
+ 		 * VFNUMQ value should be set to (number of queues - 1). A value
+ 		 * of 0 means 1 queue and a value of 255 means 256 queues
+ 		 */
+ 		reg = (((vsi->rxq_map[0] << VPLAN_RX_QBASE_VFFIRSTQ_S) &
+ 			VPLAN_RX_QBASE_VFFIRSTQ_M) |
+ 		       (((max_rxq - 1) << VPLAN_RX_QBASE_VFNUMQ_S) &
+ 			VPLAN_RX_QBASE_VFNUMQ_M));
+ 		wr32(hw, VPLAN_RX_QBASE(vf->vf_id), reg);
+ 	} else {
+ 		dev_err(dev, "Scattered mode for VF Rx queues is not yet implemented\n");
+ 	}
+ }
+ 
+ /**
+  * ice_ena_vf_mappings - enable VF MSIX and queue mapping
+  * @vf: pointer to the VF structure
+  */
+ static void ice_ena_vf_mappings(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_ena_vf_msix_mappings(vf);
+ 	ice_ena_vf_q_mappings(vf, vsi->alloc_txq, vsi->alloc_rxq);
+ }
+ 
+ /**
+  * ice_calc_vf_reg_idx - Calculate the VF's register index in the PF space
+  * @vf: VF to calculate the register index for
+  * @q_vector: a q_vector associated to the VF
+  */
+ int ice_calc_vf_reg_idx(struct ice_vf *vf, struct ice_q_vector *q_vector)
+ {
+ 	struct ice_pf *pf;
+ 
+ 	if (!vf || !q_vector)
+ 		return -EINVAL;
+ 
+ 	pf = vf->pf;
+ 
+ 	/* always add one to account for the OICR being the first MSIX */
+ 	return pf->sriov_base_vector + pf->vfs.num_msix_per * vf->vf_id +
+ 		q_vector->v_idx + 1;
+ }
+ 
+ /**
+  * ice_get_max_valid_res_idx - Get the max valid resource index
+  * @res: pointer to the resource to find the max valid index for
+  *
+  * Start from the end of the ice_res_tracker and return right when we find the
+  * first res->list entry with the ICE_RES_VALID_BIT set. This function is only
+  * valid for SR-IOV because it is the only consumer that manipulates the
+  * res->end and this is always called when res->end is set to res->num_entries.
+  */
+ static int ice_get_max_valid_res_idx(struct ice_res_tracker *res)
+ {
+ 	int i;
+ 
+ 	if (!res)
+ 		return -EINVAL;
+ 
+ 	for (i = res->num_entries - 1; i >= 0; i--)
+ 		if (res->list[i] & ICE_RES_VALID_BIT)
+ 			return i;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_set_msix_res - Set any used MSIX resources
+  * @pf: pointer to PF structure
+  * @num_msix_needed: number of MSIX vectors needed for all SR-IOV VFs
+  *
+  * This function allows SR-IOV resources to be taken from the end of the PF's
+  * allowed HW MSIX vectors so that the irq_tracker will not be affected. We
+  * just set the pf->sriov_base_vector and return success.
+  *
+  * If there are not enough resources available, return an error. This should
+  * always be caught by ice_set_per_vf_res().
+  *
+  * Return 0 on success, and -EINVAL when there are not enough MSIX vectors
+  * in the PF's space available for SR-IOV.
+  */
+ static int ice_sriov_set_msix_res(struct ice_pf *pf, u16 num_msix_needed)
+ {
+ 	u16 total_vectors = pf->hw.func_caps.common_cap.num_msix_vectors;
+ 	int vectors_used = pf->irq_tracker->num_entries;
+ 	int sriov_base_vector;
+ 
+ 	sriov_base_vector = total_vectors - num_msix_needed;
+ 
+ 	/* make sure we only grab irq_tracker entries from the list end and
+ 	 * that we have enough available MSIX vectors
+ 	 */
+ 	if (sriov_base_vector < vectors_used)
+ 		return -EINVAL;
+ 
+ 	pf->sriov_base_vector = sriov_base_vector;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_set_per_vf_res - check if vectors and queues are available
+  * @pf: pointer to the PF structure
+  * @num_vfs: the number of SR-IOV VFs being configured
+  *
+  * First, determine HW interrupts from common pool. If we allocate fewer VFs, we
+  * get more vectors and can enable more queues per VF. Note that this does not
+  * grab any vectors from the SW pool already allocated. Also note, that all
+  * vector counts include one for each VF's miscellaneous interrupt vector
+  * (i.e. OICR).
+  *
+  * Minimum VFs - 2 vectors, 1 queue pair
+  * Small VFs - 5 vectors, 4 queue pairs
+  * Medium VFs - 17 vectors, 16 queue pairs
+  *
+  * Second, determine number of queue pairs per VF by starting with a pre-defined
+  * maximum each VF supports. If this is not possible, then we adjust based on
+  * queue pairs available on the device.
+  *
+  * Lastly, set queue and MSI-X VF variables tracked by the PF so it can be used
+  * by each VF during VF initialization and reset.
+  */
+ static int ice_set_per_vf_res(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	int max_valid_res_idx = ice_get_max_valid_res_idx(pf->irq_tracker);
+ 	u16 num_msix_per_vf, num_txq, num_rxq, avail_qs;
+ 	int msix_avail_per_vf, msix_avail_for_sriov;
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	lockdep_assert_held(&pf->vfs.table_lock);
+ 
+ 	if (!num_vfs)
+ 		return -EINVAL;
+ 
+ 	if (max_valid_res_idx < 0)
+ 		return -ENOSPC;
+ 
+ 	/* determine MSI-X resources per VF */
+ 	msix_avail_for_sriov = pf->hw.func_caps.common_cap.num_msix_vectors -
+ 		pf->irq_tracker->num_entries;
+ 	msix_avail_per_vf = msix_avail_for_sriov / num_vfs;
+ 	if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MED) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_MED;
+ 	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_SMALL) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_SMALL;
+ 	} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MULTIQ_MIN) {
+ 		num_msix_per_vf = ICE_NUM_VF_MSIX_MULTIQ_MIN;
+ 	} else if (msix_avail_per_vf >= ICE_MIN_INTR_PER_VF) {
+ 		num_msix_per_vf = ICE_MIN_INTR_PER_VF;
+ 	} else {
+ 		dev_err(dev, "Only %d MSI-X interrupts available for SR-IOV. Not enough to support minimum of %d MSI-X interrupts per VF for %d VFs\n",
+ 			msix_avail_for_sriov, ICE_MIN_INTR_PER_VF,
+ 			num_vfs);
+ 		return -ENOSPC;
+ 	}
+ 
+ 	num_txq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
+ 			ICE_MAX_RSS_QS_PER_VF);
+ 	avail_qs = ice_get_avail_txq_count(pf) / num_vfs;
+ 	if (!avail_qs)
+ 		num_txq = 0;
+ 	else if (num_txq > avail_qs)
+ 		num_txq = rounddown_pow_of_two(avail_qs);
+ 
+ 	num_rxq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,
+ 			ICE_MAX_RSS_QS_PER_VF);
+ 	avail_qs = ice_get_avail_rxq_count(pf) / num_vfs;
+ 	if (!avail_qs)
+ 		num_rxq = 0;
+ 	else if (num_rxq > avail_qs)
+ 		num_rxq = rounddown_pow_of_two(avail_qs);
+ 
+ 	if (num_txq < ICE_MIN_QS_PER_VF || num_rxq < ICE_MIN_QS_PER_VF) {
+ 		dev_err(dev, "Not enough queues to support minimum of %d queue pairs per VF for %d VFs\n",
+ 			ICE_MIN_QS_PER_VF, num_vfs);
+ 		return -ENOSPC;
+ 	}
+ 
+ 	err = ice_sriov_set_msix_res(pf, num_msix_per_vf * num_vfs);
+ 	if (err) {
+ 		dev_err(dev, "Unable to set MSI-X resources for %d VFs, err %d\n",
+ 			num_vfs, err);
+ 		return err;
+ 	}
+ 
+ 	/* only allow equal Tx/Rx queue count (i.e. queue pairs) */
+ 	pf->vfs.num_qps_per = min_t(int, num_txq, num_rxq);
+ 	pf->vfs.num_msix_per = num_msix_per_vf;
+ 	dev_info(dev, "Enabling %d VFs with %d vectors and %d queues per VF\n",
+ 		 num_vfs, pf->vfs.num_msix_per, pf->vfs.num_qps_per);
+ 
+ 	return 0;
+ }
+ 
+ static void ice_vf_clear_counters(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	vf->num_mac = 0;
+ 	vsi->num_vlan = 0;
+ 	memset(&vf->mdd_tx_events, 0, sizeof(vf->mdd_tx_events));
+ 	memset(&vf->mdd_rx_events, 0, sizeof(vf->mdd_rx_events));
+ }
+ 
+ /**
+  * ice_vf_pre_vsi_rebuild - tasks to be done prior to VSI rebuild
+  * @vf: VF to perform pre VSI rebuild tasks
+  *
+  * These tasks are items that don't need to be amortized since they are most
+  * likely called in a for loop with all VF(s) in the reset_all_vfs() case.
+  */
+ static void ice_vf_pre_vsi_rebuild(struct ice_vf *vf)
+ {
+ 	ice_vf_clear_counters(vf);
+ 	vf->vf_ops->clear_reset_trigger(vf);
+ }
+ 
+ /**
+  * ice_vf_rebuild_aggregator_node_cfg - rebuild aggregator node config
+  * @vsi: Pointer to VSI
+  *
+  * This function moves VSI into corresponding scheduler aggregator node
+  * based on cached value of "aggregator node info" per VSI
+  */
+ static void ice_vf_rebuild_aggregator_node_cfg(struct ice_vsi *vsi)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct device *dev;
+ 	int status;
+ 
+ 	if (!vsi->agg_node)
+ 		return;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	if (vsi->agg_node->num_vsis == ICE_MAX_VSIS_IN_AGG_NODE) {
+ 		dev_dbg(dev,
+ 			"agg_id %u already has reached max_num_vsis %u\n",
+ 			vsi->agg_node->agg_id, vsi->agg_node->num_vsis);
+ 		return;
+ 	}
+ 
+ 	status = ice_move_vsi_to_agg(pf->hw.port_info, vsi->agg_node->agg_id,
+ 				     vsi->idx, vsi->tc_cfg.ena_tc);
+ 	if (status)
+ 		dev_dbg(dev, "unable to move VSI idx %u into aggregator %u node",
+ 			vsi->idx, vsi->agg_node->agg_id);
+ 	else
+ 		vsi->agg_node->num_vsis++;
+ }
+ 
+ /**
+  * ice_vf_rebuild_host_cfg - host admin configuration is persistent across reset
+  * @vf: VF to rebuild host configuration on
+  */
+ static void ice_vf_rebuild_host_cfg(struct ice_vf *vf)
+ {
+ 	struct device *dev = ice_pf_to_dev(vf->pf);
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_vf_set_host_trust_cfg(vf);
+ 
+ 	if (ice_vf_rebuild_host_mac_cfg(vf))
+ 		dev_err(dev, "failed to rebuild default MAC configuration for VF %d\n",
+ 			vf->vf_id);
+ 
+ 	if (ice_vf_rebuild_host_vlan_cfg(vf, vsi))
+ 		dev_err(dev, "failed to rebuild VLAN configuration for VF %u\n",
+ 			vf->vf_id);
+ 
+ 	if (ice_vf_rebuild_host_tx_rate_cfg(vf))
+ 		dev_err(dev, "failed to rebuild Tx rate limiting configuration for VF %u\n",
+ 			vf->vf_id);
+ 
+ 	if (ice_vsi_apply_spoofchk(vsi, vf->spoofchk))
+ 		dev_err(dev, "failed to rebuild spoofchk configuration for VF %d\n",
+ 			vf->vf_id);
+ 
+ 	/* rebuild aggregator node config for main VF VSI */
+ 	ice_vf_rebuild_aggregator_node_cfg(vsi);
+ }
+ 
+ /**
+  * ice_vf_rebuild_vsi - rebuild the VF's VSI
+  * @vf: VF to rebuild the VSI for
+  *
+  * This is only called when all VF(s) are being reset (i.e. PCIe Reset on the
+  * host, PFR, CORER, etc.).
+  */
+ static int ice_vf_rebuild_vsi(struct ice_vf *vf)
+ {
+ 	struct ice_vsi *vsi = ice_get_vf_vsi(vf);
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	if (ice_vsi_rebuild(vsi, true)) {
+ 		dev_err(ice_pf_to_dev(pf), "failed to rebuild VF %d VSI\n",
+ 			vf->vf_id);
+ 		return -EIO;
+ 	}
+ 	/* vsi->idx will remain the same in this case so don't update
+ 	 * vf->lan_vsi_idx
+ 	 */
+ 	vsi->vsi_num = ice_get_hw_vsi_num(&pf->hw, vsi->idx);
+ 	vf->lan_vsi_num = vsi->vsi_num;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_reset_all_vfs - reset all allocated VFs in one go
+  * @pf: pointer to the PF structure
+  * @is_vflr: true if VFLR was issued, false if not
+  *
+  * First, tell the hardware to reset each VF, then do all the waiting in one
+  * chunk, and finally finish restoring each VF after the wait. This is useful
+  * during PF routines which need to reset all VFs, as otherwise it must perform
+  * these resets in a serialized fashion.
+  *
+  * Returns true if any VFs were reset, and false otherwise.
+  */
+ bool ice_reset_all_vfs(struct ice_pf *pf, bool is_vflr)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	/* If we don't have any VFs, then there is nothing to reset */
+ 	if (!ice_has_vfs(pf))
+ 		return false;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 
+ 	/* clear all malicious info if the VFs are getting reset */
+ 	ice_for_each_vf(pf, bkt, vf)
+ 		if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
+ 					ICE_MAX_SRIOV_VFS, vf->vf_id))
+ 			dev_dbg(dev, "failed to clear malicious VF state for VF %u\n",
+ 				vf->vf_id);
+ 
+ 	/* If VFs have been disabled, there is no need to reset */
+ 	if (test_and_set_bit(ICE_VF_DIS, pf->state)) {
+ 		mutex_unlock(&pf->vfs.table_lock);
+ 		return false;
+ 	}
+ 
+ 	/* Begin reset on all VFs at once */
+ 	ice_for_each_vf(pf, bkt, vf)
+ 		ice_trigger_vf_reset(vf, is_vflr, true);
+ 
+ 	/* HW requires some time to make sure it can flush the FIFO for a VF
+ 	 * when it resets it. Now that we've triggered all of the VFs, iterate
+ 	 * the table again and wait for each VF to complete.
+ 	 */
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		if (!vf->vf_ops->poll_reset_status(vf)) {
+ 			/* Display a warning if at least one VF didn't manage
+ 			 * to reset in time, but continue on with the
+ 			 * operation.
+ 			 */
+ 			dev_warn(dev, "VF %u reset check timeout\n", vf->vf_id);
+ 			break;
+ 		}
+ 	}
+ 
+ 	/* free VF resources to begin resetting the VSI state */
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		mutex_lock(&vf->cfg_lock);
+ 
+ 		vf->driver_caps = 0;
+ 		ice_vc_set_default_allowlist(vf);
+ 
+ 		ice_vf_fdir_exit(vf);
+ 		ice_vf_fdir_init(vf);
+ 		/* clean VF control VSI when resetting VFs since it should be
+ 		 * setup only when VF creates its first FDIR rule.
+ 		 */
+ 		if (vf->ctrl_vsi_idx != ICE_NO_VSI)
+ 			ice_vf_ctrl_invalidate_vsi(vf);
+ 
+ 		ice_vf_pre_vsi_rebuild(vf);
+ 		ice_vf_rebuild_vsi(vf);
+ 		vf->vf_ops->post_vsi_rebuild(vf);
+ 
+ 		mutex_unlock(&vf->cfg_lock);
+ 	}
+ 
+ 	if (ice_is_eswitch_mode_switchdev(pf))
+ 		if (ice_eswitch_rebuild(pf))
+ 			dev_warn(dev, "eswitch rebuild failed\n");
+ 
+ 	ice_flush(hw);
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * ice_reset_vf - Reset a particular VF
+  * @vf: pointer to the VF structure
+  * @is_vflr: true if VFLR was issued, false if not
+  *
+  * Returns true if the VF is currently in reset, resets successfully, or resets
+  * are disabled and false otherwise.
+  */
+ bool ice_reset_vf(struct ice_vf *vf, bool is_vflr)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	struct ice_hw *hw;
+ 	u8 promisc_m;
+ 	bool rsd;
+ 
+ 	lockdep_assert_held(&vf->cfg_lock);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	hw = &pf->hw;
+ 
+ 	if (test_bit(ICE_VF_RESETS_DISABLED, pf->state)) {
+ 		dev_dbg(dev, "Trying to reset VF %d, but all VF resets are disabled\n",
+ 			vf->vf_id);
+ 		return true;
+ 	}
+ 
+ 	if (ice_is_vf_disabled(vf)) {
+ 		dev_dbg(dev, "VF is already disabled, there is no need for resetting it, telling VM, all is fine %d\n",
+ 			vf->vf_id);
+ 		return true;
+ 	}
+ 
+ 	/* Set VF disable bit state here, before triggering reset */
+ 	set_bit(ICE_VF_STATE_DIS, vf->vf_states);
+ 	ice_trigger_vf_reset(vf, is_vflr, false);
+ 
+ 	vsi = ice_get_vf_vsi(vf);
+ 
+ 	ice_dis_vf_qs(vf);
+ 
+ 	/* Call Disable LAN Tx queue AQ whether or not queues are
+ 	 * enabled. This is needed for successful completion of VFR.
+ 	 */
+ 	ice_dis_vsi_txq(vsi->port_info, vsi->idx, 0, 0, NULL, NULL,
+ 			NULL, vf->vf_ops->reset_type, vf->vf_id, NULL);
+ 
+ 	/* poll VPGEN_VFRSTAT reg to make sure
+ 	 * that reset is complete
+ 	 */
+ 	rsd = vf->vf_ops->poll_reset_status(vf);
+ 
+ 	/* Display a warning if VF didn't manage to reset in time, but need to
+ 	 * continue on with the operation.
+ 	 */
+ 	if (!rsd)
+ 		dev_warn(dev, "VF reset check timeout on VF %d\n", vf->vf_id);
+ 
+ 	vf->driver_caps = 0;
+ 	ice_vc_set_default_allowlist(vf);
+ 
+ 	/* disable promiscuous modes in case they were enabled
+ 	 * ignore any error if disabling process failed
+ 	 */
+ 	if (test_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states) ||
+ 	    test_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states)) {
+ 		if (ice_vf_is_port_vlan_ena(vf) || vsi->num_vlan)
+ 			promisc_m = ICE_UCAST_VLAN_PROMISC_BITS;
+ 		else
+ 			promisc_m = ICE_UCAST_PROMISC_BITS;
+ 
+ 		if (ice_vf_clear_vsi_promisc(vf, vsi, promisc_m))
+ 			dev_err(dev, "disabling promiscuous mode failed\n");
+ 	}
+ 
+ 	ice_eswitch_del_vf_mac_rule(vf);
+ 
+ 	ice_vf_fdir_exit(vf);
+ 	ice_vf_fdir_init(vf);
+ 	/* clean VF control VSI when resetting VF since it should be setup
+ 	 * only when VF creates its first FDIR rule.
+ 	 */
+ 	if (vf->ctrl_vsi_idx != ICE_NO_VSI)
+ 		ice_vf_ctrl_vsi_release(vf);
+ 
+ 	ice_vf_pre_vsi_rebuild(vf);
+ 
+ 	if (vf->vf_ops->vsi_rebuild(vf)) {
+ 		dev_err(dev, "Failed to release and setup the VF%u's VSI\n", vf->vf_id);
+ 		return false;
+ 	}
+ 
+ 	vf->vf_ops->post_vsi_rebuild(vf);
+ 	vsi = ice_get_vf_vsi(vf);
+ 	ice_eswitch_update_repr(vsi);
+ 	ice_eswitch_replay_vf_mac_rule(vf);
+ 
+ 	/* if the VF has been reset allow it to come up again */
+ 	if (ice_mbx_clear_malvf(&hw->mbx_snapshot, pf->vfs.malvfs,
+ 				ICE_MAX_SRIOV_VFS, vf->vf_id))
+ 		dev_dbg(dev, "failed to clear malicious VF state for VF %u\n",
+ 			vf->vf_id);
+ 
+ 	return true;
+ }
+ 
+ /**
+  * ice_vc_notify_link_state - Inform all VFs on a PF of link status
+  * @pf: pointer to the PF structure
+  */
+ void ice_vc_notify_link_state(struct ice_pf *pf)
+ {
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 	ice_for_each_vf(pf, bkt, vf)
+ 		ice_vc_notify_vf_link_state(vf);
+ 	mutex_unlock(&pf->vfs.table_lock);
+ }
+ 
+ /**
+  * ice_vc_notify_reset - Send pending reset message to all VFs
+  * @pf: pointer to the PF structure
+  *
+  * indicate a pending reset to all VFs on a given PF
+  */
+ void ice_vc_notify_reset(struct ice_pf *pf)
+ {
+ 	struct virtchnl_pf_event pfe;
+ 
+ 	if (!ice_has_vfs(pf))
+ 		return;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
+ 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
+ 	ice_vc_vf_broadcast(pf, VIRTCHNL_OP_EVENT, VIRTCHNL_STATUS_SUCCESS,
+ 			    (u8 *)&pfe, sizeof(struct virtchnl_pf_event));
+ }
+ 
+ /**
+  * ice_vc_notify_vf_reset - Notify VF of a reset event
+  * @vf: pointer to the VF structure
+  */
+ static void ice_vc_notify_vf_reset(struct ice_vf *vf)
+ {
+ 	struct virtchnl_pf_event pfe;
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	/* Bail out if VF is in disabled state, neither initialized, nor active
+ 	 * state - otherwise proceed with notifications
+ 	 */
+ 	if ((!test_bit(ICE_VF_STATE_INIT, vf->vf_states) &&
+ 	     !test_bit(ICE_VF_STATE_ACTIVE, vf->vf_states)) ||
+ 	    test_bit(ICE_VF_STATE_DIS, vf->vf_states))
+ 		return;
+ 
+ 	pfe.event = VIRTCHNL_EVENT_RESET_IMPENDING;
+ 	pfe.severity = PF_EVENT_SEVERITY_CERTAIN_DOOM;
+ 	ice_aq_send_msg_to_vf(&pf->hw, vf->vf_id, VIRTCHNL_OP_EVENT,
+ 			      VIRTCHNL_STATUS_SUCCESS, (u8 *)&pfe, sizeof(pfe),
+ 			      NULL);
+ }
+ 
+ /**
+  * ice_init_vf_vsi_res - initialize/setup VF VSI resources
+  * @vf: VF to initialize/setup the VSI for
+  *
+  * This function creates a VSI for the VF, adds a VLAN 0 filter, and sets up the
+  * VF VSI's broadcast filter and is only used during initial VF creation.
+  */
+ static int ice_init_vf_vsi_res(struct ice_vf *vf)
+ {
+ 	struct ice_vsi_vlan_ops *vlan_ops;
+ 	struct ice_pf *pf = vf->pf;
+ 	u8 broadcast[ETH_ALEN];
+ 	struct ice_vsi *vsi;
+ 	struct device *dev;
+ 	int err;
+ 
+ 	vf->first_vector_idx = ice_calc_vf_first_vector_idx(pf, vf);
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	vsi = ice_vf_vsi_setup(vf);
+ 	if (!vsi)
+ 		return -ENOMEM;
+ 
+ 	err = ice_vsi_add_vlan_zero(vsi);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to add VLAN 0 filter for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);
+ 	err = vlan_ops->ena_rx_filtering(vsi);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to enable Rx VLAN filtering for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	eth_broadcast_addr(broadcast);
+ 	err = ice_fltr_add_mac(vsi, broadcast, ICE_FWD_TO_VSI);
+ 	if (err) {
+ 		dev_err(dev, "Failed to add broadcast MAC filter for VF %d, error %d\n",
+ 			vf->vf_id, err);
+ 		goto release_vsi;
+ 	}
+ 
+ 	err = ice_vsi_apply_spoofchk(vsi, vf->spoofchk);
+ 	if (err) {
+ 		dev_warn(dev, "Failed to initialize spoofchk setting for VF %d\n",
+ 			 vf->vf_id);
+ 		goto release_vsi;
+ 	}
+ 
+ 	vf->num_mac = 1;
+ 
+ 	return 0;
+ 
+ release_vsi:
+ 	ice_vf_vsi_release(vf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_start_vfs - start VFs so they are ready to be used by SR-IOV
+  * @pf: PF the VFs are associated with
+  */
+ static int ice_start_vfs(struct ice_pf *pf)
+ {
+ 	struct ice_hw *hw = &pf->hw;
+ 	unsigned int bkt, it_cnt;
+ 	struct ice_vf *vf;
+ 	int retval;
+ 
+ 	lockdep_assert_held(&pf->vfs.table_lock);
+ 
+ 	it_cnt = 0;
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		vf->vf_ops->clear_reset_trigger(vf);
+ 
+ 		retval = ice_init_vf_vsi_res(vf);
+ 		if (retval) {
+ 			dev_err(ice_pf_to_dev(pf), "Failed to initialize VSI resources for VF %d, error %d\n",
+ 				vf->vf_id, retval);
+ 			goto teardown;
+ 		}
+ 
+ 		set_bit(ICE_VF_STATE_INIT, vf->vf_states);
+ 		ice_ena_vf_mappings(vf);
+ 		wr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
+ 		it_cnt++;
+ 	}
+ 
+ 	ice_flush(hw);
+ 	return 0;
+ 
+ teardown:
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		if (it_cnt == 0)
+ 			break;
+ 
+ 		ice_dis_vf_mappings(vf);
+ 		ice_vf_vsi_release(vf);
+ 		it_cnt--;
+ 	}
+ 
+ 	return retval;
+ }
+ 
+ /**
+  * ice_sriov_free_vf - Free VF memory after all references are dropped
+  * @vf: pointer to VF to free
+  *
+  * Called by ice_put_vf through ice_release_vf once the last reference to a VF
+  * structure has been dropped.
+  */
+ static void ice_sriov_free_vf(struct ice_vf *vf)
+ {
+ 	mutex_destroy(&vf->cfg_lock);
+ 
+ 	kfree_rcu(vf, rcu);
+ }
+ 
+ /**
+  * ice_sriov_clear_mbx_register - clears SRIOV VF's mailbox registers
+  * @vf: the vf to configure
+  */
+ static void ice_sriov_clear_mbx_register(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	wr32(&pf->hw, VF_MBX_ARQLEN(vf->vf_id), 0);
+ 	wr32(&pf->hw, VF_MBX_ATQLEN(vf->vf_id), 0);
+ }
+ 
+ /**
+  * ice_sriov_trigger_reset_register - trigger VF reset for SRIOV VF
+  * @vf: pointer to VF structure
+  * @is_vflr: true if reset occurred due to VFLR
+  *
+  * Trigger and cleanup after a VF reset for a SR-IOV VF.
+  */
+ static void ice_sriov_trigger_reset_register(struct ice_vf *vf, bool is_vflr)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	u32 reg, reg_idx, bit_idx;
+ 	unsigned int vf_abs_id, i;
+ 	struct device *dev;
+ 	struct ice_hw *hw;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	hw = &pf->hw;
+ 	vf_abs_id = vf->vf_id + hw->func_caps.vf_base_id;
+ 
+ 	/* In the case of a VFLR, HW has already reset the VF and we just need
+ 	 * to clean up. Otherwise we must first trigger the reset using the
+ 	 * VFRTRIG register.
+ 	 */
+ 	if (!is_vflr) {
+ 		reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
+ 		reg |= VPGEN_VFRTRIG_VFSWR_M;
+ 		wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
+ 	}
+ 
+ 	/* clear the VFLR bit in GLGEN_VFLRSTAT */
+ 	reg_idx = (vf_abs_id) / 32;
+ 	bit_idx = (vf_abs_id) % 32;
+ 	wr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));
+ 	ice_flush(hw);
+ 
+ 	wr32(hw, PF_PCI_CIAA,
+ 	     VF_DEVICE_STATUS | (vf_abs_id << PF_PCI_CIAA_VF_NUM_S));
+ 	for (i = 0; i < ICE_PCI_CIAD_WAIT_COUNT; i++) {
+ 		reg = rd32(hw, PF_PCI_CIAD);
+ 		/* no transactions pending so stop polling */
+ 		if ((reg & VF_TRANS_PENDING_M) == 0)
+ 			break;
+ 
+ 		dev_err(dev, "VF %u PCI transactions stuck\n", vf->vf_id);
+ 		udelay(ICE_PCI_CIAD_WAIT_DELAY_US);
+ 	}
+ }
+ 
+ /**
+  * ice_sriov_poll_reset_status - poll SRIOV VF reset status
+  * @vf: pointer to VF structure
+  *
+  * Returns true when reset is successful, else returns false
+  */
+ static bool ice_sriov_poll_reset_status(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 	unsigned int i;
+ 	u32 reg;
+ 
+ 	for (i = 0; i < 10; i++) {
+ 		/* VF reset requires driver to first reset the VF and then
+ 		 * poll the status register to make sure that the reset
+ 		 * completed successfully.
+ 		 */
+ 		reg = rd32(&pf->hw, VPGEN_VFRSTAT(vf->vf_id));
+ 		if (reg & VPGEN_VFRSTAT_VFRD_M)
+ 			return true;
+ 
+ 		/* only sleep if the reset is not done */
+ 		usleep_range(10, 20);
+ 	}
+ 	return false;
+ }
+ 
+ /**
+  * ice_sriov_clear_reset_trigger - enable VF to access hardware
+  * @vf: VF to enabled hardware access for
+  */
+ static void ice_sriov_clear_reset_trigger(struct ice_vf *vf)
+ {
+ 	struct ice_hw *hw = &vf->pf->hw;
+ 	u32 reg;
+ 
+ 	reg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));
+ 	reg &= ~VPGEN_VFRTRIG_VFSWR_M;
+ 	wr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);
+ 	ice_flush(hw);
+ }
+ 
+ /**
+  * ice_sriov_vsi_rebuild - release and rebuild VF's VSI
+  * @vf: VF to release and setup the VSI for
+  *
+  * This is only called when a single VF is being reset (i.e. VFR, VFLR, host VF
+  * configuration change, etc.).
+  */
+ static int ice_sriov_vsi_rebuild(struct ice_vf *vf)
+ {
+ 	struct ice_pf *pf = vf->pf;
+ 
+ 	ice_vf_vsi_release(vf);
+ 	if (!ice_vf_vsi_setup(vf)) {
+ 		dev_err(ice_pf_to_dev(pf),
+ 			"Failed to release and setup the VF%u's VSI\n",
+ 			vf->vf_id);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_post_vsi_rebuild - tasks to do after the VF's VSI have been rebuilt
+  * @vf: VF to perform tasks on
+  */
+ static void ice_sriov_post_vsi_rebuild(struct ice_vf *vf)
+ {
+ 	ice_vf_rebuild_host_cfg(vf);
+ 	ice_vf_set_initialized(vf);
+ 	ice_ena_vf_mappings(vf);
+ 	wr32(&vf->pf->hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);
+ }
+ 
+ static const struct ice_vf_ops ice_sriov_vf_ops = {
+ 	.reset_type = ICE_VF_RESET,
+ 	.free = ice_sriov_free_vf,
+ 	.clear_mbx_register = ice_sriov_clear_mbx_register,
+ 	.trigger_reset_register = ice_sriov_trigger_reset_register,
+ 	.poll_reset_status = ice_sriov_poll_reset_status,
+ 	.clear_reset_trigger = ice_sriov_clear_reset_trigger,
+ 	.vsi_rebuild = ice_sriov_vsi_rebuild,
+ 	.post_vsi_rebuild = ice_sriov_post_vsi_rebuild,
+ };
+ 
+ /**
+  * ice_create_vf_entries - Allocate and insert VF entries
+  * @pf: pointer to the PF structure
+  * @num_vfs: the number of VFs to allocate
+  *
+  * Allocate new VF entries and insert them into the hash table. Set some
+  * basic default fields for initializing the new VFs.
+  *
+  * After this function exits, the hash table will have num_vfs entries
+  * inserted.
+  *
+  * Returns 0 on success or an integer error code on failure.
+  */
+ static int ice_create_vf_entries(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	struct ice_vfs *vfs = &pf->vfs;
+ 	struct ice_vf *vf;
+ 	u16 vf_id;
+ 	int err;
+ 
+ 	lockdep_assert_held(&vfs->table_lock);
+ 
+ 	for (vf_id = 0; vf_id < num_vfs; vf_id++) {
+ 		vf = kzalloc(sizeof(*vf), GFP_KERNEL);
+ 		if (!vf) {
+ 			err = -ENOMEM;
+ 			goto err_free_entries;
+ 		}
+ 		kref_init(&vf->refcnt);
+ 
+ 		vf->pf = pf;
+ 		vf->vf_id = vf_id;
+ 
+ 		/* set sriov vf ops for VFs created during SRIOV flow */
+ 		vf->vf_ops = &ice_sriov_vf_ops;
+ 
+ 		vf->vf_sw_id = pf->first_sw;
+ 		/* assign default capabilities */
+ 		vf->spoofchk = true;
+ 		vf->num_vf_qs = pf->vfs.num_qps_per;
+ 		ice_vc_set_default_allowlist(vf);
+ 
+ 		/* ctrl_vsi_idx will be set to a valid value only when VF
+ 		 * creates its first fdir rule.
+ 		 */
+ 		ice_vf_ctrl_invalidate_vsi(vf);
+ 		ice_vf_fdir_init(vf);
+ 
+ 		ice_virtchnl_set_dflt_ops(vf);
+ 
+ 		mutex_init(&vf->cfg_lock);
+ 
+ 		hash_add_rcu(vfs->table, &vf->entry, vf_id);
+ 	}
+ 
+ 	return 0;
+ 
+ err_free_entries:
+ 	ice_free_vf_entries(pf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_ena_vfs - enable VFs so they are ready to be used
+  * @pf: pointer to the PF structure
+  * @num_vfs: number of VFs to enable
+  */
+ static int ice_ena_vfs(struct ice_pf *pf, u16 num_vfs)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_hw *hw = &pf->hw;
+ 	int ret;
+ 
+ 	/* Disable global interrupt 0 so we don't try to handle the VFLR. */
+ 	wr32(hw, GLINT_DYN_CTL(pf->oicr_idx),
+ 	     ICE_ITR_NONE << GLINT_DYN_CTL_ITR_INDX_S);
+ 	set_bit(ICE_OICR_INTR_DIS, pf->state);
+ 	ice_flush(hw);
+ 
+ 	ret = pci_enable_sriov(pf->pdev, num_vfs);
+ 	if (ret)
+ 		goto err_unroll_intr;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 
+ 	ret = ice_set_per_vf_res(pf, num_vfs);
+ 	if (ret) {
+ 		dev_err(dev, "Not enough resources for %d VFs, err %d. Try with fewer number of VFs\n",
+ 			num_vfs, ret);
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	ret = ice_create_vf_entries(pf, num_vfs);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to allocate VF entries for %d VFs\n",
+ 			num_vfs);
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	ret = ice_start_vfs(pf);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to start %d VFs, err %d\n", num_vfs, ret);
+ 		ret = -EAGAIN;
+ 		goto err_unroll_vf_entries;
+ 	}
+ 
+ 	clear_bit(ICE_VF_DIS, pf->state);
+ 
+ 	ret = ice_eswitch_configure(pf);
+ 	if (ret) {
+ 		dev_err(dev, "Failed to configure eswitch, err %d\n", ret);
+ 		goto err_unroll_sriov;
+ 	}
+ 
+ 	/* rearm global interrupts */
+ 	if (test_and_clear_bit(ICE_OICR_INTR_DIS, pf->state))
+ 		ice_irq_dynamic_ena(hw, NULL, NULL);
+ 
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 
+ 	return 0;
+ 
+ err_unroll_vf_entries:
+ 	ice_free_vf_entries(pf);
+ err_unroll_sriov:
+ 	mutex_unlock(&pf->vfs.table_lock);
+ 	pci_disable_sriov(pf->pdev);
+ err_unroll_intr:
+ 	/* rearm interrupts here */
+ 	ice_irq_dynamic_ena(hw, NULL, NULL);
+ 	clear_bit(ICE_OICR_INTR_DIS, pf->state);
+ 	return ret;
+ }
+ 
+ /**
+  * ice_pci_sriov_ena - Enable or change number of VFs
+  * @pf: pointer to the PF structure
+  * @num_vfs: number of VFs to allocate
+  *
+  * Returns 0 on success and negative on failure
+  */
+ static int ice_pci_sriov_ena(struct ice_pf *pf, int num_vfs)
+ {
+ 	int pre_existing_vfs = pci_num_vf(pf->pdev);
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	if (pre_existing_vfs && pre_existing_vfs != num_vfs)
+ 		ice_free_vfs(pf);
+ 	else if (pre_existing_vfs && pre_existing_vfs == num_vfs)
+ 		return 0;
+ 
+ 	if (num_vfs > pf->vfs.num_supported) {
+ 		dev_err(dev, "Can't enable %d VFs, max VFs supported is %d\n",
+ 			num_vfs, pf->vfs.num_supported);
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	dev_info(dev, "Enabling %d VFs\n", num_vfs);
+ 	err = ice_ena_vfs(pf, num_vfs);
+ 	if (err) {
+ 		dev_err(dev, "Failed to enable SR-IOV: %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	set_bit(ICE_FLAG_SRIOV_ENA, pf->flags);
+ 	return 0;
+ }
+ 
+ /**
+  * ice_check_sriov_allowed - check if SR-IOV is allowed based on various checks
+  * @pf: PF to enabled SR-IOV on
+  */
+ static int ice_check_sriov_allowed(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	if (!test_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags)) {
+ 		dev_err(dev, "This device is not capable of SR-IOV\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (ice_is_safe_mode(pf)) {
+ 		dev_err(dev, "SR-IOV cannot be configured - Device is in Safe Mode\n");
+ 		return -EOPNOTSUPP;
+ 	}
+ 
+ 	if (!ice_pf_state_is_nominal(pf)) {
+ 		dev_err(dev, "Cannot enable SR-IOV, device not ready\n");
+ 		return -EBUSY;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_sriov_configure - Enable or change number of VFs via sysfs
+  * @pdev: pointer to a pci_dev structure
+  * @num_vfs: number of VFs to allocate or 0 to free VFs
+  *
+  * This function is called when the user updates the number of VFs in sysfs. On
+  * success return whatever num_vfs was set to by the caller. Return negative on
+  * failure.
+  */
+ int ice_sriov_configure(struct pci_dev *pdev, int num_vfs)
+ {
+ 	struct ice_pf *pf = pci_get_drvdata(pdev);
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	err = ice_check_sriov_allowed(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	if (!num_vfs) {
+ 		if (!pci_vfs_assigned(pdev)) {
+ 			ice_mbx_deinit_snapshot(&pf->hw);
+ 			ice_free_vfs(pf);
+ 			if (pf->lag)
+ 				ice_enable_lag(pf->lag);
+ 			return 0;
+ 		}
+ 
+ 		dev_err(dev, "can't free VFs because some are assigned to VMs.\n");
+ 		return -EBUSY;
+ 	}
+ 
+ 	err = ice_mbx_init_snapshot(&pf->hw, num_vfs);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ice_pci_sriov_ena(pf, num_vfs);
+ 	if (err) {
+ 		ice_mbx_deinit_snapshot(&pf->hw);
+ 		return err;
+ 	}
+ 
+ 	if (pf->lag)
+ 		ice_disable_lag(pf->lag);
+ 	return num_vfs;
+ }
+ 
+ /**
+  * ice_process_vflr_event - Free VF resources via IRQ calls
+  * @pf: pointer to the PF structure
+  *
+  * called from the VFLR IRQ handler to
+  * free up VF resources and state variables
+  */
+ void ice_process_vflr_event(struct ice_pf *pf)
+ {
+ 	struct ice_hw *hw = &pf->hw;
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 	u32 reg;
+ 
+ 	if (!test_and_clear_bit(ICE_VFLR_EVENT_PENDING, pf->state) ||
+ 	    !ice_has_vfs(pf))
+ 		return;
+ 
+ 	mutex_lock(&pf->vfs.table_lock);
+ 	ice_for_each_vf(pf, bkt, vf) {
+ 		u32 reg_idx, bit_idx;
+ 
+ 		reg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;
+ 		bit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;
+ 		/* read GLGEN_VFLRSTAT register to find out the flr VFs */
+ 		reg = rd32(hw, GLGEN_VFLRSTAT(reg_idx));
+ 		if (reg & BIT(bit_idx)) {
+ 			/* GLGEN_VFLRSTAT bit will be cleared in ice_reset_vf */
+ 			mutex_lock(&vf->cfg_lock);
+ 			ice_reset_vf(vf, true);
+ 			mutex_unlock(&vf->cfg_lock);
+ 		}
+ 	}
+ 	mutex_unlock(&pf->vfs.table_lock);
+ }
+ 
+ /**
+  * ice_vc_reset_vf - Perform software reset on the VF after informing the AVF
+  * @vf: pointer to the VF info
+  */
+ static void ice_vc_reset_vf(struct ice_vf *vf)
+ {
+ 	ice_vc_notify_vf_reset(vf);
+ 	ice_reset_vf(vf, false);
+ }
+ 
+ /**
+  * ice_get_vf_from_pfq - get the VF who owns the PF space queue passed in
+  * @pf: PF used to index all VFs
+  * @pfq: queue index relative to the PF's function space
+  *
+  * If no VF is found who owns the pfq then return NULL, otherwise return a
+  * pointer to the VF who owns the pfq
+  *
+  * If this function returns non-NULL, it acquires a reference count of the VF
+  * structure. The caller is responsible for calling ice_put_vf() to drop this
+  * reference.
+  */
+ static struct ice_vf *ice_get_vf_from_pfq(struct ice_pf *pf, u16 pfq)
+ {
+ 	struct ice_vf *vf;
+ 	unsigned int bkt;
+ 
+ 	rcu_read_lock();
+ 	ice_for_each_vf_rcu(pf, bkt, vf) {
+ 		struct ice_vsi *vsi;
+ 		u16 rxq_idx;
+ 
+ 		vsi = ice_get_vf_vsi(vf);
+ 
+ 		ice_for_each_rxq(vsi, rxq_idx)
+ 			if (vsi->rxq_map[rxq_idx] == pfq) {
+ 				struct ice_vf *found;
+ 
+ 				if (kref_get_unless_zero(&vf->refcnt))
+ 					found = vf;
+ 				else
+ 					found = NULL;
+ 				rcu_read_unlock();
+ 				return found;
+ 			}
+ 	}
+ 	rcu_read_unlock();
+ 
+ 	return NULL;
+ }
+ 
+ /**
+  * ice_globalq_to_pfq - convert from global queue index to PF space queue index
+  * @pf: PF used for conversion
+  * @globalq: global queue index used to convert to PF space queue index
+  */
+ static u32 ice_globalq_to_pfq(struct ice_pf *pf, u32 globalq)
+ {
+ 	return globalq - pf->hw.func_caps.common_cap.rxq_first_id;
+ }
+ 
+ /**
+  * ice_vf_lan_overflow_event - handle LAN overflow event for a VF
+  * @pf: PF that the LAN overflow event happened on
+  * @event: structure holding the event information for the LAN overflow event
+  *
+  * Determine if the LAN overflow event was caused by a VF queue. If it was not
+  * caused by a VF, do nothing. If a VF caused this LAN overflow event trigger a
+  * reset on the offending VF.
+  */
+ void
+ ice_vf_lan_overflow_event(struct ice_pf *pf, struct ice_rq_event_info *event)
+ {
+ 	u32 gldcb_rtctq, queue;
+ 	struct ice_vf *vf;
+ 
+ 	gldcb_rtctq = le32_to_cpu(event->desc.params.lan_overflow.prtdcb_ruptq);
+ 	dev_dbg(ice_pf_to_dev(pf), "GLDCB_RTCTQ: 0x%08x\n", gldcb_rtctq);
+ 
+ 	/* event returns device global Rx queue number */
+ 	queue = (gldcb_rtctq & GLDCB_RTCTQ_RXQNUM_M) >>
+ 		GLDCB_RTCTQ_RXQNUM_S;
+ 
+ 	vf = ice_get_vf_from_pfq(pf, ice_globalq_to_pfq(pf, queue));
+ 	if (!vf)
+ 		return;
+ 
+ 	mutex_lock(&vf->cfg_lock);
+ 	ice_vc_reset_vf(vf);
+ 	mutex_unlock(&vf->cfg_lock);
+ 
+ 	ice_put_vf(vf);
+ }
+ 
+ /**
+  * ice_vc_send_msg_to_vf - Send message to VF
+  * @vf: pointer to the VF info
+  * @v_opcode: virtual channel opcode
+  * @v_retval: virtual channel return value
+  * @msg: pointer to the msg buffer
+  * @msglen: msg length
+  *
+  * send msg to VF
++>>>>>>> 9c6f787897f6 (ice: introduce VF operations structure for reset flows)
   */
  int
 -ice_vc_send_msg_to_vf(struct ice_vf *vf, u32 v_opcode,
 -		      enum virtchnl_status_code v_retval, u8 *msg, u16 msglen)
 +ice_aq_send_msg_to_vf(struct ice_hw *hw, u16 vfid, u32 v_opcode, u32 v_retval,
 +		      u8 *msg, u16 msglen, struct ice_sq_cd *cd)
  {
 -	struct device *dev;
 -	struct ice_pf *pf;
 -	int aq_ret;
 +	struct ice_aqc_pf_vf_msg *cmd;
 +	struct ice_aq_desc desc;
  
 -	pf = vf->pf;
 -	dev = ice_pf_to_dev(pf);
 +	ice_fill_dflt_direct_cmd_desc(&desc, ice_mbx_opc_send_msg_to_vf);
  
 -	aq_ret = ice_aq_send_msg_to_vf(&pf->hw, vf->vf_id, v_opcode, v_retval,
 -				       msg, msglen, NULL);
 -	if (aq_ret && pf->hw.mailboxq.sq_last_status != ICE_AQ_RC_ENOSYS) {
 -		dev_info(dev, "Unable to send the message to VF %d ret %d aq_err %s\n",
 -			 vf->vf_id, aq_ret,
 -			 ice_aq_str(pf->hw.mailboxq.sq_last_status));
 -		return -EIO;
 -	}
 +	cmd = &desc.params.virt;
 +	cmd->id = cpu_to_le32(vfid);
  
 -	return 0;
 +	desc.cookie_high = cpu_to_le32(v_opcode);
 +	desc.cookie_low = cpu_to_le32(v_retval);
 +
 +	if (msglen)
 +		desc.flags |= cpu_to_le16(ICE_AQ_FLAG_RD);
 +
 +	return ice_sq_send_cmd(hw, &hw->mailboxq, &desc, msg, msglen, cd);
  }
  
  /**
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.h
* Unmerged path drivers/net/ethernet/intel/ice/ice_sriov.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_vf_lib.h
