arm64: entry: avoid kprobe recursion

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit 024f4b2e1f874934943eb2d3d288ebc52c79f55c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/024f4b2e.failed

The cortex_a76_erratum_1463225_debug_handler() function is called when
handling debug exceptions (and synchronous exceptions from BRK
instructions), and so is called when a probed function executes. If the
compiler does not inline cortex_a76_erratum_1463225_debug_handler(), it
can be probed.

If cortex_a76_erratum_1463225_debug_handler() is probed, any debug
exception or software breakpoint exception will result in recursive
exceptions leading to a stack overflow. This can be triggered with the
ftrace multiple_probes selftest, and as per the example splat below.

This is a regression caused by commit:

  6459b8469753e9fe ("arm64: entry: consolidate Cortex-A76 erratum 1463225 workaround")

... which removed the NOKPROBE_SYMBOL() annotation associated with the
function.

My intent was that cortex_a76_erratum_1463225_debug_handler() would be
inlined into its caller, el1_dbg(), which is marked noinstr and cannot
be probed. Mark cortex_a76_erratum_1463225_debug_handler() as
__always_inline to ensure this.

Example splat prior to this patch (with recursive entries elided):

| # echo p cortex_a76_erratum_1463225_debug_handler > /sys/kernel/debug/tracing/kprobe_events
| # echo p do_el0_svc >> /sys/kernel/debug/tracing/kprobe_events
| # echo 1 > /sys/kernel/debug/tracing/events/kprobes/enable
| Insufficient stack space to handle exception!
| ESR: 0x0000000096000047 -- DABT (current EL)
| FAR: 0xffff800009cefff0
| Task stack:     [0xffff800009cf0000..0xffff800009cf4000]
| IRQ stack:      [0xffff800008000000..0xffff800008004000]
| Overflow stack: [0xffff00007fbc00f0..0xffff00007fbc10f0]
| CPU: 0 PID: 145 Comm: sh Not tainted 6.0.0 #2
| Hardware name: linux,dummy-virt (DT)
| pstate: 604003c5 (nZCv DAIF +PAN -UAO -TCO -DIT -SSBS BTYPE=--)
| pc : arm64_enter_el1_dbg+0x4/0x20
| lr : el1_dbg+0x24/0x5c
| sp : ffff800009cf0000
| x29: ffff800009cf0000 x28: ffff000002c74740 x27: 0000000000000000
| x26: 0000000000000000 x25: 0000000000000000 x24: 0000000000000000
| x23: 00000000604003c5 x22: ffff80000801745c x21: 0000aaaac95ac068
| x20: 00000000f2000004 x19: ffff800009cf0040 x18: 0000000000000000
| x17: 0000000000000000 x16: 0000000000000000 x15: 0000000000000000
| x14: 0000000000000000 x13: 0000000000000000 x12: 0000000000000000
| x11: 0000000000000010 x10: ffff800008c87190 x9 : ffff800008ca00d0
| x8 : 000000000000003c x7 : 0000000000000000 x6 : 0000000000000000
| x5 : 0000000000000000 x4 : 0000000000000000 x3 : 00000000000043a4
| x2 : 00000000f2000004 x1 : 00000000f2000004 x0 : ffff800009cf0040
| Kernel panic - not syncing: kernel stack overflow
| CPU: 0 PID: 145 Comm: sh Not tainted 6.0.0 #2
| Hardware name: linux,dummy-virt (DT)
| Call trace:
|  dump_backtrace+0xe4/0x104
|  show_stack+0x18/0x4c
|  dump_stack_lvl+0x64/0x7c
|  dump_stack+0x18/0x38
|  panic+0x14c/0x338
|  test_taint+0x0/0x2c
|  panic_bad_stack+0x104/0x118
|  handle_bad_stack+0x34/0x48
|  __bad_stack+0x78/0x7c
|  arm64_enter_el1_dbg+0x4/0x20
|  el1h_64_sync_handler+0x40/0x98
|  el1h_64_sync+0x64/0x68
|  cortex_a76_erratum_1463225_debug_handler+0x0/0x34
...
|  el1h_64_sync_handler+0x40/0x98
|  el1h_64_sync+0x64/0x68
|  cortex_a76_erratum_1463225_debug_handler+0x0/0x34
...
|  el1h_64_sync_handler+0x40/0x98
|  el1h_64_sync+0x64/0x68
|  cortex_a76_erratum_1463225_debug_handler+0x0/0x34
|  el1h_64_sync_handler+0x40/0x98
|  el1h_64_sync+0x64/0x68
|  do_el0_svc+0x0/0x28
|  el0t_64_sync_handler+0x84/0xf0
|  el0t_64_sync+0x18c/0x190
| Kernel Offset: disabled
| CPU features: 0x0080,00005021,19001080
| Memory Limit: none
| ---[ end Kernel panic - not syncing: kernel stack overflow ]---

With this patch, cortex_a76_erratum_1463225_debug_handler() is inlined
into el1_dbg(), and el1_dbg() cannot be probed:

| # echo p cortex_a76_erratum_1463225_debug_handler > /sys/kernel/debug/tracing/kprobe_events
| sh: write error: No such file or directory
| # grep -w cortex_a76_erratum_1463225_debug_handler /proc/kallsyms | wc -l
| 0
| # echo p el1_dbg > /sys/kernel/debug/tracing/kprobe_events
| sh: write error: Invalid argument
| # grep -w el1_dbg /proc/kallsyms | wc -l
| 1

Fixes: 6459b8469753 ("arm64: entry: consolidate Cortex-A76 erratum 1463225 workaround")
	Cc: <stable@vger.kernel.org> # 5.12.x
	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lore.kernel.org/r/20221017090157.2881408-1-mark.rutland@arm.com
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit 024f4b2e1f874934943eb2d3d288ebc52c79f55c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry-common.c
diff --cc arch/arm64/kernel/entry-common.c
index c764ba2d11d7,27369fa1c032..000000000000
--- a/arch/arm64/kernel/entry-common.c
+++ b/arch/arm64/kernel/entry-common.c
@@@ -59,26 -84,280 +59,91 @@@ static void noinstr exit_to_kernel_mode
  		trace_hardirqs_on();
  	} else {
  		if (regs->exit_rcu)
 -			ct_irq_exit();
 -	}
 -}
 -
 -static void noinstr exit_to_kernel_mode(struct pt_regs *regs)
 -{
 -	mte_check_tfsr_exit();
 -	__exit_to_kernel_mode(regs);
 -}
 -
 -/*
 - * Handle IRQ/context state management when entering from user mode.
 - * Before this function is called it is not safe to call regular kernel code,
 - * intrumentable code, or any code which may trigger an exception.
 - */
 -static __always_inline void __enter_from_user_mode(void)
 -{
 -	lockdep_hardirqs_off(CALLER_ADDR0);
 -	CT_WARN_ON(ct_state() != CONTEXT_USER);
 -	user_exit_irqoff();
 -	trace_hardirqs_off_finish();
 -	mte_disable_tco_entry(current);
 -}
 -
 -static __always_inline void enter_from_user_mode(struct pt_regs *regs)
 -{
 -	__enter_from_user_mode();
 -}
 -
 -/*
 - * Handle IRQ/context state management when exiting to user mode.
 - * After this function returns it is not safe to call regular kernel code,
 - * intrumentable code, or any code which may trigger an exception.
 - */
 -static __always_inline void __exit_to_user_mode(void)
 -{
 -	trace_hardirqs_on_prepare();
 -	lockdep_hardirqs_on_prepare();
 -	user_enter_irqoff();
 -	lockdep_hardirqs_on(CALLER_ADDR0);
 -}
 -
 -static __always_inline void prepare_exit_to_user_mode(struct pt_regs *regs)
 -{
 -	unsigned long flags;
 -
 -	local_daif_mask();
 -
 -	flags = read_thread_flags();
 -	if (unlikely(flags & _TIF_WORK_MASK))
 -		do_notify_resume(regs, flags);
 -}
 -
 -static __always_inline void exit_to_user_mode(struct pt_regs *regs)
 -{
 -	prepare_exit_to_user_mode(regs);
 -	mte_check_tfsr_exit();
 -	__exit_to_user_mode();
 -}
 -
 -asmlinkage void noinstr asm_exit_to_user_mode(struct pt_regs *regs)
 -{
 -	exit_to_user_mode(regs);
 -}
 -
 -/*
 - * Handle IRQ/context state management when entering an NMI from user/kernel
 - * mode. Before this function is called it is not safe to call regular kernel
 - * code, intrumentable code, or any code which may trigger an exception.
 - */
 -static void noinstr arm64_enter_nmi(struct pt_regs *regs)
 -{
 -	regs->lockdep_hardirqs = lockdep_hardirqs_enabled();
 -
 -	__nmi_enter();
 -	lockdep_hardirqs_off(CALLER_ADDR0);
 -	lockdep_hardirq_enter();
 -	ct_nmi_enter();
 -
 -	trace_hardirqs_off_finish();
 -	ftrace_nmi_enter();
 -}
 -
 -/*
 - * Handle IRQ/context state management when exiting an NMI from user/kernel
 - * mode. After this function returns it is not safe to call regular kernel
 - * code, intrumentable code, or any code which may trigger an exception.
 - */
 -static void noinstr arm64_exit_nmi(struct pt_regs *regs)
 -{
 -	bool restore = regs->lockdep_hardirqs;
 -
 -	ftrace_nmi_exit();
 -	if (restore) {
 -		trace_hardirqs_on_prepare();
 -		lockdep_hardirqs_on_prepare();
 -	}
 -
 -	ct_nmi_exit();
 -	lockdep_hardirq_exit();
 -	if (restore)
 -		lockdep_hardirqs_on(CALLER_ADDR0);
 -	__nmi_exit();
 -}
 -
 -/*
 - * Handle IRQ/context state management when entering a debug exception from
 - * kernel mode. Before this function is called it is not safe to call regular
 - * kernel code, intrumentable code, or any code which may trigger an exception.
 - */
 -static void noinstr arm64_enter_el1_dbg(struct pt_regs *regs)
 -{
 -	regs->lockdep_hardirqs = lockdep_hardirqs_enabled();
 -
 -	lockdep_hardirqs_off(CALLER_ADDR0);
 -	ct_nmi_enter();
 -
 -	trace_hardirqs_off_finish();
 -}
 -
 -/*
 - * Handle IRQ/context state management when exiting a debug exception from
 - * kernel mode. After this function returns it is not safe to call regular
 - * kernel code, intrumentable code, or any code which may trigger an exception.
 - */
 -static void noinstr arm64_exit_el1_dbg(struct pt_regs *regs)
 -{
 -	bool restore = regs->lockdep_hardirqs;
 -
 -	if (restore) {
 -		trace_hardirqs_on_prepare();
 -		lockdep_hardirqs_on_prepare();
 +			rcu_irq_exit();
  	}
 -
 -	ct_nmi_exit();
 -	if (restore)
 -		lockdep_hardirqs_on(CALLER_ADDR0);
 -}
 -
 -#ifdef CONFIG_PREEMPT_DYNAMIC
 -DEFINE_STATIC_KEY_TRUE(sk_dynamic_irqentry_exit_cond_resched);
 -#define need_irq_preemption() \
 -	(static_branch_unlikely(&sk_dynamic_irqentry_exit_cond_resched))
 -#else
 -#define need_irq_preemption()	(IS_ENABLED(CONFIG_PREEMPTION))
 -#endif
 -
 -static void __sched arm64_preempt_schedule_irq(void)
 -{
 -	if (!need_irq_preemption())
 -		return;
 -
 -	/*
 -	 * Note: thread_info::preempt_count includes both thread_info::count
 -	 * and thread_info::need_resched, and is not equivalent to
 -	 * preempt_count().
 -	 */
 -	if (READ_ONCE(current_thread_info()->preempt_count) != 0)
 -		return;
 -
 -	/*
 -	 * DAIF.DA are cleared at the start of IRQ/FIQ handling, and when GIC
 -	 * priority masking is used the GIC irqchip driver will clear DAIF.IF
 -	 * using gic_arch_enable_irqs() for normal IRQs. If anything is set in
 -	 * DAIF we must have handled an NMI, so skip preemption.
 -	 */
 -	if (system_uses_irq_prio_masking() && read_sysreg(daif))
 -		return;
 -
 -	/*
 -	 * Preempting a task from an IRQ means we leave copies of PSTATE
 -	 * on the stack. cpufeature's enable calls may modify PSTATE, but
 -	 * resuming one of these preempted tasks would undo those changes.
 -	 *
 -	 * Only allow a task to be preempted once cpufeatures have been
 -	 * enabled.
 -	 */
 -	if (system_capabilities_finalized())
 -		preempt_schedule_irq();
  }
  
 -static void do_interrupt_handler(struct pt_regs *regs,
 -				 void (*handler)(struct pt_regs *))
 +asmlinkage void noinstr enter_el1_irq_or_nmi(struct pt_regs *regs)
  {
 -	struct pt_regs *old_regs = set_irq_regs(regs);
 -
 -	if (on_thread_stack())
 -		call_on_irq_stack(regs, handler);
 +	if (IS_ENABLED(CONFIG_ARM64_PSEUDO_NMI) && !interrupts_enabled(regs))
 +		nmi_enter();
  	else
 -		handler(regs);
 -
 -	set_irq_regs(old_regs);
 +		enter_from_kernel_mode(regs);
  }
  
 -extern void (*handle_arch_irq)(struct pt_regs *);
 -extern void (*handle_arch_fiq)(struct pt_regs *);
 -
 -static void noinstr __panic_unhandled(struct pt_regs *regs, const char *vector,
 -				      unsigned long esr)
 +asmlinkage void noinstr exit_el1_irq_or_nmi(struct pt_regs *regs)
  {
 -	arm64_enter_nmi(regs);
 -
 -	console_verbose();
 -
 -	pr_crit("Unhandled %s exception on CPU%d, ESR 0x%016lx -- %s\n",
 -		vector, smp_processor_id(), esr,
 -		esr_get_class_string(esr));
 -
 -	__show_regs(regs);
 -	panic("Unhandled exception");
 +	if (IS_ENABLED(CONFIG_ARM64_PSEUDO_NMI) && !interrupts_enabled(regs))
 +		nmi_exit();
 +	else
 +		exit_to_kernel_mode(regs);
  }
  
++<<<<<<< HEAD
++=======
+ #define UNHANDLED(el, regsize, vector)							\
+ asmlinkage void noinstr el##_##regsize##_##vector##_handler(struct pt_regs *regs)	\
+ {											\
+ 	const char *desc = #regsize "-bit " #el " " #vector;				\
+ 	__panic_unhandled(regs, desc, read_sysreg(esr_el1));				\
+ }
+ 
+ #ifdef CONFIG_ARM64_ERRATUM_1463225
+ static DEFINE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+ 
+ static void cortex_a76_erratum_1463225_svc_handler(void)
+ {
+ 	u32 reg, val;
+ 
+ 	if (!unlikely(test_thread_flag(TIF_SINGLESTEP)))
+ 		return;
+ 
+ 	if (!unlikely(this_cpu_has_cap(ARM64_WORKAROUND_1463225)))
+ 		return;
+ 
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
+ 	reg = read_sysreg(mdscr_el1);
+ 	val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
+ 	write_sysreg(val, mdscr_el1);
+ 	asm volatile("msr daifclr, #8");
+ 	isb();
+ 
+ 	/* We will have taken a single-step exception by this point */
+ 
+ 	write_sysreg(reg, mdscr_el1);
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
+ }
+ 
+ static __always_inline bool
+ cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	if (!__this_cpu_read(__in_cortex_a76_erratum_1463225_wa))
+ 		return false;
+ 
+ 	/*
+ 	 * We've taken a dummy step exception from the kernel to ensure
+ 	 * that interrupts are re-enabled on the syscall path. Return back
+ 	 * to cortex_a76_erratum_1463225_svc_handler() with debug exceptions
+ 	 * masked so that we can safely restore the mdscr and get on with
+ 	 * handling the syscall.
+ 	 */
+ 	regs->pstate |= PSR_D_BIT;
+ 	return true;
+ }
+ #else /* CONFIG_ARM64_ERRATUM_1463225 */
+ static void cortex_a76_erratum_1463225_svc_handler(void) { }
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_ARM64_ERRATUM_1463225 */
+ 
+ UNHANDLED(el1t, 64, sync)
+ UNHANDLED(el1t, 64, irq)
+ UNHANDLED(el1t, 64, fiq)
+ UNHANDLED(el1t, 64, error)
+ 
++>>>>>>> 024f4b2e1f87 (arm64: entry: avoid kprobe recursion)
  static void noinstr el1_abort(struct pt_regs *regs, unsigned long esr)
  {
  	unsigned long far = read_sysreg(far_el1);
* Unmerged path arch/arm64/kernel/entry-common.c
