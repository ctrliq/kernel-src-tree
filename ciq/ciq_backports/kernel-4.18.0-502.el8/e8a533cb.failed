treewide: use get_random_u32_inclusive() when possible

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
Rebuild_CHGLOG: - Revert "treewide: use get_random_u32_inclusive() when possible" (Jocelyn Falempe) [2160452]
Rebuild_FUZZ: 92.31%
commit-author Jason A. Donenfeld <Jason@zx2c4.com>
commit e8a533cbeb79809206f8724e89961e0079508c3c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/e8a533cb.failed

These cases were done with this Coccinelle:

@@
expression H;
expression L;
@@
- (get_random_u32_below(H) + L)
+ get_random_u32_inclusive(L, H + L - 1)

@@
expression H;
expression L;
expression E;
@@
  get_random_u32_inclusive(L,
  H
- + E
- - E
  )

@@
expression H;
expression L;
expression E;
@@
  get_random_u32_inclusive(L,
  H
- - E
- + E
  )

@@
expression H;
expression L;
expression E;
expression F;
@@
  get_random_u32_inclusive(L,
  H
- - E
  + F
- + E
  )

@@
expression H;
expression L;
expression E;
expression F;
@@
  get_random_u32_inclusive(L,
  H
- + E
  + F
- - E
  )

And then subsequently cleaned up by hand, with several automatic cases
rejected if it didn't make sense contextually.

	Reviewed-by: Kees Cook <keescook@chromium.org>
	Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com> # for infiniband
	Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
(cherry picked from commit e8a533cbeb79809206f8724e89961e0079508c3c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/module.c
#	crypto/rsa-pkcs1pad.c
#	crypto/testmgr.c
#	drivers/bus/mhi/host/internal.h
#	drivers/dma-buf/st-dma-fence-chain.c
#	drivers/infiniband/core/cma.c
#	drivers/infiniband/hw/hns/hns_roce_ah.c
#	drivers/mtd/nand/raw/nandsim.c
#	drivers/net/wireguard/selftest/allowedips.c
#	drivers/net/wireless/broadcom/brcm80211/brcmfmac/p2p.c
#	drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
#	fs/f2fs/segment.c
#	kernel/kcsan/selftest.c
#	lib/test_hexdump.c
#	lib/test_kasan.c
#	lib/test_printf.c
#	lib/test_vmalloc.c
#	mm/kfence/kfence_test.c
#	mm/swapfile.c
#	net/bluetooth/mgmt.c
#	net/core/pktgen.c
#	net/ipv4/tcp_input.c
#	net/ipv6/addrconf.c
#	net/xfrm/xfrm_state.c
diff --cc arch/x86/kernel/module.c
index e18a12d64318,a98687642dd0..000000000000
--- a/arch/x86/kernel/module.c
+++ b/arch/x86/kernel/module.c
@@@ -65,7 -53,7 +65,11 @@@ static unsigned long int get_module_loa
  		 */
  		if (module_load_offset == 0)
  			module_load_offset =
++<<<<<<< HEAD
 +				(get_random_int() % 1024 + 1) * PAGE_SIZE;
++=======
+ 				get_random_u32_inclusive(1, 1024) * PAGE_SIZE;
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		mutex_unlock(&module_kaslr_mutex);
  	}
  	return module_load_offset;
diff --cc crypto/rsa-pkcs1pad.c
index 3bd75907a2a3,e75728f87ce5..000000000000
--- a/crypto/rsa-pkcs1pad.c
+++ b/crypto/rsa-pkcs1pad.c
@@@ -255,7 -253,7 +255,11 @@@ static int pkcs1pad_encrypt(struct akci
  	ps_end = ctx->key_size - req->src_len - 2;
  	req_ctx->in_buf[0] = 0x02;
  	for (i = 1; i < ps_end; i++)
++<<<<<<< HEAD
 +		req_ctx->in_buf[i] = 1 + prandom_u32_max(255);
++=======
+ 		req_ctx->in_buf[i] = get_random_u32_inclusive(1, 255);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  	req_ctx->in_buf[ps_end] = 0x00;
  
  	pkcs1pad_sg_set_buf(req_ctx->in_sg, req_ctx->in_buf,
diff --cc crypto/testmgr.c
index 3dd3f8428f73,e669acd2ebdd..000000000000
--- a/crypto/testmgr.c
+++ b/crypto/testmgr.c
@@@ -172,10 -191,2179 +172,2183 @@@ static void testmgr_free_buf(char *buf[
  	int i;
  
  	for (i = 0; i < XBUFSIZE; i++)
 -		free_pages((unsigned long)buf[i], order);
 +		free_page((unsigned long)buf[i]);
  }
  
++<<<<<<< HEAD
 +static int ahash_guard_result(char *result, char c, int size)
++=======
+ static void testmgr_free_buf(char *buf[XBUFSIZE])
+ {
+ 	__testmgr_free_buf(buf, 0);
+ }
+ 
+ #define TESTMGR_POISON_BYTE	0xfe
+ #define TESTMGR_POISON_LEN	16
+ 
+ static inline void testmgr_poison(void *addr, size_t len)
+ {
+ 	memset(addr, TESTMGR_POISON_BYTE, len);
+ }
+ 
+ /* Is the memory region still fully poisoned? */
+ static inline bool testmgr_is_poison(const void *addr, size_t len)
+ {
+ 	return memchr_inv(addr, TESTMGR_POISON_BYTE, len) == NULL;
+ }
+ 
+ /* flush type for hash algorithms */
+ enum flush_type {
+ 	/* merge with update of previous buffer(s) */
+ 	FLUSH_TYPE_NONE = 0,
+ 
+ 	/* update with previous buffer(s) before doing this one */
+ 	FLUSH_TYPE_FLUSH,
+ 
+ 	/* likewise, but also export and re-import the intermediate state */
+ 	FLUSH_TYPE_REIMPORT,
+ };
+ 
+ /* finalization function for hash algorithms */
+ enum finalization_type {
+ 	FINALIZATION_TYPE_FINAL,	/* use final() */
+ 	FINALIZATION_TYPE_FINUP,	/* use finup() */
+ 	FINALIZATION_TYPE_DIGEST,	/* use digest() */
+ };
+ 
+ /*
+  * Whether the crypto operation will occur in-place, and if so whether the
+  * source and destination scatterlist pointers will coincide (req->src ==
+  * req->dst), or whether they'll merely point to two separate scatterlists
+  * (req->src != req->dst) that reference the same underlying memory.
+  *
+  * This is only relevant for algorithm types that support in-place operation.
+  */
+ enum inplace_mode {
+ 	OUT_OF_PLACE,
+ 	INPLACE_ONE_SGLIST,
+ 	INPLACE_TWO_SGLISTS,
+ };
+ 
+ #define TEST_SG_TOTAL	10000
+ 
+ /**
+  * struct test_sg_division - description of a scatterlist entry
+  *
+  * This struct describes one entry of a scatterlist being constructed to check a
+  * crypto test vector.
+  *
+  * @proportion_of_total: length of this chunk relative to the total length,
+  *			 given as a proportion out of TEST_SG_TOTAL so that it
+  *			 scales to fit any test vector
+  * @offset: byte offset into a 2-page buffer at which this chunk will start
+  * @offset_relative_to_alignmask: if true, add the algorithm's alignmask to the
+  *				  @offset
+  * @flush_type: for hashes, whether an update() should be done now vs.
+  *		continuing to accumulate data
+  * @nosimd: if doing the pending update(), do it with SIMD disabled?
+  */
+ struct test_sg_division {
+ 	unsigned int proportion_of_total;
+ 	unsigned int offset;
+ 	bool offset_relative_to_alignmask;
+ 	enum flush_type flush_type;
+ 	bool nosimd;
+ };
+ 
+ /**
+  * struct testvec_config - configuration for testing a crypto test vector
+  *
+  * This struct describes the data layout and other parameters with which each
+  * crypto test vector can be tested.
+  *
+  * @name: name of this config, logged for debugging purposes if a test fails
+  * @inplace_mode: whether and how to operate on the data in-place, if applicable
+  * @req_flags: extra request_flags, e.g. CRYPTO_TFM_REQ_MAY_SLEEP
+  * @src_divs: description of how to arrange the source scatterlist
+  * @dst_divs: description of how to arrange the dst scatterlist, if applicable
+  *	      for the algorithm type.  Defaults to @src_divs if unset.
+  * @iv_offset: misalignment of the IV in the range [0..MAX_ALGAPI_ALIGNMASK+1],
+  *	       where 0 is aligned to a 2*(MAX_ALGAPI_ALIGNMASK+1) byte boundary
+  * @iv_offset_relative_to_alignmask: if true, add the algorithm's alignmask to
+  *				     the @iv_offset
+  * @key_offset: misalignment of the key, where 0 is default alignment
+  * @key_offset_relative_to_alignmask: if true, add the algorithm's alignmask to
+  *				      the @key_offset
+  * @finalization_type: what finalization function to use for hashes
+  * @nosimd: execute with SIMD disabled?  Requires !CRYPTO_TFM_REQ_MAY_SLEEP.
+  */
+ struct testvec_config {
+ 	const char *name;
+ 	enum inplace_mode inplace_mode;
+ 	u32 req_flags;
+ 	struct test_sg_division src_divs[XBUFSIZE];
+ 	struct test_sg_division dst_divs[XBUFSIZE];
+ 	unsigned int iv_offset;
+ 	unsigned int key_offset;
+ 	bool iv_offset_relative_to_alignmask;
+ 	bool key_offset_relative_to_alignmask;
+ 	enum finalization_type finalization_type;
+ 	bool nosimd;
+ };
+ 
+ #define TESTVEC_CONFIG_NAMELEN	192
+ 
+ /*
+  * The following are the lists of testvec_configs to test for each algorithm
+  * type when the basic crypto self-tests are enabled, i.e. when
+  * CONFIG_CRYPTO_MANAGER_DISABLE_TESTS is unset.  They aim to provide good test
+  * coverage, while keeping the test time much shorter than the full fuzz tests
+  * so that the basic tests can be enabled in a wider range of circumstances.
+  */
+ 
+ /* Configs for skciphers and aeads */
+ static const struct testvec_config default_cipher_testvec_configs[] = {
+ 	{
+ 		.name = "in-place (one sglist)",
+ 		.inplace_mode = INPLACE_ONE_SGLIST,
+ 		.src_divs = { { .proportion_of_total = 10000 } },
+ 	}, {
+ 		.name = "in-place (two sglists)",
+ 		.inplace_mode = INPLACE_TWO_SGLISTS,
+ 		.src_divs = { { .proportion_of_total = 10000 } },
+ 	}, {
+ 		.name = "out-of-place",
+ 		.inplace_mode = OUT_OF_PLACE,
+ 		.src_divs = { { .proportion_of_total = 10000 } },
+ 	}, {
+ 		.name = "unaligned buffer, offset=1",
+ 		.src_divs = { { .proportion_of_total = 10000, .offset = 1 } },
+ 		.iv_offset = 1,
+ 		.key_offset = 1,
+ 	}, {
+ 		.name = "buffer aligned only to alignmask",
+ 		.src_divs = {
+ 			{
+ 				.proportion_of_total = 10000,
+ 				.offset = 1,
+ 				.offset_relative_to_alignmask = true,
+ 			},
+ 		},
+ 		.iv_offset = 1,
+ 		.iv_offset_relative_to_alignmask = true,
+ 		.key_offset = 1,
+ 		.key_offset_relative_to_alignmask = true,
+ 	}, {
+ 		.name = "two even aligned splits",
+ 		.src_divs = {
+ 			{ .proportion_of_total = 5000 },
+ 			{ .proportion_of_total = 5000 },
+ 		},
+ 	}, {
+ 		.name = "uneven misaligned splits, may sleep",
+ 		.req_flags = CRYPTO_TFM_REQ_MAY_SLEEP,
+ 		.src_divs = {
+ 			{ .proportion_of_total = 1900, .offset = 33 },
+ 			{ .proportion_of_total = 3300, .offset = 7  },
+ 			{ .proportion_of_total = 4800, .offset = 18 },
+ 		},
+ 		.iv_offset = 3,
+ 		.key_offset = 3,
+ 	}, {
+ 		.name = "misaligned splits crossing pages, inplace",
+ 		.inplace_mode = INPLACE_ONE_SGLIST,
+ 		.src_divs = {
+ 			{
+ 				.proportion_of_total = 7500,
+ 				.offset = PAGE_SIZE - 32
+ 			}, {
+ 				.proportion_of_total = 2500,
+ 				.offset = PAGE_SIZE - 7
+ 			},
+ 		},
+ 	}
+ };
+ 
+ static const struct testvec_config default_hash_testvec_configs[] = {
+ 	{
+ 		.name = "init+update+final aligned buffer",
+ 		.src_divs = { { .proportion_of_total = 10000 } },
+ 		.finalization_type = FINALIZATION_TYPE_FINAL,
+ 	}, {
+ 		.name = "init+finup aligned buffer",
+ 		.src_divs = { { .proportion_of_total = 10000 } },
+ 		.finalization_type = FINALIZATION_TYPE_FINUP,
+ 	}, {
+ 		.name = "digest aligned buffer",
+ 		.src_divs = { { .proportion_of_total = 10000 } },
+ 		.finalization_type = FINALIZATION_TYPE_DIGEST,
+ 	}, {
+ 		.name = "init+update+final misaligned buffer",
+ 		.src_divs = { { .proportion_of_total = 10000, .offset = 1 } },
+ 		.finalization_type = FINALIZATION_TYPE_FINAL,
+ 		.key_offset = 1,
+ 	}, {
+ 		.name = "digest buffer aligned only to alignmask",
+ 		.src_divs = {
+ 			{
+ 				.proportion_of_total = 10000,
+ 				.offset = 1,
+ 				.offset_relative_to_alignmask = true,
+ 			},
+ 		},
+ 		.finalization_type = FINALIZATION_TYPE_DIGEST,
+ 		.key_offset = 1,
+ 		.key_offset_relative_to_alignmask = true,
+ 	}, {
+ 		.name = "init+update+update+final two even splits",
+ 		.src_divs = {
+ 			{ .proportion_of_total = 5000 },
+ 			{
+ 				.proportion_of_total = 5000,
+ 				.flush_type = FLUSH_TYPE_FLUSH,
+ 			},
+ 		},
+ 		.finalization_type = FINALIZATION_TYPE_FINAL,
+ 	}, {
+ 		.name = "digest uneven misaligned splits, may sleep",
+ 		.req_flags = CRYPTO_TFM_REQ_MAY_SLEEP,
+ 		.src_divs = {
+ 			{ .proportion_of_total = 1900, .offset = 33 },
+ 			{ .proportion_of_total = 3300, .offset = 7  },
+ 			{ .proportion_of_total = 4800, .offset = 18 },
+ 		},
+ 		.finalization_type = FINALIZATION_TYPE_DIGEST,
+ 	}, {
+ 		.name = "digest misaligned splits crossing pages",
+ 		.src_divs = {
+ 			{
+ 				.proportion_of_total = 7500,
+ 				.offset = PAGE_SIZE - 32,
+ 			}, {
+ 				.proportion_of_total = 2500,
+ 				.offset = PAGE_SIZE - 7,
+ 			},
+ 		},
+ 		.finalization_type = FINALIZATION_TYPE_DIGEST,
+ 	}, {
+ 		.name = "import/export",
+ 		.src_divs = {
+ 			{
+ 				.proportion_of_total = 6500,
+ 				.flush_type = FLUSH_TYPE_REIMPORT,
+ 			}, {
+ 				.proportion_of_total = 3500,
+ 				.flush_type = FLUSH_TYPE_REIMPORT,
+ 			},
+ 		},
+ 		.finalization_type = FINALIZATION_TYPE_FINAL,
+ 	}
+ };
+ 
+ static unsigned int count_test_sg_divisions(const struct test_sg_division *divs)
+ {
+ 	unsigned int remaining = TEST_SG_TOTAL;
+ 	unsigned int ndivs = 0;
+ 
+ 	do {
+ 		remaining -= divs[ndivs++].proportion_of_total;
+ 	} while (remaining);
+ 
+ 	return ndivs;
+ }
+ 
+ #define SGDIVS_HAVE_FLUSHES	BIT(0)
+ #define SGDIVS_HAVE_NOSIMD	BIT(1)
+ 
+ static bool valid_sg_divisions(const struct test_sg_division *divs,
+ 			       unsigned int count, int *flags_ret)
+ {
+ 	unsigned int total = 0;
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < count && total != TEST_SG_TOTAL; i++) {
+ 		if (divs[i].proportion_of_total <= 0 ||
+ 		    divs[i].proportion_of_total > TEST_SG_TOTAL - total)
+ 			return false;
+ 		total += divs[i].proportion_of_total;
+ 		if (divs[i].flush_type != FLUSH_TYPE_NONE)
+ 			*flags_ret |= SGDIVS_HAVE_FLUSHES;
+ 		if (divs[i].nosimd)
+ 			*flags_ret |= SGDIVS_HAVE_NOSIMD;
+ 	}
+ 	return total == TEST_SG_TOTAL &&
+ 		memchr_inv(&divs[i], 0, (count - i) * sizeof(divs[0])) == NULL;
+ }
+ 
+ /*
+  * Check whether the given testvec_config is valid.  This isn't strictly needed
+  * since every testvec_config should be valid, but check anyway so that people
+  * don't unknowingly add broken configs that don't do what they wanted.
+  */
+ static bool valid_testvec_config(const struct testvec_config *cfg)
+ {
+ 	int flags = 0;
+ 
+ 	if (cfg->name == NULL)
+ 		return false;
+ 
+ 	if (!valid_sg_divisions(cfg->src_divs, ARRAY_SIZE(cfg->src_divs),
+ 				&flags))
+ 		return false;
+ 
+ 	if (cfg->dst_divs[0].proportion_of_total) {
+ 		if (!valid_sg_divisions(cfg->dst_divs,
+ 					ARRAY_SIZE(cfg->dst_divs), &flags))
+ 			return false;
+ 	} else {
+ 		if (memchr_inv(cfg->dst_divs, 0, sizeof(cfg->dst_divs)))
+ 			return false;
+ 		/* defaults to dst_divs=src_divs */
+ 	}
+ 
+ 	if (cfg->iv_offset +
+ 	    (cfg->iv_offset_relative_to_alignmask ? MAX_ALGAPI_ALIGNMASK : 0) >
+ 	    MAX_ALGAPI_ALIGNMASK + 1)
+ 		return false;
+ 
+ 	if ((flags & (SGDIVS_HAVE_FLUSHES | SGDIVS_HAVE_NOSIMD)) &&
+ 	    cfg->finalization_type == FINALIZATION_TYPE_DIGEST)
+ 		return false;
+ 
+ 	if ((cfg->nosimd || (flags & SGDIVS_HAVE_NOSIMD)) &&
+ 	    (cfg->req_flags & CRYPTO_TFM_REQ_MAY_SLEEP))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ struct test_sglist {
+ 	char *bufs[XBUFSIZE];
+ 	struct scatterlist sgl[XBUFSIZE];
+ 	struct scatterlist sgl_saved[XBUFSIZE];
+ 	struct scatterlist *sgl_ptr;
+ 	unsigned int nents;
+ };
+ 
+ static int init_test_sglist(struct test_sglist *tsgl)
+ {
+ 	return __testmgr_alloc_buf(tsgl->bufs, 1 /* two pages per buffer */);
+ }
+ 
+ static void destroy_test_sglist(struct test_sglist *tsgl)
+ {
+ 	return __testmgr_free_buf(tsgl->bufs, 1 /* two pages per buffer */);
+ }
+ 
+ /**
+  * build_test_sglist() - build a scatterlist for a crypto test
+  *
+  * @tsgl: the scatterlist to build.  @tsgl->bufs[] contains an array of 2-page
+  *	  buffers which the scatterlist @tsgl->sgl[] will be made to point into.
+  * @divs: the layout specification on which the scatterlist will be based
+  * @alignmask: the algorithm's alignmask
+  * @total_len: the total length of the scatterlist to build in bytes
+  * @data: if non-NULL, the buffers will be filled with this data until it ends.
+  *	  Otherwise the buffers will be poisoned.  In both cases, some bytes
+  *	  past the end of each buffer will be poisoned to help detect overruns.
+  * @out_divs: if non-NULL, the test_sg_division to which each scatterlist entry
+  *	      corresponds will be returned here.  This will match @divs except
+  *	      that divisions resolving to a length of 0 are omitted as they are
+  *	      not included in the scatterlist.
+  *
+  * Return: 0 or a -errno value
+  */
+ static int build_test_sglist(struct test_sglist *tsgl,
+ 			     const struct test_sg_division *divs,
+ 			     const unsigned int alignmask,
+ 			     const unsigned int total_len,
+ 			     struct iov_iter *data,
+ 			     const struct test_sg_division *out_divs[XBUFSIZE])
+ {
+ 	struct {
+ 		const struct test_sg_division *div;
+ 		size_t length;
+ 	} partitions[XBUFSIZE];
+ 	const unsigned int ndivs = count_test_sg_divisions(divs);
+ 	unsigned int len_remaining = total_len;
+ 	unsigned int i;
+ 
+ 	BUILD_BUG_ON(ARRAY_SIZE(partitions) != ARRAY_SIZE(tsgl->sgl));
+ 	if (WARN_ON(ndivs > ARRAY_SIZE(partitions)))
+ 		return -EINVAL;
+ 
+ 	/* Calculate the (div, length) pairs */
+ 	tsgl->nents = 0;
+ 	for (i = 0; i < ndivs; i++) {
+ 		unsigned int len_this_sg =
+ 			min(len_remaining,
+ 			    (total_len * divs[i].proportion_of_total +
+ 			     TEST_SG_TOTAL / 2) / TEST_SG_TOTAL);
+ 
+ 		if (len_this_sg != 0) {
+ 			partitions[tsgl->nents].div = &divs[i];
+ 			partitions[tsgl->nents].length = len_this_sg;
+ 			tsgl->nents++;
+ 			len_remaining -= len_this_sg;
+ 		}
+ 	}
+ 	if (tsgl->nents == 0) {
+ 		partitions[tsgl->nents].div = &divs[0];
+ 		partitions[tsgl->nents].length = 0;
+ 		tsgl->nents++;
+ 	}
+ 	partitions[tsgl->nents - 1].length += len_remaining;
+ 
+ 	/* Set up the sgl entries and fill the data or poison */
+ 	sg_init_table(tsgl->sgl, tsgl->nents);
+ 	for (i = 0; i < tsgl->nents; i++) {
+ 		unsigned int offset = partitions[i].div->offset;
+ 		void *addr;
+ 
+ 		if (partitions[i].div->offset_relative_to_alignmask)
+ 			offset += alignmask;
+ 
+ 		while (offset + partitions[i].length + TESTMGR_POISON_LEN >
+ 		       2 * PAGE_SIZE) {
+ 			if (WARN_ON(offset <= 0))
+ 				return -EINVAL;
+ 			offset /= 2;
+ 		}
+ 
+ 		addr = &tsgl->bufs[i][offset];
+ 		sg_set_buf(&tsgl->sgl[i], addr, partitions[i].length);
+ 
+ 		if (out_divs)
+ 			out_divs[i] = partitions[i].div;
+ 
+ 		if (data) {
+ 			size_t copy_len, copied;
+ 
+ 			copy_len = min(partitions[i].length, data->count);
+ 			copied = copy_from_iter(addr, copy_len, data);
+ 			if (WARN_ON(copied != copy_len))
+ 				return -EINVAL;
+ 			testmgr_poison(addr + copy_len, partitions[i].length +
+ 				       TESTMGR_POISON_LEN - copy_len);
+ 		} else {
+ 			testmgr_poison(addr, partitions[i].length +
+ 				       TESTMGR_POISON_LEN);
+ 		}
+ 	}
+ 
+ 	sg_mark_end(&tsgl->sgl[tsgl->nents - 1]);
+ 	tsgl->sgl_ptr = tsgl->sgl;
+ 	memcpy(tsgl->sgl_saved, tsgl->sgl, tsgl->nents * sizeof(tsgl->sgl[0]));
+ 	return 0;
+ }
+ 
+ /*
+  * Verify that a scatterlist crypto operation produced the correct output.
+  *
+  * @tsgl: scatterlist containing the actual output
+  * @expected_output: buffer containing the expected output
+  * @len_to_check: length of @expected_output in bytes
+  * @unchecked_prefix_len: number of ignored bytes in @tsgl prior to real result
+  * @check_poison: verify that the poison bytes after each chunk are intact?
+  *
+  * Return: 0 if correct, -EINVAL if incorrect, -EOVERFLOW if buffer overrun.
+  */
+ static int verify_correct_output(const struct test_sglist *tsgl,
+ 				 const char *expected_output,
+ 				 unsigned int len_to_check,
+ 				 unsigned int unchecked_prefix_len,
+ 				 bool check_poison)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < tsgl->nents; i++) {
+ 		struct scatterlist *sg = &tsgl->sgl_ptr[i];
+ 		unsigned int len = sg->length;
+ 		unsigned int offset = sg->offset;
+ 		const char *actual_output;
+ 
+ 		if (unchecked_prefix_len) {
+ 			if (unchecked_prefix_len >= len) {
+ 				unchecked_prefix_len -= len;
+ 				continue;
+ 			}
+ 			offset += unchecked_prefix_len;
+ 			len -= unchecked_prefix_len;
+ 			unchecked_prefix_len = 0;
+ 		}
+ 		len = min(len, len_to_check);
+ 		actual_output = page_address(sg_page(sg)) + offset;
+ 		if (memcmp(expected_output, actual_output, len) != 0)
+ 			return -EINVAL;
+ 		if (check_poison &&
+ 		    !testmgr_is_poison(actual_output + len, TESTMGR_POISON_LEN))
+ 			return -EOVERFLOW;
+ 		len_to_check -= len;
+ 		expected_output += len;
+ 	}
+ 	if (WARN_ON(len_to_check != 0))
+ 		return -EINVAL;
+ 	return 0;
+ }
+ 
+ static bool is_test_sglist_corrupted(const struct test_sglist *tsgl)
+ {
+ 	unsigned int i;
+ 
+ 	for (i = 0; i < tsgl->nents; i++) {
+ 		if (tsgl->sgl[i].page_link != tsgl->sgl_saved[i].page_link)
+ 			return true;
+ 		if (tsgl->sgl[i].offset != tsgl->sgl_saved[i].offset)
+ 			return true;
+ 		if (tsgl->sgl[i].length != tsgl->sgl_saved[i].length)
+ 			return true;
+ 	}
+ 	return false;
+ }
+ 
+ struct cipher_test_sglists {
+ 	struct test_sglist src;
+ 	struct test_sglist dst;
+ };
+ 
+ static struct cipher_test_sglists *alloc_cipher_test_sglists(void)
+ {
+ 	struct cipher_test_sglists *tsgls;
+ 
+ 	tsgls = kmalloc(sizeof(*tsgls), GFP_KERNEL);
+ 	if (!tsgls)
+ 		return NULL;
+ 
+ 	if (init_test_sglist(&tsgls->src) != 0)
+ 		goto fail_kfree;
+ 	if (init_test_sglist(&tsgls->dst) != 0)
+ 		goto fail_destroy_src;
+ 
+ 	return tsgls;
+ 
+ fail_destroy_src:
+ 	destroy_test_sglist(&tsgls->src);
+ fail_kfree:
+ 	kfree(tsgls);
+ 	return NULL;
+ }
+ 
+ static void free_cipher_test_sglists(struct cipher_test_sglists *tsgls)
+ {
+ 	if (tsgls) {
+ 		destroy_test_sglist(&tsgls->src);
+ 		destroy_test_sglist(&tsgls->dst);
+ 		kfree(tsgls);
+ 	}
+ }
+ 
+ /* Build the src and dst scatterlists for an skcipher or AEAD test */
+ static int build_cipher_test_sglists(struct cipher_test_sglists *tsgls,
+ 				     const struct testvec_config *cfg,
+ 				     unsigned int alignmask,
+ 				     unsigned int src_total_len,
+ 				     unsigned int dst_total_len,
+ 				     const struct kvec *inputs,
+ 				     unsigned int nr_inputs)
+ {
+ 	struct iov_iter input;
+ 	int err;
+ 
+ 	iov_iter_kvec(&input, WRITE, inputs, nr_inputs, src_total_len);
+ 	err = build_test_sglist(&tsgls->src, cfg->src_divs, alignmask,
+ 				cfg->inplace_mode != OUT_OF_PLACE ?
+ 					max(dst_total_len, src_total_len) :
+ 					src_total_len,
+ 				&input, NULL);
+ 	if (err)
+ 		return err;
+ 
+ 	/*
+ 	 * In-place crypto operations can use the same scatterlist for both the
+ 	 * source and destination (req->src == req->dst), or can use separate
+ 	 * scatterlists (req->src != req->dst) which point to the same
+ 	 * underlying memory.  Make sure to test both cases.
+ 	 */
+ 	if (cfg->inplace_mode == INPLACE_ONE_SGLIST) {
+ 		tsgls->dst.sgl_ptr = tsgls->src.sgl;
+ 		tsgls->dst.nents = tsgls->src.nents;
+ 		return 0;
+ 	}
+ 	if (cfg->inplace_mode == INPLACE_TWO_SGLISTS) {
+ 		/*
+ 		 * For now we keep it simple and only test the case where the
+ 		 * two scatterlists have identical entries, rather than
+ 		 * different entries that split up the same memory differently.
+ 		 */
+ 		memcpy(tsgls->dst.sgl, tsgls->src.sgl,
+ 		       tsgls->src.nents * sizeof(tsgls->src.sgl[0]));
+ 		memcpy(tsgls->dst.sgl_saved, tsgls->src.sgl,
+ 		       tsgls->src.nents * sizeof(tsgls->src.sgl[0]));
+ 		tsgls->dst.sgl_ptr = tsgls->dst.sgl;
+ 		tsgls->dst.nents = tsgls->src.nents;
+ 		return 0;
+ 	}
+ 	/* Out of place */
+ 	return build_test_sglist(&tsgls->dst,
+ 				 cfg->dst_divs[0].proportion_of_total ?
+ 					cfg->dst_divs : cfg->src_divs,
+ 				 alignmask, dst_total_len, NULL, NULL);
+ }
+ 
+ /*
+  * Support for testing passing a misaligned key to setkey():
+  *
+  * If cfg->key_offset is set, copy the key into a new buffer at that offset,
+  * optionally adding alignmask.  Else, just use the key directly.
+  */
+ static int prepare_keybuf(const u8 *key, unsigned int ksize,
+ 			  const struct testvec_config *cfg,
+ 			  unsigned int alignmask,
+ 			  const u8 **keybuf_ret, const u8 **keyptr_ret)
+ {
+ 	unsigned int key_offset = cfg->key_offset;
+ 	u8 *keybuf = NULL, *keyptr = (u8 *)key;
+ 
+ 	if (key_offset != 0) {
+ 		if (cfg->key_offset_relative_to_alignmask)
+ 			key_offset += alignmask;
+ 		keybuf = kmalloc(key_offset + ksize, GFP_KERNEL);
+ 		if (!keybuf)
+ 			return -ENOMEM;
+ 		keyptr = keybuf + key_offset;
+ 		memcpy(keyptr, key, ksize);
+ 	}
+ 	*keybuf_ret = keybuf;
+ 	*keyptr_ret = keyptr;
+ 	return 0;
+ }
+ 
+ /* Like setkey_f(tfm, key, ksize), but sometimes misalign the key */
+ #define do_setkey(setkey_f, tfm, key, ksize, cfg, alignmask)		\
+ ({									\
+ 	const u8 *keybuf, *keyptr;					\
+ 	int err;							\
+ 									\
+ 	err = prepare_keybuf((key), (ksize), (cfg), (alignmask),	\
+ 			     &keybuf, &keyptr);				\
+ 	if (err == 0) {							\
+ 		err = setkey_f((tfm), keyptr, (ksize));			\
+ 		kfree(keybuf);						\
+ 	}								\
+ 	err;								\
+ })
+ 
+ #ifdef CONFIG_CRYPTO_MANAGER_EXTRA_TESTS
+ 
+ /* Generate a random length in range [0, max_len], but prefer smaller values */
+ static unsigned int generate_random_length(unsigned int max_len)
+ {
+ 	unsigned int len = get_random_u32_below(max_len + 1);
+ 
+ 	switch (get_random_u32_below(4)) {
+ 	case 0:
+ 		return len % 64;
+ 	case 1:
+ 		return len % 256;
+ 	case 2:
+ 		return len % 1024;
+ 	default:
+ 		return len;
+ 	}
+ }
+ 
+ /* Flip a random bit in the given nonempty data buffer */
+ static void flip_random_bit(u8 *buf, size_t size)
+ {
+ 	size_t bitpos;
+ 
+ 	bitpos = get_random_u32_below(size * 8);
+ 	buf[bitpos / 8] ^= 1 << (bitpos % 8);
+ }
+ 
+ /* Flip a random byte in the given nonempty data buffer */
+ static void flip_random_byte(u8 *buf, size_t size)
+ {
+ 	buf[get_random_u32_below(size)] ^= 0xff;
+ }
+ 
+ /* Sometimes make some random changes to the given nonempty data buffer */
+ static void mutate_buffer(u8 *buf, size_t size)
+ {
+ 	size_t num_flips;
+ 	size_t i;
+ 
+ 	/* Sometimes flip some bits */
+ 	if (get_random_u32_below(4) == 0) {
+ 		num_flips = min_t(size_t, 1 << get_random_u32_below(8), size * 8);
+ 		for (i = 0; i < num_flips; i++)
+ 			flip_random_bit(buf, size);
+ 	}
+ 
+ 	/* Sometimes flip some bytes */
+ 	if (get_random_u32_below(4) == 0) {
+ 		num_flips = min_t(size_t, 1 << get_random_u32_below(8), size);
+ 		for (i = 0; i < num_flips; i++)
+ 			flip_random_byte(buf, size);
+ 	}
+ }
+ 
+ /* Randomly generate 'count' bytes, but sometimes make them "interesting" */
+ static void generate_random_bytes(u8 *buf, size_t count)
+ {
+ 	u8 b;
+ 	u8 increment;
+ 	size_t i;
+ 
+ 	if (count == 0)
+ 		return;
+ 
+ 	switch (get_random_u32_below(8)) { /* Choose a generation strategy */
+ 	case 0:
+ 	case 1:
+ 		/* All the same byte, plus optional mutations */
+ 		switch (get_random_u32_below(4)) {
+ 		case 0:
+ 			b = 0x00;
+ 			break;
+ 		case 1:
+ 			b = 0xff;
+ 			break;
+ 		default:
+ 			b = get_random_u8();
+ 			break;
+ 		}
+ 		memset(buf, b, count);
+ 		mutate_buffer(buf, count);
+ 		break;
+ 	case 2:
+ 		/* Ascending or descending bytes, plus optional mutations */
+ 		increment = get_random_u8();
+ 		b = get_random_u8();
+ 		for (i = 0; i < count; i++, b += increment)
+ 			buf[i] = b;
+ 		mutate_buffer(buf, count);
+ 		break;
+ 	default:
+ 		/* Fully random bytes */
+ 		for (i = 0; i < count; i++)
+ 			buf[i] = get_random_u8();
+ 	}
+ }
+ 
+ static char *generate_random_sgl_divisions(struct test_sg_division *divs,
+ 					   size_t max_divs, char *p, char *end,
+ 					   bool gen_flushes, u32 req_flags)
+ {
+ 	struct test_sg_division *div = divs;
+ 	unsigned int remaining = TEST_SG_TOTAL;
+ 
+ 	do {
+ 		unsigned int this_len;
+ 		const char *flushtype_str;
+ 
+ 		if (div == &divs[max_divs - 1] || get_random_u32_below(2) == 0)
+ 			this_len = remaining;
+ 		else
+ 			this_len = get_random_u32_inclusive(1, remaining);
+ 		div->proportion_of_total = this_len;
+ 
+ 		if (get_random_u32_below(4) == 0)
+ 			div->offset = get_random_u32_inclusive(PAGE_SIZE - 128, PAGE_SIZE - 1);
+ 		else if (get_random_u32_below(2) == 0)
+ 			div->offset = get_random_u32_below(32);
+ 		else
+ 			div->offset = get_random_u32_below(PAGE_SIZE);
+ 		if (get_random_u32_below(8) == 0)
+ 			div->offset_relative_to_alignmask = true;
+ 
+ 		div->flush_type = FLUSH_TYPE_NONE;
+ 		if (gen_flushes) {
+ 			switch (get_random_u32_below(4)) {
+ 			case 0:
+ 				div->flush_type = FLUSH_TYPE_REIMPORT;
+ 				break;
+ 			case 1:
+ 				div->flush_type = FLUSH_TYPE_FLUSH;
+ 				break;
+ 			}
+ 		}
+ 
+ 		if (div->flush_type != FLUSH_TYPE_NONE &&
+ 		    !(req_flags & CRYPTO_TFM_REQ_MAY_SLEEP) &&
+ 		    get_random_u32_below(2) == 0)
+ 			div->nosimd = true;
+ 
+ 		switch (div->flush_type) {
+ 		case FLUSH_TYPE_FLUSH:
+ 			if (div->nosimd)
+ 				flushtype_str = "<flush,nosimd>";
+ 			else
+ 				flushtype_str = "<flush>";
+ 			break;
+ 		case FLUSH_TYPE_REIMPORT:
+ 			if (div->nosimd)
+ 				flushtype_str = "<reimport,nosimd>";
+ 			else
+ 				flushtype_str = "<reimport>";
+ 			break;
+ 		default:
+ 			flushtype_str = "";
+ 			break;
+ 		}
+ 
+ 		BUILD_BUG_ON(TEST_SG_TOTAL != 10000); /* for "%u.%u%%" */
+ 		p += scnprintf(p, end - p, "%s%u.%u%%@%s+%u%s", flushtype_str,
+ 			       this_len / 100, this_len % 100,
+ 			       div->offset_relative_to_alignmask ?
+ 					"alignmask" : "",
+ 			       div->offset, this_len == remaining ? "" : ", ");
+ 		remaining -= this_len;
+ 		div++;
+ 	} while (remaining);
+ 
+ 	return p;
+ }
+ 
+ /* Generate a random testvec_config for fuzz testing */
+ static void generate_random_testvec_config(struct testvec_config *cfg,
+ 					   char *name, size_t max_namelen)
+ {
+ 	char *p = name;
+ 	char * const end = name + max_namelen;
+ 
+ 	memset(cfg, 0, sizeof(*cfg));
+ 
+ 	cfg->name = name;
+ 
+ 	p += scnprintf(p, end - p, "random:");
+ 
+ 	switch (get_random_u32_below(4)) {
+ 	case 0:
+ 	case 1:
+ 		cfg->inplace_mode = OUT_OF_PLACE;
+ 		break;
+ 	case 2:
+ 		cfg->inplace_mode = INPLACE_ONE_SGLIST;
+ 		p += scnprintf(p, end - p, " inplace_one_sglist");
+ 		break;
+ 	default:
+ 		cfg->inplace_mode = INPLACE_TWO_SGLISTS;
+ 		p += scnprintf(p, end - p, " inplace_two_sglists");
+ 		break;
+ 	}
+ 
+ 	if (get_random_u32_below(2) == 0) {
+ 		cfg->req_flags |= CRYPTO_TFM_REQ_MAY_SLEEP;
+ 		p += scnprintf(p, end - p, " may_sleep");
+ 	}
+ 
+ 	switch (get_random_u32_below(4)) {
+ 	case 0:
+ 		cfg->finalization_type = FINALIZATION_TYPE_FINAL;
+ 		p += scnprintf(p, end - p, " use_final");
+ 		break;
+ 	case 1:
+ 		cfg->finalization_type = FINALIZATION_TYPE_FINUP;
+ 		p += scnprintf(p, end - p, " use_finup");
+ 		break;
+ 	default:
+ 		cfg->finalization_type = FINALIZATION_TYPE_DIGEST;
+ 		p += scnprintf(p, end - p, " use_digest");
+ 		break;
+ 	}
+ 
+ 	if (!(cfg->req_flags & CRYPTO_TFM_REQ_MAY_SLEEP) &&
+ 	    get_random_u32_below(2) == 0) {
+ 		cfg->nosimd = true;
+ 		p += scnprintf(p, end - p, " nosimd");
+ 	}
+ 
+ 	p += scnprintf(p, end - p, " src_divs=[");
+ 	p = generate_random_sgl_divisions(cfg->src_divs,
+ 					  ARRAY_SIZE(cfg->src_divs), p, end,
+ 					  (cfg->finalization_type !=
+ 					   FINALIZATION_TYPE_DIGEST),
+ 					  cfg->req_flags);
+ 	p += scnprintf(p, end - p, "]");
+ 
+ 	if (cfg->inplace_mode == OUT_OF_PLACE && get_random_u32_below(2) == 0) {
+ 		p += scnprintf(p, end - p, " dst_divs=[");
+ 		p = generate_random_sgl_divisions(cfg->dst_divs,
+ 						  ARRAY_SIZE(cfg->dst_divs),
+ 						  p, end, false,
+ 						  cfg->req_flags);
+ 		p += scnprintf(p, end - p, "]");
+ 	}
+ 
+ 	if (get_random_u32_below(2) == 0) {
+ 		cfg->iv_offset = get_random_u32_inclusive(1, MAX_ALGAPI_ALIGNMASK);
+ 		p += scnprintf(p, end - p, " iv_offset=%u", cfg->iv_offset);
+ 	}
+ 
+ 	if (get_random_u32_below(2) == 0) {
+ 		cfg->key_offset = get_random_u32_inclusive(1, MAX_ALGAPI_ALIGNMASK);
+ 		p += scnprintf(p, end - p, " key_offset=%u", cfg->key_offset);
+ 	}
+ 
+ 	WARN_ON_ONCE(!valid_testvec_config(cfg));
+ }
+ 
+ static void crypto_disable_simd_for_test(void)
+ {
+ 	migrate_disable();
+ 	__this_cpu_write(crypto_simd_disabled_for_test, true);
+ }
+ 
+ static void crypto_reenable_simd_for_test(void)
+ {
+ 	__this_cpu_write(crypto_simd_disabled_for_test, false);
+ 	migrate_enable();
+ }
+ 
+ /*
+  * Given an algorithm name, build the name of the generic implementation of that
+  * algorithm, assuming the usual naming convention.  Specifically, this appends
+  * "-generic" to every part of the name that is not a template name.  Examples:
+  *
+  *	aes => aes-generic
+  *	cbc(aes) => cbc(aes-generic)
+  *	cts(cbc(aes)) => cts(cbc(aes-generic))
+  *	rfc7539(chacha20,poly1305) => rfc7539(chacha20-generic,poly1305-generic)
+  *
+  * Return: 0 on success, or -ENAMETOOLONG if the generic name would be too long
+  */
+ static int build_generic_driver_name(const char *algname,
+ 				     char driver_name[CRYPTO_MAX_ALG_NAME])
+ {
+ 	const char *in = algname;
+ 	char *out = driver_name;
+ 	size_t len = strlen(algname);
+ 
+ 	if (len >= CRYPTO_MAX_ALG_NAME)
+ 		goto too_long;
+ 	do {
+ 		const char *in_saved = in;
+ 
+ 		while (*in && *in != '(' && *in != ')' && *in != ',')
+ 			*out++ = *in++;
+ 		if (*in != '(' && in > in_saved) {
+ 			len += 8;
+ 			if (len >= CRYPTO_MAX_ALG_NAME)
+ 				goto too_long;
+ 			memcpy(out, "-generic", 8);
+ 			out += 8;
+ 		}
+ 	} while ((*out++ = *in++) != '\0');
+ 	return 0;
+ 
+ too_long:
+ 	pr_err("alg: generic driver name for \"%s\" would be too long\n",
+ 	       algname);
+ 	return -ENAMETOOLONG;
+ }
+ #else /* !CONFIG_CRYPTO_MANAGER_EXTRA_TESTS */
+ static void crypto_disable_simd_for_test(void)
+ {
+ }
+ 
+ static void crypto_reenable_simd_for_test(void)
+ {
+ }
+ #endif /* !CONFIG_CRYPTO_MANAGER_EXTRA_TESTS */
+ 
+ static int build_hash_sglist(struct test_sglist *tsgl,
+ 			     const struct hash_testvec *vec,
+ 			     const struct testvec_config *cfg,
+ 			     unsigned int alignmask,
+ 			     const struct test_sg_division *divs[XBUFSIZE])
+ {
+ 	struct kvec kv;
+ 	struct iov_iter input;
+ 
+ 	kv.iov_base = (void *)vec->plaintext;
+ 	kv.iov_len = vec->psize;
+ 	iov_iter_kvec(&input, WRITE, &kv, 1, vec->psize);
+ 	return build_test_sglist(tsgl, cfg->src_divs, alignmask, vec->psize,
+ 				 &input, divs);
+ }
+ 
+ static int check_hash_result(const char *type,
+ 			     const u8 *result, unsigned int digestsize,
+ 			     const struct hash_testvec *vec,
+ 			     const char *vec_name,
+ 			     const char *driver,
+ 			     const struct testvec_config *cfg)
+ {
+ 	if (memcmp(result, vec->digest, digestsize) != 0) {
+ 		pr_err("alg: %s: %s test failed (wrong result) on test vector %s, cfg=\"%s\"\n",
+ 		       type, driver, vec_name, cfg->name);
+ 		return -EINVAL;
+ 	}
+ 	if (!testmgr_is_poison(&result[digestsize], TESTMGR_POISON_LEN)) {
+ 		pr_err("alg: %s: %s overran result buffer on test vector %s, cfg=\"%s\"\n",
+ 		       type, driver, vec_name, cfg->name);
+ 		return -EOVERFLOW;
+ 	}
+ 	return 0;
+ }
+ 
+ static inline int check_shash_op(const char *op, int err,
+ 				 const char *driver, const char *vec_name,
+ 				 const struct testvec_config *cfg)
+ {
+ 	if (err)
+ 		pr_err("alg: shash: %s %s() failed with err %d on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, err, vec_name, cfg->name);
+ 	return err;
+ }
+ 
+ /* Test one hash test vector in one configuration, using the shash API */
+ static int test_shash_vec_cfg(const struct hash_testvec *vec,
+ 			      const char *vec_name,
+ 			      const struct testvec_config *cfg,
+ 			      struct shash_desc *desc,
+ 			      struct test_sglist *tsgl,
+ 			      u8 *hashstate)
+ {
+ 	struct crypto_shash *tfm = desc->tfm;
+ 	const unsigned int alignmask = crypto_shash_alignmask(tfm);
+ 	const unsigned int digestsize = crypto_shash_digestsize(tfm);
+ 	const unsigned int statesize = crypto_shash_statesize(tfm);
+ 	const char *driver = crypto_shash_driver_name(tfm);
+ 	const struct test_sg_division *divs[XBUFSIZE];
+ 	unsigned int i;
+ 	u8 result[HASH_MAX_DIGESTSIZE + TESTMGR_POISON_LEN];
+ 	int err;
+ 
+ 	/* Set the key, if specified */
+ 	if (vec->ksize) {
+ 		err = do_setkey(crypto_shash_setkey, tfm, vec->key, vec->ksize,
+ 				cfg, alignmask);
+ 		if (err) {
+ 			if (err == vec->setkey_error)
+ 				return 0;
+ 			pr_err("alg: shash: %s setkey failed on test vector %s; expected_error=%d, actual_error=%d, flags=%#x\n",
+ 			       driver, vec_name, vec->setkey_error, err,
+ 			       crypto_shash_get_flags(tfm));
+ 			return err;
+ 		}
+ 		if (vec->setkey_error) {
+ 			pr_err("alg: shash: %s setkey unexpectedly succeeded on test vector %s; expected_error=%d\n",
+ 			       driver, vec_name, vec->setkey_error);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	/* Build the scatterlist for the source data */
+ 	err = build_hash_sglist(tsgl, vec, cfg, alignmask, divs);
+ 	if (err) {
+ 		pr_err("alg: shash: %s: error preparing scatterlist for test vector %s, cfg=\"%s\"\n",
+ 		       driver, vec_name, cfg->name);
+ 		return err;
+ 	}
+ 
+ 	/* Do the actual hashing */
+ 
+ 	testmgr_poison(desc->__ctx, crypto_shash_descsize(tfm));
+ 	testmgr_poison(result, digestsize + TESTMGR_POISON_LEN);
+ 
+ 	if (cfg->finalization_type == FINALIZATION_TYPE_DIGEST ||
+ 	    vec->digest_error) {
+ 		/* Just using digest() */
+ 		if (tsgl->nents != 1)
+ 			return 0;
+ 		if (cfg->nosimd)
+ 			crypto_disable_simd_for_test();
+ 		err = crypto_shash_digest(desc, sg_virt(&tsgl->sgl[0]),
+ 					  tsgl->sgl[0].length, result);
+ 		if (cfg->nosimd)
+ 			crypto_reenable_simd_for_test();
+ 		if (err) {
+ 			if (err == vec->digest_error)
+ 				return 0;
+ 			pr_err("alg: shash: %s digest() failed on test vector %s; expected_error=%d, actual_error=%d, cfg=\"%s\"\n",
+ 			       driver, vec_name, vec->digest_error, err,
+ 			       cfg->name);
+ 			return err;
+ 		}
+ 		if (vec->digest_error) {
+ 			pr_err("alg: shash: %s digest() unexpectedly succeeded on test vector %s; expected_error=%d, cfg=\"%s\"\n",
+ 			       driver, vec_name, vec->digest_error, cfg->name);
+ 			return -EINVAL;
+ 		}
+ 		goto result_ready;
+ 	}
+ 
+ 	/* Using init(), zero or more update(), then final() or finup() */
+ 
+ 	if (cfg->nosimd)
+ 		crypto_disable_simd_for_test();
+ 	err = crypto_shash_init(desc);
+ 	if (cfg->nosimd)
+ 		crypto_reenable_simd_for_test();
+ 	err = check_shash_op("init", err, driver, vec_name, cfg);
+ 	if (err)
+ 		return err;
+ 
+ 	for (i = 0; i < tsgl->nents; i++) {
+ 		if (i + 1 == tsgl->nents &&
+ 		    cfg->finalization_type == FINALIZATION_TYPE_FINUP) {
+ 			if (divs[i]->nosimd)
+ 				crypto_disable_simd_for_test();
+ 			err = crypto_shash_finup(desc, sg_virt(&tsgl->sgl[i]),
+ 						 tsgl->sgl[i].length, result);
+ 			if (divs[i]->nosimd)
+ 				crypto_reenable_simd_for_test();
+ 			err = check_shash_op("finup", err, driver, vec_name,
+ 					     cfg);
+ 			if (err)
+ 				return err;
+ 			goto result_ready;
+ 		}
+ 		if (divs[i]->nosimd)
+ 			crypto_disable_simd_for_test();
+ 		err = crypto_shash_update(desc, sg_virt(&tsgl->sgl[i]),
+ 					  tsgl->sgl[i].length);
+ 		if (divs[i]->nosimd)
+ 			crypto_reenable_simd_for_test();
+ 		err = check_shash_op("update", err, driver, vec_name, cfg);
+ 		if (err)
+ 			return err;
+ 		if (divs[i]->flush_type == FLUSH_TYPE_REIMPORT) {
+ 			/* Test ->export() and ->import() */
+ 			testmgr_poison(hashstate + statesize,
+ 				       TESTMGR_POISON_LEN);
+ 			err = crypto_shash_export(desc, hashstate);
+ 			err = check_shash_op("export", err, driver, vec_name,
+ 					     cfg);
+ 			if (err)
+ 				return err;
+ 			if (!testmgr_is_poison(hashstate + statesize,
+ 					       TESTMGR_POISON_LEN)) {
+ 				pr_err("alg: shash: %s export() overran state buffer on test vector %s, cfg=\"%s\"\n",
+ 				       driver, vec_name, cfg->name);
+ 				return -EOVERFLOW;
+ 			}
+ 			testmgr_poison(desc->__ctx, crypto_shash_descsize(tfm));
+ 			err = crypto_shash_import(desc, hashstate);
+ 			err = check_shash_op("import", err, driver, vec_name,
+ 					     cfg);
+ 			if (err)
+ 				return err;
+ 		}
+ 	}
+ 
+ 	if (cfg->nosimd)
+ 		crypto_disable_simd_for_test();
+ 	err = crypto_shash_final(desc, result);
+ 	if (cfg->nosimd)
+ 		crypto_reenable_simd_for_test();
+ 	err = check_shash_op("final", err, driver, vec_name, cfg);
+ 	if (err)
+ 		return err;
+ result_ready:
+ 	return check_hash_result("shash", result, digestsize, vec, vec_name,
+ 				 driver, cfg);
+ }
+ 
+ static int do_ahash_op(int (*op)(struct ahash_request *req),
+ 		       struct ahash_request *req,
+ 		       struct crypto_wait *wait, bool nosimd)
+ {
+ 	int err;
+ 
+ 	if (nosimd)
+ 		crypto_disable_simd_for_test();
+ 
+ 	err = op(req);
+ 
+ 	if (nosimd)
+ 		crypto_reenable_simd_for_test();
+ 
+ 	return crypto_wait_req(err, wait);
+ }
+ 
+ static int check_nonfinal_ahash_op(const char *op, int err,
+ 				   u8 *result, unsigned int digestsize,
+ 				   const char *driver, const char *vec_name,
+ 				   const struct testvec_config *cfg)
+ {
+ 	if (err) {
+ 		pr_err("alg: ahash: %s %s() failed with err %d on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, err, vec_name, cfg->name);
+ 		return err;
+ 	}
+ 	if (!testmgr_is_poison(result, digestsize)) {
+ 		pr_err("alg: ahash: %s %s() used result buffer on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, cfg->name);
+ 		return -EINVAL;
+ 	}
+ 	return 0;
+ }
+ 
+ /* Test one hash test vector in one configuration, using the ahash API */
+ static int test_ahash_vec_cfg(const struct hash_testvec *vec,
+ 			      const char *vec_name,
+ 			      const struct testvec_config *cfg,
+ 			      struct ahash_request *req,
+ 			      struct test_sglist *tsgl,
+ 			      u8 *hashstate)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	const unsigned int alignmask = crypto_ahash_alignmask(tfm);
+ 	const unsigned int digestsize = crypto_ahash_digestsize(tfm);
+ 	const unsigned int statesize = crypto_ahash_statesize(tfm);
+ 	const char *driver = crypto_ahash_driver_name(tfm);
+ 	const u32 req_flags = CRYPTO_TFM_REQ_MAY_BACKLOG | cfg->req_flags;
+ 	const struct test_sg_division *divs[XBUFSIZE];
+ 	DECLARE_CRYPTO_WAIT(wait);
+ 	unsigned int i;
+ 	struct scatterlist *pending_sgl;
+ 	unsigned int pending_len;
+ 	u8 result[HASH_MAX_DIGESTSIZE + TESTMGR_POISON_LEN];
+ 	int err;
+ 
+ 	/* Set the key, if specified */
+ 	if (vec->ksize) {
+ 		err = do_setkey(crypto_ahash_setkey, tfm, vec->key, vec->ksize,
+ 				cfg, alignmask);
+ 		if (err) {
+ 			if (err == vec->setkey_error)
+ 				return 0;
+ 			pr_err("alg: ahash: %s setkey failed on test vector %s; expected_error=%d, actual_error=%d, flags=%#x\n",
+ 			       driver, vec_name, vec->setkey_error, err,
+ 			       crypto_ahash_get_flags(tfm));
+ 			return err;
+ 		}
+ 		if (vec->setkey_error) {
+ 			pr_err("alg: ahash: %s setkey unexpectedly succeeded on test vector %s; expected_error=%d\n",
+ 			       driver, vec_name, vec->setkey_error);
+ 			return -EINVAL;
+ 		}
+ 	}
+ 
+ 	/* Build the scatterlist for the source data */
+ 	err = build_hash_sglist(tsgl, vec, cfg, alignmask, divs);
+ 	if (err) {
+ 		pr_err("alg: ahash: %s: error preparing scatterlist for test vector %s, cfg=\"%s\"\n",
+ 		       driver, vec_name, cfg->name);
+ 		return err;
+ 	}
+ 
+ 	/* Do the actual hashing */
+ 
+ 	testmgr_poison(req->__ctx, crypto_ahash_reqsize(tfm));
+ 	testmgr_poison(result, digestsize + TESTMGR_POISON_LEN);
+ 
+ 	if (cfg->finalization_type == FINALIZATION_TYPE_DIGEST ||
+ 	    vec->digest_error) {
+ 		/* Just using digest() */
+ 		ahash_request_set_callback(req, req_flags, crypto_req_done,
+ 					   &wait);
+ 		ahash_request_set_crypt(req, tsgl->sgl, result, vec->psize);
+ 		err = do_ahash_op(crypto_ahash_digest, req, &wait, cfg->nosimd);
+ 		if (err) {
+ 			if (err == vec->digest_error)
+ 				return 0;
+ 			pr_err("alg: ahash: %s digest() failed on test vector %s; expected_error=%d, actual_error=%d, cfg=\"%s\"\n",
+ 			       driver, vec_name, vec->digest_error, err,
+ 			       cfg->name);
+ 			return err;
+ 		}
+ 		if (vec->digest_error) {
+ 			pr_err("alg: ahash: %s digest() unexpectedly succeeded on test vector %s; expected_error=%d, cfg=\"%s\"\n",
+ 			       driver, vec_name, vec->digest_error, cfg->name);
+ 			return -EINVAL;
+ 		}
+ 		goto result_ready;
+ 	}
+ 
+ 	/* Using init(), zero or more update(), then final() or finup() */
+ 
+ 	ahash_request_set_callback(req, req_flags, crypto_req_done, &wait);
+ 	ahash_request_set_crypt(req, NULL, result, 0);
+ 	err = do_ahash_op(crypto_ahash_init, req, &wait, cfg->nosimd);
+ 	err = check_nonfinal_ahash_op("init", err, result, digestsize,
+ 				      driver, vec_name, cfg);
+ 	if (err)
+ 		return err;
+ 
+ 	pending_sgl = NULL;
+ 	pending_len = 0;
+ 	for (i = 0; i < tsgl->nents; i++) {
+ 		if (divs[i]->flush_type != FLUSH_TYPE_NONE &&
+ 		    pending_sgl != NULL) {
+ 			/* update() with the pending data */
+ 			ahash_request_set_callback(req, req_flags,
+ 						   crypto_req_done, &wait);
+ 			ahash_request_set_crypt(req, pending_sgl, result,
+ 						pending_len);
+ 			err = do_ahash_op(crypto_ahash_update, req, &wait,
+ 					  divs[i]->nosimd);
+ 			err = check_nonfinal_ahash_op("update", err,
+ 						      result, digestsize,
+ 						      driver, vec_name, cfg);
+ 			if (err)
+ 				return err;
+ 			pending_sgl = NULL;
+ 			pending_len = 0;
+ 		}
+ 		if (divs[i]->flush_type == FLUSH_TYPE_REIMPORT) {
+ 			/* Test ->export() and ->import() */
+ 			testmgr_poison(hashstate + statesize,
+ 				       TESTMGR_POISON_LEN);
+ 			err = crypto_ahash_export(req, hashstate);
+ 			err = check_nonfinal_ahash_op("export", err,
+ 						      result, digestsize,
+ 						      driver, vec_name, cfg);
+ 			if (err)
+ 				return err;
+ 			if (!testmgr_is_poison(hashstate + statesize,
+ 					       TESTMGR_POISON_LEN)) {
+ 				pr_err("alg: ahash: %s export() overran state buffer on test vector %s, cfg=\"%s\"\n",
+ 				       driver, vec_name, cfg->name);
+ 				return -EOVERFLOW;
+ 			}
+ 
+ 			testmgr_poison(req->__ctx, crypto_ahash_reqsize(tfm));
+ 			err = crypto_ahash_import(req, hashstate);
+ 			err = check_nonfinal_ahash_op("import", err,
+ 						      result, digestsize,
+ 						      driver, vec_name, cfg);
+ 			if (err)
+ 				return err;
+ 		}
+ 		if (pending_sgl == NULL)
+ 			pending_sgl = &tsgl->sgl[i];
+ 		pending_len += tsgl->sgl[i].length;
+ 	}
+ 
+ 	ahash_request_set_callback(req, req_flags, crypto_req_done, &wait);
+ 	ahash_request_set_crypt(req, pending_sgl, result, pending_len);
+ 	if (cfg->finalization_type == FINALIZATION_TYPE_FINAL) {
+ 		/* finish with update() and final() */
+ 		err = do_ahash_op(crypto_ahash_update, req, &wait, cfg->nosimd);
+ 		err = check_nonfinal_ahash_op("update", err, result, digestsize,
+ 					      driver, vec_name, cfg);
+ 		if (err)
+ 			return err;
+ 		err = do_ahash_op(crypto_ahash_final, req, &wait, cfg->nosimd);
+ 		if (err) {
+ 			pr_err("alg: ahash: %s final() failed with err %d on test vector %s, cfg=\"%s\"\n",
+ 			       driver, err, vec_name, cfg->name);
+ 			return err;
+ 		}
+ 	} else {
+ 		/* finish with finup() */
+ 		err = do_ahash_op(crypto_ahash_finup, req, &wait, cfg->nosimd);
+ 		if (err) {
+ 			pr_err("alg: ahash: %s finup() failed with err %d on test vector %s, cfg=\"%s\"\n",
+ 			       driver, err, vec_name, cfg->name);
+ 			return err;
+ 		}
+ 	}
+ 
+ result_ready:
+ 	return check_hash_result("ahash", result, digestsize, vec, vec_name,
+ 				 driver, cfg);
+ }
+ 
+ static int test_hash_vec_cfg(const struct hash_testvec *vec,
+ 			     const char *vec_name,
+ 			     const struct testvec_config *cfg,
+ 			     struct ahash_request *req,
+ 			     struct shash_desc *desc,
+ 			     struct test_sglist *tsgl,
+ 			     u8 *hashstate)
+ {
+ 	int err;
+ 
+ 	/*
+ 	 * For algorithms implemented as "shash", most bugs will be detected by
+ 	 * both the shash and ahash tests.  Test the shash API first so that the
+ 	 * failures involve less indirection, so are easier to debug.
+ 	 */
+ 
+ 	if (desc) {
+ 		err = test_shash_vec_cfg(vec, vec_name, cfg, desc, tsgl,
+ 					 hashstate);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ 	return test_ahash_vec_cfg(vec, vec_name, cfg, req, tsgl, hashstate);
+ }
+ 
+ static int test_hash_vec(const struct hash_testvec *vec, unsigned int vec_num,
+ 			 struct ahash_request *req, struct shash_desc *desc,
+ 			 struct test_sglist *tsgl, u8 *hashstate)
+ {
+ 	char vec_name[16];
+ 	unsigned int i;
+ 	int err;
+ 
+ 	sprintf(vec_name, "%u", vec_num);
+ 
+ 	for (i = 0; i < ARRAY_SIZE(default_hash_testvec_configs); i++) {
+ 		err = test_hash_vec_cfg(vec, vec_name,
+ 					&default_hash_testvec_configs[i],
+ 					req, desc, tsgl, hashstate);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ #ifdef CONFIG_CRYPTO_MANAGER_EXTRA_TESTS
+ 	if (!noextratests) {
+ 		struct testvec_config cfg;
+ 		char cfgname[TESTVEC_CONFIG_NAMELEN];
+ 
+ 		for (i = 0; i < fuzz_iterations; i++) {
+ 			generate_random_testvec_config(&cfg, cfgname,
+ 						       sizeof(cfgname));
+ 			err = test_hash_vec_cfg(vec, vec_name, &cfg,
+ 						req, desc, tsgl, hashstate);
+ 			if (err)
+ 				return err;
+ 			cond_resched();
+ 		}
+ 	}
+ #endif
+ 	return 0;
+ }
+ 
+ #ifdef CONFIG_CRYPTO_MANAGER_EXTRA_TESTS
+ /*
+  * Generate a hash test vector from the given implementation.
+  * Assumes the buffers in 'vec' were already allocated.
+  */
+ static void generate_random_hash_testvec(struct shash_desc *desc,
+ 					 struct hash_testvec *vec,
+ 					 unsigned int maxkeysize,
+ 					 unsigned int maxdatasize,
+ 					 char *name, size_t max_namelen)
+ {
+ 	/* Data */
+ 	vec->psize = generate_random_length(maxdatasize);
+ 	generate_random_bytes((u8 *)vec->plaintext, vec->psize);
+ 
+ 	/*
+ 	 * Key: length in range [1, maxkeysize], but usually choose maxkeysize.
+ 	 * If algorithm is unkeyed, then maxkeysize == 0 and set ksize = 0.
+ 	 */
+ 	vec->setkey_error = 0;
+ 	vec->ksize = 0;
+ 	if (maxkeysize) {
+ 		vec->ksize = maxkeysize;
+ 		if (get_random_u32_below(4) == 0)
+ 			vec->ksize = get_random_u32_inclusive(1, maxkeysize);
+ 		generate_random_bytes((u8 *)vec->key, vec->ksize);
+ 
+ 		vec->setkey_error = crypto_shash_setkey(desc->tfm, vec->key,
+ 							vec->ksize);
+ 		/* If the key couldn't be set, no need to continue to digest. */
+ 		if (vec->setkey_error)
+ 			goto done;
+ 	}
+ 
+ 	/* Digest */
+ 	vec->digest_error = crypto_shash_digest(desc, vec->plaintext,
+ 						vec->psize, (u8 *)vec->digest);
+ done:
+ 	snprintf(name, max_namelen, "\"random: psize=%u ksize=%u\"",
+ 		 vec->psize, vec->ksize);
+ }
+ 
+ /*
+  * Test the hash algorithm represented by @req against the corresponding generic
+  * implementation, if one is available.
+  */
+ static int test_hash_vs_generic_impl(const char *generic_driver,
+ 				     unsigned int maxkeysize,
+ 				     struct ahash_request *req,
+ 				     struct shash_desc *desc,
+ 				     struct test_sglist *tsgl,
+ 				     u8 *hashstate)
+ {
+ 	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+ 	const unsigned int digestsize = crypto_ahash_digestsize(tfm);
+ 	const unsigned int blocksize = crypto_ahash_blocksize(tfm);
+ 	const unsigned int maxdatasize = (2 * PAGE_SIZE) - TESTMGR_POISON_LEN;
+ 	const char *algname = crypto_hash_alg_common(tfm)->base.cra_name;
+ 	const char *driver = crypto_ahash_driver_name(tfm);
+ 	char _generic_driver[CRYPTO_MAX_ALG_NAME];
+ 	struct crypto_shash *generic_tfm = NULL;
+ 	struct shash_desc *generic_desc = NULL;
+ 	unsigned int i;
+ 	struct hash_testvec vec = { 0 };
+ 	char vec_name[64];
+ 	struct testvec_config *cfg;
+ 	char cfgname[TESTVEC_CONFIG_NAMELEN];
+ 	int err;
+ 
+ 	if (noextratests)
+ 		return 0;
+ 
+ 	if (!generic_driver) { /* Use default naming convention? */
+ 		err = build_generic_driver_name(algname, _generic_driver);
+ 		if (err)
+ 			return err;
+ 		generic_driver = _generic_driver;
+ 	}
+ 
+ 	if (strcmp(generic_driver, driver) == 0) /* Already the generic impl? */
+ 		return 0;
+ 
+ 	generic_tfm = crypto_alloc_shash(generic_driver, 0, 0);
+ 	if (IS_ERR(generic_tfm)) {
+ 		err = PTR_ERR(generic_tfm);
+ 		if (err == -ENOENT) {
+ 			pr_warn("alg: hash: skipping comparison tests for %s because %s is unavailable\n",
+ 				driver, generic_driver);
+ 			return 0;
+ 		}
+ 		pr_err("alg: hash: error allocating %s (generic impl of %s): %d\n",
+ 		       generic_driver, algname, err);
+ 		return err;
+ 	}
+ 
+ 	cfg = kzalloc(sizeof(*cfg), GFP_KERNEL);
+ 	if (!cfg) {
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	generic_desc = kzalloc(sizeof(*desc) +
+ 			       crypto_shash_descsize(generic_tfm), GFP_KERNEL);
+ 	if (!generic_desc) {
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 	generic_desc->tfm = generic_tfm;
+ 
+ 	/* Check the algorithm properties for consistency. */
+ 
+ 	if (digestsize != crypto_shash_digestsize(generic_tfm)) {
+ 		pr_err("alg: hash: digestsize for %s (%u) doesn't match generic impl (%u)\n",
+ 		       driver, digestsize,
+ 		       crypto_shash_digestsize(generic_tfm));
+ 		err = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	if (blocksize != crypto_shash_blocksize(generic_tfm)) {
+ 		pr_err("alg: hash: blocksize for %s (%u) doesn't match generic impl (%u)\n",
+ 		       driver, blocksize, crypto_shash_blocksize(generic_tfm));
+ 		err = -EINVAL;
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * Now generate test vectors using the generic implementation, and test
+ 	 * the other implementation against them.
+ 	 */
+ 
+ 	vec.key = kmalloc(maxkeysize, GFP_KERNEL);
+ 	vec.plaintext = kmalloc(maxdatasize, GFP_KERNEL);
+ 	vec.digest = kmalloc(digestsize, GFP_KERNEL);
+ 	if (!vec.key || !vec.plaintext || !vec.digest) {
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	for (i = 0; i < fuzz_iterations * 8; i++) {
+ 		generate_random_hash_testvec(generic_desc, &vec,
+ 					     maxkeysize, maxdatasize,
+ 					     vec_name, sizeof(vec_name));
+ 		generate_random_testvec_config(cfg, cfgname, sizeof(cfgname));
+ 
+ 		err = test_hash_vec_cfg(&vec, vec_name, cfg,
+ 					req, desc, tsgl, hashstate);
+ 		if (err)
+ 			goto out;
+ 		cond_resched();
+ 	}
+ 	err = 0;
+ out:
+ 	kfree(cfg);
+ 	kfree(vec.key);
+ 	kfree(vec.plaintext);
+ 	kfree(vec.digest);
+ 	crypto_free_shash(generic_tfm);
+ 	kfree_sensitive(generic_desc);
+ 	return err;
+ }
+ #else /* !CONFIG_CRYPTO_MANAGER_EXTRA_TESTS */
+ static int test_hash_vs_generic_impl(const char *generic_driver,
+ 				     unsigned int maxkeysize,
+ 				     struct ahash_request *req,
+ 				     struct shash_desc *desc,
+ 				     struct test_sglist *tsgl,
+ 				     u8 *hashstate)
+ {
+ 	return 0;
+ }
+ #endif /* !CONFIG_CRYPTO_MANAGER_EXTRA_TESTS */
+ 
+ static int alloc_shash(const char *driver, u32 type, u32 mask,
+ 		       struct crypto_shash **tfm_ret,
+ 		       struct shash_desc **desc_ret)
+ {
+ 	struct crypto_shash *tfm;
+ 	struct shash_desc *desc;
+ 
+ 	tfm = crypto_alloc_shash(driver, type, mask);
+ 	if (IS_ERR(tfm)) {
+ 		if (PTR_ERR(tfm) == -ENOENT) {
+ 			/*
+ 			 * This algorithm is only available through the ahash
+ 			 * API, not the shash API, so skip the shash tests.
+ 			 */
+ 			return 0;
+ 		}
+ 		pr_err("alg: hash: failed to allocate shash transform for %s: %ld\n",
+ 		       driver, PTR_ERR(tfm));
+ 		return PTR_ERR(tfm);
+ 	}
+ 
+ 	desc = kmalloc(sizeof(*desc) + crypto_shash_descsize(tfm), GFP_KERNEL);
+ 	if (!desc) {
+ 		crypto_free_shash(tfm);
+ 		return -ENOMEM;
+ 	}
+ 	desc->tfm = tfm;
+ 
+ 	*tfm_ret = tfm;
+ 	*desc_ret = desc;
+ 	return 0;
+ }
+ 
+ static int __alg_test_hash(const struct hash_testvec *vecs,
+ 			   unsigned int num_vecs, const char *driver,
+ 			   u32 type, u32 mask,
+ 			   const char *generic_driver, unsigned int maxkeysize)
+ {
+ 	struct crypto_ahash *atfm = NULL;
+ 	struct ahash_request *req = NULL;
+ 	struct crypto_shash *stfm = NULL;
+ 	struct shash_desc *desc = NULL;
+ 	struct test_sglist *tsgl = NULL;
+ 	u8 *hashstate = NULL;
+ 	unsigned int statesize;
+ 	unsigned int i;
+ 	int err;
+ 
+ 	/*
+ 	 * Always test the ahash API.  This works regardless of whether the
+ 	 * algorithm is implemented as ahash or shash.
+ 	 */
+ 
+ 	atfm = crypto_alloc_ahash(driver, type, mask);
+ 	if (IS_ERR(atfm)) {
+ 		pr_err("alg: hash: failed to allocate transform for %s: %ld\n",
+ 		       driver, PTR_ERR(atfm));
+ 		return PTR_ERR(atfm);
+ 	}
+ 	driver = crypto_ahash_driver_name(atfm);
+ 
+ 	req = ahash_request_alloc(atfm, GFP_KERNEL);
+ 	if (!req) {
+ 		pr_err("alg: hash: failed to allocate request for %s\n",
+ 		       driver);
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	/*
+ 	 * If available also test the shash API, to cover corner cases that may
+ 	 * be missed by testing the ahash API only.
+ 	 */
+ 	err = alloc_shash(driver, type, mask, &stfm, &desc);
+ 	if (err)
+ 		goto out;
+ 
+ 	tsgl = kmalloc(sizeof(*tsgl), GFP_KERNEL);
+ 	if (!tsgl || init_test_sglist(tsgl) != 0) {
+ 		pr_err("alg: hash: failed to allocate test buffers for %s\n",
+ 		       driver);
+ 		kfree(tsgl);
+ 		tsgl = NULL;
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	statesize = crypto_ahash_statesize(atfm);
+ 	if (stfm)
+ 		statesize = max(statesize, crypto_shash_statesize(stfm));
+ 	hashstate = kmalloc(statesize + TESTMGR_POISON_LEN, GFP_KERNEL);
+ 	if (!hashstate) {
+ 		pr_err("alg: hash: failed to allocate hash state buffer for %s\n",
+ 		       driver);
+ 		err = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	for (i = 0; i < num_vecs; i++) {
+ 		if (fips_enabled && vecs[i].fips_skip)
+ 			continue;
+ 
+ 		err = test_hash_vec(&vecs[i], i, req, desc, tsgl, hashstate);
+ 		if (err)
+ 			goto out;
+ 		cond_resched();
+ 	}
+ 	err = test_hash_vs_generic_impl(generic_driver, maxkeysize, req,
+ 					desc, tsgl, hashstate);
+ out:
+ 	kfree(hashstate);
+ 	if (tsgl) {
+ 		destroy_test_sglist(tsgl);
+ 		kfree(tsgl);
+ 	}
+ 	kfree(desc);
+ 	crypto_free_shash(stfm);
+ 	ahash_request_free(req);
+ 	crypto_free_ahash(atfm);
+ 	return err;
+ }
+ 
+ static int alg_test_hash(const struct alg_test_desc *desc, const char *driver,
+ 			 u32 type, u32 mask)
+ {
+ 	const struct hash_testvec *template = desc->suite.hash.vecs;
+ 	unsigned int tcount = desc->suite.hash.count;
+ 	unsigned int nr_unkeyed, nr_keyed;
+ 	unsigned int maxkeysize = 0;
+ 	int err;
+ 
+ 	/*
+ 	 * For OPTIONAL_KEY algorithms, we have to do all the unkeyed tests
+ 	 * first, before setting a key on the tfm.  To make this easier, we
+ 	 * require that the unkeyed test vectors (if any) are listed first.
+ 	 */
+ 
+ 	for (nr_unkeyed = 0; nr_unkeyed < tcount; nr_unkeyed++) {
+ 		if (template[nr_unkeyed].ksize)
+ 			break;
+ 	}
+ 	for (nr_keyed = 0; nr_unkeyed + nr_keyed < tcount; nr_keyed++) {
+ 		if (!template[nr_unkeyed + nr_keyed].ksize) {
+ 			pr_err("alg: hash: test vectors for %s out of order, "
+ 			       "unkeyed ones must come first\n", desc->alg);
+ 			return -EINVAL;
+ 		}
+ 		maxkeysize = max_t(unsigned int, maxkeysize,
+ 				   template[nr_unkeyed + nr_keyed].ksize);
+ 	}
+ 
+ 	err = 0;
+ 	if (nr_unkeyed) {
+ 		err = __alg_test_hash(template, nr_unkeyed, driver, type, mask,
+ 				      desc->generic_driver, maxkeysize);
+ 		template += nr_unkeyed;
+ 	}
+ 
+ 	if (!err && nr_keyed)
+ 		err = __alg_test_hash(template, nr_keyed, driver, type, mask,
+ 				      desc->generic_driver, maxkeysize);
+ 
+ 	return err;
+ }
+ 
+ static int test_aead_vec_cfg(int enc, const struct aead_testvec *vec,
+ 			     const char *vec_name,
+ 			     const struct testvec_config *cfg,
+ 			     struct aead_request *req,
+ 			     struct cipher_test_sglists *tsgls)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	const unsigned int alignmask = crypto_aead_alignmask(tfm);
+ 	const unsigned int ivsize = crypto_aead_ivsize(tfm);
+ 	const unsigned int authsize = vec->clen - vec->plen;
+ 	const char *driver = crypto_aead_driver_name(tfm);
+ 	const u32 req_flags = CRYPTO_TFM_REQ_MAY_BACKLOG | cfg->req_flags;
+ 	const char *op = enc ? "encryption" : "decryption";
+ 	DECLARE_CRYPTO_WAIT(wait);
+ 	u8 _iv[3 * (MAX_ALGAPI_ALIGNMASK + 1) + MAX_IVLEN];
+ 	u8 *iv = PTR_ALIGN(&_iv[0], 2 * (MAX_ALGAPI_ALIGNMASK + 1)) +
+ 		 cfg->iv_offset +
+ 		 (cfg->iv_offset_relative_to_alignmask ? alignmask : 0);
+ 	struct kvec input[2];
+ 	int err;
+ 
+ 	/* Set the key */
+ 	if (vec->wk)
+ 		crypto_aead_set_flags(tfm, CRYPTO_TFM_REQ_FORBID_WEAK_KEYS);
+ 	else
+ 		crypto_aead_clear_flags(tfm, CRYPTO_TFM_REQ_FORBID_WEAK_KEYS);
+ 
+ 	err = do_setkey(crypto_aead_setkey, tfm, vec->key, vec->klen,
+ 			cfg, alignmask);
+ 	if (err && err != vec->setkey_error) {
+ 		pr_err("alg: aead: %s setkey failed on test vector %s; expected_error=%d, actual_error=%d, flags=%#x\n",
+ 		       driver, vec_name, vec->setkey_error, err,
+ 		       crypto_aead_get_flags(tfm));
+ 		return err;
+ 	}
+ 	if (!err && vec->setkey_error) {
+ 		pr_err("alg: aead: %s setkey unexpectedly succeeded on test vector %s; expected_error=%d\n",
+ 		       driver, vec_name, vec->setkey_error);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* Set the authentication tag size */
+ 	err = crypto_aead_setauthsize(tfm, authsize);
+ 	if (err && err != vec->setauthsize_error) {
+ 		pr_err("alg: aead: %s setauthsize failed on test vector %s; expected_error=%d, actual_error=%d\n",
+ 		       driver, vec_name, vec->setauthsize_error, err);
+ 		return err;
+ 	}
+ 	if (!err && vec->setauthsize_error) {
+ 		pr_err("alg: aead: %s setauthsize unexpectedly succeeded on test vector %s; expected_error=%d\n",
+ 		       driver, vec_name, vec->setauthsize_error);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (vec->setkey_error || vec->setauthsize_error)
+ 		return 0;
+ 
+ 	/* The IV must be copied to a buffer, as the algorithm may modify it */
+ 	if (WARN_ON(ivsize > MAX_IVLEN))
+ 		return -EINVAL;
+ 	if (vec->iv)
+ 		memcpy(iv, vec->iv, ivsize);
+ 	else
+ 		memset(iv, 0, ivsize);
+ 
+ 	/* Build the src/dst scatterlists */
+ 	input[0].iov_base = (void *)vec->assoc;
+ 	input[0].iov_len = vec->alen;
+ 	input[1].iov_base = enc ? (void *)vec->ptext : (void *)vec->ctext;
+ 	input[1].iov_len = enc ? vec->plen : vec->clen;
+ 	err = build_cipher_test_sglists(tsgls, cfg, alignmask,
+ 					vec->alen + (enc ? vec->plen :
+ 						     vec->clen),
+ 					vec->alen + (enc ? vec->clen :
+ 						     vec->plen),
+ 					input, 2);
+ 	if (err) {
+ 		pr_err("alg: aead: %s %s: error preparing scatterlists for test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, cfg->name);
+ 		return err;
+ 	}
+ 
+ 	/* Do the actual encryption or decryption */
+ 	testmgr_poison(req->__ctx, crypto_aead_reqsize(tfm));
+ 	aead_request_set_callback(req, req_flags, crypto_req_done, &wait);
+ 	aead_request_set_crypt(req, tsgls->src.sgl_ptr, tsgls->dst.sgl_ptr,
+ 			       enc ? vec->plen : vec->clen, iv);
+ 	aead_request_set_ad(req, vec->alen);
+ 	if (cfg->nosimd)
+ 		crypto_disable_simd_for_test();
+ 	err = enc ? crypto_aead_encrypt(req) : crypto_aead_decrypt(req);
+ 	if (cfg->nosimd)
+ 		crypto_reenable_simd_for_test();
+ 	err = crypto_wait_req(err, &wait);
+ 
+ 	/* Check that the algorithm didn't overwrite things it shouldn't have */
+ 	if (req->cryptlen != (enc ? vec->plen : vec->clen) ||
+ 	    req->assoclen != vec->alen ||
+ 	    req->iv != iv ||
+ 	    req->src != tsgls->src.sgl_ptr ||
+ 	    req->dst != tsgls->dst.sgl_ptr ||
+ 	    crypto_aead_reqtfm(req) != tfm ||
+ 	    req->base.complete != crypto_req_done ||
+ 	    req->base.flags != req_flags ||
+ 	    req->base.data != &wait) {
+ 		pr_err("alg: aead: %s %s corrupted request struct on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, cfg->name);
+ 		if (req->cryptlen != (enc ? vec->plen : vec->clen))
+ 			pr_err("alg: aead: changed 'req->cryptlen'\n");
+ 		if (req->assoclen != vec->alen)
+ 			pr_err("alg: aead: changed 'req->assoclen'\n");
+ 		if (req->iv != iv)
+ 			pr_err("alg: aead: changed 'req->iv'\n");
+ 		if (req->src != tsgls->src.sgl_ptr)
+ 			pr_err("alg: aead: changed 'req->src'\n");
+ 		if (req->dst != tsgls->dst.sgl_ptr)
+ 			pr_err("alg: aead: changed 'req->dst'\n");
+ 		if (crypto_aead_reqtfm(req) != tfm)
+ 			pr_err("alg: aead: changed 'req->base.tfm'\n");
+ 		if (req->base.complete != crypto_req_done)
+ 			pr_err("alg: aead: changed 'req->base.complete'\n");
+ 		if (req->base.flags != req_flags)
+ 			pr_err("alg: aead: changed 'req->base.flags'\n");
+ 		if (req->base.data != &wait)
+ 			pr_err("alg: aead: changed 'req->base.data'\n");
+ 		return -EINVAL;
+ 	}
+ 	if (is_test_sglist_corrupted(&tsgls->src)) {
+ 		pr_err("alg: aead: %s %s corrupted src sgl on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, cfg->name);
+ 		return -EINVAL;
+ 	}
+ 	if (tsgls->dst.sgl_ptr != tsgls->src.sgl &&
+ 	    is_test_sglist_corrupted(&tsgls->dst)) {
+ 		pr_err("alg: aead: %s %s corrupted dst sgl on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, cfg->name);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* Check for unexpected success or failure, or wrong error code */
+ 	if ((err == 0 && vec->novrfy) ||
+ 	    (err != vec->crypt_error && !(err == -EBADMSG && vec->novrfy))) {
+ 		char expected_error[32];
+ 
+ 		if (vec->novrfy &&
+ 		    vec->crypt_error != 0 && vec->crypt_error != -EBADMSG)
+ 			sprintf(expected_error, "-EBADMSG or %d",
+ 				vec->crypt_error);
+ 		else if (vec->novrfy)
+ 			sprintf(expected_error, "-EBADMSG");
+ 		else
+ 			sprintf(expected_error, "%d", vec->crypt_error);
+ 		if (err) {
+ 			pr_err("alg: aead: %s %s failed on test vector %s; expected_error=%s, actual_error=%d, cfg=\"%s\"\n",
+ 			       driver, op, vec_name, expected_error, err,
+ 			       cfg->name);
+ 			return err;
+ 		}
+ 		pr_err("alg: aead: %s %s unexpectedly succeeded on test vector %s; expected_error=%s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, expected_error, cfg->name);
+ 		return -EINVAL;
+ 	}
+ 	if (err) /* Expectedly failed. */
+ 		return 0;
+ 
+ 	/* Check for the correct output (ciphertext or plaintext) */
+ 	err = verify_correct_output(&tsgls->dst, enc ? vec->ctext : vec->ptext,
+ 				    enc ? vec->clen : vec->plen,
+ 				    vec->alen,
+ 				    enc || cfg->inplace_mode == OUT_OF_PLACE);
+ 	if (err == -EOVERFLOW) {
+ 		pr_err("alg: aead: %s %s overran dst buffer on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, cfg->name);
+ 		return err;
+ 	}
+ 	if (err) {
+ 		pr_err("alg: aead: %s %s test failed (wrong result) on test vector %s, cfg=\"%s\"\n",
+ 		       driver, op, vec_name, cfg->name);
+ 		return err;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int test_aead_vec(int enc, const struct aead_testvec *vec,
+ 			 unsigned int vec_num, struct aead_request *req,
+ 			 struct cipher_test_sglists *tsgls)
+ {
+ 	char vec_name[16];
+ 	unsigned int i;
+ 	int err;
+ 
+ 	if (enc && vec->novrfy)
+ 		return 0;
+ 
+ 	sprintf(vec_name, "%u", vec_num);
+ 
+ 	for (i = 0; i < ARRAY_SIZE(default_cipher_testvec_configs); i++) {
+ 		err = test_aead_vec_cfg(enc, vec, vec_name,
+ 					&default_cipher_testvec_configs[i],
+ 					req, tsgls);
+ 		if (err)
+ 			return err;
+ 	}
+ 
+ #ifdef CONFIG_CRYPTO_MANAGER_EXTRA_TESTS
+ 	if (!noextratests) {
+ 		struct testvec_config cfg;
+ 		char cfgname[TESTVEC_CONFIG_NAMELEN];
+ 
+ 		for (i = 0; i < fuzz_iterations; i++) {
+ 			generate_random_testvec_config(&cfg, cfgname,
+ 						       sizeof(cfgname));
+ 			err = test_aead_vec_cfg(enc, vec, vec_name,
+ 						&cfg, req, tsgls);
+ 			if (err)
+ 				return err;
+ 			cond_resched();
+ 		}
+ 	}
+ #endif
+ 	return 0;
+ }
+ 
+ #ifdef CONFIG_CRYPTO_MANAGER_EXTRA_TESTS
+ 
+ struct aead_extra_tests_ctx {
+ 	struct aead_request *req;
+ 	struct crypto_aead *tfm;
+ 	const struct alg_test_desc *test_desc;
+ 	struct cipher_test_sglists *tsgls;
+ 	unsigned int maxdatasize;
+ 	unsigned int maxkeysize;
+ 
+ 	struct aead_testvec vec;
+ 	char vec_name[64];
+ 	char cfgname[TESTVEC_CONFIG_NAMELEN];
+ 	struct testvec_config cfg;
+ };
+ 
+ /*
+  * Make at least one random change to a (ciphertext, AAD) pair.  "Ciphertext"
+  * here means the full ciphertext including the authentication tag.  The
+  * authentication tag (and hence also the ciphertext) is assumed to be nonempty.
+  */
+ static void mutate_aead_message(struct aead_testvec *vec, bool aad_iv,
+ 				unsigned int ivsize)
+ {
+ 	const unsigned int aad_tail_size = aad_iv ? ivsize : 0;
+ 	const unsigned int authsize = vec->clen - vec->plen;
+ 
+ 	if (get_random_u32_below(2) == 0 && vec->alen > aad_tail_size) {
+ 		 /* Mutate the AAD */
+ 		flip_random_bit((u8 *)vec->assoc, vec->alen - aad_tail_size);
+ 		if (get_random_u32_below(2) == 0)
+ 			return;
+ 	}
+ 	if (get_random_u32_below(2) == 0) {
+ 		/* Mutate auth tag (assuming it's at the end of ciphertext) */
+ 		flip_random_bit((u8 *)vec->ctext + vec->plen, authsize);
+ 	} else {
+ 		/* Mutate any part of the ciphertext */
+ 		flip_random_bit((u8 *)vec->ctext, vec->clen);
+ 	}
+ }
+ 
+ /*
+  * Minimum authentication tag size in bytes at which we assume that we can
+  * reliably generate inauthentic messages, i.e. not generate an authentic
+  * message by chance.
+  */
+ #define MIN_COLLISION_FREE_AUTHSIZE 8
+ 
+ static void generate_aead_message(struct aead_request *req,
+ 				  const struct aead_test_suite *suite,
+ 				  struct aead_testvec *vec,
+ 				  bool prefer_inauthentic)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	const unsigned int ivsize = crypto_aead_ivsize(tfm);
+ 	const unsigned int authsize = vec->clen - vec->plen;
+ 	const bool inauthentic = (authsize >= MIN_COLLISION_FREE_AUTHSIZE) &&
+ 				 (prefer_inauthentic || get_random_u32_below(4) == 0);
+ 
+ 	/* Generate the AAD. */
+ 	generate_random_bytes((u8 *)vec->assoc, vec->alen);
+ 	if (suite->aad_iv && vec->alen >= ivsize)
+ 		/* Avoid implementation-defined behavior. */
+ 		memcpy((u8 *)vec->assoc + vec->alen - ivsize, vec->iv, ivsize);
+ 
+ 	if (inauthentic && get_random_u32_below(2) == 0) {
+ 		/* Generate a random ciphertext. */
+ 		generate_random_bytes((u8 *)vec->ctext, vec->clen);
+ 	} else {
+ 		int i = 0;
+ 		struct scatterlist src[2], dst;
+ 		u8 iv[MAX_IVLEN];
+ 		DECLARE_CRYPTO_WAIT(wait);
+ 
+ 		/* Generate a random plaintext and encrypt it. */
+ 		sg_init_table(src, 2);
+ 		if (vec->alen)
+ 			sg_set_buf(&src[i++], vec->assoc, vec->alen);
+ 		if (vec->plen) {
+ 			generate_random_bytes((u8 *)vec->ptext, vec->plen);
+ 			sg_set_buf(&src[i++], vec->ptext, vec->plen);
+ 		}
+ 		sg_init_one(&dst, vec->ctext, vec->alen + vec->clen);
+ 		memcpy(iv, vec->iv, ivsize);
+ 		aead_request_set_callback(req, 0, crypto_req_done, &wait);
+ 		aead_request_set_crypt(req, src, &dst, vec->plen, iv);
+ 		aead_request_set_ad(req, vec->alen);
+ 		vec->crypt_error = crypto_wait_req(crypto_aead_encrypt(req),
+ 						   &wait);
+ 		/* If encryption failed, we're done. */
+ 		if (vec->crypt_error != 0)
+ 			return;
+ 		memmove((u8 *)vec->ctext, vec->ctext + vec->alen, vec->clen);
+ 		if (!inauthentic)
+ 			return;
+ 		/*
+ 		 * Mutate the authentic (ciphertext, AAD) pair to get an
+ 		 * inauthentic one.
+ 		 */
+ 		mutate_aead_message(vec, suite->aad_iv, ivsize);
+ 	}
+ 	vec->novrfy = 1;
+ 	if (suite->einval_allowed)
+ 		vec->crypt_error = -EINVAL;
+ }
+ 
+ /*
+  * Generate an AEAD test vector 'vec' using the implementation specified by
+  * 'req'.  The buffers in 'vec' must already be allocated.
+  *
+  * If 'prefer_inauthentic' is true, then this function will generate inauthentic
+  * test vectors (i.e. vectors with 'vec->novrfy=1') more often.
+  */
+ static void generate_random_aead_testvec(struct aead_request *req,
+ 					 struct aead_testvec *vec,
+ 					 const struct aead_test_suite *suite,
+ 					 unsigned int maxkeysize,
+ 					 unsigned int maxdatasize,
+ 					 char *name, size_t max_namelen,
+ 					 bool prefer_inauthentic)
+ {
+ 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+ 	const unsigned int ivsize = crypto_aead_ivsize(tfm);
+ 	const unsigned int maxauthsize = crypto_aead_maxauthsize(tfm);
+ 	unsigned int authsize;
+ 	unsigned int total_len;
+ 
+ 	/* Key: length in [0, maxkeysize], but usually choose maxkeysize */
+ 	vec->klen = maxkeysize;
+ 	if (get_random_u32_below(4) == 0)
+ 		vec->klen = get_random_u32_below(maxkeysize + 1);
+ 	generate_random_bytes((u8 *)vec->key, vec->klen);
+ 	vec->setkey_error = crypto_aead_setkey(tfm, vec->key, vec->klen);
+ 
+ 	/* IV */
+ 	generate_random_bytes((u8 *)vec->iv, ivsize);
+ 
+ 	/* Tag length: in [0, maxauthsize], but usually choose maxauthsize */
+ 	authsize = maxauthsize;
+ 	if (get_random_u32_below(4) == 0)
+ 		authsize = get_random_u32_below(maxauthsize + 1);
+ 	if (prefer_inauthentic && authsize < MIN_COLLISION_FREE_AUTHSIZE)
+ 		authsize = MIN_COLLISION_FREE_AUTHSIZE;
+ 	if (WARN_ON(authsize > maxdatasize))
+ 		authsize = maxdatasize;
+ 	maxdatasize -= authsize;
+ 	vec->setauthsize_error = crypto_aead_setauthsize(tfm, authsize);
+ 
+ 	/* AAD, plaintext, and ciphertext lengths */
+ 	total_len = generate_random_length(maxdatasize);
+ 	if (get_random_u32_below(4) == 0)
+ 		vec->alen = 0;
+ 	else
+ 		vec->alen = generate_random_length(total_len);
+ 	vec->plen = total_len - vec->alen;
+ 	vec->clen = vec->plen + authsize;
+ 
+ 	/*
+ 	 * Generate the AAD, plaintext, and ciphertext.  Not applicable if the
+ 	 * key or the authentication tag size couldn't be set.
+ 	 */
+ 	vec->novrfy = 0;
+ 	vec->crypt_error = 0;
+ 	if (vec->setkey_error == 0 && vec->setauthsize_error == 0)
+ 		generate_aead_message(req, suite, vec, prefer_inauthentic);
+ 	snprintf(name, max_namelen,
+ 		 "\"random: alen=%u plen=%u authsize=%u klen=%u novrfy=%d\"",
+ 		 vec->alen, vec->plen, authsize, vec->klen, vec->novrfy);
+ }
+ 
+ static void try_to_generate_inauthentic_testvec(
+ 					struct aead_extra_tests_ctx *ctx)
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  {
  	int i;
  
diff --cc drivers/bus/mhi/host/internal.h
index 01fd10a399b6,2e139e76de4c..000000000000
--- a/drivers/bus/mhi/host/internal.h
+++ b/drivers/bus/mhi/host/internal.h
@@@ -129,7 -129,7 +129,11 @@@ enum mhi_pm_state 
  #define PRIMARY_CMD_RING				0
  #define MHI_DEV_WAKE_DB					127
  #define MHI_MAX_MTU					0xffff
++<<<<<<< HEAD
 +#define MHI_RANDOM_U32_NONZERO(bmsk)			(prandom_u32_max(bmsk) + 1)
++=======
+ #define MHI_RANDOM_U32_NONZERO(bmsk)			(get_random_u32_inclusive(1, bmsk))
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  enum mhi_er_type {
  	MHI_ER_TYPE_INVALID = 0x0,
diff --cc drivers/dma-buf/st-dma-fence-chain.c
index 8ce1ea59d31b,c0979c8049b5..000000000000
--- a/drivers/dma-buf/st-dma-fence-chain.c
+++ b/drivers/dma-buf/st-dma-fence-chain.c
@@@ -396,7 -400,7 +396,11 @@@ static int __find_race(void *arg
  		struct dma_fence *fence = dma_fence_get(data->fc.tail);
  		int seqno;
  
++<<<<<<< HEAD
 +		seqno = prandom_u32_max(data->fc.chain_length) + 1;
++=======
+ 		seqno = get_random_u32_inclusive(1, data->fc.chain_length);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  		err = dma_fence_chain_find_seqno(&fence, seqno);
  		if (err) {
diff --cc drivers/infiniband/core/cma.c
index 008eec1bd530,aacd6254df77..000000000000
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@@ -3703,7 -3807,7 +3703,11 @@@ static int cma_alloc_any_port(enum rdma
  
  	inet_get_local_port_range(net, &low, &high);
  	remaining = (high - low) + 1;
++<<<<<<< HEAD
 +	rover = prandom_u32_max(remaining) + low;
++=======
+ 	rover = get_random_u32_inclusive(low, remaining + low - 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  retry:
  	if (last_used_port != rover) {
  		struct rdma_bind_list *bind_list;
diff --cc drivers/infiniband/hw/hns/hns_roce_ah.c
index b222dce832f0,e77fcc74f15c..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_ah.c
+++ b/drivers/infiniband/hw/hns/hns_roce_ah.c
@@@ -35,9 -35,19 +35,25 @@@
  #include <rdma/ib_cache.h>
  #include "hns_roce_device.h"
  
++<<<<<<< HEAD
 +#define HNS_ROCE_PORT_NUM_SHIFT		24
 +#define HNS_ROCE_VLAN_SL_BIT_MASK	7
 +#define HNS_ROCE_VLAN_SL_SHIFT		13
++=======
+ static inline u16 get_ah_udp_sport(const struct rdma_ah_attr *ah_attr)
+ {
+ 	u32 fl = ah_attr->grh.flow_label;
+ 	u16 sport;
+ 
+ 	if (!fl)
+ 		sport = get_random_u32_inclusive(IB_ROCE_UDP_ENCAP_VALID_PORT_MIN,
+ 						 IB_ROCE_UDP_ENCAP_VALID_PORT_MAX);
+ 	else
+ 		sport = rdma_flow_label_to_udp_sport(fl);
+ 
+ 	return sport;
+ }
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  int hns_roce_create_ah(struct ib_ah *ibah, struct rdma_ah_init_attr *init_attr,
  		       struct ib_udata *udata)
diff --cc drivers/mtd/nand/raw/nandsim.c
index f8edacde49ab,c21abf748948..000000000000
--- a/drivers/mtd/nand/raw/nandsim.c
+++ b/drivers/mtd/nand/raw/nandsim.c
@@@ -1411,14 -1400,14 +1411,18 @@@ static int do_read_error(struct nandsi
  	return 0;
  }
  
 -static void ns_do_bit_flips(struct nandsim *ns, int num)
 +static void do_bit_flips(struct nandsim *ns, int num)
  {
 -	if (bitflips && get_random_u16() < (1 << 6)) {
 +	if (bitflips && prandom_u32() < (1 << 22)) {
  		int flips = 1;
  		if (bitflips > 1)
++<<<<<<< HEAD
 +			flips = (prandom_u32() % (int) bitflips) + 1;
++=======
+ 			flips = get_random_u32_inclusive(1, bitflips);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		while (flips--) {
 -			int pos = get_random_u32_below(num * 8);
 +			int pos = prandom_u32() % (num * 8);
  			ns->buf.byte[pos / 8] ^= (1 << (pos % 8));
  			NS_WARN("read_page: flipping bit %d in page %d "
  				"reading from %d ecc: corrected=%u failed=%u\n",
diff --cc drivers/net/wireless/broadcom/brcm80211/brcmfmac/p2p.c
index da11e2f655dc,c704ca752138..000000000000
--- a/drivers/net/wireless/broadcom/brcm80211/brcmfmac/p2p.c
+++ b/drivers/net/wireless/broadcom/brcm80211/brcmfmac/p2p.c
@@@ -1128,7 -1128,7 +1128,11 @@@ static void brcmf_p2p_afx_handler(struc
  	if (afx_hdl->is_listen && afx_hdl->my_listen_chan)
  		/* 100ms ~ 300ms */
  		err = brcmf_p2p_discover_listen(p2p, afx_hdl->my_listen_chan,
++<<<<<<< HEAD
 +						100 * (1 + prandom_u32_max(3)));
++=======
+ 						100 * get_random_u32_inclusive(1, 3));
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  	else
  		err = brcmf_p2p_act_frm_search(p2p, afx_hdl->peer_listen_chan);
  
diff --cc drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
index 83abfe996138,3a7a44bb3c60..000000000000
--- a/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
+++ b/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
@@@ -1143,7 -1099,7 +1143,11 @@@ static void iwl_mvm_mac_ctxt_cmd_fill_a
  			iwl_mvm_mac_ap_iterator, &data);
  
  		if (data.beacon_device_ts) {
++<<<<<<< HEAD
 +			u32 rand = prandom_u32_max(64 - 36) + 36;
++=======
+ 			u32 rand = get_random_u32_inclusive(36, 63);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  			mvmvif->ap_beacon_time = data.beacon_device_ts +
  				ieee80211_tu_to_usec(data.beacon_int * rand /
  						     100);
diff --cc fs/f2fs/segment.c
index 9efce174c51a,b304692c0cf5..000000000000
--- a/fs/f2fs/segment.c
+++ b/fs/f2fs/segment.c
@@@ -2192,12 -2586,15 +2192,18 @@@ static void new_curseg(struct f2fs_sb_i
  	curseg->next_segno = segno;
  	reset_curseg(sbi, type, 1);
  	curseg->alloc_type = LFS;
++<<<<<<< HEAD
++=======
+ 	if (F2FS_OPTION(sbi).fs_mode == FS_MODE_FRAGMENT_BLK)
+ 		curseg->fragment_remained_chunk =
+ 				get_random_u32_inclusive(1, sbi->max_fragment_chunk);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  }
  
 -static int __next_free_blkoff(struct f2fs_sb_info *sbi,
 -					int segno, block_t start)
 +static void __next_free_blkoff(struct f2fs_sb_info *sbi,
 +			struct curseg_info *seg, block_t start)
  {
 -	struct seg_entry *se = get_seg_entry(sbi, segno);
 +	struct seg_entry *se = get_seg_entry(sbi, seg->segno);
  	int entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);
  	unsigned long *target_map = SIT_I(sbi)->tmp_map;
  	unsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;
@@@ -2220,10 -2615,27 +2226,28 @@@
  static void __refresh_next_blkoff(struct f2fs_sb_info *sbi,
  				struct curseg_info *seg)
  {
 -	if (seg->alloc_type == SSR) {
 -		seg->next_blkoff =
 -			__next_free_blkoff(sbi, seg->segno,
 -						seg->next_blkoff + 1);
 -	} else {
 +	if (seg->alloc_type == SSR)
 +		__next_free_blkoff(sbi, seg, seg->next_blkoff + 1);
 +	else
  		seg->next_blkoff++;
++<<<<<<< HEAD
++=======
+ 		if (F2FS_OPTION(sbi).fs_mode == FS_MODE_FRAGMENT_BLK) {
+ 			/* To allocate block chunks in different sizes, use random number */
+ 			if (--seg->fragment_remained_chunk <= 0) {
+ 				seg->fragment_remained_chunk =
+ 				   get_random_u32_inclusive(1, sbi->max_fragment_chunk);
+ 				seg->next_blkoff +=
+ 				   get_random_u32_inclusive(1, sbi->max_fragment_hole);
+ 			}
+ 		}
+ 	}
+ }
+ 
+ bool f2fs_segment_has_free_slot(struct f2fs_sb_info *sbi, int segno)
+ {
+ 	return __next_free_blkoff(sbi, segno, 0) < sbi->blocks_per_seg;
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  }
  
  /*
diff --cc lib/test_hexdump.c
index 626f580b4ff7,b916801f23a8..000000000000
--- a/lib/test_hexdump.c
+++ b/lib/test_hexdump.c
@@@ -149,7 -149,7 +149,11 @@@ static void __init test_hexdump(size_t 
  static void __init test_hexdump_set(int rowsize, bool ascii)
  {
  	size_t d = min_t(size_t, sizeof(data_b), rowsize);
++<<<<<<< HEAD
 +	size_t len = get_random_int() % d + 1;
++=======
+ 	size_t len = get_random_u32_inclusive(1, d);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  	test_hexdump(len, rowsize, 4, ascii);
  	test_hexdump(len, rowsize, 2, ascii);
@@@ -208,7 -208,7 +212,11 @@@ static void __init test_hexdump_overflo
  static void __init test_hexdump_overflow_set(size_t buflen, bool ascii)
  {
  	unsigned int i = 0;
++<<<<<<< HEAD
 +	int rs = (get_random_int() % 2 + 1) * 16;
++=======
+ 	int rs = get_random_u32_inclusive(1, 2) * 16;
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  	do {
  		int gs = 1 << i;
@@@ -223,11 -223,11 +231,19 @@@ static int __init test_hexdump_init(voi
  	unsigned int i;
  	int rowsize;
  
++<<<<<<< HEAD
 +	rowsize = (get_random_int() % 2 + 1) * 16;
 +	for (i = 0; i < 16; i++)
 +		test_hexdump_set(rowsize, false);
 +
 +	rowsize = (get_random_int() % 2 + 1) * 16;
++=======
+ 	rowsize = get_random_u32_inclusive(1, 2) * 16;
+ 	for (i = 0; i < 16; i++)
+ 		test_hexdump_set(rowsize, false);
+ 
+ 	rowsize = get_random_u32_inclusive(1, 2) * 16;
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  	for (i = 0; i < 16; i++)
  		test_hexdump_set(rowsize, true);
  
diff --cc lib/test_kasan.c
index 24f275434e30,54181eba3e24..000000000000
--- a/lib/test_kasan.c
+++ b/lib/test_kasan.c
@@@ -1036,7 -1299,7 +1036,11 @@@ static void match_all_not_assigned(stru
  	KASAN_TEST_NEEDS_CONFIG_OFF(test, CONFIG_KASAN_GENERIC);
  
  	for (i = 0; i < 256; i++) {
++<<<<<<< HEAD:lib/test_kasan.c
 +		size = (get_random_int() % 1024) + 1;
++=======
+ 		size = get_random_u32_inclusive(1, 1024);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible):mm/kasan/kasan_test.c
  		ptr = kmalloc(size, GFP_KERNEL);
  		KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
  		KUNIT_EXPECT_GE(test, (u8)get_tag(ptr), (u8)KASAN_TAG_MIN);
@@@ -1045,7 -1308,7 +1049,11 @@@
  	}
  
  	for (i = 0; i < 256; i++) {
++<<<<<<< HEAD:lib/test_kasan.c
 +		order = (get_random_int() % 4) + 1;
++=======
+ 		order = get_random_u32_inclusive(1, 4);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible):mm/kasan/kasan_test.c
  		pages = alloc_pages(GFP_KERNEL, order);
  		ptr = page_address(pages);
  		KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
@@@ -1053,6 -1316,18 +1061,21 @@@
  		KUNIT_EXPECT_LT(test, (u8)get_tag(ptr), (u8)KASAN_TAG_KERNEL);
  		free_pages((unsigned long)ptr, order);
  	}
++<<<<<<< HEAD:lib/test_kasan.c
++=======
+ 
+ 	if (!IS_ENABLED(CONFIG_KASAN_VMALLOC))
+ 		return;
+ 
+ 	for (i = 0; i < 256; i++) {
+ 		size = get_random_u32_inclusive(1, 1024);
+ 		ptr = vmalloc(size);
+ 		KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ptr);
+ 		KUNIT_EXPECT_GE(test, (u8)get_tag(ptr), (u8)KASAN_TAG_MIN);
+ 		KUNIT_EXPECT_LT(test, (u8)get_tag(ptr), (u8)KASAN_TAG_KERNEL);
+ 		vfree(ptr);
+ 	}
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible):mm/kasan/kasan_test.c
  }
  
  /* Check that 0xff works as a match-all pointer tag for tag-based modes. */
diff --cc lib/test_printf.c
index 02332d9fd5db,f098914a48d5..000000000000
--- a/lib/test_printf.c
+++ b/lib/test_printf.c
@@@ -115,7 -126,7 +115,11 @@@ __test(const char *expect, int elen, co
  	 * be able to print it as expected.
  	 */
  	failed_tests += do_test(BUF_SIZE, expect, elen, fmt, ap);
++<<<<<<< HEAD
 +	rand = 1 + prandom_u32_max(elen+1);
++=======
+ 	rand = get_random_u32_inclusive(1, elen + 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  	/* Since elen < BUF_SIZE, we have 1 <= rand <= BUF_SIZE. */
  	failed_tests += do_test(rand, expect, elen, fmt, ap);
  	failed_tests += do_test(0, expect, elen, fmt, ap);
diff --cc lib/test_vmalloc.c
index f832b095afba,f90d2c27675b..000000000000
--- a/lib/test_vmalloc.c
+++ b/lib/test_vmalloc.c
@@@ -162,9 -151,7 +162,13 @@@ static int random_size_alloc_test(void
  	int i;
  
  	for (i = 0; i < test_loop_count; i++) {
++<<<<<<< HEAD
 +		get_random_bytes(&n, sizeof(i));
 +		n = (n % 100) + 1;
 +
++=======
+ 		n = get_random_u32_inclusive(1, 100);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		p = vmalloc(n * PAGE_SIZE);
  
  		if (!p)
@@@ -304,16 -291,12 +308,24 @@@ pcpu_alloc_test(void
  		return -1;
  
  	for (i = 0; i < 35000; i++) {
++<<<<<<< HEAD
 +		unsigned int r;
 +
 +		get_random_bytes(&r, sizeof(i));
 +		size = (r % (PAGE_SIZE / 4)) + 1;
++=======
+ 		size = get_random_u32_inclusive(1, PAGE_SIZE / 4);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  		/*
  		 * Maximum PAGE_SIZE
  		 */
++<<<<<<< HEAD
 +		get_random_bytes(&r, sizeof(i));
 +		align = 1 << ((i % 11) + 1);
++=======
+ 		align = 1 << get_random_u32_inclusive(1, 11);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  		pcpu[i] = __alloc_percpu(size, align);
  		if (!pcpu[i])
diff --cc mm/swapfile.c
index e7cd9c05c763,4ee31056d3f8..000000000000
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@@ -714,8 -772,7 +714,12 @@@ static void set_cluster_next(struct swa
  		/* No free swap slots available */
  		if (si->highest_bit <= si->lowest_bit)
  			return;
++<<<<<<< HEAD
 +		next = si->lowest_bit +
 +			prandom_u32_max(si->highest_bit - si->lowest_bit + 1);
++=======
+ 		next = get_random_u32_inclusive(si->lowest_bit, si->highest_bit);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		next = ALIGN_DOWN(next, SWAP_ADDRESS_SPACE_PAGES);
  		next = max_t(unsigned int, next, si->lowest_bit);
  	}
@@@ -3308,7 -3088,7 +3312,11 @@@ SYSCALL_DEFINE2(swapon, const char __us
  		 */
  		for_each_possible_cpu(cpu) {
  			per_cpu(*p->cluster_next_cpu, cpu) =
++<<<<<<< HEAD
 +				1 + prandom_u32_max(p->highest_bit);
++=======
+ 				get_random_u32_inclusive(1, p->highest_bit);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		}
  		nr_cluster = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);
  
diff --cc net/bluetooth/mgmt.c
index 1bfb5e35836f,81ce668b0b77..000000000000
--- a/net/bluetooth/mgmt.c
+++ b/net/bluetooth/mgmt.c
@@@ -6215,9 -7373,8 +6215,14 @@@ static int get_conn_info(struct sock *s
  	/* To avoid client trying to guess when to poll again for information we
  	 * calculate conn info age as random value between min/max set in hdev.
  	 */
++<<<<<<< HEAD
 +	conn_info_age = hdev->conn_info_min_age +
 +			prandom_u32_max(hdev->conn_info_max_age -
 +					hdev->conn_info_min_age);
++=======
+ 	conn_info_age = get_random_u32_inclusive(hdev->conn_info_min_age,
+ 						 hdev->conn_info_max_age - 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  	/* Query controller to refresh cached values if they are too old or were
  	 * never read.
diff --cc net/core/pktgen.c
index e791962b909f,760238196db1..000000000000
--- a/net/core/pktgen.c
+++ b/net/core/pktgen.c
@@@ -2281,10 -2380,8 +2281,15 @@@ static void set_cur_queue_map(struct pk
  	else if (pkt_dev->queue_map_min <= pkt_dev->queue_map_max) {
  		__u16 t;
  		if (pkt_dev->flags & F_QUEUE_MAP_RND) {
++<<<<<<< HEAD
 +			t = prandom_u32() %
 +				(pkt_dev->queue_map_max -
 +				 pkt_dev->queue_map_min + 1)
 +				+ pkt_dev->queue_map_min;
++=======
+ 			t = get_random_u32_inclusive(pkt_dev->queue_map_min,
+ 						     pkt_dev->queue_map_max);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		} else {
  			t = pkt_dev->cur_queue_map + 1;
  			if (t > pkt_dev->queue_map_max)
@@@ -2380,9 -2477,8 +2385,14 @@@ static void mod_cur_headers(struct pktg
  
  	if (pkt_dev->udp_src_min < pkt_dev->udp_src_max) {
  		if (pkt_dev->flags & F_UDPSRC_RND)
++<<<<<<< HEAD
 +			pkt_dev->cur_udp_src = prandom_u32() %
 +				(pkt_dev->udp_src_max - pkt_dev->udp_src_min)
 +				+ pkt_dev->udp_src_min;
++=======
+ 			pkt_dev->cur_udp_src = get_random_u32_inclusive(pkt_dev->udp_src_min,
+ 									pkt_dev->udp_src_max - 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  
  		else {
  			pkt_dev->cur_udp_src++;
@@@ -2393,9 -2489,8 +2403,14 @@@
  
  	if (pkt_dev->udp_dst_min < pkt_dev->udp_dst_max) {
  		if (pkt_dev->flags & F_UDPDST_RND) {
++<<<<<<< HEAD
 +			pkt_dev->cur_udp_dst = prandom_u32() %
 +				(pkt_dev->udp_dst_max - pkt_dev->udp_dst_min)
 +				+ pkt_dev->udp_dst_min;
++=======
+ 			pkt_dev->cur_udp_dst = get_random_u32_inclusive(pkt_dev->udp_dst_min,
+ 									pkt_dev->udp_dst_max - 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		} else {
  			pkt_dev->cur_udp_dst++;
  			if (pkt_dev->cur_udp_dst >= pkt_dev->udp_dst_max)
@@@ -2410,7 -2505,7 +2425,11 @@@
  		if (imn < imx) {
  			__u32 t;
  			if (pkt_dev->flags & F_IPSRC_RND)
++<<<<<<< HEAD
 +				t = prandom_u32() % (imx - imn) + imn;
++=======
+ 				t = get_random_u32_inclusive(imn, imx - 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  			else {
  				t = ntohl(pkt_dev->cur_saddr);
  				t++;
@@@ -2432,8 -2527,7 +2451,12 @@@
  				if (pkt_dev->flags & F_IPDST_RND) {
  
  					do {
++<<<<<<< HEAD
 +						t = prandom_u32() %
 +							(imx - imn) + imn;
++=======
+ 						t = get_random_u32_inclusive(imn, imx - 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  						s = htonl(t);
  					} while (ipv4_is_loopback(s) ||
  						ipv4_is_multicast(s) ||
@@@ -2480,9 -2574,8 +2503,14 @@@
  	if (pkt_dev->min_pkt_size < pkt_dev->max_pkt_size) {
  		__u32 t;
  		if (pkt_dev->flags & F_TXSIZE_RND) {
++<<<<<<< HEAD
 +			t = prandom_u32() %
 +				(pkt_dev->max_pkt_size - pkt_dev->min_pkt_size)
 +				+ pkt_dev->min_pkt_size;
++=======
+ 			t = get_random_u32_inclusive(pkt_dev->min_pkt_size,
+ 						     pkt_dev->max_pkt_size - 1);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		} else {
  			t = pkt_dev->cur_pkt_size + 1;
  			if (t > pkt_dev->max_pkt_size)
diff --cc net/ipv4/tcp_input.c
index 09c185b3d9d3,23cf418efe4f..000000000000
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@@ -3549,18 -3636,23 +3549,24 @@@ static void tcp_send_challenge_ack(stru
  				   &tp->last_oow_ack_time))
  		return;
  
 -	ack_limit = READ_ONCE(net->ipv4.sysctl_tcp_challenge_ack_limit);
 -	if (ack_limit == INT_MAX)
 -		goto send_ack;
 -
  	/* Then check host-wide RFC 5961 rate limit. */
  	now = jiffies / HZ;
 -	if (now != READ_ONCE(net->ipv4.tcp_challenge_timestamp)) {
 +	if (now != challenge_timestamp) {
 +		u32 ack_limit = net->ipv4.sysctl_tcp_challenge_ack_limit;
  		u32 half = (ack_limit + 1) >> 1;
  
++<<<<<<< HEAD
 +		challenge_timestamp = now;
 +		WRITE_ONCE(challenge_count, half + prandom_u32_max(ack_limit));
++=======
+ 		WRITE_ONCE(net->ipv4.tcp_challenge_timestamp, now);
+ 		WRITE_ONCE(net->ipv4.tcp_challenge_count,
+ 			   get_random_u32_inclusive(half, ack_limit + half - 1));
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  	}
 -	count = READ_ONCE(net->ipv4.tcp_challenge_count);
 +	count = READ_ONCE(challenge_count);
  	if (count > 0) {
 -		WRITE_ONCE(net->ipv4.tcp_challenge_count, count - 1);
 -send_ack:
 +		WRITE_ONCE(challenge_count, count - 1);
  		NET_INC_STATS(net, LINUX_MIB_TCPCHALLENGEACK);
  		tcp_send_ack(sk);
  	}
diff --cc net/ipv6/addrconf.c
index f7293f68fada,d720f6f5de3f..000000000000
--- a/net/ipv6/addrconf.c
+++ b/net/ipv6/addrconf.c
@@@ -107,7 -104,7 +107,11 @@@ static inline u32 cstamp_delta(unsigne
  static inline s32 rfc3315_s14_backoff_init(s32 irt)
  {
  	/* multiply 'initial retransmission time' by 0.9 .. 1.1 */
++<<<<<<< HEAD
 +	u64 tmp = (900000 + prandom_u32() % 200001) * (u64)irt;
++=======
+ 	u64 tmp = get_random_u32_inclusive(900000, 1100000) * (u64)irt;
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  	do_div(tmp, 1000000);
  	return (s32)tmp;
  }
@@@ -115,11 -112,11 +119,19 @@@
  static inline s32 rfc3315_s14_backoff_update(s32 rt, s32 mrt)
  {
  	/* multiply 'retransmission timeout' by 1.9 .. 2.1 */
++<<<<<<< HEAD
 +	u64 tmp = (1900000 + prandom_u32() % 200001) * (u64)rt;
 +	do_div(tmp, 1000000);
 +	if ((s32)tmp > mrt) {
 +		/* multiply 'maximum retransmission time' by 0.9 .. 1.1 */
 +		tmp = (900000 + prandom_u32() % 200001) * (u64)mrt;
++=======
+ 	u64 tmp = get_random_u32_inclusive(1900000, 2100000) * (u64)rt;
+ 	do_div(tmp, 1000000);
+ 	if ((s32)tmp > mrt) {
+ 		/* multiply 'maximum retransmission time' by 0.9 .. 1.1 */
+ 		tmp = get_random_u32_inclusive(900000, 1100000) * (u64)mrt;
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  		do_div(tmp, 1000000);
  	}
  	return (s32)tmp;
diff --cc net/xfrm/xfrm_state.c
index e57288275a83,d63a3644ee1a..000000000000
--- a/net/xfrm/xfrm_state.c
+++ b/net/xfrm/xfrm_state.c
@@@ -1854,7 -2072,7 +1854,11 @@@ int xfrm_alloc_spi(struct xfrm_state *x
  	} else {
  		u32 spi = 0;
  		for (h = 0; h < high-low+1; h++) {
++<<<<<<< HEAD
 +			spi = low + prandom_u32()%(high-low+1);
++=======
+ 			spi = get_random_u32_inclusive(low, high);
++>>>>>>> e8a533cbeb79 (treewide: use get_random_u32_inclusive() when possible)
  			x0 = xfrm_state_lookup(net, mark, &x->id.daddr, htonl(spi), x->id.proto, x->props.family);
  			if (x0 == NULL) {
  				newspi = htonl(spi);
* Unmerged path drivers/net/wireguard/selftest/allowedips.c
* Unmerged path kernel/kcsan/selftest.c
* Unmerged path mm/kfence/kfence_test.c
* Unmerged path arch/x86/kernel/module.c
* Unmerged path crypto/rsa-pkcs1pad.c
* Unmerged path crypto/testmgr.c
* Unmerged path drivers/bus/mhi/host/internal.h
* Unmerged path drivers/dma-buf/st-dma-fence-chain.c
* Unmerged path drivers/infiniband/core/cma.c
* Unmerged path drivers/infiniband/hw/hns/hns_roce_ah.c
* Unmerged path drivers/mtd/nand/raw/nandsim.c
* Unmerged path drivers/net/wireguard/selftest/allowedips.c
* Unmerged path drivers/net/wireless/broadcom/brcm80211/brcmfmac/p2p.c
* Unmerged path drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
* Unmerged path fs/f2fs/segment.c
* Unmerged path kernel/kcsan/selftest.c
* Unmerged path lib/test_hexdump.c
* Unmerged path lib/test_kasan.c
* Unmerged path lib/test_printf.c
* Unmerged path lib/test_vmalloc.c
* Unmerged path mm/kfence/kfence_test.c
* Unmerged path mm/swapfile.c
* Unmerged path net/bluetooth/mgmt.c
* Unmerged path net/core/pktgen.c
* Unmerged path net/ipv4/tcp_input.c
* Unmerged path net/ipv6/addrconf.c
* Unmerged path net/xfrm/xfrm_state.c
