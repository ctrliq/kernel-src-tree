arm64: entry: template the entry asm functions

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit a5b43a87a7609d49ed4a453a2b99b6d36ab1e5d0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/a5b43a87.failed

Now that the majority of the exception triage logic has been converted
to C, the entry assembly functions all have a uniform structure.

Let's generate them all with an assembly macro to reduce the amount of
code and to ensure they all remain in sync if we make changes in future.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <maz@kernel.org>
	Reviewed-by: Joey Gouly <joey.gouly@arm.com>
	Cc: James Morse <james.morse@arm.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lore.kernel.org/r/20210607094624.34689-14-mark.rutland@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit a5b43a87a7609d49ed4a453a2b99b6d36ab1e5d0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry.S
diff --cc arch/arm64/kernel/entry.S
index 3f9d46ff532e,b719ac26f7d1..000000000000
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@@ -569,147 -604,44 +569,184 @@@ SYM_CODE_START_LOCAL(el1_error_invalid
  	inv_entry 1, BAD_ERROR
  SYM_CODE_END(el1_error_invalid)
  
+ 	.macro entry_handler el:req, regsize:req, label:req
+ SYM_CODE_START_LOCAL(el\el\()_\label)
+ 	kernel_entry \el, \regsize
+ 	mov	x0, sp
+ 	bl	el\el\()_\label\()_handler
+ 	.if \el == 0
+ 	b	ret_to_user
+ 	.else
+ 	b	ret_to_kernel
+ 	.endif
+ SYM_CODE_END(el\el\()_\label)
+ 	.endm
+ 
  /*
-  * EL1 mode handlers.
+  * Early exception handlers
   */
++<<<<<<< HEAD
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el1_sync)
 +	kernel_entry 1
 +	mov	x0, sp
 +	bl	el1_sync_handler
 +	kernel_exit 1
 +SYM_CODE_END(el1_sync)
 +
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el1_irq)
 +	kernel_entry 1
 +	gic_prio_irq_setup pmr=x20, tmp=x1
 +	enable_da_f
 +
 +	mov	x0, sp
 +	bl	enter_el1_irq_or_nmi
 +
 +	irq_handler
 +
 +#ifdef CONFIG_PREEMPT
 +	ldr	x24, [tsk, #TSK_TI_PREEMPT]	// get preempt count
 +alternative_if ARM64_HAS_IRQ_PRIO_MASKING
 +	/*
 +	 * DA_F were cleared at start of handling. If anything is set in DAIF,
 +	 * we come back from an NMI, so skip preemption
 +	 */
 +	mrs	x0, daif
 +	orr	x24, x24, x0
 +alternative_else_nop_endif
 +	cbnz	x24, 1f				// preempt count != 0 || NMI return path
 +	bl	arm64_preempt_schedule_irq	// irq en/disable is done inside
 +1:
 +#endif
 +
 +	mov	x0, sp
 +	bl	exit_el1_irq_or_nmi
 +
 +	kernel_exit 1
 +SYM_CODE_END(el1_irq)
 +
 +SYM_CODE_START_LOCAL_NOALIGN(el1_fiq)
 +	kernel_entry 1
 +	el1_interrupt_handler handle_arch_fiq
++=======
+ 	entry_handler	1, 64, sync
+ 	entry_handler	1, 64, irq
+ 	entry_handler	1, 64, fiq
+ 	entry_handler	1, 64, error
+ 
+ 	entry_handler	0, 64, sync
+ 	entry_handler	0, 64, irq
+ 	entry_handler	0, 64, fiq
+ 	entry_handler	0, 64, error
+ 
+ #ifdef CONFIG_COMPAT
+ 	entry_handler	0, 32, sync_compat
+ 	entry_handler	0, 32, irq_compat
+ 	entry_handler	0, 32, fiq_compat
+ 	entry_handler	0, 32, error_compat
+ #endif
+ 
+ SYM_CODE_START_LOCAL(ret_to_kernel)
++>>>>>>> a5b43a87a760 (arm64: entry: template the entry asm functions)
 +	kernel_exit 1
 +SYM_CODE_END(el1_fiq)
 +
 +/*
++<<<<<<< HEAD
 + * EL0 mode handlers.
 + */
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_sync)
 +	kernel_entry 0
 +	mov	x0, sp
 +	bl	el0_sync_handler
 +	b	ret_to_user
 +SYM_CODE_END(el0_sync)
 +
 +#ifdef CONFIG_COMPAT
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_sync_compat)
 +	kernel_entry 0, 32
 +	mov	x0, sp
 +	bl	el0_sync_compat_handler
 +	b	ret_to_user
 +SYM_CODE_END(el0_sync_compat)
 +
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_irq_compat)
 +	kernel_entry 0, 32
 +	b	el0_irq_naked
 +SYM_CODE_END(el0_irq_compat)
 +
 +SYM_CODE_START_LOCAL_NOALIGN(el0_fiq_compat)
 +	kernel_entry 0, 32
 +	b	el0_fiq_naked
 +SYM_CODE_END(el0_fiq_compat)
 +
 +SYM_CODE_START_LOCAL_NOALIGN(el0_error_compat)
 +	kernel_entry 0, 32
 +	b	el0_error_naked
 +SYM_CODE_END(el0_error_compat)
 +#endif
 +
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_irq)
 +	kernel_entry 0
 +el0_irq_naked:
 +	gic_prio_irq_setup pmr=x20, tmp=x0
 +	ct_user_exit_irqoff
 +	enable_da_f
 +
 +#ifdef CONFIG_TRACE_IRQFLAGS
 +	bl	trace_hardirqs_off
 +#endif
 +
 +	tbz	x22, #55, 1f
 +	bl	do_el0_irq_bp_hardening
 +1:
 +	irq_handler
 +
 +#ifdef CONFIG_TRACE_IRQFLAGS
 +	bl	trace_hardirqs_on
 +#endif
 +	b	ret_to_user
 +SYM_CODE_END(el0_irq)
 +
 +SYM_CODE_START_LOCAL_NOALIGN(el0_fiq)
 +	kernel_entry 0
 +el0_fiq_naked:
 +	el0_interrupt_handler handle_arch_fiq
 +	b	ret_to_user
 +SYM_CODE_END(el0_fiq)
 +
 +SYM_CODE_START_LOCAL(el1_error)
 +	kernel_entry 1
 +	mrs	x1, esr_el1
 +	gic_prio_kentry_setup tmp=x2
 +	enable_dbg
 +	mov	x0, sp
 +	bl	do_serror
  	kernel_exit 1
 -SYM_CODE_END(ret_to_kernel)
 +SYM_CODE_END(el1_error)
 +
 +SYM_CODE_START_LOCAL(el0_error)
 +	kernel_entry 0
 +el0_error_naked:
 +	mrs	x25, esr_el1
 +	gic_prio_kentry_setup tmp=x2
 +	ct_user_exit_irqoff
 +	enable_dbg
 +	mov	x0, sp
 +	mov	x1, x25
 +	bl	do_serror
 +	enable_da_f
 +	b	ret_to_user
 +SYM_CODE_END(el0_error)
  
  /*
++=======
++>>>>>>> a5b43a87a760 (arm64: entry: template the entry asm functions)
   * "slow" syscall return path.
   */
  SYM_CODE_START_LOCAL(ret_to_user)
* Unmerged path arch/arm64/kernel/entry.S
