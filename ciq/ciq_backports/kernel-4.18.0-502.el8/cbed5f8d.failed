arm64: entry: move bad_mode() to entry-common.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit cbed5f8d3feb5ecc84c998b81db7e004b3fb2135
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/cbed5f8d.failed

In subsequent patches we'll rework the way bad_mode() is called by
exception entry code. In preparation for this, let's move bad_mode()
itself into entry-common.c.

Let's also mark it as noinstr (e.g. to prevent it being kprobed), and
let's also make the `handler` array a local variable, as this is only
use by bad_mode(), and will be removed entirely in a subsequent patch.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <maz@kernel.org>
	Reviewed-by: Joey Gouly <joey.gouly@arm.com>
	Cc: James Morse <james.morse@arm.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lore.kernel.org/r/20210607094624.34689-12-mark.rutland@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit cbed5f8d3feb5ecc84c998b81db7e004b3fb2135)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry-common.c
#	arch/arm64/kernel/traps.c
diff --cc arch/arm64/kernel/entry-common.c
index c764ba2d11d7,74d09fd3dafa..000000000000
--- a/arch/arm64/kernel/entry-common.c
+++ b/arch/arm64/kernel/entry-common.c
@@@ -15,7 -19,10 +15,8 @@@
  #include <asm/exception.h>
  #include <asm/kprobes.h>
  #include <asm/mmu.h>
 -#include <asm/processor.h>
 -#include <asm/stacktrace.h>
  #include <asm/sysreg.h>
+ #include <asm/system_misc.h>
  
  /*
   * This is intended to match the logic in irqentry_enter(), handling the kernel
@@@ -79,6 -120,118 +80,121 @@@ asmlinkage void noinstr exit_el1_irq_or
  		exit_to_kernel_mode(regs);
  }
  
++<<<<<<< HEAD
++=======
+ static void __sched arm64_preempt_schedule_irq(void)
+ {
+ 	lockdep_assert_irqs_disabled();
+ 
+ 	/*
+ 	 * DAIF.DA are cleared at the start of IRQ/FIQ handling, and when GIC
+ 	 * priority masking is used the GIC irqchip driver will clear DAIF.IF
+ 	 * using gic_arch_enable_irqs() for normal IRQs. If anything is set in
+ 	 * DAIF we must have handled an NMI, so skip preemption.
+ 	 */
+ 	if (system_uses_irq_prio_masking() && read_sysreg(daif))
+ 		return;
+ 
+ 	/*
+ 	 * Preempting a task from an IRQ means we leave copies of PSTATE
+ 	 * on the stack. cpufeature's enable calls may modify PSTATE, but
+ 	 * resuming one of these preempted tasks would undo those changes.
+ 	 *
+ 	 * Only allow a task to be preempted once cpufeatures have been
+ 	 * enabled.
+ 	 */
+ 	if (system_capabilities_finalized())
+ 		preempt_schedule_irq();
+ }
+ 
+ static void do_interrupt_handler(struct pt_regs *regs,
+ 				 void (*handler)(struct pt_regs *))
+ {
+ 	if (on_thread_stack())
+ 		call_on_irq_stack(regs, handler);
+ 	else
+ 		handler(regs);
+ }
+ 
+ extern void (*handle_arch_irq)(struct pt_regs *);
+ extern void (*handle_arch_fiq)(struct pt_regs *);
+ 
+ /*
+  * bad_mode handles the impossible case in the exception vector. This is always
+  * fatal.
+  */
+ asmlinkage void noinstr bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
+ {
+ 	const char *handler[] = {
+ 		"Synchronous Abort",
+ 		"IRQ",
+ 		"FIQ",
+ 		"Error"
+ 	};
+ 
+ 	arm64_enter_nmi(regs);
+ 
+ 	console_verbose();
+ 
+ 	pr_crit("Bad mode in %s handler detected on CPU%d, code 0x%08x -- %s\n",
+ 		handler[reason], smp_processor_id(), esr,
+ 		esr_get_class_string(esr));
+ 
+ 	__show_regs(regs);
+ 	panic("bad mode");
+ }
+ 
+ 
+ #ifdef CONFIG_ARM64_ERRATUM_1463225
+ static DEFINE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+ 
+ static void cortex_a76_erratum_1463225_svc_handler(void)
+ {
+ 	u32 reg, val;
+ 
+ 	if (!unlikely(test_thread_flag(TIF_SINGLESTEP)))
+ 		return;
+ 
+ 	if (!unlikely(this_cpu_has_cap(ARM64_WORKAROUND_1463225)))
+ 		return;
+ 
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
+ 	reg = read_sysreg(mdscr_el1);
+ 	val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
+ 	write_sysreg(val, mdscr_el1);
+ 	asm volatile("msr daifclr, #8");
+ 	isb();
+ 
+ 	/* We will have taken a single-step exception by this point */
+ 
+ 	write_sysreg(reg, mdscr_el1);
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
+ }
+ 
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	if (!__this_cpu_read(__in_cortex_a76_erratum_1463225_wa))
+ 		return false;
+ 
+ 	/*
+ 	 * We've taken a dummy step exception from the kernel to ensure
+ 	 * that interrupts are re-enabled on the syscall path. Return back
+ 	 * to cortex_a76_erratum_1463225_svc_handler() with debug exceptions
+ 	 * masked so that we can safely restore the mdscr and get on with
+ 	 * handling the syscall.
+ 	 */
+ 	regs->pstate |= PSR_D_BIT;
+ 	return true;
+ }
+ #else /* CONFIG_ARM64_ERRATUM_1463225 */
+ static void cortex_a76_erratum_1463225_svc_handler(void) { }
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_ARM64_ERRATUM_1463225 */
+ 
++>>>>>>> cbed5f8d3feb (arm64: entry: move bad_mode() to entry-common.c)
  static void noinstr el1_abort(struct pt_regs *regs, unsigned long esr)
  {
  	unsigned long far = read_sysreg(far_el1);
diff --cc arch/arm64/kernel/traps.c
index af27331eeca1,7def18ff02e2..000000000000
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@@ -55,20 -45,8 +55,23 @@@
  #include <asm/system_misc.h>
  #include <asm/sysreg.h>
  
++<<<<<<< HEAD
 +static const char *handler[]= {
 +	"Synchronous Abort",
 +	"IRQ",
 +	"FIQ",
 +	"Error"
 +};
 +
++=======
++>>>>>>> cbed5f8d3feb (arm64: entry: move bad_mode() to entry-common.c)
  int show_unhandled_signals = 0;
  
 +static void dump_backtrace_entry(unsigned long where)
 +{
 +	printk(" %pS\n", (void *)where);
 +}
 +
  static void dump_kernel_instr(const char *lvl, struct pt_regs *regs)
  {
  	unsigned long addr = instruction_pointer(regs);
@@@ -822,22 -744,6 +825,25 @@@ const char *esr_get_class_string(u32 es
  }
  
  /*
++<<<<<<< HEAD
 + * bad_mode handles the impossible case in the exception vector. This is always
 + * fatal.
 + */
 +asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 +{
 +	console_verbose();
 +
 +	pr_crit("Bad mode in %s handler detected on CPU%d, code 0x%08x -- %s\n",
 +		handler[reason], smp_processor_id(), esr,
 +		esr_get_class_string(esr));
 +
 +	__show_regs(regs);
 +	panic("bad mode");
 +}
 +
 +/*
++=======
++>>>>>>> cbed5f8d3feb (arm64: entry: move bad_mode() to entry-common.c)
   * bad_el0_sync handles unexpected, but potentially recoverable synchronous
   * exceptions taken from EL0. Unlike bad_mode, this returns.
   */
* Unmerged path arch/arm64/kernel/entry-common.c
* Unmerged path arch/arm64/kernel/traps.c
