arm64: entry: improve bad_mode()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit ca0c2647f54c34000b4026c6632268d2dc304c67
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/ca0c2647.failed

Our use of bad_mode() has a few rough edges:

* AArch64 doesn't use the term "mode", and refers to "Execution
  states", "Exception levels", and "Selected stack pointer".

* We log the exception type (SYNC/IRQ/FIQ/SError), but not the actual
  "mode" (though this can be decoded from the SPSR value).

* We use bad_mode() as a second-level handler for unexpected synchronous
  exceptions, where the "mode" is legitimate, but the specific exception
  is not.

* We dump the ESR value, but call this "code", and so it's not clear to
  all readers that this is the ESR.

... and all of this can be somewhat opaque to those who aren't extremely
familiar with the code.

Let's make this a bit clearer by having bad_mode() log "Unhandled
${TYPE} exception" rather than "Bad mode in ${TYPE} handler", using
"ESR" rather than "code", and having the final panic() log "Unhandled
exception" rather than "Bad mode".

In future we'd like to log the specific architectural vector rather than
just the type of exception, so we also split the core of bad_mode() out
into a helper called __panic_unhandled(), which takes the vector as a
string argument.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <maz@kernel.org>
	Reviewed-by: Joey Gouly <joey.gouly@arm.com>
	Cc: James Morse <james.morse@arm.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lore.kernel.org/r/20210607094624.34689-13-mark.rutland@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit ca0c2647f54c34000b4026c6632268d2dc304c67)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry-common.c
diff --cc arch/arm64/kernel/entry-common.c
index c764ba2d11d7,d0f9a6394067..000000000000
--- a/arch/arm64/kernel/entry-common.c
+++ b/arch/arm64/kernel/entry-common.c
@@@ -79,6 -120,119 +79,122 @@@ asmlinkage void noinstr exit_el1_irq_or
  		exit_to_kernel_mode(regs);
  }
  
++<<<<<<< HEAD
++=======
+ static void __sched arm64_preempt_schedule_irq(void)
+ {
+ 	lockdep_assert_irqs_disabled();
+ 
+ 	/*
+ 	 * DAIF.DA are cleared at the start of IRQ/FIQ handling, and when GIC
+ 	 * priority masking is used the GIC irqchip driver will clear DAIF.IF
+ 	 * using gic_arch_enable_irqs() for normal IRQs. If anything is set in
+ 	 * DAIF we must have handled an NMI, so skip preemption.
+ 	 */
+ 	if (system_uses_irq_prio_masking() && read_sysreg(daif))
+ 		return;
+ 
+ 	/*
+ 	 * Preempting a task from an IRQ means we leave copies of PSTATE
+ 	 * on the stack. cpufeature's enable calls may modify PSTATE, but
+ 	 * resuming one of these preempted tasks would undo those changes.
+ 	 *
+ 	 * Only allow a task to be preempted once cpufeatures have been
+ 	 * enabled.
+ 	 */
+ 	if (system_capabilities_finalized())
+ 		preempt_schedule_irq();
+ }
+ 
+ static void do_interrupt_handler(struct pt_regs *regs,
+ 				 void (*handler)(struct pt_regs *))
+ {
+ 	if (on_thread_stack())
+ 		call_on_irq_stack(regs, handler);
+ 	else
+ 		handler(regs);
+ }
+ 
+ extern void (*handle_arch_irq)(struct pt_regs *);
+ extern void (*handle_arch_fiq)(struct pt_regs *);
+ 
+ static void noinstr __panic_unhandled(struct pt_regs *regs, const char *vector,
+ 				      unsigned int esr)
+ {
+ 	arm64_enter_nmi(regs);
+ 
+ 	console_verbose();
+ 
+ 	pr_crit("Unhandled %s exception on CPU%d, ESR 0x%08x -- %s\n",
+ 		vector, smp_processor_id(), esr,
+ 		esr_get_class_string(esr));
+ 
+ 	__show_regs(regs);
+ 	panic("Unhandled exception");
+ }
+ 
+ asmlinkage void noinstr bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
+ {
+ 	const char *handler[] = {
+ 		"Synchronous Abort",
+ 		"IRQ",
+ 		"FIQ",
+ 		"Error"
+ 	};
+ 
+ 	__panic_unhandled(regs, handler[reason], esr);
+ }
+ 
+ #ifdef CONFIG_ARM64_ERRATUM_1463225
+ static DEFINE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+ 
+ static void cortex_a76_erratum_1463225_svc_handler(void)
+ {
+ 	u32 reg, val;
+ 
+ 	if (!unlikely(test_thread_flag(TIF_SINGLESTEP)))
+ 		return;
+ 
+ 	if (!unlikely(this_cpu_has_cap(ARM64_WORKAROUND_1463225)))
+ 		return;
+ 
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
+ 	reg = read_sysreg(mdscr_el1);
+ 	val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
+ 	write_sysreg(val, mdscr_el1);
+ 	asm volatile("msr daifclr, #8");
+ 	isb();
+ 
+ 	/* We will have taken a single-step exception by this point */
+ 
+ 	write_sysreg(reg, mdscr_el1);
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
+ }
+ 
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	if (!__this_cpu_read(__in_cortex_a76_erratum_1463225_wa))
+ 		return false;
+ 
+ 	/*
+ 	 * We've taken a dummy step exception from the kernel to ensure
+ 	 * that interrupts are re-enabled on the syscall path. Return back
+ 	 * to cortex_a76_erratum_1463225_svc_handler() with debug exceptions
+ 	 * masked so that we can safely restore the mdscr and get on with
+ 	 * handling the syscall.
+ 	 */
+ 	regs->pstate |= PSR_D_BIT;
+ 	return true;
+ }
+ #else /* CONFIG_ARM64_ERRATUM_1463225 */
+ static void cortex_a76_erratum_1463225_svc_handler(void) { }
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_ARM64_ERRATUM_1463225 */
+ 
++>>>>>>> ca0c2647f54c (arm64: entry: improve bad_mode())
  static void noinstr el1_abort(struct pt_regs *regs, unsigned long esr)
  {
  	unsigned long far = read_sysreg(far_el1);
* Unmerged path arch/arm64/kernel/entry-common.c
