x86/microcode: Default-disable late loading

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Borislav Petkov <bp@suse.de>
commit a77a94f86273ce42a39cb479217dd8d68acfe0ff
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/a77a94f8.failed

It is dangerous and it should not be used anyway - there's a nice early
loading already.

Requested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Link: https://lore.kernel.org/r/20220525161232.14924-3-bp@alien8.de
(cherry picked from commit a77a94f86273ce42a39cb479217dd8d68acfe0ff)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/Kconfig
diff --cc arch/x86/Kconfig
index f26d1d596e9a,976309d57a58..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -1360,9 -1350,16 +1360,22 @@@ config MICROCODE_AM
  	  If you select this option, microcode patch loading support for AMD
  	  processors will be enabled.
  
++<<<<<<< HEAD
 +config MICROCODE_OLD_INTERFACE
 +	def_bool y
 +	depends on MICROCODE
++=======
+ config MICROCODE_LATE_LOADING
+ 	bool "Late microcode loading (DANGEROUS)"
+ 	default n
+ 	depends on MICROCODE
+ 	help
+ 	  Loading microcode late, when the system is up and executing instructions
+ 	  is a tricky business and should be avoided if possible. Just the sequence
+ 	  of synchronizing all cores and SMT threads is one fragile dance which does
+ 	  not guarantee that cores might not softlock after the loading. Therefore,
+ 	  use this at your own risk. Late loading taints the kernel too.
++>>>>>>> a77a94f86273 (x86/microcode: Default-disable late loading)
  
  config X86_MSR
  	tristate "/dev/cpu/*/msr - Model-specific register support"
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index e2a0a17f458d..7fa616c98962 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -2181,6 +2181,7 @@ void cpu_init_secondary(void)
 }
 #endif
 
+#ifdef CONFIG_MICROCODE_LATE_LOADING
 /*
  * The microcode loader calls this upon late microcode load to recheck features,
  * only when microcode has been updated. Caller holds microcode_mutex and CPU
@@ -2210,6 +2211,7 @@ void microcode_check(void)
 	pr_warn("x86/CPU: CPU features have changed after loading microcode, but might not take effect.\n");
 	pr_warn("x86/CPU: Please consider either early loading through initrd/built-in or a potential BIOS update.\n");
 }
+#endif
 
 /*
  * Invoked from core CPU hotplug code after hotplug operations
diff --git a/arch/x86/kernel/cpu/microcode/core.c b/arch/x86/kernel/cpu/microcode/core.c
index fb1f03e6cf05..54cea6554088 100644
--- a/arch/x86/kernel/cpu/microcode/core.c
+++ b/arch/x86/kernel/cpu/microcode/core.c
@@ -487,6 +487,7 @@ static void __exit microcode_dev_exit(void)
 /* fake device for request_firmware */
 static struct platform_device	*microcode_pdev;
 
+#ifdef CONFIG_MICROCODE_LATE_LOADING
 /*
  * Late loading dance. Why the heavy-handed stomp_machine effort?
  *
@@ -654,6 +655,9 @@ static ssize_t reload_store(struct device *dev,
 	return ret;
 }
 
+static DEVICE_ATTR_WO(reload);
+#endif
+
 static ssize_t version_show(struct device *dev,
 			struct device_attribute *attr, char *buf)
 {
@@ -670,7 +674,6 @@ static ssize_t pf_show(struct device *dev,
 	return sprintf(buf, "0x%x\n", uci->cpu_sig.pf);
 }
 
-static DEVICE_ATTR_WO(reload);
 static DEVICE_ATTR(version, 0444, version_show, NULL);
 static DEVICE_ATTR(processor_flags, 0444, pf_show, NULL);
 
@@ -823,7 +826,9 @@ static int mc_cpu_down_prep(unsigned int cpu)
 }
 
 static struct attribute *cpu_root_microcode_attrs[] = {
+#ifdef CONFIG_MICROCODE_LATE_LOADING
 	&dev_attr_reload.attr,
+#endif
 	NULL
 };
 
