x86/resctrl: Fix event counts regression in reused RMIDs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Peter Newman <peternewman@google.com>
commit 2a81160d29d65b5876ab3f824fda99ae0219f05e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/2a81160d.failed

When creating a new monitoring group, the RMID allocated for it may have
been used by a group which was previously removed. In this case, the
hardware counters will have non-zero values which should be deducted
from what is reported in the new group's counts.

resctrl_arch_reset_rmid() initializes the prev_msr value for counters to
0, causing the initial count to be charged to the new group. Resurrect
__rmid_read() and use it to initialize prev_msr correctly.

Unlike before, __rmid_read() checks for error bits in the MSR read so
that callers don't need to.

Fixes: 1d81d15db39c ("x86/resctrl: Move mbm_overflow_count() into resctrl_arch_rmid_read()")
	Signed-off-by: Peter Newman <peternewman@google.com>
	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
	Reviewed-by: Reinette Chatre <reinette.chatre@intel.com>
	Tested-by: Babu Moger <babu.moger@amd.com>
	Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/r/20221220164132.443083-1-peternewman@google.com
(cherry picked from commit 2a81160d29d65b5876ab3f824fda99ae0219f05e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/resctrl/monitor.c
diff --cc arch/x86/kernel/cpu/resctrl/monitor.c
index 810114991b5b,77538abeb72a..000000000000
--- a/arch/x86/kernel/cpu/resctrl/monitor.c
+++ b/arch/x86/kernel/cpu/resctrl/monitor.c
@@@ -145,29 -146,101 +145,113 @@@ static inline struct rmid_entry *__rmid
  	return entry;
  }
  
++<<<<<<< HEAD
 +static u64 __rmid_read(u32 rmid, u32 eventid)
 +{
 +	u64 val;
 +
 +	/*
 +	 * As per the SDM, when IA32_QM_EVTSEL.EvtID (bits 7:0) is configured
 +	 * with a valid event code for supported resource type and the bits
 +	 * IA32_QM_EVTSEL.RMID (bits 41:32) are configured with valid RMID,
 +	 * IA32_QM_CTR.data (bits 61:0) reports the monitored data.
 +	 * IA32_QM_CTR.Error (bit 63) and IA32_QM_CTR.Unavailable (bit 62)
 +	 * are error bits.
 +	 */
 +	wrmsr(MSR_IA32_QM_EVTSEL, eventid, rmid);
 +	rdmsrl(MSR_IA32_QM_CTR, val);
 +
 +	return val;
 +}
++=======
+ static int __rmid_read(u32 rmid, enum resctrl_event_id eventid, u64 *val)
+ {
+ 	u64 msr_val;
+ 
+ 	/*
+ 	 * As per the SDM, when IA32_QM_EVTSEL.EvtID (bits 7:0) is configured
+ 	 * with a valid event code for supported resource type and the bits
+ 	 * IA32_QM_EVTSEL.RMID (bits 41:32) are configured with valid RMID,
+ 	 * IA32_QM_CTR.data (bits 61:0) reports the monitored data.
+ 	 * IA32_QM_CTR.Error (bit 63) and IA32_QM_CTR.Unavailable (bit 62)
+ 	 * are error bits.
+ 	 */
+ 	wrmsr(MSR_IA32_QM_EVTSEL, eventid, rmid);
+ 	rdmsrl(MSR_IA32_QM_CTR, msr_val);
+ 
+ 	if (msr_val & RMID_VAL_ERROR)
+ 		return -EIO;
+ 	if (msr_val & RMID_VAL_UNAVAIL)
+ 		return -EINVAL;
+ 
+ 	*val = msr_val;
+ 	return 0;
+ }
+ 
+ static struct arch_mbm_state *get_arch_mbm_state(struct rdt_hw_domain *hw_dom,
+ 						 u32 rmid,
+ 						 enum resctrl_event_id eventid)
+ {
+ 	switch (eventid) {
+ 	case QOS_L3_OCCUP_EVENT_ID:
+ 		return NULL;
+ 	case QOS_L3_MBM_TOTAL_EVENT_ID:
+ 		return &hw_dom->arch_mbm_total[rmid];
+ 	case QOS_L3_MBM_LOCAL_EVENT_ID:
+ 		return &hw_dom->arch_mbm_local[rmid];
+ 	}
+ 
+ 	/* Never expect to get here */
+ 	WARN_ON_ONCE(1);
+ 
+ 	return NULL;
+ }
+ 
+ void resctrl_arch_reset_rmid(struct rdt_resource *r, struct rdt_domain *d,
+ 			     u32 rmid, enum resctrl_event_id eventid)
+ {
+ 	struct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);
+ 	struct arch_mbm_state *am;
+ 
+ 	am = get_arch_mbm_state(hw_dom, rmid, eventid);
+ 	if (am) {
+ 		memset(am, 0, sizeof(*am));
+ 
+ 		/* Record any initial, non-zero count value. */
+ 		__rmid_read(rmid, eventid, &am->prev_msr);
+ 	}
+ }
+ 
+ static u64 mbm_overflow_count(u64 prev_msr, u64 cur_msr, unsigned int width)
+ {
+ 	u64 shift = 64 - width, chunks;
+ 
+ 	chunks = (cur_msr << shift) - (prev_msr << shift);
+ 	return chunks >> shift;
+ }
+ 
+ int resctrl_arch_rmid_read(struct rdt_resource *r, struct rdt_domain *d,
+ 			   u32 rmid, enum resctrl_event_id eventid, u64 *val)
+ {
+ 	struct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);
+ 	struct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);
+ 	struct arch_mbm_state *am;
+ 	u64 msr_val, chunks;
+ 	int ret;
+ 
+ 	if (!cpumask_test_cpu(smp_processor_id(), &d->cpu_mask))
+ 		return -EINVAL;
+ 
+ 	ret = __rmid_read(rmid, eventid, &msr_val);
+ 	if (ret)
+ 		return ret;
++>>>>>>> 2a81160d29d6 (x86/resctrl: Fix event counts regression in reused RMIDs)
  
 -	am = get_arch_mbm_state(hw_dom, rmid, eventid);
 -	if (am) {
 -		am->chunks += mbm_overflow_count(am->prev_msr, msr_val,
 -						 hw_res->mbm_width);
 -		chunks = get_corrected_mbm_count(rmid, am->chunks);
 -		am->prev_msr = msr_val;
 -	} else {
 -		chunks = msr_val;
 -	}
 -
 -	*val = chunks * hw_res->mon_scale;
 +static bool rmid_dirty(struct rmid_entry *entry)
 +{
 +	u64 val = __rmid_read(entry->rmid, QOS_L3_OCCUP_EVENT_ID);
  
 -	return 0;
 +	return val >= resctrl_cqm_threshold;
  }
  
  /*
* Unmerged path arch/x86/kernel/cpu/resctrl/monitor.c
