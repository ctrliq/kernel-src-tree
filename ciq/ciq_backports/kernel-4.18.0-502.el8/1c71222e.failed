mm: replace vma->vm_flags direct modifications with modifier calls

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
Rebuild_CHGLOG: - Revert "mm: replace vma->vm_flags direct modifications with modifier calls" (Jocelyn Falempe) [2160452]
Rebuild_FUZZ: 93.62%
commit-author Suren Baghdasaryan <surenb@google.com>
commit 1c71222e5f2393b5ea1a41795c67589eea7e3490
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/1c71222e.failed

Replace direct modifications to vma->vm_flags with calls to modifier
functions to be able to track flag changes and to keep vma locking
correctness.

[akpm@linux-foundation.org: fix drivers/misc/open-dice.c, per Hyeonggon Yoo]
Link: https://lkml.kernel.org/r/20230126193752.297968-5-surenb@google.com
	Signed-off-by: Suren Baghdasaryan <surenb@google.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Acked-by: Mel Gorman <mgorman@techsingularity.net>
	Acked-by: Mike Rapoport (IBM) <rppt@kernel.org>
	Acked-by: Sebastian Reichel <sebastian.reichel@collabora.com>
	Reviewed-by: Liam R. Howlett <Liam.Howlett@Oracle.com>
	Reviewed-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Arjun Roy <arjunroy@google.com>
	Cc: Axel Rasmussen <axelrasmussen@google.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: David Howells <dhowells@redhat.com>
	Cc: Davidlohr Bueso <dave@stgolabs.net>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Eric Dumazet <edumazet@google.com>
	Cc: Greg Thelen <gthelen@google.com>
	Cc: Hugh Dickins <hughd@google.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jann Horn <jannh@google.com>
	Cc: Joel Fernandes <joelaf@google.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Kent Overstreet <kent.overstreet@linux.dev>
	Cc: Laurent Dufour <ldufour@linux.ibm.com>
	Cc: Lorenzo Stoakes <lstoakes@gmail.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Minchan Kim <minchan@google.com>
	Cc: Paul E. McKenney <paulmck@kernel.org>
	Cc: Peter Oskolkov <posk@google.com>
	Cc: Peter Xu <peterx@redhat.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Punit Agrawal <punit.agrawal@bytedance.com>
	Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Cc: Shakeel Butt <shakeelb@google.com>
	Cc: Soheil Hassas Yeganeh <soheil@google.com>
	Cc: Song Liu <songliubraving@fb.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Cc: Will Deacon <will@kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 1c71222e5f2393b5ea1a41795c67589eea7e3490)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/kernel/process.c
#	arch/ia64/mm/init.c
#	arch/loongarch/include/asm/tlb.h
#	arch/powerpc/mm/book3s64/subpage_prot.c
#	arch/s390/mm/gmap.c
#	arch/x86/entry/vsyscall/vsyscall_64.c
#	arch/x86/um/mem_32.c
#	drivers/char/mspec.c
#	drivers/crypto/hisilicon/qm.c
#	drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
#	drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
#	drivers/gpu/drm/etnaviv/etnaviv_gem.c
#	drivers/gpu/drm/exynos/exynos_drm_gem.c
#	drivers/gpu/drm/mediatek/mtk_drm_gem.c
#	drivers/gpu/drm/msm/msm_gem.c
#	drivers/gpu/drm/rockchip/rockchip_drm_gem.c
#	drivers/infiniband/hw/hfi1/file_ops.c
#	drivers/misc/habanalabs/common/memory.c
#	drivers/misc/habanalabs/gaudi/gaudi.c
#	drivers/misc/habanalabs/gaudi2/gaudi2.c
#	drivers/misc/habanalabs/goya/goya.c
#	drivers/misc/open-dice.c
#	drivers/misc/uacce/uacce.c
#	drivers/staging/media/atomisp/pci/hmm/hmm_bo.c
#	drivers/vdpa/vdpa_user/iova_domain.c
#	drivers/vfio/pci/vfio_pci_core.c
#	fs/erofs/data.c
#	fs/proc/task_mmu.c
#	fs/userfaultfd.c
#	include/linux/mm.h
#	kernel/bpf/ringbuf.c
#	kernel/kcov.c
#	mm/madvise.c
#	mm/memory.c
#	mm/mlock.c
#	mm/mmap.c
#	mm/mprotect.c
#	mm/mremap.c
#	mm/secretmem.c
#	mm/shmem.c
diff --cc arch/arm/kernel/process.c
index d9c299133111,61c30b9a24ea..000000000000
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@@ -340,6 -314,9 +340,12 @@@ static int __init gate_vma_init(void
  {
  	vma_init(&gate_vma, NULL);
  	gate_vma.vm_page_prot = PAGE_READONLY_EXEC;
++<<<<<<< HEAD
++=======
+ 	gate_vma.vm_start = 0xffff0000;
+ 	gate_vma.vm_end	= 0xffff0000 + PAGE_SIZE;
+ 	vm_flags_init(&gate_vma, VM_READ | VM_EXEC | VM_MAYREAD | VM_MAYEXEC);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	return 0;
  }
  arch_initcall(gate_vma_init);
diff --cc arch/ia64/mm/init.c
index 5ab64d9d3462,7f5353e28516..000000000000
--- a/arch/ia64/mm/init.c
+++ b/arch/ia64/mm/init.c
@@@ -118,11 -109,11 +118,11 @@@ ia64_init_addr_space (void
  		vma_set_anonymous(vma);
  		vma->vm_start = current->thread.rbs_bot & PAGE_MASK;
  		vma->vm_end = vma->vm_start + PAGE_SIZE;
- 		vma->vm_flags = VM_DATA_DEFAULT_FLAGS|VM_GROWSUP|VM_ACCOUNT;
+ 		vm_flags_init(vma, VM_DATA_DEFAULT_FLAGS|VM_GROWSUP|VM_ACCOUNT);
  		vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 -		mmap_write_lock(current->mm);
 +		down_write(&current->mm->mmap_sem);
  		if (insert_vm_struct(current->mm, vma)) {
 -			mmap_write_unlock(current->mm);
 +			up_write(&current->mm->mmap_sem);
  			vm_area_free(vma);
  			return;
  		}
@@@ -136,11 -127,11 +136,17 @@@
  			vma_set_anonymous(vma);
  			vma->vm_end = PAGE_SIZE;
  			vma->vm_page_prot = __pgprot(pgprot_val(PAGE_READONLY) | _PAGE_MA_NAT);
++<<<<<<< HEAD
 +			vma->vm_flags = VM_READ | VM_MAYREAD | VM_IO |
 +					VM_DONTEXPAND | VM_DONTDUMP;
 +			down_write(&current->mm->mmap_sem);
++=======
+ 			vm_flags_init(vma, VM_READ | VM_MAYREAD | VM_IO |
+ 				      VM_DONTEXPAND | VM_DONTDUMP);
+ 			mmap_write_lock(current->mm);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  			if (insert_vm_struct(current->mm, vma)) {
 -				mmap_write_unlock(current->mm);
 +				up_write(&current->mm->mmap_sem);
  				vm_area_free(vma);
  				return;
  			}
@@@ -277,8 -272,8 +283,13 @@@ static int __init gate_vma_init(void
  	vma_init(&gate_vma, NULL);
  	gate_vma.vm_start = FIXADDR_USER_START;
  	gate_vma.vm_end = FIXADDR_USER_END;
++<<<<<<< HEAD
 +	gate_vma.vm_flags = VM_READ | VM_MAYREAD | VM_EXEC | VM_MAYEXEC;
 +	gate_vma.vm_page_prot = __P101;
++=======
+ 	vm_flags_init(&gate_vma, VM_READ | VM_MAYREAD | VM_EXEC | VM_MAYEXEC);
+ 	gate_vma.vm_page_prot = __pgprot(__ACCESS_BITS | _PAGE_PL_3 | _PAGE_AR_RX);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	return 0;
  }
diff --cc arch/powerpc/mm/book3s64/subpage_prot.c
index b70cf0e901c7,b75a9fb99599..000000000000
--- a/arch/powerpc/mm/book3s64/subpage_prot.c
+++ b/arch/powerpc/mm/book3s64/subpage_prot.c
@@@ -153,19 -155,9 +153,24 @@@ static void subpage_mark_vma_nohuge(str
  	 * We don't try too hard, we just mark all the vma in that range
  	 * VM_NOHUGEPAGE and split them.
  	 */
++<<<<<<< HEAD
 +	vma = find_vma(mm, addr);
 +	/*
 +	 * If the range is in unmapped range, just return
 +	 */
 +	if (vma && ((addr + len) <= vma->vm_start))
 +		return;
 +
 +	while (vma) {
 +		if (vma->vm_start >= (addr + len))
 +			break;
 +		vma->vm_flags |= VM_NOHUGEPAGE;
++=======
+ 	for_each_vma_range(vmi, vma, addr + len) {
+ 		vm_flags_set(vma, VM_NOHUGEPAGE);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  		walk_page_vma(vma, &subpage_walk_ops, NULL);
 +		vma = vma->vm_next;
  	}
  }
  #else
diff --cc arch/s390/mm/gmap.c
index d529b159fbb8,ab836597419d..000000000000
--- a/arch/s390/mm/gmap.c
+++ b/arch/s390/mm/gmap.c
@@@ -2515,10 -2519,10 +2515,15 @@@ static const struct mm_walk_ops thp_spl
  static inline void thp_split_mm(struct mm_struct *mm)
  {
  	struct vm_area_struct *vma;
 -	VMA_ITERATOR(vmi, mm, 0);
  
++<<<<<<< HEAD
 +	for (vma = mm->mmap; vma != NULL; vma = vma->vm_next) {
 +		vma->vm_flags &= ~VM_HUGEPAGE;
 +		vma->vm_flags |= VM_NOHUGEPAGE;
++=======
+ 	for_each_vma(vmi, vma) {
+ 		vm_flags_mod(vma, VM_NOHUGEPAGE, VM_HUGEPAGE);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  		walk_page_vma(vma, &thp_split_walk_ops, NULL);
  	}
  	mm->def_flags |= VM_NOHUGEPAGE;
diff --cc arch/x86/entry/vsyscall/vsyscall_64.c
index ad8cc6ae94af,d234ca797e4a..000000000000
--- a/arch/x86/entry/vsyscall/vsyscall_64.c
+++ b/arch/x86/entry/vsyscall/vsyscall_64.c
@@@ -363,6 -390,9 +363,12 @@@ void __init map_vsyscall(void
  		set_vsyscall_pgtable_user_bits(swapper_pg_dir);
  	}
  
++<<<<<<< HEAD
++=======
+ 	if (vsyscall_mode == XONLY)
+ 		vm_flags_init(&gate_vma, VM_EXEC);
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	BUILD_BUG_ON((unsigned long)__fix_to_virt(VSYSCALL_PAGE) !=
  		     (unsigned long)VSYSCALL_ADDR);
  }
diff --cc arch/x86/um/mem_32.c
index 56c44d865f7b,29b2203bc82c..000000000000
--- a/arch/x86/um/mem_32.c
+++ b/arch/x86/um/mem_32.c
@@@ -19,8 -16,8 +19,13 @@@ static int __init gate_vma_init(void
  	vma_init(&gate_vma, NULL);
  	gate_vma.vm_start = FIXADDR_USER_START;
  	gate_vma.vm_end = FIXADDR_USER_END;
++<<<<<<< HEAD
 +	gate_vma.vm_flags = VM_READ | VM_MAYREAD | VM_EXEC | VM_MAYEXEC;
 +	gate_vma.vm_page_prot = __P101;
++=======
+ 	vm_flags_init(&gate_vma, VM_READ | VM_MAYREAD | VM_EXEC | VM_MAYEXEC);
+ 	gate_vma.vm_page_prot = PAGE_READONLY;
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	return 0;
  }
diff --cc drivers/char/mspec.c
index d2b7f099ae90,b35f651837c8..000000000000
--- a/drivers/char/mspec.c
+++ b/drivers/char/mspec.c
@@@ -271,8 -206,8 +271,13 @@@ mspec_mmap(struct file *file, struct vm
  	refcount_set(&vdata->refcnt, 1);
  	vma->vm_private_data = vdata;
  
++<<<<<<< HEAD
 +	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
 +	if (vdata->type == MSPEC_FETCHOP || vdata->type == MSPEC_UNCACHED)
++=======
+ 	vm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
+ 	if (vdata->type == MSPEC_UNCACHED)
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
  	vma->vm_ops = &mspec_vm_ops;
  
diff --cc drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
index 91571b1324f2,a69fd6fdabb4..000000000000
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
@@@ -255,8 -256,8 +255,13 @@@ static int amdgpu_gem_object_mmap(struc
  	 * becoming writable and makes is_cow_mapping(vm_flags) false.
  	 */
  	if (is_cow_mapping(vma->vm_flags) &&
++<<<<<<< HEAD
 +	    !(vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))
 +		vma->vm_flags &= ~VM_MAYWRITE;
++=======
+ 	    !(vma->vm_flags & VM_ACCESS_FLAGS))
+ 		vm_flags_clear(vma, VM_MAYWRITE);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	return drm_gem_ttm_mmap(obj, vma);
  }
diff --cc drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
index cb3d2ccc5100,cbef2e147da5..000000000000
--- a/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
@@@ -157,8 -157,10 +157,15 @@@ int kfd_doorbell_mmap(struct kfd_dev *d
  
  	/* Calculate physical address of doorbell */
  	address = kfd_get_process_doorbells(pdd);
++<<<<<<< HEAD
 +	vma->vm_flags |= VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE |
 +				VM_DONTDUMP | VM_PFNMAP;
++=======
+ 	if (!address)
+ 		return -ENOMEM;
+ 	vm_flags_set(vma, VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE |
+ 				VM_DONTDUMP | VM_PFNMAP);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
  
diff --cc drivers/gpu/drm/etnaviv/etnaviv_gem.c
index 00a8d81176d3,b5f73502e3dd..000000000000
--- a/drivers/gpu/drm/etnaviv/etnaviv_gem.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_gem.c
@@@ -129,8 -130,7 +129,12 @@@ static int etnaviv_gem_mmap_obj(struct 
  {
  	pgprot_t vm_page_prot;
  
++<<<<<<< HEAD
 +	vma->vm_flags &= ~VM_PFNMAP;
 +	vma->vm_flags |= VM_MIXEDMAP;
++=======
+ 	vm_flags_set(vma, VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	vm_page_prot = vm_get_page_prot(vma->vm_flags);
  
diff --cc drivers/gpu/drm/exynos/exynos_drm_gem.c
index bdf5a7655228,638ca96830e9..000000000000
--- a/drivers/gpu/drm/exynos/exynos_drm_gem.c
+++ b/drivers/gpu/drm/exynos/exynos_drm_gem.c
@@@ -457,7 -365,13 +457,17 @@@ static int exynos_drm_gem_mmap_obj(stru
  	struct exynos_drm_gem *exynos_gem = to_exynos_gem(obj);
  	int ret;
  
++<<<<<<< HEAD
 +	DRM_DEBUG_KMS("flags = 0x%x\n", exynos_gem->flags);
++=======
+ 	if (obj->import_attach)
+ 		return dma_buf_mmap(obj->dma_buf, vma, 0);
+ 
+ 	vm_flags_set(vma, VM_IO | VM_DONTEXPAND | VM_DONTDUMP);
+ 
+ 	DRM_DEV_DEBUG_KMS(to_dma_dev(obj->dev), "flags = 0x%x\n",
+ 			  exynos_gem->flags);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	/* non-cachable as default. */
  	if (exynos_gem->flags & EXYNOS_BO_CACHABLE)
diff --cc drivers/gpu/drm/mediatek/mtk_drm_gem.c
index 259b7b0de1d2,28659514bf20..000000000000
--- a/drivers/gpu/drm/mediatek/mtk_drm_gem.c
+++ b/drivers/gpu/drm/mediatek/mtk_drm_gem.c
@@@ -143,8 -158,9 +143,14 @@@ static int mtk_drm_gem_object_mmap(stru
  	 * dma_alloc_attrs() allocated a struct page table for mtk_gem, so clear
  	 * VM_PFNMAP flag that was set by drm_gem_mmap_obj()/drm_gem_mmap().
  	 */
++<<<<<<< HEAD
 +	vma->vm_flags &= ~VM_PFNMAP;
 +	vma->vm_pgoff = 0;
++=======
+ 	vm_flags_set(vma, VM_IO | VM_DONTEXPAND | VM_DONTDUMP);
+ 	vma->vm_page_prot = pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
+ 	vma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	ret = dma_mmap_attrs(priv->dma_dev, vma, mtk_gem->cookie,
  			     mtk_gem->dma_addr, obj->size, mtk_gem->dma_attrs);
diff --cc drivers/gpu/drm/msm/msm_gem.c
index 50b04ddb11b3,c2fb98a94bc3..000000000000
--- a/drivers/gpu/drm/msm/msm_gem.c
+++ b/drivers/gpu/drm/msm/msm_gem.c
@@@ -840,9 -1008,20 +840,22 @@@ void msm_gem_free_object(struct drm_gem
  	kfree(msm_obj);
  }
  
++<<<<<<< HEAD
++=======
+ static int msm_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)
+ {
+ 	struct msm_gem_object *msm_obj = to_msm_bo(obj);
+ 
+ 	vm_flags_set(vma, VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
+ 	vma->vm_page_prot = msm_gem_pgprot(msm_obj, vm_get_page_prot(vma->vm_flags));
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  /* convenience method to construct a GEM buffer object, and userspace handle */
  int msm_gem_new_handle(struct drm_device *dev, struct drm_file *file,
 -		uint32_t size, uint32_t flags, uint32_t *handle,
 -		char *name)
 +		uint32_t size, uint32_t flags, uint32_t *handle)
  {
  	struct drm_gem_object *obj;
  	int ret;
diff --cc drivers/gpu/drm/rockchip/rockchip_drm_gem.c
index 0d18e10e284e,8ea09d915c3c..000000000000
--- a/drivers/gpu/drm/rockchip/rockchip_drm_gem.c
+++ b/drivers/gpu/drm/rockchip/rockchip_drm_gem.c
@@@ -264,7 -251,10 +264,11 @@@ static int rockchip_drm_gem_object_mmap
  	 * We allocated a struct page table for rk_obj, so clear
  	 * VM_PFNMAP flag that was set by drm_gem_mmap_obj()/drm_gem_mmap().
  	 */
++<<<<<<< HEAD
 +	vma->vm_flags &= ~VM_PFNMAP;
++=======
+ 	vm_flags_mod(vma, VM_IO | VM_DONTEXPAND | VM_DONTDUMP, VM_PFNMAP);
 -
 -	vma->vm_page_prot = pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
 -	vma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	if (rk_obj->pages)
  		ret = rockchip_drm_gem_object_mmap_iommu(obj, vma);
diff --cc drivers/infiniband/hw/hfi1/file_ops.c
index bba52e2cd442,c6e59bc480f9..000000000000
--- a/drivers/infiniband/hw/hfi1/file_ops.c
+++ b/drivers/infiniband/hw/hfi1/file_ops.c
@@@ -424,17 -403,8 +424,22 @@@ static int hfi1_file_mmap(struct file *
  			ret = -EPERM;
  			goto done;
  		}
++<<<<<<< HEAD
 +		vma->vm_flags &= ~VM_MAYWRITE;
 +		/*
 +		 * Mmap multiple separate allocations into a single vma.  From
 +		 * here, dma_mmap_coherent() calls dma_direct_mmap(), which
 +		 * requires the mmap to exactly fill the vma starting at
 +		 * vma_start.  Adjust the vma start and end for each eager
 +		 * buffer segment mapped.  Restore the originals when done.
 +		 */
 +		vm_start_save = vma->vm_start;
 +		vm_end_save = vma->vm_end;
 +		vma->vm_end = vma->vm_start;
++=======
+ 		vm_flags_clear(vma, VM_MAYWRITE);
+ 		addr = vma->vm_start;
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  		for (i = 0 ; i < uctxt->egrbufs.numbufs; i++) {
  			memlen = uctxt->egrbufs.buffers[i].len;
  			memvirt = uctxt->egrbufs.buffers[i].addr;
@@@ -560,9 -528,11 +565,17 @@@
  		goto done;
  	}
  
++<<<<<<< HEAD
 +	vma->vm_flags = flags;
 +	mmap_cdbg(ctxt, subctxt, type, mapio, vmf, memaddr, memvirt, memdma, 
 +		  memlen, vma);
++=======
+ 	vm_flags_reset(vma, flags);
+ 	hfi1_cdbg(PROC,
+ 		  "%u:%u type:%u io/vf:%d/%d, addr:0x%llx, len:%lu(%lu), flags:0x%lx\n",
+ 		    ctxt, subctxt, type, mapio, vmf, memaddr, memlen,
+ 		    vma->vm_end - vma->vm_start, vma->vm_flags);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	if (vmf) {
  		vma->vm_pgoff = PFN_DOWN(memaddr);
  		vma->vm_ops = &vm_ops;
diff --cc fs/proc/task_mmu.c
index 3e6b456742ef,6a96e1713fd5..000000000000
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@@ -1155,28 -1296,10 +1155,32 @@@ static ssize_t clear_refs_write(struct 
  		}
  
  		if (type == CLEAR_REFS_SOFT_DIRTY) {
 -			for_each_vma(vmi, vma) {
 +			for (vma = mm->mmap; vma; vma = vma->vm_next) {
  				if (!(vma->vm_flags & VM_SOFTDIRTY))
  					continue;
++<<<<<<< HEAD
 +				/*
 +				 * Avoid to modify vma->vm_flags
 +				 * without locked ops while the
 +				 * coredump reads the vm_flags.
 +				 */
 +				if (!mmget_still_valid(mm)) {
 +					/*
 +					 * Silently return "count"
 +					 * like if get_task_mm()
 +					 * failed. FIXME: should this
 +					 * function have returned
 +					 * -ESRCH if get_task_mm()
 +					 * failed like if
 +					 * get_proc_task() fails?
 +					 */
 +					mmap_write_unlock(mm);
 +					goto out_mm;
 +				}
 +				vma->vm_flags &= ~VM_SOFTDIRTY;
++=======
+ 				vm_flags_clear(vma, VM_SOFTDIRTY);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  				vma_set_page_prot(vma);
  			}
  
diff --cc fs/userfaultfd.c
index 9ba1fa459921,44d1ee429eb0..000000000000
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@@ -93,6 -100,29 +93,32 @@@ struct userfaultfd_wake_range 
  	unsigned long len;
  };
  
++<<<<<<< HEAD
++=======
+ /* internal indication that UFFD_API ioctl was successfully executed */
+ #define UFFD_FEATURE_INITIALIZED		(1u << 31)
+ 
+ static bool userfaultfd_is_initialized(struct userfaultfd_ctx *ctx)
+ {
+ 	return ctx->features & UFFD_FEATURE_INITIALIZED;
+ }
+ 
+ static void userfaultfd_set_vm_flags(struct vm_area_struct *vma,
+ 				     vm_flags_t flags)
+ {
+ 	const bool uffd_wp_changed = (vma->vm_flags ^ flags) & VM_UFFD_WP;
+ 
+ 	vm_flags_reset(vma, flags);
+ 	/*
+ 	 * For shared mappings, we want to enable writenotify while
+ 	 * userfaultfd-wp is enabled (see vma_wants_writenotify()). We'll simply
+ 	 * recalculate vma->vm_page_prot whenever userfaultfd-wp changes.
+ 	 */
+ 	if ((vma->vm_flags & VM_SHARED) && uffd_wp_changed)
+ 		vma_set_page_prot(vma);
+ }
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  static int userfaultfd_wake_function(wait_queue_entry_t *wq, unsigned mode,
  				     int wake_flags, void *key)
  {
diff --cc include/linux/mm.h
index 53aaec12a37d,ce6d9d765aae..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -3126,5 -3627,48 +3126,52 @@@ void mem_dump_obj(void *object)
  static inline void mem_dump_obj(void *object) {}
  #endif
  
++<<<<<<< HEAD
 +#endif /* __KERNEL__ */
++=======
+ /**
+  * seal_check_future_write - Check for F_SEAL_FUTURE_WRITE flag and handle it
+  * @seals: the seals to check
+  * @vma: the vma to operate on
+  *
+  * Check whether F_SEAL_FUTURE_WRITE is set; if so, do proper check/handling on
+  * the vma flags.  Return 0 if check pass, or <0 for errors.
+  */
+ static inline int seal_check_future_write(int seals, struct vm_area_struct *vma)
+ {
+ 	if (seals & F_SEAL_FUTURE_WRITE) {
+ 		/*
+ 		 * New PROT_WRITE and MAP_SHARED mmaps are not allowed when
+ 		 * "future write" seal active.
+ 		 */
+ 		if ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_WRITE))
+ 			return -EPERM;
+ 
+ 		/*
+ 		 * Since an F_SEAL_FUTURE_WRITE sealed memfd can be mapped as
+ 		 * MAP_SHARED and read-only, take care to not allow mprotect to
+ 		 * revert protections on such mappings. Do this only for shared
+ 		 * mappings. For private mappings, don't need to mask
+ 		 * VM_MAYWRITE as we still want them to be COW-writable.
+ 		 */
+ 		if (vma->vm_flags & VM_SHARED)
+ 			vm_flags_clear(vma, VM_MAYWRITE);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ #ifdef CONFIG_ANON_VMA_NAME
+ int madvise_set_anon_name(struct mm_struct *mm, unsigned long start,
+ 			  unsigned long len_in,
+ 			  struct anon_vma_name *anon_name);
+ #else
+ static inline int
+ madvise_set_anon_name(struct mm_struct *mm, unsigned long start,
+ 		      unsigned long len_in, struct anon_vma_name *anon_name) {
+ 	return 0;
+ }
+ #endif
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  #endif /* _LINUX_MM_H */
diff --cc kernel/bpf/ringbuf.c
index 9e0c10c6892a,8732e0aadf36..000000000000
--- a/kernel/bpf/ringbuf.c
+++ b/kernel/bpf/ringbuf.c
@@@ -241,6 -276,26 +241,29 @@@ static int ringbuf_map_mmap(struct bpf_
  				   vma->vm_pgoff + RINGBUF_PGOFF);
  }
  
++<<<<<<< HEAD
++=======
+ static int ringbuf_map_mmap_user(struct bpf_map *map, struct vm_area_struct *vma)
+ {
+ 	struct bpf_ringbuf_map *rb_map;
+ 
+ 	rb_map = container_of(map, struct bpf_ringbuf_map, map);
+ 
+ 	if (vma->vm_flags & VM_WRITE) {
+ 		if (vma->vm_pgoff == 0)
+ 			/* Disallow writable mappings to the consumer pointer,
+ 			 * and allow writable mappings to both the producer
+ 			 * position, and the ring buffer data itself.
+ 			 */
+ 			return -EPERM;
+ 	} else {
+ 		vm_flags_clear(vma, VM_MAYWRITE);
+ 	}
+ 	/* remap_vmalloc_range() checks size and offset constraints */
+ 	return remap_vmalloc_range(vma, rb_map->rb, vma->vm_pgoff + RINGBUF_PGOFF);
+ }
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  static unsigned long ringbuf_avail_data_sz(struct bpf_ringbuf *rb)
  {
  	unsigned long cons_pos, prod_pos;
diff --cc kernel/kcov.c
index 3ebd09efe72a,84c717337df0..000000000000
--- a/kernel/kcov.c
+++ b/kernel/kcov.c
@@@ -287,20 -488,19 +287,31 @@@ static int kcov_mmap(struct file *filep
  		res = -EINVAL;
  		goto exit;
  	}
++<<<<<<< HEAD
 +	if (!kcov->area) {
 +		kcov->area = area;
 +		vma->vm_flags |= VM_DONTEXPAND;
 +		spin_unlock(&kcov->lock);
 +		for (off = 0; off < size; off += PAGE_SIZE) {
 +			page = vmalloc_to_page(kcov->area + off);
 +			if (vm_insert_page(vma, vma->vm_start + off, page))
 +				WARN_ONCE(1, "vm_insert_page() failed");
++=======
+ 	spin_unlock_irqrestore(&kcov->lock, flags);
+ 	vm_flags_set(vma, VM_DONTEXPAND);
+ 	for (off = 0; off < size; off += PAGE_SIZE) {
+ 		page = vmalloc_to_page(kcov->area + off);
+ 		res = vm_insert_page(vma, vma->vm_start + off, page);
+ 		if (res) {
+ 			pr_warn_once("kcov: vm_insert_page() failed\n");
+ 			return res;
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  		}
 +		return 0;
  	}
 -	return 0;
  exit:
 -	spin_unlock_irqrestore(&kcov->lock, flags);
 +	spin_unlock(&kcov->lock);
 +	vfree(area);
  	return res;
  }
  
diff --cc mm/madvise.c
index 7598ec47d91f,5a5a687d03c2..000000000000
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@@ -195,9 -176,14 +195,20 @@@ success
  	/*
  	 * vm_flags is protected by the mmap_lock held in write mode.
  	 */
++<<<<<<< HEAD
 +	vma->vm_flags = new_flags;
 +out:
 +	return error;
++=======
+ 	vm_flags_reset(vma, new_flags);
+ 	if (!vma->vm_file || vma_is_anon_shmem(vma)) {
+ 		error = replace_anon_vma_name(vma, anon_name);
+ 		if (error)
+ 			return error;
+ 	}
+ 
+ 	return 0;
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  }
  
  #ifdef CONFIG_SWAP
diff --cc mm/memory.c
index 1ab44f5e0706,4354b7987f36..000000000000
--- a/mm/memory.c
+++ b/mm/memory.c
@@@ -1611,6 -1830,123 +1611,126 @@@ out
  	return retval;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef pte_index
+ static int insert_page_in_batch_locked(struct vm_area_struct *vma, pte_t *pte,
+ 			unsigned long addr, struct page *page, pgprot_t prot)
+ {
+ 	int err;
+ 
+ 	if (!page_count(page))
+ 		return -EINVAL;
+ 	err = validate_page_before_insert(page);
+ 	if (err)
+ 		return err;
+ 	return insert_page_into_pte_locked(vma, pte, addr, page, prot);
+ }
+ 
+ /* insert_pages() amortizes the cost of spinlock operations
+  * when inserting pages in a loop. Arch *must* define pte_index.
+  */
+ static int insert_pages(struct vm_area_struct *vma, unsigned long addr,
+ 			struct page **pages, unsigned long *num, pgprot_t prot)
+ {
+ 	pmd_t *pmd = NULL;
+ 	pte_t *start_pte, *pte;
+ 	spinlock_t *pte_lock;
+ 	struct mm_struct *const mm = vma->vm_mm;
+ 	unsigned long curr_page_idx = 0;
+ 	unsigned long remaining_pages_total = *num;
+ 	unsigned long pages_to_write_in_pmd;
+ 	int ret;
+ more:
+ 	ret = -EFAULT;
+ 	pmd = walk_to_pmd(mm, addr);
+ 	if (!pmd)
+ 		goto out;
+ 
+ 	pages_to_write_in_pmd = min_t(unsigned long,
+ 		remaining_pages_total, PTRS_PER_PTE - pte_index(addr));
+ 
+ 	/* Allocate the PTE if necessary; takes PMD lock once only. */
+ 	ret = -ENOMEM;
+ 	if (pte_alloc(mm, pmd))
+ 		goto out;
+ 
+ 	while (pages_to_write_in_pmd) {
+ 		int pte_idx = 0;
+ 		const int batch_size = min_t(int, pages_to_write_in_pmd, 8);
+ 
+ 		start_pte = pte_offset_map_lock(mm, pmd, addr, &pte_lock);
+ 		for (pte = start_pte; pte_idx < batch_size; ++pte, ++pte_idx) {
+ 			int err = insert_page_in_batch_locked(vma, pte,
+ 				addr, pages[curr_page_idx], prot);
+ 			if (unlikely(err)) {
+ 				pte_unmap_unlock(start_pte, pte_lock);
+ 				ret = err;
+ 				remaining_pages_total -= pte_idx;
+ 				goto out;
+ 			}
+ 			addr += PAGE_SIZE;
+ 			++curr_page_idx;
+ 		}
+ 		pte_unmap_unlock(start_pte, pte_lock);
+ 		pages_to_write_in_pmd -= batch_size;
+ 		remaining_pages_total -= batch_size;
+ 	}
+ 	if (remaining_pages_total)
+ 		goto more;
+ 	ret = 0;
+ out:
+ 	*num = remaining_pages_total;
+ 	return ret;
+ }
+ #endif  /* ifdef pte_index */
+ 
+ /**
+  * vm_insert_pages - insert multiple pages into user vma, batching the pmd lock.
+  * @vma: user vma to map to
+  * @addr: target start user address of these pages
+  * @pages: source kernel pages
+  * @num: in: number of pages to map. out: number of pages that were *not*
+  * mapped. (0 means all pages were successfully mapped).
+  *
+  * Preferred over vm_insert_page() when inserting multiple pages.
+  *
+  * In case of error, we may have mapped a subset of the provided
+  * pages. It is the caller's responsibility to account for this case.
+  *
+  * The same restrictions apply as in vm_insert_page().
+  */
+ int vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,
+ 			struct page **pages, unsigned long *num)
+ {
+ #ifdef pte_index
+ 	const unsigned long end_addr = addr + (*num * PAGE_SIZE) - 1;
+ 
+ 	if (addr < vma->vm_start || end_addr >= vma->vm_end)
+ 		return -EFAULT;
+ 	if (!(vma->vm_flags & VM_MIXEDMAP)) {
+ 		BUG_ON(mmap_read_trylock(vma->vm_mm));
+ 		BUG_ON(vma->vm_flags & VM_PFNMAP);
+ 		vm_flags_set(vma, VM_MIXEDMAP);
+ 	}
+ 	/* Defer page refcount checking till we're about to map that page. */
+ 	return insert_pages(vma, addr, pages, num, vma->vm_page_prot);
+ #else
+ 	unsigned long idx = 0, pgcount = *num;
+ 	int err = -EINVAL;
+ 
+ 	for (; idx < pgcount; ++idx) {
+ 		err = vm_insert_page(vma, addr + (PAGE_SIZE * idx), pages[idx]);
+ 		if (err)
+ 			break;
+ 	}
+ 	*num = pgcount - idx;
+ 	return err;
+ #endif  /* ifdef pte_index */
+ }
+ EXPORT_SYMBOL(vm_insert_pages);
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  /**
   * vm_insert_page - insert single page into user vma
   * @vma: user vma to map to
@@@ -2122,11 -2452,7 +2242,15 @@@ int remap_pfn_range(struct vm_area_stru
  		vma->vm_pgoff = pfn;
  	}
  
++<<<<<<< HEAD
 +	err = track_pfn_remap(vma, &prot, remap_pfn, addr, PAGE_ALIGN(size));
 +	if (err)
 +		return -EINVAL;
 +
 +	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
++=======
+ 	vm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
  	BUG_ON(addr >= end);
  	pfn -= addr >> PAGE_SHIFT;
diff --cc mm/mlock.c
index 06908a1c766e,ed49459e343e..000000000000
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@@ -128,357 -162,233 +128,369 @@@ static void __munlock_isolated_page(str
  }
  
  /*
 - * Flags held in the low bits of a struct folio pointer on the mlock_fbatch.
 + * Accounting for page isolation fail during munlock
 + *
 + * Performs accounting when page isolation fails in munlock. There is nothing
 + * else to do because it means some other task has already removed the page
 + * from the LRU. putback_lru_page() will take care of removing the page from
 + * the unevictable list, if necessary. vmscan [page_referenced()] will move
 + * the page back to the unevictable list if some other vma has it mlocked.
   */
 -#define LRU_FOLIO 0x1
 -#define NEW_FOLIO 0x2
 -static inline struct folio *mlock_lru(struct folio *folio)
 +static void __munlock_isolation_failed(struct page *page)
  {
 -	return (struct folio *)((unsigned long)folio + LRU_FOLIO);
 -}
 +	int nr_pages = thp_nr_pages(page);
  
 -static inline struct folio *mlock_new(struct folio *folio)
 -{
 -	return (struct folio *)((unsigned long)folio + NEW_FOLIO);
 +	if (PageUnevictable(page))
 +		__count_vm_events(UNEVICTABLE_PGSTRANDED, nr_pages);
 +	else
 +		__count_vm_events(UNEVICTABLE_PGMUNLOCKED, nr_pages);
  }
  
 -/*
 - * mlock_folio_batch() is derived from folio_batch_move_lru(): perhaps that can
 - * make use of such folio pointer flags in future, but for now just keep it for
 - * mlock.  We could use three separate folio batches instead, but one feels
 - * better (munlocking a full folio batch does not need to drain mlocking folio
 - * batches first).
 +/**
 + * munlock_vma_page - munlock a vma page
 + * @page: page to be unlocked, either a normal page or THP page head
 + *
 + * returns the size of the page as a page mask (0 for normal page,
 + *         HPAGE_PMD_NR - 1 for THP head page)
 + *
 + * called from munlock()/munmap() path with page supposedly on the LRU.
 + * When we munlock a page, because the vma where we found the page is being
 + * munlock()ed or munmap()ed, we want to check whether other vmas hold the
 + * page locked so that we can leave it on the unevictable lru list and not
 + * bother vmscan with it.  However, to walk the page's rmap list in
 + * try_to_munlock() we must isolate the page from the LRU.  If some other
 + * task has removed the page from the LRU, we won't be able to do that.
 + * So we clear the PageMlocked as we might not get another chance.  If we
 + * can't isolate the page, we leave it for putback_lru_page() and vmscan
 + * [page_referenced()/try_to_unmap()] to deal with.
   */
 -static void mlock_folio_batch(struct folio_batch *fbatch)
 +unsigned int munlock_vma_page(struct page *page)
  {
 -	struct lruvec *lruvec = NULL;
 -	unsigned long mlock;
 -	struct folio *folio;
 -	int i;
 -
 -	for (i = 0; i < folio_batch_count(fbatch); i++) {
 -		folio = fbatch->folios[i];
 -		mlock = (unsigned long)folio & (LRU_FOLIO | NEW_FOLIO);
 -		folio = (struct folio *)((unsigned long)folio - mlock);
 -		fbatch->folios[i] = folio;
 -
 -		if (mlock & LRU_FOLIO)
 -			lruvec = __mlock_folio(folio, lruvec);
 -		else if (mlock & NEW_FOLIO)
 -			lruvec = __mlock_new_folio(folio, lruvec);
 -		else
 -			lruvec = __munlock_folio(folio, lruvec);
 -	}
 +	int nr_pages;
  
 -	if (lruvec)
 -		unlock_page_lruvec_irq(lruvec);
 -	release_pages(fbatch->folios, fbatch->nr);
 -	folio_batch_reinit(fbatch);
 -}
 +	/* For try_to_munlock() and to serialize with page migration */
 +	BUG_ON(!PageLocked(page));
 +	VM_BUG_ON_PAGE(PageTail(page), page);
  
 -void mlock_drain_local(void)
 -{
 -	struct folio_batch *fbatch;
 +	if (!TestClearPageMlocked(page)) {
 +		/* Potentially, PTE-mapped THP: do not skip the rest PTEs */
 +		return 0;
 +	}
  
 -	local_lock(&mlock_fbatch.lock);
 -	fbatch = this_cpu_ptr(&mlock_fbatch.fbatch);
 -	if (folio_batch_count(fbatch))
 -		mlock_folio_batch(fbatch);
 -	local_unlock(&mlock_fbatch.lock);
 -}
 +	nr_pages = thp_nr_pages(page);
 +	mod_zone_page_state(page_zone(page), NR_MLOCK, -nr_pages);
  
 -void mlock_drain_remote(int cpu)
 -{
 -	struct folio_batch *fbatch;
 +	if (!isolate_lru_page(page))
 +		__munlock_isolated_page(page);
 +	else
 +		__munlock_isolation_failed(page);
  
 -	WARN_ON_ONCE(cpu_online(cpu));
 -	fbatch = &per_cpu(mlock_fbatch.fbatch, cpu);
 -	if (folio_batch_count(fbatch))
 -		mlock_folio_batch(fbatch);
 +	return nr_pages - 1;
  }
  
 -bool need_mlock_drain(int cpu)
 +/*
 + * convert get_user_pages() return value to posix mlock() error
 + */
 +static int __mlock_posix_error_return(long retval)
  {
 -	return folio_batch_count(&per_cpu(mlock_fbatch.fbatch, cpu));
 +	if (retval == -EFAULT)
 +		retval = -ENOMEM;
 +	else if (retval == -ENOMEM)
 +		retval = -EAGAIN;
 +	return retval;
  }
  
 -/**
 - * mlock_folio - mlock a folio already on (or temporarily off) LRU
 - * @folio: folio to be mlocked.
 +/*
 + * Prepare page for fast batched LRU putback via putback_lru_evictable_pagevec()
 + *
 + * The fast path is available only for evictable pages with single mapping.
 + * Then we can bypass the per-cpu pvec and get better performance.
 + * when mapcount > 1 we need try_to_munlock() which can fail.
 + * when !page_evictable(), we need the full redo logic of putback_lru_page to
 + * avoid leaving evictable page in unevictable list.
 + *
 + * In case of success, @page is added to @pvec and @pgrescued is incremented
 + * in case that the page was previously unevictable. @page is also unlocked.
   */
 -void mlock_folio(struct folio *folio)
 +static bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,
 +		int *pgrescued)
  {
 -	struct folio_batch *fbatch;
 -
 -	local_lock(&mlock_fbatch.lock);
 -	fbatch = this_cpu_ptr(&mlock_fbatch.fbatch);
 -
 -	if (!folio_test_set_mlocked(folio)) {
 -		int nr_pages = folio_nr_pages(folio);
 -
 -		zone_stat_mod_folio(folio, NR_MLOCK, nr_pages);
 -		__count_vm_events(UNEVICTABLE_PGMLOCKED, nr_pages);
 +	VM_BUG_ON_PAGE(PageLRU(page), page);
 +	VM_BUG_ON_PAGE(!PageLocked(page), page);
 +
 +	if (page_mapcount(page) <= 1 && page_evictable(page)) {
 +		pagevec_add(pvec, page);
 +		if (TestClearPageUnevictable(page))
 +			(*pgrescued)++;
 +		unlock_page(page);
 +		return true;
  	}
  
 -	folio_get(folio);
 -	if (!folio_batch_add(fbatch, mlock_lru(folio)) ||
 -	    folio_test_large(folio) || lru_cache_disabled())
 -		mlock_folio_batch(fbatch);
 -	local_unlock(&mlock_fbatch.lock);
 +	return false;
  }
  
 -/**
 - * mlock_new_folio - mlock a newly allocated folio not yet on LRU
 - * @folio: folio to be mlocked, either normal or a THP head.
 +/*
 + * Putback multiple evictable pages to the LRU
 + *
 + * Batched putback of evictable pages that bypasses the per-cpu pvec. Some of
 + * the pages might have meanwhile become unevictable but that is OK.
   */
 -void mlock_new_folio(struct folio *folio)
 +static void __putback_lru_fast(struct pagevec *pvec, int pgrescued)
  {
 -	struct folio_batch *fbatch;
 -	int nr_pages = folio_nr_pages(folio);
 -
 -	local_lock(&mlock_fbatch.lock);
 -	fbatch = this_cpu_ptr(&mlock_fbatch.fbatch);
 -	folio_set_mlocked(folio);
 -
 -	zone_stat_mod_folio(folio, NR_MLOCK, nr_pages);
 -	__count_vm_events(UNEVICTABLE_PGMLOCKED, nr_pages);
 -
 -	folio_get(folio);
 -	if (!folio_batch_add(fbatch, mlock_new(folio)) ||
 -	    folio_test_large(folio) || lru_cache_disabled())
 -		mlock_folio_batch(fbatch);
 -	local_unlock(&mlock_fbatch.lock);
 +	count_vm_events(UNEVICTABLE_PGMUNLOCKED, pagevec_count(pvec));
 +	/*
 +	 *__pagevec_lru_add() calls release_pages() so we don't call
 +	 * put_page() explicitly
 +	 */
 +	__pagevec_lru_add(pvec);
 +	count_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);
  }
  
 -/**
 - * munlock_folio - munlock a folio
 - * @folio: folio to be munlocked, either normal or a THP head.
 +/*
 + * Munlock a batch of pages from the same zone
 + *
 + * The work is split to two main phases. First phase clears the Mlocked flag
 + * and attempts to isolate the pages, all under a single zone lru lock.
 + * The second phase finishes the munlock only for pages where isolation
 + * succeeded.
 + *
 + * Note that the pagevec may be modified during the process.
   */
 -void munlock_folio(struct folio *folio)
 +static void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)
  {
 -	struct folio_batch *fbatch;
 +	int i;
 +	int nr = pagevec_count(pvec);
 +	int delta_munlocked = -nr;
 +	struct pagevec pvec_putback;
 +	struct lruvec *lruvec = NULL;
 +	int pgrescued = 0;
 +
 +	pagevec_init(&pvec_putback);
 +
 +	/* Phase 1: page isolation */
 +	for (i = 0; i < nr; i++) {
 +		struct page *page = pvec->pages[i];
 +
 +		if (TestClearPageMlocked(page)) {
 +			/*
 +			 * We already have pin from follow_page_mask()
 +			 * so we can spare the get_page() here.
 +			 */
 +			if (TestClearPageLRU(page)) {
 +				lruvec = relock_page_lruvec_irq(page, lruvec);
 +				del_page_from_lru_list(page, lruvec);
 +				continue;
 +			} else
 +				__munlock_isolation_failed(page);
 +		} else {
 +			delta_munlocked++;
 +		}
 +
 +		/*
 +		 * We won't be munlocking this page in the next phase
 +		 * but we still need to release the follow_page_mask()
 +		 * pin. We cannot do it under lru_lock however. If it's
 +		 * the last pin, __page_cache_release() would deadlock.
 +		 */
 +		pagevec_add(&pvec_putback, pvec->pages[i]);
 +		pvec->pages[i] = NULL;
 +	}
 +	if (lruvec) {
 +		__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);
 +		unlock_page_lruvec_irq(lruvec);
 +	} else if (delta_munlocked) {
 +		mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);
 +	}
 +
 +	/* Now we can release pins of pages that we are not munlocking */
 +	pagevec_release(&pvec_putback);
 +
 +	/* Phase 2: page munlock */
 +	for (i = 0; i < nr; i++) {
 +		struct page *page = pvec->pages[i];
 +
 +		if (page) {
 +			lock_page(page);
 +			if (!__putback_lru_fast_prepare(page, &pvec_putback,
 +					&pgrescued)) {
 +				/*
 +				 * Slow path. We don't want to lose the last
 +				 * pin before unlock_page()
 +				 */
 +				get_page(page); /* for putback_lru_page() */
 +				__munlock_isolated_page(page);
 +				unlock_page(page);
 +				put_page(page); /* from follow_page_mask() */
 +			}
 +		}
 +	}
  
 -	local_lock(&mlock_fbatch.lock);
 -	fbatch = this_cpu_ptr(&mlock_fbatch.fbatch);
  	/*
 -	 * folio_test_clear_mlocked(folio) must be left to __munlock_folio(),
 -	 * which will check whether the folio is multiply mlocked.
 +	 * Phase 3: page putback for pages that qualified for the fast path
 +	 * This will also call put_page() to return pin from follow_page_mask()
  	 */
 -	folio_get(folio);
 -	if (!folio_batch_add(fbatch, folio) ||
 -	    folio_test_large(folio) || lru_cache_disabled())
 -		mlock_folio_batch(fbatch);
 -	local_unlock(&mlock_fbatch.lock);
++<<<<<<< HEAD
 +	if (pagevec_count(&pvec_putback))
 +		__putback_lru_fast(&pvec_putback, pgrescued);
  }
++=======
++	if (newflags & VM_LOCKED)
++		newflags |= VM_IO;
++	vm_flags_reset(vma, newflags);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  
 -static int mlock_pte_range(pmd_t *pmd, unsigned long addr,
 -			   unsigned long end, struct mm_walk *walk)
 -
 +/*
 + * Fill up pagevec for __munlock_pagevec using pte walk
 + *
 + * The function expects that the struct page corresponding to @start address is
 + * a non-TPH page already pinned and in the @pvec, and that it belongs to @zone.
 + *
 + * The rest of @pvec is filled by subsequent pages within the same pmd and same
 + * zone, as long as the pte's are present and vm_normal_page() succeeds. These
 + * pages also get pinned.
 + *
 + * Returns the address of the next page that should be scanned. This equals
 + * @start + PAGE_SIZE when no page could be added by the pte walk.
 + */
 +static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,
 +			struct vm_area_struct *vma, struct zone *zone,
 +			unsigned long start, unsigned long end)
  {
 -	struct vm_area_struct *vma = walk->vma;
 +	pte_t *pte;
  	spinlock_t *ptl;
 -	pte_t *start_pte, *pte;
 -	struct folio *folio;
  
 -	ptl = pmd_trans_huge_lock(pmd, vma);
 -	if (ptl) {
 -		if (!pmd_present(*pmd))
 -			goto out;
 -		if (is_huge_zero_pmd(*pmd))
 -			goto out;
 -		folio = page_folio(pmd_page(*pmd));
 -		if (vma->vm_flags & VM_LOCKED)
 -			mlock_folio(folio);
 -		else
 -			munlock_folio(folio);
 -		goto out;
 -	}
++<<<<<<< HEAD
 +	/*
 +	 * Initialize pte walk starting at the already pinned page where we
 +	 * are sure that there is a pte, as it was pinned under the same
 +	 * mmap_lock write op.
 +	 */
 +	pte = get_locked_pte(vma->vm_mm, start,	&ptl);
 +	/* Make sure we do not cross the page table boundary */
 +	end = pgd_addr_end(start, end);
 +	end = p4d_addr_end(start, end);
 +	end = pud_addr_end(start, end);
 +	end = pmd_addr_end(start, end);
 +
 +	/* The page next to the pinned page is the first we will try to get */
 +	start += PAGE_SIZE;
 +	while (start < end) {
 +		struct page *page = NULL;
 +		pte++;
 +		if (pte_present(*pte))
 +			page = vm_normal_page(vma, start, *pte);
 +		/*
 +		 * Break if page could not be obtained or the page's node+zone does not
 +		 * match
 +		 */
 +		if (!page || page_zone(page) != zone)
 +			break;
  
 -	start_pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 -	for (pte = start_pte; addr != end; pte++, addr += PAGE_SIZE) {
 -		if (!pte_present(*pte))
 -			continue;
 -		folio = vm_normal_folio(vma, addr, *pte);
 -		if (!folio || folio_is_zone_device(folio))
 -			continue;
 -		if (folio_test_large(folio))
 -			continue;
 -		if (vma->vm_flags & VM_LOCKED)
 -			mlock_folio(folio);
 -		else
 -			munlock_folio(folio);
 +		/*
 +		 * Do not use pagevec for PTE-mapped THP,
 +		 * munlock_vma_pages_range() will handle them.
 +		 */
 +		if (PageTransCompound(page))
 +			break;
 +
 +		get_page(page);
 +		/*
 +		 * Increase the address that will be returned *before* the
 +		 * eventual break due to pvec becoming full by adding the page
 +		 */
 +		start += PAGE_SIZE;
 +		if (pagevec_add(pvec, page) == 0)
 +			break;
  	}
 -	pte_unmap(start_pte);
 -out:
 -	spin_unlock(ptl);
 -	cond_resched();
 -	return 0;
 +	pte_unmap_unlock(pte, ptl);
 +	return start;
  }
  
  /*
 - * mlock_vma_pages_range() - mlock any pages already in the range,
 - *                           or munlock all pages in the range.
 - * @vma - vma containing range to be mlock()ed or munlock()ed
 + * munlock_vma_pages_range() - munlock all pages in the vma range.'
 + * @vma - vma containing range to be munlock()ed.
   * @start - start address in @vma of the range
 - * @end - end of range in @vma
 - * @newflags - the new set of flags for @vma.
 + * @end - end of range in @vma.
   *
 - * Called for mlock(), mlock2() and mlockall(), to set @vma VM_LOCKED;
 - * called for munlock() and munlockall(), to clear VM_LOCKED from @vma.
 + *  For mremap(), munmap() and exit().
 + *
 + * Called with @vma VM_LOCKED.
 + *
 + * Returns with VM_LOCKED cleared.  Callers must be prepared to
 + * deal with this.
 + *
 + * We don't save and restore VM_LOCKED here because pages are
 + * still on lru.  In unmap path, pages might be scanned by reclaim
 + * and re-mlocked by try_to_{munlock|unmap} before we unmap and
 + * free them.  This will result in freeing mlocked pages.
   */
 -static void mlock_vma_pages_range(struct vm_area_struct *vma,
 -	unsigned long start, unsigned long end, vm_flags_t newflags)
 +void munlock_vma_pages_range(struct vm_area_struct *vma,
 +			     unsigned long start, unsigned long end)
  {
 -	static const struct mm_walk_ops mlock_walk_ops = {
 -		.pmd_entry = mlock_pte_range,
 -	};
 -
 -	/*
 -	 * There is a slight chance that concurrent page migration,
 -	 * or page reclaim finding a page of this now-VM_LOCKED vma,
 -	 * will call mlock_vma_folio() and raise page's mlock_count:
 -	 * double counting, leaving the page unevictable indefinitely.
 -	 * Communicate this danger to mlock_vma_folio() with VM_IO,
 -	 * which is a VM_SPECIAL flag not allowed on VM_LOCKED vmas.
 -	 * mmap_lock is held in write mode here, so this weird
 -	 * combination should not be visible to other mmap_lock users;
 -	 * but WRITE_ONCE so rmap walkers must see VM_IO if VM_LOCKED.
 -	 */
 -	if (newflags & VM_LOCKED)
 -		newflags |= VM_IO;
 -	vm_flags_reset(vma, newflags);
 +	vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
  
 -	lru_add_drain();
 -	walk_page_range(vma->vm_mm, start, end, &mlock_walk_ops, NULL);
 -	lru_add_drain();
 +	while (start < end) {
 +		struct page *page;
 +		unsigned int page_mask = 0;
 +		unsigned long page_increm;
 +		struct pagevec pvec;
 +		struct zone *zone;
  
 +		pagevec_init(&pvec);
 +		/*
 +		 * Although FOLL_DUMP is intended for get_dump_page(),
 +		 * it just so happens that its special treatment of the
 +		 * ZERO_PAGE (returning an error instead of doing get_page)
 +		 * suits munlock very well (and if somehow an abnormal page
 +		 * has sneaked into the range, we won't oops here: great).
 +		 */
 +		page = follow_page(vma, start, FOLL_GET | FOLL_DUMP);
 +
 +		if (page && !IS_ERR(page)) {
 +			if (PageTransTail(page)) {
 +				VM_BUG_ON_PAGE(PageMlocked(page), page);
 +				put_page(page); /* follow_page_mask() */
 +			} else if (PageTransHuge(page)) {
 +				lock_page(page);
 +				/*
 +				 * Any THP page found by follow_page_mask() may
 +				 * have gotten split before reaching
 +				 * munlock_vma_page(), so we need to compute
 +				 * the page_mask here instead.
 +				 */
 +				page_mask = munlock_vma_page(page);
 +				unlock_page(page);
 +				put_page(page); /* follow_page_mask() */
 +			} else {
 +				/*
 +				 * Non-huge pages are handled in batches via
 +				 * pagevec. The pin from follow_page_mask()
 +				 * prevents them from collapsing by THP.
 +				 */
 +				pagevec_add(&pvec, page);
 +				zone = page_zone(page);
 +
 +				/*
 +				 * Try to fill the rest of pagevec using fast
 +				 * pte walk. This will also update start to
 +				 * the next page to process. Then munlock the
 +				 * pagevec.
 +				 */
 +				start = __munlock_pagevec_fill(&pvec, vma,
 +						zone, start, end);
 +				__munlock_pagevec(&pvec, zone);
 +				goto next;
 +			}
 +		}
 +		page_increm = 1 + page_mask;
 +		start += page_increm * PAGE_SIZE;
 +next:
 +		cond_resched();
++=======
+ 	if (newflags & VM_IO) {
+ 		newflags &= ~VM_IO;
+ 		vm_flags_reset(vma, newflags);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	}
  }
  
@@@ -545,11 -455,12 +557,20 @@@ success
  	 * set VM_LOCKED, populate_vma_page_range will bring it back.
  	 */
  
++<<<<<<< HEAD
 +	if (lock)
 +		vma->vm_flags = newflags;
 +	else
 +		munlock_vma_pages_range(vma, start, end);
 +
++=======
+ 	if ((newflags & VM_LOCKED) && (oldflags & VM_LOCKED)) {
+ 		/* No work to do, and mlocking twice would be wrong */
+ 		vm_flags_reset(vma, newflags);
+ 	} else {
+ 		mlock_vma_pages_range(vma, start, end, newflags);
+ 	}
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  out:
  	*prev = vma;
  	return ret;
diff --cc mm/mmap.c
index c762f8e19537,33c638c7ec04..000000000000
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@@ -2830,7 -2451,267 +2830,265 @@@ int __do_munmap(struct mm_struct *mm, u
  int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
  	      struct list_head *uf)
  {
++<<<<<<< HEAD
 +	return __do_munmap(mm, start, len, uf, false);
++=======
+ 	VMA_ITERATOR(vmi, mm, start);
+ 
+ 	return do_vmi_munmap(&vmi, mm, start, len, uf, false);
+ }
+ 
+ unsigned long mmap_region(struct file *file, unsigned long addr,
+ 		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
+ 		struct list_head *uf)
+ {
+ 	struct mm_struct *mm = current->mm;
+ 	struct vm_area_struct *vma = NULL;
+ 	struct vm_area_struct *next, *prev, *merge;
+ 	pgoff_t pglen = len >> PAGE_SHIFT;
+ 	unsigned long charged = 0;
+ 	unsigned long end = addr + len;
+ 	unsigned long merge_start = addr, merge_end = end;
+ 	pgoff_t vm_pgoff;
+ 	int error;
+ 	VMA_ITERATOR(vmi, mm, addr);
+ 
+ 	/* Check against address space limit. */
+ 	if (!may_expand_vm(mm, vm_flags, len >> PAGE_SHIFT)) {
+ 		unsigned long nr_pages;
+ 
+ 		/*
+ 		 * MAP_FIXED may remove pages of mappings that intersects with
+ 		 * requested mapping. Account for the pages it would unmap.
+ 		 */
+ 		nr_pages = count_vma_pages_range(mm, addr, end);
+ 
+ 		if (!may_expand_vm(mm, vm_flags,
+ 					(len >> PAGE_SHIFT) - nr_pages))
+ 			return -ENOMEM;
+ 	}
+ 
+ 	/* Unmap any existing mapping in the area */
+ 	if (do_vmi_munmap(&vmi, mm, addr, len, uf, false))
+ 		return -ENOMEM;
+ 
+ 	/*
+ 	 * Private writable mapping: check memory availability
+ 	 */
+ 	if (accountable_mapping(file, vm_flags)) {
+ 		charged = len >> PAGE_SHIFT;
+ 		if (security_vm_enough_memory_mm(mm, charged))
+ 			return -ENOMEM;
+ 		vm_flags |= VM_ACCOUNT;
+ 	}
+ 
+ 	next = vma_next(&vmi);
+ 	prev = vma_prev(&vmi);
+ 	if (vm_flags & VM_SPECIAL)
+ 		goto cannot_expand;
+ 
+ 	/* Attempt to expand an old mapping */
+ 	/* Check next */
+ 	if (next && next->vm_start == end && !vma_policy(next) &&
+ 	    can_vma_merge_before(next, vm_flags, NULL, file, pgoff+pglen,
+ 				 NULL_VM_UFFD_CTX, NULL)) {
+ 		merge_end = next->vm_end;
+ 		vma = next;
+ 		vm_pgoff = next->vm_pgoff - pglen;
+ 	}
+ 
+ 	/* Check prev */
+ 	if (prev && prev->vm_end == addr && !vma_policy(prev) &&
+ 	    (vma ? can_vma_merge_after(prev, vm_flags, vma->anon_vma, file,
+ 				       pgoff, vma->vm_userfaultfd_ctx, NULL) :
+ 		   can_vma_merge_after(prev, vm_flags, NULL, file, pgoff,
+ 				       NULL_VM_UFFD_CTX, NULL))) {
+ 		merge_start = prev->vm_start;
+ 		vma = prev;
+ 		vm_pgoff = prev->vm_pgoff;
+ 	}
+ 
+ 
+ 	/* Actually expand, if possible */
+ 	if (vma &&
+ 	    !vma_expand(&vmi, vma, merge_start, merge_end, vm_pgoff, next)) {
+ 		khugepaged_enter_vma(vma, vm_flags);
+ 		goto expanded;
+ 	}
+ 
+ cannot_expand:
+ 	/*
+ 	 * Determine the object being mapped and call the appropriate
+ 	 * specific mapper. the address has already been validated, but
+ 	 * not unmapped, but the maps are removed from the list.
+ 	 */
+ 	vma = vm_area_alloc(mm);
+ 	if (!vma) {
+ 		error = -ENOMEM;
+ 		goto unacct_error;
+ 	}
+ 
+ 	vma_iter_set(&vmi, addr);
+ 	vma->vm_start = addr;
+ 	vma->vm_end = end;
+ 	vm_flags_init(vma, vm_flags);
+ 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
+ 	vma->vm_pgoff = pgoff;
+ 
+ 	if (file) {
+ 		if (vm_flags & VM_SHARED) {
+ 			error = mapping_map_writable(file->f_mapping);
+ 			if (error)
+ 				goto free_vma;
+ 		}
+ 
+ 		vma->vm_file = get_file(file);
+ 		error = call_mmap(file, vma);
+ 		if (error)
+ 			goto unmap_and_free_vma;
+ 
+ 		/*
+ 		 * Expansion is handled above, merging is handled below.
+ 		 * Drivers should not alter the address of the VMA.
+ 		 */
+ 		error = -EINVAL;
+ 		if (WARN_ON((addr != vma->vm_start)))
+ 			goto close_and_free_vma;
+ 
+ 		vma_iter_set(&vmi, addr);
+ 		/*
+ 		 * If vm_flags changed after call_mmap(), we should try merge
+ 		 * vma again as we may succeed this time.
+ 		 */
+ 		if (unlikely(vm_flags != vma->vm_flags && prev)) {
+ 			merge = vma_merge(&vmi, mm, prev, vma->vm_start,
+ 				    vma->vm_end, vma->vm_flags, NULL,
+ 				    vma->vm_file, vma->vm_pgoff, NULL,
+ 				    NULL_VM_UFFD_CTX, NULL);
+ 			if (merge) {
+ 				/*
+ 				 * ->mmap() can change vma->vm_file and fput
+ 				 * the original file. So fput the vma->vm_file
+ 				 * here or we would add an extra fput for file
+ 				 * and cause general protection fault
+ 				 * ultimately.
+ 				 */
+ 				fput(vma->vm_file);
+ 				vm_area_free(vma);
+ 				vma = merge;
+ 				/* Update vm_flags to pick up the change. */
+ 				vm_flags = vma->vm_flags;
+ 				goto unmap_writable;
+ 			}
+ 		}
+ 
+ 		vm_flags = vma->vm_flags;
+ 	} else if (vm_flags & VM_SHARED) {
+ 		error = shmem_zero_setup(vma);
+ 		if (error)
+ 			goto free_vma;
+ 	} else {
+ 		vma_set_anonymous(vma);
+ 	}
+ 
+ 	if (map_deny_write_exec(vma, vma->vm_flags)) {
+ 		error = -EACCES;
+ 		if (file)
+ 			goto close_and_free_vma;
+ 		else if (vma->vm_file)
+ 			goto unmap_and_free_vma;
+ 		else
+ 			goto free_vma;
+ 	}
+ 
+ 	/* Allow architectures to sanity-check the vm_flags */
+ 	error = -EINVAL;
+ 	if (!arch_validate_flags(vma->vm_flags))
+ 		goto close_and_free_vma;
+ 
+ 	error = -ENOMEM;
+ 	if (vma_iter_prealloc(&vmi))
+ 		goto close_and_free_vma;
+ 
+ 	if (vma->vm_file)
+ 		i_mmap_lock_write(vma->vm_file->f_mapping);
+ 
+ 	vma_iter_store(&vmi, vma);
+ 	mm->map_count++;
+ 	if (vma->vm_file) {
+ 		if (vma->vm_flags & VM_SHARED)
+ 			mapping_allow_writable(vma->vm_file->f_mapping);
+ 
+ 		flush_dcache_mmap_lock(vma->vm_file->f_mapping);
+ 		vma_interval_tree_insert(vma, &vma->vm_file->f_mapping->i_mmap);
+ 		flush_dcache_mmap_unlock(vma->vm_file->f_mapping);
+ 		i_mmap_unlock_write(vma->vm_file->f_mapping);
+ 	}
+ 
+ 	/*
+ 	 * vma_merge() calls khugepaged_enter_vma() either, the below
+ 	 * call covers the non-merge case.
+ 	 */
+ 	khugepaged_enter_vma(vma, vma->vm_flags);
+ 
+ 	/* Once vma denies write, undo our temporary denial count */
+ unmap_writable:
+ 	if (file && vm_flags & VM_SHARED)
+ 		mapping_unmap_writable(file->f_mapping);
+ 	file = vma->vm_file;
+ expanded:
+ 	perf_event_mmap(vma);
+ 
+ 	vm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);
+ 	if (vm_flags & VM_LOCKED) {
+ 		if ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||
+ 					is_vm_hugetlb_page(vma) ||
+ 					vma == get_gate_vma(current->mm))
+ 			vm_flags_clear(vma, VM_LOCKED_MASK);
+ 		else
+ 			mm->locked_vm += (len >> PAGE_SHIFT);
+ 	}
+ 
+ 	if (file)
+ 		uprobe_mmap(vma);
+ 
+ 	/*
+ 	 * New (or expanded) vma always get soft dirty status.
+ 	 * Otherwise user-space soft-dirty page tracker won't
+ 	 * be able to distinguish situation when vma area unmapped,
+ 	 * then new mapped in-place (which must be aimed as
+ 	 * a completely new data area).
+ 	 */
+ 	vm_flags_set(vma, VM_SOFTDIRTY);
+ 
+ 	vma_set_page_prot(vma);
+ 
+ 	validate_mm(mm);
+ 	return addr;
+ 
+ close_and_free_vma:
+ 	if (file && vma->vm_ops && vma->vm_ops->close)
+ 		vma->vm_ops->close(vma);
+ 
+ 	if (file || vma->vm_file) {
+ unmap_and_free_vma:
+ 		fput(vma->vm_file);
+ 		vma->vm_file = NULL;
+ 
+ 		/* Undo any partial mapping done by a device driver. */
+ 		unmap_region(mm, &mm->mm_mt, vma, prev, next, vma->vm_start,
+ 			     vma->vm_end);
+ 	}
+ 	if (file && (vm_flags & VM_SHARED))
+ 		mapping_unmap_writable(file->f_mapping);
+ free_vma:
+ 	vm_area_free(vma);
+ unacct_error:
+ 	if (charged)
+ 		vm_unacct_memory(charged);
+ 	validate_mm(mm);
+ 	return error;
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  }
  
  static int __vm_munmap(unsigned long start, size_t len, bool downgrade)
@@@ -3015,36 -2895,58 +3273,68 @@@ static int do_brk_flags(unsigned long a
  	if (security_vm_enough_memory_mm(mm, len >> PAGE_SHIFT))
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	/* Can we just expand an old private anonymous mapping? */
 +	vma = vma_merge(mm, prev, addr, addr + len, flags,
 +			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);
 +	if (vma)
++=======
+ 	/*
+ 	 * Expand the existing vma if possible; Note that singular lists do not
+ 	 * occur after forking, so the expand will only happen on new VMAs.
+ 	 */
+ 	if (vma && vma->vm_end == addr && !vma_policy(vma) &&
+ 	    can_vma_merge_after(vma, flags, NULL, NULL,
+ 				addr >> PAGE_SHIFT, NULL_VM_UFFD_CTX, NULL)) {
+ 		if (vma_iter_prealloc(vmi))
+ 			goto unacct_fail;
+ 
+ 		vma_adjust_trans_huge(vma, vma->vm_start, addr + len, 0);
+ 		init_vma_prep(&vp, vma);
+ 		vma_prepare(&vp);
+ 		vma->vm_end = addr + len;
+ 		vm_flags_set(vma, VM_SOFTDIRTY);
+ 		vma_iter_store(vmi, vma);
+ 
+ 		vma_complete(&vp, vmi, mm);
+ 		khugepaged_enter_vma(vma, flags);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  		goto out;
 -	}
  
 -	/* create a vma struct for an anonymous mapping */
 +	/*
 +	 * create a vma struct for an anonymous mapping
 +	 */
  	vma = vm_area_alloc(mm);
 -	if (!vma)
 -		goto unacct_fail;
 +	if (!vma) {
 +		vm_unacct_memory(len >> PAGE_SHIFT);
 +		return -ENOMEM;
 +	}
  
  	vma_set_anonymous(vma);
  	vma->vm_start = addr;
  	vma->vm_end = addr + len;
++<<<<<<< HEAD
 +	vma->vm_pgoff = pgoff;
 +	vma->vm_flags = flags;
++=======
+ 	vma->vm_pgoff = addr >> PAGE_SHIFT;
+ 	vm_flags_init(vma, flags);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	vma->vm_page_prot = vm_get_page_prot(flags);
 -	if (vma_iter_store_gfp(vmi, vma, GFP_KERNEL))
 -		goto mas_store_fail;
 -
 -	mm->map_count++;
 +	vma_link(mm, vma, prev, rb_link, rb_parent);
  out:
  	perf_event_mmap(vma);
  	mm->total_vm += len >> PAGE_SHIFT;
  	mm->data_vm += len >> PAGE_SHIFT;
  	if (flags & VM_LOCKED)
  		mm->locked_vm += (len >> PAGE_SHIFT);
++<<<<<<< HEAD
 +	vma->vm_flags |= VM_SOFTDIRTY;
++=======
+ 	vm_flags_set(vma, VM_SOFTDIRTY);
+ 	validate_mm(mm);
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	return 0;
 -
 -mas_store_fail:
 -	vm_area_free(vma);
 -unacct_fail:
 -	vm_unacct_memory(len >> PAGE_SHIFT);
 -	return -ENOMEM;
  }
  
  int vm_brk_flags(unsigned long addr, unsigned long request, unsigned long flags)
diff --cc mm/mprotect.c
index 87f0e57bbe11,1d4843c97c2a..000000000000
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@@ -480,12 -670,12 +480,18 @@@ success
  	 * vm_flags and vm_page_prot are protected by the mmap_lock
  	 * held in write mode.
  	 */
++<<<<<<< HEAD
 +	vma->vm_flags = newflags;
 +	dirty_accountable = vma_wants_writenotify(vma, vma->vm_page_prot);
++=======
+ 	vm_flags_reset(vma, newflags);
+ 	if (vma_wants_manual_pte_write_upgrade(vma))
+ 		mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	vma_set_page_prot(vma);
  
 -	change_protection(tlb, vma, start, end, mm_cp_flags);
 +	change_protection(vma, start, end, vma->vm_page_prot,
 +			  dirty_accountable ? MM_CP_DIRTY_ACCT : 0);
  
  	/*
  	 * Private VM_LOCKED VMA becoming writable: trigger COW to avoid major
diff --cc mm/mremap.c
index cead8ecce38a,411a85682b58..000000000000
--- a/mm/mremap.c
+++ b/mm/mremap.c
@@@ -616,12 -661,12 +616,21 @@@ static unsigned long move_vma(struct vm
  	}
  
  	/* Conceal VM_ACCOUNT so old reservation is not undone */
++<<<<<<< HEAD
 +	if (vm_flags & VM_ACCOUNT) {
 +		vma->vm_flags &= ~VM_ACCOUNT;
 +		excess = vma->vm_end - vma->vm_start - old_len;
 +		if (old_addr > vma->vm_start &&
 +		    old_addr + old_len < vma->vm_end)
 +			split = 1;
++=======
+ 	if (vm_flags & VM_ACCOUNT && !(flags & MREMAP_DONTUNMAP)) {
+ 		vm_flags_clear(vma, VM_ACCOUNT);
+ 		if (vma->vm_start < old_addr)
+ 			account_start = vma->vm_start;
+ 		if (vma->vm_end > old_addr + old_len)
+ 			account_end = vma->vm_end;
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	}
  
  	/*
@@@ -659,6 -714,19 +668,22 @@@
  		*locked = true;
  	}
  
++<<<<<<< HEAD
++=======
+ 	mm->hiwater_vm = hiwater_vm;
+ 
+ 	/* Restore VM_ACCOUNT if one or two pieces of vma left */
+ 	if (account_start) {
+ 		vma = vma_prev(&vmi);
+ 		vm_flags_set(vma, VM_ACCOUNT);
+ 	}
+ 
+ 	if (account_end) {
+ 		vma = vma_next(&vmi);
+ 		vm_flags_set(vma, VM_ACCOUNT);
+ 	}
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	return new_addr;
  }
  
diff --cc mm/shmem.c
index 6663310c30e0,732969afabd1..000000000000
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@@ -2206,13 -2295,23 +2206,27 @@@ out_nomem
  
  static int shmem_mmap(struct file *file, struct vm_area_struct *vma)
  {
++<<<<<<< HEAD
++=======
+ 	struct inode *inode = file_inode(file);
+ 	struct shmem_inode_info *info = SHMEM_I(inode);
+ 	int ret;
+ 
+ 	ret = seal_check_future_write(info->seals, vma);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* arm64 - allow memory tagging on RAM-based files */
+ 	vm_flags_set(vma, VM_MTE_ALLOWED);
+ 
++>>>>>>> 1c71222e5f23 (mm: replace vma->vm_flags direct modifications with modifier calls)
  	file_accessed(file);
 -	/* This is anonymous shared memory if it is unlinked at the time of mmap */
 -	if (inode->i_nlink)
 -		vma->vm_ops = &shmem_vm_ops;
 -	else
 -		vma->vm_ops = &shmem_anon_vm_ops;
 +	vma->vm_ops = &shmem_vm_ops;
 +	if (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&
 +			((vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK) <
 +			(vma->vm_end & HPAGE_PMD_MASK)) {
 +		khugepaged_enter(vma, vma->vm_flags);
 +	}
  	return 0;
  }
  
* Unmerged path arch/loongarch/include/asm/tlb.h
* Unmerged path drivers/crypto/hisilicon/qm.c
* Unmerged path drivers/misc/habanalabs/common/memory.c
* Unmerged path drivers/misc/habanalabs/gaudi/gaudi.c
* Unmerged path drivers/misc/habanalabs/gaudi2/gaudi2.c
* Unmerged path drivers/misc/habanalabs/goya/goya.c
* Unmerged path drivers/misc/open-dice.c
* Unmerged path drivers/misc/uacce/uacce.c
* Unmerged path drivers/staging/media/atomisp/pci/hmm/hmm_bo.c
* Unmerged path drivers/vdpa/vdpa_user/iova_domain.c
* Unmerged path drivers/vfio/pci/vfio_pci_core.c
* Unmerged path fs/erofs/data.c
* Unmerged path mm/secretmem.c
* Unmerged path arch/arm/kernel/process.c
* Unmerged path arch/ia64/mm/init.c
* Unmerged path arch/loongarch/include/asm/tlb.h
diff --git a/arch/powerpc/kvm/book3s_xive_native.c b/arch/powerpc/kvm/book3s_xive_native.c
index a5278fac0e75..8d2fda15ea44 100644
--- a/arch/powerpc/kvm/book3s_xive_native.c
+++ b/arch/powerpc/kvm/book3s_xive_native.c
@@ -319,7 +319,7 @@ static int kvmppc_xive_native_mmap(struct kvm_device *dev,
 		return -EINVAL;
 	}
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached_wc(vma->vm_page_prot);
 
 	/*
* Unmerged path arch/powerpc/mm/book3s64/subpage_prot.c
diff --git a/arch/powerpc/platforms/book3s/vas-api.c b/arch/powerpc/platforms/book3s/vas-api.c
index 4fd03dcc4b0d..711f9f17bdf6 100644
--- a/arch/powerpc/platforms/book3s/vas-api.c
+++ b/arch/powerpc/platforms/book3s/vas-api.c
@@ -526,7 +526,7 @@ static int coproc_mmap(struct file *fp, struct vm_area_struct *vma)
 	pfn = paste_addr >> PAGE_SHIFT;
 
 	/* flags, page_prot from cxl_mmap(), except we want cachable */
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_cached(vma->vm_page_prot);
 
 	prot = __pgprot(pgprot_val(vma->vm_page_prot) | _PAGE_DIRTY);
diff --git a/arch/powerpc/platforms/cell/spufs/file.c b/arch/powerpc/platforms/cell/spufs/file.c
index c7c6d7065868..14e6e3f39d94 100644
--- a/arch/powerpc/platforms/cell/spufs/file.c
+++ b/arch/powerpc/platforms/cell/spufs/file.c
@@ -297,7 +297,7 @@ static int spufs_mem_mmap(struct file *file, struct vm_area_struct *vma)
 	if (!(vma->vm_flags & VM_SHARED))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached_wc(vma->vm_page_prot);
 
 	vma->vm_ops = &spufs_mem_mmap_vmops;
@@ -387,7 +387,7 @@ static int spufs_cntl_mmap(struct file *file, struct vm_area_struct *vma)
 	if (!(vma->vm_flags & VM_SHARED))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	vma->vm_ops = &spufs_cntl_mmap_vmops;
@@ -1067,7 +1067,7 @@ static int spufs_signal1_mmap(struct file *file, struct vm_area_struct *vma)
 	if (!(vma->vm_flags & VM_SHARED))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	vma->vm_ops = &spufs_signal1_mmap_vmops;
@@ -1205,7 +1205,7 @@ static int spufs_signal2_mmap(struct file *file, struct vm_area_struct *vma)
 	if (!(vma->vm_flags & VM_SHARED))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	vma->vm_ops = &spufs_signal2_mmap_vmops;
@@ -1328,7 +1328,7 @@ static int spufs_mss_mmap(struct file *file, struct vm_area_struct *vma)
 	if (!(vma->vm_flags & VM_SHARED))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	vma->vm_ops = &spufs_mss_mmap_vmops;
@@ -1390,7 +1390,7 @@ static int spufs_psmap_mmap(struct file *file, struct vm_area_struct *vma)
 	if (!(vma->vm_flags & VM_SHARED))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	vma->vm_ops = &spufs_psmap_mmap_vmops;
@@ -1450,7 +1450,7 @@ static int spufs_mfc_mmap(struct file *file, struct vm_area_struct *vma)
 	if (!(vma->vm_flags & VM_SHARED))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
 	vma->vm_ops = &spufs_mfc_mmap_vmops;
* Unmerged path arch/s390/mm/gmap.c
* Unmerged path arch/x86/entry/vsyscall/vsyscall_64.c
diff --git a/arch/x86/kernel/cpu/sgx/driver.c b/arch/x86/kernel/cpu/sgx/driver.c
index 2e8d1b028d7e..85750bfbf762 100644
--- a/arch/x86/kernel/cpu/sgx/driver.c
+++ b/arch/x86/kernel/cpu/sgx/driver.c
@@ -101,7 +101,7 @@ static int sgx_mmap(struct file *file, struct vm_area_struct *vma)
 		return ret;
 
 	vma->vm_ops = &sgx_vm_ops;
-	vma->vm_flags |= VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP | VM_IO;
+	vm_flags_set(vma, VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP | VM_IO);
 	vma->vm_private_data = encl;
 
 	return 0;
diff --git a/arch/x86/kernel/cpu/sgx/virt.c b/arch/x86/kernel/cpu/sgx/virt.c
index 69965aff31b2..da00ebf65d0b 100644
--- a/arch/x86/kernel/cpu/sgx/virt.c
+++ b/arch/x86/kernel/cpu/sgx/virt.c
@@ -105,7 +105,7 @@ static int sgx_vepc_mmap(struct file *file, struct vm_area_struct *vma)
 
 	vma->vm_ops = &sgx_vepc_vm_ops;
 	/* Don't copy VMA in fork() */
-	vma->vm_flags |= VM_PFNMAP | VM_IO | VM_DONTDUMP | VM_DONTCOPY;
+	vm_flags_set(vma, VM_PFNMAP | VM_IO | VM_DONTDUMP | VM_DONTCOPY);
 	vma->vm_private_data = vepc;
 
 	return 0;
diff --git a/arch/x86/mm/pat/memtype.c b/arch/x86/mm/pat/memtype.c
index 15b3db3c7300..4c699bcf31a8 100644
--- a/arch/x86/mm/pat/memtype.c
+++ b/arch/x86/mm/pat/memtype.c
@@ -1036,7 +1036,7 @@ int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,
 
 		ret = reserve_pfn_range(paddr, size, prot, 0);
 		if (ret == 0 && vma)
-			vma->vm_flags |= VM_PAT;
+			vm_flags_set(vma, VM_PAT);
 		return ret;
 	}
 
@@ -1102,7 +1102,7 @@ void untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,
 	}
 	free_pfn_range(paddr, size);
 	if (vma)
-		vma->vm_flags &= ~VM_PAT;
+		vm_flags_clear(vma, VM_PAT);
 }
 
 /*
@@ -1112,7 +1112,7 @@ void untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,
  */
 void untrack_pfn_moved(struct vm_area_struct *vma)
 {
-	vma->vm_flags &= ~VM_PAT;
+	vm_flags_clear(vma, VM_PAT);
 }
 
 pgprot_t pgprot_writecombine(pgprot_t prot)
* Unmerged path arch/x86/um/mem_32.c
diff --git a/drivers/acpi/pfr_telemetry.c b/drivers/acpi/pfr_telemetry.c
index 9abf350bd7a5..dbd4b5ce5ebf 100644
--- a/drivers/acpi/pfr_telemetry.c
+++ b/drivers/acpi/pfr_telemetry.c
@@ -310,7 +310,7 @@ pfrt_log_mmap(struct file *file, struct vm_area_struct *vma)
 		return -EROFS;
 
 	/* changing from read to write with mprotect is not allowed */
-	vma->vm_flags &= ~VM_MAYWRITE;
+	vm_flags_clear(vma, VM_MAYWRITE);
 
 	pfrt_log_dev = to_pfrt_log_dev(file);
 
diff --git a/drivers/android/binder.c b/drivers/android/binder.c
index 7fe31c671fce..b5fee86a91ed 100644
--- a/drivers/android/binder.c
+++ b/drivers/android/binder.c
@@ -4727,8 +4727,7 @@ static int binder_mmap(struct file *filp, struct vm_area_struct *vma)
 		failure_string = "bad vm_flags";
 		goto err_bad_arg;
 	}
-	vma->vm_flags |= VM_DONTCOPY | VM_MIXEDMAP;
-	vma->vm_flags &= ~VM_MAYWRITE;
+	vm_flags_mod(vma, VM_DONTCOPY | VM_MIXEDMAP, VM_MAYWRITE);
 
 	vma->vm_ops = &binder_vm_ops;
 	vma->vm_private_data = proc;
* Unmerged path drivers/char/mspec.c
* Unmerged path drivers/crypto/hisilicon/qm.c
diff --git a/drivers/dax/device.c b/drivers/dax/device.c
index 41209ae0fbde..b37f59ab6f68 100644
--- a/drivers/dax/device.c
+++ b/drivers/dax/device.c
@@ -301,7 +301,7 @@ static int dax_mmap(struct file *filp, struct vm_area_struct *vma)
 		return rc;
 
 	vma->vm_ops = &dax_vm_ops;
-	vma->vm_flags |= VM_HUGEPAGE;
+	vm_flags_set(vma, VM_HUGEPAGE);
 	return 0;
 }
 
diff --git a/drivers/dma/idxd/cdev.c b/drivers/dma/idxd/cdev.c
index e13e92609943..674bfefca088 100644
--- a/drivers/dma/idxd/cdev.c
+++ b/drivers/dma/idxd/cdev.c
@@ -201,7 +201,7 @@ static int idxd_cdev_mmap(struct file *filp, struct vm_area_struct *vma)
 	if (rc < 0)
 		return rc;
 
-	vma->vm_flags |= VM_DONTCOPY;
+	vm_flags_set(vma, VM_DONTCOPY);
 	pfn = (base + idxd_get_wq_portal_full_offset(wq->id,
 				IDXD_PORTAL_LIMITED)) >> PAGE_SHIFT;
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
* Unmerged path drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
index 033fcd594edc..6b4a43ec94f6 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
@@ -2850,8 +2850,8 @@ static int kfd_mmio_mmap(struct kfd_dev *dev, struct kfd_process *process,
 
 	address = dev->adev->rmmio_remap.bus_addr;
 
-	vma->vm_flags |= VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE |
-				VM_DONTDUMP | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE |
+				VM_DONTDUMP | VM_PFNMAP);
 
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
* Unmerged path drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.c b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
index 729d26d648af..dd0436bf349a 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
@@ -1052,8 +1052,8 @@ int kfd_event_mmap(struct kfd_process *p, struct vm_area_struct *vma)
 	pfn = __pa(page->kernel_address);
 	pfn >>= PAGE_SHIFT;
 
-	vma->vm_flags |= VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE
-		       | VM_DONTDUMP | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE
+		       | VM_DONTDUMP | VM_PFNMAP);
 
 	pr_debug("Mapping signal page\n");
 	pr_debug("     start user address  == 0x%08lx\n", vma->vm_start);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process.c b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
index febf0e9f7af1..8569eb1cb44a 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
@@ -1983,8 +1983,8 @@ int kfd_reserved_mem_mmap(struct kfd_dev *dev, struct kfd_process *process,
 		return -ENOMEM;
 	}
 
-	vma->vm_flags |= VM_IO | VM_DONTCOPY | VM_DONTEXPAND
-		| VM_NORESERVE | VM_DONTDUMP | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_DONTCOPY | VM_DONTEXPAND
+		| VM_NORESERVE | VM_DONTDUMP | VM_PFNMAP);
 	/* Mapping pages to user process */
 	return remap_pfn_range(vma, vma->vm_start,
 			       PFN_DOWN(__pa(qpd->cwsr_kaddr)),
diff --git a/drivers/gpu/drm/drm_gem.c b/drivers/gpu/drm/drm_gem.c
index ad068865ba20..d5c783114a32 100644
--- a/drivers/gpu/drm/drm_gem.c
+++ b/drivers/gpu/drm/drm_gem.c
@@ -1045,7 +1045,7 @@ int drm_gem_mmap_obj(struct drm_gem_object *obj, unsigned long obj_size,
 			goto err_drm_gem_object_put;
 		}
 
-		vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
+		vm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
 		vma->vm_page_prot = pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
 		vma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);
 	}
diff --git a/drivers/gpu/drm/drm_gem_cma_helper.c b/drivers/gpu/drm/drm_gem_cma_helper.c
index 36d8237e79b2..6597f9f89b4e 100644
--- a/drivers/gpu/drm/drm_gem_cma_helper.c
+++ b/drivers/gpu/drm/drm_gem_cma_helper.c
@@ -531,8 +531,7 @@ int drm_gem_cma_mmap(struct drm_gem_cma_object *cma_obj, struct vm_area_struct *
 	 * the whole buffer.
 	 */
 	vma->vm_pgoff -= drm_vma_node_start(&obj->vma_node);
-	vma->vm_flags &= ~VM_PFNMAP;
-	vma->vm_flags |= VM_DONTEXPAND;
+	vm_flags_mod(vma, VM_DONTEXPAND, VM_PFNMAP);
 
 	if (cma_obj->map_noncoherent) {
 		vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
diff --git a/drivers/gpu/drm/drm_gem_shmem_helper.c b/drivers/gpu/drm/drm_gem_shmem_helper.c
index 619644680902..2fef22970a7c 100644
--- a/drivers/gpu/drm/drm_gem_shmem_helper.c
+++ b/drivers/gpu/drm/drm_gem_shmem_helper.c
@@ -631,7 +631,7 @@ int drm_gem_shmem_mmap(struct drm_gem_shmem_object *shmem, struct vm_area_struct
 	if (ret)
 		return ret;
 
-	vma->vm_flags |= VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 	if (shmem->map_wc)
 		vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
diff --git a/drivers/gpu/drm/drm_vm.c b/drivers/gpu/drm/drm_vm.c
index f024dc93939e..87c9fe55dec7 100644
--- a/drivers/gpu/drm/drm_vm.c
+++ b/drivers/gpu/drm/drm_vm.c
@@ -476,7 +476,7 @@ static int drm_mmap_dma(struct file *filp, struct vm_area_struct *vma)
 
 	if (!capable(CAP_SYS_ADMIN) &&
 	    (dma->flags & _DRM_DMA_USE_PCI_RO)) {
-		vma->vm_flags &= ~(VM_WRITE | VM_MAYWRITE);
+		vm_flags_clear(vma, VM_WRITE | VM_MAYWRITE);
 #if defined(__i386__) || defined(__x86_64__)
 		pgprot_val(vma->vm_page_prot) &= ~_PAGE_RW;
 #else
@@ -492,7 +492,7 @@ static int drm_mmap_dma(struct file *filp, struct vm_area_struct *vma)
 
 	vma->vm_ops = &drm_vm_dma_ops;
 
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 
 	drm_vm_open_locked(dev, vma);
 	return 0;
@@ -560,7 +560,7 @@ static int drm_mmap_locked(struct file *filp, struct vm_area_struct *vma)
 		return -EINVAL;
 
 	if (!capable(CAP_SYS_ADMIN) && (map->flags & _DRM_READ_ONLY)) {
-		vma->vm_flags &= ~(VM_WRITE | VM_MAYWRITE);
+		vm_flags_clear(vma, VM_WRITE | VM_MAYWRITE);
 #if defined(__i386__) || defined(__x86_64__)
 		pgprot_val(vma->vm_page_prot) &= ~_PAGE_RW;
 #else
@@ -628,7 +628,7 @@ static int drm_mmap_locked(struct file *filp, struct vm_area_struct *vma)
 	default:
 		return -EINVAL;	/* This should never happen. */
 	}
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 
 	drm_vm_open_locked(dev, vma);
 	return 0;
* Unmerged path drivers/gpu/drm/etnaviv/etnaviv_gem.c
* Unmerged path drivers/gpu/drm/exynos/exynos_drm_gem.c
diff --git a/drivers/gpu/drm/gma500/framebuffer.c b/drivers/gpu/drm/gma500/framebuffer.c
index aa3ecf771fd3..f013a75a75d4 100644
--- a/drivers/gpu/drm/gma500/framebuffer.c
+++ b/drivers/gpu/drm/gma500/framebuffer.c
@@ -139,7 +139,7 @@ static int psbfb_mmap(struct fb_info *info, struct vm_area_struct *vma)
 	 */
 	vma->vm_ops = &psbfb_vm_ops;
 	vma->vm_private_data = (void *)fb;
-	vma->vm_flags |= VM_IO | VM_MIXEDMAP | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_IO | VM_MIXEDMAP | VM_DONTEXPAND | VM_DONTDUMP);
 	return 0;
 }
 
diff --git a/drivers/gpu/drm/i810/i810_dma.c b/drivers/gpu/drm/i810/i810_dma.c
index 9fb4dd63342f..01967dd88762 100644
--- a/drivers/gpu/drm/i810/i810_dma.c
+++ b/drivers/gpu/drm/i810/i810_dma.c
@@ -102,7 +102,7 @@ static int i810_mmap_buffers(struct file *filp, struct vm_area_struct *vma)
 	buf = dev_priv->mmap_buffer;
 	buf_priv = buf->dev_private;
 
-	vma->vm_flags |= VM_DONTCOPY;
+	vm_flags_set(vma, VM_DONTCOPY);
 
 	buf_priv->currently_mapped = I810_BUF_MAPPED;
 
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_mman.c b/drivers/gpu/drm/i915/gem/i915_gem_mman.c
index 6f579cb8f2ff..a6a42fb00db1 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_mman.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_mman.c
@@ -963,7 +963,7 @@ int i915_gem_mmap(struct file *filp, struct vm_area_struct *vma)
 			i915_gem_object_put(obj);
 			return -EINVAL;
 		}
-		vma->vm_flags &= ~VM_MAYWRITE;
+		vm_flags_clear(vma, VM_MAYWRITE);
 	}
 
 	anon = mmap_singleton(to_i915(dev));
@@ -972,7 +972,7 @@ int i915_gem_mmap(struct file *filp, struct vm_area_struct *vma)
 		return PTR_ERR(anon);
 	}
 
-	vma->vm_flags |= VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP | VM_IO;
+	vm_flags_set(vma, VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP | VM_IO);
 
 	/*
 	 * We keep the ref on mmo->obj, not vm_file, but we require
* Unmerged path drivers/gpu/drm/mediatek/mtk_drm_gem.c
* Unmerged path drivers/gpu/drm/msm/msm_gem.c
diff --git a/drivers/gpu/drm/omapdrm/omap_gem.c b/drivers/gpu/drm/omapdrm/omap_gem.c
index b4d05da33c60..f2d800dd0338 100644
--- a/drivers/gpu/drm/omapdrm/omap_gem.c
+++ b/drivers/gpu/drm/omapdrm/omap_gem.c
@@ -578,8 +578,7 @@ int omap_gem_mmap_obj(struct drm_gem_object *obj,
 {
 	struct omap_gem_object *omap_obj = to_omap_bo(obj);
 
-	vma->vm_flags &= ~VM_PFNMAP;
-	vma->vm_flags |= VM_MIXEDMAP;
+	vm_flags_mod(vma, VM_MIXEDMAP, VM_PFNMAP);
 
 	if (omap_obj->flags & OMAP_BO_WC) {
 		vma->vm_page_prot = pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
* Unmerged path drivers/gpu/drm/rockchip/rockchip_drm_gem.c
diff --git a/drivers/gpu/drm/tegra/gem.c b/drivers/gpu/drm/tegra/gem.c
index ba0a1efc0e03..e0bf04d4ea93 100644
--- a/drivers/gpu/drm/tegra/gem.c
+++ b/drivers/gpu/drm/tegra/gem.c
@@ -435,7 +435,7 @@ int __tegra_gem_mmap(struct drm_gem_object *gem, struct vm_area_struct *vma)
 		 * and set the vm_pgoff (used as a fake buffer offset by DRM)
 		 * to 0 as we want to map the whole buffer.
 		 */
-		vma->vm_flags &= ~VM_PFNMAP;
+		vm_flags_clear(vma, VM_PFNMAP);
 		vma->vm_pgoff = 0;
 
 		err = dma_mmap_wc(gem->dev->dev, vma, bo->vaddr, bo->paddr,
@@ -449,8 +449,7 @@ int __tegra_gem_mmap(struct drm_gem_object *gem, struct vm_area_struct *vma)
 	} else {
 		pgprot_t prot = vm_get_page_prot(vma->vm_flags);
 
-		vma->vm_flags |= VM_MIXEDMAP;
-		vma->vm_flags &= ~VM_PFNMAP;
+		vm_flags_mod(vma, VM_MIXEDMAP, VM_PFNMAP);
 
 		vma->vm_page_prot = pgprot_writecombine(prot);
 	}
diff --git a/drivers/gpu/drm/ttm/ttm_bo_vm.c b/drivers/gpu/drm/ttm/ttm_bo_vm.c
index 38119311284d..429241c5b18b 100644
--- a/drivers/gpu/drm/ttm/ttm_bo_vm.c
+++ b/drivers/gpu/drm/ttm/ttm_bo_vm.c
@@ -468,8 +468,7 @@ int ttm_bo_mmap_obj(struct vm_area_struct *vma, struct ttm_buffer_object *bo)
 
 	vma->vm_private_data = bo;
 
-	vma->vm_flags |= VM_PFNMAP;
-	vma->vm_flags |= VM_IO | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_PFNMAP | VM_IO | VM_DONTEXPAND | VM_DONTDUMP);
 	return 0;
 }
 EXPORT_SYMBOL(ttm_bo_mmap_obj);
diff --git a/drivers/gpu/drm/virtio/virtgpu_vram.c b/drivers/gpu/drm/virtio/virtgpu_vram.c
index 6b45b0429fef..25df81c02783 100644
--- a/drivers/gpu/drm/virtio/virtgpu_vram.c
+++ b/drivers/gpu/drm/virtio/virtgpu_vram.c
@@ -46,7 +46,7 @@ static int virtio_gpu_vram_mmap(struct drm_gem_object *obj,
 		return -EINVAL;
 
 	vma->vm_pgoff -= drm_vma_node_start(&obj->vma_node);
-	vma->vm_flags |= VM_MIXEDMAP | VM_DONTEXPAND;
+	vm_flags_set(vma, VM_MIXEDMAP | VM_DONTEXPAND);
 	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 	vma->vm_page_prot = pgprot_decrypted(vma->vm_page_prot);
 	vma->vm_ops = &virtio_gpu_vram_vm_ops;
diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_ttm_glue.c b/drivers/gpu/drm/vmwgfx/vmwgfx_ttm_glue.c
index 265f7c48d856..90097d04b45f 100644
--- a/drivers/gpu/drm/vmwgfx/vmwgfx_ttm_glue.c
+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_ttm_glue.c
@@ -97,7 +97,7 @@ int vmw_mmap(struct file *filp, struct vm_area_struct *vma)
 
 	/* Use VM_PFNMAP rather than VM_MIXEDMAP if not a COW mapping */
 	if (!is_cow_mapping(vma->vm_flags))
-		vma->vm_flags = (vma->vm_flags & ~VM_MIXEDMAP) | VM_PFNMAP;
+		vm_flags_mod(vma, VM_PFNMAP, VM_MIXEDMAP);
 
 	ttm_bo_put(bo); /* release extra ref taken by ttm_bo_mmap_obj() */
 
diff --git a/drivers/gpu/drm/xen/xen_drm_front_gem.c b/drivers/gpu/drm/xen/xen_drm_front_gem.c
index e31554d7139f..339efaa949e9 100644
--- a/drivers/gpu/drm/xen/xen_drm_front_gem.c
+++ b/drivers/gpu/drm/xen/xen_drm_front_gem.c
@@ -70,8 +70,7 @@ static int xen_drm_front_gem_object_mmap(struct drm_gem_object *gem_obj,
 	 * vm_pgoff (used as a fake buffer offset by DRM) to 0 as we want to map
 	 * the whole buffer.
 	 */
-	vma->vm_flags &= ~VM_PFNMAP;
-	vma->vm_flags |= VM_MIXEDMAP | VM_DONTEXPAND;
+	vm_flags_mod(vma, VM_MIXEDMAP | VM_DONTEXPAND, VM_PFNMAP);
 	vma->vm_pgoff = 0;
 
 	/*
diff --git a/drivers/hsi/clients/cmt_speech.c b/drivers/hsi/clients/cmt_speech.c
index a1d4b9366496..35e4b89d3eda 100644
--- a/drivers/hsi/clients/cmt_speech.c
+++ b/drivers/hsi/clients/cmt_speech.c
@@ -1278,7 +1278,7 @@ static int cs_char_mmap(struct file *file, struct vm_area_struct *vma)
 	if (vma_pages(vma) != 1)
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_DONTDUMP | VM_DONTEXPAND;
+	vm_flags_set(vma, VM_IO | VM_DONTDUMP | VM_DONTEXPAND);
 	vma->vm_ops = &cs_char_vm_ops;
 	vma->vm_private_data = file->private_data;
 
diff --git a/drivers/hwtracing/intel_th/msu.c b/drivers/hwtracing/intel_th/msu.c
index b717f9e0ef6f..2caa7935d55c 100644
--- a/drivers/hwtracing/intel_th/msu.c
+++ b/drivers/hwtracing/intel_th/msu.c
@@ -1633,7 +1633,7 @@ static int intel_th_msc_mmap(struct file *file, struct vm_area_struct *vma)
 		atomic_dec(&msc->user_count);
 
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTCOPY;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTCOPY);
 	vma->vm_ops = &msc_mmap_ops;
 	return ret;
 }
diff --git a/drivers/hwtracing/stm/core.c b/drivers/hwtracing/stm/core.c
index 2712e699ba08..534fbefc7f6a 100644
--- a/drivers/hwtracing/stm/core.c
+++ b/drivers/hwtracing/stm/core.c
@@ -715,7 +715,7 @@ static int stm_char_mmap(struct file *file, struct vm_area_struct *vma)
 	pm_runtime_get_sync(&stm->dev);
 
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
-	vma->vm_flags |= VM_IO | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_IO | VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &stm_mmap_vmops;
 	vm_iomap_memory(vma, phys, size);
 
* Unmerged path drivers/infiniband/hw/hfi1/file_ops.c
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index 2164c763a9cf..3d06ca626ca7 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -2092,7 +2092,7 @@ static int mlx5_ib_mmap_clock_info_page(struct mlx5_ib_dev *dev,
 
 	if (vma->vm_flags & (VM_WRITE | VM_EXEC))
 		return -EPERM;
-	vma->vm_flags &= ~VM_MAYWRITE;
+	vm_flags_clear(vma, VM_MAYWRITE);
 
 	if (!dev->mdev->clock_info)
 		return -EOPNOTSUPP;
@@ -2316,7 +2316,7 @@ static int mlx5_ib_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vm
 
 		if (vma->vm_flags & VM_WRITE)
 			return -EPERM;
-		vma->vm_flags &= ~VM_MAYWRITE;
+		vm_flags_clear(vma, VM_MAYWRITE);
 
 		/* Don't expose to user-space information it shouldn't have */
 		if (PAGE_SIZE > 4096)
diff --git a/drivers/infiniband/hw/qib/qib_file_ops.c b/drivers/infiniband/hw/qib/qib_file_ops.c
index 9d247d8bb4f2..d5ae8a11ebfc 100644
--- a/drivers/infiniband/hw/qib/qib_file_ops.c
+++ b/drivers/infiniband/hw/qib/qib_file_ops.c
@@ -733,7 +733,7 @@ static int qib_mmap_mem(struct vm_area_struct *vma, struct qib_ctxtdata *rcd,
 		}
 
 		/* don't allow them to later change with mprotect */
-		vma->vm_flags &= ~VM_MAYWRITE;
+		vm_flags_clear(vma, VM_MAYWRITE);
 	}
 
 	pfn = virt_to_phys(kvaddr) >> PAGE_SHIFT;
@@ -769,7 +769,7 @@ static int mmap_ureg(struct vm_area_struct *vma, struct qib_devdata *dd,
 		phys = dd->physaddr + ureg;
 		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
-		vma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND;
+		vm_flags_set(vma, VM_DONTCOPY | VM_DONTEXPAND);
 		ret = io_remap_pfn_range(vma, vma->vm_start,
 					 phys >> PAGE_SHIFT,
 					 vma->vm_end - vma->vm_start,
@@ -810,8 +810,7 @@ static int mmap_piobufs(struct vm_area_struct *vma,
 	 * don't allow them to later change to readable with mprotect (for when
 	 * not initially mapped readable, as is normally the case)
 	 */
-	vma->vm_flags &= ~VM_MAYREAD;
-	vma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND;
+	vm_flags_mod(vma, VM_DONTCOPY | VM_DONTEXPAND, VM_MAYREAD);
 
 	/* We used PAT if wc_cookie == 0 */
 	if (!dd->wc_cookie)
@@ -852,7 +851,7 @@ static int mmap_rcvegrbufs(struct vm_area_struct *vma,
 		goto bail;
 	}
 	/* don't allow them to later change to writable with mprotect */
-	vma->vm_flags &= ~VM_MAYWRITE;
+	vm_flags_clear(vma, VM_MAYWRITE);
 
 	start = vma->vm_start;
 
@@ -944,7 +943,7 @@ static int mmap_kvaddr(struct vm_area_struct *vma, u64 pgaddr,
 		 * Don't allow permission to later change to writable
 		 * with mprotect.
 		 */
-		vma->vm_flags &= ~VM_MAYWRITE;
+		vm_flags_clear(vma, VM_MAYWRITE);
 	} else
 		goto bail;
 	len = vma->vm_end - vma->vm_start;
@@ -955,7 +954,7 @@ static int mmap_kvaddr(struct vm_area_struct *vma, u64 pgaddr,
 
 	vma->vm_pgoff = (unsigned long) addr >> PAGE_SHIFT;
 	vma->vm_ops = &qib_file_vm_ops;
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	ret = 1;
 
 bail:
diff --git a/drivers/infiniband/hw/usnic/usnic_ib_verbs.c b/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
index 6e8c4fbb8083..6289238cc5af 100644
--- a/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
+++ b/drivers/infiniband/hw/usnic/usnic_ib_verbs.c
@@ -672,7 +672,7 @@ int usnic_ib_mmap(struct ib_ucontext *context,
 	usnic_dbg("\n");
 
 	us_ibdev = to_usdev(context->device);
-	vma->vm_flags |= VM_IO;
+	vm_flags_set(vma, VM_IO);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	vfid = vma->vm_pgoff;
 	usnic_dbg("Page Offset %lu PAGE_SHIFT %u VFID %u\n",
diff --git a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.c b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.c
index 19176583dbde..9f54aa90a35a 100644
--- a/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.c
+++ b/drivers/infiniband/hw/vmw_pvrdma/pvrdma_verbs.c
@@ -408,7 +408,7 @@ int pvrdma_mmap(struct ib_ucontext *ibcontext, struct vm_area_struct *vma)
 	}
 
 	/* Map UAR to kernel space, VM_LOCKED? */
-	vma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND;
+	vm_flags_set(vma, VM_DONTCOPY | VM_DONTEXPAND);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	if (io_remap_pfn_range(vma, start, context->uar.pfn, size,
 			       vma->vm_page_prot))
diff --git a/drivers/media/common/videobuf2/videobuf2-dma-contig.c b/drivers/media/common/videobuf2/videobuf2-dma-contig.c
index 2f7f3c5230c1..594b0a2e192f 100644
--- a/drivers/media/common/videobuf2/videobuf2-dma-contig.c
+++ b/drivers/media/common/videobuf2/videobuf2-dma-contig.c
@@ -205,7 +205,7 @@ static int vb2_dc_mmap(void *buf_priv, struct vm_area_struct *vma)
 		return ret;
 	}
 
-	vma->vm_flags		|= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_private_data	= &buf->handler;
 	vma->vm_ops		= &vb2_common_vm_ops;
 
diff --git a/drivers/media/common/videobuf2/videobuf2-vmalloc.c b/drivers/media/common/videobuf2/videobuf2-vmalloc.c
index 3c369d1635f9..466cb86f11f1 100644
--- a/drivers/media/common/videobuf2/videobuf2-vmalloc.c
+++ b/drivers/media/common/videobuf2/videobuf2-vmalloc.c
@@ -185,7 +185,7 @@ static int vb2_vmalloc_mmap(void *buf_priv, struct vm_area_struct *vma)
 	/*
 	 * Make sure that vm_areas for 2 buffers won't be merged together
 	 */
-	vma->vm_flags		|= VM_DONTEXPAND;
+	vm_flags_set(vma, VM_DONTEXPAND);
 
 	/*
 	 * Use common vm_area operations to track buffer refcount.
diff --git a/drivers/media/pci/meye/meye.c b/drivers/media/pci/meye/meye.c
index 8001d3e9134e..dd851e66b048 100644
--- a/drivers/media/pci/meye/meye.c
+++ b/drivers/media/pci/meye/meye.c
@@ -1494,8 +1494,8 @@ static int meye_mmap(struct file *file, struct vm_area_struct *vma)
 	}
 
 	vma->vm_ops = &meye_vm_ops;
-	vma->vm_flags &= ~VM_IO;	/* not I/O memory */
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	/* not I/O memory */
+	vm_flags_mod(vma, VM_DONTEXPAND | VM_DONTDUMP, VM_IO);
 	vma->vm_private_data = (void *) (offset / gbufsize);
 	meye_vm_open(vma);
 
diff --git a/drivers/media/usb/stkwebcam/stk-webcam.c b/drivers/media/usb/stkwebcam/stk-webcam.c
index edf270bf3d30..cb686be9d877 100644
--- a/drivers/media/usb/stkwebcam/stk-webcam.c
+++ b/drivers/media/usb/stkwebcam/stk-webcam.c
@@ -780,7 +780,7 @@ static int v4l_stk_mmap(struct file *fp, struct vm_area_struct *vma)
 	ret = remap_vmalloc_range(vma, sbuf->buffer, 0);
 	if (ret)
 		return ret;
-	vma->vm_flags |= VM_DONTEXPAND;
+	vm_flags_set(vma, VM_DONTEXPAND);
 	vma->vm_private_data = sbuf;
 	vma->vm_ops = &stk_v4l_vm_ops;
 	sbuf->v4lbuf.flags |= V4L2_BUF_FLAG_MAPPED;
diff --git a/drivers/media/v4l2-core/videobuf-dma-contig.c b/drivers/media/v4l2-core/videobuf-dma-contig.c
index 61c7826bde6f..f9434ddea4b5 100644
--- a/drivers/media/v4l2-core/videobuf-dma-contig.c
+++ b/drivers/media/v4l2-core/videobuf-dma-contig.c
@@ -324,7 +324,7 @@ static int __videobuf_mmap_mapper(struct videobuf_queue *q,
 	}
 
 	vma->vm_ops = &videobuf_vm_ops;
-	vma->vm_flags |= VM_DONTEXPAND;
+	vm_flags_set(vma, VM_DONTEXPAND);
 	vma->vm_private_data = map;
 
 	dev_dbg(q->dev, "mmap %p: q=%p %08lx-%08lx (%lx) pgoff %08lx buf %d\n",
diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c
index 4a9e919828ad..f5ff0c25bc16 100644
--- a/drivers/media/v4l2-core/videobuf-dma-sg.c
+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c
@@ -636,8 +636,8 @@ static int __videobuf_mmap_mapper(struct videobuf_queue *q,
 	map->count    = 1;
 	map->q        = q;
 	vma->vm_ops   = &videobuf_vm_ops;
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
-	vma->vm_flags &= ~VM_IO; /* using shared anonymous pages */
+	/* using shared anonymous pages */
+	vm_flags_mod(vma, VM_DONTEXPAND | VM_DONTDUMP, VM_IO);
 	vma->vm_private_data = map;
 	dprintk(1, "mmap %p: q=%p %08lx-%08lx pgoff %08lx bufs %d-%d\n",
 		map, q, vma->vm_start, vma->vm_end, vma->vm_pgoff, first, last);
diff --git a/drivers/media/v4l2-core/videobuf-vmalloc.c b/drivers/media/v4l2-core/videobuf-vmalloc.c
index 45fe781aeeec..3375ea49bde6 100644
--- a/drivers/media/v4l2-core/videobuf-vmalloc.c
+++ b/drivers/media/v4l2-core/videobuf-vmalloc.c
@@ -270,7 +270,7 @@ static int __videobuf_mmap_mapper(struct videobuf_queue *q,
 	}
 
 	vma->vm_ops          = &videobuf_vm_ops;
-	vma->vm_flags       |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_private_data = map;
 
 	dprintk(1, "mmap %p: q=%p %08lx-%08lx (%lx) pgoff %08lx buf %d\n",
diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index c6ec872800a2..aa322b797430 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -225,7 +225,7 @@ int cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)
 	pr_devel("%s: mmio physical: %llx pe: %i master:%i\n", __func__,
 		 ctx->psn_phys, ctx->pe , ctx->master);
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	vma->vm_ops = &cxl_mmap_vmops;
 	return 0;
* Unmerged path drivers/misc/habanalabs/common/memory.c
* Unmerged path drivers/misc/habanalabs/gaudi/gaudi.c
* Unmerged path drivers/misc/habanalabs/gaudi2/gaudi2.c
* Unmerged path drivers/misc/habanalabs/goya/goya.c
diff --git a/drivers/misc/ocxl/context.c b/drivers/misc/ocxl/context.c
index 1ce29bd0d926..e7dd45fd61e0 100644
--- a/drivers/misc/ocxl/context.c
+++ b/drivers/misc/ocxl/context.c
@@ -180,7 +180,7 @@ static int check_mmap_afu_irq(struct ocxl_context *ctx,
 	if ((vma->vm_flags & VM_READ) || (vma->vm_flags & VM_EXEC) ||
 		!(vma->vm_flags & VM_WRITE))
 		return -EINVAL;
-	vma->vm_flags &= ~(VM_MAYREAD | VM_MAYEXEC);
+	vm_flags_clear(vma, VM_MAYREAD | VM_MAYEXEC);
 	return 0;
 }
 
@@ -204,7 +204,7 @@ int ocxl_context_mmap(struct ocxl_context *ctx, struct vm_area_struct *vma)
 	if (rc)
 		return rc;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	vma->vm_ops = &ocxl_vmops;
 	return 0;
diff --git a/drivers/misc/ocxl/sysfs.c b/drivers/misc/ocxl/sysfs.c
index 58f1ba264206..ad37e3489a70 100644
--- a/drivers/misc/ocxl/sysfs.c
+++ b/drivers/misc/ocxl/sysfs.c
@@ -99,7 +99,7 @@ static int global_mmio_mmap(struct file *filp, struct kobject *kobj,
 		(afu->config.global_mmio_size >> PAGE_SHIFT))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	vma->vm_ops = &global_mmio_vmops;
 	vma->vm_private_data = afu;
* Unmerged path drivers/misc/open-dice.c
diff --git a/drivers/misc/sgi-gru/grufile.c b/drivers/misc/sgi-gru/grufile.c
index 7ffcfc0bb587..a3d659c11cc4 100644
--- a/drivers/misc/sgi-gru/grufile.c
+++ b/drivers/misc/sgi-gru/grufile.c
@@ -101,8 +101,8 @@ static int gru_file_mmap(struct file *file, struct vm_area_struct *vma)
 				vma->vm_end & (GRU_GSEG_PAGESIZE - 1))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_LOCKED |
-			 VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP | VM_LOCKED |
+			 VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_page_prot = PAGE_SHARED;
 	vma->vm_ops = &gru_vm_ops;
 
* Unmerged path drivers/misc/uacce/uacce.c
diff --git a/drivers/sbus/char/oradax.c b/drivers/sbus/char/oradax.c
index 524f9ea62e52..e41cf9db1aad 100644
--- a/drivers/sbus/char/oradax.c
+++ b/drivers/sbus/char/oradax.c
@@ -401,7 +401,7 @@ static int dax_devmap(struct file *f, struct vm_area_struct *vma)
 	/* completion area is mapped read-only for user */
 	if (vma->vm_flags & VM_WRITE)
 		return -EPERM;
-	vma->vm_flags &= ~VM_MAYWRITE;
+	vm_flags_clear(vma, VM_MAYWRITE);
 
 	if (remap_pfn_range(vma, vma->vm_start, ctx->ca_buf_ra >> PAGE_SHIFT,
 			    len, vma->vm_page_prot))
diff --git a/drivers/scsi/cxlflash/ocxl_hw.c b/drivers/scsi/cxlflash/ocxl_hw.c
index 91f98c7e3d0f..f55f4539a47a 100644
--- a/drivers/scsi/cxlflash/ocxl_hw.c
+++ b/drivers/scsi/cxlflash/ocxl_hw.c
@@ -1188,7 +1188,7 @@ static int afu_mmap(struct file *file, struct vm_area_struct *vma)
 	    (ctx->psn_size >> PAGE_SHIFT))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	vma->vm_ops = &ocxlflash_vmops;
 	return 0;
diff --git a/drivers/scsi/sg.c b/drivers/scsi/sg.c
index c51df381932b..779bb9a56853 100644
--- a/drivers/scsi/sg.c
+++ b/drivers/scsi/sg.c
@@ -1269,7 +1269,7 @@ sg_mmap(struct file *filp, struct vm_area_struct *vma)
 	}
 
 	sfp->mmap_called = 1;
-	vma->vm_flags |= VM_IO | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_IO | VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_private_data = sfp;
 	vma->vm_ops = &sg_mmap_vm_ops;
 out:
* Unmerged path drivers/staging/media/atomisp/pci/hmm/hmm_bo.c
diff --git a/drivers/target/target_core_user.c b/drivers/target/target_core_user.c
index ee961d526384..8b5ca9e1db42 100644
--- a/drivers/target/target_core_user.c
+++ b/drivers/target/target_core_user.c
@@ -1939,7 +1939,7 @@ static int tcmu_mmap(struct uio_info *info, struct vm_area_struct *vma)
 {
 	struct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);
 
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &tcmu_vm_ops;
 
 	vma->vm_private_data = udev;
diff --git a/drivers/uio/uio.c b/drivers/uio/uio.c
index 7d402e736ccf..2666daa49eda 100644
--- a/drivers/uio/uio.c
+++ b/drivers/uio/uio.c
@@ -711,7 +711,7 @@ static const struct vm_operations_struct uio_logical_vm_ops = {
 
 static int uio_mmap_logical(struct vm_area_struct *vma)
 {
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &uio_logical_vm_ops;
 	return 0;
 }
diff --git a/drivers/usb/core/devio.c b/drivers/usb/core/devio.c
index c55d7b3619e8..d1978fca425e 100644
--- a/drivers/usb/core/devio.c
+++ b/drivers/usb/core/devio.c
@@ -266,8 +266,7 @@ static int usbdev_mmap(struct file *file, struct vm_area_struct *vma)
 		}
 	}
 
-	vma->vm_flags |= VM_IO;
-	vma->vm_flags |= (VM_DONTEXPAND | VM_DONTDUMP);
+	vm_flags_set(vma, VM_IO | VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &usbdev_vm_ops;
 	vma->vm_private_data = usbm;
 
diff --git a/drivers/usb/mon/mon_bin.c b/drivers/usb/mon/mon_bin.c
index 094e812e9e69..abb1cd35d8a6 100644
--- a/drivers/usb/mon/mon_bin.c
+++ b/drivers/usb/mon/mon_bin.c
@@ -1272,8 +1272,7 @@ static int mon_bin_mmap(struct file *filp, struct vm_area_struct *vma)
 	if (vma->vm_flags & VM_WRITE)
 		return -EPERM;
 
-	vma->vm_flags &= ~VM_MAYWRITE;
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_mod(vma, VM_DONTEXPAND | VM_DONTDUMP, VM_MAYWRITE);
 	vma->vm_private_data = filp->private_data;
 	mon_bin_vma_open(vma);
 	return 0;
* Unmerged path drivers/vdpa/vdpa_user/iova_domain.c
* Unmerged path drivers/vfio/pci/vfio_pci_core.c
diff --git a/drivers/vhost/vdpa.c b/drivers/vhost/vdpa.c
index 7ff67a1871bb..a2692ca544e5 100644
--- a/drivers/vhost/vdpa.c
+++ b/drivers/vhost/vdpa.c
@@ -1084,7 +1084,7 @@ static int vhost_vdpa_mmap(struct file *file, struct vm_area_struct *vma)
 	if (vma->vm_end - vma->vm_start != notify.size)
 		return -ENOTSUPP;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &vhost_vdpa_vm_ops;
 	return 0;
 }
diff --git a/drivers/video/fbdev/68328fb.c b/drivers/video/fbdev/68328fb.c
index d48e96088f76..be1b0b56d2a4 100644
--- a/drivers/video/fbdev/68328fb.c
+++ b/drivers/video/fbdev/68328fb.c
@@ -394,7 +394,7 @@ static int mc68x328fb_mmap(struct fb_info *info, struct vm_area_struct *vma)
 #ifndef MMU
 	/* this is uClinux (no MMU) specific code */
 
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_start = videomemory;
 
 	return 0;
diff --git a/drivers/video/fbdev/core/fb_defio.c b/drivers/video/fbdev/core/fb_defio.c
index 2396587d407e..055b2a3c5c0e 100644
--- a/drivers/video/fbdev/core/fb_defio.c
+++ b/drivers/video/fbdev/core/fb_defio.c
@@ -216,9 +216,9 @@ static const struct address_space_operations fb_deferred_io_aops = {
 int fb_deferred_io_mmap(struct fb_info *info, struct vm_area_struct *vma)
 {
 	vma->vm_ops = &fb_deferred_io_vm_ops;
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	if (!(info->flags & FBINFO_VIRTFB))
-		vma->vm_flags |= VM_IO;
+		vm_flags_set(vma, VM_IO);
 	vma->vm_private_data = info;
 	return 0;
 }
diff --git a/drivers/xen/gntalloc.c b/drivers/xen/gntalloc.c
index 3fa40c723e8e..424bc12f4006 100644
--- a/drivers/xen/gntalloc.c
+++ b/drivers/xen/gntalloc.c
@@ -539,7 +539,7 @@ static int gntalloc_mmap(struct file *filp, struct vm_area_struct *vma)
 
 	vma->vm_private_data = vm_priv;
 
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 
 	vma->vm_ops = &gntalloc_vmops;
 
diff --git a/drivers/xen/gntdev.c b/drivers/xen/gntdev.c
index 7faf273ba78b..80f578546dec 100644
--- a/drivers/xen/gntdev.c
+++ b/drivers/xen/gntdev.c
@@ -997,10 +997,10 @@ static int gntdev_mmap(struct file *flip, struct vm_area_struct *vma)
 
 	vma->vm_ops = &gntdev_vmops;
 
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP | VM_MIXEDMAP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP | VM_MIXEDMAP);
 
 	if (use_ptemod)
-		vma->vm_flags |= VM_DONTCOPY;
+		vm_flags_set(vma, VM_DONTCOPY);
 
 	vma->vm_private_data = map;
 
diff --git a/drivers/xen/privcmd-buf.c b/drivers/xen/privcmd-buf.c
index df1ed37c3269..e4fe6c152943 100644
--- a/drivers/xen/privcmd-buf.c
+++ b/drivers/xen/privcmd-buf.c
@@ -171,7 +171,7 @@ static int privcmd_buf_mmap(struct file *file, struct vm_area_struct *vma)
 	vma_priv->file_priv = file_priv;
 	vma_priv->users = 1;
 
-	vma->vm_flags |= VM_IO | VM_DONTEXPAND;
+	vm_flags_set(vma, VM_IO | VM_DONTEXPAND);
 	vma->vm_ops = &privcmd_buf_vm_ops;
 	vma->vm_private_data = vma_priv;
 
diff --git a/drivers/xen/privcmd.c b/drivers/xen/privcmd.c
index a0052b287a53..861abe5ace0b 100644
--- a/drivers/xen/privcmd.c
+++ b/drivers/xen/privcmd.c
@@ -951,8 +951,8 @@ static int privcmd_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	/* DONTCOPY is essential for Xen because copy_page_range doesn't know
 	 * how to recreate these mappings */
-	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTCOPY |
-			 VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTCOPY |
+			 VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &privcmd_vm_ops;
 	vma->vm_private_data = NULL;
 
diff --git a/fs/aio.c b/fs/aio.c
index 23d91809f170..c9809aff0125 100644
--- a/fs/aio.c
+++ b/fs/aio.c
@@ -335,7 +335,7 @@ static const struct vm_operations_struct aio_ring_vm_ops = {
 
 static int aio_ring_mmap(struct file *file, struct vm_area_struct *vma)
 {
-	vma->vm_flags |= VM_DONTEXPAND;
+	vm_flags_set(vma, VM_DONTEXPAND);
 	vma->vm_ops = &aio_ring_vm_ops;
 	return 0;
 }
diff --git a/fs/cramfs/inode.c b/fs/cramfs/inode.c
index 995cddb7bf44..44cd1d4eecc9 100644
--- a/fs/cramfs/inode.c
+++ b/fs/cramfs/inode.c
@@ -417,7 +417,7 @@ static int cramfs_physmem_mmap(struct file *file, struct vm_area_struct *vma)
 		 * unpopulated ptes via cramfs_readpage().
 		 */
 		int i;
-		vma->vm_flags |= VM_MIXEDMAP;
+		vm_flags_set(vma, VM_MIXEDMAP);
 		for (i = 0; i < pages && !ret; i++) {
 			vm_fault_t vmf;
 			unsigned long off = i * PAGE_SIZE;
* Unmerged path fs/erofs/data.c
diff --git a/fs/exec.c b/fs/exec.c
index 176facb6fbf9..4c2231098020 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -310,7 +310,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 	BUILD_BUG_ON(VM_STACK_FLAGS & VM_STACK_INCOMPLETE_SETUP);
 	vma->vm_end = STACK_TOP_MAX;
 	vma->vm_start = vma->vm_end - PAGE_SIZE;
-	vma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;
+	vm_flags_init(vma, VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP);
 	vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 
 	err = insert_vm_struct(mm, vma);
@@ -767,7 +767,7 @@ int setup_arg_pages(struct linux_binprm *bprm,
 	}
 
 	/* mprotect_fixup is overkill to remove the temporary stack flags */
-	vma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;
+	vm_flags_clear(vma, VM_STACK_INCOMPLETE_SETUP);
 
 	stack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */
 	stack_size = vma->vm_end - vma->vm_start;
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index b7bc9e10d9dd..be09b73b1090 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -389,7 +389,7 @@ static int ext4_file_mmap(struct file *file, struct vm_area_struct *vma)
 	file_accessed(file);
 	if (IS_DAX(file_inode(file))) {
 		vma->vm_ops = &ext4_dax_vm_ops;
-		vma->vm_flags |= VM_HUGEPAGE;
+		vm_flags_set(vma, VM_HUGEPAGE);
 	} else {
 		vma->vm_ops = &ext4_file_vm_ops;
 	}
diff --git a/fs/fuse/dax.c b/fs/fuse/dax.c
index ff99ab2a3c43..a43b0bff8e47 100644
--- a/fs/fuse/dax.c
+++ b/fs/fuse/dax.c
@@ -864,7 +864,7 @@ int fuse_dax_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	file_accessed(file);
 	vma->vm_ops = &fuse_dax_vm_ops;
-	vma->vm_flags |= VM_MIXEDMAP | VM_HUGEPAGE;
+	vm_flags_set(vma, VM_MIXEDMAP | VM_HUGEPAGE);
 	return 0;
 }
 
diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index d502f2a9b13d..adb2bdb49834 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -142,7 +142,7 @@ static int hugetlbfs_file_mmap(struct file *file, struct vm_area_struct *vma)
 	 * way when do_mmap unwinds (may be important on powerpc
 	 * and ia64).
 	 */
-	vma->vm_flags |= VM_HUGETLB | VM_DONTEXPAND;
+	vm_flags_set(vma, VM_HUGETLB | VM_DONTEXPAND);
 	vma->vm_ops = &hugetlb_vm_ops;
 
 	/*
@@ -627,7 +627,7 @@ static long hugetlbfs_fallocate(struct file *file, int mode, loff_t offset,
 	 */
 	memset(&pseudo_vma, 0, sizeof(struct vm_area_struct));
 	vma_init(&pseudo_vma, mm);
-	pseudo_vma.vm_flags = (VM_HUGETLB | VM_MAYSHARE | VM_SHARED);
+	vm_flags_init(&pseudo_vma, VM_HUGETLB | VM_MAYSHARE | VM_SHARED);
 	pseudo_vma.vm_file = file;
 
 	for (index = start; index < end; index++) {
diff --git a/fs/orangefs/file.c b/fs/orangefs/file.c
index 57bc030b3a29..d8eae0585158 100644
--- a/fs/orangefs/file.c
+++ b/fs/orangefs/file.c
@@ -584,8 +584,7 @@ static int orangefs_file_mmap(struct file *file, struct vm_area_struct *vma)
 		return -EINVAL;
 
 	/* set the sequential readahead hint */
-	vma->vm_flags |= VM_SEQ_READ;
-	vma->vm_flags &= ~VM_RAND_READ;
+	vm_flags_mod(vma, VM_SEQ_READ, VM_RAND_READ);
 
 	file_accessed(file);
 	vma->vm_ops = &orangefs_file_vm_ops;
* Unmerged path fs/proc/task_mmu.c
diff --git a/fs/proc/vmcore.c b/fs/proc/vmcore.c
index 38747df25644..82058a212f94 100644
--- a/fs/proc/vmcore.c
+++ b/fs/proc/vmcore.c
@@ -560,8 +560,7 @@ static int mmap_vmcore(struct file *file, struct vm_area_struct *vma)
 	if (vma->vm_flags & (VM_WRITE | VM_EXEC))
 		return -EPERM;
 
-	vma->vm_flags &= ~(VM_MAYWRITE | VM_MAYEXEC);
-	vma->vm_flags |= VM_MIXEDMAP;
+	vm_flags_mod(vma, VM_MIXEDMAP, VM_MAYWRITE | VM_MAYEXEC);
 	vma->vm_ops = &vmcore_mmap_ops;
 
 	len = 0;
* Unmerged path fs/userfaultfd.c
diff --git a/fs/xfs/xfs_file.c b/fs/xfs/xfs_file.c
index 137f800d7415..dafde22031a0 100644
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@ -1426,7 +1426,7 @@ xfs_file_mmap(
 	file_accessed(file);
 	vma->vm_ops = &xfs_file_vm_ops;
 	if (IS_DAX(inode))
-		vma->vm_flags |= VM_HUGEPAGE;
+		vm_flags_set(vma, VM_HUGEPAGE);
 	return 0;
 }
 
* Unmerged path include/linux/mm.h
* Unmerged path kernel/bpf/ringbuf.c
diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 566aa4b2e25c..64b526861c5b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -678,10 +678,10 @@ static int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)
 	/* set default open/close callbacks */
 	vma->vm_ops = &bpf_map_default_vmops;
 	vma->vm_private_data = map;
-	vma->vm_flags &= ~VM_MAYEXEC;
+	vm_flags_clear(vma, VM_MAYEXEC);
 	if (!(vma->vm_flags & VM_WRITE))
 		/* disallow re-mapping with PROT_WRITE */
-		vma->vm_flags &= ~VM_MAYWRITE;
+		vm_flags_clear(vma, VM_MAYWRITE);
 
 	err = map->ops->map_mmap(map, vma);
 	if (err)
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 56aa807ee548..81895dc44b8a 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -6375,7 +6375,7 @@ static int perf_mmap(struct file *file, struct vm_area_struct *vma)
 	 * Since pinned accounting is per vm we cannot allow fork() to copy our
 	 * vma.
 	 */
-	vma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &perf_mmap_vmops;
 
 	if (event->pmu->event_mapped)
* Unmerged path kernel/kcov.c
diff --git a/kernel/relay.c b/kernel/relay.c
index aaf74268dbd4..9d25d0cff93d 100644
--- a/kernel/relay.c
+++ b/kernel/relay.c
@@ -94,7 +94,7 @@ static int relay_mmap_buf(struct rchan_buf *buf, struct vm_area_struct *vma)
 		return -EINVAL;
 
 	vma->vm_ops = &relay_file_mmap_ops;
-	vma->vm_flags |= VM_DONTEXPAND;
+	vm_flags_set(vma, VM_DONTEXPAND);
 	vma->vm_private_data = buf;
 
 	return 0;
* Unmerged path mm/madvise.c
* Unmerged path mm/memory.c
* Unmerged path mm/mlock.c
* Unmerged path mm/mmap.c
* Unmerged path mm/mprotect.c
* Unmerged path mm/mremap.c
diff --git a/mm/nommu.c b/mm/nommu.c
index fad662e89156..50ae2d5162f7 100644
--- a/mm/nommu.c
+++ b/mm/nommu.c
@@ -165,7 +165,7 @@ static void *__vmalloc_user_flags(unsigned long size, gfp_t flags)
 		mmap_write_lock(current->mm);
 		vma = find_vma(current->mm, (unsigned long)ret);
 		if (vma)
-			vma->vm_flags |= VM_USERMAP;
+			vm_flags_set(vma, VM_USERMAP);
 		mmap_write_unlock(current->mm);
 	}
 
@@ -1040,7 +1040,8 @@ static int do_mmap_private(struct vm_area_struct *vma,
 
 	atomic_long_add(total, &mmap_pages_allocated);
 
-	region->vm_flags = vma->vm_flags |= VM_MAPPED_COPY;
+	vm_flags_set(vma, VM_MAPPED_COPY);
+	region->vm_flags = vma->vm_flags;
 	region->vm_start = (unsigned long) base;
 	region->vm_end   = region->vm_start + len;
 	region->vm_top   = region->vm_start + (total << PAGE_SHIFT);
@@ -1132,7 +1133,7 @@ unsigned long do_mmap(struct file *file,
 	region->vm_flags = vm_flags;
 	region->vm_pgoff = pgoff;
 
-	vma->vm_flags = vm_flags;
+	vm_flags_init(vma, vm_flags);
 	vma->vm_pgoff = pgoff;
 
 	if (file) {
@@ -1196,7 +1197,7 @@ unsigned long do_mmap(struct file *file,
 			vma->vm_end = start + len;
 
 			if (pregion->vm_flags & VM_MAPPED_COPY)
-				vma->vm_flags |= VM_MAPPED_COPY;
+				vm_flags_set(vma, VM_MAPPED_COPY);
 			else {
 				ret = do_mmap_shared_file(vma);
 				if (ret < 0) {
@@ -1645,7 +1646,7 @@ int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
 	if (addr != (pfn << PAGE_SHIFT))
 		return -EINVAL;
 
-	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);
 	return 0;
 }
 EXPORT_SYMBOL(remap_pfn_range);
* Unmerged path mm/secretmem.c
* Unmerged path mm/shmem.c
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index 7a4fe1bbcfb2..d4eadf37daaf 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -3088,7 +3088,7 @@ int remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,
 		size -= PAGE_SIZE;
 	} while (size > 0);
 
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 
 	return 0;
 }
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 0fd0c487e9a6..1f9628a165bb 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -1770,10 +1770,10 @@ int tcp_mmap(struct file *file, struct socket *sock,
 {
 	if (vma->vm_flags & (VM_WRITE | VM_EXEC))
 		return -EPERM;
-	vma->vm_flags &= ~(VM_MAYWRITE | VM_MAYEXEC);
+	vm_flags_clear(vma, VM_MAYWRITE | VM_MAYEXEC);
 
 	/* Instruct vm_insert_page() to not mmap_read_lock(mm) */
-	vma->vm_flags |= VM_MIXEDMAP;
+	vm_flags_set(vma, VM_MIXEDMAP);
 
 	vma->vm_ops = &tcp_vm_ops;
 	return 0;
diff --git a/security/selinux/selinuxfs.c b/security/selinux/selinuxfs.c
index 4460c70f3734..2dd45b95c7df 100644
--- a/security/selinux/selinuxfs.c
+++ b/security/selinux/selinuxfs.c
@@ -261,7 +261,7 @@ static int sel_mmap_handle_status(struct file *filp,
 	if (vma->vm_flags & VM_WRITE)
 		return -EPERM;
 	/* disallow mprotect() turns it into writable */
-	vma->vm_flags &= ~VM_MAYWRITE;
+	vm_flags_clear(vma, VM_MAYWRITE);
 
 	return remap_pfn_range(vma, vma->vm_start,
 			       page_to_pfn(status),
@@ -496,13 +496,13 @@ static int sel_mmap_policy(struct file *filp, struct vm_area_struct *vma)
 {
 	if (vma->vm_flags & VM_SHARED) {
 		/* do not allow mprotect to make mapping writable */
-		vma->vm_flags &= ~VM_MAYWRITE;
+		vm_flags_clear(vma, VM_MAYWRITE);
 
 		if (vma->vm_flags & VM_WRITE)
 			return -EACCES;
 	}
 
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_ops = &sel_mmap_policy_ops;
 
 	return 0;
diff --git a/sound/core/oss/pcm_oss.c b/sound/core/oss/pcm_oss.c
index 8b81f67ba997..6e56b2725d5e 100644
--- a/sound/core/oss/pcm_oss.c
+++ b/sound/core/oss/pcm_oss.c
@@ -2906,7 +2906,7 @@ static int snd_pcm_oss_mmap(struct file *file, struct vm_area_struct *area)
 	}
 	/* set VM_READ access as well to fix memset() routines that do
 	   reads before writes (to improve performance) */
-	area->vm_flags |= VM_READ;
+	vm_flags_set(area, VM_READ);
 	if (substream == NULL)
 		return -ENXIO;
 	runtime = substream->runtime;
diff --git a/sound/core/pcm_native.c b/sound/core/pcm_native.c
index be8f16a5c29d..1c428b251d5a 100644
--- a/sound/core/pcm_native.c
+++ b/sound/core/pcm_native.c
@@ -3674,8 +3674,9 @@ static int snd_pcm_mmap_status(struct snd_pcm_substream *substream, struct file
 		return -EINVAL;
 	area->vm_ops = &snd_pcm_vm_ops_status;
 	area->vm_private_data = substream;
-	area->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
-	area->vm_flags &= ~(VM_WRITE | VM_MAYWRITE);
+	vm_flags_mod(area, VM_DONTEXPAND | VM_DONTDUMP,
+		     VM_WRITE | VM_MAYWRITE);
+
 	return 0;
 }
 
@@ -3711,7 +3712,7 @@ static int snd_pcm_mmap_control(struct snd_pcm_substream *substream, struct file
 		return -EINVAL;
 	area->vm_ops = &snd_pcm_vm_ops_control;
 	area->vm_private_data = substream;
-	area->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(area, VM_DONTEXPAND | VM_DONTDUMP);
 	return 0;
 }
 
@@ -3827,7 +3828,7 @@ static const struct vm_operations_struct snd_pcm_vm_ops_data_fault = {
 int snd_pcm_lib_default_mmap(struct snd_pcm_substream *substream,
 			     struct vm_area_struct *area)
 {
-	area->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(area, VM_DONTEXPAND | VM_DONTDUMP);
 	if (!substream->ops->page &&
 	    !snd_dma_buffer_mmap(snd_pcm_get_dma_buf(substream), area))
 		return 0;
diff --git a/sound/soc/pxa/mmp-sspa.c b/sound/soc/pxa/mmp-sspa.c
index 1a35dd3cb564..3417d46df2ab 100644
--- a/sound/soc/pxa/mmp-sspa.c
+++ b/sound/soc/pxa/mmp-sspa.c
@@ -403,7 +403,7 @@ static int mmp_pcm_mmap(struct snd_soc_component *component,
 			struct snd_pcm_substream *substream,
 			struct vm_area_struct *vma)
 {
-	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 	return remap_pfn_range(vma, vma->vm_start,
 		substream->dma_buffer.addr >> PAGE_SHIFT,
diff --git a/sound/usb/usx2y/us122l.c b/sound/usb/usx2y/us122l.c
index e558931cce16..709ccad972e2 100644
--- a/sound/usb/usx2y/us122l.c
+++ b/sound/usb/usx2y/us122l.c
@@ -224,9 +224,9 @@ static int usb_stream_hwdep_mmap(struct snd_hwdep *hw,
 	}
 
 	area->vm_ops = &usb_stream_hwdep_vm_ops;
-	area->vm_flags |= VM_DONTDUMP;
+	vm_flags_set(area, VM_DONTDUMP);
 	if (!read)
-		area->vm_flags |= VM_DONTEXPAND;
+		vm_flags_set(area, VM_DONTEXPAND);
 	area->vm_private_data = us122l;
 	atomic_inc(&us122l->mmap_count);
 out:
diff --git a/sound/usb/usx2y/usX2Yhwdep.c b/sound/usb/usx2y/usX2Yhwdep.c
index c29da0341bc5..4937ede0b5d7 100644
--- a/sound/usb/usx2y/usX2Yhwdep.c
+++ b/sound/usb/usx2y/usX2Yhwdep.c
@@ -61,7 +61,7 @@ static int snd_us428ctls_mmap(struct snd_hwdep *hw, struct file *filp, struct vm
 	}
 
 	area->vm_ops = &us428ctls_vm_ops;
-	area->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(area, VM_DONTEXPAND | VM_DONTDUMP);
 	area->vm_private_data = hw->private_data;
 	return 0;
 }
diff --git a/sound/usb/usx2y/usx2yhwdeppcm.c b/sound/usb/usx2y/usx2yhwdeppcm.c
index 767a227d54da..36f2e31168fb 100644
--- a/sound/usb/usx2y/usx2yhwdeppcm.c
+++ b/sound/usb/usx2y/usx2yhwdeppcm.c
@@ -706,7 +706,7 @@ static int snd_usx2y_hwdep_pcm_mmap(struct snd_hwdep *hw, struct file *filp, str
 		return -ENODEV;
 
 	area->vm_ops = &snd_usx2y_hwdep_pcm_vm_ops;
-	area->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vm_flags_set(area, VM_DONTEXPAND | VM_DONTDUMP);
 	area->vm_private_data = hw->private_data;
 	return 0;
 }
