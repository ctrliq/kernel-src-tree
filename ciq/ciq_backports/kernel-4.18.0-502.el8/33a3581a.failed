arm64: entry: move arm64_preempt_schedule_irq to entry-common.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit 33a3581a76f3a36c7dcc9864120ce681bcfbcff1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/33a3581a.failed

Subsequent patches will pull more of the IRQ entry handling into C. To
keep this in one place, let's move arm64_preempt_schedule_irq() into
entry-common.c along with the other entry management functions.

We no longer need to include <linux/lockdep.h> in process.c, so the
include directive is removed.

There should be no functional change as a result of this patch.

	Reviewed-by Joey Gouly <joey.gouly@arm.com>

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <maz@kernel.org>
	Cc: James Morse <james.morse@arm.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lore.kernel.org/r/20210607094624.34689-5-mark.rutland@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 33a3581a76f3a36c7dcc9864120ce681bcfbcff1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry-common.c
#	arch/arm64/kernel/process.c
diff --cc arch/arm64/kernel/entry-common.c
index c764ba2d11d7,1fe60578e556..000000000000
--- a/arch/arm64/kernel/entry-common.c
+++ b/arch/arm64/kernel/entry-common.c
@@@ -79,6 -117,71 +83,74 @@@ asmlinkage void noinstr exit_el1_irq_or
  		exit_to_kernel_mode(regs);
  }
  
++<<<<<<< HEAD
++=======
+ asmlinkage void __sched arm64_preempt_schedule_irq(void)
+ {
+ 	lockdep_assert_irqs_disabled();
+ 
+ 	/*
+ 	 * Preempting a task from an IRQ means we leave copies of PSTATE
+ 	 * on the stack. cpufeature's enable calls may modify PSTATE, but
+ 	 * resuming one of these preempted tasks would undo those changes.
+ 	 *
+ 	 * Only allow a task to be preempted once cpufeatures have been
+ 	 * enabled.
+ 	 */
+ 	if (system_capabilities_finalized())
+ 		preempt_schedule_irq();
+ }
+ 
+ #ifdef CONFIG_ARM64_ERRATUM_1463225
+ static DEFINE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+ 
+ static void cortex_a76_erratum_1463225_svc_handler(void)
+ {
+ 	u32 reg, val;
+ 
+ 	if (!unlikely(test_thread_flag(TIF_SINGLESTEP)))
+ 		return;
+ 
+ 	if (!unlikely(this_cpu_has_cap(ARM64_WORKAROUND_1463225)))
+ 		return;
+ 
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
+ 	reg = read_sysreg(mdscr_el1);
+ 	val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
+ 	write_sysreg(val, mdscr_el1);
+ 	asm volatile("msr daifclr, #8");
+ 	isb();
+ 
+ 	/* We will have taken a single-step exception by this point */
+ 
+ 	write_sysreg(reg, mdscr_el1);
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
+ }
+ 
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	if (!__this_cpu_read(__in_cortex_a76_erratum_1463225_wa))
+ 		return false;
+ 
+ 	/*
+ 	 * We've taken a dummy step exception from the kernel to ensure
+ 	 * that interrupts are re-enabled on the syscall path. Return back
+ 	 * to cortex_a76_erratum_1463225_svc_handler() with debug exceptions
+ 	 * masked so that we can safely restore the mdscr and get on with
+ 	 * handling the syscall.
+ 	 */
+ 	regs->pstate |= PSR_D_BIT;
+ 	return true;
+ }
+ #else /* CONFIG_ARM64_ERRATUM_1463225 */
+ static void cortex_a76_erratum_1463225_svc_handler(void) { }
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_ARM64_ERRATUM_1463225 */
+ 
++>>>>>>> 33a3581a76f3 (arm64: entry: move arm64_preempt_schedule_irq to entry-common.c)
  static void noinstr el1_abort(struct pt_regs *regs, unsigned long esr)
  {
  	unsigned long far = read_sysreg(far_el1);
diff --cc arch/arm64/kernel/process.c
index e5d7925d7a3a,2e7337709155..000000000000
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@@ -28,8 -18,9 +28,12 @@@
  #include <linux/sched/task.h>
  #include <linux/sched/task_stack.h>
  #include <linux/kernel.h>
++<<<<<<< HEAD
 +#include <linux/lockdep.h>
++=======
+ #include <linux/mman.h>
++>>>>>>> 33a3581a76f3 (arm64: entry: move arm64_preempt_schedule_irq to entry-common.c)
  #include <linux/mm.h>
 -#include <linux/nospec.h>
  #include <linux/stddef.h>
  #include <linux/sysctl.h>
  #include <linux/unistd.h>
@@@ -714,3 -722,25 +718,28 @@@ static int __init tagged_addr_init(void
  
  core_initcall(tagged_addr_init);
  #endif	/* CONFIG_ARM64_TAGGED_ADDR_ABI */
++<<<<<<< HEAD
++=======
+ 
+ #ifdef CONFIG_BINFMT_ELF
+ int arch_elf_adjust_prot(int prot, const struct arch_elf_state *state,
+ 			 bool has_interp, bool is_interp)
+ {
+ 	/*
+ 	 * For dynamically linked executables the interpreter is
+ 	 * responsible for setting PROT_BTI on everything except
+ 	 * itself.
+ 	 */
+ 	if (is_interp != has_interp)
+ 		return prot;
+ 
+ 	if (!(state->flags & ARM64_ELF_BTI))
+ 		return prot;
+ 
+ 	if (prot & PROT_EXEC)
+ 		prot |= PROT_BTI;
+ 
+ 	return prot;
+ }
+ #endif
++>>>>>>> 33a3581a76f3 (arm64: entry: move arm64_preempt_schedule_irq to entry-common.c)
* Unmerged path arch/arm64/kernel/entry-common.c
* Unmerged path arch/arm64/kernel/process.c
