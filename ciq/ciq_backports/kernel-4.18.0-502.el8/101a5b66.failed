arm64: entry: move NMI preempt logic to C

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit 101a5b665dcdff169ae7ad90556604c483d9027e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/101a5b66.failed

Currently portions of our preempt logic are written in C while other
parts are written in assembly. Let's clean this up a little bit by
moving the NMI preempt checks to C. For now, the preempt count (and
need_resched) checking is left in assembly, and will be converted
with the body of the IRQ handler in subsequent patches.

Other than the increased lockdep coverage there should be no functional
change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <maz@kernel.org>
	Reviewed-by: Joey Gouly <joey.gouly@arm.com>
	Cc: James Morse <james.morse@arm.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lore.kernel.org/r/20210607094624.34689-6-mark.rutland@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 101a5b665dcdff169ae7ad90556604c483d9027e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry-common.c
#	arch/arm64/kernel/entry.S
diff --cc arch/arm64/kernel/entry-common.c
index c764ba2d11d7,08d17eb0ce13..000000000000
--- a/arch/arm64/kernel/entry-common.c
+++ b/arch/arm64/kernel/entry-common.c
@@@ -79,6 -117,80 +79,83 @@@ asmlinkage void noinstr exit_el1_irq_or
  		exit_to_kernel_mode(regs);
  }
  
++<<<<<<< HEAD
++=======
+ asmlinkage void __sched arm64_preempt_schedule_irq(void)
+ {
+ 	lockdep_assert_irqs_disabled();
+ 
+ 	/*
+ 	 * DAIF.DA are cleared at the start of IRQ/FIQ handling, and when GIC
+ 	 * priority masking is used the GIC irqchip driver will clear DAIF.IF
+ 	 * using gic_arch_enable_irqs() for normal IRQs. If anything is set in
+ 	 * DAIF we must have handled an NMI, so skip preemption.
+ 	 */
+ 	if (system_uses_irq_prio_masking() && read_sysreg(daif))
+ 		return;
+ 
+ 	/*
+ 	 * Preempting a task from an IRQ means we leave copies of PSTATE
+ 	 * on the stack. cpufeature's enable calls may modify PSTATE, but
+ 	 * resuming one of these preempted tasks would undo those changes.
+ 	 *
+ 	 * Only allow a task to be preempted once cpufeatures have been
+ 	 * enabled.
+ 	 */
+ 	if (system_capabilities_finalized())
+ 		preempt_schedule_irq();
+ }
+ 
+ #ifdef CONFIG_ARM64_ERRATUM_1463225
+ static DEFINE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+ 
+ static void cortex_a76_erratum_1463225_svc_handler(void)
+ {
+ 	u32 reg, val;
+ 
+ 	if (!unlikely(test_thread_flag(TIF_SINGLESTEP)))
+ 		return;
+ 
+ 	if (!unlikely(this_cpu_has_cap(ARM64_WORKAROUND_1463225)))
+ 		return;
+ 
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
+ 	reg = read_sysreg(mdscr_el1);
+ 	val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
+ 	write_sysreg(val, mdscr_el1);
+ 	asm volatile("msr daifclr, #8");
+ 	isb();
+ 
+ 	/* We will have taken a single-step exception by this point */
+ 
+ 	write_sysreg(reg, mdscr_el1);
+ 	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
+ }
+ 
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	if (!__this_cpu_read(__in_cortex_a76_erratum_1463225_wa))
+ 		return false;
+ 
+ 	/*
+ 	 * We've taken a dummy step exception from the kernel to ensure
+ 	 * that interrupts are re-enabled on the syscall path. Return back
+ 	 * to cortex_a76_erratum_1463225_svc_handler() with debug exceptions
+ 	 * masked so that we can safely restore the mdscr and get on with
+ 	 * handling the syscall.
+ 	 */
+ 	regs->pstate |= PSR_D_BIT;
+ 	return true;
+ }
+ #else /* CONFIG_ARM64_ERRATUM_1463225 */
+ static void cortex_a76_erratum_1463225_svc_handler(void) { }
+ static bool cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+ {
+ 	return false;
+ }
+ #endif /* CONFIG_ARM64_ERRATUM_1463225 */
+ 
++>>>>>>> 101a5b665dcd (arm64: entry: move NMI preempt logic to C)
  static void noinstr el1_abort(struct pt_regs *regs, unsigned long esr)
  {
  	unsigned long far = read_sysreg(far_el1);
diff --cc arch/arm64/kernel/entry.S
index ba52009f8045,449628290ce8..000000000000
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@@ -451,13 -552,33 +451,29 @@@ tsk	.req	x28		// current thread_inf
  #endif
  	.endm
  
++<<<<<<< HEAD
 +	.macro	gic_prio_irq_setup, pmr:req, tmp:req
 +#ifdef CONFIG_ARM64_PSEUDO_NMI
 +	alternative_if ARM64_HAS_IRQ_PRIO_MASKING
 +	orr	\tmp, \pmr, #GIC_PRIO_PSR_I_SET
 +	msr_s	SYS_ICC_PMR_EL1, \tmp
 +	alternative_else_nop_endif
++=======
+ 	.macro el1_interrupt_handler, handler:req
+ 	enable_da
+ 
+ 	mov	x0, sp
+ 	bl	enter_el1_irq_or_nmi
+ 
+ 	irq_handler	\handler
+ 
+ #ifdef CONFIG_PREEMPTION
+ 	ldr	x24, [tsk, #TSK_TI_PREEMPT]	// get preempt count
+ 	cbnz	x24, 1f				// preempt count != 0
+ 	bl	arm64_preempt_schedule_irq	// irq en/disable is done inside
+ 1:
++>>>>>>> 101a5b665dcd (arm64: entry: move NMI preempt logic to C)
  #endif
 -
 -	mov	x0, sp
 -	bl	exit_el1_irq_or_nmi
 -	.endm
 -
 -	.macro el0_interrupt_handler, handler:req
 -	user_exit_irqoff
 -	enable_da
 -
 -	tbz	x22, #55, 1f
 -	bl	do_el0_irq_bp_hardening
 -1:
 -	irq_handler	\handler
  	.endm
  
  	.text
* Unmerged path arch/arm64/kernel/entry-common.c
* Unmerged path arch/arm64/kernel/entry.S
