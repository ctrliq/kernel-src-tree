net/mlx5: Keep only one bulk of full available DEKs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Jianbo Liu <jianbol@nvidia.com>
commit 8a6fa6df61ffc44681727ee05f051fd6df420a81
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/8a6fa6df.failed

One bulk with full available keys is left undestroyed, to service the
possible requests from users quickly.

	Signed-off-by: Jianbo Liu <jianbol@nvidia.com>
	Reviewed-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 8a6fa6df61ffc44681727ee05f051fd6df420a81)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/lib/crypto.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/lib/crypto.c
index e995f8378df7,3a94b8f8031e..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/lib/crypto.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lib/crypto.c
@@@ -71,3 -271,504 +71,507 @@@ void mlx5_destroy_encryption_key(struc
  
  	mlx5_cmd_exec(mdev, in, sizeof(in), out, sizeof(out));
  }
++<<<<<<< HEAD
++=======
+ 
+ int mlx5_create_encryption_key(struct mlx5_core_dev *mdev,
+ 			       const void *key, u32 sz_bytes,
+ 			       u32 key_type, u32 *p_key_id)
+ {
+ 	return mlx5_crypto_create_dek_key(mdev, key, sz_bytes, key_type, p_key_id);
+ }
+ 
+ void mlx5_destroy_encryption_key(struct mlx5_core_dev *mdev, u32 key_id)
+ {
+ 	mlx5_crypto_destroy_dek_key(mdev, key_id);
+ }
+ 
+ static struct mlx5_crypto_dek_bulk *
+ mlx5_crypto_dek_bulk_create(struct mlx5_crypto_dek_pool *pool)
+ {
+ 	struct mlx5_crypto_dek_priv *dek_priv = pool->mdev->mlx5e_res.dek_priv;
+ 	struct mlx5_core_dev *mdev = pool->mdev;
+ 	struct mlx5_crypto_dek_bulk *bulk;
+ 	int num_deks, base_obj_id;
+ 	int err;
+ 
+ 	bulk = kzalloc(sizeof(*bulk), GFP_KERNEL);
+ 	if (!bulk)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	num_deks = 1 << dek_priv->log_dek_obj_range;
+ 	bulk->need_sync = bitmap_zalloc(num_deks, GFP_KERNEL);
+ 	if (!bulk->need_sync) {
+ 		err = -ENOMEM;
+ 		goto err_out;
+ 	}
+ 
+ 	bulk->in_use = bitmap_zalloc(num_deks, GFP_KERNEL);
+ 	if (!bulk->in_use) {
+ 		err = -ENOMEM;
+ 		goto err_out;
+ 	}
+ 
+ 	err = mlx5_crypto_create_dek_bulk(mdev, pool->key_purpose,
+ 					  dek_priv->log_dek_obj_range,
+ 					  &base_obj_id);
+ 	if (err)
+ 		goto err_out;
+ 
+ 	bulk->base_obj_id = base_obj_id;
+ 	bulk->num_deks = num_deks;
+ 	bulk->avail_deks = num_deks;
+ 	bulk->mdev = mdev;
+ 
+ 	return bulk;
+ 
+ err_out:
+ 	bitmap_free(bulk->in_use);
+ 	bitmap_free(bulk->need_sync);
+ 	kfree(bulk);
+ 	return ERR_PTR(err);
+ }
+ 
+ static struct mlx5_crypto_dek_bulk *
+ mlx5_crypto_dek_pool_add_bulk(struct mlx5_crypto_dek_pool *pool)
+ {
+ 	struct mlx5_crypto_dek_bulk *bulk;
+ 
+ 	bulk = mlx5_crypto_dek_bulk_create(pool);
+ 	if (IS_ERR(bulk))
+ 		return bulk;
+ 
+ 	pool->avail_deks += bulk->num_deks;
+ 	pool->num_deks += bulk->num_deks;
+ 	list_add(&bulk->entry, &pool->partial_list);
+ 
+ 	return bulk;
+ }
+ 
+ static void mlx5_crypto_dek_bulk_free(struct mlx5_crypto_dek_bulk *bulk)
+ {
+ 	mlx5_crypto_destroy_dek_key(bulk->mdev, bulk->base_obj_id);
+ 	bitmap_free(bulk->need_sync);
+ 	bitmap_free(bulk->in_use);
+ 	kfree(bulk);
+ }
+ 
+ static void mlx5_crypto_dek_pool_remove_bulk(struct mlx5_crypto_dek_pool *pool,
+ 					     struct mlx5_crypto_dek_bulk *bulk,
+ 					     bool delay)
+ {
+ 	pool->num_deks -= bulk->num_deks;
+ 	pool->avail_deks -= bulk->avail_deks;
+ 	pool->in_use_deks -= bulk->in_use_deks;
+ 	list_del(&bulk->entry);
+ 	if (!delay)
+ 		mlx5_crypto_dek_bulk_free(bulk);
+ }
+ 
+ static struct mlx5_crypto_dek_bulk *
+ mlx5_crypto_dek_pool_pop(struct mlx5_crypto_dek_pool *pool, u32 *obj_offset)
+ {
+ 	struct mlx5_crypto_dek_bulk *bulk;
+ 	int pos;
+ 
+ 	mutex_lock(&pool->lock);
+ 	bulk = list_first_entry_or_null(&pool->partial_list,
+ 					struct mlx5_crypto_dek_bulk, entry);
+ 
+ 	if (bulk) {
+ 		pos = find_next_zero_bit(bulk->need_sync, bulk->num_deks,
+ 					 bulk->avail_start);
+ 		if (pos == bulk->num_deks) {
+ 			mlx5_core_err(pool->mdev, "Wrong DEK bulk avail_start.\n");
+ 			pos = find_first_zero_bit(bulk->need_sync, bulk->num_deks);
+ 		}
+ 		WARN_ON(pos == bulk->num_deks);
+ 	} else {
+ 		bulk = list_first_entry_or_null(&pool->avail_list,
+ 						struct mlx5_crypto_dek_bulk,
+ 						entry);
+ 		if (bulk) {
+ 			list_move(&bulk->entry, &pool->partial_list);
+ 		} else {
+ 			bulk = mlx5_crypto_dek_pool_add_bulk(pool);
+ 			if (IS_ERR(bulk))
+ 				goto out;
+ 		}
+ 		pos = 0;
+ 	}
+ 
+ 	*obj_offset = pos;
+ 	bitmap_set(bulk->need_sync, pos, 1);
+ 	bitmap_set(bulk->in_use, pos, 1);
+ 	bulk->in_use_deks++;
+ 	bulk->avail_deks--;
+ 	if (!bulk->avail_deks) {
+ 		list_move(&bulk->entry, &pool->full_list);
+ 		bulk->avail_start = bulk->num_deks;
+ 	} else {
+ 		bulk->avail_start = pos + 1;
+ 	}
+ 	pool->avail_deks--;
+ 	pool->in_use_deks++;
+ 
+ out:
+ 	mutex_unlock(&pool->lock);
+ 	return bulk;
+ }
+ 
+ static bool mlx5_crypto_dek_need_sync(struct mlx5_crypto_dek_pool *pool)
+ {
+ 	return !pool->syncing &&
+ 	       MLX5_CRYPTO_DEK_POOL_CALC_FREED(pool) > MLX5_CRYPTO_DEK_POOL_SYNC_THRESH;
+ }
+ 
+ static int mlx5_crypto_dek_free_locked(struct mlx5_crypto_dek_pool *pool,
+ 				       struct mlx5_crypto_dek *dek)
+ {
+ 	struct mlx5_crypto_dek_bulk *bulk = dek->bulk;
+ 	int obj_offset;
+ 	bool old_val;
+ 	int err = 0;
+ 
+ 	obj_offset = dek->obj_id - bulk->base_obj_id;
+ 	old_val = test_and_clear_bit(obj_offset, bulk->in_use);
+ 	WARN_ON_ONCE(!old_val);
+ 	if (!old_val) {
+ 		err = -ENOENT;
+ 		goto out_free;
+ 	}
+ 	pool->in_use_deks--;
+ 	bulk->in_use_deks--;
+ 	if (!bulk->avail_deks && !bulk->in_use_deks)
+ 		list_move(&bulk->entry, &pool->sync_list);
+ 
+ 	if (mlx5_crypto_dek_need_sync(pool) && schedule_work(&pool->sync_work))
+ 		pool->syncing = true;
+ 
+ out_free:
+ 	kfree(dek);
+ 	return err;
+ }
+ 
+ static int mlx5_crypto_dek_pool_push(struct mlx5_crypto_dek_pool *pool,
+ 				     struct mlx5_crypto_dek *dek)
+ {
+ 	int err = 0;
+ 
+ 	mutex_lock(&pool->lock);
+ 	if (pool->syncing)
+ 		list_add(&dek->entry, &pool->wait_for_free);
+ 	else
+ 		err = mlx5_crypto_dek_free_locked(pool, dek);
+ 	mutex_unlock(&pool->lock);
+ 
+ 	return err;
+ }
+ 
+ /* Update the bits for a bulk while sync, and avail_next for search.
+  * As the combinations of (need_sync, in_use) of one DEK are
+  *    - (0,0) means the key is ready for use,
+  *    - (1,1) means the key is currently being used by a user,
+  *    - (1,0) means the key is freed, and waiting for being synced,
+  *    - (0,1) is invalid state.
+  * the number of revalidated DEKs can be calculated by
+  * hweight_long(need_sync XOR in_use), and the need_sync bits can be reset
+  * by simply copying from in_use bits.
+  */
+ static void mlx5_crypto_dek_bulk_reset_synced(struct mlx5_crypto_dek_pool *pool,
+ 					      struct mlx5_crypto_dek_bulk *bulk)
+ {
+ 	unsigned long *need_sync = bulk->need_sync;
+ 	unsigned long *in_use = bulk->in_use;
+ 	int i, freed, reused, avail_next;
+ 	bool first = true;
+ 
+ 	freed = MLX5_CRYPTO_DEK_BULK_CALC_FREED(bulk);
+ 
+ 	for (i = 0; freed && i < BITS_TO_LONGS(bulk->num_deks);
+ 			i++, need_sync++, in_use++) {
+ 		reused = hweight_long((*need_sync) ^ (*in_use));
+ 		if (!reused)
+ 			continue;
+ 
+ 		bulk->avail_deks += reused;
+ 		pool->avail_deks += reused;
+ 		*need_sync = *in_use;
+ 		if (first) {
+ 			avail_next = i * BITS_PER_TYPE(long);
+ 			if (bulk->avail_start > avail_next)
+ 				bulk->avail_start = avail_next;
+ 			first = false;
+ 		}
+ 
+ 		freed -= reused;
+ 	}
+ }
+ 
+ /* Return true if the bulk is reused, false if destroyed with delay */
+ static bool mlx5_crypto_dek_bulk_handle_avail(struct mlx5_crypto_dek_pool *pool,
+ 					      struct mlx5_crypto_dek_bulk *bulk,
+ 					      struct list_head *destroy_list)
+ {
+ 	if (list_empty(&pool->avail_list)) {
+ 		list_move(&bulk->entry, &pool->avail_list);
+ 		return true;
+ 	}
+ 
+ 	mlx5_crypto_dek_pool_remove_bulk(pool, bulk, true);
+ 	list_add(&bulk->entry, destroy_list);
+ 	return false;
+ }
+ 
+ static void mlx5_crypto_dek_pool_splice_destroy_list(struct mlx5_crypto_dek_pool *pool,
+ 						     struct list_head *list,
+ 						     struct list_head *head)
+ {
+ 	spin_lock(&pool->destroy_lock);
+ 	list_splice_init(list, head);
+ 	spin_unlock(&pool->destroy_lock);
+ }
+ 
+ static void mlx5_crypto_dek_pool_free_wait_keys(struct mlx5_crypto_dek_pool *pool)
+ {
+ 	struct mlx5_crypto_dek *dek, *next;
+ 
+ 	list_for_each_entry_safe(dek, next, &pool->wait_for_free, entry) {
+ 		list_del(&dek->entry);
+ 		mlx5_crypto_dek_free_locked(pool, dek);
+ 	}
+ }
+ 
+ /* For all the bulks in each list, reset the bits while sync.
+  * Move them to different lists according to the number of available DEKs.
+  * Destrory all the idle bulks, except one for quick service.
+  * And free DEKs in the waiting list at the end of this func.
+  */
+ static void mlx5_crypto_dek_pool_reset_synced(struct mlx5_crypto_dek_pool *pool)
+ {
+ 	struct mlx5_crypto_dek_bulk *bulk, *tmp;
+ 	LIST_HEAD(destroy_list);
+ 
+ 	list_for_each_entry_safe(bulk, tmp, &pool->partial_list, entry) {
+ 		mlx5_crypto_dek_bulk_reset_synced(pool, bulk);
+ 		if (MLX5_CRYPTO_DEK_BULK_IDLE(bulk))
+ 			mlx5_crypto_dek_bulk_handle_avail(pool, bulk, &destroy_list);
+ 	}
+ 
+ 	list_for_each_entry_safe(bulk, tmp, &pool->full_list, entry) {
+ 		mlx5_crypto_dek_bulk_reset_synced(pool, bulk);
+ 
+ 		if (!bulk->avail_deks)
+ 			continue;
+ 
+ 		if (MLX5_CRYPTO_DEK_BULK_IDLE(bulk))
+ 			mlx5_crypto_dek_bulk_handle_avail(pool, bulk, &destroy_list);
+ 		else
+ 			list_move(&bulk->entry, &pool->partial_list);
+ 	}
+ 
+ 	list_for_each_entry_safe(bulk, tmp, &pool->sync_list, entry) {
+ 		bulk->avail_deks = bulk->num_deks;
+ 		pool->avail_deks += bulk->num_deks;
+ 		if (mlx5_crypto_dek_bulk_handle_avail(pool, bulk, &destroy_list)) {
+ 			memset(bulk->need_sync, 0, BITS_TO_BYTES(bulk->num_deks));
+ 			bulk->avail_start = 0;
+ 		}
+ 	}
+ 
+ 	mlx5_crypto_dek_pool_free_wait_keys(pool);
+ 
+ 	if (!list_empty(&destroy_list)) {
+ 		mlx5_crypto_dek_pool_splice_destroy_list(pool, &destroy_list,
+ 							 &pool->destroy_list);
+ 		schedule_work(&pool->destroy_work);
+ 	}
+ }
+ 
+ static void mlx5_crypto_dek_sync_work_fn(struct work_struct *work)
+ {
+ 	struct mlx5_crypto_dek_pool *pool =
+ 		container_of(work, struct mlx5_crypto_dek_pool, sync_work);
+ 	int err;
+ 
+ 	err = mlx5_crypto_cmd_sync_crypto(pool->mdev, BIT(pool->key_purpose));
+ 	mutex_lock(&pool->lock);
+ 	if (!err)
+ 		mlx5_crypto_dek_pool_reset_synced(pool);
+ 	pool->syncing = false;
+ 	mutex_unlock(&pool->lock);
+ }
+ 
+ struct mlx5_crypto_dek *mlx5_crypto_dek_create(struct mlx5_crypto_dek_pool *dek_pool,
+ 					       const void *key, u32 sz_bytes)
+ {
+ 	struct mlx5_crypto_dek_priv *dek_priv = dek_pool->mdev->mlx5e_res.dek_priv;
+ 	struct mlx5_core_dev *mdev = dek_pool->mdev;
+ 	u32 key_purpose = dek_pool->key_purpose;
+ 	struct mlx5_crypto_dek_bulk *bulk;
+ 	struct mlx5_crypto_dek *dek;
+ 	int obj_offset;
+ 	int err;
+ 
+ 	dek = kzalloc(sizeof(*dek), GFP_KERNEL);
+ 	if (!dek)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	if (!dek_priv) {
+ 		err = mlx5_crypto_create_dek_key(mdev, key, sz_bytes,
+ 						 key_purpose, &dek->obj_id);
+ 		goto out;
+ 	}
+ 
+ 	bulk = mlx5_crypto_dek_pool_pop(dek_pool, &obj_offset);
+ 	if (IS_ERR(bulk)) {
+ 		err = PTR_ERR(bulk);
+ 		goto out;
+ 	}
+ 
+ 	dek->bulk = bulk;
+ 	dek->obj_id = bulk->base_obj_id + obj_offset;
+ 	err = mlx5_crypto_modify_dek_key(mdev, key, sz_bytes, key_purpose,
+ 					 bulk->base_obj_id, obj_offset);
+ 	if (err) {
+ 		mlx5_crypto_dek_pool_push(dek_pool, dek);
+ 		return ERR_PTR(err);
+ 	}
+ 
+ out:
+ 	if (err) {
+ 		kfree(dek);
+ 		return ERR_PTR(err);
+ 	}
+ 
+ 	return dek;
+ }
+ 
+ void mlx5_crypto_dek_destroy(struct mlx5_crypto_dek_pool *dek_pool,
+ 			     struct mlx5_crypto_dek *dek)
+ {
+ 	struct mlx5_crypto_dek_priv *dek_priv = dek_pool->mdev->mlx5e_res.dek_priv;
+ 	struct mlx5_core_dev *mdev = dek_pool->mdev;
+ 
+ 	if (!dek_priv) {
+ 		mlx5_crypto_destroy_dek_key(mdev, dek->obj_id);
+ 		kfree(dek);
+ 	} else {
+ 		mlx5_crypto_dek_pool_push(dek_pool, dek);
+ 	}
+ }
+ 
+ static void mlx5_crypto_dek_free_destroy_list(struct list_head *destroy_list)
+ {
+ 	struct mlx5_crypto_dek_bulk *bulk, *tmp;
+ 
+ 	list_for_each_entry_safe(bulk, tmp, destroy_list, entry)
+ 		mlx5_crypto_dek_bulk_free(bulk);
+ }
+ 
+ static void mlx5_crypto_dek_destroy_work_fn(struct work_struct *work)
+ {
+ 	struct mlx5_crypto_dek_pool *pool =
+ 		container_of(work, struct mlx5_crypto_dek_pool, destroy_work);
+ 	LIST_HEAD(destroy_list);
+ 
+ 	mlx5_crypto_dek_pool_splice_destroy_list(pool, &pool->destroy_list,
+ 						 &destroy_list);
+ 	mlx5_crypto_dek_free_destroy_list(&destroy_list);
+ }
+ 
+ struct mlx5_crypto_dek_pool *
+ mlx5_crypto_dek_pool_create(struct mlx5_core_dev *mdev, int key_purpose)
+ {
+ 	struct mlx5_crypto_dek_pool *pool;
+ 
+ 	pool = kzalloc(sizeof(*pool), GFP_KERNEL);
+ 	if (!pool)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	pool->mdev = mdev;
+ 	pool->key_purpose = key_purpose;
+ 
+ 	mutex_init(&pool->lock);
+ 	INIT_LIST_HEAD(&pool->avail_list);
+ 	INIT_LIST_HEAD(&pool->partial_list);
+ 	INIT_LIST_HEAD(&pool->full_list);
+ 	INIT_LIST_HEAD(&pool->sync_list);
+ 	INIT_LIST_HEAD(&pool->wait_for_free);
+ 	INIT_WORK(&pool->sync_work, mlx5_crypto_dek_sync_work_fn);
+ 	spin_lock_init(&pool->destroy_lock);
+ 	INIT_LIST_HEAD(&pool->destroy_list);
+ 	INIT_WORK(&pool->destroy_work, mlx5_crypto_dek_destroy_work_fn);
+ 
+ 	return pool;
+ }
+ 
+ void mlx5_crypto_dek_pool_destroy(struct mlx5_crypto_dek_pool *pool)
+ {
+ 	struct mlx5_crypto_dek_bulk *bulk, *tmp;
+ 
+ 	cancel_work_sync(&pool->sync_work);
+ 	cancel_work_sync(&pool->destroy_work);
+ 
+ 	mlx5_crypto_dek_pool_free_wait_keys(pool);
+ 
+ 	list_for_each_entry_safe(bulk, tmp, &pool->avail_list, entry)
+ 		mlx5_crypto_dek_pool_remove_bulk(pool, bulk, false);
+ 
+ 	list_for_each_entry_safe(bulk, tmp, &pool->full_list, entry)
+ 		mlx5_crypto_dek_pool_remove_bulk(pool, bulk, false);
+ 
+ 	list_for_each_entry_safe(bulk, tmp, &pool->sync_list, entry)
+ 		mlx5_crypto_dek_pool_remove_bulk(pool, bulk, false);
+ 
+ 	list_for_each_entry_safe(bulk, tmp, &pool->partial_list, entry)
+ 		mlx5_crypto_dek_pool_remove_bulk(pool, bulk, false);
+ 
+ 	mlx5_crypto_dek_free_destroy_list(&pool->destroy_list);
+ 
+ 	mutex_destroy(&pool->lock);
+ 
+ 	kfree(pool);
+ }
+ 
+ void mlx5_crypto_dek_cleanup(struct mlx5_crypto_dek_priv *dek_priv)
+ {
+ 	if (!dek_priv)
+ 		return;
+ 
+ 	kfree(dek_priv);
+ }
+ 
+ struct mlx5_crypto_dek_priv *mlx5_crypto_dek_init(struct mlx5_core_dev *mdev)
+ {
+ 	struct mlx5_crypto_dek_priv *dek_priv;
+ 	int err;
+ 
+ 	if (!MLX5_CAP_CRYPTO(mdev, log_dek_max_alloc))
+ 		return NULL;
+ 
+ 	dek_priv = kzalloc(sizeof(*dek_priv), GFP_KERNEL);
+ 	if (!dek_priv)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	dek_priv->mdev = mdev;
+ 	dek_priv->log_dek_obj_range = min_t(int, 12,
+ 					    MLX5_CAP_CRYPTO(mdev, log_dek_max_alloc));
+ 
+ 	/* sync all types of objects */
+ 	err = mlx5_crypto_cmd_sync_crypto(mdev, MLX5_CRYPTO_DEK_ALL_TYPE);
+ 	if (err)
+ 		goto err_sync_crypto;
+ 
+ 	mlx5_core_dbg(mdev, "Crypto DEK enabled, %d deks per alloc (max %d), total %d\n",
+ 		      1 << dek_priv->log_dek_obj_range,
+ 		      1 << MLX5_CAP_CRYPTO(mdev, log_dek_max_alloc),
+ 		      1 << MLX5_CAP_CRYPTO(mdev, log_max_num_deks));
+ 
+ 	return dek_priv;
+ 
+ err_sync_crypto:
+ 	kfree(dek_priv);
+ 	return ERR_PTR(err);
+ }
++>>>>>>> 8a6fa6df61ff (net/mlx5: Keep only one bulk of full available DEKs)
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/lib/crypto.c
