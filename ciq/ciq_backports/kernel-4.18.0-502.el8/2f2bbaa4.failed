arm64: entry: organise entry handlers consistently

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-502.el8
commit-author Mark Rutland <mark.rutland@arm.com>
commit 2f2bbaa4eda027d0bf0f3f23d0c206b2b76e2180
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-502.el8/2f2bbaa4.failed

In entry.S we have two comments which distinguish EL0 and EL1 exception
handlers, but the code isn't actually laid out to match, and there are a
few other inconsistencies that would be good to clear up.

This patch organizes the entry handers consistently:

* The handlers are laid out in order of the vectors, to make them easier
  to navigate.

* The inconsistently-applied alignment is removed

* The handlers are consistently marked with SYM_CODE_START_LOCAL()
  rather than SYM_CODE_START_LOCAL_NOALIGN(), giving them the same
  default alignment as other assembly code snippets.

There should be no functional change as a result of this patch.

	Signed-off-by: Mark Rutland <mark.rutland@arm.com>
	Acked-by: Catalin Marinas <catalin.marinas@arm.com>
	Acked-by: Marc Zyngier <maz@kernel.org>
	Reviewed-by: Joey Gouly <joey.gouly@arm.com>
	Cc: James Morse <james.morse@arm.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lore.kernel.org/r/20210607094624.34689-9-mark.rutland@arm.com
	Signed-off-by: Will Deacon <will@kernel.org>
(cherry picked from commit 2f2bbaa4eda027d0bf0f3f23d0c206b2b76e2180)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kernel/entry.S
diff --cc arch/arm64/kernel/entry.S
index 346ef18f2448,ed7c55d57afe..000000000000
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@@ -580,132 -614,55 +579,158 @@@ SYM_CODE_START_LOCAL(el1_sync
  	kernel_exit 1
  SYM_CODE_END(el1_sync)
  
- 	.align	6
- SYM_CODE_START_LOCAL_NOALIGN(el1_irq)
+ SYM_CODE_START_LOCAL(el1_irq)
  	kernel_entry 1
 +	gic_prio_irq_setup pmr=x20, tmp=x1
 +	enable_da_f
 +
 +	mov	x0, sp
 +	bl	enter_el1_irq_or_nmi
 +
 +	irq_handler
 +
 +#ifdef CONFIG_PREEMPT
 +	ldr	x24, [tsk, #TSK_TI_PREEMPT]	// get preempt count
 +alternative_if ARM64_HAS_IRQ_PRIO_MASKING
 +	/*
 +	 * DA_F were cleared at start of handling. If anything is set in DAIF,
 +	 * we come back from an NMI, so skip preemption
 +	 */
 +	mrs	x0, daif
 +	orr	x24, x24, x0
 +alternative_else_nop_endif
 +	cbnz	x24, 1f				// preempt count != 0 || NMI return path
 +	bl	arm64_preempt_schedule_irq	// irq en/disable is done inside
 +1:
 +#endif
 +
  	mov	x0, sp
 -	bl	el1_irq_handler
 +	bl	exit_el1_irq_or_nmi
 +
  	kernel_exit 1
  SYM_CODE_END(el1_irq)
  
- SYM_CODE_START_LOCAL_NOALIGN(el1_fiq)
+ SYM_CODE_START_LOCAL(el1_fiq)
  	kernel_entry 1
 -	mov	x0, sp
 -	bl	el1_fiq_handler
 +	el1_interrupt_handler handle_arch_fiq
  	kernel_exit 1
  SYM_CODE_END(el1_fiq)
  
++<<<<<<< HEAD
 +/*
 + * EL0 mode handlers.
 + */
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_sync)
 +	kernel_entry 0
 +	mov	x0, sp
 +	bl	el0_sync_handler
 +	b	ret_to_user
 +SYM_CODE_END(el0_sync)
 +
 +#ifdef CONFIG_COMPAT
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_sync_compat)
 +	kernel_entry 0, 32
 +	mov	x0, sp
 +	bl	el0_sync_compat_handler
 +	b	ret_to_user
 +SYM_CODE_END(el0_sync_compat)
 +
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_irq_compat)
 +	kernel_entry 0, 32
 +	b	el0_irq_naked
 +SYM_CODE_END(el0_irq_compat)
 +
 +SYM_CODE_START_LOCAL_NOALIGN(el0_fiq_compat)
 +	kernel_entry 0, 32
 +	b	el0_fiq_naked
 +SYM_CODE_END(el0_fiq_compat)
 +
 +SYM_CODE_START_LOCAL_NOALIGN(el0_error_compat)
 +	kernel_entry 0, 32
 +	b	el0_error_naked
 +SYM_CODE_END(el0_error_compat)
 +#endif
 +
 +	.align	6
 +SYM_CODE_START_LOCAL_NOALIGN(el0_irq)
 +	kernel_entry 0
 +el0_irq_naked:
 +	gic_prio_irq_setup pmr=x20, tmp=x0
 +	ct_user_exit_irqoff
 +	enable_da_f
 +
 +#ifdef CONFIG_TRACE_IRQFLAGS
 +	bl	trace_hardirqs_off
 +#endif
 +
 +	tbz	x22, #55, 1f
 +	bl	do_el0_irq_bp_hardening
 +1:
 +	irq_handler
 +
 +#ifdef CONFIG_TRACE_IRQFLAGS
 +	bl	trace_hardirqs_on
 +#endif
 +	b	ret_to_user
 +SYM_CODE_END(el0_irq)
 +
 +SYM_CODE_START_LOCAL_NOALIGN(el0_fiq)
 +	kernel_entry 0
 +el0_fiq_naked:
 +	el0_interrupt_handler handle_arch_fiq
 +	b	ret_to_user
 +SYM_CODE_END(el0_fiq)
 +
++=======
++>>>>>>> 2f2bbaa4eda0 (arm64: entry: organise entry handlers consistently)
  SYM_CODE_START_LOCAL(el1_error)
  	kernel_entry 1
 +	mrs	x1, esr_el1
 +	gic_prio_kentry_setup tmp=x2
 +	enable_dbg
  	mov	x0, sp
 -	bl	el1_error_handler
 +	bl	do_serror
  	kernel_exit 1
  SYM_CODE_END(el1_error)
  
+ /*
+  * EL0 mode handlers.
+  */
+ SYM_CODE_START_LOCAL(el0_sync)
+ 	kernel_entry 0
+ 	mov	x0, sp
+ 	bl	el0_sync_handler
+ 	b	ret_to_user
+ SYM_CODE_END(el0_sync)
+ 
+ SYM_CODE_START_LOCAL(el0_irq)
+ 	kernel_entry 0
+ 	mov	x0, sp
+ 	bl	el0_irq_handler
+ 	b	ret_to_user
+ SYM_CODE_END(el0_irq)
+ 
+ SYM_CODE_START_LOCAL(el0_fiq)
+ 	kernel_entry 0
+ 	mov	x0, sp
+ 	bl	el0_fiq_handler
+ 	b	ret_to_user
+ SYM_CODE_END(el0_fiq)
+ 
  SYM_CODE_START_LOCAL(el0_error)
  	kernel_entry 0
 +el0_error_naked:
 +	mrs	x25, esr_el1
 +	gic_prio_kentry_setup tmp=x2
 +	ct_user_exit_irqoff
 +	enable_dbg
  	mov	x0, sp
 -	bl	el0_error_handler
 +	mov	x1, x25
 +	bl	do_serror
 +	enable_da_f
  	b	ret_to_user
  SYM_CODE_END(el0_error)
  
* Unmerged path arch/arm64/kernel/entry.S
