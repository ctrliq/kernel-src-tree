gfs2: Update to the evict / remote delete documentation

jira LE-2290
Rebuild_History Non-Buildable kernel-5.14.0-503.21.1.el9_5
commit-author Andreas Gruenbacher <agruenba@redhat.com>
commit a6033333ccce01ecada39b3ddabc03fd967e60c0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-5.14.0-503.21.1.el9_5/a6033333.failed

Try to be a bit more clear and remove some duplications.  We cannot
actually get rid of the verification step eventually, so remove the
comment saying so.

	Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
(cherry picked from commit a6033333ccce01ecada39b3ddabc03fd967e60c0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/gfs2/glock.c
diff --cc fs/gfs2/glock.c
index 65d0f1b4bee1,8fff36846145..000000000000
--- a/fs/gfs2/glock.c
+++ b/fs/gfs2/glock.c
@@@ -1042,32 -1030,8 +1045,37 @@@ static void delete_work_func(struct wor
  	struct gfs2_sbd *sdp = gl->gl_name.ln_sbd;
  	bool verify_delete = test_and_clear_bit(GLF_VERIFY_DELETE, &gl->gl_flags);
  
++<<<<<<< HEAD
 +	if (test_and_clear_bit(GLF_TRY_TO_EVICT, &gl->gl_flags)) {
 +		/*
 +		 * If we can evict the inode, give the remote node trying to
 +		 * delete the inode some time before verifying that the delete
 +		 * has happened.  Otherwise, if we cause contention on the inode glock
 +		 * immediately, the remote node will think that we still have
 +		 * the inode in use, and so it will give up waiting.
 +		 *
 +		 * If we can't evict the inode, signal to the remote node that
 +		 * the inode is still in use.  We'll later try to delete the
 +		 * inode locally in gfs2_evict_inode.
 +		 *
 +		 * FIXME: We only need to verify that the remote node has
 +		 * deleted the inode because nodes before this remote delete
 +		 * rework won't cooperate.  At a later time, when we no longer
 +		 * care about compatibility with such nodes, we can skip this
 +		 * step entirely.
 +		 */
 +		if (gfs2_try_evict(gl)) {
 +			if (!test_bit(SDF_KILL, &sdp->sd_flags)) {
 +				gfs2_glock_hold(gl);
 +				if (!gfs2_queue_verify_delete(gl, true))
 +					gfs2_glock_put(gl);
 +			}
 +		}
 +	}
++=======
+ 	if (test_and_clear_bit(GLF_TRY_TO_EVICT, &gl->gl_flags))
+ 		gfs2_try_evict(gl);
++>>>>>>> a6033333ccce (gfs2: Update to the evict / remote delete documentation)
  
  	if (verify_delete) {
  		u64 no_addr = gl->gl_name.ln_number;
* Unmerged path fs/gfs2/glock.c
diff --git a/fs/gfs2/super.c b/fs/gfs2/super.c
index 245ffcb2e060..92253b406649 100644
--- a/fs/gfs2/super.c
+++ b/fs/gfs2/super.c
@@ -1286,9 +1286,9 @@ static bool gfs2_upgrade_iopen_glock(struct inode *inode)
 	 * exclusive access to the iopen glock here.
 	 *
 	 * Otherwise, the other nodes holding the lock will be notified about
-	 * our locking request.  If they do not have the inode open, they are
-	 * expected to evict the cached inode and release the lock, allowing us
-	 * to proceed.
+	 * our locking request (see iopen_go_callback()).  If they do not have
+	 * the inode open, they are expected to evict the cached inode and
+	 * release the lock, allowing us to proceed.
 	 *
 	 * Otherwise, if they cannot evict the inode, they are expected to poke
 	 * the inode glock (note: not the iopen glock).  We will notice that
