cpufreq: amd-pstate: Add guided mode control support via sysfs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-518.el8
commit-author Wyes Karny <wyes.karny@amd.com>
commit 3ca7bc818d8ccf399cb0366d8d9a915c04a446f9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-518.el8/3ca7bc81.failed

amd_pstate driver's `status` sysfs entry helps to control the driver's
mode dynamically by user. After the addition of guided mode the
combinations of mode transitions have been increased (16 combinations).
Therefore optimise the amd_pstate_update_status function by implementing
a state transition table.

There are 4 states amd_pstate supports, namely: 'disable', 'passive',
'active', and 'guided'.  The transition from any state to any other
state is possible after this change.

Sysfs interface:

To disable amd_pstate driver:
 # echo disable > /sys/devices/system/cpu/amd_pstate/status

To enable passive mode:
 # echo passive > /sys/devices/system/cpu/amd_pstate/status

To change mode to active:
 # echo active > /sys/devices/system/cpu/amd_pstate/status

To change mode to guided:
 # echo guided > /sys/devices/system/cpu/amd_pstate/status

	Acked-by: Huang Rui <ray.huang@amd.com>
	Reviewed-by: Mario Limonciello <mario.limonciello@amd.com>
	Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
	Signed-off-by: Wyes Karny <wyes.karny@amd.com>
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit 3ca7bc818d8ccf399cb0366d8d9a915c04a446f9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/amd-pstate.c
diff --cc drivers/cpufreq/amd-pstate.c
index 2c71ef90154d,7955cfc91c31..000000000000
--- a/drivers/cpufreq/amd-pstate.c
+++ b/drivers/cpufreq/amd-pstate.c
@@@ -58,68 -59,174 +58,200 @@@
   * we disable it by default to go acpi-cpufreq on these processors and add a
   * module parameter to be able to enable it manually for debugging.
   */
 -static struct cpufreq_driver *current_pstate_driver;
  static struct cpufreq_driver amd_pstate_driver;
 -static struct cpufreq_driver amd_pstate_epp_driver;
 -static int cppc_state = AMD_PSTATE_DISABLE;
 -struct kobject *amd_pstate_kobj;
 +static int cppc_load __initdata;
  
 -/*
 - * AMD Energy Preference Performance (EPP)
 - * The EPP is used in the CCLK DPM controller to drive
 - * the frequency that a core is going to operate during
 - * short periods of activity. EPP values will be utilized for
 - * different OS profiles (balanced, performance, power savings)
 - * display strings corresponding to EPP index in the
 - * energy_perf_strings[]
 - *	index		String
 - *-------------------------------------
 - *	0		default
 - *	1		performance
 - *	2		balance_performance
 - *	3		balance_power
 - *	4		power
 +/**
 + * struct  amd_aperf_mperf
 + * @aperf: actual performance frequency clock count
 + * @mperf: maximum performance frequency clock count
 + * @tsc:   time stamp counter
   */
 -enum energy_perf_value_index {
 -	EPP_INDEX_DEFAULT = 0,
 -	EPP_INDEX_PERFORMANCE,
 -	EPP_INDEX_BALANCE_PERFORMANCE,
 -	EPP_INDEX_BALANCE_POWERSAVE,
 -	EPP_INDEX_POWERSAVE,
 +struct amd_aperf_mperf {
 +	u64 aperf;
 +	u64 mperf;
 +	u64 tsc;
  };
  
 -static const char * const energy_perf_strings[] = {
 -	[EPP_INDEX_DEFAULT] = "default",
 -	[EPP_INDEX_PERFORMANCE] = "performance",
 -	[EPP_INDEX_BALANCE_PERFORMANCE] = "balance_performance",
 -	[EPP_INDEX_BALANCE_POWERSAVE] = "balance_power",
 -	[EPP_INDEX_POWERSAVE] = "power",
 -	NULL
 +/**
 + * struct amd_cpudata - private CPU data for AMD P-State
 + * @cpu: CPU number
 + * @req: constraint request to apply
 + * @cppc_req_cached: cached performance request hints
 + * @highest_perf: the maximum performance an individual processor may reach,
 + *		  assuming ideal conditions
 + * @nominal_perf: the maximum sustained performance level of the processor,
 + *		  assuming ideal operating conditions
 + * @lowest_nonlinear_perf: the lowest performance level at which nonlinear power
 + *			   savings are achieved
 + * @lowest_perf: the absolute lowest performance level of the processor
 + * @max_freq: the frequency that mapped to highest_perf
 + * @min_freq: the frequency that mapped to lowest_perf
 + * @nominal_freq: the frequency that mapped to nominal_perf
 + * @lowest_nonlinear_freq: the frequency that mapped to lowest_nonlinear_perf
 + * @cur: Difference of Aperf/Mperf/tsc count between last and current sample
 + * @prev: Last Aperf/Mperf/tsc count value read from register
 + * @freq: current cpu frequency value
 + * @boost_supported: check whether the Processor or SBIOS supports boost mode
 + *
 + * The amd_cpudata is key private data for each CPU thread in AMD P-State, and
 + * represents all the attributes and goals that AMD P-State requests at runtime.
 + */
 +struct amd_cpudata {
 +	int	cpu;
 +
 +	struct	freq_qos_request req[2];
 +	u64	cppc_req_cached;
 +
 +	u32	highest_perf;
 +	u32	nominal_perf;
 +	u32	lowest_nonlinear_perf;
 +	u32	lowest_perf;
 +
 +	u32	max_freq;
 +	u32	min_freq;
 +	u32	nominal_freq;
 +	u32	lowest_nonlinear_freq;
 +
 +	struct amd_aperf_mperf cur;
 +	struct amd_aperf_mperf prev;
 +
 +	u64 	freq;
 +	bool	boost_supported;
  };
  
++<<<<<<< HEAD
++=======
+ static unsigned int epp_values[] = {
+ 	[EPP_INDEX_DEFAULT] = 0,
+ 	[EPP_INDEX_PERFORMANCE] = AMD_CPPC_EPP_PERFORMANCE,
+ 	[EPP_INDEX_BALANCE_PERFORMANCE] = AMD_CPPC_EPP_BALANCE_PERFORMANCE,
+ 	[EPP_INDEX_BALANCE_POWERSAVE] = AMD_CPPC_EPP_BALANCE_POWERSAVE,
+ 	[EPP_INDEX_POWERSAVE] = AMD_CPPC_EPP_POWERSAVE,
+  };
+ 
+ typedef int (*cppc_mode_transition_fn)(int);
+ 
+ static inline int get_mode_idx_from_str(const char *str, size_t size)
+ {
+ 	int i;
+ 
+ 	for (i=0; i < AMD_PSTATE_MAX; i++) {
+ 		if (!strncmp(str, amd_pstate_mode_string[i], size))
+ 			return i;
+ 	}
+ 	return -EINVAL;
+ }
+ 
+ static DEFINE_MUTEX(amd_pstate_limits_lock);
+ static DEFINE_MUTEX(amd_pstate_driver_lock);
+ 
+ static s16 amd_pstate_get_epp(struct amd_cpudata *cpudata, u64 cppc_req_cached)
+ {
+ 	u64 epp;
+ 	int ret;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		if (!cppc_req_cached) {
+ 			epp = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ,
+ 					&cppc_req_cached);
+ 			if (epp)
+ 				return epp;
+ 		}
+ 		epp = (cppc_req_cached >> 24) & 0xFF;
+ 	} else {
+ 		ret = cppc_get_epp_perf(cpudata->cpu, &epp);
+ 		if (ret < 0) {
+ 			pr_debug("Could not retrieve energy perf value (%d)\n", ret);
+ 			return -EIO;
+ 		}
+ 	}
+ 
+ 	return (s16)(epp & 0xff);
+ }
+ 
+ static int amd_pstate_get_energy_pref_index(struct amd_cpudata *cpudata)
+ {
+ 	s16 epp;
+ 	int index = -EINVAL;
+ 
+ 	epp = amd_pstate_get_epp(cpudata, 0);
+ 	if (epp < 0)
+ 		return epp;
+ 
+ 	switch (epp) {
+ 	case AMD_CPPC_EPP_PERFORMANCE:
+ 		index = EPP_INDEX_PERFORMANCE;
+ 		break;
+ 	case AMD_CPPC_EPP_BALANCE_PERFORMANCE:
+ 		index = EPP_INDEX_BALANCE_PERFORMANCE;
+ 		break;
+ 	case AMD_CPPC_EPP_BALANCE_POWERSAVE:
+ 		index = EPP_INDEX_BALANCE_POWERSAVE;
+ 		break;
+ 	case AMD_CPPC_EPP_POWERSAVE:
+ 		index = EPP_INDEX_POWERSAVE;
+ 		break;
+ 	default:
+ 		break;
+ 	}
+ 
+ 	return index;
+ }
+ 
+ static int amd_pstate_set_epp(struct amd_cpudata *cpudata, u32 epp)
+ {
+ 	int ret;
+ 	struct cppc_perf_ctrls perf_ctrls;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		u64 value = READ_ONCE(cpudata->cppc_req_cached);
+ 
+ 		value &= ~GENMASK_ULL(31, 24);
+ 		value |= (u64)epp << 24;
+ 		WRITE_ONCE(cpudata->cppc_req_cached, value);
+ 
+ 		ret = wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
+ 		if (!ret)
+ 			cpudata->epp_cached = epp;
+ 	} else {
+ 		perf_ctrls.energy_perf = epp;
+ 		ret = cppc_set_epp_perf(cpudata->cpu, &perf_ctrls, 1);
+ 		if (ret) {
+ 			pr_debug("failed to set energy perf value (%d)\n", ret);
+ 			return ret;
+ 		}
+ 		cpudata->epp_cached = epp;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static int amd_pstate_set_energy_pref_index(struct amd_cpudata *cpudata,
+ 		int pref_index)
+ {
+ 	int epp = -EINVAL;
+ 	int ret;
+ 
+ 	if (!pref_index) {
+ 		pr_debug("EPP pref_index is invalid\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (epp == -EINVAL)
+ 		epp = epp_values[pref_index];
+ 
+ 	if (epp > 0 && cpudata->policy == CPUFREQ_POLICY_PERFORMANCE) {
+ 		pr_debug("EPP cannot be set under performance policy\n");
+ 		return -EBUSY;
+ 	}
+ 
+ 	ret = amd_pstate_set_epp(cpudata, epp);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 3ca7bc818d8c (cpufreq: amd-pstate: Add guided mode control support via sysfs)
  static inline int pstate_enable(bool enable)
  {
  	return wrmsrl_safe(MSR_AMD_CPPC_ENABLE, enable);
@@@ -650,6 -791,198 +782,201 @@@ static ssize_t show_amd_pstate_highest_
  	return sysfs_emit(buf, "%u\n", perf);
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t show_energy_performance_available_preferences(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	int i = 0;
+ 	int offset = 0;
+ 
+ 	while (energy_perf_strings[i] != NULL)
+ 		offset += sysfs_emit_at(buf, offset, "%s ", energy_perf_strings[i++]);
+ 
+ 	sysfs_emit_at(buf, offset, "\n");
+ 
+ 	return offset;
+ }
+ 
+ static ssize_t store_energy_performance_preference(
+ 		struct cpufreq_policy *policy, const char *buf, size_t count)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	char str_preference[21];
+ 	ssize_t ret;
+ 
+ 	ret = sscanf(buf, "%20s", str_preference);
+ 	if (ret != 1)
+ 		return -EINVAL;
+ 
+ 	ret = match_string(energy_perf_strings, -1, str_preference);
+ 	if (ret < 0)
+ 		return -EINVAL;
+ 
+ 	mutex_lock(&amd_pstate_limits_lock);
+ 	ret = amd_pstate_set_energy_pref_index(cpudata, ret);
+ 	mutex_unlock(&amd_pstate_limits_lock);
+ 
+ 	return ret ?: count;
+ }
+ 
+ static ssize_t show_energy_performance_preference(
+ 				struct cpufreq_policy *policy, char *buf)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	int preference;
+ 
+ 	preference = amd_pstate_get_energy_pref_index(cpudata);
+ 	if (preference < 0)
+ 		return preference;
+ 
+ 	return sysfs_emit(buf, "%s\n", energy_perf_strings[preference]);
+ }
+ 
+ static void amd_pstate_driver_cleanup(void)
+ {
+ 	amd_pstate_enable(false);
+ 	cppc_state = AMD_PSTATE_DISABLE;
+ 	current_pstate_driver = NULL;
+ }
+ 
+ static int amd_pstate_register_driver(int mode)
+ {
+ 	int ret;
+ 
+ 	if (mode == AMD_PSTATE_PASSIVE || mode == AMD_PSTATE_GUIDED)
+ 		current_pstate_driver = &amd_pstate_driver;
+ 	else if (mode == AMD_PSTATE_ACTIVE)
+ 		current_pstate_driver = &amd_pstate_epp_driver;
+ 	else
+ 		return -EINVAL;
+ 
+ 	cppc_state = mode;
+ 	ret = cpufreq_register_driver(current_pstate_driver);
+ 	if (ret) {
+ 		amd_pstate_driver_cleanup();
+ 		return ret;
+ 	}
+ 	return 0;
+ }
+ 
+ static int amd_pstate_unregister_driver(int dummy)
+ {
+ 	cpufreq_unregister_driver(current_pstate_driver);
+ 	amd_pstate_driver_cleanup();
+ 	return 0;
+ }
+ 
+ static int amd_pstate_change_mode_without_dvr_change(int mode)
+ {
+ 	int cpu = 0;
+ 
+ 	cppc_state = mode;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CPPC) || cppc_state == AMD_PSTATE_ACTIVE)
+ 		return 0;
+ 
+ 	for_each_present_cpu(cpu) {
+ 		cppc_set_auto_sel(cpu, (cppc_state == AMD_PSTATE_PASSIVE) ? 0 : 1);
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static int amd_pstate_change_driver_mode(int mode)
+ {
+ 	int ret;
+ 
+ 	ret = amd_pstate_unregister_driver(0);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = amd_pstate_register_driver(mode);
+ 	if (ret)
+ 		return ret;
+ 
+ 	return 0;
+ }
+ 
+ cppc_mode_transition_fn mode_state_machine[AMD_PSTATE_MAX][AMD_PSTATE_MAX] = {
+ 	[AMD_PSTATE_DISABLE]         = {
+ 		[AMD_PSTATE_DISABLE]     = NULL,
+ 		[AMD_PSTATE_PASSIVE]     = amd_pstate_register_driver,
+ 		[AMD_PSTATE_ACTIVE]      = amd_pstate_register_driver,
+ 		[AMD_PSTATE_GUIDED]      = amd_pstate_register_driver,
+ 	},
+ 	[AMD_PSTATE_PASSIVE]         = {
+ 		[AMD_PSTATE_DISABLE]     = amd_pstate_unregister_driver,
+ 		[AMD_PSTATE_PASSIVE]     = NULL,
+ 		[AMD_PSTATE_ACTIVE]      = amd_pstate_change_driver_mode,
+ 		[AMD_PSTATE_GUIDED]      = amd_pstate_change_mode_without_dvr_change,
+ 	},
+ 	[AMD_PSTATE_ACTIVE]          = {
+ 		[AMD_PSTATE_DISABLE]     = amd_pstate_unregister_driver,
+ 		[AMD_PSTATE_PASSIVE]     = amd_pstate_change_driver_mode,
+ 		[AMD_PSTATE_ACTIVE]      = NULL,
+ 		[AMD_PSTATE_GUIDED]      = amd_pstate_change_driver_mode,
+ 	},
+ 	[AMD_PSTATE_GUIDED]          = {
+ 		[AMD_PSTATE_DISABLE]     = amd_pstate_unregister_driver,
+ 		[AMD_PSTATE_PASSIVE]     = amd_pstate_change_mode_without_dvr_change,
+ 		[AMD_PSTATE_ACTIVE]      = amd_pstate_change_driver_mode,
+ 		[AMD_PSTATE_GUIDED]      = NULL,
+ 	},
+ };
+ 
+ static ssize_t amd_pstate_show_status(char *buf)
+ {
+ 	if (!current_pstate_driver)
+ 		return sysfs_emit(buf, "disable\n");
+ 
+ 	return sysfs_emit(buf, "%s\n", amd_pstate_mode_string[cppc_state]);
+ }
+ 
+ static int amd_pstate_update_status(const char *buf, size_t size)
+ {
+ 	int mode_idx;
+ 
+ 	if (size > strlen("passive") || size < strlen("active"))
+ 		return -EINVAL;
+ 
+ 	mode_idx = get_mode_idx_from_str(buf, size);
+ 
+ 	if (mode_idx < 0 || mode_idx >= AMD_PSTATE_MAX)
+ 		return -EINVAL;
+ 
+ 	if (mode_state_machine[cppc_state][mode_idx])
+ 		return mode_state_machine[cppc_state][mode_idx](mode_idx);
+ 
+ 	return 0;
+ }
+ 
+ static ssize_t show_status(struct kobject *kobj,
+ 			   struct kobj_attribute *attr, char *buf)
+ {
+ 	ssize_t ret;
+ 
+ 	mutex_lock(&amd_pstate_driver_lock);
+ 	ret = amd_pstate_show_status(buf);
+ 	mutex_unlock(&amd_pstate_driver_lock);
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t store_status(struct kobject *a, struct kobj_attribute *b,
+ 			    const char *buf, size_t count)
+ {
+ 	char *p = memchr(buf, '\n', count);
+ 	int ret;
+ 
+ 	mutex_lock(&amd_pstate_driver_lock);
+ 	ret = amd_pstate_update_status(buf, p ? p - buf : count);
+ 	mutex_unlock(&amd_pstate_driver_lock);
+ 
+ 	return ret < 0 ? ret : count;
+ }
+ 
++>>>>>>> 3ca7bc818d8c (cpufreq: amd-pstate: Add guided mode control support via sysfs)
  cpufreq_freq_attr_ro(amd_pstate_max_freq);
  cpufreq_freq_attr_ro(amd_pstate_lowest_nonlinear_freq);
  
* Unmerged path drivers/cpufreq/amd-pstate.c
