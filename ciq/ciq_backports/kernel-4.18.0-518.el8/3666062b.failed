cpufreq: amd-pstate: move to use bus_get_dev_root()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-518.el8
commit-author Greg Kroah-Hartman <gregkh@linuxfoundation.org>
commit 3666062b87ec8be4b85dc475dfb54bb17e10a7f6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-518.el8/3666062b.failed

Direct access to the struct bus_type dev_root pointer is going away soon
so replace that with a call to bus_get_dev_root() instead, which is what
it is there for.

In doing so, remove the unneded kobject structure that was only being
created to cause a subdirectory for the attributes.  The name of the
attribute group is the correct way to do this, saving code and
complexity as well as allowing the attributes to properly show up to
userspace tools (the raw kobject would not allow that.)

	Cc: "Rafael J. Wysocki" <rafael@kernel.org>
	Cc: Viresh Kumar <viresh.kumar@linaro.org>
	Cc: linux-pm@vger.kernel.org
	Acked-by: Huang Rui <ray.huang@.amd.com>
Link: https://lore.kernel.org/r/20230313182918.1312597-20-gregkh@linuxfoundation.org
	Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
(cherry picked from commit 3666062b87ec8be4b85dc475dfb54bb17e10a7f6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/amd-pstate.c
diff --cc drivers/cpufreq/amd-pstate.c
index 2c71ef90154d,b92454c50118..000000000000
--- a/drivers/cpufreq/amd-pstate.c
+++ b/drivers/cpufreq/amd-pstate.c
@@@ -58,67 -59,170 +58,72 @@@
   * we disable it by default to go acpi-cpufreq on these processors and add a
   * module parameter to be able to enable it manually for debugging.
   */
 -static struct cpufreq_driver *current_pstate_driver;
  static struct cpufreq_driver amd_pstate_driver;
++<<<<<<< HEAD
 +static int cppc_load __initdata;
++=======
+ static struct cpufreq_driver amd_pstate_epp_driver;
+ static int cppc_state = AMD_PSTATE_DISABLE;
++>>>>>>> 3666062b87ec (cpufreq: amd-pstate: move to use bus_get_dev_root())
  
 -/*
 - * AMD Energy Preference Performance (EPP)
 - * The EPP is used in the CCLK DPM controller to drive
 - * the frequency that a core is going to operate during
 - * short periods of activity. EPP values will be utilized for
 - * different OS profiles (balanced, performance, power savings)
 - * display strings corresponding to EPP index in the
 - * energy_perf_strings[]
 - *	index		String
 - *-------------------------------------
 - *	0		default
 - *	1		performance
 - *	2		balance_performance
 - *	3		balance_power
 - *	4		power
 +/**
 + * struct  amd_aperf_mperf
 + * @aperf: actual performance frequency clock count
 + * @mperf: maximum performance frequency clock count
 + * @tsc:   time stamp counter
   */
 -enum energy_perf_value_index {
 -	EPP_INDEX_DEFAULT = 0,
 -	EPP_INDEX_PERFORMANCE,
 -	EPP_INDEX_BALANCE_PERFORMANCE,
 -	EPP_INDEX_BALANCE_POWERSAVE,
 -	EPP_INDEX_POWERSAVE,
 -};
 -
 -static const char * const energy_perf_strings[] = {
 -	[EPP_INDEX_DEFAULT] = "default",
 -	[EPP_INDEX_PERFORMANCE] = "performance",
 -	[EPP_INDEX_BALANCE_PERFORMANCE] = "balance_performance",
 -	[EPP_INDEX_BALANCE_POWERSAVE] = "balance_power",
 -	[EPP_INDEX_POWERSAVE] = "power",
 -	NULL
 +struct amd_aperf_mperf {
 +	u64 aperf;
 +	u64 mperf;
 +	u64 tsc;
  };
  
 -static unsigned int epp_values[] = {
 -	[EPP_INDEX_DEFAULT] = 0,
 -	[EPP_INDEX_PERFORMANCE] = AMD_CPPC_EPP_PERFORMANCE,
 -	[EPP_INDEX_BALANCE_PERFORMANCE] = AMD_CPPC_EPP_BALANCE_PERFORMANCE,
 -	[EPP_INDEX_BALANCE_POWERSAVE] = AMD_CPPC_EPP_BALANCE_POWERSAVE,
 -	[EPP_INDEX_POWERSAVE] = AMD_CPPC_EPP_POWERSAVE,
 - };
 -
 -static inline int get_mode_idx_from_str(const char *str, size_t size)
 -{
 -	int i;
 -
 -	for (i=0; i < AMD_PSTATE_MAX; i++) {
 -		if (!strncmp(str, amd_pstate_mode_string[i], size))
 -			return i;
 -	}
 -	return -EINVAL;
 -}
 -
 -static DEFINE_MUTEX(amd_pstate_limits_lock);
 -static DEFINE_MUTEX(amd_pstate_driver_lock);
 -
 -static s16 amd_pstate_get_epp(struct amd_cpudata *cpudata, u64 cppc_req_cached)
 -{
 -	u64 epp;
 -	int ret;
 -
 -	if (boot_cpu_has(X86_FEATURE_CPPC)) {
 -		if (!cppc_req_cached) {
 -			epp = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ,
 -					&cppc_req_cached);
 -			if (epp)
 -				return epp;
 -		}
 -		epp = (cppc_req_cached >> 24) & 0xFF;
 -	} else {
 -		ret = cppc_get_epp_perf(cpudata->cpu, &epp);
 -		if (ret < 0) {
 -			pr_debug("Could not retrieve energy perf value (%d)\n", ret);
 -			return -EIO;
 -		}
 -	}
 -
 -	return (s16)(epp & 0xff);
 -}
 -
 -static int amd_pstate_get_energy_pref_index(struct amd_cpudata *cpudata)
 -{
 -	s16 epp;
 -	int index = -EINVAL;
 -
 -	epp = amd_pstate_get_epp(cpudata, 0);
 -	if (epp < 0)
 -		return epp;
 -
 -	switch (epp) {
 -	case AMD_CPPC_EPP_PERFORMANCE:
 -		index = EPP_INDEX_PERFORMANCE;
 -		break;
 -	case AMD_CPPC_EPP_BALANCE_PERFORMANCE:
 -		index = EPP_INDEX_BALANCE_PERFORMANCE;
 -		break;
 -	case AMD_CPPC_EPP_BALANCE_POWERSAVE:
 -		index = EPP_INDEX_BALANCE_POWERSAVE;
 -		break;
 -	case AMD_CPPC_EPP_POWERSAVE:
 -		index = EPP_INDEX_POWERSAVE;
 -		break;
 -	default:
 -		break;
 -	}
 -
 -	return index;
 -}
 -
 -static int amd_pstate_set_epp(struct amd_cpudata *cpudata, u32 epp)
 -{
 -	int ret;
 -	struct cppc_perf_ctrls perf_ctrls;
 -
 -	if (boot_cpu_has(X86_FEATURE_CPPC)) {
 -		u64 value = READ_ONCE(cpudata->cppc_req_cached);
 -
 -		value &= ~GENMASK_ULL(31, 24);
 -		value |= (u64)epp << 24;
 -		WRITE_ONCE(cpudata->cppc_req_cached, value);
 -
 -		ret = wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
 -		if (!ret)
 -			cpudata->epp_cached = epp;
 -	} else {
 -		perf_ctrls.energy_perf = epp;
 -		ret = cppc_set_epp_perf(cpudata->cpu, &perf_ctrls, 1);
 -		if (ret) {
 -			pr_debug("failed to set energy perf value (%d)\n", ret);
 -			return ret;
 -		}
 -		cpudata->epp_cached = epp;
 -	}
 -
 -	return ret;
 -}
 -
 -static int amd_pstate_set_energy_pref_index(struct amd_cpudata *cpudata,
 -		int pref_index)
 -{
 -	int epp = -EINVAL;
 -	int ret;
 +/**
 + * struct amd_cpudata - private CPU data for AMD P-State
 + * @cpu: CPU number
 + * @req: constraint request to apply
 + * @cppc_req_cached: cached performance request hints
 + * @highest_perf: the maximum performance an individual processor may reach,
 + *		  assuming ideal conditions
 + * @nominal_perf: the maximum sustained performance level of the processor,
 + *		  assuming ideal operating conditions
 + * @lowest_nonlinear_perf: the lowest performance level at which nonlinear power
 + *			   savings are achieved
 + * @lowest_perf: the absolute lowest performance level of the processor
 + * @max_freq: the frequency that mapped to highest_perf
 + * @min_freq: the frequency that mapped to lowest_perf
 + * @nominal_freq: the frequency that mapped to nominal_perf
 + * @lowest_nonlinear_freq: the frequency that mapped to lowest_nonlinear_perf
 + * @cur: Difference of Aperf/Mperf/tsc count between last and current sample
 + * @prev: Last Aperf/Mperf/tsc count value read from register
 + * @freq: current cpu frequency value
 + * @boost_supported: check whether the Processor or SBIOS supports boost mode
 + *
 + * The amd_cpudata is key private data for each CPU thread in AMD P-State, and
 + * represents all the attributes and goals that AMD P-State requests at runtime.
 + */
 +struct amd_cpudata {
 +	int	cpu;
  
 -	if (!pref_index) {
 -		pr_debug("EPP pref_index is invalid\n");
 -		return -EINVAL;
 -	}
 +	struct	freq_qos_request req[2];
 +	u64	cppc_req_cached;
  
 -	if (epp == -EINVAL)
 -		epp = epp_values[pref_index];
 +	u32	highest_perf;
 +	u32	nominal_perf;
 +	u32	lowest_nonlinear_perf;
 +	u32	lowest_perf;
  
 -	if (epp > 0 && cpudata->policy == CPUFREQ_POLICY_PERFORMANCE) {
 -		pr_debug("EPP cannot be set under performance policy\n");
 -		return -EBUSY;
 -	}
 +	u32	max_freq;
 +	u32	min_freq;
 +	u32	nominal_freq;
 +	u32	lowest_nonlinear_freq;
  
 -	ret = amd_pstate_set_epp(cpudata, epp);
 +	struct amd_aperf_mperf cur;
 +	struct amd_aperf_mperf prev;
  
 -	return ret;
 -}
 +	u64 	freq;
 +	bool	boost_supported;
 +};
  
  static inline int pstate_enable(bool enable)
  {
@@@ -662,6 -916,314 +667,317 @@@ static struct freq_attr *amd_pstate_att
  	NULL,
  };
  
++<<<<<<< HEAD
++=======
+ static struct freq_attr *amd_pstate_epp_attr[] = {
+ 	&amd_pstate_max_freq,
+ 	&amd_pstate_lowest_nonlinear_freq,
+ 	&amd_pstate_highest_perf,
+ 	&energy_performance_preference,
+ 	&energy_performance_available_preferences,
+ 	NULL,
+ };
+ 
+ static struct attribute *pstate_global_attributes[] = {
+ 	&status.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group amd_pstate_global_attr_group = {
+ 	.name = "amd_pstate",
+ 	.attrs = pstate_global_attributes,
+ };
+ 
+ static int amd_pstate_epp_cpu_init(struct cpufreq_policy *policy)
+ {
+ 	int min_freq, max_freq, nominal_freq, lowest_nonlinear_freq, ret;
+ 	struct amd_cpudata *cpudata;
+ 	struct device *dev;
+ 	u64 value;
+ 
+ 	/*
+ 	 * Resetting PERF_CTL_MSR will put the CPU in P0 frequency,
+ 	 * which is ideal for initialization process.
+ 	 */
+ 	amd_perf_ctl_reset(policy->cpu);
+ 	dev = get_cpu_device(policy->cpu);
+ 	if (!dev)
+ 		return -ENODEV;
+ 
+ 	cpudata = kzalloc(sizeof(*cpudata), GFP_KERNEL);
+ 	if (!cpudata)
+ 		return -ENOMEM;
+ 
+ 	cpudata->cpu = policy->cpu;
+ 	cpudata->epp_policy = 0;
+ 
+ 	ret = amd_pstate_init_perf(cpudata);
+ 	if (ret)
+ 		goto free_cpudata1;
+ 
+ 	min_freq = amd_get_min_freq(cpudata);
+ 	max_freq = amd_get_max_freq(cpudata);
+ 	nominal_freq = amd_get_nominal_freq(cpudata);
+ 	lowest_nonlinear_freq = amd_get_lowest_nonlinear_freq(cpudata);
+ 	if (min_freq < 0 || max_freq < 0 || min_freq > max_freq) {
+ 		dev_err(dev, "min_freq(%d) or max_freq(%d) value is incorrect\n",
+ 				min_freq, max_freq);
+ 		ret = -EINVAL;
+ 		goto free_cpudata1;
+ 	}
+ 
+ 	policy->cpuinfo.min_freq = min_freq;
+ 	policy->cpuinfo.max_freq = max_freq;
+ 	/* It will be updated by governor */
+ 	policy->cur = policy->cpuinfo.min_freq;
+ 
+ 	/* Initial processor data capability frequencies */
+ 	cpudata->max_freq = max_freq;
+ 	cpudata->min_freq = min_freq;
+ 	cpudata->nominal_freq = nominal_freq;
+ 	cpudata->lowest_nonlinear_freq = lowest_nonlinear_freq;
+ 
+ 	policy->driver_data = cpudata;
+ 
+ 	cpudata->epp_cached = amd_pstate_get_epp(cpudata, 0);
+ 
+ 	policy->min = policy->cpuinfo.min_freq;
+ 	policy->max = policy->cpuinfo.max_freq;
+ 
+ 	/*
+ 	 * Set the policy to powersave to provide a valid fallback value in case
+ 	 * the default cpufreq governor is neither powersave nor performance.
+ 	 */
+ 	policy->policy = CPUFREQ_POLICY_POWERSAVE;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		policy->fast_switch_possible = true;
+ 		ret = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, &value);
+ 		if (ret)
+ 			return ret;
+ 		WRITE_ONCE(cpudata->cppc_req_cached, value);
+ 
+ 		ret = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_CAP1, &value);
+ 		if (ret)
+ 			return ret;
+ 		WRITE_ONCE(cpudata->cppc_cap1_cached, value);
+ 	}
+ 	amd_pstate_boost_init(cpudata);
+ 
+ 	return 0;
+ 
+ free_cpudata1:
+ 	kfree(cpudata);
+ 	return ret;
+ }
+ 
+ static int amd_pstate_epp_cpu_exit(struct cpufreq_policy *policy)
+ {
+ 	pr_debug("CPU %d exiting\n", policy->cpu);
+ 	policy->fast_switch_possible = false;
+ 	return 0;
+ }
+ 
+ static void amd_pstate_epp_init(unsigned int cpu)
+ {
+ 	struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	u32 max_perf, min_perf;
+ 	u64 value;
+ 	s16 epp;
+ 
+ 	max_perf = READ_ONCE(cpudata->highest_perf);
+ 	min_perf = READ_ONCE(cpudata->lowest_perf);
+ 
+ 	value = READ_ONCE(cpudata->cppc_req_cached);
+ 
+ 	if (cpudata->policy == CPUFREQ_POLICY_PERFORMANCE)
+ 		min_perf = max_perf;
+ 
+ 	/* Initial min/max values for CPPC Performance Controls Register */
+ 	value &= ~AMD_CPPC_MIN_PERF(~0L);
+ 	value |= AMD_CPPC_MIN_PERF(min_perf);
+ 
+ 	value &= ~AMD_CPPC_MAX_PERF(~0L);
+ 	value |= AMD_CPPC_MAX_PERF(max_perf);
+ 
+ 	/* CPPC EPP feature require to set zero to the desire perf bit */
+ 	value &= ~AMD_CPPC_DES_PERF(~0L);
+ 	value |= AMD_CPPC_DES_PERF(0);
+ 
+ 	if (cpudata->epp_policy == cpudata->policy)
+ 		goto skip_epp;
+ 
+ 	cpudata->epp_policy = cpudata->policy;
+ 
+ 	/* Get BIOS pre-defined epp value */
+ 	epp = amd_pstate_get_epp(cpudata, value);
+ 	if (epp < 0) {
+ 		/**
+ 		 * This return value can only be negative for shared_memory
+ 		 * systems where EPP register read/write not supported.
+ 		 */
+ 		goto skip_epp;
+ 	}
+ 
+ 	if (cpudata->policy == CPUFREQ_POLICY_PERFORMANCE)
+ 		epp = 0;
+ 
+ 	/* Set initial EPP value */
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		value &= ~GENMASK_ULL(31, 24);
+ 		value |= (u64)epp << 24;
+ 	}
+ 
+ 	WRITE_ONCE(cpudata->cppc_req_cached, value);
+ 	amd_pstate_set_epp(cpudata, epp);
+ skip_epp:
+ 	cpufreq_cpu_put(policy);
+ }
+ 
+ static int amd_pstate_epp_set_policy(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	if (!policy->cpuinfo.max_freq)
+ 		return -ENODEV;
+ 
+ 	pr_debug("set_policy: cpuinfo.max %u policy->max %u\n",
+ 				policy->cpuinfo.max_freq, policy->max);
+ 
+ 	cpudata->policy = policy->policy;
+ 
+ 	amd_pstate_epp_init(policy->cpu);
+ 
+ 	return 0;
+ }
+ 
+ static void amd_pstate_epp_reenable(struct amd_cpudata *cpudata)
+ {
+ 	struct cppc_perf_ctrls perf_ctrls;
+ 	u64 value, max_perf;
+ 	int ret;
+ 
+ 	ret = amd_pstate_enable(true);
+ 	if (ret)
+ 		pr_err("failed to enable amd pstate during resume, return %d\n", ret);
+ 
+ 	value = READ_ONCE(cpudata->cppc_req_cached);
+ 	max_perf = READ_ONCE(cpudata->highest_perf);
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
+ 	} else {
+ 		perf_ctrls.max_perf = max_perf;
+ 		perf_ctrls.energy_perf = AMD_CPPC_ENERGY_PERF_PREF(cpudata->epp_cached);
+ 		cppc_set_perf(cpudata->cpu, &perf_ctrls);
+ 	}
+ }
+ 
+ static int amd_pstate_epp_cpu_online(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	pr_debug("AMD CPU Core %d going online\n", cpudata->cpu);
+ 
+ 	if (cppc_state == AMD_PSTATE_ACTIVE) {
+ 		amd_pstate_epp_reenable(cpudata);
+ 		cpudata->suspended = false;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void amd_pstate_epp_offline(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	struct cppc_perf_ctrls perf_ctrls;
+ 	int min_perf;
+ 	u64 value;
+ 
+ 	min_perf = READ_ONCE(cpudata->lowest_perf);
+ 	value = READ_ONCE(cpudata->cppc_req_cached);
+ 
+ 	mutex_lock(&amd_pstate_limits_lock);
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		cpudata->epp_policy = CPUFREQ_POLICY_UNKNOWN;
+ 
+ 		/* Set max perf same as min perf */
+ 		value &= ~AMD_CPPC_MAX_PERF(~0L);
+ 		value |= AMD_CPPC_MAX_PERF(min_perf);
+ 		value &= ~AMD_CPPC_MIN_PERF(~0L);
+ 		value |= AMD_CPPC_MIN_PERF(min_perf);
+ 		wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
+ 	} else {
+ 		perf_ctrls.desired_perf = 0;
+ 		perf_ctrls.max_perf = min_perf;
+ 		perf_ctrls.energy_perf = AMD_CPPC_ENERGY_PERF_PREF(HWP_EPP_BALANCE_POWERSAVE);
+ 		cppc_set_perf(cpudata->cpu, &perf_ctrls);
+ 	}
+ 	mutex_unlock(&amd_pstate_limits_lock);
+ }
+ 
+ static int amd_pstate_epp_cpu_offline(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	pr_debug("AMD CPU Core %d going offline\n", cpudata->cpu);
+ 
+ 	if (cpudata->suspended)
+ 		return 0;
+ 
+ 	if (cppc_state == AMD_PSTATE_ACTIVE)
+ 		amd_pstate_epp_offline(policy);
+ 
+ 	return 0;
+ }
+ 
+ static int amd_pstate_epp_verify_policy(struct cpufreq_policy_data *policy)
+ {
+ 	cpufreq_verify_within_cpu_limits(policy);
+ 	pr_debug("policy_max =%d, policy_min=%d\n", policy->max, policy->min);
+ 	return 0;
+ }
+ 
+ static int amd_pstate_epp_suspend(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	int ret;
+ 
+ 	/* avoid suspending when EPP is not enabled */
+ 	if (cppc_state != AMD_PSTATE_ACTIVE)
+ 		return 0;
+ 
+ 	/* set this flag to avoid setting core offline*/
+ 	cpudata->suspended = true;
+ 
+ 	/* disable CPPC in lowlevel firmware */
+ 	ret = amd_pstate_enable(false);
+ 	if (ret)
+ 		pr_err("failed to suspend, return %d\n", ret);
+ 
+ 	return 0;
+ }
+ 
+ static int amd_pstate_epp_resume(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	if (cpudata->suspended) {
+ 		mutex_lock(&amd_pstate_limits_lock);
+ 
+ 		/* enable amd pstate from suspend state*/
+ 		amd_pstate_epp_reenable(cpudata);
+ 
+ 		mutex_unlock(&amd_pstate_limits_lock);
+ 
+ 		cpudata->suspended = false;
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 3666062b87ec (cpufreq: amd-pstate: move to use bus_get_dev_root())
  static struct cpufreq_driver amd_pstate_driver = {
  	.flags		= CPUFREQ_CONST_LOOPS | CPUFREQ_NEED_UPDATE_LIMITS,
  	.verify		= amd_pstate_verify,
@@@ -675,8 -1237,23 +991,9 @@@
  	.attr		= amd_pstate_attr,
  };
  
 -static struct cpufreq_driver amd_pstate_epp_driver = {
 -	.flags		= CPUFREQ_CONST_LOOPS,
 -	.verify		= amd_pstate_epp_verify_policy,
 -	.setpolicy	= amd_pstate_epp_set_policy,
 -	.init		= amd_pstate_epp_cpu_init,
 -	.exit		= amd_pstate_epp_cpu_exit,
 -	.offline	= amd_pstate_epp_cpu_offline,
 -	.online		= amd_pstate_epp_cpu_online,
 -	.suspend	= amd_pstate_epp_suspend,
 -	.resume		= amd_pstate_epp_resume,
 -	.name		= "amd_pstate_epp",
 -	.attr		= amd_pstate_epp_attr,
 -};
 -
  static int __init amd_pstate_init(void)
  {
+ 	struct device *dev_root;
  	int ret;
  
  	if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD)
@@@ -718,11 -1296,24 +1035,28 @@@
  		return ret;
  	}
  
 -	ret = cpufreq_register_driver(current_pstate_driver);
 +	ret = cpufreq_register_driver(&amd_pstate_driver);
  	if (ret)
 -		pr_err("failed to register with return %d\n", ret);
 +		pr_err("failed to register amd_pstate_driver with return %d\n",
 +		       ret);
  
++<<<<<<< HEAD
++=======
+ 	dev_root = bus_get_dev_root(&cpu_subsys);
+ 	if (dev_root) {
+ 		ret = sysfs_create_group(&dev_root->kobj, &amd_pstate_global_attr_group);
+ 		put_device(dev_root);
+ 		if (ret) {
+ 			pr_err("sysfs attribute export failed with error %d.\n", ret);
+ 			goto global_attr_free;
+ 		}
+ 	}
+ 
+ 	return ret;
+ 
+ global_attr_free:
+ 	cpufreq_unregister_driver(current_pstate_driver);
++>>>>>>> 3666062b87ec (cpufreq: amd-pstate: move to use bus_get_dev_root())
  	return ret;
  }
  device_initcall(amd_pstate_init);
* Unmerged path drivers/cpufreq/amd-pstate.c
