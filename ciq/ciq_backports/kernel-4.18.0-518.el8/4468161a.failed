irq_work: Trace self-IPIs sent via arch_irq_work_raise()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-518.el8
commit-author Valentin Schneider <vschneid@redhat.com>
commit 4468161a5ca2ea239c92de7c0a0dca61854ec4da
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-518.el8/4468161a.failed

IPIs sent to remote CPUs via irq_work_queue_on() are now covered by
trace_ipi_send_cpumask(), add another instance of the tracepoint to cover
self-IPIs.

	Signed-off-by: Valentin Schneider <vschneid@redhat.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Steven Rostedt (Google) <rostedt@goodmis.org>
Link: https://lore.kernel.org/r/20230307143558.294354-5-vschneid@redhat.com
(cherry picked from commit 4468161a5ca2ea239c92de7c0a0dca61854ec4da)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/irq_work.c
diff --cc kernel/irq_work.c
index 64ae81d50b5b,c33e88e32a67..000000000000
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@@ -20,8 -21,35 +20,10 @@@
  #include <asm/processor.h>
  #include <linux/kasan.h>
  
+ #include <trace/events/ipi.h>
+ 
  static DEFINE_PER_CPU(struct llist_head, raised_list);
  static DEFINE_PER_CPU(struct llist_head, lazy_list);
 -static DEFINE_PER_CPU(struct task_struct *, irq_workd);
 -
 -static void wake_irq_workd(void)
 -{
 -	struct task_struct *tsk = __this_cpu_read(irq_workd);
 -
 -	if (!llist_empty(this_cpu_ptr(&lazy_list)) && tsk)
 -		wake_up_process(tsk);
 -}
 -
 -#ifdef CONFIG_SMP
 -static void irq_work_wake(struct irq_work *entry)
 -{
 -	wake_irq_workd();
 -}
 -
 -static DEFINE_PER_CPU(struct irq_work, irq_work_wakeup) =
 -	IRQ_WORK_INIT_HARD(irq_work_wake);
 -#endif
 -
 -static int irq_workd_should_run(unsigned int cpu)
 -{
 -	return !llist_empty(this_cpu_ptr(&lazy_list));
 -}
  
  /*
   * Claim the entry so that no one else will poke at it.
@@@ -51,15 -89,29 +63,20 @@@ static __always_inline void irq_work_ra
  /* Enqueue on current CPU, work must already be claimed and preempt disabled */
  static void __irq_work_queue_local(struct irq_work *work)
  {
 -	struct llist_head *list;
 -	bool rt_lazy_work = false;
 -	bool lazy_work = false;
 -	int work_flags;
 -
 -	work_flags = atomic_read(&work->node.a_flags);
 -	if (work_flags & IRQ_WORK_LAZY)
 -		lazy_work = true;
 -	else if (IS_ENABLED(CONFIG_PREEMPT_RT) &&
 -		 !(work_flags & IRQ_WORK_HARD_IRQ))
 -		rt_lazy_work = true;
 -
 -	if (lazy_work || rt_lazy_work)
 -		list = this_cpu_ptr(&lazy_list);
 -	else
 -		list = this_cpu_ptr(&raised_list);
 -
 -	if (!llist_add(&work->node.llist, list))
 -		return;
 -
  	/* If the work is "lazy", handle it from next tick if any */
++<<<<<<< HEAD
 +	if (atomic_read(&work->node.a_flags) & IRQ_WORK_LAZY) {
 +		if (llist_add(&work->node.llist, this_cpu_ptr(&lazy_list)) &&
 +		    tick_nohz_tick_stopped())
 +			arch_irq_work_raise();
 +	} else {
 +		if (llist_add(&work->node.llist, this_cpu_ptr(&raised_list)))
 +			arch_irq_work_raise();
 +	}
++=======
+ 	if (!lazy_work || tick_nohz_tick_stopped())
+ 		irq_work_raise(work);
++>>>>>>> 4468161a5ca2 (irq_work: Trace self-IPIs sent via arch_irq_work_raise())
  }
  
  /* Enqueue the irq work @work on the current CPU */
* Unmerged path kernel/irq_work.c
