cpufreq: amd-pstate: add driver working mode switch support

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-518.el8
commit-author Perry Yuan <Perry.Yuan@amd.com>
commit abd61c08ef349af08df0bf587d33f5bde5996a89
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-518.el8/abd61c08.failed

While amd-pstate driver was loaded with specific driver mode, it will
need to check which mode is enabled for the pstate driver,add this sysfs
entry to show the current status

$ cat /sys/devices/system/cpu/amd-pstate/status
active

Meanwhile, user can switch the pstate driver mode with writing mode
string to sysfs entry as below.

Enable passive mode:
$ sudo bash -c "echo passive >  /sys/devices/system/cpu/amd-pstate/status"

Enable active mode (EPP driver mode):
$ sudo bash -c "echo active > /sys/devices/system/cpu/amd-pstate/status"

	Acked-by: Huang Rui <ray.huang@amd.com>
	Reviewed-by: Mario Limonciello <mario.limonciello@amd.com>
	Reviewed-by: Wyes Karny <wyes.karny@amd.com>
	Tested-by: Wyes Karny <wyes.karny@amd.com>
	Signed-off-by: Perry Yuan <Perry.Yuan@amd.com>
	Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
(cherry picked from commit abd61c08ef349af08df0bf587d33f5bde5996a89)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/cpufreq/amd-pstate.c
diff --cc drivers/cpufreq/amd-pstate.c
index f0b221585cc8,1ae2e0d56ed1..000000000000
--- a/drivers/cpufreq/amd-pstate.c
+++ b/drivers/cpufreq/amd-pstate.c
@@@ -58,67 -59,171 +58,73 @@@
   * we disable it by default to go acpi-cpufreq on these processors and add a
   * module parameter to be able to enable it manually for debugging.
   */
 -static struct cpufreq_driver *current_pstate_driver;
  static struct cpufreq_driver amd_pstate_driver;
++<<<<<<< HEAD
 +static int cppc_load __initdata;
++=======
+ static struct cpufreq_driver amd_pstate_epp_driver;
+ static int cppc_state = AMD_PSTATE_DISABLE;
+ struct kobject *amd_pstate_kobj;
++>>>>>>> abd61c08ef34 (cpufreq: amd-pstate: add driver working mode switch support)
  
 -/*
 - * AMD Energy Preference Performance (EPP)
 - * The EPP is used in the CCLK DPM controller to drive
 - * the frequency that a core is going to operate during
 - * short periods of activity. EPP values will be utilized for
 - * different OS profiles (balanced, performance, power savings)
 - * display strings corresponding to EPP index in the
 - * energy_perf_strings[]
 - *	index		String
 - *-------------------------------------
 - *	0		default
 - *	1		performance
 - *	2		balance_performance
 - *	3		balance_power
 - *	4		power
 +/**
 + * struct  amd_aperf_mperf
 + * @aperf: actual performance frequency clock count
 + * @mperf: maximum performance frequency clock count
 + * @tsc:   time stamp counter
   */
 -enum energy_perf_value_index {
 -	EPP_INDEX_DEFAULT = 0,
 -	EPP_INDEX_PERFORMANCE,
 -	EPP_INDEX_BALANCE_PERFORMANCE,
 -	EPP_INDEX_BALANCE_POWERSAVE,
 -	EPP_INDEX_POWERSAVE,
 -};
 -
 -static const char * const energy_perf_strings[] = {
 -	[EPP_INDEX_DEFAULT] = "default",
 -	[EPP_INDEX_PERFORMANCE] = "performance",
 -	[EPP_INDEX_BALANCE_PERFORMANCE] = "balance_performance",
 -	[EPP_INDEX_BALANCE_POWERSAVE] = "balance_power",
 -	[EPP_INDEX_POWERSAVE] = "power",
 -	NULL
 +struct amd_aperf_mperf {
 +	u64 aperf;
 +	u64 mperf;
 +	u64 tsc;
  };
  
 -static unsigned int epp_values[] = {
 -	[EPP_INDEX_DEFAULT] = 0,
 -	[EPP_INDEX_PERFORMANCE] = AMD_CPPC_EPP_PERFORMANCE,
 -	[EPP_INDEX_BALANCE_PERFORMANCE] = AMD_CPPC_EPP_BALANCE_PERFORMANCE,
 -	[EPP_INDEX_BALANCE_POWERSAVE] = AMD_CPPC_EPP_BALANCE_POWERSAVE,
 -	[EPP_INDEX_POWERSAVE] = AMD_CPPC_EPP_POWERSAVE,
 - };
 -
 -static inline int get_mode_idx_from_str(const char *str, size_t size)
 -{
 -	int i;
 -
 -	for (i=0; i < AMD_PSTATE_MAX; i++) {
 -		if (!strncmp(str, amd_pstate_mode_string[i], size))
 -			return i;
 -	}
 -	return -EINVAL;
 -}
 -
 -static DEFINE_MUTEX(amd_pstate_limits_lock);
 -static DEFINE_MUTEX(amd_pstate_driver_lock);
 -
 -static s16 amd_pstate_get_epp(struct amd_cpudata *cpudata, u64 cppc_req_cached)
 -{
 -	u64 epp;
 -	int ret;
 -
 -	if (boot_cpu_has(X86_FEATURE_CPPC)) {
 -		if (!cppc_req_cached) {
 -			epp = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ,
 -					&cppc_req_cached);
 -			if (epp)
 -				return epp;
 -		}
 -		epp = (cppc_req_cached >> 24) & 0xFF;
 -	} else {
 -		ret = cppc_get_epp_perf(cpudata->cpu, &epp);
 -		if (ret < 0) {
 -			pr_debug("Could not retrieve energy perf value (%d)\n", ret);
 -			return -EIO;
 -		}
 -	}
 -
 -	return (s16)(epp & 0xff);
 -}
 -
 -static int amd_pstate_get_energy_pref_index(struct amd_cpudata *cpudata)
 -{
 -	s16 epp;
 -	int index = -EINVAL;
 -
 -	epp = amd_pstate_get_epp(cpudata, 0);
 -	if (epp < 0)
 -		return epp;
 -
 -	switch (epp) {
 -	case AMD_CPPC_EPP_PERFORMANCE:
 -		index = EPP_INDEX_PERFORMANCE;
 -		break;
 -	case AMD_CPPC_EPP_BALANCE_PERFORMANCE:
 -		index = EPP_INDEX_BALANCE_PERFORMANCE;
 -		break;
 -	case AMD_CPPC_EPP_BALANCE_POWERSAVE:
 -		index = EPP_INDEX_BALANCE_POWERSAVE;
 -		break;
 -	case AMD_CPPC_EPP_POWERSAVE:
 -		index = EPP_INDEX_POWERSAVE;
 -		break;
 -	default:
 -		break;
 -	}
 -
 -	return index;
 -}
 -
 -static int amd_pstate_set_epp(struct amd_cpudata *cpudata, u32 epp)
 -{
 -	int ret;
 -	struct cppc_perf_ctrls perf_ctrls;
 -
 -	if (boot_cpu_has(X86_FEATURE_CPPC)) {
 -		u64 value = READ_ONCE(cpudata->cppc_req_cached);
 -
 -		value &= ~GENMASK_ULL(31, 24);
 -		value |= (u64)epp << 24;
 -		WRITE_ONCE(cpudata->cppc_req_cached, value);
 -
 -		ret = wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
 -		if (!ret)
 -			cpudata->epp_cached = epp;
 -	} else {
 -		perf_ctrls.energy_perf = epp;
 -		ret = cppc_set_epp_perf(cpudata->cpu, &perf_ctrls, 1);
 -		if (ret) {
 -			pr_debug("failed to set energy perf value (%d)\n", ret);
 -			return ret;
 -		}
 -		cpudata->epp_cached = epp;
 -	}
 -
 -	return ret;
 -}
 -
 -static int amd_pstate_set_energy_pref_index(struct amd_cpudata *cpudata,
 -		int pref_index)
 -{
 -	int epp = -EINVAL;
 -	int ret;
 +/**
 + * struct amd_cpudata - private CPU data for AMD P-State
 + * @cpu: CPU number
 + * @req: constraint request to apply
 + * @cppc_req_cached: cached performance request hints
 + * @highest_perf: the maximum performance an individual processor may reach,
 + *		  assuming ideal conditions
 + * @nominal_perf: the maximum sustained performance level of the processor,
 + *		  assuming ideal operating conditions
 + * @lowest_nonlinear_perf: the lowest performance level at which nonlinear power
 + *			   savings are achieved
 + * @lowest_perf: the absolute lowest performance level of the processor
 + * @max_freq: the frequency that mapped to highest_perf
 + * @min_freq: the frequency that mapped to lowest_perf
 + * @nominal_freq: the frequency that mapped to nominal_perf
 + * @lowest_nonlinear_freq: the frequency that mapped to lowest_nonlinear_perf
 + * @cur: Difference of Aperf/Mperf/tsc count between last and current sample
 + * @prev: Last Aperf/Mperf/tsc count value read from register
 + * @freq: current cpu frequency value
 + * @boost_supported: check whether the Processor or SBIOS supports boost mode
 + *
 + * The amd_cpudata is key private data for each CPU thread in AMD P-State, and
 + * represents all the attributes and goals that AMD P-State requests at runtime.
 + */
 +struct amd_cpudata {
 +	int	cpu;
  
 -	if (!pref_index) {
 -		pr_debug("EPP pref_index is invalid\n");
 -		return -EINVAL;
 -	}
 +	struct	freq_qos_request req[2];
 +	u64	cppc_req_cached;
  
 -	if (epp == -EINVAL)
 -		epp = epp_values[pref_index];
 +	u32	highest_perf;
 +	u32	nominal_perf;
 +	u32	lowest_nonlinear_perf;
 +	u32	lowest_perf;
  
 -	if (epp > 0 && cpudata->policy == CPUFREQ_POLICY_PERFORMANCE) {
 -		pr_debug("EPP cannot be set under performance policy\n");
 -		return -EBUSY;
 -	}
 +	u32	max_freq;
 +	u32	min_freq;
 +	u32	nominal_freq;
 +	u32	lowest_nonlinear_freq;
  
 -	ret = amd_pstate_set_epp(cpudata, epp);
 +	struct amd_aperf_mperf cur;
 +	struct amd_aperf_mperf prev;
  
 -	return ret;
 -}
 +	u64 	freq;
 +	bool	boost_supported;
 +};
  
  static inline int pstate_enable(bool enable)
  {
@@@ -647,13 -764,151 +655,105 @@@ static ssize_t show_amd_pstate_highest_
  
  	perf = READ_ONCE(cpudata->highest_perf);
  
 -	return sprintf(&buf[0], "%u\n", perf);
 -}
 -
 -static ssize_t show_energy_performance_available_preferences(
 -				struct cpufreq_policy *policy, char *buf)
 -{
 -	int i = 0;
 -	int offset = 0;
 -
 -	while (energy_perf_strings[i] != NULL)
 -		offset += sysfs_emit_at(buf, offset, "%s ", energy_perf_strings[i++]);
 -
 -	sysfs_emit_at(buf, offset, "\n");
 -
 -	return offset;
 -}
 -
 -static ssize_t store_energy_performance_preference(
 -		struct cpufreq_policy *policy, const char *buf, size_t count)
 -{
 -	struct amd_cpudata *cpudata = policy->driver_data;
 -	char str_preference[21];
 -	ssize_t ret;
 -
 -	ret = sscanf(buf, "%20s", str_preference);
 -	if (ret != 1)
 -		return -EINVAL;
 -
 -	ret = match_string(energy_perf_strings, -1, str_preference);
 -	if (ret < 0)
 -		return -EINVAL;
 -
 -	mutex_lock(&amd_pstate_limits_lock);
 -	ret = amd_pstate_set_energy_pref_index(cpudata, ret);
 -	mutex_unlock(&amd_pstate_limits_lock);
 -
 -	return ret ?: count;
 -}
 -
 -static ssize_t show_energy_performance_preference(
 -				struct cpufreq_policy *policy, char *buf)
 -{
 -	struct amd_cpudata *cpudata = policy->driver_data;
 -	int preference;
 -
 -	preference = amd_pstate_get_energy_pref_index(cpudata);
 -	if (preference < 0)
 -		return preference;
 -
 -	return sysfs_emit(buf, "%s\n", energy_perf_strings[preference]);
 +	return sysfs_emit(buf, "%u\n", perf);
  }
  
+ static ssize_t amd_pstate_show_status(char *buf)
+ {
+ 	if (!current_pstate_driver)
+ 		return sysfs_emit(buf, "disable\n");
+ 
+ 	return sysfs_emit(buf, "%s\n", amd_pstate_mode_string[cppc_state]);
+ }
+ 
+ static void amd_pstate_driver_cleanup(void)
+ {
+ 	current_pstate_driver = NULL;
+ }
+ 
+ static int amd_pstate_update_status(const char *buf, size_t size)
+ {
+ 	int ret;
+ 	int mode_idx;
+ 
+ 	if (size > 7 || size < 6)
+ 		return -EINVAL;
+ 	mode_idx = get_mode_idx_from_str(buf, size);
+ 
+ 	switch(mode_idx) {
+ 	case AMD_PSTATE_DISABLE:
+ 		if (!current_pstate_driver)
+ 			return -EINVAL;
+ 		if (cppc_state == AMD_PSTATE_ACTIVE)
+ 			return -EBUSY;
+ 		ret = cpufreq_unregister_driver(current_pstate_driver);
+ 		amd_pstate_driver_cleanup();
+ 		break;
+ 	case AMD_PSTATE_PASSIVE:
+ 		if (current_pstate_driver) {
+ 			if (current_pstate_driver == &amd_pstate_driver)
+ 				return 0;
+ 			cpufreq_unregister_driver(current_pstate_driver);
+ 			cppc_state = AMD_PSTATE_PASSIVE;
+ 			current_pstate_driver = &amd_pstate_driver;
+ 		}
+ 
+ 		ret = cpufreq_register_driver(current_pstate_driver);
+ 		break;
+ 	case AMD_PSTATE_ACTIVE:
+ 		if (current_pstate_driver) {
+ 			if (current_pstate_driver == &amd_pstate_epp_driver)
+ 				return 0;
+ 			cpufreq_unregister_driver(current_pstate_driver);
+ 			current_pstate_driver = &amd_pstate_epp_driver;
+ 			cppc_state = AMD_PSTATE_ACTIVE;
+ 		}
+ 
+ 		ret = cpufreq_register_driver(current_pstate_driver);
+ 		break;
+ 	default:
+ 		ret = -EINVAL;
+ 		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t show_status(struct kobject *kobj,
+ 			   struct kobj_attribute *attr, char *buf)
+ {
+ 	ssize_t ret;
+ 
+ 	mutex_lock(&amd_pstate_driver_lock);
+ 	ret = amd_pstate_show_status(buf);
+ 	mutex_unlock(&amd_pstate_driver_lock);
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t store_status(struct kobject *a, struct kobj_attribute *b,
+ 			    const char *buf, size_t count)
+ {
+ 	char *p = memchr(buf, '\n', count);
+ 	int ret;
+ 
+ 	mutex_lock(&amd_pstate_driver_lock);
+ 	ret = amd_pstate_update_status(buf, p ? p - buf : count);
+ 	mutex_unlock(&amd_pstate_driver_lock);
+ 
+ 	return ret < 0 ? ret : count;
+ }
+ 
  cpufreq_freq_attr_ro(amd_pstate_max_freq);
  cpufreq_freq_attr_ro(amd_pstate_lowest_nonlinear_freq);
  
  cpufreq_freq_attr_ro(amd_pstate_highest_perf);
++<<<<<<< HEAD
++=======
+ cpufreq_freq_attr_rw(energy_performance_preference);
+ cpufreq_freq_attr_ro(energy_performance_available_preferences);
+ define_one_global_rw(status);
++>>>>>>> abd61c08ef34 (cpufreq: amd-pstate: add driver working mode switch support)
  
  static struct freq_attr *amd_pstate_attr[] = {
  	&amd_pstate_max_freq,
@@@ -662,6 -917,313 +762,316 @@@
  	NULL,
  };
  
++<<<<<<< HEAD
++=======
+ static struct freq_attr *amd_pstate_epp_attr[] = {
+ 	&amd_pstate_max_freq,
+ 	&amd_pstate_lowest_nonlinear_freq,
+ 	&amd_pstate_highest_perf,
+ 	&energy_performance_preference,
+ 	&energy_performance_available_preferences,
+ 	NULL,
+ };
+ 
+ static struct attribute *pstate_global_attributes[] = {
+ 	&status.attr,
+ 	NULL
+ };
+ 
+ static const struct attribute_group amd_pstate_global_attr_group = {
+ 	.attrs = pstate_global_attributes,
+ };
+ 
+ static int amd_pstate_epp_cpu_init(struct cpufreq_policy *policy)
+ {
+ 	int min_freq, max_freq, nominal_freq, lowest_nonlinear_freq, ret;
+ 	struct amd_cpudata *cpudata;
+ 	struct device *dev;
+ 	int rc;
+ 	u64 value;
+ 
+ 	/*
+ 	 * Resetting PERF_CTL_MSR will put the CPU in P0 frequency,
+ 	 * which is ideal for initialization process.
+ 	 */
+ 	amd_perf_ctl_reset(policy->cpu);
+ 	dev = get_cpu_device(policy->cpu);
+ 	if (!dev)
+ 		goto free_cpudata1;
+ 
+ 	cpudata = kzalloc(sizeof(*cpudata), GFP_KERNEL);
+ 	if (!cpudata)
+ 		return -ENOMEM;
+ 
+ 	cpudata->cpu = policy->cpu;
+ 	cpudata->epp_policy = 0;
+ 
+ 	rc = amd_pstate_init_perf(cpudata);
+ 	if (rc)
+ 		goto free_cpudata1;
+ 
+ 	min_freq = amd_get_min_freq(cpudata);
+ 	max_freq = amd_get_max_freq(cpudata);
+ 	nominal_freq = amd_get_nominal_freq(cpudata);
+ 	lowest_nonlinear_freq = amd_get_lowest_nonlinear_freq(cpudata);
+ 	if (min_freq < 0 || max_freq < 0 || min_freq > max_freq) {
+ 		dev_err(dev, "min_freq(%d) or max_freq(%d) value is incorrect\n",
+ 				min_freq, max_freq);
+ 		ret = -EINVAL;
+ 		goto free_cpudata1;
+ 	}
+ 
+ 	policy->cpuinfo.min_freq = min_freq;
+ 	policy->cpuinfo.max_freq = max_freq;
+ 	/* It will be updated by governor */
+ 	policy->cur = policy->cpuinfo.min_freq;
+ 
+ 	/* Initial processor data capability frequencies */
+ 	cpudata->max_freq = max_freq;
+ 	cpudata->min_freq = min_freq;
+ 	cpudata->nominal_freq = nominal_freq;
+ 	cpudata->lowest_nonlinear_freq = lowest_nonlinear_freq;
+ 
+ 	policy->driver_data = cpudata;
+ 
+ 	cpudata->epp_cached = amd_pstate_get_epp(cpudata, 0);
+ 
+ 	policy->min = policy->cpuinfo.min_freq;
+ 	policy->max = policy->cpuinfo.max_freq;
+ 
+ 	/*
+ 	 * Set the policy to powersave to provide a valid fallback value in case
+ 	 * the default cpufreq governor is neither powersave nor performance.
+ 	 */
+ 	policy->policy = CPUFREQ_POLICY_POWERSAVE;
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		policy->fast_switch_possible = true;
+ 		ret = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, &value);
+ 		if (ret)
+ 			return ret;
+ 		WRITE_ONCE(cpudata->cppc_req_cached, value);
+ 
+ 		ret = rdmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_CAP1, &value);
+ 		if (ret)
+ 			return ret;
+ 		WRITE_ONCE(cpudata->cppc_cap1_cached, value);
+ 	}
+ 	amd_pstate_boost_init(cpudata);
+ 
+ 	return 0;
+ 
+ free_cpudata1:
+ 	kfree(cpudata);
+ 	return ret;
+ }
+ 
+ static int amd_pstate_epp_cpu_exit(struct cpufreq_policy *policy)
+ {
+ 	pr_debug("CPU %d exiting\n", policy->cpu);
+ 	policy->fast_switch_possible = false;
+ 	return 0;
+ }
+ 
+ static void amd_pstate_epp_init(unsigned int cpu)
+ {
+ 	struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	u32 max_perf, min_perf;
+ 	u64 value;
+ 	s16 epp;
+ 
+ 	max_perf = READ_ONCE(cpudata->highest_perf);
+ 	min_perf = READ_ONCE(cpudata->lowest_perf);
+ 
+ 	value = READ_ONCE(cpudata->cppc_req_cached);
+ 
+ 	if (cpudata->policy == CPUFREQ_POLICY_PERFORMANCE)
+ 		min_perf = max_perf;
+ 
+ 	/* Initial min/max values for CPPC Performance Controls Register */
+ 	value &= ~AMD_CPPC_MIN_PERF(~0L);
+ 	value |= AMD_CPPC_MIN_PERF(min_perf);
+ 
+ 	value &= ~AMD_CPPC_MAX_PERF(~0L);
+ 	value |= AMD_CPPC_MAX_PERF(max_perf);
+ 
+ 	/* CPPC EPP feature require to set zero to the desire perf bit */
+ 	value &= ~AMD_CPPC_DES_PERF(~0L);
+ 	value |= AMD_CPPC_DES_PERF(0);
+ 
+ 	if (cpudata->epp_policy == cpudata->policy)
+ 		goto skip_epp;
+ 
+ 	cpudata->epp_policy = cpudata->policy;
+ 
+ 	if (cpudata->policy == CPUFREQ_POLICY_PERFORMANCE) {
+ 		epp = amd_pstate_get_epp(cpudata, value);
+ 		if (epp < 0)
+ 			goto skip_epp;
+ 		/* force the epp value to be zero for performance policy */
+ 		epp = 0;
+ 	} else {
+ 		/* Get BIOS pre-defined epp value */
+ 		epp = amd_pstate_get_epp(cpudata, value);
+ 		if (epp)
+ 			goto skip_epp;
+ 	}
+ 	/* Set initial EPP value */
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		value &= ~GENMASK_ULL(31, 24);
+ 		value |= (u64)epp << 24;
+ 	}
+ 
+ skip_epp:
+ 	WRITE_ONCE(cpudata->cppc_req_cached, value);
+ 	amd_pstate_set_epp(cpudata, epp);
+ 	cpufreq_cpu_put(policy);
+ }
+ 
+ static int amd_pstate_epp_set_policy(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	if (!policy->cpuinfo.max_freq)
+ 		return -ENODEV;
+ 
+ 	pr_debug("set_policy: cpuinfo.max %u policy->max %u\n",
+ 				policy->cpuinfo.max_freq, policy->max);
+ 
+ 	cpudata->policy = policy->policy;
+ 
+ 	amd_pstate_epp_init(policy->cpu);
+ 
+ 	return 0;
+ }
+ 
+ static void amd_pstate_epp_reenable(struct amd_cpudata *cpudata)
+ {
+ 	struct cppc_perf_ctrls perf_ctrls;
+ 	u64 value, max_perf;
+ 	int ret;
+ 
+ 	ret = amd_pstate_enable(true);
+ 	if (ret)
+ 		pr_err("failed to enable amd pstate during resume, return %d\n", ret);
+ 
+ 	value = READ_ONCE(cpudata->cppc_req_cached);
+ 	max_perf = READ_ONCE(cpudata->highest_perf);
+ 
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
+ 	} else {
+ 		perf_ctrls.max_perf = max_perf;
+ 		perf_ctrls.energy_perf = AMD_CPPC_ENERGY_PERF_PREF(cpudata->epp_cached);
+ 		cppc_set_perf(cpudata->cpu, &perf_ctrls);
+ 	}
+ }
+ 
+ static int amd_pstate_epp_cpu_online(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	pr_debug("AMD CPU Core %d going online\n", cpudata->cpu);
+ 
+ 	if (cppc_state == AMD_PSTATE_ACTIVE) {
+ 		amd_pstate_epp_reenable(cpudata);
+ 		cpudata->suspended = false;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void amd_pstate_epp_offline(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	struct cppc_perf_ctrls perf_ctrls;
+ 	int min_perf;
+ 	u64 value;
+ 
+ 	min_perf = READ_ONCE(cpudata->lowest_perf);
+ 	value = READ_ONCE(cpudata->cppc_req_cached);
+ 
+ 	mutex_lock(&amd_pstate_limits_lock);
+ 	if (boot_cpu_has(X86_FEATURE_CPPC)) {
+ 		cpudata->epp_policy = CPUFREQ_POLICY_UNKNOWN;
+ 
+ 		/* Set max perf same as min perf */
+ 		value &= ~AMD_CPPC_MAX_PERF(~0L);
+ 		value |= AMD_CPPC_MAX_PERF(min_perf);
+ 		value &= ~AMD_CPPC_MIN_PERF(~0L);
+ 		value |= AMD_CPPC_MIN_PERF(min_perf);
+ 		wrmsrl_on_cpu(cpudata->cpu, MSR_AMD_CPPC_REQ, value);
+ 	} else {
+ 		perf_ctrls.desired_perf = 0;
+ 		perf_ctrls.max_perf = min_perf;
+ 		perf_ctrls.energy_perf = AMD_CPPC_ENERGY_PERF_PREF(HWP_EPP_BALANCE_POWERSAVE);
+ 		cppc_set_perf(cpudata->cpu, &perf_ctrls);
+ 	}
+ 	mutex_unlock(&amd_pstate_limits_lock);
+ }
+ 
+ static int amd_pstate_epp_cpu_offline(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	pr_debug("AMD CPU Core %d going offline\n", cpudata->cpu);
+ 
+ 	if (cpudata->suspended)
+ 		return 0;
+ 
+ 	if (cppc_state == AMD_PSTATE_ACTIVE)
+ 		amd_pstate_epp_offline(policy);
+ 
+ 	return 0;
+ }
+ 
+ static int amd_pstate_epp_verify_policy(struct cpufreq_policy_data *policy)
+ {
+ 	cpufreq_verify_within_cpu_limits(policy);
+ 	pr_debug("policy_max =%d, policy_min=%d\n", policy->max, policy->min);
+ 	return 0;
+ }
+ 
+ static int amd_pstate_epp_suspend(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 	int ret;
+ 
+ 	/* avoid suspending when EPP is not enabled */
+ 	if (cppc_state != AMD_PSTATE_ACTIVE)
+ 		return 0;
+ 
+ 	/* set this flag to avoid setting core offline*/
+ 	cpudata->suspended = true;
+ 
+ 	/* disable CPPC in lowlevel firmware */
+ 	ret = amd_pstate_enable(false);
+ 	if (ret)
+ 		pr_err("failed to suspend, return %d\n", ret);
+ 
+ 	return 0;
+ }
+ 
+ static int amd_pstate_epp_resume(struct cpufreq_policy *policy)
+ {
+ 	struct amd_cpudata *cpudata = policy->driver_data;
+ 
+ 	if (cpudata->suspended) {
+ 		mutex_lock(&amd_pstate_limits_lock);
+ 
+ 		/* enable amd pstate from suspend state*/
+ 		amd_pstate_epp_reenable(cpudata);
+ 
+ 		mutex_unlock(&amd_pstate_limits_lock);
+ 
+ 		cpudata->suspended = false;
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> abd61c08ef34 (cpufreq: amd-pstate: add driver working mode switch support)
  static struct cpufreq_driver amd_pstate_driver = {
  	.flags		= CPUFREQ_CONST_LOOPS | CPUFREQ_NEED_UPDATE_LIMITS,
  	.verify		= amd_pstate_verify,
@@@ -718,11 -1295,29 +1128,30 @@@ static int __init amd_pstate_init(void
  		return ret;
  	}
  
 -	ret = cpufreq_register_driver(current_pstate_driver);
 +	ret = cpufreq_register_driver(&amd_pstate_driver);
  	if (ret)
 -		pr_err("failed to register with return %d\n", ret);
 +		pr_err("failed to register amd_pstate_driver with return %d\n",
 +		       ret);
  
+ 	amd_pstate_kobj = kobject_create_and_add("amd_pstate", &cpu_subsys.dev_root->kobj);
+ 	if (!amd_pstate_kobj) {
+ 		ret = -EINVAL;
+ 		pr_err("global sysfs registration failed.\n");
+ 		goto kobject_free;
+ 	}
+ 
+ 	ret = sysfs_create_group(amd_pstate_kobj, &amd_pstate_global_attr_group);
+ 	if (ret) {
+ 		pr_err("sysfs attribute export failed with error %d.\n", ret);
+ 		goto global_attr_free;
+ 	}
+ 
+ 	return ret;
+ 
+ global_attr_free:
+ 	kobject_put(amd_pstate_kobj);
+ kobject_free:
+ 	cpufreq_unregister_driver(current_pstate_driver);
  	return ret;
  }
  device_initcall(amd_pstate_init);
* Unmerged path drivers/cpufreq/amd-pstate.c
