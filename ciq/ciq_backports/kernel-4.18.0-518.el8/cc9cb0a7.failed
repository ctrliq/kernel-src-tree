sched, smp: Trace IPIs sent via send_call_function_single_ipi()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-518.el8
commit-author Valentin Schneider <vschneid@redhat.com>
commit cc9cb0a71725aa8dd8d8f534a9b562bbf7981f75
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-518.el8/cc9cb0a7.failed

send_call_function_single_ipi() is the thing that sends IPIs at the bottom
of smp_call_function*() via either generic_exec_single() or
smp_call_function_many_cond(). Give it an IPI-related tracepoint.

Note that this ends up tracing any IPI sent via __smp_call_single_queue(),
which covers __ttwu_queue_wakelist() and irq_work_queue_on() "for free".

	Signed-off-by: Valentin Schneider <vschneid@redhat.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Steven Rostedt (Google) <rostedt@goodmis.org>
	Acked-by: Ingo Molnar <mingo@kernel.org>
Link: https://lore.kernel.org/r/20230307143558.294354-3-vschneid@redhat.com
(cherry picked from commit cc9cb0a71725aa8dd8d8f534a9b562bbf7981f75)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 9ba44485051c,c26a2cd99ec7..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -5,26 -6,97 +5,31 @@@
   *
   *  Copyright (C) 1991-2002  Linus Torvalds
   */
 -#include <linux/highmem.h>
 -#include <linux/hrtimer_api.h>
 -#include <linux/ktime_api.h>
 -#include <linux/sched/signal.h>
 -#include <linux/syscalls_api.h>
 -#include <linux/debug_locks.h>
 -#include <linux/prefetch.h>
 -#include <linux/capability.h>
 -#include <linux/pgtable_api.h>
 -#include <linux/wait_bit.h>
 -#include <linux/jiffies.h>
 -#include <linux/spinlock_api.h>
 -#include <linux/cpumask_api.h>
 -#include <linux/lockdep_api.h>
 -#include <linux/hardirq.h>
 -#include <linux/softirq.h>
 -#include <linux/refcount_api.h>
 -#include <linux/topology.h>
 -#include <linux/sched/clock.h>
 -#include <linux/sched/cond_resched.h>
 -#include <linux/sched/cputime.h>
 -#include <linux/sched/debug.h>
 -#include <linux/sched/hotplug.h>
 -#include <linux/sched/init.h>
 -#include <linux/sched/isolation.h>
 -#include <linux/sched/loadavg.h>
 -#include <linux/sched/mm.h>
 -#include <linux/sched/nohz.h>
 -#include <linux/sched/rseq_api.h>
 -#include <linux/sched/rt.h>
 -
 -#include <linux/blkdev.h>
 -#include <linux/context_tracking.h>
 -#include <linux/cpuset.h>
 -#include <linux/delayacct.h>
 -#include <linux/init_task.h>
 -#include <linux/interrupt.h>
 -#include <linux/ioprio.h>
 -#include <linux/kallsyms.h>
 -#include <linux/kcov.h>
 -#include <linux/kprobes.h>
 -#include <linux/llist_api.h>
 -#include <linux/mmu_context.h>
 -#include <linux/mmzone.h>
 -#include <linux/mutex_api.h>
 -#include <linux/nmi.h>
 -#include <linux/nospec.h>
 -#include <linux/perf_event_api.h>
 -#include <linux/profile.h>
 -#include <linux/psi.h>
 -#include <linux/rcuwait_api.h>
 -#include <linux/sched/wake_q.h>
 -#include <linux/scs.h>
 -#include <linux/slab.h>
 -#include <linux/syscalls.h>
 -#include <linux/vtime.h>
 -#include <linux/wait_api.h>
 -#include <linux/workqueue_api.h>
 -
 -#ifdef CONFIG_PREEMPT_DYNAMIC
 -# ifdef CONFIG_GENERIC_ENTRY
 -#  include <linux/entry-common.h>
 -# endif
 -#endif
 -
 -#include <uapi/linux/sched/types.h>
 -
 -#include <asm/irq_regs.h>
 -#include <asm/switch_to.h>
 -#include <asm/tlb.h>
 -
  #define CREATE_TRACE_POINTS
 -#include <linux/sched/rseq_api.h>
  #include <trace/events/sched.h>
+ #include <trace/events/ipi.h>
  #undef CREATE_TRACE_POINTS
  
  #include "sched.h"
 -#include "stats.h"
 -#include "autogroup.h"
  
 -#include "autogroup.h"
 -#include "pelt.h"
 -#include "smp.h"
 -#include "stats.h"
 +#include <linux/nospec.h>
 +
 +#include <linux/kcov.h>
 +#include <linux/scs.h>
 +
 +#include <asm/switch_to.h>
 +#include <asm/tlb.h>
  
  #include "../workqueue_internal.h"
 -#include "../../io_uring/io-wq.h"
 +#include "../../fs/io-wq.h"
  #include "../smpboot.h"
  
++<<<<<<< HEAD
 +#include "pelt.h"
 +#include "smp.h"
++=======
+ EXPORT_TRACEPOINT_SYMBOL_GPL(ipi_send_cpumask);
++>>>>>>> cc9cb0a71725 (sched, smp: Trace IPIs sent via send_call_function_single_ipi())
  
  /*
   * Export tracepoints that act as a bare tracehook (ie: have no trace event
@@@ -2402,171 -4573,202 +2407,173 @@@ static int ttwu_runnable(struct task_st
  
  	return ret;
  }
 -__setup("schedstats=", setup_schedstats);
  
 -#ifdef CONFIG_PROC_SYSCTL
 -static int sysctl_schedstats(struct ctl_table *table, int write, void *buffer,
 -		size_t *lenp, loff_t *ppos)
 +#ifdef CONFIG_SMP
 +void sched_ttwu_pending(void *arg)
  {
 -	struct ctl_table t;
 -	int err;
 -	int state = static_branch_likely(&sched_schedstats);
 +	struct llist_node *llist = arg;
 +	struct rq *rq = this_rq();
 +	struct task_struct *p, *t;
 +	struct rq_flags rf;
  
 -	if (write && !capable(CAP_SYS_ADMIN))
 -		return -EPERM;
 +	if (!llist)
 +		return;
  
 -	t = *table;
 -	t.data = &state;
 -	err = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
 -	if (err < 0)
 -		return err;
 -	if (write)
 -		set_schedstats(state);
 -	return err;
 +	rq_lock_irqsave(rq, &rf);
 +	update_rq_clock(rq);
 +
 +	llist_for_each_entry_safe(p, t, llist, wake_entry.llist) {
 +		if (WARN_ON_ONCE(p->on_cpu))
 +			smp_cond_load_acquire(&p->on_cpu, !VAL);
 +
 +		if (WARN_ON_ONCE(task_cpu(p) != cpu_of(rq)))
 +			set_task_cpu(p, cpu_of(rq));
 +
 +		ttwu_do_activate(rq, p, p->sched_remote_wakeup ? WF_MIGRATED : 0, &rf);
 +	}
 +
 +	/*
 +	 * Must be after enqueueing at least once task such that
 +	 * idle_cpu() does not observe a false-negative -- if it does,
 +	 * it is possible for select_idle_siblings() to stack a number
 +	 * of tasks on this CPU during that window.
 +	 *
 +	 * It is ok to clear ttwu_pending when another task pending.
 +	 * We will receive IPI after local irq enabled and then enqueue it.
 +	 * Since now nr_running > 0, idle_cpu() will always get correct result.
 +	 */
 +	WRITE_ONCE(rq->ttwu_pending, 0);
 +	rq_unlock_irqrestore(rq, &rf);
  }
 -#endif /* CONFIG_PROC_SYSCTL */
 -#endif /* CONFIG_SCHEDSTATS */
  
 -#ifdef CONFIG_SYSCTL
 -static struct ctl_table sched_core_sysctls[] = {
 -#ifdef CONFIG_SCHEDSTATS
 -	{
 -		.procname       = "sched_schedstats",
 -		.data           = NULL,
 -		.maxlen         = sizeof(unsigned int),
 -		.mode           = 0644,
 -		.proc_handler   = sysctl_schedstats,
 -		.extra1         = SYSCTL_ZERO,
 -		.extra2         = SYSCTL_ONE,
 -	},
 -#endif /* CONFIG_SCHEDSTATS */
 -#ifdef CONFIG_UCLAMP_TASK
 -	{
 -		.procname       = "sched_util_clamp_min",
 -		.data           = &sysctl_sched_uclamp_util_min,
 -		.maxlen         = sizeof(unsigned int),
 -		.mode           = 0644,
 -		.proc_handler   = sysctl_sched_uclamp_handler,
 -	},
 -	{
 -		.procname       = "sched_util_clamp_max",
 -		.data           = &sysctl_sched_uclamp_util_max,
 -		.maxlen         = sizeof(unsigned int),
 -		.mode           = 0644,
 -		.proc_handler   = sysctl_sched_uclamp_handler,
 -	},
 -	{
 -		.procname       = "sched_util_clamp_min_rt_default",
 -		.data           = &sysctl_sched_uclamp_util_min_rt_default,
 -		.maxlen         = sizeof(unsigned int),
 -		.mode           = 0644,
 -		.proc_handler   = sysctl_sched_uclamp_handler,
 -	},
 -#endif /* CONFIG_UCLAMP_TASK */
 -#ifdef CONFIG_NUMA_BALANCING
 -	{
 -		.procname	= "numa_balancing",
 -		.data		= NULL, /* filled in by handler */
 -		.maxlen		= sizeof(unsigned int),
 -		.mode		= 0644,
 -		.proc_handler	= sysctl_numa_balancing,
 -		.extra1		= SYSCTL_ZERO,
 -		.extra2		= SYSCTL_FOUR,
 -	},
 -#endif /* CONFIG_NUMA_BALANCING */
 -	{}
 -};
 -static int __init sched_core_sysctl_init(void)
 +void send_call_function_single_ipi(int cpu)
  {
 -	register_sysctl_init("kernel", sched_core_sysctls);
 -	return 0;
 +	struct rq *rq = cpu_rq(cpu);
 +
- 	if (!set_nr_if_polling(rq->idle))
++	if (!set_nr_if_polling(rq->idle)) {
++		trace_ipi_send_cpumask(cpumask_of(cpu), _RET_IP_, NULL);
 +		arch_send_call_function_single_ipi(cpu);
- 	else
++	} else {
 +		trace_sched_wake_idle_without_ipi(cpu);
++	}
  }
 -late_initcall(sched_core_sysctl_init);
 -#endif /* CONFIG_SYSCTL */
  
  /*
 - * fork()/clone()-time setup:
 + * Queue a task on the target CPUs wake_list and wake the CPU via IPI if
 + * necessary. The wakee CPU on receipt of the IPI will queue the task
 + * via sched_ttwu_wakeup() for activation so the wakee incurs the cost
 + * of the wakeup instead of the waker.
   */
 -int sched_fork(unsigned long clone_flags, struct task_struct *p)
 +static void __ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
  {
 -	__sched_fork(clone_flags, p);
 -	/*
 -	 * We mark the process as NEW here. This guarantees that
 -	 * nobody will actually run it, and a signal or other external
 -	 * event cannot wake it up and insert it on the runqueue either.
 -	 */
 -	p->__state = TASK_NEW;
 -
 -	/*
 -	 * Make sure we do not leak PI boosting priority to the child.
 -	 */
 -	p->prio = current->normal_prio;
 -
 -	uclamp_fork(p);
 +	struct rq *rq = cpu_rq(cpu);
  
 -	/*
 -	 * Revert to default priority/policy on fork if requested.
 -	 */
 -	if (unlikely(p->sched_reset_on_fork)) {
 -		if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
 -			p->policy = SCHED_NORMAL;
 -			p->static_prio = NICE_TO_PRIO(0);
 -			p->rt_priority = 0;
 -		} else if (PRIO_TO_NICE(p->static_prio) < 0)
 -			p->static_prio = NICE_TO_PRIO(0);
 +	p->sched_remote_wakeup = !!(wake_flags & WF_MIGRATED);
  
 -		p->prio = p->normal_prio = p->static_prio;
 -		set_load_weight(p, false);
 +	WRITE_ONCE(rq->ttwu_pending, 1);
 +	__smp_call_single_queue(cpu, &p->wake_entry.llist);
 +}
  
 -		/*
 -		 * We don't need the reset flag anymore after the fork. It has
 -		 * fulfilled its duty:
 -		 */
 -		p->sched_reset_on_fork = 0;
 -	}
 +void wake_up_if_idle(int cpu)
 +{
 +	struct rq *rq = cpu_rq(cpu);
 +	struct rq_flags rf;
  
 -	if (dl_prio(p->prio))
 -		return -EAGAIN;
 -	else if (rt_prio(p->prio))
 -		p->sched_class = &rt_sched_class;
 -	else
 -		p->sched_class = &fair_sched_class;
 +	rcu_read_lock();
  
 -	init_entity_runnable_average(&p->se);
 +	if (!is_idle_task(rcu_dereference(rq->curr)))
 +		goto out;
  
 +	rq_lock_irqsave(rq, &rf);
 +	if (is_idle_task(rq->curr))
 +		resched_curr(rq);
 +	/* Else CPU is not idle, do nothing here: */
 +	rq_unlock_irqrestore(rq, &rf);
  
 -#ifdef CONFIG_SCHED_INFO
 -	if (likely(sched_info_on()))
 -		memset(&p->sched_info, 0, sizeof(p->sched_info));
 -#endif
 -#if defined(CONFIG_SMP)
 -	p->on_cpu = 0;
 -#endif
 -	init_task_preempt_count(p);
 -#ifdef CONFIG_SMP
 -	plist_node_init(&p->pushable_tasks, MAX_PRIO);
 -	RB_CLEAR_NODE(&p->pushable_dl_tasks);
 -#endif
 -	return 0;
 +out:
 +	rcu_read_unlock();
  }
  
 -void sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs)
 +bool cpus_share_cache(int this_cpu, int that_cpu)
  {
 -	unsigned long flags;
 +	if (this_cpu == that_cpu)
 +		return true;
  
 +	return per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);
 +}
 +
 +static inline bool ttwu_queue_cond(struct task_struct *p, int cpu)
 +{
  	/*
 -	 * Because we're not yet on the pid-hash, p->pi_lock isn't strictly
 -	 * required yet, but lockdep gets upset if rules are violated.
 +	 * Do not complicate things with the async wake_list while the CPU is
 +	 * in hotplug state.
  	 */
 -	raw_spin_lock_irqsave(&p->pi_lock, flags);
 -#ifdef CONFIG_CGROUP_SCHED
 -	if (1) {
 -		struct task_group *tg;
 -		tg = container_of(kargs->cset->subsys[cpu_cgrp_id],
 -				  struct task_group, css);
 -		tg = autogroup_task_group(p, tg);
 -		p->sched_task_group = tg;
 -	}
 -#endif
 -	rseq_migrate(p);
 +	if (!cpu_active(cpu))
 +		return false;
 +
 +	/* Ensure the task will still be allowed to run on the CPU. */
 +	if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 +		return false;
 +
  	/*
 -	 * We're setting the CPU for the first time, we don't migrate,
 -	 * so use __set_task_cpu().
 +	 * If the CPU does not share cache, then queue the task on the
 +	 * remote rqs wakelist to avoid accessing remote data.
  	 */
 -	__set_task_cpu(p, smp_processor_id());
 -	if (p->sched_class->task_fork)
 -		p->sched_class->task_fork(p);
 -	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 +	if (!cpus_share_cache(smp_processor_id(), cpu))
 +		return true;
 +
 +	if (cpu == smp_processor_id())
 +		return false;
 +
 +	/*
 +	 * If the wakee cpu is idle, or the task is descheduling and the
 +	 * only running task on the CPU, then use the wakelist to offload
 +	 * the task activation to the idle (or soon-to-be-idle) CPU as
 +	 * the current CPU is likely busy. nr_running is checked to
 +	 * avoid unnecessary task stacking.
 +	 *
 +	 * Note that we can only get here with (wakee) p->on_rq=0,
 +	 * p->on_cpu can be whatever, we've done the dequeue, so
 +	 * the wakee has been accounted out of ->nr_running.
 +	 */
 +	if (!cpu_rq(cpu)->nr_running)
 +		return true;
 +
 +	return false;
  }
  
 -void sched_post_fork(struct task_struct *p)
 +static bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
  {
 -	uclamp_post_fork(p);
 +	if (sched_feat(TTWU_QUEUE) && ttwu_queue_cond(p, cpu)) {
 +		sched_clock_cpu(cpu); /* Sync clocks across CPUs */
 +		__ttwu_queue_wakelist(p, cpu, wake_flags);
 +		return true;
 +	}
 +
 +	return false;
  }
  
 -unsigned long to_ratio(u64 period, u64 runtime)
 +#else /* !CONFIG_SMP */
 +
 +static inline bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags)
  {
 -	if (runtime == RUNTIME_INF)
 -		return BW_UNIT;
 +	return false;
 +}
  
 -	/*
 -	 * Doing this here saves a lot of checks in all
 -	 * the calling paths, and returning zero seems
 -	 * safe for them anyway.
 -	 */
 -	if (period == 0)
 -		return 0;
 +#endif /* CONFIG_SMP */
  
 -	return div64_u64(runtime << BW_SHIFT, period);
 +static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags)
 +{
 +	struct rq *rq = cpu_rq(cpu);
 +	struct rq_flags rf;
 +
 +	if (ttwu_queue_wakelist(p, cpu, wake_flags))
 +		return;
 +
 +	rq_lock(rq, &rf);
 +	update_rq_clock(rq);
 +	ttwu_do_activate(rq, p, wake_flags, &rf);
 +	rq_unlock(rq, &rf);
  }
  
  /*
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index 3ee72472caf2..42d17862f1da 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -51,7 +51,6 @@
 #include <asm/mach/arch.h>
 #include <asm/mpu.h>
 
-#define CREATE_TRACE_POINTS
 #include <trace/events/ipi.h>
 
 /*
diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 42d733d78987..27c261f98f5d 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -63,7 +63,6 @@
 #include <asm/ptrace.h>
 #include <asm/virt.h>
 
-#define CREATE_TRACE_POINTS
 #include <trace/events/ipi.h>
 
 DEFINE_PER_CPU_READ_MOSTLY(int, cpu_number);
* Unmerged path kernel/sched/core.c
diff --git a/kernel/smp.c b/kernel/smp.c
index ff38bf8f2556..dbbcfe6ac963 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -21,6 +21,8 @@
 #include <linux/sched/idle.h>
 #include <linux/hypervisor.h>
 
+#include <trace/events/ipi.h>
+
 #include "smpboot.h"
 #include "sched/smp.h"
 
