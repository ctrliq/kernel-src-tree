irqtime: Move irqtime entry accounting after irq offset incrementation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-425.13.1.el8_7
commit-author Frederic Weisbecker <frederic@kernel.org>
commit d3759e7184f8f6187e62f8c4e7dcb1f6c47c075a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-425.13.1.el8_7/d3759e71.failed

IRQ time entry is currently accounted before HARDIRQ_OFFSET or
SOFTIRQ_OFFSET are incremented. This is convenient to decide to which
index the cputime to account is dispatched.

Unfortunately it prevents tick_irq_enter() from being called under
HARDIRQ_OFFSET because tick_irq_enter() has to be called before the IRQ
entry accounting due to the necessary clock catch up. As a result we
don't benefit from appropriate lockdep coverage on tick_irq_enter().

To prepare for fixing this, move the IRQ entry cputime accounting after
the preempt offset is incremented. This requires the cputime dispatch
code to handle the extra offset.

	Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lore.kernel.org/r/20201202115732.27827-5-frederic@kernel.org

(cherry picked from commit d3759e7184f8f6187e62f8c4e7dcb1f6c47c075a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/vtime.h
#	kernel/sched/cputime.c
diff --cc include/linux/vtime.h
index c82a20fde06f,041d6524d144..000000000000
--- a/include/linux/vtime.h
+++ b/include/linux/vtime.h
@@@ -84,36 -83,46 +84,69 @@@ static inline void vtime_init_idle(stru
  #endif
  
  #ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
++<<<<<<< HEAD
 +extern void vtime_account_irq_enter(struct task_struct *tsk);
 +static inline void vtime_account_irq_exit(struct task_struct *tsk)
 +{
 +	/* On hard|softirq exit we always account to hard|softirq cputime */
 +	vtime_account_kernel(tsk);
 +}
 +extern void vtime_flush(struct task_struct *tsk);
 +#else /* !CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */
 +static inline void vtime_account_irq_enter(struct task_struct *tsk) { }
 +static inline void vtime_account_irq_exit(struct task_struct *tsk) { }
++=======
+ extern void vtime_account_irq(struct task_struct *tsk, unsigned int offset);
+ extern void vtime_account_softirq(struct task_struct *tsk);
+ extern void vtime_account_hardirq(struct task_struct *tsk);
+ extern void vtime_flush(struct task_struct *tsk);
+ #else /* !CONFIG_VIRT_CPU_ACCOUNTING_NATIVE */
+ static inline void vtime_account_irq(struct task_struct *tsk, unsigned int offset) { }
+ static inline void vtime_account_softirq(struct task_struct *tsk) { }
+ static inline void vtime_account_hardirq(struct task_struct *tsk) { }
++>>>>>>> d3759e7184f8 (irqtime: Move irqtime entry accounting after irq offset incrementation)
  static inline void vtime_flush(struct task_struct *tsk) { }
  #endif
  
  
  #ifdef CONFIG_IRQ_TIME_ACCOUNTING
- extern void irqtime_account_irq(struct task_struct *tsk);
+ extern void irqtime_account_irq(struct task_struct *tsk, unsigned int offset);
  #else
- static inline void irqtime_account_irq(struct task_struct *tsk) { }
+ static inline void irqtime_account_irq(struct task_struct *tsk, unsigned int offset) { }
  #endif
  
- static inline void account_irq_enter_time(struct task_struct *tsk)
+ static inline void account_softirq_enter(struct task_struct *tsk)
  {
++<<<<<<< HEAD
 +	vtime_account_irq_enter(tsk);
 +	irqtime_account_irq(tsk);
++=======
+ 	vtime_account_irq(tsk, SOFTIRQ_OFFSET);
+ 	irqtime_account_irq(tsk, SOFTIRQ_OFFSET);
++>>>>>>> d3759e7184f8 (irqtime: Move irqtime entry accounting after irq offset incrementation)
  }
  
- static inline void account_irq_exit_time(struct task_struct *tsk)
+ static inline void account_softirq_exit(struct task_struct *tsk)
  {
++<<<<<<< HEAD
 +	vtime_account_irq_exit(tsk);
 +	irqtime_account_irq(tsk);
++=======
+ 	vtime_account_softirq(tsk);
+ 	irqtime_account_irq(tsk, 0);
+ }
+ 
+ static inline void account_hardirq_enter(struct task_struct *tsk)
+ {
+ 	vtime_account_irq(tsk, HARDIRQ_OFFSET);
+ 	irqtime_account_irq(tsk, HARDIRQ_OFFSET);
+ }
+ 
+ static inline void account_hardirq_exit(struct task_struct *tsk)
+ {
+ 	vtime_account_hardirq(tsk);
+ 	irqtime_account_irq(tsk, 0);
++>>>>>>> d3759e7184f8 (irqtime: Move irqtime entry accounting after irq offset incrementation)
  }
  
  #endif /* _LINUX_KERNEL_VTIME_H */
diff --cc kernel/sched/cputime.c
index f39744545731,5f611658eeab..000000000000
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@@ -418,23 -419,21 +420,36 @@@ void vtime_task_switch(struct task_stru
  }
  # endif
  
++<<<<<<< HEAD
 +/*
 + * Archs that account the whole time spent in the idle task
 + * (outside irq) as idle time can rely on this and just implement
 + * vtime_account_kernel() and vtime_account_idle(). Archs that
 + * have other meaning of the idle time (s390 only includes the
 + * time spent by the CPU when it's in low power mode) must override
 + * vtime_account().
 + */
 +#ifndef __ARCH_HAS_VTIME_ACCOUNT
 +void vtime_account_irq_enter(struct task_struct *tsk)
 +{
 +	if (!in_interrupt() && is_idle_task(tsk))
++=======
+ void vtime_account_irq(struct task_struct *tsk, unsigned int offset)
+ {
+ 	unsigned int pc = preempt_count() - offset;
+ 
+ 	if (pc & HARDIRQ_OFFSET) {
+ 		vtime_account_hardirq(tsk);
+ 	} else if (pc & SOFTIRQ_OFFSET) {
+ 		vtime_account_softirq(tsk);
+ 	} else if (!IS_ENABLED(CONFIG_HAVE_VIRT_CPU_ACCOUNTING_IDLE) &&
+ 		   is_idle_task(tsk)) {
++>>>>>>> d3759e7184f8 (irqtime: Move irqtime entry accounting after irq offset incrementation)
  		vtime_account_idle(tsk);
 -	} else {
 +	else
  		vtime_account_kernel(tsk);
 -	}
  }
 +#endif /* __ARCH_HAS_VTIME_ACCOUNT */
  
  void cputime_adjust(struct task_cputime *curr, struct prev_cputime *prev,
  		    u64 *ut, u64 *st)
diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index b740a75d4867..844c424c10ca 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -32,9 +32,9 @@ static __always_inline void rcu_irq_enter_check_tick(void)
  */
 #define __irq_enter()					\
 	do {						\
-		account_irq_enter_time(current);	\
 		preempt_count_add(HARDIRQ_OFFSET);	\
 		lockdep_hardirq_enter();		\
+		account_hardirq_enter(current);		\
 	} while (0)
 
 /*
@@ -62,8 +62,8 @@ void irq_enter_rcu(void);
  */
 #define __irq_exit()					\
 	do {						\
+		account_hardirq_exit(current);		\
 		lockdep_hardirq_exit();			\
-		account_irq_exit_time(current);		\
 		preempt_count_sub(HARDIRQ_OFFSET);	\
 	} while (0)
 
* Unmerged path include/linux/vtime.h
* Unmerged path kernel/sched/cputime.c
diff --git a/kernel/softirq.c b/kernel/softirq.c
index fd29444a090e..66ab1163fa35 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -330,10 +330,10 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 	current->flags &= ~PF_MEMALLOC;
 
 	pending = local_softirq_pending();
-	account_irq_enter_time(current);
 
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
 	in_hardirq = lockdep_softirq_start();
+	account_softirq_enter(current);
 
 restart:
 	/* Reset the pending bitmask before enabling irqs */
@@ -380,8 +380,8 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 		wakeup_softirqd();
 	}
 
+	account_softirq_exit(current);
 	lockdep_softirq_end(in_hardirq);
-	account_irq_exit_time(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
 	WARN_ON_ONCE(in_interrupt());
 	current_restore_flags(old_flags, PF_MEMALLOC);
@@ -438,7 +438,7 @@ void irq_exit_rcu(void)
 #else
 	lockdep_assert_irqs_disabled();
 #endif
-	account_irq_exit_time(current);
+	account_hardirq_exit(current);
 	preempt_count_sub(HARDIRQ_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
