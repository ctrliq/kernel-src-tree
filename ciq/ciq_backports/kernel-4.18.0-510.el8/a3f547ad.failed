x86/mm: Do not shuffle CPU entry areas without KASLR

jira LE-1907
cve CVE-2023-0597
Rebuild_History Non-Buildable kernel-4.18.0-510.el8
commit-author Michal Koutný <mkoutny@suse.com>
commit a3f547addcaa10df5a226526bc9e2d9a94542344
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-510.el8/a3f547ad.failed

The commit 97e3d26b5e5f ("x86/mm: Randomize per-cpu entry area") fixed
an omission of KASLR on CPU entry areas. It doesn't take into account
KASLR switches though, which may result in unintended non-determinism
when a user wants to avoid it (e.g. debugging, benchmarking).

Generate only a single combination of CPU entry areas offsets -- the
linear array that existed prior randomization when KASLR is turned off.

Since we have 3f148f331814 ("x86/kasan: Map shadow for percpu pages on
demand") and followups, we can use the more relaxed guard
kasrl_enabled() (in contrast to kaslr_memory_enabled()).

Fixes: 97e3d26b5e5f ("x86/mm: Randomize per-cpu entry area")
	Signed-off-by: Michal Koutný <mkoutny@suse.com>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/all/20230306193144.24605-1-mkoutny%40suse.com
(cherry picked from commit a3f547addcaa10df5a226526bc9e2d9a94542344)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/mm/cpu_entry_area.c
diff --cc arch/x86/mm/cpu_entry_area.c
index 463c6ea6086b,e91500a80963..000000000000
--- a/arch/x86/mm/cpu_entry_area.c
+++ b/arch/x86/mm/cpu_entry_area.c
@@@ -16,6 -17,53 +17,56 @@@ static DEFINE_PER_CPU_PAGE_ALIGNED(stru
  #ifdef CONFIG_X86_64
  static DEFINE_PER_CPU_PAGE_ALIGNED(struct exception_stacks, exception_stacks);
  DEFINE_PER_CPU(struct cea_exception_stacks*, cea_exception_stacks);
++<<<<<<< HEAD
++=======
+ 
+ static DEFINE_PER_CPU_READ_MOSTLY(unsigned long, _cea_offset);
+ 
+ static __always_inline unsigned int cea_offset(unsigned int cpu)
+ {
+ 	return per_cpu(_cea_offset, cpu);
+ }
+ 
+ static __init void init_cea_offsets(void)
+ {
+ 	unsigned int max_cea;
+ 	unsigned int i, j;
+ 
+ 	if (!kaslr_enabled()) {
+ 		for_each_possible_cpu(i)
+ 			per_cpu(_cea_offset, i) = i;
+ 		return;
+ 	}
+ 
+ 	max_cea = (CPU_ENTRY_AREA_MAP_SIZE - PAGE_SIZE) / CPU_ENTRY_AREA_SIZE;
+ 
+ 	/* O(sodding terrible) */
+ 	for_each_possible_cpu(i) {
+ 		unsigned int cea;
+ 
+ again:
+ 		cea = get_random_u32_below(max_cea);
+ 
+ 		for_each_possible_cpu(j) {
+ 			if (cea_offset(j) == cea)
+ 				goto again;
+ 
+ 			if (i == j)
+ 				break;
+ 		}
+ 
+ 		per_cpu(_cea_offset, i) = cea;
+ 	}
+ }
+ #else /* !X86_64 */
+ DECLARE_PER_CPU_PAGE_ALIGNED(struct doublefault_stack, doublefault_stack);
+ 
+ static __always_inline unsigned int cea_offset(unsigned int cpu)
+ {
+ 	return cpu;
+ }
+ static inline void init_cea_offsets(void) { }
++>>>>>>> a3f547addcaa (x86/mm: Do not shuffle CPU entry areas without KASLR)
  #endif
  
  /* Is called from entry code, so must be noinstr */
* Unmerged path arch/x86/mm/cpu_entry_area.c
