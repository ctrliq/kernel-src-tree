x86/its: Add support for ITS-safe return thunk

jira LE-4694
cve CVE-2024-28956
Rebuild_History Non-Buildable kernel-6.12.0-55.43.1.el10_0
commit-author Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
commit a75bf27fe41abe658c53276a0c486c4bf9adecfc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-6.12.0-55.43.1.el10_0/a75bf27f.failed

RETs in the lower half of cacheline may be affected by ITS bug,
specifically when the RSB-underflows. Use ITS-safe return thunk for such
RETs.

RETs that are not patched:

- RET in retpoline sequence does not need to be patched, because the
  sequence itself fills an RSB before RET.
- RET in Call Depth Tracking (CDT) thunks __x86_indirect_{call|jump}_thunk
  and call_depth_return_thunk are not patched because CDT by design
  prevents RSB-underflow.
- RETs in .init section are not reachable after init.
- RETs that are explicitly marked safe with ANNOTATE_UNRET_SAFE.

	Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
(cherry picked from commit a75bf27fe41abe658c53276a0c486c4bf9adecfc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/vmlinux.lds.S
#	arch/x86/lib/retpoline.S
diff --cc arch/x86/kernel/vmlinux.lds.S
index feb8102a9ca7,e97f5773c8df..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -530,4 -497,29 +530,17 @@@ INIT_PER_CPU(irq_stack_backing_store)
  		"SRSO function pair won't alias");
  #endif
  
++<<<<<<< HEAD
++=======
+ #if defined(CONFIG_MITIGATION_ITS) && !defined(CONFIG_DEBUG_FORCE_FUNCTION_ALIGN_64B)
+ . = ASSERT(__x86_indirect_its_thunk_rax & 0x20, "__x86_indirect_thunk_rax not in second half of cacheline");
+ . = ASSERT(((__x86_indirect_its_thunk_rcx - __x86_indirect_its_thunk_rax) % 64) == 0, "Indirect thunks are not cacheline apart");
+ . = ASSERT(__x86_indirect_its_thunk_array == __x86_indirect_its_thunk_rax, "Gap in ITS thunk array");
+ #endif
+ 
+ #if defined(CONFIG_MITIGATION_ITS) && !defined(CONFIG_DEBUG_FORCE_FUNCTION_ALIGN_64B)
+ . = ASSERT(its_return_thunk & 0x20, "its_return_thunk not in second half of cacheline");
+ #endif
+ 
++>>>>>>> a75bf27fe41a (x86/its: Add support for ITS-safe return thunk)
  #endif /* CONFIG_X86_64 */
 -
 -/*
 - * The symbols below are referenced using relative relocations in the
 - * respective ELF notes. This produces build time constants that the
 - * linker will never mark as relocatable. (Using just ABSOLUTE() is not
 - * sufficient for that).
 - */
 -#ifdef CONFIG_XEN_PV
 -xen_elfnote_entry_value =
 -	ABSOLUTE(xen_elfnote_entry) + ABSOLUTE(startup_xen);
 -#endif
 -#ifdef CONFIG_PVH
 -xen_elfnote_phys32_entry_value =
 -	ABSOLUTE(xen_elfnote_phys32_entry) + ABSOLUTE(pvh_start_xen - LOAD_OFFSET);
 -#endif
diff --cc arch/x86/lib/retpoline.S
index 391059b2c6fb,ebca28fe7e31..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -366,6 -367,45 +366,48 @@@ SYM_FUNC_END(call_depth_return_thunk
  
  #endif /* CONFIG_MITIGATION_CALL_DEPTH_TRACKING */
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_MITIGATION_ITS
+ 
+ .macro ITS_THUNK reg
+ 
+ SYM_INNER_LABEL(__x86_indirect_its_thunk_\reg, SYM_L_GLOBAL)
+ 	UNWIND_HINT_UNDEFINED
+ 	ANNOTATE_NOENDBR
+ 	ANNOTATE_RETPOLINE_SAFE
+ 	jmp *%\reg
+ 	int3
+ 	.align 32, 0xcc		/* fill to the end of the line */
+ 	.skip  32, 0xcc		/* skip to the next upper half */
+ .endm
+ 
+ /* ITS mitigation requires thunks be aligned to upper half of cacheline */
+ .align 64, 0xcc
+ .skip 32, 0xcc
+ SYM_CODE_START(__x86_indirect_its_thunk_array)
+ 
+ #define GEN(reg) ITS_THUNK reg
+ #include <asm/GEN-for-each-reg.h>
+ #undef GEN
+ 
+ 	.align 64, 0xcc
+ SYM_CODE_END(__x86_indirect_its_thunk_array)
+ 
+ .align 64, 0xcc
+ .skip 32, 0xcc
+ SYM_CODE_START(its_return_thunk)
+ 	UNWIND_HINT_FUNC
+ 	ANNOTATE_NOENDBR
+ 	ANNOTATE_UNRET_SAFE
+ 	ret
+ 	int3
+ SYM_CODE_END(its_return_thunk)
+ EXPORT_SYMBOL(its_return_thunk)
+ 
+ #endif /* CONFIG_MITIGATION_ITS */
+ 
++>>>>>>> a75bf27fe41a (x86/its: Add support for ITS-safe return thunk)
  /*
   * This function name is magical and is used by -mfunction-return=thunk-extern
   * for the compiler to generate JMPs to it.
diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
index ca9ae606aab9..5398d3fdc2d8 100644
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@ -134,6 +134,20 @@ static __always_inline int x86_call_depth_emit_accounting(u8 **pprog,
 }
 #endif
 
+#if defined(CONFIG_MITIGATION_RETHUNK) && defined(CONFIG_OBJTOOL)
+extern bool cpu_wants_rethunk(void);
+extern bool cpu_wants_rethunk_at(void *addr);
+#else
+static __always_inline bool cpu_wants_rethunk(void)
+{
+	return false;
+}
+static __always_inline bool cpu_wants_rethunk_at(void *addr)
+{
+	return false;
+}
+#endif
+
 #ifdef CONFIG_SMP
 extern void alternatives_smp_module_add(struct module *mod, char *name,
 					void *locks, void *locks_end,
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index 96b410b1d4e8..24f1bd659697 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -383,6 +383,12 @@ static inline void srso_return_thunk(void) {}
 static inline void srso_alias_return_thunk(void) {}
 #endif
 
+#ifdef CONFIG_MITIGATION_ITS
+extern void its_return_thunk(void);
+#else
+static inline void its_return_thunk(void) {}
+#endif
+
 extern void retbleed_return_thunk(void);
 extern void srso_return_thunk(void);
 extern void srso_alias_return_thunk(void);
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index d17518ca19b8..1aeb070f6ae4 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -770,6 +770,21 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 
 #ifdef CONFIG_MITIGATION_RETHUNK
 
+bool cpu_wants_rethunk(void)
+{
+	return cpu_feature_enabled(X86_FEATURE_RETHUNK);
+}
+
+bool cpu_wants_rethunk_at(void *addr)
+{
+	if (!cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		return false;
+	if (x86_return_thunk != its_return_thunk)
+		return true;
+
+	return !((unsigned long)addr & 0x20);
+}
+
 /*
  * Rewrite the compiler generated return thunk tail-calls.
  *
@@ -786,7 +801,7 @@ static int patch_return(void *addr, struct insn *insn, u8 *bytes)
 	int i = 0;
 
 	/* Patch the custom return thunks... */
-	if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
+	if (cpu_wants_rethunk_at(addr)) {
 		i = JMP32_INSN_SIZE;
 		__text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);
 	} else {
@@ -803,7 +818,7 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 {
 	s32 *s;
 
-	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+	if (cpu_wants_rethunk())
 		static_call_force_reinit();
 
 	for (s = start; s < end; s++) {
diff --git a/arch/x86/kernel/ftrace.c b/arch/x86/kernel/ftrace.c
index 8da0e66ca22d..bfab966ea56e 100644
--- a/arch/x86/kernel/ftrace.c
+++ b/arch/x86/kernel/ftrace.c
@@ -354,7 +354,7 @@ create_trampoline(struct ftrace_ops *ops, unsigned int *tramp_size)
 		goto fail;
 
 	ip = trampoline + size;
-	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+	if (cpu_wants_rethunk_at(ip))
 		__text_gen_insn(ip, JMP32_INSN_OPCODE, ip, x86_return_thunk, JMP32_INSN_SIZE);
 	else
 		memcpy(ip, retq, sizeof(retq));
diff --git a/arch/x86/kernel/static_call.c b/arch/x86/kernel/static_call.c
index 4eefaac64c6c..c321b1873c30 100644
--- a/arch/x86/kernel/static_call.c
+++ b/arch/x86/kernel/static_call.c
@@ -81,7 +81,7 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 		break;
 
 	case RET:
-		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		if (cpu_wants_rethunk_at(insn))
 			code = text_gen_insn(JMP32_INSN_OPCODE, insn, x86_return_thunk);
 		else
 			code = &retinsn;
@@ -90,7 +90,7 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	case JCC:
 		if (!func) {
 			func = __static_call_return;
-			if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+			if (cpu_wants_rethunk())
 				func = x86_return_thunk;
 		}
 
* Unmerged path arch/x86/kernel/vmlinux.lds.S
* Unmerged path arch/x86/lib/retpoline.S
diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index 06b080b61aa5..88d47bf201e3 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -659,7 +659,7 @@ static void emit_return(u8 **pprog, u8 *ip)
 {
 	u8 *prog = *pprog;
 
-	if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
+	if (cpu_wants_rethunk()) {
 		emit_jump(&prog, x86_return_thunk, ip);
 	} else {
 		EMIT1(0xC3);		/* ret */
