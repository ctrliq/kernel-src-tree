swiotlb: reduce the number of areas to match actual memory pool size

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-537.el8
commit-author Petr Tesarik <petr.tesarik.ext@huawei.com>
commit 8ac04063354a01a484d2e55d20ed1958aa0d3392
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-537.el8/8ac04063.failed

Although the desired size of the SWIOTLB memory pool is increased in
swiotlb_adjust_nareas() to match the number of areas, the actual allocation
may be smaller, which may require reducing the number of areas.

For example, Xen uses swiotlb_init_late(), which in turn uses the page
allocator. On x86, page size is 4 KiB and MAX_ORDER is 10 (1024 pages),
resulting in a maximum memory pool size of 4 MiB. This corresponds to 2048
slots of 2 KiB each. The minimum area size is 128 (IO_TLB_SEGSIZE),
allowing at most 2048 / 128 = 16 areas.

If num_possible_cpus() is greater than the maximum number of areas, areas
are smaller than IO_TLB_SEGSIZE and contiguous groups of free slots will
span multiple areas. When allocating and freeing slots, only one area will
be properly locked, causing race conditions on the unlocked slots and
ultimately data corruption, kernel hangs and crashes.

Fixes: 20347fca71a3 ("swiotlb: split up the global swiotlb lock")
	Signed-off-by: Petr Tesarik <petr.tesarik.ext@huawei.com>
	Reviewed-by: Roberto Sassu <roberto.sassu@huawei.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 8ac04063354a01a484d2e55d20ed1958aa0d3392)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index 2f75af589723,2b83e3ad9dca..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -392,11 -424,8 +413,16 @@@ retry
  			(PAGE_SIZE << order) >> 20);
  	}
  
++<<<<<<< HEAD
 +	if (!default_nareas)
 +		swiotlb_adjust_nareas(num_possible_cpus());
 +
 +	area_order = get_order(array_size(sizeof(*mem->areas),
 +		default_nareas));
++=======
+ 	nareas = limit_nareas(default_nareas, nslabs);
+ 	area_order = get_order(array_size(sizeof(*mem->areas), nareas));
++>>>>>>> 8ac04063354a (swiotlb: reduce the number of areas to match actual memory pool size)
  	mem->areas = (struct io_tlb_area *)
  		__get_free_pages(GFP_KERNEL | __GFP_ZERO, area_order);
  	if (!mem->areas)
* Unmerged path kernel/dma/swiotlb.c
