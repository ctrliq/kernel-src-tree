sched/cpuset: Bring back cpuset_mutex

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-537.el8
commit-author Juri Lelli <juri.lelli@redhat.com>
commit 111cd11bbc54850f24191c52ff217da88a5e639b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-537.el8/111cd11b.failed

Turns out percpu_cpuset_rwsem - commit 1243dc518c9d ("cgroup/cpuset:
Convert cpuset_mutex to percpu_rwsem") - wasn't such a brilliant idea,
as it has been reported to cause slowdowns in workloads that need to
change cpuset configuration frequently and it is also not implementing
priority inheritance (which causes troubles with realtime workloads).

Convert percpu_cpuset_rwsem back to regular cpuset_mutex. Also grab it
only for SCHED_DEADLINE tasks (other policies don't care about stable
cpusets anyway).

	Signed-off-by: Juri Lelli <juri.lelli@redhat.com>
	Reviewed-by: Waiman Long <longman@redhat.com>
	Signed-off-by: Tejun Heo <tj@kernel.org>
(cherry picked from commit 111cd11bbc54850f24191c52ff217da88a5e639b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/cgroup/cpuset.c
#	kernel/sched/core.c
diff --cc kernel/cgroup/cpuset.c
index a1280d02c8f3,041c0809adaf..000000000000
--- a/kernel/cgroup/cpuset.c
+++ b/kernel/cgroup/cpuset.c
@@@ -495,11 -497,21 +496,11 @@@ static inline bool partition_is_populat
   * One way or another, we guarantee to return some non-empty subset
   * of cpu_online_mask.
   *
-  * Call with callback_lock or cpuset_rwsem held.
+  * Call with callback_lock or cpuset_mutex held.
   */
 -static void guarantee_online_cpus(struct task_struct *tsk,
 -				  struct cpumask *pmask)
 +static void guarantee_online_cpus(struct cpuset *cs, struct cpumask *pmask)
  {
 -	const struct cpumask *possible_mask = task_cpu_possible_mask(tsk);
 -	struct cpuset *cs;
 -
 -	if (WARN_ON(!cpumask_and(pmask, possible_mask, cpu_online_mask)))
 -		cpumask_copy(pmask, cpu_online_mask);
 -
 -	rcu_read_lock();
 -	cs = task_cs(tsk);
 -
 -	while (!cpumask_intersects(cs->effective_cpus, pmask)) {
 +	while (!cpumask_intersects(cs->effective_cpus, cpu_online_mask)) {
  		cs = parent_cs(cs);
  		if (unlikely(!cs)) {
  			/*
@@@ -1192,12 -1206,15 +1193,19 @@@ void rebuild_sched_domains(void
  /**
   * update_tasks_cpumask - Update the cpumasks of tasks in the cpuset.
   * @cs: the cpuset in which each task's cpus_allowed mask needs to be changed
 - * @new_cpus: the temp variable for the new effective_cpus mask
   *
   * Iterate through each task of @cs updating its cpus_allowed to the
++<<<<<<< HEAD
 + * effective cpuset's.  As this function is called with cpuset_rwsem held,
 + * cpuset membership stays stable.
++=======
+  * effective cpuset's.  As this function is called with cpuset_mutex held,
+  * cpuset membership stays stable. For top_cpuset, task_cpu_possible_mask()
+  * is used instead of effective_cpus to make sure all offline CPUs are also
+  * included as hotplug code won't update cpumasks for tasks in top_cpuset.
++>>>>>>> 111cd11bbc54 (sched/cpuset: Bring back cpuset_mutex)
   */
 -static void update_tasks_cpumask(struct cpuset *cs, struct cpumask *new_cpus)
 +static void update_tasks_cpumask(struct cpuset *cs)
  {
  	struct css_task_iter it;
  	struct task_struct *task;
@@@ -2427,7 -2459,21 +2435,25 @@@ static int fmeter_getrate(struct fmete
  
  static struct cpuset *cpuset_attach_old_cs;
  
++<<<<<<< HEAD
 +/* Called by cgroups to determine if a cpuset is usable; cpuset_rwsem held */
++=======
+ /*
+  * Check to see if a cpuset can accept a new task
+  * For v1, cpus_allowed and mems_allowed can't be empty.
+  * For v2, effective_cpus can't be empty.
+  * Note that in v1, effective_cpus = cpus_allowed.
+  */
+ static int cpuset_can_attach_check(struct cpuset *cs)
+ {
+ 	if (cpumask_empty(cs->effective_cpus) ||
+ 	   (!is_in_v2_mode() && nodes_empty(cs->mems_allowed)))
+ 		return -ENOSPC;
+ 	return 0;
+ }
+ 
+ /* Called by cgroups to determine if a cpuset is usable; cpuset_mutex held */
++>>>>>>> 111cd11bbc54 (sched/cpuset: Bring back cpuset_mutex)
  static int cpuset_can_attach(struct cgroup_taskset *tset)
  {
  	struct cgroup_subsys_state *css;
@@@ -2439,18 -2485,11 +2465,18 @@@
  	cpuset_attach_old_cs = task_cs(cgroup_taskset_first(tset, &css));
  	cs = css_cs(css);
  
- 	percpu_down_write(&cpuset_rwsem);
+ 	mutex_lock(&cpuset_mutex);
  
 -	/* Check to see if task is allowed in the cpuset */
 -	ret = cpuset_can_attach_check(cs);
 -	if (ret)
 +	/* allow moving tasks into an empty cpuset if on default hierarchy */
 +	ret = -ENOSPC;
 +	if (!is_in_v2_mode() &&
 +	    (cpumask_empty(cs->cpus_allowed) || nodes_empty(cs->mems_allowed)))
 +		goto out_unlock;
 +
 +	/*
 +	 * Task cannot be moved to a cpuset with empty effective cpus.
 +	 */
 +	if (cpumask_empty(cs->effective_cpus))
  		goto out_unlock;
  
  	cgroup_taskset_for_each(task, css, tset) {
@@@ -2467,9 -2506,8 +2493,9 @@@
  	 * changes which zero cpus/mems_allowed.
  	 */
  	cs->attach_in_progress++;
 +	ret = 0;
  out_unlock:
- 	percpu_up_write(&cpuset_rwsem);
+ 	mutex_unlock(&cpuset_mutex);
  	return ret;
  }
  
@@@ -2489,11 -2527,31 +2515,38 @@@ static void cpuset_cancel_attach(struc
  }
  
  /*
++<<<<<<< HEAD
 + * Protected by cpuset_rwsem.  cpus_attach is used only by cpuset_attach()
++=======
+  * Protected by cpuset_mutex. cpus_attach is used only by cpuset_attach_task()
++>>>>>>> 111cd11bbc54 (sched/cpuset: Bring back cpuset_mutex)
   * but we can't allocate it dynamically there.  Define it global and
   * allocate from cpuset_init().
   */
  static cpumask_var_t cpus_attach;
++<<<<<<< HEAD
++=======
+ static nodemask_t cpuset_attach_nodemask_to;
+ 
+ static void cpuset_attach_task(struct cpuset *cs, struct task_struct *task)
+ {
+ 	lockdep_assert_held(&cpuset_mutex);
+ 
+ 	if (cs != &top_cpuset)
+ 		guarantee_online_cpus(task, cpus_attach);
+ 	else
+ 		cpumask_andnot(cpus_attach, task_cpu_possible_mask(task),
+ 			       cs->subparts_cpus);
+ 	/*
+ 	 * can_attach beforehand should guarantee that this doesn't
+ 	 * fail.  TODO: have a better way to handle failure here
+ 	 */
+ 	WARN_ON_ONCE(set_cpus_allowed_ptr(task, cpus_attach));
+ 
+ 	cpuset_change_task_nodemask(task, &cpuset_attach_nodemask_to);
+ 	cpuset_update_task_spread_flags(cs, task);
+ }
++>>>>>>> 111cd11bbc54 (sched/cpuset: Bring back cpuset_mutex)
  
  static void cpuset_attach(struct cgroup_taskset *tset)
  {
@@@ -3234,21 -3274,105 +3287,99 @@@ static void cpuset_bind(struct cgroup_s
  	}
  
  	spin_unlock_irq(&callback_lock);
- 	percpu_up_write(&cpuset_rwsem);
+ 	mutex_unlock(&cpuset_mutex);
+ }
+ 
+ /*
++<<<<<<< HEAD
++=======
+  * In case the child is cloned into a cpuset different from its parent,
+  * additional checks are done to see if the move is allowed.
+  */
+ static int cpuset_can_fork(struct task_struct *task, struct css_set *cset)
+ {
+ 	struct cpuset *cs = css_cs(cset->subsys[cpuset_cgrp_id]);
+ 	bool same_cs;
+ 	int ret;
+ 
+ 	rcu_read_lock();
+ 	same_cs = (cs == task_cs(current));
+ 	rcu_read_unlock();
+ 
+ 	if (same_cs)
+ 		return 0;
+ 
+ 	lockdep_assert_held(&cgroup_mutex);
+ 	mutex_lock(&cpuset_mutex);
+ 
+ 	/* Check to see if task is allowed in the cpuset */
+ 	ret = cpuset_can_attach_check(cs);
+ 	if (ret)
+ 		goto out_unlock;
+ 
+ 	ret = task_can_attach(task, cs->effective_cpus);
+ 	if (ret)
+ 		goto out_unlock;
+ 
+ 	ret = security_task_setscheduler(task);
+ 	if (ret)
+ 		goto out_unlock;
+ 
+ 	/*
+ 	 * Mark attach is in progress.  This makes validate_change() fail
+ 	 * changes which zero cpus/mems_allowed.
+ 	 */
+ 	cs->attach_in_progress++;
+ out_unlock:
+ 	mutex_unlock(&cpuset_mutex);
+ 	return ret;
+ }
+ 
+ static void cpuset_cancel_fork(struct task_struct *task, struct css_set *cset)
+ {
+ 	struct cpuset *cs = css_cs(cset->subsys[cpuset_cgrp_id]);
+ 	bool same_cs;
+ 
+ 	rcu_read_lock();
+ 	same_cs = (cs == task_cs(current));
+ 	rcu_read_unlock();
+ 
+ 	if (same_cs)
+ 		return;
+ 
+ 	mutex_lock(&cpuset_mutex);
+ 	cs->attach_in_progress--;
+ 	if (!cs->attach_in_progress)
+ 		wake_up(&cpuset_attach_wq);
+ 	mutex_unlock(&cpuset_mutex);
  }
  
  /*
++>>>>>>> 111cd11bbc54 (sched/cpuset: Bring back cpuset_mutex)
   * Make sure the new task conform to the current state of its parent,
   * which could have been changed by cpuset just after it inherits the
   * state from the parent and before it sits on the cgroup's task list.
   */
  static void cpuset_fork(struct task_struct *task)
  {
 -	struct cpuset *cs;
 -	bool same_cs;
 -
 -	rcu_read_lock();
 -	cs = task_cs(task);
 -	same_cs = (cs == task_cs(current));
 -	rcu_read_unlock();
 -
 -	if (same_cs) {
 -		if (cs == &top_cpuset)
 -			return;
 -
 -		set_cpus_allowed_ptr(task, current->cpus_ptr);
 -		task->mems_allowed = current->mems_allowed;
 +	if (task_css_is_root(task, cpuset_cgrp_id))
  		return;
 -	}
  
++<<<<<<< HEAD
 +	set_cpus_allowed_ptr(task, current->cpus_ptr);
 +	task->mems_allowed = current->mems_allowed;
++=======
+ 	/* CLONE_INTO_CGROUP */
+ 	mutex_lock(&cpuset_mutex);
+ 	guarantee_online_mems(cs, &cpuset_attach_nodemask_to);
+ 	cpuset_attach_task(cs, task);
+ 
+ 	cs->attach_in_progress--;
+ 	if (!cs->attach_in_progress)
+ 		wake_up(&cpuset_attach_wq);
+ 
+ 	mutex_unlock(&cpuset_mutex);
++>>>>>>> 111cd11bbc54 (sched/cpuset: Bring back cpuset_mutex)
  }
  
  struct cgroup_subsys cpuset_cgrp_subsys = {
diff --cc kernel/sched/core.c
index 51f1c4f52cf1,d826bec1c522..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -5378,8 -7633,21 +5379,26 @@@ recheck
  			return retval;
  	}
  
++<<<<<<< HEAD
 +	if (pi)
 +		cpuset_read_lock();
++=======
+ 	/* Update task specific "requested" clamps */
+ 	if (attr->sched_flags & SCHED_FLAG_UTIL_CLAMP) {
+ 		retval = uclamp_validate(p, attr);
+ 		if (retval)
+ 			return retval;
+ 	}
+ 
+ 	/*
+ 	 * SCHED_DEADLINE bandwidth accounting relies on stable cpusets
+ 	 * information.
+ 	 */
+ 	if (dl_policy(policy) || dl_policy(p->policy)) {
+ 		cpuset_locked = true;
+ 		cpuset_lock();
+ 	}
++>>>>>>> 111cd11bbc54 (sched/cpuset: Bring back cpuset_mutex)
  
  	/*
  	 * Make sure no PI-waiters arrive (or leave) while we are
diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 495e93904dbe..2e6f820cd05c 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -70,8 +70,8 @@ extern void cpuset_init_smp(void);
 extern void cpuset_force_rebuild(void);
 extern void cpuset_update_active_cpus(void);
 extern void cpuset_wait_for_hotplug(void);
-extern void cpuset_read_lock(void);
-extern void cpuset_read_unlock(void);
+extern void cpuset_lock(void);
+extern void cpuset_unlock(void);
 extern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);
 extern void cpuset_cpus_allowed_fallback(struct task_struct *p);
 extern nodemask_t cpuset_mems_allowed(struct task_struct *p);
@@ -195,8 +195,8 @@ static inline void cpuset_update_active_cpus(void)
 
 static inline void cpuset_wait_for_hotplug(void) { }
 
-static inline void cpuset_read_lock(void) { }
-static inline void cpuset_read_unlock(void) { }
+static inline void cpuset_lock(void) { }
+static inline void cpuset_unlock(void) { }
 
 static inline void cpuset_cpus_allowed(struct task_struct *p,
 				       struct cpumask *mask)
* Unmerged path kernel/cgroup/cpuset.c
* Unmerged path kernel/sched/core.c
