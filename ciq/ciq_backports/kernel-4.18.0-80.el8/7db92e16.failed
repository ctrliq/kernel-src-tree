x86/kvm: Move l1tf setup function

jira LE-1907
cve CVE-2018-3646
cve CVE-2018-3620
Rebuild_History Non-Buildable kernel-4.18.0-80.el8
commit-author Thomas Gleixner <tglx@linutronix.de>
commit 7db92e165ac814487264632ab2624e832f20ae38
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-80.el8/7db92e16.failed

In preparation of allowing run time control for L1D flushing, move the
setup code to the module parameter handler.

In case of pre module init parsing, just store the value and let vmx_init()
do the actual setup after running kvm_init() so that enable_ept is having
the correct state.

During run-time invoke it directly from the parameter setter to prepare for
run-time control.

	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Tested-by: Jiri Kosina <jkosina@suse.cz>
	Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
Link: https://lkml.kernel.org/r/20180713142322.694063239@linutronix.de

(cherry picked from commit 7db92e165ac814487264632ab2624e832f20ae38)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kvm/vmx.c
diff --cc arch/x86/kvm/vmx.c
index d9e0ef7725c0,ce06701916ae..000000000000
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@@ -192,12 -191,101 +192,110 @@@ module_param(ple_window_max, uint, 0444
  
  extern const ulong vmx_return;
  
++<<<<<<< HEAD
 +enum ept_pointers_status {
 +	EPT_POINTERS_CHECK = 0,
 +	EPT_POINTERS_MATCH = 1,
 +	EPT_POINTERS_MISMATCH = 2
 +};
 +
++=======
+ static DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);
+ 
+ /* Storage for pre module init parameter parsing */
+ static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;
+ 
+ static const struct {
+ 	const char *option;
+ 	enum vmx_l1d_flush_state cmd;
+ } vmentry_l1d_param[] = {
+ 	{"auto",	VMENTER_L1D_FLUSH_AUTO},
+ 	{"never",	VMENTER_L1D_FLUSH_NEVER},
+ 	{"cond",	VMENTER_L1D_FLUSH_COND},
+ 	{"always",	VMENTER_L1D_FLUSH_ALWAYS},
+ };
+ 
+ #define L1D_CACHE_ORDER 4
+ static void *vmx_l1d_flush_pages;
+ 
+ static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
+ {
+ 	struct page *page;
+ 
+ 	/* If set to 'auto' select 'cond' */
+ 	if (l1tf == VMENTER_L1D_FLUSH_AUTO)
+ 		l1tf = VMENTER_L1D_FLUSH_COND;
+ 
+ 	if (!enable_ept) {
+ 		l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;
+ 		return 0;
+ 	}
+ 
+ 	if (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&
+ 	    !boot_cpu_has(X86_FEATURE_FLUSH_L1D)) {
+ 		page = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);
+ 		if (!page)
+ 			return -ENOMEM;
+ 		vmx_l1d_flush_pages = page_address(page);
+ 	}
+ 
+ 	l1tf_vmx_mitigation = l1tf;
+ 
+ 	if (l1tf != VMENTER_L1D_FLUSH_NEVER)
+ 		static_branch_enable(&vmx_l1d_should_flush);
+ 	return 0;
+ }
+ 
+ static int vmentry_l1d_flush_parse(const char *s)
+ {
+ 	unsigned int i;
+ 
+ 	if (s) {
+ 		for (i = 0; i < ARRAY_SIZE(vmentry_l1d_param); i++) {
+ 			if (!strcmp(s, vmentry_l1d_param[i].option))
+ 				return vmentry_l1d_param[i].cmd;
+ 		}
+ 	}
+ 	return -EINVAL;
+ }
+ 
+ static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
+ {
+ 	int l1tf;
+ 
+ 	if (!boot_cpu_has(X86_BUG_L1TF))
+ 		return 0;
+ 
+ 	l1tf = vmentry_l1d_flush_parse(s);
+ 	if (l1tf < 0)
+ 		return l1tf;
+ 
+ 	/*
+ 	 * Has vmx_init() run already? If not then this is the pre init
+ 	 * parameter parsing. In that case just store the value and let
+ 	 * vmx_init() do the proper setup after enable_ept has been
+ 	 * established.
+ 	 */
+ 	if (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {
+ 		vmentry_l1d_flush_param = l1tf;
+ 		return 0;
+ 	}
+ 
+ 	return vmx_setup_l1d_flush(l1tf);
+ }
+ 
+ static int vmentry_l1d_flush_get(char *s, const struct kernel_param *kp)
+ {
+ 	return sprintf(s, "%s\n", vmentry_l1d_param[l1tf_vmx_mitigation].option);
+ }
+ 
+ static const struct kernel_param_ops vmentry_l1d_flush_ops = {
+ 	.set = vmentry_l1d_flush_set,
+ 	.get = vmentry_l1d_flush_get,
+ };
+ module_param_cb(vmentry_l1d_flush, &vmentry_l1d_flush_ops, NULL, S_IRUGO);
+ 
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
  struct kvm_vmx {
  	struct kvm kvm;
  
@@@ -9950,6 -9635,65 +10048,68 @@@ static int vmx_handle_exit(struct kvm_v
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Software based L1D cache flush which is used when microcode providing
+  * the cache control MSR is not loaded.
+  *
+  * The L1D cache is 32 KiB on Nehalem and later microarchitectures, but to
+  * flush it is required to read in 64 KiB because the replacement algorithm
+  * is not exactly LRU. This could be sized at runtime via topology
+  * information but as all relevant affected CPUs have 32KiB L1D cache size
+  * there is no point in doing so.
+  */
+ #define L1D_CACHE_ORDER 4
+ static void *vmx_l1d_flush_pages;
+ 
+ static void vmx_l1d_flush(struct kvm_vcpu *vcpu)
+ {
+ 	int size = PAGE_SIZE << L1D_CACHE_ORDER;
+ 	bool always;
+ 
+ 	/*
+ 	 * This code is only executed when the the flush mode is 'cond' or
+ 	 * 'always'
+ 	 *
+ 	 * If 'flush always', keep the flush bit set, otherwise clear
+ 	 * it. The flush bit gets set again either from vcpu_run() or from
+ 	 * one of the unsafe VMEXIT handlers.
+ 	 */
+ 	always = l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_ALWAYS;
+ 	vcpu->arch.l1tf_flush_l1d = always;
+ 
+ 	vcpu->stat.l1d_flush++;
+ 
+ 	if (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {
+ 		wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);
+ 		return;
+ 	}
+ 
+ 	asm volatile(
+ 		/* First ensure the pages are in the TLB */
+ 		"xorl	%%eax, %%eax\n"
+ 		".Lpopulate_tlb:\n\t"
+ 		"movzbl	(%[empty_zp], %%" _ASM_AX "), %%ecx\n\t"
+ 		"addl	$4096, %%eax\n\t"
+ 		"cmpl	%%eax, %[size]\n\t"
+ 		"jne	.Lpopulate_tlb\n\t"
+ 		"xorl	%%eax, %%eax\n\t"
+ 		"cpuid\n\t"
+ 		/* Now fill the cache */
+ 		"xorl	%%eax, %%eax\n"
+ 		".Lfill_cache:\n"
+ 		"movzbl	(%[empty_zp], %%" _ASM_AX "), %%ecx\n\t"
+ 		"addl	$64, %%eax\n\t"
+ 		"cmpl	%%eax, %[size]\n\t"
+ 		"jne	.Lfill_cache\n\t"
+ 		"lfence\n"
+ 		:: [empty_zp] "r" (vmx_l1d_flush_pages),
+ 		    [size] "r" (size)
+ 		: "eax", "ebx", "ecx", "edx");
+ }
+ 
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
  static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
  {
  	struct vmcs12 *vmcs12 = get_vmcs12(vcpu);
@@@ -13877,6 -13250,51 +14037,54 @@@ static struct kvm_x86_ops vmx_x86_ops _
  	.enable_smi_window = enable_smi_window,
  };
  
++<<<<<<< HEAD
++=======
+ static void vmx_cleanup_l1d_flush(void)
+ {
+ 	if (vmx_l1d_flush_pages) {
+ 		free_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);
+ 		vmx_l1d_flush_pages = NULL;
+ 	}
+ 	/* Restore state so sysfs ignores VMX */
+ 	l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
+ }
+ 
+ static void vmx_exit(void)
+ {
+ #ifdef CONFIG_KEXEC_CORE
+ 	RCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);
+ 	synchronize_rcu();
+ #endif
+ 
+ 	kvm_exit();
+ 
+ #if IS_ENABLED(CONFIG_HYPERV)
+ 	if (static_branch_unlikely(&enable_evmcs)) {
+ 		int cpu;
+ 		struct hv_vp_assist_page *vp_ap;
+ 		/*
+ 		 * Reset everything to support using non-enlightened VMCS
+ 		 * access later (e.g. when we reload the module with
+ 		 * enlightened_vmcs=0)
+ 		 */
+ 		for_each_online_cpu(cpu) {
+ 			vp_ap =	hv_get_vp_assist_page(cpu);
+ 
+ 			if (!vp_ap)
+ 				continue;
+ 
+ 			vp_ap->current_nested_vmcs = 0;
+ 			vp_ap->enlighten_vmentry = 0;
+ 		}
+ 
+ 		static_branch_disable(&enable_evmcs);
+ 	}
+ #endif
+ 	vmx_cleanup_l1d_flush();
+ }
+ module_exit(vmx_exit);
+ 
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
  static int __init vmx_init(void)
  {
  	int r;
@@@ -13915,6 -13333,21 +14123,24 @@@
  	if (r)
  		return r;
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Must be called after kvm_init() so enable_ept is properly set
+ 	 * up. Hand the parameter mitigation value in which was stored in
+ 	 * the pre module init parser. If no parameter was given, it will
+ 	 * contain 'auto' which will be turned into the default 'cond'
+ 	 * mitigation mode.
+ 	 */
+ 	if (boot_cpu_has(X86_BUG_L1TF)) {
+ 		r = vmx_setup_l1d_flush(vmentry_l1d_flush_param);
+ 		if (r) {
+ 			vmx_exit();
+ 			return r;
+ 		}
+ 	}
+ 
++>>>>>>> 7db92e165ac8 (x86/kvm: Move l1tf setup function)
  #ifdef CONFIG_KEXEC_CORE
  	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
  			   crash_vmclear_local_loaded_vmcss);
* Unmerged path arch/x86/kvm/vmx.c
