mm/memcg: protect per-CPU counter by disabling preemption on PREEMPT_RT where needed.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Sebastian Andrzej Siewior <bigeasy@linutronix.de>
commit be3e67b54b437123e6144da31cf312ddcaa5aef2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/be3e67b5.failed

The per-CPU counter are modified with the non-atomic modifier.  The
consistency is ensured by disabling interrupts for the update.  On non
PREEMPT_RT configuration this works because acquiring a spinlock_t typed
lock with the _irq() suffix disables interrupts.  On PREEMPT_RT
configurations the RMW operation can be interrupted.

Another problem is that mem_cgroup_swapout() expects to be invoked with
disabled interrupts because the caller has to acquire a spinlock_t which
is acquired with disabled interrupts.  Since spinlock_t never disables
interrupts on PREEMPT_RT the interrupts are never disabled at this
point.

The code is never called from in_irq() context on PREEMPT_RT therefore
disabling preemption during the update is sufficient on PREEMPT_RT.  The
sections which explicitly disable interrupts can remain on PREEMPT_RT
because the sections remain short and they don't involve sleeping locks
(memcg_check_events() is doing nothing on PREEMPT_RT).

Disable preemption during update of the per-CPU variables which do not
explicitly disable interrupts.

Link: https://lkml.kernel.org/r/20220226204144.1008339-4-bigeasy@linutronix.de
	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Acked-by: Roman Gushchin <guro@fb.com>
	Reviewed-by: Shakeel Butt <shakeelb@google.com
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: kernel test robot <oliver.sang@intel.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Michal Koutn√Ω <mkoutny@suse.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
	Cc: Waiman Long <longman@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit be3e67b54b437123e6144da31cf312ddcaa5aef2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index 413502fb22c9,bc60694b4e45..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -640,27 -609,90 +640,66 @@@ mem_cgroup_largest_soft_limit_node(stru
  }
  
  /*
 - * memcg and lruvec stats flushing
 - *
 - * Many codepaths leading to stats update or read are performance sensitive and
 - * adding stats flushing in such codepaths is not desirable. So, to optimize the
 - * flushing the kernel does:
 - *
 - * 1) Periodically and asynchronously flush the stats every 2 seconds to not let
 - *    rstat update tree grow unbounded.
 + * Return the active percpu stats memcg and optionally mem_cgroup_per_node.
   *
 - * 2) Flush the stats synchronously on reader side only when there are more than
 - *    (MEMCG_CHARGE_BATCH * nr_cpus) update events. Though this optimization
 - *    will let stats be out of sync by atmost (MEMCG_CHARGE_BATCH * nr_cpus) but
 - *    only for 2 seconds due to (1).
 + * When percpu_stats_disabled, the percpu stats update is transferred to
 + * its parent.
   */
++<<<<<<< HEAD
 +static inline struct mem_cgroup *
 +percpu_stats_memcg(struct mem_cgroup *memcg, struct mem_cgroup_per_node **pn)
++=======
+ static void flush_memcg_stats_dwork(struct work_struct *w);
+ static DECLARE_DEFERRABLE_WORK(stats_flush_dwork, flush_memcg_stats_dwork);
+ static DEFINE_SPINLOCK(stats_flush_lock);
+ static DEFINE_PER_CPU(unsigned int, stats_updates);
+ static atomic_t stats_flush_threshold = ATOMIC_INIT(0);
+ 
+ /*
+  * Accessors to ensure that preemption is disabled on PREEMPT_RT because it can
+  * not rely on this as part of an acquired spinlock_t lock. These functions are
+  * never used in hardirq context on PREEMPT_RT and therefore disabling preemtion
+  * is sufficient.
+  */
+ static void memcg_stats_lock(void)
+ {
+ #ifdef CONFIG_PREEMPT_RT
+       preempt_disable();
+ #else
+       VM_BUG_ON(!irqs_disabled());
+ #endif
+ }
+ 
+ static void __memcg_stats_lock(void)
+ {
+ #ifdef CONFIG_PREEMPT_RT
+       preempt_disable();
+ #endif
+ }
+ 
+ static void memcg_stats_unlock(void)
+ {
+ #ifdef CONFIG_PREEMPT_RT
+       preempt_enable();
+ #endif
+ }
+ 
+ static inline void memcg_rstat_updated(struct mem_cgroup *memcg, int val)
++>>>>>>> be3e67b54b43 (mm/memcg: protect per-CPU counter by disabling preemption on PREEMPT_RT where needed.)
  {
 -	unsigned int x;
 -
 -	cgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());
 -
 -	x = __this_cpu_add_return(stats_updates, abs(val));
 -	if (x > MEMCG_CHARGE_BATCH) {
 -		atomic_add(x / MEMCG_CHARGE_BATCH, &stats_flush_threshold);
 -		__this_cpu_write(stats_updates, 0);
 -	}
 -}
 -
 -static void __mem_cgroup_flush_stats(void)
 -{
 -	unsigned long flag;
 -
 -	if (!spin_trylock_irqsave(&stats_flush_lock, flag))
 -		return;
 +	if (likely(!memcg->percpu_stats_disabled))
 +		return memcg;
  
 -	cgroup_rstat_flush_irqsafe(root_mem_cgroup->css.cgroup);
 -	atomic_set(&stats_flush_threshold, 0);
 -	spin_unlock_irqrestore(&stats_flush_lock, flag);
 -}
 +	do {
 +		memcg = parent_mem_cgroup(memcg);
 +	} while (memcg->percpu_stats_disabled);
  
 -void mem_cgroup_flush_stats(void)
 -{
 -	if (atomic_read(&stats_flush_threshold) > num_online_cpus())
 -		__mem_cgroup_flush_stats();
 -}
 +	if (pn) {
 +		unsigned int nid = (*pn)->nid;
  
 -static void flush_memcg_stats_dwork(struct work_struct *w)
 -{
 -	__mem_cgroup_flush_stats();
 -	queue_delayed_work(system_unbound_wq, &stats_flush_dwork, 2UL*HZ);
 +		*pn = memcg->nodeinfo[nid];
 +	}
 +	return memcg;
  }
  
  /**
@@@ -723,32 -730,39 +762,65 @@@ void __mod_memcg_lruvec_state(struct lr
  {
  	struct mem_cgroup_per_node *pn;
  	struct mem_cgroup *memcg;
 +	long x, threshold = MEMCG_CHARGE_BATCH;
  
  	pn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);
++<<<<<<< HEAD
++=======
+ 	memcg = pn->memcg;
+ 
+ 	/*
+ 	 * The caller from rmap relay on disabled preemption becase they never
+ 	 * update their counter from in-interrupt context. For these two
+ 	 * counters we check that the update is never performed from an
+ 	 * interrupt context while other caller need to have disabled interrupt.
+ 	 */
+ 	__memcg_stats_lock();
+ 	if (IS_ENABLED(CONFIG_DEBUG_VM) && !IS_ENABLED(CONFIG_PREEMPT_RT)) {
+ 		switch (idx) {
+ 		case NR_ANON_MAPPED:
+ 		case NR_FILE_MAPPED:
+ 		case NR_ANON_THPS:
+ 		case NR_SHMEM_PMDMAPPED:
+ 		case NR_FILE_PMDMAPPED:
+ 			WARN_ON_ONCE(!in_task());
+ 			break;
+ 		default:
+ 			WARN_ON_ONCE(!irqs_disabled());
+ 		}
+ 	}
+ 
+ 	/* Update memcg */
+ 	__this_cpu_add(memcg->vmstats_percpu->state[idx], val);
++>>>>>>> be3e67b54b43 (mm/memcg: protect per-CPU counter by disabling preemption on PREEMPT_RT where needed.)
  
  	/* Update lruvec */
 -	__this_cpu_add(pn->lruvec_stats_percpu->state[idx], val);
 +	if (!pn->memcg->percpu_stats_disabled)
 +		__this_cpu_add(pn->lruvec_stat_local->count[idx], val);
 +
++<<<<<<< HEAD
 +	memcg = percpu_stats_memcg(pn->memcg, &pn);
  
 +	/* Update memcg */
 +	__mod_memcg_state(memcg, idx, val);
 +
 +	if (vmstat_item_in_bytes(idx))
 +		threshold <<= PAGE_SHIFT;
 +
 +	x = val + __this_cpu_read(pn->lruvec_stat_cpu->count[idx]);
 +	if (unlikely(abs(x) > threshold)) {
 +		pg_data_t *pgdat = lruvec_pgdat(lruvec);
 +		struct mem_cgroup_per_node *pi;
 +
 +		for (pi = pn; pi; pi = parent_nodeinfo(pi, pgdat->node_id))
 +			atomic_long_add(x, &pi->lruvec_stat[idx]);
 +		x = 0;
 +	}
 +	__this_cpu_write(pn->lruvec_stat_cpu->count[idx], x);
++=======
+ 	memcg_rstat_updated(memcg, val);
+ 	memcg_stats_unlock();
++>>>>>>> be3e67b54b43 (mm/memcg: protect per-CPU counter by disabling preemption on PREEMPT_RT where needed.)
  }
  
  /**
@@@ -830,10 -844,11 +902,16 @@@ void __count_memcg_events(struct mem_cg
  {
  	if (mem_cgroup_disabled())
  		return;
 +	memcg = percpu_stats_memcg(memcg, NULL);
  
+ 	memcg_stats_lock();
  	__this_cpu_add(memcg->vmstats_percpu->events[idx], count);
++<<<<<<< HEAD
 +	cgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());
++=======
+ 	memcg_rstat_updated(memcg, count);
+ 	memcg_stats_unlock();
++>>>>>>> be3e67b54b43 (mm/memcg: protect per-CPU counter by disabling preemption on PREEMPT_RT where needed.)
  }
  
  static unsigned long memcg_events(struct mem_cgroup *memcg, int event)
@@@ -7382,9 -7207,10 +7460,14 @@@ void mem_cgroup_swapout(struct page *pa
  	 * important here to have the interrupts disabled because it is the
  	 * only synchronisation we have for updating the per-CPU variables.
  	 */
- 	VM_BUG_ON(!irqs_disabled());
+ 	memcg_stats_lock();
  	mem_cgroup_charge_statistics(memcg, -nr_entries);
++<<<<<<< HEAD
 +	memcg_check_events(memcg, page);
++=======
+ 	memcg_stats_unlock();
+ 	memcg_check_events(memcg, page_to_nid(page));
++>>>>>>> be3e67b54b43 (mm/memcg: protect per-CPU counter by disabling preemption on PREEMPT_RT where needed.)
  
  	css_put(&memcg->css);
  }
* Unmerged path mm/memcontrol.c
