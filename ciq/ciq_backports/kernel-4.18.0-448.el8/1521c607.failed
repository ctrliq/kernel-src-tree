swiotlb: don't panic when the swiotlb buffer can't be allocated

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 1521c607cabe7c7edb028e211e88ba1e0f19714e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/1521c607.failed

For historical reasons the switlb code paniced when the metadata could
not be allocated, but just printed a warning when the actual main
swiotlb buffer could not be allocated.  Restore this somewhat unexpected
behavior as changing it caused a boot failure on the Microchip RISC-V
PolarFire SoC Icicle kit.

Fixes: 6424e31b1c05 ("swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl")
	Reported-by: Conor Dooley <Conor.Dooley@microchip.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>
	Acked-by: Conor Dooley <conor.dooley@microchip.com>
	Tested-by: Conor Dooley <Conor.Dooley@microchip.com>
(cherry picked from commit 1521c607cabe7c7edb028e211e88ba1e0f19714e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index eb8c08292910,3e992a308c8a..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -228,19 -225,50 +228,47 @@@ static void swiotlb_init_io_tlb_mem(str
  	return;
  }
  
 -/*
 - * Statically reserve bounce buffer space and initialize bounce buffer data
 - * structures for the software IO TLB used to implement the DMA API.
 - */
 -void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 -		int (*remap)(void *tlb, unsigned long nslabs))
 +int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  {
  	struct io_tlb_mem *mem = &io_tlb_default_mem;
 -	unsigned long nslabs = default_nslabs;
 -	size_t alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
 -	size_t bytes;
 -	void *tlb;
 +	size_t alloc_size;
  
 -	if (!addressing_limit && !swiotlb_force_bounce)
 -		return;
 -	if (swiotlb_force_disable)
 -		return;
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
 +		return 0;
  
++<<<<<<< HEAD
 +	/* protect against double initialization */
 +	if (WARN_ON_ONCE(mem->nslabs))
 +		return -ENOMEM;
++=======
+ 	/*
+ 	 * By default allocate the bounce buffer memory from low memory, but
+ 	 * allow to pick a location everywhere for hypervisors with guest
+ 	 * memory encryption.
+ 	 */
+ retry:
+ 	bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
+ 	if (flags & SWIOTLB_ANY)
+ 		tlb = memblock_alloc(bytes, PAGE_SIZE);
+ 	else
+ 		tlb = memblock_alloc_low(bytes, PAGE_SIZE);
+ 	if (!tlb) {
+ 		pr_warn("%s: failed to allocate tlb structure\n", __func__);
+ 		return;
+ 	}
+ 
+ 	if (remap && remap(tlb, nslabs) < 0) {
+ 		memblock_free(tlb, PAGE_ALIGN(bytes));
+ 
+ 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
+ 		if (nslabs < IO_TLB_MIN_SLABS)
+ 			panic("%s: Failed to remap %zu bytes\n",
+ 			      __func__, bytes);
+ 		goto retry;
+ 	}
++>>>>>>> 1521c607cabe (swiotlb: don't panic when the swiotlb buffer can't be allocated)
  
 +	alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
  	mem->slots = memblock_alloc(alloc_size, PAGE_SIZE);
  	if (!mem->slots)
  		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
* Unmerged path kernel/dma/swiotlb.c
