vfio: remove VFIO_GROUP_NOTIFY_SET_KVM

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
Rebuild_CHGLOG: - Revert "vfio: remove VFIO_GROUP_NOTIFY_SET_KVM" (Jocelyn Falempe) [2115880]
Rebuild_FUZZ: 89.41%
commit-author Matthew Rosato <mjrosato@linux.ibm.com>
commit 421cfe6596f6cb316991c02bf30a93bd81092853
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/421cfe65.failed

Rather than relying on a notifier for associating the KVM with
the group, let's assume that the association has already been
made prior to device_open.  The first time a device is opened
associate the group KVM with the device.

This fixes a user-triggerable oops in GVT.

	Reviewed-by: Tony Krowiak <akrowiak@linux.ibm.com>
	Reviewed-by: Kevin Tian <kevin.tian@intel.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
	Signed-off-by: Matthew Rosato <mjrosato@linux.ibm.com>
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
	Acked-by: Zhi Wang <zhi.a.wang@intel.com>
Link: https://lore.kernel.org/r/20220519183311.582380-2-mjrosato@linux.ibm.com
	Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
(cherry picked from commit 421cfe6596f6cb316991c02bf30a93bd81092853)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/gvt/gtt.c
#	drivers/gpu/drm/i915/gvt/gvt.h
#	drivers/gpu/drm/i915/gvt/kvmgt.c
#	drivers/s390/crypto/vfio_ap_ops.c
#	drivers/vfio/vfio.c
#	include/linux/vfio.h
diff --cc drivers/gpu/drm/i915/gvt/gtt.c
index cc2c05e18206,b4f69364f9a1..000000000000
--- a/drivers/gpu/drm/i915/gvt/gtt.c
+++ b/drivers/gpu/drm/i915/gvt/gtt.c
@@@ -47,6 -49,22 +47,25 @@@
  static bool enable_out_of_sync = false;
  static int preallocated_oos_pages = 8192;
  
++<<<<<<< HEAD
++=======
+ static bool intel_gvt_is_valid_gfn(struct intel_vgpu *vgpu, unsigned long gfn)
+ {
+ 	struct kvm *kvm = vgpu->vfio_device.kvm;
+ 	int idx;
+ 	bool ret;
+ 
+ 	if (!vgpu->attached)
+ 		return false;
+ 
+ 	idx = srcu_read_lock(&kvm->srcu);
+ 	ret = kvm_is_visible_gfn(kvm, gfn);
+ 	srcu_read_unlock(&kvm->srcu, idx);
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  /*
   * validate a gm address and related range size,
   * translate it to host gm address
@@@ -1165,10 -1183,11 +1184,17 @@@ static int is_2MB_gtt_possible(struct i
  	if (!HAS_PAGE_SIZES(vgpu->gvt->gt->i915, I915_GTT_PAGE_SIZE_2M))
  		return 0;
  
++<<<<<<< HEAD
 +	pfn = intel_gvt_hypervisor_gfn_to_mfn(vgpu, ops->get_pfn(entry));
 +	if (pfn == INTEL_GVT_INVALID_ADDR)
++=======
+ 	if (!vgpu->attached)
+ 		return -EINVAL;
+ 	pfn = gfn_to_pfn(vgpu->vfio_device.kvm, ops->get_pfn(entry));
+ 	if (is_error_noslot_pfn(pfn))
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  		return -EINVAL;
 +
  	return PageTransHuge(pfn_to_page(pfn));
  }
  
diff --cc drivers/gpu/drm/i915/gvt/gvt.h
index 0c0615602343,aee1a45da74b..000000000000
--- a/drivers/gpu/drm/i915/gvt/gvt.h
+++ b/drivers/gpu/drm/i915/gvt/gvt.h
@@@ -218,13 -210,31 +218,39 @@@ struct intel_vgpu 
  	struct intel_vgpu_vblank_timer vblank_timer;
  
  	u32 scan_nonprivbb;
++<<<<<<< HEAD
++=======
+ 
+ 	struct vfio_device vfio_device;
+ 	struct vfio_region *region;
+ 	int num_regions;
+ 	struct eventfd_ctx *intx_trigger;
+ 	struct eventfd_ctx *msi_trigger;
+ 
+ 	/*
+ 	 * Two caches are used to avoid mapping duplicated pages (eg.
+ 	 * scratch pages). This help to reduce dma setup overhead.
+ 	 */
+ 	struct rb_root gfn_cache;
+ 	struct rb_root dma_addr_cache;
+ 	unsigned long nr_cache_entries;
+ 	struct mutex cache_lock;
+ 
+ 	struct notifier_block iommu_notifier;
+ 	atomic_t released;
+ 
+ 	struct kvm_page_track_notifier_node track_node;
+ #define NR_BKT (1 << 18)
+ 	struct hlist_head ptable[NR_BKT];
+ #undef NR_BKT
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  };
  
 +static inline void *intel_vgpu_vdev(struct intel_vgpu *vgpu)
 +{
 +	return vgpu->vdev;
 +}
 +
  /* validating GM healthy status*/
  #define vgpu_is_vm_unhealthy(ret_val) \
  	(((ret_val) == -EBADRQC) || ((ret_val) == -EFAULT))
diff --cc drivers/gpu/drm/i915/gvt/kvmgt.c
index 5bd0c74fc4ad,e2f6c56ab342..000000000000
--- a/drivers/gpu/drm/i915/gvt/kvmgt.c
+++ b/drivers/gpu/drm/i915/gvt/kvmgt.c
@@@ -252,10 -228,6 +252,13 @@@ static void intel_gvt_cleanup_vgpu_type
  	}
  }
  
++<<<<<<< HEAD
 +static int kvmgt_guest_init(struct mdev_device *mdev);
 +static void intel_vgpu_release_work(struct work_struct *work);
 +static bool kvmgt_guest_exit(struct kvmgt_guest_info *info);
 +
++=======
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  static void gvt_unpin_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,
  		unsigned long size)
  {
@@@ -883,89 -759,82 +886,151 @@@ static int intel_vgpu_iommu_notifier(st
  	return NOTIFY_OK;
  }
  
++<<<<<<< HEAD
 +static int intel_vgpu_group_notifier(struct notifier_block *nb,
 +				     unsigned long action, void *data)
 +{
 +	struct kvmgt_vdev *vdev = container_of(nb,
 +					       struct kvmgt_vdev,
 +					       group_notifier);
 +
 +	/* the only action we care about */
 +	if (action == VFIO_GROUP_NOTIFY_SET_KVM) {
 +		vdev->kvm = data;
 +
 +		if (!data)
 +			schedule_work(&vdev->release_work);
 +	}
 +
 +	return NOTIFY_OK;
 +}
 +
 +static int intel_vgpu_open(struct mdev_device *mdev)
 +{
 +	struct intel_vgpu *vgpu = mdev_get_drvdata(mdev);
 +	struct kvmgt_vdev *vdev = kvmgt_vdev(vgpu);
++=======
+ static bool __kvmgt_vgpu_exist(struct intel_vgpu *vgpu)
+ {
+ 	struct intel_vgpu *itr;
+ 	int id;
+ 	bool ret = false;
+ 
+ 	mutex_lock(&vgpu->gvt->lock);
+ 	for_each_active_vgpu(vgpu->gvt, itr, id) {
+ 		if (!itr->attached)
+ 			continue;
+ 
+ 		if (vgpu->vfio_device.kvm == itr->vfio_device.kvm) {
+ 			ret = true;
+ 			goto out;
+ 		}
+ 	}
+ out:
+ 	mutex_unlock(&vgpu->gvt->lock);
+ 	return ret;
+ }
+ 
+ static int intel_vgpu_open_device(struct vfio_device *vfio_dev)
+ {
+ 	struct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	unsigned long events;
  	int ret;
 +	struct vfio_group *vfio_group;
  
++<<<<<<< HEAD
 +	vdev->iommu_notifier.notifier_call = intel_vgpu_iommu_notifier;
 +	vdev->group_notifier.notifier_call = intel_vgpu_group_notifier;
++=======
+ 	vgpu->iommu_notifier.notifier_call = intel_vgpu_iommu_notifier;
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
  	events = VFIO_IOMMU_NOTIFY_DMA_UNMAP;
 -	ret = vfio_register_notifier(vfio_dev, VFIO_IOMMU_NOTIFY, &events,
 -				     &vgpu->iommu_notifier);
 +	ret = vfio_register_notifier(mdev_dev(mdev), VFIO_IOMMU_NOTIFY, &events,
 +				&vdev->iommu_notifier);
  	if (ret != 0) {
  		gvt_vgpu_err("vfio_register_notifier for iommu failed: %d\n",
  			ret);
  		goto out;
  	}
  
++<<<<<<< HEAD
 +	events = VFIO_GROUP_NOTIFY_SET_KVM;
 +	ret = vfio_register_notifier(mdev_dev(mdev), VFIO_GROUP_NOTIFY, &events,
 +				&vdev->group_notifier);
 +	if (ret != 0) {
 +		gvt_vgpu_err("vfio_register_notifier for group failed: %d\n",
 +			ret);
 +		goto undo_iommu;
 +	}
 +
 +	vfio_group = vfio_group_get_external_user_from_dev(mdev_dev(mdev));
 +	if (IS_ERR_OR_NULL(vfio_group)) {
 +		ret = !vfio_group ? -EFAULT : PTR_ERR(vfio_group);
 +		gvt_vgpu_err("vfio_group_get_external_user_from_dev failed\n");
 +		goto undo_register;
 +	}
 +	vdev->vfio_group = vfio_group;
 +
 +	/* Take a module reference as mdev core doesn't take
 +	 * a reference for vendor driver.
 +	 */
 +	if (!try_module_get(THIS_MODULE)) {
 +		ret = -ENODEV;
 +		goto undo_group;
 +	}
 +
 +	ret = kvmgt_guest_init(mdev);
 +	if (ret)
 +		goto undo_group;
++=======
+ 	ret = -EEXIST;
+ 	if (vgpu->attached)
+ 		goto undo_iommu;
+ 
+ 	ret = -ESRCH;
+ 	if (!vgpu->vfio_device.kvm ||
+ 	    vgpu->vfio_device.kvm->mm != current->mm) {
+ 		gvt_vgpu_err("KVM is required to use Intel vGPU\n");
+ 		goto undo_iommu;
+ 	}
+ 
+ 	kvm_get_kvm(vgpu->vfio_device.kvm);
+ 
+ 	ret = -EEXIST;
+ 	if (__kvmgt_vgpu_exist(vgpu))
+ 		goto undo_iommu;
+ 
+ 	vgpu->attached = true;
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
 -	kvmgt_protect_table_init(vgpu);
 -	gvt_cache_init(vgpu);
 +	intel_gvt_ops->vgpu_activate(vgpu);
  
++<<<<<<< HEAD
 +	atomic_set(&vdev->released, 0);
 +	return ret;
++=======
+ 	vgpu->track_node.track_write = kvmgt_page_track_write;
+ 	vgpu->track_node.track_flush_slot = kvmgt_page_track_flush_slot;
+ 	kvm_page_track_register_notifier(vgpu->vfio_device.kvm,
+ 					 &vgpu->track_node);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
 -	debugfs_create_ulong(KVMGT_DEBUGFS_FILENAME, 0444, vgpu->debugfs,
 -			     &vgpu->nr_cache_entries);
 -
 -	intel_gvt_activate_vgpu(vgpu);
 +undo_group:
 +	vfio_group_put_external_user(vdev->vfio_group);
 +	vdev->vfio_group = NULL;
  
 -	atomic_set(&vgpu->released, 0);
 -	return 0;
++<<<<<<< HEAD
 +undo_register:
 +	vfio_unregister_notifier(mdev_dev(mdev), VFIO_GROUP_NOTIFY,
 +					&vdev->group_notifier);
  
++=======
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  undo_iommu:
 -	vfio_unregister_notifier(vfio_dev, VFIO_IOMMU_NOTIFY,
 -				 &vgpu->iommu_notifier);
 +	vfio_unregister_notifier(mdev_dev(mdev), VFIO_IOMMU_NOTIFY,
 +					&vdev->iommu_notifier);
  out:
  	return ret;
  }
@@@ -982,57 -850,38 +1047,77 @@@ static void intel_vgpu_release_msi_even
  	}
  }
  
- static void __intel_vgpu_release(struct intel_vgpu *vgpu)
+ static void intel_vgpu_close_device(struct vfio_device *vfio_dev)
  {
++<<<<<<< HEAD
 +	struct kvmgt_vdev *vdev = kvmgt_vdev(vgpu);
++=======
+ 	struct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	struct drm_i915_private *i915 = vgpu->gvt->gt->i915;
 +	struct kvmgt_guest_info *info;
  	int ret;
  
 -	if (!vgpu->attached)
 +	if (!handle_valid(vgpu->handle))
  		return;
  
 -	if (atomic_cmpxchg(&vgpu->released, 0, 1))
 +	if (atomic_cmpxchg(&vdev->released, 0, 1))
  		return;
  
 -	intel_gvt_release_vgpu(vgpu);
 +	intel_gvt_ops->vgpu_release(vgpu);
  
 -	ret = vfio_unregister_notifier(&vgpu->vfio_device, VFIO_IOMMU_NOTIFY,
 -				       &vgpu->iommu_notifier);
 +	ret = vfio_unregister_notifier(mdev_dev(vdev->mdev), VFIO_IOMMU_NOTIFY,
 +					&vdev->iommu_notifier);
  	drm_WARN(&i915->drm, ret,
  		 "vfio_unregister_notifier for iommu failed: %d\n", ret);
  
++<<<<<<< HEAD
 +	ret = vfio_unregister_notifier(mdev_dev(vdev->mdev), VFIO_GROUP_NOTIFY,
 +					&vdev->group_notifier);
 +	drm_WARN(&i915->drm, ret,
 +		 "vfio_unregister_notifier for group failed: %d\n", ret);
 +
 +	/* dereference module reference taken at open */
 +	module_put(THIS_MODULE);
 +
 +	info = (struct kvmgt_guest_info *)vgpu->handle;
 +	kvmgt_guest_exit(info);
++=======
+ 	debugfs_remove(debugfs_lookup(KVMGT_DEBUGFS_FILENAME, vgpu->debugfs));
+ 
+ 	kvm_page_track_unregister_notifier(vgpu->vfio_device.kvm,
+ 					   &vgpu->track_node);
+ 	kvmgt_protect_table_destroy(vgpu);
+ 	gvt_cache_destroy(vgpu);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
  	intel_vgpu_release_msi_eventfd_ctx(vgpu);
 +	vfio_group_put_external_user(vdev->vfio_group);
 +
++<<<<<<< HEAD
 +	vdev->kvm = NULL;
 +	vgpu->handle = 0;
 +}
 +
 +static void intel_vgpu_release(struct mdev_device *mdev)
 +{
 +	struct intel_vgpu *vgpu = mdev_get_drvdata(mdev);
 +
 +	__intel_vgpu_release(vgpu);
 +}
  
 +static void intel_vgpu_release_work(struct work_struct *work)
 +{
 +	struct kvmgt_vdev *vdev = container_of(work, struct kvmgt_vdev,
 +					       release_work);
 +
 +	__intel_vgpu_release(vdev->vgpu);
++=======
+ 	vgpu->attached = false;
+ 
+ 	if (vgpu->vfio_device.kvm)
+ 		kvm_put_kvm(vgpu->vfio_device.kvm);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  }
  
  static u64 intel_vgpu_get_bar_addr(struct intel_vgpu *vgpu, int bar)
@@@ -1764,48 -1603,71 +1849,72 @@@ static const struct attribute_group *in
  	NULL,
  };
  
 -static const struct vfio_device_ops intel_vgpu_dev_ops = {
 -	.open_device	= intel_vgpu_open_device,
 -	.close_device	= intel_vgpu_close_device,
 -	.read		= intel_vgpu_read,
 -	.write		= intel_vgpu_write,
 -	.mmap		= intel_vgpu_mmap,
 -	.ioctl		= intel_vgpu_ioctl,
 +static struct mdev_parent_ops intel_vgpu_ops = {
 +	.mdev_attr_groups       = intel_vgpu_groups,
 +	.create			= intel_vgpu_create,
 +	.remove			= intel_vgpu_remove,
 +
 +	.open			= intel_vgpu_open,
 +	.release		= intel_vgpu_release,
 +
 +	.read			= intel_vgpu_read,
 +	.write			= intel_vgpu_write,
 +	.mmap			= intel_vgpu_mmap,
 +	.ioctl			= intel_vgpu_ioctl,
  };
  
 -static int intel_vgpu_probe(struct mdev_device *mdev)
 +static int kvmgt_host_init(struct device *dev, void *gvt, const void *ops)
  {
 -	struct device *pdev = mdev_parent_dev(mdev);
 -	struct intel_gvt *gvt = kdev_to_i915(pdev)->gvt;
 -	struct intel_vgpu_type *type;
 -	struct intel_vgpu *vgpu;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = intel_gvt_init_vgpu_type_groups((struct intel_gvt *)gvt);
 +	if (ret)
++=======
+ 	type = &gvt->types[mdev_get_type_group_id(mdev)];
+ 	if (!type)
+ 		return -EINVAL;
+ 
+ 	vgpu = intel_gvt_create_vgpu(gvt, type);
+ 	if (IS_ERR(vgpu)) {
+ 		gvt_err("failed to create intel vgpu: %ld\n", PTR_ERR(vgpu));
+ 		return PTR_ERR(vgpu);
+ 	}
+ 
+ 	vfio_init_group_dev(&vgpu->vfio_device, &mdev->dev,
+ 			    &intel_vgpu_dev_ops);
+ 
+ 	dev_set_drvdata(&mdev->dev, vgpu);
+ 	ret = vfio_register_emulated_iommu_dev(&vgpu->vfio_device);
+ 	if (ret) {
+ 		intel_gvt_destroy_vgpu(vgpu);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  		return ret;
 -	}
  
 -	gvt_dbg_core("intel_vgpu_create succeeded for mdev: %s\n",
 -		     dev_name(mdev_dev(mdev)));
 -	return 0;
 +	intel_gvt_ops = ops;
 +	intel_vgpu_ops.supported_type_groups = gvt_vgpu_type_groups;
 +
 +	ret = mdev_register_device(dev, &intel_vgpu_ops);
 +	if (ret)
 +		intel_gvt_cleanup_vgpu_type_groups((struct intel_gvt *)gvt);
 +
 +	return ret;
  }
  
 -static void intel_vgpu_remove(struct mdev_device *mdev)
 +static void kvmgt_host_exit(struct device *dev, void *gvt)
  {
 -	struct intel_vgpu *vgpu = dev_get_drvdata(&mdev->dev);
 -
 -	if (WARN_ON_ONCE(vgpu->attached))
 -		return;
 -	intel_gvt_destroy_vgpu(vgpu);
 -}
 -
 -static struct mdev_driver intel_vgpu_mdev_driver = {
 -	.driver = {
 -		.name		= "intel_vgpu_mdev",
 -		.owner		= THIS_MODULE,
 -		.dev_groups	= intel_vgpu_groups,
 -	},
 -	.probe		= intel_vgpu_probe,
 -	.remove		= intel_vgpu_remove,
 -	.supported_type_groups	= gvt_vgpu_type_groups,
 -};
 +	mdev_unregister_device(dev);
 +	intel_gvt_cleanup_vgpu_type_groups((struct intel_gvt *)gvt);
 +}
  
 -int intel_gvt_page_track_add(struct intel_vgpu *info, u64 gfn)
 +static int kvmgt_page_track_add(unsigned long handle, u64 gfn)
  {
++<<<<<<< HEAD
 +	struct kvmgt_guest_info *info;
 +	struct kvm *kvm;
++=======
+ 	struct kvm *kvm = info->vfio_device.kvm;
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	struct kvm_memory_slot *slot;
  	int idx;
  
@@@ -1836,10 -1695,9 +1945,14 @@@ out
  	return 0;
  }
  
 -int intel_gvt_page_track_remove(struct intel_vgpu *info, u64 gfn)
 +static int kvmgt_page_track_remove(unsigned long handle, u64 gfn)
  {
++<<<<<<< HEAD
 +	struct kvmgt_guest_info *info;
 +	struct kvm *kvm;
++=======
+ 	struct kvm *kvm = info->vfio_device.kvm;
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	struct kvm_memory_slot *slot;
  	int idx;
  
diff --cc drivers/s390/crypto/vfio_ap_ops.c
index 69b05ef08538,a7d2a95796d3..000000000000
--- a/drivers/s390/crypto/vfio_ap_ops.c
+++ b/drivers/s390/crypto/vfio_ap_ops.c
@@@ -1154,38 -1284,17 +1154,41 @@@ static void vfio_ap_mdev_unset_kvm(stru
  	}
  }
  
++<<<<<<< HEAD
 +static int vfio_ap_mdev_group_notifier(struct notifier_block *nb,
 +				       unsigned long action, void *data)
 +{
 +	int notify_rc = NOTIFY_OK;
 +	struct ap_matrix_mdev *matrix_mdev;
 +
 +	if (action != VFIO_GROUP_NOTIFY_SET_KVM)
 +		return NOTIFY_OK;
 +
 +	matrix_mdev = container_of(nb, struct ap_matrix_mdev, group_notifier);
 +
 +	if (!data)
 +		vfio_ap_mdev_unset_kvm(matrix_mdev, matrix_mdev->kvm);
 +	else if (vfio_ap_mdev_set_kvm(matrix_mdev, data))
 +		notify_rc = NOTIFY_DONE;
 +
 +	return notify_rc;
 +}
 +
++=======
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  static struct vfio_ap_queue *vfio_ap_find_queue(int apqn)
  {
 -	struct device *dev;
 +	struct ap_queue *queue;
  	struct vfio_ap_queue *q = NULL;
  
 -	dev = driver_find_device(&matrix_dev->vfio_ap_drv->driver, NULL,
 -				 &apqn, match_apqn);
 -	if (dev) {
 -		q = dev_get_drvdata(dev);
 -		put_device(dev);
 -	}
 +	queue = ap_get_qdev(apqn);
 +	if (!queue)
 +		return NULL;
 +
 +	if (queue->ap_dev.device.driver == &matrix_dev->vfio_ap_drv->driver)
 +		q = dev_get_drvdata(&queue->ap_dev.device);
 +
 +	put_device(&queue->ap_dev.device);
  
  	return q;
  }
@@@ -1274,43 -1383,34 +1277,66 @@@ static int vfio_ap_mdev_open(struct mde
  	unsigned long events;
  	int ret;
  
++<<<<<<< HEAD
 +
 +	if (!try_module_get(THIS_MODULE))
 +		return -ENODEV;
 +
 +	matrix_mdev->group_notifier.notifier_call = vfio_ap_mdev_group_notifier;
 +	events = VFIO_GROUP_NOTIFY_SET_KVM;
 +
 +	ret = vfio_register_notifier(mdev_dev(mdev), VFIO_GROUP_NOTIFY,
 +				     &events, &matrix_mdev->group_notifier);
 +	if (ret) {
 +		module_put(THIS_MODULE);
++=======
+ 	if (!vdev->kvm)
+ 		return -EINVAL;
+ 
+ 	ret = vfio_ap_mdev_set_kvm(matrix_mdev, vdev->kvm);
+ 	if (ret)
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  		return ret;
 +	}
  
  	matrix_mdev->iommu_notifier.notifier_call = vfio_ap_mdev_iommu_notifier;
  	events = VFIO_IOMMU_NOTIFY_DMA_UNMAP;
++<<<<<<< HEAD
 +	ret = vfio_register_notifier(mdev_dev(mdev), VFIO_IOMMU_NOTIFY,
 +				     &events, &matrix_mdev->iommu_notifier);
 +	if (!ret)
 +		return ret;
 +
 +	vfio_unregister_notifier(mdev_dev(mdev), VFIO_GROUP_NOTIFY,
 +				 &matrix_mdev->group_notifier);
 +	module_put(THIS_MODULE);
++=======
+ 	ret = vfio_register_notifier(vdev, VFIO_IOMMU_NOTIFY, &events,
+ 				     &matrix_mdev->iommu_notifier);
+ 	if (ret)
+ 		goto err_kvm;
+ 	return 0;
+ 
+ err_kvm:
+ 	vfio_ap_mdev_unset_kvm(matrix_mdev);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	return ret;
  }
  
 -static void vfio_ap_mdev_close_device(struct vfio_device *vdev)
 +static void vfio_ap_mdev_release(struct mdev_device *mdev)
  {
 -	struct ap_matrix_mdev *matrix_mdev =
 -		container_of(vdev, struct ap_matrix_mdev, vdev);
 +	struct ap_matrix_mdev *matrix_mdev = mdev_get_drvdata(mdev);
  
 -	vfio_unregister_notifier(vdev, VFIO_IOMMU_NOTIFY,
 +	vfio_unregister_notifier(mdev_dev(mdev), VFIO_IOMMU_NOTIFY,
  				 &matrix_mdev->iommu_notifier);
++<<<<<<< HEAD
 +	vfio_unregister_notifier(mdev_dev(mdev), VFIO_GROUP_NOTIFY,
 +				 &matrix_mdev->group_notifier);
 +	vfio_ap_mdev_unset_kvm(matrix_mdev, matrix_mdev->kvm);
 +	module_put(THIS_MODULE);
++=======
+ 	vfio_ap_mdev_unset_kvm(matrix_mdev);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  }
  
  static int vfio_ap_mdev_get_device_info(unsigned long arg)
diff --cc drivers/vfio/vfio.c
index 6b6579dad759,e22be13e6771..000000000000
--- a/drivers/vfio/vfio.c
+++ b/drivers/vfio/vfio.c
@@@ -1441,22 -1075,36 +1441,42 @@@ static int vfio_group_get_device_fd(str
  	struct file *filep;
  	int ret;
  
 -	down_write(&device->group->group_rwsem);
 -	ret = vfio_device_assign_container(device);
 -	up_write(&device->group->group_rwsem);
 -	if (ret)
 -		return ERR_PTR(ret);
 +	if (0 == atomic_read(&group->container_users) ||
 +	    !group->container->iommu_driver || !vfio_group_viable(group))
 +		return -EINVAL;
  
 -	if (!try_module_get(device->dev->driver->owner)) {
 -		ret = -ENODEV;
 -		goto err_unassign_container;
 -	}
 +	if (group->noiommu && !capable(CAP_SYS_RAWIO))
 +		return -EPERM;
 +
++<<<<<<< HEAD
 +	device = vfio_device_get_from_name(group, buf);
 +	if (IS_ERR(device))
 +		return PTR_ERR(device);
  
 +	ret = device->ops->open(device->device_data);
 +	if (ret) {
 +		vfio_device_put(device);
 +		return ret;
++=======
+ 	mutex_lock(&device->dev_set->lock);
+ 	device->open_count++;
+ 	if (device->open_count == 1) {
+ 		/*
+ 		 * Here we pass the KVM pointer with the group under the read
+ 		 * lock.  If the device driver will use it, it must obtain a
+ 		 * reference and release it during close_device.
+ 		 */
+ 		down_read(&device->group->group_rwsem);
+ 		device->kvm = device->group->kvm;
+ 
+ 		if (device->ops->open_device) {
+ 			ret = device->ops->open_device(device);
+ 			if (ret)
+ 				goto err_undo_count;
+ 		}
+ 		up_read(&device->group->group_rwsem);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	}
 -	mutex_unlock(&device->dev_set->lock);
  
  	/*
  	 * We can't use anon_inode_getfd() because we need to modify
@@@ -1486,14 -1124,62 +1506,64 @@@
  	 */
  	filep->f_mode |= (FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE);
  
 -	if (device->group->type == VFIO_NO_IOMMU)
 +	atomic_inc(&group->container_users);
 +
 +	fd_install(ret, filep);
 +
 +	if (group->noiommu)
  		dev_warn(device->dev, "vfio-noiommu device opened by user "
  			 "(%s:%d)\n", current->comm, task_pid_nr(current));
 -	/*
 -	 * On success the ref of device is moved to the file and
 -	 * put in vfio_device_fops_release()
 -	 */
 -	return filep;
  
++<<<<<<< HEAD
++=======
+ err_close_device:
+ 	mutex_lock(&device->dev_set->lock);
+ 	down_read(&device->group->group_rwsem);
+ 	if (device->open_count == 1 && device->ops->close_device)
+ 		device->ops->close_device(device);
+ err_undo_count:
+ 	device->open_count--;
+ 	if (device->open_count == 0 && device->kvm)
+ 		device->kvm = NULL;
+ 	up_read(&device->group->group_rwsem);
+ 	mutex_unlock(&device->dev_set->lock);
+ 	module_put(device->dev->driver->owner);
+ err_unassign_container:
+ 	vfio_device_unassign_container(device);
+ 	return ERR_PTR(ret);
+ }
+ 
+ static int vfio_group_get_device_fd(struct vfio_group *group, char *buf)
+ {
+ 	struct vfio_device *device;
+ 	struct file *filep;
+ 	int fdno;
+ 	int ret;
+ 
+ 	device = vfio_device_get_from_name(group, buf);
+ 	if (IS_ERR(device))
+ 		return PTR_ERR(device);
+ 
+ 	fdno = get_unused_fd_flags(O_CLOEXEC);
+ 	if (fdno < 0) {
+ 		ret = fdno;
+ 		goto err_put_device;
+ 	}
+ 
+ 	filep = vfio_device_open(device);
+ 	if (IS_ERR(filep)) {
+ 		ret = PTR_ERR(filep);
+ 		goto err_put_fdno;
+ 	}
+ 
+ 	fd_install(fdno, filep);
+ 	return fdno;
+ 
+ err_put_fdno:
+ 	put_unused_fd(fdno);
+ err_put_device:
+ 	vfio_device_put(device);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	return ret;
  }
  
@@@ -1616,28 -1589,52 +1686,41 @@@ static int vfio_group_fops_release(stru
  	return 0;
  }
  
 -static int vfio_ioctl_device_feature(struct vfio_device *device,
 -				     struct vfio_device_feature __user *arg)
 -{
 -	size_t minsz = offsetofend(struct vfio_device_feature, flags);
 -	struct vfio_device_feature feature;
 +static const struct file_operations vfio_group_fops = {
 +	.owner		= THIS_MODULE,
 +	.unlocked_ioctl	= vfio_group_fops_unl_ioctl,
 +	.compat_ioctl	= compat_ptr_ioctl,
 +	.open		= vfio_group_fops_open,
 +	.release	= vfio_group_fops_release,
 +};
  
 -	if (copy_from_user(&feature, arg, minsz))
 -		return -EFAULT;
 +/**
 + * VFIO Device fd
 + */
 +static int vfio_device_fops_release(struct inode *inode, struct file *filep)
 +{
 +	struct vfio_device *device = filep->private_data;
  
 -	if (feature.argsz < minsz)
 -		return -EINVAL;
++<<<<<<< HEAD
 +	device->ops->release(device->device_data);
++=======
++	mutex_lock(&device->dev_set->lock);
++	vfio_assert_device_open(device);
++	down_read(&device->group->group_rwsem);
++	if (device->open_count == 1 && device->ops->close_device)
++		device->ops->close_device(device);
++	up_read(&device->group->group_rwsem);
++	device->open_count--;
++	if (device->open_count == 0)
++		device->kvm = NULL;
++	mutex_unlock(&device->dev_set->lock);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
 -	/* Check unknown flags */
 -	if (feature.flags &
 -	    ~(VFIO_DEVICE_FEATURE_MASK | VFIO_DEVICE_FEATURE_SET |
 -	      VFIO_DEVICE_FEATURE_GET | VFIO_DEVICE_FEATURE_PROBE))
 -		return -EINVAL;
 +	vfio_group_try_dissolve_container(device->group);
  
 -	/* GET & SET are mutually exclusive except with PROBE */
 -	if (!(feature.flags & VFIO_DEVICE_FEATURE_PROBE) &&
 -	    (feature.flags & VFIO_DEVICE_FEATURE_SET) &&
 -	    (feature.flags & VFIO_DEVICE_FEATURE_GET))
 -		return -EINVAL;
 +	vfio_device_put(device);
  
 -	switch (feature.flags & VFIO_DEVICE_FEATURE_MASK) {
 -	case VFIO_DEVICE_FEATURE_MIGRATION:
 -		return vfio_ioctl_device_feature_migration(
 -			device, feature.flags, arg->data,
 -			feature.argsz - minsz);
 -	case VFIO_DEVICE_FEATURE_MIG_DEVICE_STATE:
 -		return vfio_ioctl_device_feature_mig_device_state(
 -			device, feature.flags, arg->data,
 -			feature.argsz - minsz);
 -	default:
 -		if (unlikely(!device->ops->device_feature))
 -			return -EINVAL;
 -		return device->ops->device_feature(device, feature.flags,
 -						   arg->data,
 -						   feature.argsz - minsz);
 -	}
 +	return 0;
  }
  
  static long vfio_device_fops_unl_ioctl(struct file *filep,
@@@ -1695,117 -1696,95 +1778,159 @@@ static const struct file_operations vfi
  };
  
  /**
 - * vfio_file_iommu_group - Return the struct iommu_group for the vfio group file
 - * @file: VFIO group file
 + * External user API, exported by symbols to be linked dynamically.
   *
 - * The returned iommu_group is valid as long as a ref is held on the file.
 + * The protocol includes:
 + *  1. do normal VFIO init operation:
 + *	- opening a new container;
 + *	- attaching group(s) to it;
 + *	- setting an IOMMU driver for a container.
 + * When IOMMU is set for a container, all groups in it are
 + * considered ready to use by an external user.
 + *
 + * 2. User space passes a group fd to an external user.
 + * The external user calls vfio_group_get_external_user()
 + * to verify that:
 + *	- the group is initialized;
 + *	- IOMMU is set for it.
 + * If both checks passed, vfio_group_get_external_user()
 + * increments the container user counter to prevent
 + * the VFIO group from disposal before KVM exits.
 + *
 + * 3. The external user calls vfio_external_user_iommu_id()
 + * to know an IOMMU ID.
 + *
 + * 4. When the external KVM finishes, it calls
 + * vfio_group_put_external_user() to release the VFIO group.
 + * This call decrements the container user counter.
   */
 -struct iommu_group *vfio_file_iommu_group(struct file *file)
 +struct vfio_group *vfio_group_get_external_user(struct file *filep)
  {
 -	struct vfio_group *group = file->private_data;
 +	struct vfio_group *group = filep->private_data;
 +	int ret;
  
 -	if (file->f_op != &vfio_group_fops)
 -		return NULL;
 -	return group->iommu_group;
 +	if (filep->f_op != &vfio_group_fops)
 +		return ERR_PTR(-EINVAL);
 +
 +	ret = vfio_group_add_container_user(group);
 +	if (ret)
 +		return ERR_PTR(ret);
 +
 +	vfio_group_get(group);
 +
 +	return group;
  }
 -EXPORT_SYMBOL_GPL(vfio_file_iommu_group);
 +EXPORT_SYMBOL_GPL(vfio_group_get_external_user);
  
  /**
 - * vfio_file_enforced_coherent - True if the DMA associated with the VFIO file
 - *        is always CPU cache coherent
 - * @file: VFIO group file
 + * External user API, exported by symbols to be linked dynamically.
 + * The external user passes in a device pointer
 + * to verify that:
 + *	- A VFIO group is assiciated with the device;
 + *	- IOMMU is set for the group.
 + * If both checks passed, vfio_group_get_external_user_from_dev()
 + * increments the container user counter to prevent the VFIO group
 + * from disposal before external user exits and returns the pointer
 + * to the VFIO group.
   *
 - * Enforced coherency means that the IOMMU ignores things like the PCIe no-snoop
 - * bit in DMA transactions. A return of false indicates that the user has
 - * rights to access additional instructions such as wbinvd on x86.
 + * When the external user finishes using the VFIO group, it calls
 + * vfio_group_put_external_user() to release the VFIO group and
 + * decrement the container user counter.
 + *
 + * @dev [in]	: device
 + * Return error PTR or pointer to VFIO group.
   */
 -bool vfio_file_enforced_coherent(struct file *file)
 +
 +struct vfio_group *vfio_group_get_external_user_from_dev(struct device *dev)
  {
 -	struct vfio_group *group = file->private_data;
 -	bool ret;
 +	struct vfio_group *group;
 +	int ret;
  
 -	if (file->f_op != &vfio_group_fops)
 -		return true;
 +	group = vfio_group_get_from_dev(dev);
 +	if (!group)
 +		return ERR_PTR(-ENODEV);
  
 -	down_read(&group->group_rwsem);
 -	if (group->container) {
 -		ret = vfio_ioctl_check_extension(group->container,
 -						 VFIO_DMA_CC_IOMMU);
 -	} else {
 -		/*
 -		 * Since the coherency state is determined only once a container
 -		 * is attached the user must do so before they can prove they
 -		 * have permission.
 -		 */
 -		ret = true;
 +	ret = vfio_group_add_container_user(group);
 +	if (ret) {
 +		vfio_group_put(group);
 +		return ERR_PTR(ret);
  	}
 -	up_read(&group->group_rwsem);
 -	return ret;
 +
 +	return group;
 +}
 +EXPORT_SYMBOL_GPL(vfio_group_get_external_user_from_dev);
 +
 +void vfio_group_put_external_user(struct vfio_group *group)
 +{
 +	vfio_group_try_dissolve_container(group);
 +	vfio_group_put(group);
 +}
 +EXPORT_SYMBOL_GPL(vfio_group_put_external_user);
 +
 +bool vfio_external_group_match_file(struct vfio_group *test_group,
 +				    struct file *filep)
 +{
 +	struct vfio_group *group = filep->private_data;
 +
 +	return (filep->f_op == &vfio_group_fops) && (group == test_group);
 +}
 +EXPORT_SYMBOL_GPL(vfio_external_group_match_file);
 +
 +int vfio_external_user_iommu_id(struct vfio_group *group)
 +{
 +	return iommu_group_id(group->iommu_group);
 +}
 +EXPORT_SYMBOL_GPL(vfio_external_user_iommu_id);
 +
 +long vfio_external_check_extension(struct vfio_group *group, unsigned long arg)
 +{
 +	return vfio_ioctl_check_extension(group->container, arg);
  }
 -EXPORT_SYMBOL_GPL(vfio_file_enforced_coherent);
 +EXPORT_SYMBOL_GPL(vfio_external_check_extension);
  
  /**
++<<<<<<< HEAD
++=======
+  * vfio_file_set_kvm - Link a kvm with VFIO drivers
+  * @file: VFIO group file
+  * @kvm: KVM to link
+  *
+  * When a VFIO device is first opened the KVM will be available in
+  * device->kvm if one was associated with the group.
+  */
+ void vfio_file_set_kvm(struct file *file, struct kvm *kvm)
+ {
+ 	struct vfio_group *group = file->private_data;
+ 
+ 	if (file->f_op != &vfio_group_fops)
+ 		return;
+ 
+ 	down_write(&group->group_rwsem);
+ 	group->kvm = kvm;
+ 	up_write(&group->group_rwsem);
+ }
+ EXPORT_SYMBOL_GPL(vfio_file_set_kvm);
+ 
+ /**
+  * vfio_file_has_dev - True if the VFIO file is a handle for device
+  * @file: VFIO file to check
+  * @device: Device that must be part of the file
+  *
+  * Returns true if given file has permission to manipulate the given device.
+  */
+ bool vfio_file_has_dev(struct file *file, struct vfio_device *device)
+ {
+ 	struct vfio_group *group = file->private_data;
+ 
+ 	if (file->f_op != &vfio_group_fops)
+ 		return false;
+ 
+ 	return group == device->group;
+ }
+ EXPORT_SYMBOL_GPL(vfio_file_has_dev);
+ 
+ /*
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
   * Sub-module support
   */
  /*
@@@ -2175,9 -2028,7 +2300,13 @@@ static int vfio_register_iommu_notifier
  	struct vfio_iommu_driver *driver;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = vfio_group_add_container_user(group);
 +	if (ret)
 +		return -EINVAL;
++=======
+ 	lockdep_assert_held_read(&group->group_rwsem);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
  	container = group->container;
  	driver = container->iommu_driver;
@@@ -2186,8 -2037,6 +2315,11 @@@
  						     events, nb);
  	else
  		ret = -ENOTTY;
++<<<<<<< HEAD
 +
 +	vfio_group_try_dissolve_container(group);
++=======
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
  	return ret;
  }
@@@ -2199,9 -2048,7 +2331,13 @@@ static int vfio_unregister_iommu_notifi
  	struct vfio_iommu_driver *driver;
  	int ret;
  
++<<<<<<< HEAD
 +	ret = vfio_group_add_container_user(group);
 +	if (ret)
 +		return -EINVAL;
++=======
+ 	lockdep_assert_held_read(&group->group_rwsem);
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  
  	container = group->container;
  	driver = container->iommu_driver;
@@@ -2210,85 -2057,21 +2346,94 @@@
  						       nb);
  	else
  		ret = -ENOTTY;
++<<<<<<< HEAD
 +
 +	vfio_group_try_dissolve_container(group);
++=======
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
 +
 +	return ret;
 +}
 +
++<<<<<<< HEAD
 +void vfio_group_set_kvm(struct vfio_group *group, struct kvm *kvm)
 +{
 +	group->kvm = kvm;
 +	blocking_notifier_call_chain(&group->notifier,
 +				VFIO_GROUP_NOTIFY_SET_KVM, kvm);
 +}
 +EXPORT_SYMBOL_GPL(vfio_group_set_kvm);
 +
 +static int vfio_register_group_notifier(struct vfio_group *group,
 +					unsigned long *events,
 +					struct notifier_block *nb)
 +{
 +	int ret;
 +	bool set_kvm = false;
 +
 +	if (*events & VFIO_GROUP_NOTIFY_SET_KVM)
 +		set_kvm = true;
 +
 +	/* clear known events */
 +	*events &= ~VFIO_GROUP_NOTIFY_SET_KVM;
 +
 +	/* refuse to continue if still events remaining */
 +	if (*events)
 +		return -EINVAL;
 +
 +	ret = vfio_group_add_container_user(group);
 +	if (ret)
 +		return -EINVAL;
 +
 +	ret = blocking_notifier_chain_register(&group->notifier, nb);
 +
 +	/*
 +	 * The attaching of kvm and vfio_group might already happen, so
 +	 * here we replay once upon registration.
 +	 */
 +	if (!ret && set_kvm && group->kvm)
 +		blocking_notifier_call_chain(&group->notifier,
 +					VFIO_GROUP_NOTIFY_SET_KVM, group->kvm);
 +
 +	vfio_group_try_dissolve_container(group);
  
  	return ret;
  }
  
 +static int vfio_unregister_group_notifier(struct vfio_group *group,
 +					 struct notifier_block *nb)
++=======
+ int vfio_register_notifier(struct vfio_device *device,
+ 			   enum vfio_notify_type type, unsigned long *events,
+ 			   struct notifier_block *nb)
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  {
 -	struct vfio_group *group = device->group;
  	int ret;
  
 -	if (!nb || !events || (*events == 0) ||
 -	    !vfio_assert_device_open(device))
 +	ret = vfio_group_add_container_user(group);
 +	if (ret)
 +		return -EINVAL;
 +
 +	ret = blocking_notifier_chain_unregister(&group->notifier, nb);
 +
 +	vfio_group_try_dissolve_container(group);
 +
 +	return ret;
 +}
 +
 +int vfio_register_notifier(struct device *dev, enum vfio_notify_type type,
 +			   unsigned long *events, struct notifier_block *nb)
 +{
 +	struct vfio_group *group;
 +	int ret;
 +
 +	if (!dev || !nb || !events || (*events == 0))
  		return -EINVAL;
  
 +	group = vfio_group_get_from_dev(dev);
 +	if (!group)
 +		return -ENODEV;
 +
  	switch (type) {
  	case VFIO_IOMMU_NOTIFY:
  		ret = vfio_register_iommu_notifier(group, events, nb);
@@@ -2322,9 -2097,6 +2464,12 @@@ int vfio_unregister_notifier(struct dev
  	case VFIO_IOMMU_NOTIFY:
  		ret = vfio_unregister_iommu_notifier(group, nb);
  		break;
++<<<<<<< HEAD
 +	case VFIO_GROUP_NOTIFY:
 +		ret = vfio_unregister_group_notifier(group, nb);
 +		break;
++=======
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  	default:
  		ret = -EINVAL;
  	}
diff --cc include/linux/vfio.h
index 96af7a2ad4d3,aa888cc51757..000000000000
--- a/include/linux/vfio.h
+++ b/include/linux/vfio.h
@@@ -18,6 -15,37 +18,40 @@@
  #include <linux/poll.h>
  #include <uapi/linux/vfio.h>
  
++<<<<<<< HEAD
++=======
+ struct kvm;
+ 
+ /*
+  * VFIO devices can be placed in a set, this allows all devices to share this
+  * structure and the VFIO core will provide a lock that is held around
+  * open_device()/close_device() for all devices in the set.
+  */
+ struct vfio_device_set {
+ 	void *set_id;
+ 	struct mutex lock;
+ 	struct list_head device_list;
+ 	unsigned int device_count;
+ };
+ 
+ struct vfio_device {
+ 	struct device *dev;
+ 	const struct vfio_device_ops *ops;
+ 	struct vfio_group *group;
+ 	struct vfio_device_set *dev_set;
+ 	struct list_head dev_set_list;
+ 	unsigned int migration_flags;
+ 	/* Driver must reference the kvm during open_device or never touch it */
+ 	struct kvm *kvm;
+ 
+ 	/* Members below here are private, not for driver use */
+ 	refcount_t refcount;
+ 	unsigned int open_count;
+ 	struct completion comp;
+ 	struct list_head group_next;
+ };
+ 
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  /**
   * struct vfio_device_ops - VFIO bus driver device callbacks
   *
@@@ -138,10 -162,7 +171,14 @@@ enum vfio_notify_type 
  /* events for VFIO_IOMMU_NOTIFY */
  #define VFIO_IOMMU_NOTIFY_DMA_UNMAP	BIT(0)
  
++<<<<<<< HEAD
 +/* events for VFIO_GROUP_NOTIFY */
 +#define VFIO_GROUP_NOTIFY_SET_KVM	BIT(0)
 +
 +extern int vfio_register_notifier(struct device *dev,
++=======
+ extern int vfio_register_notifier(struct vfio_device *device,
++>>>>>>> 421cfe6596f6 (vfio: remove VFIO_GROUP_NOTIFY_SET_KVM)
  				  enum vfio_notify_type type,
  				  unsigned long *required_events,
  				  struct notifier_block *nb);
* Unmerged path drivers/gpu/drm/i915/gvt/gtt.c
* Unmerged path drivers/gpu/drm/i915/gvt/gvt.h
* Unmerged path drivers/gpu/drm/i915/gvt/kvmgt.c
* Unmerged path drivers/s390/crypto/vfio_ap_ops.c
diff --git a/drivers/s390/crypto/vfio_ap_private.h b/drivers/s390/crypto/vfio_ap_private.h
index 104c59398001..e0d0ae787bd9 100644
--- a/drivers/s390/crypto/vfio_ap_private.h
+++ b/drivers/s390/crypto/vfio_ap_private.h
@@ -86,8 +86,6 @@ struct ap_matrix {
  * @node:	allows the ap_matrix_mdev struct to be added to a list
  * @matrix:	the adapters, usage domains and control domains assigned to the
  *		mediated matrix device.
- * @group_notifier: notifier block used for specifying callback function for
- *		    handling the VFIO_GROUP_NOTIFY_SET_KVM event
  * @iommu_notifier: notifier block used for specifying callback function for
  *		    handling the VFIO_IOMMU_NOTIFY_DMA_UNMAP even
  * @kvm:	the struct holding guest's state
@@ -98,7 +96,6 @@ struct ap_matrix {
 struct ap_matrix_mdev {
 	struct list_head node;
 	struct ap_matrix matrix;
-	struct notifier_block group_notifier;
 	struct notifier_block iommu_notifier;
 	struct kvm *kvm;
 	crypto_hook pqap_hook;
* Unmerged path drivers/vfio/vfio.c
* Unmerged path include/linux/vfio.h
