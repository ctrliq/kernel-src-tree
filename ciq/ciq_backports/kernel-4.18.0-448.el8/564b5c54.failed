dm table: audit all dm_table_get_target() callers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Mike Snitzer <snitzer@kernel.org>
commit 564b5c5476cdb71b717340897b2b50f9c45df158
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/564b5c54.failed

All callers of dm_table_get_target() are expected to do proper bounds
checking on the index they pass.

Move dm_table_get_target() to dm-core.h to make it extra clear that only
DM core code should be using it. Switch it to be inlined while at it.

Standardize all DM core callers to use the same for loop pattern and
make associated variables as local as possible. Rename some variables
(e.g. s/table/t/ and s/tgt/ti/) along the way.

	Signed-off-by: Mike Snitzer <snitzer@kernel.org>
(cherry picked from commit 564b5c5476cdb71b717340897b2b50f9c45df158)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-ima.c
#	drivers/md/dm-table.c
#	drivers/md/dm-zone.c
#	drivers/md/dm.c
diff --cc drivers/md/dm-table.c
index f2992664f9c8,2f7bdb891013..000000000000
--- a/drivers/md/dm-table.c
+++ b/drivers/md/dm-table.c
@@@ -170,10 -171,10 +170,8 @@@ static void free_devices(struct list_he
  	}
  }
  
 -static void dm_table_destroy_crypto_profile(struct dm_table *t);
 -
  void dm_table_destroy(struct dm_table *t)
  {
- 	unsigned int i;
- 
  	if (!t)
  		return;
  
@@@ -593,15 -584,15 +591,20 @@@ static int validate_hardware_logical_bl
  	 */
  	unsigned short remaining = 0;
  
 -	struct dm_target *ti;
 +	struct dm_target *uninitialized_var(ti);
  	struct queue_limits ti_limits;
- 	unsigned i;
+ 	unsigned int i;
  
  	/*
  	 * Check each entry in the table in turn.
  	 */
++<<<<<<< HEAD
 +	for (i = 0; i < dm_table_get_num_targets(table); i++) {
 +		ti = dm_table_get_target(table, i);
++=======
+ 	for (i = 0; i < t->num_targets; i++) {
+ 		ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		blk_set_stacking_limits(&ti_limits);
  
@@@ -835,15 -823,12 +838,22 @@@ static int device_not_dax_synchronous_c
  	return !dev->dax_dev || !dax_synchronous(dev->dax_dev);
  }
  
++<<<<<<< HEAD
 +bool dm_table_supports_dax(struct dm_table *t,
 +			   iterate_devices_callout_fn iterate_fn, int *blocksize)
++=======
+ static bool dm_table_supports_dax(struct dm_table *t,
+ 				  iterate_devices_callout_fn iterate_fn)
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  {
- 	struct dm_target *ti;
- 	unsigned i;
- 
  	/* Ensure that all targets support DAX. */
++<<<<<<< HEAD
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
 +		ti = dm_table_get_target(t, i);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (!ti->type->direct_access)
  			return false;
@@@ -871,12 -856,10 +881,11 @@@ static int device_is_rq_stackable(struc
  
  static int dm_table_determine_type(struct dm_table *t)
  {
- 	unsigned i;
  	unsigned bio_based = 0, request_based = 0, hybrid = 0;
- 	struct dm_target *tgt;
+ 	struct dm_target *ti;
  	struct list_head *devices = dm_table_get_devices(t);
  	enum dm_queue_mode live_md_type = dm_get_md_type(t->md);
 +	int page_size = PAGE_SIZE;
  
  	if (t->type != DM_TYPE_NONE) {
  		/* target already set the table's type */
@@@ -995,11 -978,9 +1004,14 @@@ struct dm_target *dm_table_get_immutabl
  
  struct dm_target *dm_table_get_wildcard_target(struct dm_table *t)
  {
- 	struct dm_target *ti;
- 	unsigned i;
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
  
++<<<<<<< HEAD
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
 +		ti = dm_table_get_target(t, i);
++=======
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  		if (dm_target_is_wildcard(ti->type))
  			return ti;
  	}
@@@ -1030,19 -1012,47 +1042,54 @@@ static int dm_table_alloc_md_mempools(s
  		return -EINVAL;
  	}
  
 -	pools = kzalloc_node(sizeof(*pools), GFP_KERNEL, md->numa_node_id);
 -	if (!pools)
 +	if (__table_type_bio_based(type))
 +		for (i = 0; i < t->num_targets; i++) {
 +			ti = t->targets + i;
 +			per_io_data_size = max(per_io_data_size, ti->per_io_data_size);
 +			min_pool_size = max(min_pool_size, ti->num_flush_bios);
 +		}
 +
 +	t->mempools = dm_alloc_md_mempools(md, type, t->integrity_supported,
 +					   per_io_data_size, min_pool_size);
 +	if (!t->mempools)
  		return -ENOMEM;
  
++<<<<<<< HEAD
++=======
+ 	if (type == DM_TYPE_REQUEST_BASED) {
+ 		pool_size = dm_get_reserved_rq_based_ios();
+ 		front_pad = offsetof(struct dm_rq_clone_bio_info, clone);
+ 		goto init_bs;
+ 	}
+ 
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
+ 
+ 		per_io_data_size = max(per_io_data_size, ti->per_io_data_size);
+ 		min_pool_size = max(min_pool_size, ti->num_flush_bios);
+ 	}
+ 	pool_size = max(dm_get_reserved_bio_based_ios(), min_pool_size);
+ 	front_pad = roundup(per_io_data_size,
+ 		__alignof__(struct dm_target_io)) + DM_TARGET_IO_BIO_OFFSET;
+ 
+ 	io_front_pad = roundup(per_io_data_size,
+ 		__alignof__(struct dm_io)) + DM_IO_BIO_OFFSET;
+ 	if (bioset_init(&pools->io_bs, pool_size, io_front_pad,
+ 			dm_table_supports_poll(t) ? BIOSET_PERCPU_CACHE : 0))
+ 		goto out_free_pools;
+ 	if (t->integrity_supported &&
+ 	    bioset_integrity_create(&pools->io_bs, pool_size))
+ 		goto out_free_pools;
+ init_bs:
+ 	if (bioset_init(&pools->bs, pool_size, front_pad, 0))
+ 		goto out_free_pools;
+ 	if (t->integrity_supported &&
+ 	    bioset_integrity_create(&pools->bs, pool_size))
+ 		goto out_free_pools;
+ 
+ 	t->mempools = pools;
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  	return 0;
 -
 -out_free_pools:
 -	dm_free_md_mempools(pools);
 -	return -ENOMEM;
  }
  
  static int setup_indexes(struct dm_table *t)
@@@ -1107,10 -1117,10 +1154,14 @@@ static struct gendisk * dm_table_get_in
  	struct list_head *devices = dm_table_get_devices(t);
  	struct dm_dev_internal *dd = NULL;
  	struct gendisk *prev_disk = NULL, *template_disk = NULL;
- 	unsigned i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  		struct dm_target *ti = dm_table_get_target(t, i);
+ 
  		if (!dm_target_passes_integrity(ti->type))
  			goto no_integrity;
  	}
@@@ -1187,6 -1197,206 +1238,209 @@@ static int dm_table_register_integrity(
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_BLK_INLINE_ENCRYPTION
+ 
+ struct dm_crypto_profile {
+ 	struct blk_crypto_profile profile;
+ 	struct mapped_device *md;
+ };
+ 
+ struct dm_keyslot_evict_args {
+ 	const struct blk_crypto_key *key;
+ 	int err;
+ };
+ 
+ static int dm_keyslot_evict_callback(struct dm_target *ti, struct dm_dev *dev,
+ 				     sector_t start, sector_t len, void *data)
+ {
+ 	struct dm_keyslot_evict_args *args = data;
+ 	int err;
+ 
+ 	err = blk_crypto_evict_key(bdev_get_queue(dev->bdev), args->key);
+ 	if (!args->err)
+ 		args->err = err;
+ 	/* Always try to evict the key from all devices. */
+ 	return 0;
+ }
+ 
+ /*
+  * When an inline encryption key is evicted from a device-mapper device, evict
+  * it from all the underlying devices.
+  */
+ static int dm_keyslot_evict(struct blk_crypto_profile *profile,
+ 			    const struct blk_crypto_key *key, unsigned int slot)
+ {
+ 	struct mapped_device *md =
+ 		container_of(profile, struct dm_crypto_profile, profile)->md;
+ 	struct dm_keyslot_evict_args args = { key };
+ 	struct dm_table *t;
+ 	int srcu_idx;
+ 
+ 	t = dm_get_live_table(md, &srcu_idx);
+ 	if (!t)
+ 		return 0;
+ 
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
+ 
+ 		if (!ti->type->iterate_devices)
+ 			continue;
+ 		ti->type->iterate_devices(ti, dm_keyslot_evict_callback, &args);
+ 	}
+ 
+ 	dm_put_live_table(md, srcu_idx);
+ 	return args.err;
+ }
+ 
+ static int
+ device_intersect_crypto_capabilities(struct dm_target *ti, struct dm_dev *dev,
+ 				     sector_t start, sector_t len, void *data)
+ {
+ 	struct blk_crypto_profile *parent = data;
+ 	struct blk_crypto_profile *child =
+ 		bdev_get_queue(dev->bdev)->crypto_profile;
+ 
+ 	blk_crypto_intersect_capabilities(parent, child);
+ 	return 0;
+ }
+ 
+ void dm_destroy_crypto_profile(struct blk_crypto_profile *profile)
+ {
+ 	struct dm_crypto_profile *dmcp = container_of(profile,
+ 						      struct dm_crypto_profile,
+ 						      profile);
+ 
+ 	if (!profile)
+ 		return;
+ 
+ 	blk_crypto_profile_destroy(profile);
+ 	kfree(dmcp);
+ }
+ 
+ static void dm_table_destroy_crypto_profile(struct dm_table *t)
+ {
+ 	dm_destroy_crypto_profile(t->crypto_profile);
+ 	t->crypto_profile = NULL;
+ }
+ 
+ /*
+  * Constructs and initializes t->crypto_profile with a crypto profile that
+  * represents the common set of crypto capabilities of the devices described by
+  * the dm_table.  However, if the constructed crypto profile doesn't support all
+  * crypto capabilities that are supported by the current mapped_device, it
+  * returns an error instead, since we don't support removing crypto capabilities
+  * on table changes.  Finally, if the constructed crypto profile is "empty" (has
+  * no crypto capabilities at all), it just sets t->crypto_profile to NULL.
+  */
+ static int dm_table_construct_crypto_profile(struct dm_table *t)
+ {
+ 	struct dm_crypto_profile *dmcp;
+ 	struct blk_crypto_profile *profile;
+ 	unsigned int i;
+ 	bool empty_profile = true;
+ 
+ 	dmcp = kmalloc(sizeof(*dmcp), GFP_KERNEL);
+ 	if (!dmcp)
+ 		return -ENOMEM;
+ 	dmcp->md = t->md;
+ 
+ 	profile = &dmcp->profile;
+ 	blk_crypto_profile_init(profile, 0);
+ 	profile->ll_ops.keyslot_evict = dm_keyslot_evict;
+ 	profile->max_dun_bytes_supported = UINT_MAX;
+ 	memset(profile->modes_supported, 0xFF,
+ 	       sizeof(profile->modes_supported));
+ 
+ 	for (i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
+ 
+ 		if (!dm_target_passes_crypto(ti->type)) {
+ 			blk_crypto_intersect_capabilities(profile, NULL);
+ 			break;
+ 		}
+ 		if (!ti->type->iterate_devices)
+ 			continue;
+ 		ti->type->iterate_devices(ti,
+ 					  device_intersect_crypto_capabilities,
+ 					  profile);
+ 	}
+ 
+ 	if (t->md->queue &&
+ 	    !blk_crypto_has_capabilities(profile,
+ 					 t->md->queue->crypto_profile)) {
+ 		DMWARN("Inline encryption capabilities of new DM table were more restrictive than the old table's. This is not supported!");
+ 		dm_destroy_crypto_profile(profile);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/*
+ 	 * If the new profile doesn't actually support any crypto capabilities,
+ 	 * we may as well represent it with a NULL profile.
+ 	 */
+ 	for (i = 0; i < ARRAY_SIZE(profile->modes_supported); i++) {
+ 		if (profile->modes_supported[i]) {
+ 			empty_profile = false;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (empty_profile) {
+ 		dm_destroy_crypto_profile(profile);
+ 		profile = NULL;
+ 	}
+ 
+ 	/*
+ 	 * t->crypto_profile is only set temporarily while the table is being
+ 	 * set up, and it gets set to NULL after the profile has been
+ 	 * transferred to the request_queue.
+ 	 */
+ 	t->crypto_profile = profile;
+ 
+ 	return 0;
+ }
+ 
+ static void dm_update_crypto_profile(struct request_queue *q,
+ 				     struct dm_table *t)
+ {
+ 	if (!t->crypto_profile)
+ 		return;
+ 
+ 	/* Make the crypto profile less restrictive. */
+ 	if (!q->crypto_profile) {
+ 		blk_crypto_register(t->crypto_profile, q);
+ 	} else {
+ 		blk_crypto_update_capabilities(q->crypto_profile,
+ 					       t->crypto_profile);
+ 		dm_destroy_crypto_profile(t->crypto_profile);
+ 	}
+ 	t->crypto_profile = NULL;
+ }
+ 
+ #else /* CONFIG_BLK_INLINE_ENCRYPTION */
+ 
+ static int dm_table_construct_crypto_profile(struct dm_table *t)
+ {
+ 	return 0;
+ }
+ 
+ void dm_destroy_crypto_profile(struct blk_crypto_profile *profile)
+ {
+ }
+ 
+ static void dm_table_destroy_crypto_profile(struct dm_table *t)
+ {
+ }
+ 
+ static void dm_update_crypto_profile(struct request_queue *q,
+ 				     struct dm_table *t)
+ {
+ }
+ 
+ #endif /* !CONFIG_BLK_INLINE_ENCRYPTION */
+ 
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  /*
   * Prepares the table for use by building the indices,
   * setting the type, and allocating mempools.
@@@ -1305,11 -1521,8 +1551,16 @@@ struct dm_target *dm_table_find_target(
  static bool dm_table_any_dev_attr(struct dm_table *t,
  				  iterate_devices_callout_fn func, void *data)
  {
++<<<<<<< HEAD
 +	struct dm_target *ti;
 +	unsigned int i;
 +
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
 +		ti = dm_table_get_target(t, i);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (ti->type->iterate_devices &&
  		    ti->type->iterate_devices(ti, func, data))
@@@ -1329,19 -1542,30 +1580,41 @@@ static int count_device(struct dm_targe
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static bool dm_table_supports_poll(struct dm_table *t)
+ {
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
+ 
+ 		if (!ti->type->iterate_devices ||
+ 		    ti->type->iterate_devices(ti, device_not_poll_capable, NULL))
+ 			return false;
+ 	}
+ 
+ 	return true;
+ }
+ 
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  /*
   * Check whether a table has no data devices attached using each
   * target's iterate_devices method.
   * Returns false if the result is unknown because a target doesn't
   * support iterate_devices.
   */
- bool dm_table_has_no_data_devices(struct dm_table *table)
+ bool dm_table_has_no_data_devices(struct dm_table *t)
  {
++<<<<<<< HEAD
 +	struct dm_target *ti;
 +	unsigned i, num_devices;
 +
 +	for (i = 0; i < dm_table_get_num_targets(table); i++) {
 +		ti = dm_table_get_target(table, i);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
+ 		unsigned num_devices = 0;
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (!ti->type->iterate_devices)
  			return false;
@@@ -1374,11 -1597,8 +1646,16 @@@ static int device_not_zoned_model(struc
  static bool dm_table_supports_zoned_model(struct dm_table *t,
  					  enum blk_zoned_model zoned_model)
  {
++<<<<<<< HEAD
 +	struct dm_target *ti;
 +	unsigned i;
 +
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
 +		ti = dm_table_get_target(t, i);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (dm_target_supports_zoned_hm(ti->type)) {
  			if (!ti->type->iterate_devices ||
@@@ -1451,10 -1669,10 +1726,15 @@@ int dm_calculate_queue_limits(struct dm
  
  	blk_set_stacking_limits(limits);
  
++<<<<<<< HEAD
 +	for (i = 0; i < dm_table_get_num_targets(table); i++) {
 +		blk_set_stacking_limits(&ti_limits);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
- 		ti = dm_table_get_target(table, i);
+ 		blk_set_stacking_limits(&ti_limits);
  
  		if (!ti->type->iterate_devices)
  			goto combine_limits;
@@@ -1495,25 -1713,9 +1775,25 @@@ combine_limits
  			DMWARN("%s: adding target device "
  			       "(start sect %llu len %llu) "
  			       "caused an alignment inconsistency",
- 			       dm_device_name(table->md),
+ 			       dm_device_name(t->md),
  			       (unsigned long long) ti->begin,
  			       (unsigned long long) ti->len);
 +
 +		/*
 +		 * FIXME: this should likely be moved to blk_stack_limits(), would
 +		 * also eliminate limits->zoned stacking hack in dm_set_device_limits()
 +		 */
 +		if (limits->zoned == BLK_ZONED_NONE && ti_limits.zoned != BLK_ZONED_NONE) {
 +			/*
 +			 * By default, the stacked limits zoned model is set to
 +			 * BLK_ZONED_NONE in blk_set_stacking_limits(). Update
 +			 * this model using the first target model reported
 +			 * that is not BLK_ZONED_NONE. This will be either the
 +			 * first target device zoned model or the model reported
 +			 * by the target .io_hints.
 +			 */
 +			limits->zoned = ti_limits.zoned;
 +		}
  	}
  
  	/*
@@@ -1587,8 -1786,8 +1864,13 @@@ static bool dm_table_supports_flush(str
  	 * so we need to use iterate_devices here, which targets
  	 * supporting flushes must provide.
  	 */
++<<<<<<< HEAD
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
 +		ti = dm_table_get_target(t, i);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (!ti->num_flush_bios)
  			continue;
@@@ -1671,11 -1841,8 +1953,16 @@@ static int device_not_write_zeroes_capa
  
  static bool dm_table_supports_write_zeroes(struct dm_table *t)
  {
++<<<<<<< HEAD
 +	struct dm_target *ti;
 +	unsigned i = 0;
 +
 +	while (i < dm_table_get_num_targets(t)) {
 +		ti = dm_table_get_target(t, i++);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (!ti->num_write_zeroes_bios)
  			return false;
@@@ -1698,11 -1865,8 +1985,16 @@@ static int device_not_nowait_capable(st
  
  static bool dm_table_supports_nowait(struct dm_table *t)
  {
++<<<<<<< HEAD
 +	struct dm_target *ti;
 +	unsigned i = 0;
 +
 +	while (i < dm_table_get_num_targets(t)) {
 +		ti = dm_table_get_target(t, i++);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (!dm_target_supports_nowait(ti->type))
  			return false;
@@@ -1725,11 -1887,8 +2017,16 @@@ static int device_not_discard_capable(s
  
  static bool dm_table_supports_discards(struct dm_table *t)
  {
++<<<<<<< HEAD
 +	struct dm_target *ti;
 +	unsigned i;
 +
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
 +		ti = dm_table_get_target(t, i);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (!ti->num_discard_bios)
  			return false;
@@@ -1759,11 -1916,8 +2056,16 @@@ static int device_not_secure_erase_capa
  
  static bool dm_table_supports_secure_erase(struct dm_table *t)
  {
++<<<<<<< HEAD
 +	struct dm_target *ti;
 +	unsigned int i;
 +
 +	for (i = 0; i < dm_table_get_num_targets(t); i++) {
 +		ti = dm_table_get_target(t, i);
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		if (!ti->num_secure_erase_bios)
  			return false;
diff --cc drivers/md/dm.c
index 92dde437a06d,560ad05497a9..000000000000
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@@ -488,11 -422,11 +488,11 @@@ retry
  		return r;
  
  	/* We only support devices that have a single target */
 -	if (map->num_targets != 1)
 +	if (dm_table_get_num_targets(map) != 1)
  		return r;
  
- 	tgt = dm_table_get_target(map, 0);
- 	if (!tgt->type->prepare_ioctl)
+ 	ti = dm_table_get_target(map, 0);
+ 	if (!ti->type->prepare_ioctl)
  		return r;
  
  	if (dm_suspended_md(md))
@@@ -1537,17 -1553,9 +1536,23 @@@ static void __send_empty_flush(struct c
  	ci->sector_count = 0;
  	ci->io->tio.clone.bi_iter.bi_size = 0;
  
++<<<<<<< HEAD
 +	/*
 +	 * Empty flush uses a statically initialized bio, as the base for
 +	 * cloning.  However, blkg association requires that a bdev is
 +	 * associated with a gendisk, which doesn't happen until the bdev is
 +	 * opened.  So, blkg association is done at issue time of the flush
 +	 * rather than when the device is created in alloc_dev().
 +	 */
 +	bio_set_dev(ci->bio, ci->io->md->bdev);
 +
 +	while ((ti = dm_table_get_target(ci->map, target_nr++))) {
 +		int bios;
++=======
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		unsigned int bios;
+ 		struct dm_target *ti = dm_table_get_target(t, i);
++>>>>>>> 564b5c5476cd (dm table: audit all dm_table_get_target() callers)
  
  		atomic_add(ti->num_flush_bios, &ci->io->io_count);
  		bios = __send_duplicate_bios(ci, ti, ti->num_flush_bios, NULL);
@@@ -1563,22 -1571,14 +1568,22 @@@
  	bio_uninit(ci->bio);
  }
  
 +static bool is_split_required_for_discard(struct dm_target *ti)
 +{
 +	return ti->split_discard_bios;
 +}
 +
  static void __send_changing_extent_only(struct clone_info *ci, struct dm_target *ti,
 -					unsigned num_bios)
 +					unsigned num_bios, bool is_split_required)
  {
  	unsigned len;
- 	int bios;
+ 	unsigned int bios;
  
 -	len = min_t(sector_t, ci->sector_count,
 -		    max_io_len_target_boundary(ti, dm_target_offset(ti, ci->sector)));
 +	if (!is_split_required)
 +		len = min_t(sector_t, ci->sector_count,
 +			    max_io_len_target_boundary(ti, dm_target_offset(ti, ci->sector)));
 +	else
 +		len = min_t(sector_t, ci->sector_count, max_io_len(ti, ci->sector));
  
  	atomic_add(num_bios, &ci->io->io_count);
  	bios = __send_duplicate_bios(ci, ti, num_bios, &len);
* Unmerged path drivers/md/dm-ima.c
* Unmerged path drivers/md/dm-zone.c
diff --git a/drivers/md/dm-core.h b/drivers/md/dm-core.h
index fb3b0e2e086c..571e72d030e6 100644
--- a/drivers/md/dm-core.h
+++ b/drivers/md/dm-core.h
@@ -204,6 +204,13 @@ struct dm_table {
 	struct list_head target_callbacks;
 };
 
+static inline struct dm_target *dm_table_get_target(struct dm_table *t,
+						    unsigned int index)
+{
+	BUG_ON(index >= t->num_targets);
+	return t->targets + index;
+}
+
 /*
  * One of these is allocated per clone bio.
  */
* Unmerged path drivers/md/dm-ima.c
* Unmerged path drivers/md/dm-table.c
* Unmerged path drivers/md/dm-zone.c
* Unmerged path drivers/md/dm.c
diff --git a/drivers/md/dm.h b/drivers/md/dm.h
index 4217ca4197e3..c7c2ed48f77a 100644
--- a/drivers/md/dm.h
+++ b/drivers/md/dm.h
@@ -51,7 +51,6 @@ struct dm_md_mempools;
  *---------------------------------------------------------------*/
 void dm_table_event_callback(struct dm_table *t,
 			     void (*fn)(void *), void *context);
-struct dm_target *dm_table_get_target(struct dm_table *t, unsigned int index);
 struct dm_target *dm_table_find_target(struct dm_table *t, sector_t sector);
 bool dm_table_has_no_data_devices(struct dm_table *table);
 int dm_calculate_queue_limits(struct dm_table *table,
