swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 6424e31b1c050a25aea033206d5f626f3523448c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/6424e31b.failed

No users left.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
(cherry picked from commit 6424e31b1c050a25aea033206d5f626f3523448c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/swiotlb.h
#	kernel/dma/swiotlb.c
diff --cc include/linux/swiotlb.h
index 1befd6b2ccf5,7ed35dd3de6e..000000000000
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@@ -36,11 -34,11 +36,19 @@@ enum swiotlb_force 
  /* default to 64MB */
  #define IO_TLB_DEFAULT_SIZE (64UL<<20)
  
++<<<<<<< HEAD
 +extern void swiotlb_init(int verbose);
 +int swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose);
 +unsigned long swiotlb_size_or_default(void);
 +extern int swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs);
 +int swiotlb_init_late(size_t size, gfp_t gfp_mask);
++=======
+ unsigned long swiotlb_size_or_default(void);
+ void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
+ 	int (*remap)(void *tlb, unsigned long nslabs));
+ int swiotlb_init_late(size_t size, gfp_t gfp_mask,
+ 	int (*remap)(void *tlb, unsigned long nslabs));
++>>>>>>> 6424e31b1c05 (swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl)
  extern void __init swiotlb_update_mem_attributes(void);
  
  phys_addr_t swiotlb_tbl_map_single(struct device *hwdev, phys_addr_t phys,
diff --cc kernel/dma/swiotlb.c
index eb8c08292910,e2ef0864eb1e..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -228,56 -225,63 +228,91 @@@ static void swiotlb_init_io_tlb_mem(str
  	return;
  }
  
++<<<<<<< HEAD
 +int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 +{
 +	struct io_tlb_mem *mem = &io_tlb_default_mem;
 +	size_t alloc_size;
 +
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
 +		return 0;
 +
 +	/* protect against double initialization */
 +	if (WARN_ON_ONCE(mem->nslabs))
 +		return -ENOMEM;
 +
 +	alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
 +	mem->slots = memblock_alloc(alloc_size, PAGE_SIZE);
 +	if (!mem->slots)
 +		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
 +		      __func__, alloc_size, PAGE_SIZE);
 +
 +	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
 +
 +	if (verbose)
 +		swiotlb_print_info();
 +	return 0;
 +}
 +
++=======
++>>>>>>> 6424e31b1c05 (swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl)
  /*
   * Statically reserve bounce buffer space and initialize bounce buffer data
   * structures for the software IO TLB used to implement the DMA API.
   */
 -void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 -		int (*remap)(void *tlb, unsigned long nslabs))
 +void  __init
 +swiotlb_init(int verbose)
  {
++<<<<<<< HEAD
 +	size_t bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
++=======
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
+ 	unsigned long nslabs = default_nslabs;
+ 	size_t alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
+ 	size_t bytes;
++>>>>>>> 6424e31b1c05 (swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl)
  	void *tlb;
  
 -	if (!addressing_limit && !swiotlb_force_bounce)
 -		return;
 -	if (swiotlb_force_disable)
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
  		return;
  
 -	/*
 -	 * By default allocate the bounce buffer memory from low memory, but
 -	 * allow to pick a location everywhere for hypervisors with guest
 -	 * memory encryption.
 -	 */
 -retry:
 -	bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
 -	if (flags & SWIOTLB_ANY)
 -		tlb = memblock_alloc(bytes, PAGE_SIZE);
 -	else
 -		tlb = memblock_alloc_low(bytes, PAGE_SIZE);
 +	/* Get IO TLB memory from the low pages */
 +	tlb = memblock_alloc_low_nopanic(bytes, PAGE_SIZE);
  	if (!tlb)
++<<<<<<< HEAD
 +		goto fail;
 +	if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
 +		goto fail_free_mem;
 +	return;
 +
 +fail_free_mem:
 +	memblock_free_early(__pa(tlb), bytes);
 +fail:
 +	pr_warn("Cannot allocate buffer");
++=======
+ 		panic("%s: failed to allocate tlb structure\n", __func__);
+ 
+ 	if (remap && remap(tlb, nslabs) < 0) {
+ 		memblock_free(tlb, PAGE_ALIGN(bytes));
+ 
+ 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
+ 		if (nslabs < IO_TLB_MIN_SLABS)
+ 			panic("%s: Failed to remap %zu bytes\n",
+ 			      __func__, bytes);
+ 		goto retry;
+ 	}
+ 
+ 	mem->slots = memblock_alloc(alloc_size, PAGE_SIZE);
+ 	if (!mem->slots)
+ 		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
+ 		      __func__, alloc_size, PAGE_SIZE);
+ 
+ 	swiotlb_init_io_tlb_mem(mem, __pa(tlb), default_nslabs, false);
+ 	mem->force_bounce = flags & SWIOTLB_FORCE;
+ 
+ 	if (flags & SWIOTLB_VERBOSE)
+ 		swiotlb_print_info();
 -}
 -
 -void __init swiotlb_init(bool addressing_limit, unsigned int flags)
 -{
 -	return swiotlb_init_remap(addressing_limit, flags, NULL);
++>>>>>>> 6424e31b1c05 (swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl)
  }
  
  /*
@@@ -285,8 -289,10 +320,9 @@@
   * initialize the swiotlb later using the slab allocator if needed.
   * This should be just like above, but with some error catching.
   */
 -int swiotlb_init_late(size_t size, gfp_t gfp_mask,
 -		int (*remap)(void *tlb, unsigned long nslabs))
 +int swiotlb_init_late(size_t size, gfp_t gfp_mask)
  {
+ 	struct io_tlb_mem *mem = &io_tlb_default_mem;
  	unsigned long nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);
  	unsigned long bytes;
  	unsigned char *vstart = NULL;
@@@ -316,25 -323,16 +352,38 @@@
  			(PAGE_SIZE << order) >> 20);
  		nslabs = SLABS_PER_PAGE << order;
  	}
++<<<<<<< HEAD
 +	rc = swiotlb_late_init_with_tbl(vstart, nslabs);
 +	if (rc)
 +		free_pages((unsigned long)vstart, order);
 +
 +	return rc;
 +}
 +
 +int
 +swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 +{
 +	struct io_tlb_mem *mem = &io_tlb_default_mem;
 +	unsigned long bytes = nslabs << IO_TLB_SHIFT;
 +
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
 +		return 0;
 +
 +	/* protect against double initialization */
 +	if (WARN_ON_ONCE(mem->nslabs))
 +		return -ENOMEM;
++=======
+ 	if (remap)
+ 		rc = remap(vstart, nslabs);
+ 	if (rc) {
+ 		free_pages((unsigned long)vstart, order);
+ 
+ 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
+ 		if (nslabs < IO_TLB_MIN_SLABS)
+ 			return rc;
+ 		goto retry;
+ 	}
++>>>>>>> 6424e31b1c05 (swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl)
  
  	mem->slots = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,
  		get_order(array_size(sizeof(*mem->slots), nslabs)));
* Unmerged path include/linux/swiotlb.h
* Unmerged path kernel/dma/swiotlb.c
