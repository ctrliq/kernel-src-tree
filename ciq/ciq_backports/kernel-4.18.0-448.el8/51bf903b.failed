sched/fair: Optimize and simplify rq leaf_cfs_rq_list

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Chengming Zhou <zhouchengming@bytedance.com>
commit 51bf903b64bdde4e4c9009a9e2b4a589845d9d81
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/51bf903b.failed

We notice the rq leaf_cfs_rq_list has two problems when do bugfix
backports and some test profiling.

1. cfs_rqs under throttled subtree could be added to the list, and
   make their fully decayed ancestors on the list, even though not needed.

2. #1 also make the leaf_cfs_rq_list management complex and error prone,
   this is the list of related bugfix so far:

   commit 31bc6aeaab1d ("sched/fair: Optimize update_blocked_averages()")
   commit fe61468b2cbc ("sched/fair: Fix enqueue_task_fair warning")
   commit b34cb07dde7c ("sched/fair: Fix enqueue_task_fair() warning some more")
   commit 39f23ce07b93 ("sched/fair: Fix unthrottle_cfs_rq() for leaf_cfs_rq list")
   commit 0258bdfaff5b ("sched/fair: Fix unfairness caused by missing load decay")
   commit a7b359fc6a37 ("sched/fair: Correctly insert cfs_rq's to list on unthrottle")
   commit fdaba61ef8a2 ("sched/fair: Ensure that the CFS parent is added after unthrottling")
   commit 2630cde26711 ("sched/fair: Add ancestors of unthrottled undecayed cfs_rq")

commit 31bc6aeaab1d ("sched/fair: Optimize update_blocked_averages()")
delete every cfs_rq under throttled subtree from rq->leaf_cfs_rq_list,
and delete the throttled_hierarchy() test in update_blocked_averages(),
which optimized update_blocked_averages().

But those later bugfix add cfs_rqs under throttled subtree back to
rq->leaf_cfs_rq_list again, with their fully decayed ancestors, for
the integrity of rq->leaf_cfs_rq_list.

This patch takes another method, skip all cfs_rqs under throttled
hierarchy when list_add_leaf_cfs_rq(), to completely make cfs_rqs
under throttled subtree off the leaf_cfs_rq_list.

So we don't need to consider throttled related things in
enqueue_entity(), unthrottle_cfs_rq() and enqueue_task_fair(),
which simplify the code a lot. Also optimize update_blocked_averages()
since cfs_rqs under throttled hierarchy and their ancestors
won't be on the leaf_cfs_rq_list.

	Signed-off-by: Chengming Zhou <zhouchengming@bytedance.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
Link: https://lore.kernel.org/r/20220601021848.76943-1-zhouchengming@bytedance.com
(cherry picked from commit 51bf903b64bdde4e4c9009a9e2b4a589845d9d81)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 84477504c01e,7d8ef01669a5..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -4994,43 -5057,26 +4996,49 @@@ void unthrottle_cfs_rq(struct cfs_rq *c
  	}
  
  	for_each_sched_entity(se) {
 -		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
 +		cfs_rq = cfs_rq_of(se);
  
 -		update_load_avg(qcfs_rq, se, UPDATE_TG);
 +		update_load_avg(cfs_rq, se, UPDATE_TG);
  		se_update_runnable(se);
  
 -		if (cfs_rq_is_idle(group_cfs_rq(se)))
 -			idle_task_delta = cfs_rq->h_nr_running;
 +		cfs_rq->h_nr_running += task_delta;
 +		cfs_rq->idle_h_nr_running += idle_task_delta;
  
 -		qcfs_rq->h_nr_running += task_delta;
 -		qcfs_rq->idle_h_nr_running += idle_task_delta;
  
  		/* end evaluation on encountering a throttled cfs_rq */
 -		if (cfs_rq_throttled(qcfs_rq))
 +		if (cfs_rq_throttled(cfs_rq))
  			goto unthrottle_throttle;
++<<<<<<< HEAD
 +
 +		/*
 +		 * One parent has been throttled and cfs_rq removed from the
 +		 * list. Add it back to not break the leaf list.
 +		 */
 +		if (throttled_hierarchy(cfs_rq))
 +			list_add_leaf_cfs_rq(cfs_rq);
++=======
++>>>>>>> 51bf903b64bd (sched/fair: Optimize and simplify rq leaf_cfs_rq_list)
  	}
  
  	/* At this point se is NULL and we are at root level*/
  	add_nr_running(rq, task_delta);
  
  unthrottle_throttle:
++<<<<<<< HEAD
 +	/*
 +	 * The cfs_rq_throttled() breaks in the above iteration can result in
 +	 * incomplete leaf list maintenance, resulting in triggering the
 +	 * assertion below.
 +	 */
 +	for_each_sched_entity(se) {
 +		cfs_rq = cfs_rq_of(se);
 +
 +		if (list_add_leaf_cfs_rq(cfs_rq))
 +			break;
 +	}
 +
++=======
++>>>>>>> 51bf903b64bd (sched/fair: Optimize and simplify rq leaf_cfs_rq_list)
  	assert_list_leaf_cfs_rq(rq);
  
  	/* Determine whether we need to wake up potentially idle CPU: */
* Unmerged path kernel/sched/fair.c
