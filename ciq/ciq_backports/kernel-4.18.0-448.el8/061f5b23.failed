net/mlx5: SF, Use all available cpu for setting cpu affinity

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Shay Drory <shayd@nvidia.com>
commit 061f5b23588a2b2a499643c8c798dcdb271bc059
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/061f5b23.failed

Currently all SFs are using the same CPUs. Spreading SF over CPUs, in
round-robin manner, in order to achieve better distribution of the SFs
over available CPUs.

	Signed-off-by: Shay Drory <shayd@nvidia.com>
	Reviewed-by: Moshe Shemesh <moshe@nvidia.com>
	Reviewed-by: Parav Pandit <parav@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 061f5b23588a2b2a499643c8c798dcdb271bc059)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/eq.c
#	drivers/net/ethernet/mellanox/mlx5/core/irq_affinity.c
#	drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
#	drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
#	drivers/net/ethernet/mellanox/mlx5/core/pci_irq.h
diff --cc drivers/net/ethernet/mellanox/mlx5/core/eq.c
index 5fbef7c95e48,48a45aa54a3c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@@ -784,6 -794,54 +784,57 @@@ void mlx5_eq_update_ci(struct mlx5_eq *
  }
  EXPORT_SYMBOL(mlx5_eq_update_ci);
  
++<<<<<<< HEAD
++=======
+ static void comp_irqs_release(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 
+ 	if (mlx5_core_is_sf(dev))
+ 		mlx5_irq_affinity_irqs_release(dev, table->comp_irqs, table->num_comp_eqs);
+ 	else
+ 		mlx5_irqs_release_vectors(table->comp_irqs, table->num_comp_eqs);
+ 	kfree(table->comp_irqs);
+ }
+ 
+ static int comp_irqs_request(struct mlx5_core_dev *dev)
+ {
+ 	struct mlx5_eq_table *table = dev->priv.eq_table;
+ 	int ncomp_eqs = table->num_comp_eqs;
+ 	u16 *cpus;
+ 	int ret;
+ 	int i;
+ 
+ 	ncomp_eqs = table->num_comp_eqs;
+ 	table->comp_irqs = kcalloc(ncomp_eqs, sizeof(*table->comp_irqs), GFP_KERNEL);
+ 	if (!table->comp_irqs)
+ 		return -ENOMEM;
+ 	if (mlx5_core_is_sf(dev)) {
+ 		ret = mlx5_irq_affinity_irqs_request_auto(dev, ncomp_eqs, table->comp_irqs);
+ 		if (ret < 0)
+ 			goto free_irqs;
+ 		return ret;
+ 	}
+ 
+ 	cpus = kcalloc(ncomp_eqs, sizeof(*cpus), GFP_KERNEL);
+ 	if (!cpus) {
+ 		ret = -ENOMEM;
+ 		goto free_irqs;
+ 	}
+ 	for (i = 0; i < ncomp_eqs; i++)
+ 		cpus[i] = cpumask_local_spread(i, dev->priv.numa_node);
+ 	ret = mlx5_irqs_request_vectors(dev, cpus, ncomp_eqs, table->comp_irqs);
+ 	kfree(cpus);
+ 	if (ret < 0)
+ 		goto free_irqs;
+ 	return ret;
+ 
+ free_irqs:
+ 	kfree(table->comp_irqs);
+ 	return ret;
+ }
+ 
++>>>>>>> 061f5b23588a (net/mlx5: SF, Use all available cpu for setting cpu affinity)
  static void destroy_comp_eqs(struct mlx5_core_dev *dev)
  {
  	struct mlx5_eq_table *table = dev->priv.eq_table;
diff --cc drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
index 7028e4b43837,23cb63fa4588..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
@@@ -31,4 -34,28 +31,31 @@@ int mlx5_irq_detach_nb(struct mlx5_irq 
  struct cpumask *mlx5_irq_get_affinity_mask(struct mlx5_irq *irq);
  int mlx5_irq_get_index(struct mlx5_irq *irq);
  
++<<<<<<< HEAD
++=======
+ struct mlx5_irq_pool;
+ #ifdef CONFIG_MLX5_SF
+ int mlx5_irq_affinity_irqs_request_auto(struct mlx5_core_dev *dev, int nirqs,
+ 					struct mlx5_irq **irqs);
+ struct mlx5_irq *mlx5_irq_affinity_request(struct mlx5_irq_pool *pool,
+ 					   const struct cpumask *req_mask);
+ void mlx5_irq_affinity_irqs_release(struct mlx5_core_dev *dev, struct mlx5_irq **irqs,
+ 				    int num_irqs);
+ #else
+ static inline int mlx5_irq_affinity_irqs_request_auto(struct mlx5_core_dev *dev, int nirqs,
+ 						      struct mlx5_irq **irqs)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static inline struct mlx5_irq *
+ mlx5_irq_affinity_request(struct mlx5_irq_pool *pool, const struct cpumask *req_mask)
+ {
+ 	return ERR_PTR(-EOPNOTSUPP);
+ }
+ 
+ static inline void mlx5_irq_affinity_irqs_release(struct mlx5_core_dev *dev,
+ 						  struct mlx5_irq **irqs, int num_irqs) {}
+ #endif
++>>>>>>> 061f5b23588a (net/mlx5: SF, Use all available cpu for setting cpu affinity)
  #endif /* __MLX5_IRQ_H__ */
diff --cc drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
index 4a06fb0b287d,90fec0649ef5..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
@@@ -154,15 -139,46 +154,25 @@@ static void irq_release(struct kref *kr
  	kfree(irq);
  }
  
- static void irq_put(struct mlx5_irq *irq)
+ int mlx5_irq_put(struct mlx5_irq *irq)
  {
  	struct mlx5_irq_pool *pool = irq->pool;
+ 	int ret = 0;
  
  	mutex_lock(&pool->lock);
++<<<<<<< HEAD
 +	kref_put(&irq->kref, irq_release);
++=======
+ 	irq->refcount--;
+ 	if (!irq->refcount) {
+ 		irq_release(irq);
+ 		ret = 1;
+ 	}
++>>>>>>> 061f5b23588a (net/mlx5: SF, Use all available cpu for setting cpu affinity)
  	mutex_unlock(&pool->lock);
+ 	return ret;
  }
  
 -int mlx5_irq_read_locked(struct mlx5_irq *irq)
 -{
 -	lockdep_assert_held(&irq->pool->lock);
 -	return irq->refcount;
 -}
 -
 -int mlx5_irq_get_locked(struct mlx5_irq *irq)
 -{
 -	lockdep_assert_held(&irq->pool->lock);
 -	if (WARN_ON_ONCE(!irq->refcount))
 -		return 0;
 -	irq->refcount++;
 -	return 1;
 -}
 -
 -static int irq_get(struct mlx5_irq *irq)
 -{
 -	int err;
 -
 -	mutex_lock(&irq->pool->lock);
 -	err = mlx5_irq_get_locked(irq);
 -	mutex_unlock(&irq->pool->lock);
 -	return err;
 -}
 -
  static irqreturn_t irq_int_handler(int irq, void *nh)
  {
  	atomic_notifier_call_chain(nh, 0, NULL);
@@@ -190,13 -206,8 +200,18 @@@ static void irq_set_name(struct mlx5_ir
  	snprintf(name, MLX5_MAX_IRQ_NAME, "mlx5_comp%d", vecidx);
  }
  
++<<<<<<< HEAD
 +static bool irq_pool_is_sf_pool(struct mlx5_irq_pool *pool)
 +{
 +	return !strncmp("mlx5_sf", pool->name, strlen("mlx5_sf"));
 +}
 +
 +static struct mlx5_irq *irq_request(struct mlx5_irq_pool *pool, int i,
 +				    struct cpumask *affinity)
++=======
+ struct mlx5_irq *mlx5_irq_alloc(struct mlx5_irq_pool *pool, int i,
+ 				const struct cpumask *affinity)
++>>>>>>> 061f5b23588a (net/mlx5: SF, Use all available cpu for setting cpu affinity)
  {
  	struct mlx5_core_dev *dev = pool->dev;
  	char name[MLX5_MAX_IRQ_NAME];
@@@ -259,10 -270,10 +274,17 @@@ int mlx5_irq_attach_nb(struct mlx5_irq 
  		 * on non-existing IRQ.
  		 */
  		return -ENOENT;
++<<<<<<< HEAD
 +	err = atomic_notifier_chain_register(&irq->nh, nb);
 +	if (err)
 +		irq_put(irq);
 +	return err;
++=======
+ 	ret = atomic_notifier_chain_register(&irq->nh, nb);
+ 	if (ret)
+ 		mlx5_irq_put(irq);
+ 	return ret;
++>>>>>>> 061f5b23588a (net/mlx5: SF, Use all available cpu for setting cpu affinity)
  }
  
  int mlx5_irq_detach_nb(struct mlx5_irq *irq, struct notifier_block *nb)
@@@ -404,13 -355,27 +440,31 @@@ static struct mlx5_irq_pool *ctrl_irq_p
  }
  
  /**
 - * mlx5_irqs_release - release one or more IRQs back to the system.
 - * @irqs: IRQs to be released.
 - * @nirqs: number of IRQs to be released.
 + * mlx5_irq_release - release an IRQ back to the system.
 + * @irq: irq to be released.
   */
 -static void mlx5_irqs_release(struct mlx5_irq **irqs, int nirqs)
 +void mlx5_irq_release(struct mlx5_irq *irq)
  {
++<<<<<<< HEAD
 +	synchronize_irq(irq->irqn);
 +	irq_put(irq);
++=======
+ 	int i;
+ 
+ 	for (i = 0; i < nirqs; i++) {
+ 		synchronize_irq(irqs[i]->irqn);
+ 		mlx5_irq_put(irqs[i]);
+ 	}
+ }
+ 
+ /**
+  * mlx5_ctrl_irq_release - release a ctrl IRQ back to the system.
+  * @ctrl_irq: ctrl IRQ to be released.
+  */
+ void mlx5_ctrl_irq_release(struct mlx5_irq *ctrl_irq)
+ {
+ 	mlx5_irqs_release(&ctrl_irq, 1);
++>>>>>>> 061f5b23588a (net/mlx5: SF, Use all available cpu for setting cpu affinity)
  }
  
  /**
@@@ -461,22 -426,8 +515,24 @@@ struct mlx5_irq *mlx5_irq_request(struc
  	struct mlx5_irq_pool *pool;
  	struct mlx5_irq *irq;
  
++<<<<<<< HEAD
 +	if (mlx5_core_is_sf(dev)) {
 +		pool = sf_irq_pool_get(irq_table);
 +		if (!pool)
 +			/* we don't have IRQs for SFs, using the PF IRQs */
 +			goto pf_irq;
 +		if (cpumask_empty(affinity) && !strcmp(pool->name, "mlx5_sf_comp"))
 +			/* In case an SF user request IRQ with vecidx */
 +			irq = irq_pool_request_vector(pool, vecidx, NULL);
 +		else
 +			irq = irq_pool_request_affinity(pool, affinity);
 +		goto out;
 +	}
 +pf_irq:
++=======
++>>>>>>> 061f5b23588a (net/mlx5: SF, Use all available cpu for setting cpu affinity)
  	pool = irq_table->pf_pool;
  	irq = irq_pool_request_vector(pool, vecidx, affinity);
- out:
  	if (IS_ERR(irq))
  		return irq;
  	mlx5_core_dbg(dev, "irq %u mapped to cpu %*pbl, %u EQs on this irq\n",
@@@ -513,10 -509,15 +569,11 @@@ static void irq_pool_free(struct mlx5_i
  	struct mlx5_irq *irq;
  	unsigned long index;
  
 -	/* There are cases in which we are destrying the irq_table before
 -	 * freeing all the IRQs, fast teardown for example. Hence, free the irqs
 -	 * which might not have been freed.
 -	 */
  	xa_for_each(&pool->irqs, index, irq)
 -		irq_release(irq);
 +		irq_release(&irq->kref);
  	xa_destroy(&pool->irqs);
  	mutex_destroy(&pool->lock);
+ 	kfree(pool->irqs_per_cpu);
  	kvfree(pool);
  }
  
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/irq_affinity.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/pci_irq.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/eq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/irq_affinity.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/mlx5_irq.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/pci_irq.h
