iommu: Allow enabling non-strict mode dynamically

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit 452e69b58c2889e5546edb92d9e66285410f7463
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/452e69b5.failed

Allocating and enabling a flush queue is in fact something we can
reasonably do while a DMA domain is active, without having to rebuild it
from scratch. Thus we can allow a strict -> non-strict transition from
sysfs without requiring to unbind the device's driver, which is of
particular interest to users who want to make selective relaxations to
critical devices like the one serving their root filesystem.

Disabling and draining a queue also seems technically possible to
achieve without rebuilding the whole domain, but would certainly be more
involved. Furthermore there's not such a clear use-case for tightening
up security *after* the device may already have done whatever it is that
you don't trust it not to do, so we only consider the relaxation case.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
Link: https://lore.kernel.org/r/d652966348c78457c38bf18daf369272a4ebc2c9.1628682049.git.robin.murphy@arm.com
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit 452e69b58c2889e5546edb92d9e66285410f7463)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/dma-iommu.c
diff --cc drivers/iommu/dma-iommu.c
index 36f675968a2f,bac7370ead3e..000000000000
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@@ -329,9 -317,28 +329,34 @@@ static bool dev_is_untrusted(struct dev
  	return dev_is_pci(dev) && to_pci_dev(dev)->untrusted;
  }
  
++<<<<<<< HEAD
 +static bool dev_use_swiotlb(struct device *dev)
 +{
 +	return IS_ENABLED(CONFIG_SWIOTLB) && dev_is_untrusted(dev);
++=======
+ /* sysfs updates are serialised by the mutex of the group owning @domain */
+ int iommu_dma_init_fq(struct iommu_domain *domain)
+ {
+ 	struct iommu_dma_cookie *cookie = domain->iova_cookie;
+ 	int ret;
+ 
+ 	if (cookie->fq_domain)
+ 		return 0;
+ 
+ 	ret = init_iova_flush_queue(&cookie->iovad, iommu_dma_flush_iotlb_all,
+ 				    iommu_dma_entry_dtor);
+ 	if (ret) {
+ 		pr_warn("iova flush queue initialization failed\n");
+ 		return ret;
+ 	}
+ 	/*
+ 	 * Prevent incomplete iovad->fq being observable. Pairs with path from
+ 	 * __iommu_dma_unmap() through iommu_dma_free_iova() to queue_iova()
+ 	 */
+ 	smp_wmb();
+ 	WRITE_ONCE(cookie->fq_domain, domain);
+ 	return 0;
++>>>>>>> 452e69b58c28 (iommu: Allow enabling non-strict mode dynamically)
  }
  
  /**
@@@ -387,14 -394,9 +412,20 @@@ static int iommu_dma_init_domain(struc
  
  	init_iova_domain(iovad, 1UL << order, base_pfn);
  
++<<<<<<< HEAD
 +	if (!cookie->fq_domain && !dev_is_untrusted(dev) &&
 +	    domain->ops->flush_iotlb_all && !iommu_get_dma_strict(domain)) {
 +		if (init_iova_flush_queue(iovad, iommu_dma_flush_iotlb_all,
 +					  iommu_dma_entry_dtor))
 +			pr_warn("iova flush queue initialization failed\n");
 +		else
 +			cookie->fq_domain = domain;
 +	}
++=======
+ 	/* If the FQ fails we can simply fall back to strict mode */
+ 	if (domain->type == IOMMU_DOMAIN_DMA_FQ && iommu_dma_init_fq(domain))
+ 		domain->type = IOMMU_DOMAIN_DMA;
++>>>>>>> 452e69b58c28 (iommu: Allow enabling non-strict mode dynamically)
  
  	return iova_reserve_iommu_regions(dev, domain);
  }
@@@ -503,11 -505,28 +534,11 @@@ static void __iommu_dma_unmap(struct de
  	unmapped = iommu_unmap_fast(domain, dma_addr, size, &iotlb_gather);
  	WARN_ON(unmapped != size);
  
- 	if (!cookie->fq_domain)
+ 	if (!iotlb_gather.queued)
  		iommu_iotlb_sync(domain, &iotlb_gather);
- 	iommu_dma_free_iova(cookie, dma_addr, size, iotlb_gather.freelist);
+ 	iommu_dma_free_iova(cookie, dma_addr, size, &iotlb_gather);
  }
  
 -static void __iommu_dma_unmap_swiotlb(struct device *dev, dma_addr_t dma_addr,
 -		size_t size, enum dma_data_direction dir,
 -		unsigned long attrs)
 -{
 -	struct iommu_domain *domain = iommu_get_dma_domain(dev);
 -	phys_addr_t phys;
 -
 -	phys = iommu_iova_to_phys(domain, dma_addr);
 -	if (WARN_ON(!phys))
 -		return;
 -
 -	__iommu_dma_unmap(dev, dma_addr, size);
 -
 -	if (unlikely(is_swiotlb_buffer(phys)))
 -		swiotlb_tbl_unmap_single(dev, phys, size, dir, attrs);
 -}
 -
  static dma_addr_t __iommu_dma_map(struct device *dev, phys_addr_t phys,
  		size_t size, int prot, u64 dma_mask)
  {
* Unmerged path drivers/iommu/dma-iommu.c
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index 06c678399112..b8db05453b3f 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -3250,6 +3250,14 @@ static int iommu_change_dev_def_domain(struct iommu_group *group,
 		goto out;
 	}
 
+	/* We can bring up a flush queue without tearing down the domain */
+	if (type == IOMMU_DOMAIN_DMA_FQ && prev_dom->type == IOMMU_DOMAIN_DMA) {
+		ret = iommu_dma_init_fq(prev_dom);
+		if (!ret)
+			prev_dom->type = IOMMU_DOMAIN_DMA_FQ;
+		goto out;
+	}
+
 	/* Sets group->default_domain to the newly allocated domain */
 	ret = iommu_group_alloc_default_domain(dev->bus, group, type);
 	if (ret)
@@ -3290,9 +3298,9 @@ static int iommu_change_dev_def_domain(struct iommu_group *group,
 }
 
 /*
- * Changing the default domain through sysfs requires the users to ubind the
- * drivers from the devices in the iommu group. Return failure if this doesn't
- * meet.
+ * Changing the default domain through sysfs requires the users to unbind the
+ * drivers from the devices in the iommu group, except for a DMA -> DMA-FQ
+ * transition. Return failure if this isn't met.
  *
  * We need to consider the race between this and the device release path.
  * device_lock(dev) is used here to guarantee that the device release path
@@ -3368,7 +3376,8 @@ static ssize_t iommu_group_store_type(struct iommu_group *group,
 
 	/* Check if the device in the group still has a driver bound to it */
 	device_lock(dev);
-	if (device_is_bound(dev)) {
+	if (device_is_bound(dev) && !(req_type == IOMMU_DOMAIN_DMA_FQ &&
+	    group->default_domain->type == IOMMU_DOMAIN_DMA)) {
 		pr_err_ratelimited("Device is still bound to driver\n");
 		ret = -EBUSY;
 		goto out;
diff --git a/drivers/iommu/iova.c b/drivers/iommu/iova.c
index 1a967badec05..96e63e268ebd 100644
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@ -132,8 +132,6 @@ int init_iova_flush_queue(struct iova_domain *iovad,
 		spin_lock_init(&fq->lock);
 	}
 
-	smp_wmb();
-
 	iovad->fq = queue;
 
 	timer_setup(&iovad->fq_timer, fq_flush_timeout, 0);
@@ -644,17 +642,20 @@ void queue_iova(struct iova_domain *iovad,
 		unsigned long pfn, unsigned long pages,
 		unsigned long data)
 {
-	struct iova_fq *fq = raw_cpu_ptr(iovad->fq);
+	struct iova_fq *fq;
 	unsigned long flags;
 	unsigned idx;
 
 	/*
 	 * Order against the IOMMU driver's pagetable update from unmapping
 	 * @pte, to guarantee that iova_domain_flush() observes that if called
-	 * from a different CPU before we release the lock below.
+	 * from a different CPU before we release the lock below. Full barrier
+	 * so it also pairs with iommu_dma_init_fq() to avoid seeing partially
+	 * written fq state here.
 	 */
-	smp_wmb();
+	smp_mb();
 
+	fq = raw_cpu_ptr(iovad->fq);
 	spin_lock_irqsave(&fq->lock, flags);
 
 	/*
diff --git a/include/linux/dma-iommu.h b/include/linux/dma-iommu.h
index f454735beac5..426787bc1b92 100644
--- a/include/linux/dma-iommu.h
+++ b/include/linux/dma-iommu.h
@@ -31,6 +31,7 @@ void iommu_put_dma_cookie(struct iommu_domain *domain);
 
 /* Setup call for arch DMA mapping code */
 void iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 dma_limit);
+int iommu_dma_init_fq(struct iommu_domain *domain);
 
 /* The DMA API isn't _quite_ the whole story, though... */
 /*
@@ -65,6 +66,11 @@ static inline void iommu_setup_dma_ops(struct device *dev, u64 dma_base,
 {
 }
 
+static inline int iommu_dma_init_fq(struct iommu_domain *domain)
+{
+	return -EINVAL;
+}
+
 static inline int iommu_get_dma_cookie(struct iommu_domain *domain)
 {
 	return -ENODEV;
