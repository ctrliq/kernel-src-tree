RDMA/mlx5: Remove pd from struct mlx5_core_mkey

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit c64674168b6a2f293e92caf33c917ccf10886801
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/c6467416.failed

There is no read of mkey->pd, only write. Remove it.

	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Shay Drory <shayd@nvidia.com>
	Acked-by: Michael S. Tsirkin <mst@redhat.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
(cherry picked from commit c64674168b6a2f293e92caf33c917ccf10886801)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/devx.c
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/devx.c
index abfde896b9aa,5322d787c094..000000000000
--- a/drivers/infiniband/hw/mlx5/devx.c
+++ b/drivers/infiniband/hw/mlx5/devx.c
@@@ -1303,9 -1303,6 +1303,12 @@@ static int devx_handle_mkey_indirect(st
  	mkey->key = mlx5_idx_to_mkey(
  			MLX5_GET(create_mkey_out, out, mkey_index)) | key;
  	mkey->type = MLX5_MKEY_INDIRECT_DEVX;
++<<<<<<< HEAD
 +	mkey->iova = MLX5_GET64(mkc, mkc, start_addr);
 +	mkey->size = MLX5_GET64(mkc, mkc, len);
 +	mkey->pd = MLX5_GET(mkc, mkc, pd);
++=======
++>>>>>>> c64674168b6a (RDMA/mlx5: Remove pd from struct mlx5_core_mkey)
  	devx_mr->ndescs = MLX5_GET(mkc, mkc, translations_octword_size);
  	init_waitqueue_head(&mkey->wait);
  
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 4263bf96852f,9d7f1cadaa76..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -971,10 -975,8 +971,13 @@@ static struct mlx5_ib_mr *alloc_mr_from
  
  	mr->ibmr.pd = pd;
  	mr->umem = umem;
++<<<<<<< HEAD
 +	mr->mmkey.iova = iova;
 +	mr->mmkey.size = umem->length;
 +	mr->mmkey.pd = to_mpd(pd)->pdn;
++=======
++>>>>>>> c64674168b6a (RDMA/mlx5: Remove pd from struct mlx5_core_mkey)
  	mr->page_shift = order_base_2(page_size);
 -	set_mr_fields(dev, mr, umem->length, access_flags, iova);
  
  	return mr;
  }
@@@ -1588,27 -1669,114 +1591,101 @@@ static int revoke_mr(struct mlx5_ib_mr 
  	return mlx5_ib_post_send_wait(mr_to_mdev(mr), &umrwr);
  }
  
 -/*
 - * True if the change in access flags can be done via UMR, only some access
 - * flags can be updated.
 - */
 -static bool can_use_umr_rereg_access(struct mlx5_ib_dev *dev,
 -				     unsigned int current_access_flags,
 -				     unsigned int target_access_flags)
 +static int rereg_umr(struct ib_pd *pd, struct mlx5_ib_mr *mr,
 +		     int access_flags, int flags)
  {
 -	unsigned int diffs = current_access_flags ^ target_access_flags;
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct mlx5_umr_wr umrwr = {};
 +	int err;
  
 -	if (diffs & ~(IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE |
 -		      IB_ACCESS_REMOTE_READ | IB_ACCESS_RELAXED_ORDERING))
 -		return false;
 -	return mlx5_ib_can_reconfig_with_umr(dev, current_access_flags,
 -					     target_access_flags);
 -}
 +	umrwr.wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE;
  
 -static int umr_rereg_pd_access(struct mlx5_ib_mr *mr, struct ib_pd *pd,
 -			       int access_flags)
 -{
 -	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 -	struct mlx5_umr_wr umrwr = {
 -		.wr = {
 -			.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE |
 -				      MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS,
 -			.opcode = MLX5_IB_WR_UMR,
 -		},
 -		.mkey = mr->mmkey.key,
 -		.pd = pd,
 -		.access_flags = access_flags,
 -	};
 -	int err;
 +	umrwr.wr.opcode = MLX5_IB_WR_UMR;
 +	umrwr.mkey = mr->mmkey.key;
 +
 +	if (flags & IB_MR_REREG_PD || flags & IB_MR_REREG_ACCESS) {
 +		umrwr.pd = pd;
 +		umrwr.access_flags = access_flags;
 +		umrwr.wr.send_flags |= MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
 +	}
  
  	err = mlx5_ib_post_send_wait(dev, &umrwr);
 -	if (err)
 -		return err;
  
++<<<<<<< HEAD
 +	return err;
++=======
+ 	mr->access_flags = access_flags;
+ 	return 0;
+ }
+ 
+ static bool can_use_umr_rereg_pas(struct mlx5_ib_mr *mr,
+ 				  struct ib_umem *new_umem,
+ 				  int new_access_flags, u64 iova,
+ 				  unsigned long *page_size)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
+ 
+ 	/* We only track the allocated sizes of MRs from the cache */
+ 	if (!mr->cache_ent)
+ 		return false;
+ 	if (!mlx5_ib_can_load_pas_with_umr(dev, new_umem->length))
+ 		return false;
+ 
+ 	*page_size =
+ 		mlx5_umem_find_best_pgsz(new_umem, mkc, log_page_size, 0, iova);
+ 	if (WARN_ON(!*page_size))
+ 		return false;
+ 	return (1ULL << mr->cache_ent->order) >=
+ 	       ib_umem_num_dma_blocks(new_umem, *page_size);
+ }
+ 
+ static int umr_rereg_pas(struct mlx5_ib_mr *mr, struct ib_pd *pd,
+ 			 int access_flags, int flags, struct ib_umem *new_umem,
+ 			 u64 iova, unsigned long page_size)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
+ 	int upd_flags = MLX5_IB_UPD_XLT_ADDR | MLX5_IB_UPD_XLT_ENABLE;
+ 	struct ib_umem *old_umem = mr->umem;
+ 	int err;
+ 
+ 	/*
+ 	 * To keep everything simple the MR is revoked before we start to mess
+ 	 * with it. This ensure the change is atomic relative to any use of the
+ 	 * MR.
+ 	 */
+ 	err = revoke_mr(mr);
+ 	if (err)
+ 		return err;
+ 
+ 	if (flags & IB_MR_REREG_PD) {
+ 		mr->ibmr.pd = pd;
+ 		upd_flags |= MLX5_IB_UPD_XLT_PD;
+ 	}
+ 	if (flags & IB_MR_REREG_ACCESS) {
+ 		mr->access_flags = access_flags;
+ 		upd_flags |= MLX5_IB_UPD_XLT_ACCESS;
+ 	}
+ 
+ 	mr->ibmr.length = new_umem->length;
+ 	mr->ibmr.iova = iova;
+ 	mr->ibmr.length = new_umem->length;
+ 	mr->page_shift = order_base_2(page_size);
+ 	mr->umem = new_umem;
+ 	err = mlx5_ib_update_mr_pas(mr, upd_flags);
+ 	if (err) {
+ 		/*
+ 		 * The MR is revoked at this point so there is no issue to free
+ 		 * new_umem.
+ 		 */
+ 		mr->umem = old_umem;
+ 		return err;
+ 	}
+ 
+ 	atomic_sub(ib_umem_num_pages(old_umem), &dev->mdev->priv.reg_pages);
+ 	ib_umem_release(old_umem);
+ 	atomic_add(ib_umem_num_pages(new_umem), &dev->mdev->priv.reg_pages);
+ 	return 0;
++>>>>>>> c64674168b6a (RDMA/mlx5: Remove pd from struct mlx5_core_mkey)
  }
  
  struct ib_mr *mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
* Unmerged path drivers/infiniband/hw/mlx5/devx.c
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/mr.c b/drivers/net/ethernet/mellanox/mlx5/core/mr.c
index 174f71ed5280..d5b508a94681 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/mr.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mr.c
@@ -41,7 +41,6 @@ int mlx5_core_create_mkey(struct mlx5_core_dev *dev,
 {
 	u32 lout[MLX5_ST_SZ_DW(create_mkey_out)] = {};
 	u32 mkey_index;
-	void *mkc;
 	int err;
 
 	MLX5_SET(create_mkey_in, in, opcode, MLX5_CMD_OP_CREATE_MKEY);
@@ -50,12 +49,10 @@ int mlx5_core_create_mkey(struct mlx5_core_dev *dev,
 	if (err)
 		return err;
 
-	mkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);
 	mkey_index = MLX5_GET(create_mkey_out, lout, mkey_index);
 	mkey->iova = MLX5_GET64(mkc, mkc, start_addr);
 	mkey->size = MLX5_GET64(mkc, mkc, len);
 	mkey->key = (u32)mlx5_mkey_variant(mkey->key) | mlx5_idx_to_mkey(mkey_index);
-	mkey->pd = MLX5_GET(mkc, mkc, pd);
 	init_waitqueue_head(&mkey->wait);
 
 	mlx5_core_dbg(dev, "out 0x%x, mkey 0x%x\n", mkey_index, mkey->key);
diff --git a/drivers/vdpa/mlx5/core/resources.c b/drivers/vdpa/mlx5/core/resources.c
index 15e266d0e27a..ff51858f69a6 100644
--- a/drivers/vdpa/mlx5/core/resources.c
+++ b/drivers/vdpa/mlx5/core/resources.c
@@ -203,7 +203,6 @@ int mlx5_vdpa_create_mkey(struct mlx5_vdpa_dev *mvdev, struct mlx5_core_mkey *mk
 {
 	u32 lout[MLX5_ST_SZ_DW(create_mkey_out)] = {};
 	u32 mkey_index;
-	void *mkc;
 	int err;
 
 	MLX5_SET(create_mkey_in, in, opcode, MLX5_CMD_OP_CREATE_MKEY);
@@ -213,12 +212,10 @@ int mlx5_vdpa_create_mkey(struct mlx5_vdpa_dev *mvdev, struct mlx5_core_mkey *mk
 	if (err)
 		return err;
 
-	mkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);
 	mkey_index = MLX5_GET(create_mkey_out, lout, mkey_index);
 	mkey->iova = MLX5_GET64(mkc, mkc, start_addr);
 	mkey->size = MLX5_GET64(mkc, mkc, len);
 	mkey->key |= mlx5_idx_to_mkey(mkey_index);
-	mkey->pd = MLX5_GET(mkc, mkc, pd);
 	return 0;
 }
 
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index 010ada427bed..c00fc292812d 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -366,7 +366,6 @@ struct mlx5_core_mkey {
 	u64			iova;
 	u64			size;
 	u32			key;
-	u32			pd;
 	u32			type;
 	struct wait_queue_head wait;
 	refcount_t usecount;
