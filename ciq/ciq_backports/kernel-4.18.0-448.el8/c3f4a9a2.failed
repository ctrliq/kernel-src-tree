mm/vmscan: centralise timeout values for reclaim_throttle

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Mel Gorman <mgorman@techsingularity.net>
commit c3f4a9a2b082c5392fbff17c6d8551154add5fdb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/c3f4a9a2.failed

Neil Brown raised concerns about callers of reclaim_throttle specifying
a timeout value.  The original timeout values to congestion_wait() were
probably pulled out of thin air or copy&pasted from somewhere else.
This patch centralises the timeout values and selects a timeout based on
the reason for reclaim throttling.  These figures are also pulled out of
the same thin air but better values may be derived

Running a workload that is throttling for inappropriate periods and
tracing mm_vmscan_throttled can be used to pick a more appropriate
value.  Excessive throttling would pick a lower timeout where as
excessive CPU usage in reclaim context would select a larger timeout.
Ideally a large value would always be used and the wakeups would occur
before a timeout but that requires careful testing.

Link: https://lkml.kernel.org/r/20211022144651.19914-7-mgorman@techsingularity.net
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Andreas Dilger <adilger.kernel@dilger.ca>
	Cc: "Darrick J . Wong" <djwong@kernel.org>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: NeilBrown <neilb@suse.de>
	Cc: Rik van Riel <riel@surriel.com>
	Cc: "Theodore Ts'o" <tytso@mit.edu>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit c3f4a9a2b082c5392fbff17c6d8551154add5fdb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/compaction.c
#	mm/internal.h
#	mm/vmscan.c
diff --cc mm/compaction.c
index 2c3b74d50487,151b04c4dab3..000000000000
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@@ -824,12 -826,12 +824,16 @@@ isolate_migratepages_block(struct compa
  
  		/* async migration should just abort */
  		if (cc->mode == MIGRATE_ASYNC)
 -			return -EAGAIN;
 +			return 0;
  
++<<<<<<< HEAD
 +		congestion_wait(BLK_RW_ASYNC, HZ/10);
++=======
+ 		reclaim_throttle(pgdat, VMSCAN_THROTTLE_ISOLATED);
++>>>>>>> c3f4a9a2b082 (mm/vmscan: centralise timeout values for reclaim_throttle)
  
  		if (fatal_signal_pending(current))
 -			return -EINTR;
 +			return 0;
  	}
  
  	cond_resched();
diff --cc mm/internal.h
index 544ab31e6706,f3de3a2f3e30..000000000000
--- a/mm/internal.h
+++ b/mm/internal.h
@@@ -116,6 -130,7 +116,10 @@@ extern unsigned long highest_memmap_pfn
   */
  extern int isolate_lru_page(struct page *page);
  extern void putback_lru_page(struct page *page);
++<<<<<<< HEAD
++=======
+ extern void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason);
++>>>>>>> c3f4a9a2b082 (mm/vmscan: centralise timeout values for reclaim_throttle)
  
  /*
   * in mm/rmap.c:
diff --cc mm/vmscan.c
index c09c607fa724,599e5616b123..000000000000
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@@ -981,6 -1006,91 +981,94 @@@ static void handle_write_error(struct a
  	unlock_page(page);
  }
  
++<<<<<<< HEAD
++=======
+ void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason)
+ {
+ 	wait_queue_head_t *wqh = &pgdat->reclaim_wait[reason];
+ 	long timeout, ret;
+ 	DEFINE_WAIT(wait);
+ 
+ 	/*
+ 	 * Do not throttle IO workers, kthreads other than kswapd or
+ 	 * workqueues. They may be required for reclaim to make
+ 	 * forward progress (e.g. journalling workqueues or kthreads).
+ 	 */
+ 	if (!current_is_kswapd() &&
+ 	    current->flags & (PF_IO_WORKER|PF_KTHREAD))
+ 		return;
+ 
+ 	/*
+ 	 * These figures are pulled out of thin air.
+ 	 * VMSCAN_THROTTLE_ISOLATED is a transient condition based on too many
+ 	 * parallel reclaimers which is a short-lived event so the timeout is
+ 	 * short. Failing to make progress or waiting on writeback are
+ 	 * potentially long-lived events so use a longer timeout. This is shaky
+ 	 * logic as a failure to make progress could be due to anything from
+ 	 * writeback to a slow device to excessive references pages at the tail
+ 	 * of the inactive LRU.
+ 	 */
+ 	switch(reason) {
+ 	case VMSCAN_THROTTLE_WRITEBACK:
+ 		timeout = HZ/10;
+ 
+ 		if (atomic_inc_return(&pgdat->nr_writeback_throttled) == 1) {
+ 			WRITE_ONCE(pgdat->nr_reclaim_start,
+ 				node_page_state(pgdat, NR_THROTTLED_WRITTEN));
+ 		}
+ 
+ 		break;
+ 	case VMSCAN_THROTTLE_NOPROGRESS:
+ 		timeout = HZ/10;
+ 		break;
+ 	case VMSCAN_THROTTLE_ISOLATED:
+ 		timeout = HZ/50;
+ 		break;
+ 	default:
+ 		WARN_ON_ONCE(1);
+ 		timeout = HZ;
+ 		break;
+ 	}
+ 
+ 	prepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);
+ 	ret = schedule_timeout(timeout);
+ 	finish_wait(wqh, &wait);
+ 
+ 	if (reason == VMSCAN_THROTTLE_WRITEBACK)
+ 		atomic_dec(&pgdat->nr_writeback_throttled);
+ 
+ 	trace_mm_vmscan_throttled(pgdat->node_id, jiffies_to_usecs(timeout),
+ 				jiffies_to_usecs(timeout - ret),
+ 				reason);
+ }
+ 
+ /*
+  * Account for pages written if tasks are throttled waiting on dirty
+  * pages to clean. If enough pages have been cleaned since throttling
+  * started then wakeup the throttled tasks.
+  */
+ void __acct_reclaim_writeback(pg_data_t *pgdat, struct page *page,
+ 							int nr_throttled)
+ {
+ 	unsigned long nr_written;
+ 
+ 	inc_node_page_state(page, NR_THROTTLED_WRITTEN);
+ 
+ 	/*
+ 	 * This is an inaccurate read as the per-cpu deltas may not
+ 	 * be synchronised. However, given that the system is
+ 	 * writeback throttled, it is not worth taking the penalty
+ 	 * of getting an accurate count. At worst, the throttle
+ 	 * timeout guarantees forward progress.
+ 	 */
+ 	nr_written = node_page_state(pgdat, NR_THROTTLED_WRITTEN) -
+ 		READ_ONCE(pgdat->nr_reclaim_start);
+ 
+ 	if (nr_written > SWAP_CLUSTER_MAX * nr_throttled)
+ 		wake_up(&pgdat->reclaim_wait[VMSCAN_THROTTLE_WRITEBACK]);
+ }
+ 
++>>>>>>> c3f4a9a2b082 (mm/vmscan: centralise timeout values for reclaim_throttle)
  /* possible outcome of pageout() */
  typedef enum {
  	/* failed to write page out, page is locked */
@@@ -2142,8 -2341,8 +2230,12 @@@ shrink_inactive_list(unsigned long nr_t
  			return 0;
  
  		/* wait a bit for the reclaimer. */
 +		msleep(100);
  		stalled = true;
++<<<<<<< HEAD
++=======
+ 		reclaim_throttle(pgdat, VMSCAN_THROTTLE_ISOLATED);
++>>>>>>> c3f4a9a2b082 (mm/vmscan: centralise timeout values for reclaim_throttle)
  
  		/* We are about to die and free our memory. Return now. */
  		if (fatal_signal_pending(current))
@@@ -3035,10 -3270,11 +3127,14 @@@ again
  		 * If kswapd scans pages marked for immediate
  		 * reclaim and under writeback (nr_immediate), it
  		 * implies that pages are cycling through the LRU
 -		 * faster than they are written so forcibly stall
 -		 * until some pages complete writeback.
 +		 * faster than they are written so also forcibly stall.
  		 */
  		if (sc->nr.immediate)
++<<<<<<< HEAD
 +			congestion_wait(BLK_RW_ASYNC, HZ/10);
++=======
+ 			reclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);
++>>>>>>> c3f4a9a2b082 (mm/vmscan: centralise timeout values for reclaim_throttle)
  	}
  
  	/*
@@@ -3063,7 -3298,7 +3159,11 @@@
  	if (!current_is_kswapd() && current_may_throttle() &&
  	    !sc->hibernation_mode &&
  	    test_bit(LRUVEC_CONGESTED, &target_lruvec->flags))
++<<<<<<< HEAD
 +		wait_iff_congested(BLK_RW_ASYNC, HZ/10);
++=======
+ 		reclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);
++>>>>>>> c3f4a9a2b082 (mm/vmscan: centralise timeout values for reclaim_throttle)
  
  	if (should_continue_reclaim(pgdat, sc->nr_reclaimed - nr_reclaimed,
  				    sc))
@@@ -3111,6 -3346,33 +3211,36 @@@ static inline bool compaction_ready(str
  	return zone_watermark_ok_safe(zone, 0, watermark, sc->reclaim_idx);
  }
  
++<<<<<<< HEAD
++=======
+ static void consider_reclaim_throttle(pg_data_t *pgdat, struct scan_control *sc)
+ {
+ 	/* If reclaim is making progress, wake any throttled tasks. */
+ 	if (sc->nr_reclaimed) {
+ 		wait_queue_head_t *wqh;
+ 
+ 		wqh = &pgdat->reclaim_wait[VMSCAN_THROTTLE_NOPROGRESS];
+ 		if (waitqueue_active(wqh))
+ 			wake_up(wqh);
+ 
+ 		return;
+ 	}
+ 
+ 	/*
+ 	 * Do not throttle kswapd on NOPROGRESS as it will throttle on
+ 	 * VMSCAN_THROTTLE_WRITEBACK if there are too many pages under
+ 	 * writeback and marked for immediate reclaim at the tail of
+ 	 * the LRU.
+ 	 */
+ 	if (current_is_kswapd())
+ 		return;
+ 
+ 	/* Throttle if making no progress at high prioities. */
+ 	if (sc->priority < DEF_PRIORITY - 2)
+ 		reclaim_throttle(pgdat, VMSCAN_THROTTLE_NOPROGRESS);
+ }
+ 
++>>>>>>> c3f4a9a2b082 (mm/vmscan: centralise timeout values for reclaim_throttle)
  /*
   * This is the direct reclaim path, for page-allocating processes.  We only
   * try to reclaim pages from zones which will satisfy the caller's allocation
* Unmerged path mm/compaction.c
* Unmerged path mm/internal.h
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 0bd44c63ed11..c8272fb1c90c 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -2391,7 +2391,7 @@ int do_writepages(struct address_space *mapping, struct writeback_control *wbc)
 		 * guess as any.
 		 */
 		reclaim_throttle(NODE_DATA(numa_node_id()),
-			VMSCAN_THROTTLE_WRITEBACK, HZ/50);
+			VMSCAN_THROTTLE_WRITEBACK);
 	}
 	/*
 	 * Usually few pages are written by now from those we've just submitted
* Unmerged path mm/vmscan.c
