mm/rmap: Fix typos in comments

jira LE-1907
cve CVE-2022-42703
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Adrian Huang <ahuang12@lenovo.com>
commit dd0623020e0d068d5eba0c37d5ae1277800b49c4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/dd062302.failed

Fix spelling/grammar mistakes in comments.

Link: https://lkml.kernel.org/r/20220428061522.666-1-adrianhuang0701@gmail.com
	Signed-off-by: Adrian Huang <ahuang12@lenovo.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit dd0623020e0d068d5eba0c37d5ae1277800b49c4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/rmap.c
diff --cc mm/rmap.c
index 56ff20aece75,72464378e1a6..000000000000
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@@ -1808,29 -2248,58 +1808,38 @@@ bool try_to_unmap(struct page *page, en
  }
  
  /**
++<<<<<<< HEAD
 + * try_to_munlock - try to munlock a page
 + * @page: the page to be munlocked
++=======
+  * make_device_exclusive_range() - Mark a range for exclusive use by a device
+  * @mm: mm_struct of associated target process
+  * @start: start of the region to mark for exclusive device access
+  * @end: end address of region
+  * @pages: returns the pages which were successfully marked for exclusive access
+  * @owner: passed to MMU_NOTIFY_EXCLUSIVE range notifier to allow filtering
++>>>>>>> dd0623020e0d (mm/rmap: Fix typos in comments)
   *
 - * Returns: number of pages found in the range by GUP. A page is marked for
 - * exclusive access only if the page pointer is non-NULL.
 - *
 - * This function finds ptes mapping page(s) to the given address range, locks
 - * them and replaces mappings with special swap entries preventing userspace CPU
 - * access. On fault these entries are replaced with the original mapping after
 - * calling MMU notifiers.
 - *
 - * A driver using this to program access from a device must use a mmu notifier
 - * critical section to hold a device specific lock during programming. Once
 - * programming is complete it should drop the page lock and reference after
 - * which point CPU access to the page will revoke the exclusive access.
 + * Called from munlock code.  Checks all of the VMAs mapping the page
 + * to make sure nobody else has this page mlocked. The page will be
 + * returned with PG_mlocked cleared if no other vmas have it mlocked.
   */
 -int make_device_exclusive_range(struct mm_struct *mm, unsigned long start,
 -				unsigned long end, struct page **pages,
 -				void *owner)
 +
 +void try_to_munlock(struct page *page)
  {
 -	long npages = (end - start) >> PAGE_SHIFT;
 -	long i;
 -
 -	npages = get_user_pages_remote(mm, start, npages,
 -				       FOLL_GET | FOLL_WRITE | FOLL_SPLIT_PMD,
 -				       pages, NULL, NULL);
 -	if (npages < 0)
 -		return npages;
 -
 -	for (i = 0; i < npages; i++, start += PAGE_SIZE) {
 -		struct folio *folio = page_folio(pages[i]);
 -		if (PageTail(pages[i]) || !folio_trylock(folio)) {
 -			folio_put(folio);
 -			pages[i] = NULL;
 -			continue;
 -		}
 +	struct rmap_walk_control rwc = {
 +		.rmap_one = try_to_unmap_one,
 +		.arg = (void *)TTU_MUNLOCK,
 +		.done = page_not_mapped,
 +		.anon_lock = page_lock_anon_vma_read,
  
 -		if (!folio_make_device_exclusive(folio, mm, start, owner)) {
 -			folio_unlock(folio);
 -			folio_put(folio);
 -			pages[i] = NULL;
 -		}
 -	}
 +	};
 +
 +	VM_BUG_ON_PAGE(!PageLocked(page) || PageLRU(page), page);
 +	VM_BUG_ON_PAGE(PageCompound(page) && PageDoubleMap(page), page);
  
 -	return npages;
 +	rmap_walk(page, &rwc);
  }
 -EXPORT_SYMBOL_GPL(make_device_exclusive_range);
 -#endif
  
  void __put_anon_vma(struct anon_vma *anon_vma)
  {
* Unmerged path mm/rmap.c
