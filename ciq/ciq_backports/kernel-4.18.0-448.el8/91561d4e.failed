swiotlb: remove unused fields in io_tlb_mem

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Chao Gao <chao.gao@intel.com>
commit 91561d4ecb755f056f8ff04f9dcaec210140e55c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/91561d4e.failed

Commit 20347fca71a3 ("swiotlb: split up the global swiotlb lock") splits
io_tlb_mem into multiple areas. Each area has its own lock and index. The
global ones are not used so remove them.

	Signed-off-by: Chao Gao <chao.gao@intel.com>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
(cherry picked from commit 91561d4ecb755f056f8ff04f9dcaec210140e55c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index 623d86fa3853,cbffa0b1ace5..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -203,13 -253,17 +203,20 @@@ static void swiotlb_init_io_tlb_mem(str
  	mem->nslabs = nslabs;
  	mem->start = start;
  	mem->end = mem->start + bytes;
- 	mem->index = 0;
  	mem->late_alloc = late_alloc;
 -	mem->nareas = nareas;
 -	mem->area_nslabs = nslabs / mem->nareas;
  
 -	mem->force_bounce = swiotlb_force_bounce || (flags & SWIOTLB_FORCE);
 +	if (swiotlb_force == SWIOTLB_FORCE)
 +		mem->force_bounce = true;
  
++<<<<<<< HEAD
 +	spin_lock_init(&mem->lock);
++=======
+ 	for (i = 0; i < mem->nareas; i++) {
+ 		spin_lock_init(&mem->areas[i].lock);
+ 		mem->areas[i].index = 0;
+ 	}
+ 
++>>>>>>> 91561d4ecb75 (swiotlb: remove unused fields in io_tlb_mem)
  	for (i = 0; i < mem->nslabs; i++) {
  		mem->slots[i].list = IO_TLB_SEGSIZE - io_tlb_offset(i);
  		mem->slots[i].orig_addr = INVALID_PHYS_ADDR;
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index 2d8f1b3753d4..4b7e797badab 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -81,11 +81,8 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t phys,
  * @used:	The number of used IO TLB block.
  * @list:	The free list describing the number of free entries available
  *		from each index.
- * @index:	The index to start searching in the next round.
  * @orig_addr:	The original address corresponding to a mapped entry.
  * @alloc_size:	Size of the allocated buffer.
- * @lock:	The lock to protect the above data structures in the map and
- *		unmap calls.
  * @debugfs:	The dentry to debugfs.
  * @late_alloc:	%true if allocated using the page allocator
  * @force_bounce: %true if swiotlb bouncing is forced
@@ -97,8 +94,6 @@ struct io_tlb_mem {
 	void *vaddr;
 	unsigned long nslabs;
 	unsigned long used;
-	unsigned int index;
-	spinlock_t lock;
 	struct dentry *debugfs;
 	bool late_alloc;
 	bool force_bounce;
* Unmerged path kernel/dma/swiotlb.c
