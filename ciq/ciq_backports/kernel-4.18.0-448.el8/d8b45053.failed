tlb: mmu_gather: Introduce tlb_gather_mmu_fullmm()

jira LE-1907
cve CVE-2022-39188
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Will Deacon <will@kernel.org>
commit d8b450530b90f8845ab962af18b8a10ed77fc889
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/d8b45053.failed

Passing the range '0, -1' to tlb_gather_mmu() sets the 'fullmm' flag,
which indicates that the mm_struct being operated on is going away. In
this case, some architectures (such as arm64) can elide TLB invalidation
by ensuring that the TLB tag (ASID) associated with this mm is not
immediately reclaimed. Although this behaviour is documented in
asm-generic/tlb.h, it's subtle and easily missed.

Introduce tlb_gather_mmu_fullmm() to make it clearer that this is for the
entire mm and WARN() if tlb_gather_mmu() is called with the 'fullmm'
address range.

	Signed-off-by: Will Deacon <will@kernel.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Yu Zhao <yuzhao@google.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
Link: https://lkml.kernel.org/r/20210127235347.1402-4-will@kernel.org
(cherry picked from commit d8b450530b90f8845ab962af18b8a10ed77fc889)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm_types.h
diff --cc include/linux/mm_types.h
index 5c430882072e,e49868bc12a7..000000000000
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@@ -631,8 -590,8 +631,13 @@@ static inline cpumask_t *mm_cpumask(str
  struct mmu_gather;
  extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
  				unsigned long start, unsigned long end);
++<<<<<<< HEAD
 +extern void tlb_finish_mmu(struct mmu_gather *tlb,
 +				unsigned long start, unsigned long end);
++=======
+ extern void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm);
+ extern void tlb_finish_mmu(struct mmu_gather *tlb);
++>>>>>>> d8b450530b90 (tlb: mmu_gather: Introduce tlb_gather_mmu_fullmm())
  
  static inline void init_tlb_flush_pending(struct mm_struct *mm)
  {
diff --git a/include/asm-generic/tlb.h b/include/asm-generic/tlb.h
index 27ac8002fb01..bf854eaa47d6 100644
--- a/include/asm-generic/tlb.h
+++ b/include/asm-generic/tlb.h
@@ -50,7 +50,9 @@
  *
  * The mmu_gather API consists of:
  *
- *  - tlb_gather_mmu() / tlb_finish_mmu(); start and finish a mmu_gather
+ *  - tlb_gather_mmu() / tlb_gather_mmu_fullmm() / tlb_finish_mmu()
+ *
+ *    start and finish a mmu_gather
  *
  *    Finish in particular will issue a (final) TLB invalidate and free
  *    all (remaining) queued pages.
@@ -86,7 +88,7 @@
  *
  *  - mmu_gather::fullmm
  *
- *    A flag set by tlb_gather_mmu() to indicate we're going to free
+ *    A flag set by tlb_gather_mmu_fullmm() to indicate we're going to free
  *    the entire mm; this allows a number of optimizations.
  *
  *    - We can ignore tlb_{start,end}_vma(); because we don't
* Unmerged path include/linux/mm_types.h
diff --git a/mm/mmap.c b/mm/mmap.c
index 878265a8a575..78d9158dda47 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -3143,7 +3143,7 @@ void exit_mmap(struct mm_struct *mm)
 
 	lru_add_drain();
 	flush_cache_mm(mm);
-	tlb_gather_mmu(&tlb, mm, 0, -1);
+	tlb_gather_mmu_fullmm(&tlb, mm);
 	/* update_hiwater_rss(mm) here? but nobody should be looking */
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&tlb, vma, 0, -1);
diff --git a/mm/mmu_gather.c b/mm/mmu_gather.c
index b84402038601..a37fe7028533 100644
--- a/mm/mmu_gather.c
+++ b/mm/mmu_gather.c
@@ -203,8 +203,8 @@ void tlb_flush_mmu(struct mmu_gather *tlb)
  * respectively when @mm is without users and we're going to destroy
  * the full address space (exit/execve).
  */
-void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
-			unsigned long start, unsigned long end)
+static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
+			     unsigned long start, unsigned long end)
 {
 	tlb->mm = mm;
 
@@ -231,6 +231,18 @@ void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 	inc_tlb_flush_pending(tlb->mm);
 }
 
+void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
+		    unsigned long start, unsigned long end)
+{
+	WARN_ON(!(start | (end + 1))); /* Use _fullmm() instead */
+	__tlb_gather_mmu(tlb, mm, start, end);
+}
+
+void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)
+{
+	__tlb_gather_mmu(tlb, mm, 0, -1);
+}
+
 /**
  * tlb_finish_mmu - finish an mmu_gather structure
  * @tlb: the mmu_gather structure to finish
