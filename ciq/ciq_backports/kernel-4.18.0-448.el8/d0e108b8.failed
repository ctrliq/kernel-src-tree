iommu/vt-d: Add support for IOMMU default DMA mode build options

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Zhen Lei <thunder.leizhen@huawei.com>
commit d0e108b8e962e0aae65dc74ffda63b93b6ef32f4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/d0e108b8.failed

Make IOMMU_DEFAULT_LAZY default for when INTEL_IOMMU config is set,
as is current behaviour.

Also delete global flag intel_iommu_strict:
- In intel_iommu_setup(), call iommu_set_dma_strict(true) directly. Also
  remove the print, as iommu_subsys_init() prints the mode and we have
  already marked this param as deprecated.

- For cap_caching_mode() check in intel_iommu_setup(), call
  iommu_set_dma_strict(true) directly; also reword the accompanying print
  with a level downgrade and also add the missing '\n'.

- For Ironlake GPU, again call iommu_set_dma_strict(true) directly and
  keep the accompanying print.

[jpg: Remove intel_iommu_strict]

	Signed-off-by: Zhen Lei <thunder.leizhen@huawei.com>
	Signed-off-by: John Garry <john.garry@huawei.com>
	Reviewed-by: Lu Baolu <baolu.lu@linux.intel.com>
Link: https://lore.kernel.org/r/1626088340-5838-5-git-send-email-john.garry@huawei.com
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit d0e108b8e962e0aae65dc74ffda63b93b6ef32f4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/Kconfig
diff --cc drivers/iommu/Kconfig
index bead5de04ae1,265d7a6c9d3a..000000000000
--- a/drivers/iommu/Kconfig
+++ b/drivers/iommu/Kconfig
@@@ -89,6 -90,47 +89,50 @@@ config IOMMU_DEFAULT_PASSTHROUG
  
  	  If unsure, say N here.
  
++<<<<<<< HEAD
++=======
+ choice
+ 	prompt "IOMMU default DMA IOTLB invalidation mode"
+ 	depends on IOMMU_DMA
+ 
+ 	default IOMMU_DEFAULT_LAZY if INTEL_IOMMU
+ 	default IOMMU_DEFAULT_STRICT
+ 	help
+ 	  This option allows an IOMMU DMA IOTLB invalidation mode to be
+ 	  chosen at build time, to override the default mode of each ARCH,
+ 	  removing the need to pass in kernel parameters through command line.
+ 	  It is still possible to provide common boot params to override this
+ 	  config.
+ 
+ 	  If unsure, keep the default.
+ 
+ config IOMMU_DEFAULT_STRICT
+ 	bool "strict"
+ 	help
+ 	  For every IOMMU DMA unmap operation, the flush operation of IOTLB and
+ 	  the free operation of IOVA are guaranteed to be done in the unmap
+ 	  function.
+ 
+ config IOMMU_DEFAULT_LAZY
+ 	bool "lazy"
+ 	help
+ 	  Support lazy mode, where for every IOMMU DMA unmap operation, the
+ 	  flush operation of IOTLB and the free operation of IOVA are deferred.
+ 	  They are only guaranteed to be done before the related IOVA will be
+ 	  reused.
+ 
+ 	  The isolation provided in this mode is not as secure as STRICT mode,
+ 	  such that a vulnerable time window may be created between the DMA
+ 	  unmap and the mappings cached in the IOMMU IOTLB or device TLB
+ 	  finally being invalidated, where the device could still access the
+ 	  memory which has already been unmapped by the device driver.
+ 	  However this mode may provide better performance in high throughput
+ 	  scenarios, and is still considerably more secure than passthrough
+ 	  mode or no IOMMU.
+ 
+ endchoice
+ 
++>>>>>>> d0e108b8e962 (iommu/vt-d: Add support for IOMMU default DMA mode build options)
  config OF_IOMMU
  	def_bool y
  	depends on OF && IOMMU_API
* Unmerged path drivers/iommu/Kconfig
diff --git a/drivers/iommu/intel/iommu.c b/drivers/iommu/intel/iommu.c
index 46171dfe60a5..2e3bfcb55887 100644
--- a/drivers/iommu/intel/iommu.c
+++ b/drivers/iommu/intel/iommu.c
@@ -355,7 +355,6 @@ int intel_iommu_enabled = 0;
 EXPORT_SYMBOL_GPL(intel_iommu_enabled);
 
 static int dmar_map_gfx = 1;
-static int intel_iommu_strict;
 static int intel_iommu_superpage = 1;
 static int iommu_identity_mapping;
 static int iommu_skip_te_disable;
@@ -449,8 +448,7 @@ static int __init intel_iommu_setup(char *str)
 			iommu_dma_forcedac = true;
 		} else if (!strncmp(str, "strict", 6)) {
 			pr_warn("intel_iommu=strict deprecated; use iommu.strict=1 instead\n");
-			pr_info("Disable batched IOTLB flush\n");
-			intel_iommu_strict = 1;
+			iommu_set_dma_strict(true);
 		} else if (!strncmp(str, "sp_off", 6)) {
 			pr_info("Disable supported super page\n");
 			intel_iommu_superpage = 0;
@@ -4562,9 +4560,9 @@ int __init intel_iommu_init(void)
 		 * is likely to be much lower than the overhead of synchronizing
 		 * the virtual and physical IOMMU page-tables.
 		 */
-		if (!intel_iommu_strict && cap_caching_mode(iommu->cap)) {
-			pr_warn("IOMMU batching is disabled due to virtualization");
-			intel_iommu_strict = 1;
+		if (cap_caching_mode(iommu->cap)) {
+			pr_info_once("IOMMU batching disallowed due to virtualization\n");
+			iommu_set_dma_strict(true);
 		}
 		iommu_device_sysfs_add(&iommu->iommu, NULL,
 				       intel_iommu_groups,
@@ -4573,7 +4571,6 @@ int __init intel_iommu_init(void)
 	}
 	up_read(&dmar_global_lock);
 
-	iommu_set_dma_strict(intel_iommu_strict);
 	bus_set_iommu(&pci_bus_type, &intel_iommu_ops);
 	if (si_domain && !hw_pass_through)
 		register_memory_notifier(&intel_iommu_memory_nb);
@@ -5827,8 +5824,8 @@ static void quirk_calpella_no_shadow_gtt(struct pci_dev *dev)
 	} else if (dmar_map_gfx) {
 		/* we have to ensure the gfx device is idle before we flush */
 		pci_info(dev, "Disabling batched IOTLB flush on Ironlake\n");
-		intel_iommu_strict = 1;
-       }
+		iommu_set_dma_strict(true);
+	}
 }
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x0040, quirk_calpella_no_shadow_gtt);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_INTEL, 0x0044, quirk_calpella_no_shadow_gtt);
