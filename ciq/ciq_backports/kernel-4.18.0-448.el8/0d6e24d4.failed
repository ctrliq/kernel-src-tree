asm-generic/tlb: provide MMU_GATHER_TABLE_FREE

jira LE-1907
cve CVE-2022-39188
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 0d6e24d430ef23280d8dea0ba1faeefc66c26a57
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/0d6e24d4.failed

As described in the comment, the correct order for freeing pages is:

 1) unhook page
 2) TLB invalidate page
 3) free page

This order equally applies to page directories.

Currently there are two correct options:

 - use tlb_remove_page(), when all page directores are full pages and
   there are no futher contraints placed by things like software
   walkers (HAVE_FAST_GUP).

 - use MMU_GATHER_RCU_TABLE_FREE and tlb_remove_table() when the
   architecture does not do IPI based TLB invalidate and has
   HAVE_FAST_GUP (or software TLB fill).

This however leaves architectures that don't have page based directories
but don't need RCU in a bind.  For those, provide MMU_GATHER_TABLE_FREE,
which provides the independent batching for directories without the
additional RCU freeing.

Link: http://lkml.kernel.org/r/20200116064531.483522-10-aneesh.kumar@linux.ibm.com
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 0d6e24d430ef23280d8dea0ba1faeefc66c26a57)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/Kconfig
#	arch/arm/include/asm/tlb.h
#	include/asm-generic/tlb.h
#	mm/mmu_gather.c
diff --cc arch/Kconfig
index 1e486125c2eb,98de654b79b3..000000000000
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@@ -373,25 -393,23 +373,34 @@@ config HAVE_ARCH_JUMP_LABE
  config HAVE_ARCH_JUMP_LABEL_RELATIVE
  	bool
  
++<<<<<<< HEAD
 +config HAVE_RCU_TABLE_FREE
++=======
+ config MMU_GATHER_TABLE_FREE
  	bool
  
+ config MMU_GATHER_RCU_TABLE_FREE
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
+ 	bool
+ 	select MMU_GATHER_TABLE_FREE
+ 
 -config MMU_GATHER_PAGE_SIZE
 +config HAVE_MMU_GATHER_PAGE_SIZE
  	bool
  
  config MMU_GATHER_NO_RANGE
  	bool
  
 -config MMU_GATHER_NO_GATHER
 +config HAVE_MMU_GATHER_NO_GATHER
  	bool
+ 	depends on MMU_GATHER_TABLE_FREE
  
 +config ARCH_WANT_IRQS_OFF_ACTIVATE_MM
 +	bool
 +	help
 +	  Temporary select until all architectures can be converted to have
 +	  irqs disabled over activate_mm. Architectures that do IPI based TLB
 +	  shootdowns should enable this.
 +
  config ARCH_HAVE_NMI_SAFE_CMPXCHG
  	bool
  
diff --cc arch/arm/include/asm/tlb.h
index 27d6bf4347d5,4d4e7b6aabff..000000000000
--- a/arch/arm/include/asm/tlb.h
+++ b/arch/arm/include/asm/tlb.h
@@@ -41,218 -35,14 +41,221 @@@ static inline void __tlb_remove_table(v
  	free_page_and_swap_cache((struct page *)_table);
  }
  
 -#include <asm-generic/tlb.h>
 +struct mmu_table_batch {
 +	struct rcu_head		rcu;
 +	unsigned int		nr;
 +	void			*tables[0];
 +};
 +
++<<<<<<< HEAD
 +#define MAX_TABLE_BATCH		\
 +	((PAGE_SIZE - sizeof(struct mmu_table_batch)) / sizeof(void *))
 +
 +extern void tlb_table_flush(struct mmu_gather *tlb);
 +extern void tlb_remove_table(struct mmu_gather *tlb, void *table);
 +
 +#define tlb_remove_entry(tlb, entry)	tlb_remove_table(tlb, entry)
 +#else
 +#define tlb_remove_entry(tlb, entry)	tlb_remove_page(tlb, entry)
 +#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
 +
 +/*
 + * TLB handling.  This allows us to remove pages from the page
 + * tables, and efficiently handle the TLB issues.
 + */
 +struct mmu_gather {
 +	struct mm_struct	*mm;
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	struct mmu_table_batch	*batch;
 +	unsigned int		need_flush;
 +#endif
 +	unsigned int		fullmm;
 +	struct vm_area_struct	*vma;
 +	unsigned long		start, end;
 +	unsigned long		range_start;
 +	unsigned long		range_end;
 +	unsigned int		nr;
 +	unsigned int		max;
 +	struct page		**pages;
 +	struct page		*local[MMU_GATHER_BUNDLE];
 +};
 +
 +DECLARE_PER_CPU(struct mmu_gather, mmu_gathers);
  
 +/*
 + * This is unnecessarily complex.  There's three ways the TLB shootdown
 + * code is used:
 + *  1. Unmapping a range of vmas.  See zap_page_range(), unmap_region().
 + *     tlb->fullmm = 0, and tlb_start_vma/tlb_end_vma will be called.
 + *     tlb->vma will be non-NULL.
 + *  2. Unmapping all vmas.  See exit_mmap().
 + *     tlb->fullmm = 1, and tlb_start_vma/tlb_end_vma will be called.
 + *     tlb->vma will be non-NULL.  Additionally, page tables will be freed.
 + *  3. Unmapping argument pages.  See shift_arg_pages().
 + *     tlb->fullmm = 0, but tlb_start_vma/tlb_end_vma will not be called.
 + *     tlb->vma will be NULL.
 + */
 +static inline void tlb_flush(struct mmu_gather *tlb)
 +{
 +	if (tlb->fullmm || !tlb->vma)
 +		flush_tlb_mm(tlb->mm);
 +	else if (tlb->range_end > 0) {
 +		flush_tlb_range(tlb->vma, tlb->range_start, tlb->range_end);
 +		tlb->range_start = TASK_SIZE;
 +		tlb->range_end = 0;
 +	}
 +}
 +
 +static inline void tlb_add_flush(struct mmu_gather *tlb, unsigned long addr)
 +{
 +	if (!tlb->fullmm) {
 +		if (addr < tlb->range_start)
 +			tlb->range_start = addr;
 +		if (addr + PAGE_SIZE > tlb->range_end)
 +			tlb->range_end = addr + PAGE_SIZE;
 +	}
 +}
 +
 +static inline void __tlb_alloc_page(struct mmu_gather *tlb)
 +{
 +	unsigned long addr = __get_free_pages(GFP_NOWAIT | __GFP_NOWARN, 0);
 +
 +	if (addr) {
 +		tlb->pages = (void *)addr;
 +		tlb->max = PAGE_SIZE / sizeof(struct page *);
 +	}
 +}
 +
 +static inline void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
 +{
 +	tlb_flush(tlb);
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	tlb_table_flush(tlb);
 +#endif
 +}
 +
 +static inline void tlb_flush_mmu_free(struct mmu_gather *tlb)
 +{
 +	free_pages_and_swap_cache(tlb->pages, tlb->nr);
 +	tlb->nr = 0;
 +	if (tlb->pages == tlb->local)
 +		__tlb_alloc_page(tlb);
 +}
 +
 +static inline void tlb_flush_mmu(struct mmu_gather *tlb)
 +{
 +	tlb_flush_mmu_tlbonly(tlb);
 +	tlb_flush_mmu_free(tlb);
 +}
 +
++=======
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
  static inline void
 -__pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte, unsigned long addr)
 +arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 +			unsigned long start, unsigned long end)
  {
 -	pgtable_pte_page_dtor(pte);
 +	tlb->mm = mm;
 +	tlb->fullmm = !(start | (end+1));
 +	tlb->start = start;
 +	tlb->end = end;
 +	tlb->vma = NULL;
 +	tlb->max = ARRAY_SIZE(tlb->local);
 +	tlb->pages = tlb->local;
 +	tlb->nr = 0;
 +	__tlb_alloc_page(tlb);
 +
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	tlb->batch = NULL;
 +#endif
 +}
 +
 +static inline void
 +arch_tlb_finish_mmu(struct mmu_gather *tlb,
 +			unsigned long start, unsigned long end, bool force)
 +{
 +	if (force) {
 +		tlb->range_start = start;
 +		tlb->range_end = end;
 +	}
 +
 +	tlb_flush_mmu(tlb);
 +
 +	/* keep the page table cache within bounds */
 +	check_pgt_cache();
 +
 +	if (tlb->pages != tlb->local)
 +		free_pages((unsigned long)tlb->pages, 0);
 +}
  
 -#ifndef CONFIG_ARM_LPAE
 +/*
 + * Memorize the range for the TLB flush.
 + */
 +static inline void
 +tlb_remove_tlb_entry(struct mmu_gather *tlb, pte_t *ptep, unsigned long addr)
 +{
 +	tlb_add_flush(tlb, addr);
 +}
 +
 +#define tlb_remove_huge_tlb_entry(h, tlb, ptep, address)	\
 +	tlb_remove_tlb_entry(tlb, ptep, address)
 +/*
 + * In the case of tlb vma handling, we can optimise these away in the
 + * case where we're doing a full MM flush.  When we're doing a munmap,
 + * the vmas are adjusted to only cover the region to be torn down.
 + */
 +static inline void
 +tlb_start_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
 +{
 +	if (!tlb->fullmm) {
 +		flush_cache_range(vma, vma->vm_start, vma->vm_end);
 +		tlb->vma = vma;
 +		tlb->range_start = TASK_SIZE;
 +		tlb->range_end = 0;
 +	}
 +}
 +
 +static inline void
 +tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
 +{
 +	if (!tlb->fullmm)
 +		tlb_flush(tlb);
 +}
 +
 +static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 +{
 +	tlb->pages[tlb->nr++] = page;
 +	VM_WARN_ON(tlb->nr > tlb->max);
 +	if (tlb->nr == tlb->max)
 +		return true;
 +	return false;
 +}
 +
 +static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 +{
 +	if (__tlb_remove_page(tlb, page))
 +		tlb_flush_mmu(tlb);
 +}
 +
 +static inline bool __tlb_remove_page_size(struct mmu_gather *tlb,
 +					  struct page *page, int page_size)
 +{
 +	return __tlb_remove_page(tlb, page);
 +}
 +
 +static inline void tlb_remove_page_size(struct mmu_gather *tlb,
 +					struct page *page, int page_size)
 +{
 +	return tlb_remove_page(tlb, page);
 +}
 +
 +static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte,
 +	unsigned long addr)
 +{
 +	pgtable_page_dtor(pte);
 +
 +#ifdef CONFIG_ARM_LPAE
 +	tlb_add_flush(tlb, addr);
 +#else
  	/*
  	 * With the classic ARM MMU, a pte page has two corresponding pmd
  	 * entries, each covering 1MB.
diff --cc include/asm-generic/tlb.h
index 27ac8002fb01,f391f6b500b4..000000000000
--- a/include/asm-generic/tlb.h
+++ b/include/asm-generic/tlb.h
@@@ -130,59 -135,48 +139,82 @@@
   *  This ensures we call tlb_flush() every time tlb_change_page_size() actually
   *  changes the size and provides mmu_gather::page_size to tlb_flush().
   *
++<<<<<<< HEAD
 + *  HAVE_RCU_TABLE_FREE
++=======
+  *  This might be useful if your architecture has size specific TLB
+  *  invalidation instructions.
+  *
+  *  MMU_GATHER_TABLE_FREE
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
   *
   *  This provides tlb_remove_table(), to be used instead of tlb_remove_page()
-  *  for page directores (__p*_free_tlb()). This provides separate freeing of
-  *  the page-table pages themselves in a semi-RCU fashion (see comment below).
-  *  Useful if your architecture doesn't use IPIs for remote TLB invalidates
-  *  and therefore doesn't naturally serialize with software page-table walkers.
+  *  for page directores (__p*_free_tlb()).
+  *
+  *  Useful if your architecture has non-page page directories.
   *
   *  When used, an architecture is expected to provide __tlb_remove_table()
   *  which does the actual freeing of these pages.
   *
++<<<<<<< HEAD
 + *  HAVE_RCU_TABLE_INVALIDATE
 + *
 + *  This makes HAVE_RCU_TABLE_FREE call tlb_flush_mmu_tlbonly() before freeing
 + *  the page-table pages. Required if you use HAVE_RCU_TABLE_FREE and your
 + *  architecture uses the Linux page-tables natively.
++=======
+  *  MMU_GATHER_RCU_TABLE_FREE
+  *
+  *  Like MMU_GATHER_TABLE_FREE, and adds semi-RCU semantics to the free (see
+  *  comment below).
+  *
+  *  Useful if your architecture doesn't use IPIs for remote TLB invalidates
+  *  and therefore doesn't naturally serialize with software page-table walkers.
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
   *
   *  MMU_GATHER_NO_RANGE
   *
   *  Use this if your architecture lacks an efficient flush_tlb_range().
 + */
 +
++<<<<<<< HEAD
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +/*
 + * Semi RCU freeing of the page directories.
 + *
 + * This is needed by some architectures to implement software pagetable walkers.
 + *
 + * gup_fast() and other software pagetable walkers do a lockless page-table
 + * walk and therefore needs some synchronization with the freeing of the page
 + * directories. The chosen means to accomplish that is by disabling IRQs over
 + * the walk.
 + *
 + * Architectures that use IPIs to flush TLBs will then automagically DTRT,
 + * since we unlink the page, flush TLBs, free the page. Since the disabling of
 + * IRQs delays the completion of the TLB flush we can never observe an already
 + * freed page.
   *
 - *  MMU_GATHER_NO_GATHER
 + * Architectures that do not have this (PPC) need to delay the freeing by some
 + * other means, this is that means.
   *
 - *  If the option is set the mmu_gather will not track individual pages for
 - *  delayed page free anymore. A platform that enables the option needs to
 - *  provide its own implementation of the __tlb_remove_page_size() function to
 - *  free pages.
 + * What we do is batch the freed directory pages (tables) and RCU free them.
 + * We use the sched RCU variant, as that guarantees that IRQ/preempt disabling
 + * holds off grace periods.
 + *
 + * However, in order to batch these pages we need to allocate storage, this
 + * allocation is deep inside the MM code and can thus easily fail on memory
 + * pressure. To guarantee progress we fall back to single table freeing, see
 + * the implementation of tlb_remove_table_one().
   *
 - *  This is useful if your architecture already flushes TLB entries in the
 - *  various ptep_get_and_clear() functions.
   */
 -
++=======
+ #ifdef CONFIG_MMU_GATHER_TABLE_FREE
+ 
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
  struct mmu_table_batch {
+ #ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
  	struct rcu_head		rcu;
+ #endif
  	unsigned int		nr;
  	void			*tables[0];
  };
@@@ -245,7 -250,7 +288,11 @@@ extern bool __tlb_remove_page_size(stru
  struct mmu_gather {
  	struct mm_struct	*mm;
  
++<<<<<<< HEAD
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
++=======
+ #ifdef CONFIG_MMU_GATHER_TABLE_FREE
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
  	struct mmu_table_batch	*batch;
  #endif
  
diff --cc mm/mmu_gather.c
index b84402038601,a3538cb2bcbe..000000000000
--- a/mm/mmu_gather.c
+++ b/mm/mmu_gather.c
@@@ -89,28 -89,50 +89,54 @@@ bool __tlb_remove_page_size(struct mmu_
  	return false;
  }
  
 -#endif /* MMU_GATHER_NO_GATHER */
 +#endif /* HAVE_MMU_GATHER_NO_GATHER */
  
++<<<<<<< HEAD
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
++=======
+ #ifdef CONFIG_MMU_GATHER_TABLE_FREE
  
- /*
-  * See the comment near struct mmu_table_batch.
-  */
+ static void __tlb_remove_table_free(struct mmu_table_batch *batch)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < batch->nr; i++)
+ 		__tlb_remove_table(batch->tables[i]);
+ 
+ 	free_page((unsigned long)batch);
+ }
+ 
+ #ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
  
  /*
-  * If we want tlb_remove_table() to imply TLB invalidates.
+  * Semi RCU freeing of the page directories.
+  *
+  * This is needed by some architectures to implement software pagetable walkers.
+  *
+  * gup_fast() and other software pagetable walkers do a lockless page-table
+  * walk and therefore needs some synchronization with the freeing of the page
+  * directories. The chosen means to accomplish that is by disabling IRQs over
+  * the walk.
+  *
+  * Architectures that use IPIs to flush TLBs will then automagically DTRT,
+  * since we unlink the page, flush TLBs, free the page. Since the disabling of
+  * IRQs delays the completion of the TLB flush we can never observe an already
+  * freed page.
+  *
+  * Architectures that do not have this (PPC) need to delay the freeing by some
+  * other means, this is that means.
+  *
+  * What we do is batch the freed directory pages (tables) and RCU free them.
+  * We use the sched RCU variant, as that guarantees that IRQ/preempt disabling
+  * holds off grace periods.
+  *
+  * However, in order to batch these pages we need to allocate storage, this
+  * allocation is deep inside the MM code and can thus easily fail on memory
+  * pressure. To guarantee progress we fall back to single table freeing, see
+  * the implementation of tlb_remove_table_one().
+  *
   */
- static inline void tlb_table_invalidate(struct mmu_gather *tlb)
- {
- 	if (tlb_needs_table_invalidate()) {
- 		/*
- 		 * Invalidate page-table caches used by hardware walkers. Then
- 		 * we still need to RCU-sched wait while freeing the pages
- 		 * because software walkers can still be in-flight.
- 		 */
- 		tlb_flush_mmu_tlbonly(tlb);
- 	}
- }
  
  static void tlb_remove_table_smp_sync(void *arg)
  {
@@@ -173,14 -223,22 +227,33 @@@ void tlb_remove_table(struct mmu_gathe
  		tlb_table_flush(tlb);
  }
  
++<<<<<<< HEAD
 +#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
 +
 +static void tlb_flush_mmu_free(struct mmu_gather *tlb)
 +{
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	tlb_table_flush(tlb);
 +#endif
 +#ifndef CONFIG_HAVE_MMU_GATHER_NO_GATHER
++=======
+ static inline void tlb_table_init(struct mmu_gather *tlb)
+ {
+ 	tlb->batch = NULL;
+ }
+ 
+ #else /* !CONFIG_MMU_GATHER_TABLE_FREE */
+ 
+ static inline void tlb_table_flush(struct mmu_gather *tlb) { }
+ static inline void tlb_table_init(struct mmu_gather *tlb) { }
+ 
+ #endif /* CONFIG_MMU_GATHER_TABLE_FREE */
+ 
+ static void tlb_flush_mmu_free(struct mmu_gather *tlb)
+ {
+ 	tlb_table_flush(tlb);
+ #ifndef CONFIG_MMU_GATHER_NO_GATHER
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
  	tlb_batch_pages_flush(tlb);
  #endif
  }
@@@ -220,10 -278,8 +293,15 @@@ void tlb_gather_mmu(struct mmu_gather *
  	tlb->batch_count = 0;
  #endif
  
++<<<<<<< HEAD
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	tlb->batch = NULL;
 +#endif
 +#ifdef CONFIG_HAVE_MMU_GATHER_PAGE_SIZE
++=======
+ 	tlb_table_init(tlb);
+ #ifdef CONFIG_MMU_GATHER_PAGE_SIZE
++>>>>>>> 0d6e24d430ef (asm-generic/tlb: provide MMU_GATHER_TABLE_FREE)
  	tlb->page_size = 0;
  #endif
  
* Unmerged path arch/Kconfig
* Unmerged path arch/arm/include/asm/tlb.h
* Unmerged path include/asm-generic/tlb.h
* Unmerged path mm/mmu_gather.c
