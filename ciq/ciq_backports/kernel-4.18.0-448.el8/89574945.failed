mm: simplify freeing of devmap managed pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 895749455f6054e0c7b40a6ec449a3ab6db51bdd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/89574945.failed

Make put_devmap_managed_page return if it took charge of the page
or not and remove the separate page_is_devmap_managed helper.

Link: https://lkml.kernel.org/r/20220210072828.2930359-6-hch@lst.de
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
	Reviewed-by: Chaitanya Kulkarni <kch@nvidia.com>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Tested-by: "Sierra Guiza, Alejandro (Alex)" <alex.sierra@amd.com>

	Cc: Alex Deucher <alexander.deucher@amd.com>
	Cc: Alistair Popple <apopple@nvidia.com>
	Cc: Ben Skeggs <bskeggs@redhat.com>
	Cc: Christian Knig <christian.koenig@amd.com>
	Cc: Felix Kuehling <Felix.Kuehling@amd.com>
	Cc: Karol Herbst <kherbst@redhat.com>
	Cc: Lyude Paul <lyude@redhat.com>
	Cc: Miaohe Lin <linmiaohe@huawei.com>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Cc: "Pan, Xinhui" <Xinhui.Pan@amd.com>
	Cc: Ralph Campbell <rcampbell@nvidia.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
(cherry picked from commit 895749455f6054e0c7b40a6ec449a3ab6db51bdd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm.h
#	mm/memremap.c
diff --cc include/linux/mm.h
index e3b1a16638a0,8a59f0456149..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -1003,56 -1086,33 +1003,60 @@@ static inline bool is_zone_device_page(
  }
  #endif
  
 -static inline bool is_zone_movable_page(const struct page *page)
 -{
 -	return page_zonenum(page) == ZONE_MOVABLE;
 -}
 -
  #ifdef CONFIG_DEV_PAGEMAP_OPS
 +void free_devmap_managed_page(struct page *page);
  DECLARE_STATIC_KEY_FALSE(devmap_managed_key);
  
- static inline bool page_is_devmap_managed(struct page *page)
+ bool __put_devmap_managed_page(struct page *page);
+ static inline bool put_devmap_managed_page(struct page *page)
  {
  	if (!static_branch_unlikely(&devmap_managed_key))
  		return false;
  	if (!is_zone_device_page(page))
  		return false;
- 	switch (page->pgmap->type) {
- 	case MEMORY_DEVICE_PRIVATE:
- 	case MEMORY_DEVICE_FS_DAX:
- 		return true;
- 	default:
- 		break;
- 	}
- 	return false;
+ 	if (page->pgmap->type != MEMORY_DEVICE_PRIVATE &&
+ 	    page->pgmap->type != MEMORY_DEVICE_FS_DAX)
+ 		return false;
+ 	return __put_devmap_managed_page(page);
  }
  
++<<<<<<< HEAD
 +static inline bool is_device_private_page(const struct page *page)
 +{
 +	return is_zone_device_page(page) &&
 +		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
 +}
 +
 +#ifdef CONFIG_PCI_P2PDMA
 +static inline bool is_pci_p2pdma_page(const struct page *page)
 +{
 +	return is_zone_device_page(page) &&
 +		page->pgmap->type == MEMORY_DEVICE_PCI_P2PDMA;
 +}
 +#else /* CONFIG_PCI_P2PDMA */
 +static inline bool is_pci_p2pdma_page(const struct page *page)
 +{
 +	return false;
 +}
 +#endif /* CONFIG_PCI_P2PDMA */
 +
 +void put_devmap_managed_page(struct page *page);
 +
++=======
++>>>>>>> 895749455f60 (mm: simplify freeing of devmap managed pages)
  #else /* CONFIG_DEV_PAGEMAP_OPS */
- static inline bool page_is_devmap_managed(struct page *page)
+ static inline bool put_devmap_managed_page(struct page *page)
  {
  	return false;
  }
++<<<<<<< HEAD
 +
 +static inline void put_devmap_managed_page(struct page *page)
 +{
 +}
++=======
+ #endif /* CONFIG_DEV_PAGEMAP_OPS */
++>>>>>>> 895749455f60 (mm: simplify freeing of devmap managed pages)
  
  static inline bool is_device_private_page(const struct page *page)
  {
@@@ -1091,23 -1167,56 +1095,27 @@@ static inline __must_check bool try_get
  	return true;
  }
  
 -/**
 - * folio_put - Decrement the reference count on a folio.
 - * @folio: The folio.
 - *
 - * If the folio's reference count reaches zero, the memory will be
 - * released back to the page allocator and may be used by another
 - * allocation immediately.  Do not access the memory or the struct folio
 - * after calling folio_put() unless you can be sure that it wasn't the
 - * last reference.
 - *
 - * Context: May be called in process or interrupt context, but not in NMI
 - * context.  May be called while holding a spinlock.
 - */
 -static inline void folio_put(struct folio *folio)
 -{
 -	if (folio_put_testzero(folio))
 -		__put_page(&folio->page);
 -}
 -
 -/**
 - * folio_put_refs - Reduce the reference count on a folio.
 - * @folio: The folio.
 - * @refs: The amount to subtract from the folio's reference count.
 - *
 - * If the folio's reference count reaches zero, the memory will be
 - * released back to the page allocator and may be used by another
 - * allocation immediately.  Do not access the memory or the struct folio
 - * after calling folio_put_refs() unless you can be sure that these weren't
 - * the last references.
 - *
 - * Context: May be called in process or interrupt context, but not in NMI
 - * context.  May be called while holding a spinlock.
 - */
 -static inline void folio_put_refs(struct folio *folio, int refs)
 -{
 -	if (folio_ref_sub_and_test(folio, refs))
 -		__put_page(&folio->page);
 -}
 -
  static inline void put_page(struct page *page)
  {
 -	struct folio *folio = page_folio(page);
 +	page = compound_head(page);
  
  	/*
- 	 * For devmap managed pages we need to catch refcount transition from
- 	 * 2 to 1, when refcount reach one it means the page is free and we
- 	 * need to inform the device driver through callback. See
- 	 * include/linux/memremap.h and HMM for details.
+ 	 * For some devmap managed pages we need to catch refcount transition
+ 	 * from 2 to 1:
  	 */
++<<<<<<< HEAD
 +	if (page_is_devmap_managed(page)) {
 +		put_devmap_managed_page(page);
 +		return;
 +	}
 +
 +	if (put_page_testzero(page))
 +		__put_page(page);
++=======
+ 	if (put_devmap_managed_page(&folio->page))
+ 		return;
+ 	folio_put(folio);
++>>>>>>> 895749455f60 (mm: simplify freeing of devmap managed pages)
  }
  
  /*
diff --cc mm/memremap.c
index 2455bac89506,f41233a67edb..000000000000
--- a/mm/memremap.c
+++ b/mm/memremap.c
@@@ -528,4 -501,23 +528,26 @@@ void free_devmap_managed_page(struct pa
  	page->mapping = NULL;
  	page->pgmap->ops->page_free(page);
  }
++<<<<<<< HEAD
++=======
+ 
+ bool __put_devmap_managed_page(struct page *page)
+ {
+ 	/*
+ 	 * devmap page refcounts are 1-based, rather than 0-based: if
+ 	 * refcount is 1, then the page is free and the refcount is
+ 	 * stable because nobody holds a reference on the page.
+ 	 */
+ 	switch (page_ref_dec_return(page)) {
+ 	case 1:
+ 		free_devmap_managed_page(page);
+ 		break;
+ 	case 0:
+ 		__put_page(page);
+ 		break;
+ 	}
+ 	return true;
+ }
+ EXPORT_SYMBOL(__put_devmap_managed_page);
++>>>>>>> 895749455f60 (mm: simplify freeing of devmap managed pages)
  #endif /* CONFIG_DEV_PAGEMAP_OPS */
* Unmerged path include/linux/mm.h
* Unmerged path mm/memremap.c
diff --git a/mm/swap.c b/mm/swap.c
index c9fe61e3a5e4..5f9278379e8f 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -812,16 +812,8 @@ void release_pages(struct page **pages, int nr)
 				unlock_page_lruvec_irqrestore(lruvec, flags);
 				lruvec = NULL;
 			}
-			/*
-			 * ZONE_DEVICE pages that return 'false' from
-			 * page_is_devmap_managed() do not require special
-			 * processing, and instead, expect a call to
-			 * put_page_testzero().
-			 */
-			if (page_is_devmap_managed(page)) {
-				put_devmap_managed_page(page);
+			if (put_devmap_managed_page(page))
 				continue;
-			}
 			if (put_page_testzero(page))
 				put_dev_pagemap(page->pgmap);
 			continue;
