memcg: extract memcg_vmstats from struct mem_cgroup

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit 410f8e82689e1e66044fea51ef852054a09502b7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/410f8e82.failed

Patch series "memcg: reduce memory overhead of memory cgroups".

Currently a lot of memory is wasted to maintain the vmevents for memory
cgroups as we have multiple arrays of size NR_VM_EVENT_ITEMS which can be
as large as 110.  However memcg code uses small portion of those entries. 
This patch series eliminate this overhead by removing the unneeded vmevent
entries from memory cgroup data structures.


This patch (of 3):

This is a preparatory patch to reduce the memory overhead of memory
cgroup. The struct memcg_vmstats is the largest object embedded into the
struct mem_cgroup. This patch extracts struct memcg_vmstats from struct
mem_cgroup to ease the following patches in reducing the size of struct
memcg_vmstats.

Link: https://lkml.kernel.org/r/20220907043537.3457014-1-shakeelb@google.com
Link: https://lkml.kernel.org/r/20220907043537.3457014-2-shakeelb@google.com
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: Roman Gushchin <roman.gushchin@linux.dev>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 410f8e82689e1e66044fea51ef852054a09502b7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	mm/memcontrol.c
diff --cc include/linux/memcontrol.h
index 959a92657744,dc7d40e575d5..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -357,16 -274,10 +336,16 @@@ struct mem_cgroup 
  	spinlock_t		move_lock;
  	unsigned long		move_lock_flags;
  
 -	CACHELINE_PADDING(_pad1_);
 +	MEMCG_PADDING(_pad1_);
 +	/*
 +	 * Disable percpu stats when offline, flush and free them after one
 +	 * grace period.
 +	 */
 +	struct rcu_work		percpu_stats_rwork;
 +	enum percpu_stats_state percpu_stats_disabled;
  
  	/* memory.stat */
- 	struct memcg_vmstats	vmstats;
+ 	struct memcg_vmstats	*vmstats;
  
  	/* memory.events */
  	atomic_long_t		memory_events[MEMCG_NR_MEMORY_EVENTS];
@@@ -1005,15 -965,22 +984,34 @@@ static inline void mod_memcg_state(stru
  	local_irq_restore(flags);
  }
  
++<<<<<<< HEAD
 +static inline unsigned long memcg_page_state(struct mem_cgroup *memcg, int idx)
 +{
 +	long x = READ_ONCE(memcg->vmstats.state[idx]);
 +#ifdef CONFIG_SMP
 +	if (x < 0)
 +		x = 0;
 +#endif
 +	return x;
 +}
++=======
+ static inline void mod_memcg_page_state(struct page *page,
+ 					int idx, int val)
+ {
+ 	struct mem_cgroup *memcg;
+ 
+ 	if (mem_cgroup_disabled())
+ 		return;
+ 
+ 	rcu_read_lock();
+ 	memcg = page_memcg(page);
+ 	if (memcg)
+ 		mod_memcg_state(memcg, idx, val);
+ 	rcu_read_unlock();
+ }
+ 
+ unsigned long memcg_page_state(struct mem_cgroup *memcg, int idx);
++>>>>>>> 410f8e82689e (memcg: extract memcg_vmstats from struct mem_cgroup)
  
  static inline unsigned long lruvec_page_state(struct lruvec *lruvec,
  					      enum node_stat_item idx)
diff --cc mm/memcontrol.c
index 413502fb22c9,0a44a733bb03..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -640,29 -566,143 +640,63 @@@ mem_cgroup_largest_soft_limit_node(stru
  }
  
  /*
 - * memcg and lruvec stats flushing
 + * Return the active percpu stats memcg and optionally mem_cgroup_per_node.
   *
 - * Many codepaths leading to stats update or read are performance sensitive and
 - * adding stats flushing in such codepaths is not desirable. So, to optimize the
 - * flushing the kernel does:
 - *
 - * 1) Periodically and asynchronously flush the stats every 2 seconds to not let
 - *    rstat update tree grow unbounded.
 - *
 - * 2) Flush the stats synchronously on reader side only when there are more than
 - *    (MEMCG_CHARGE_BATCH * nr_cpus) update events. Though this optimization
 - *    will let stats be out of sync by atmost (MEMCG_CHARGE_BATCH * nr_cpus) but
 - *    only for 2 seconds due to (1).
 + * When percpu_stats_disabled, the percpu stats update is transferred to
 + * its parent.
   */
 -static void flush_memcg_stats_dwork(struct work_struct *w);
 -static DECLARE_DEFERRABLE_WORK(stats_flush_dwork, flush_memcg_stats_dwork);
 -static DEFINE_SPINLOCK(stats_flush_lock);
 -static DEFINE_PER_CPU(unsigned int, stats_updates);
 -static atomic_t stats_flush_threshold = ATOMIC_INIT(0);
 -static u64 flush_next_time;
 -
 -#define FLUSH_TIME (2UL*HZ)
 -
 -/*
 - * Accessors to ensure that preemption is disabled on PREEMPT_RT because it can
 - * not rely on this as part of an acquired spinlock_t lock. These functions are
 - * never used in hardirq context on PREEMPT_RT and therefore disabling preemtion
 - * is sufficient.
 - */
 -static void memcg_stats_lock(void)
 -{
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_disable();
 -#else
 -      VM_BUG_ON(!irqs_disabled());
 -#endif
 -}
 -
 -static void __memcg_stats_lock(void)
 +static inline struct mem_cgroup *
 +percpu_stats_memcg(struct mem_cgroup *memcg, struct mem_cgroup_per_node **pn)
  {
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_disable();
 -#endif
 -}
 +	if (likely(!memcg->percpu_stats_disabled))
 +		return memcg;
  
 -static void memcg_stats_unlock(void)
 -{
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_enable();
 -#endif
 -}
 -
 -static inline void memcg_rstat_updated(struct mem_cgroup *memcg, int val)
 -{
 -	unsigned int x;
 +	do {
 +		memcg = parent_mem_cgroup(memcg);
 +	} while (memcg->percpu_stats_disabled);
  
 -	cgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());
 +	if (pn) {
 +		unsigned int nid = (*pn)->nid;
  
 -	x = __this_cpu_add_return(stats_updates, abs(val));
 -	if (x > MEMCG_CHARGE_BATCH) {
 -		/*
 -		 * If stats_flush_threshold exceeds the threshold
 -		 * (>num_online_cpus()), cgroup stats update will be triggered
 -		 * in __mem_cgroup_flush_stats(). Increasing this var further
 -		 * is redundant and simply adds overhead in atomic update.
 -		 */
 -		if (atomic_read(&stats_flush_threshold) <= num_online_cpus())
 -			atomic_add(x / MEMCG_CHARGE_BATCH, &stats_flush_threshold);
 -		__this_cpu_write(stats_updates, 0);
 +		*pn = memcg->nodeinfo[nid];
  	}
 -}
 -
 -static void __mem_cgroup_flush_stats(void)
 -{
 -	unsigned long flag;
 -
 -	if (!spin_trylock_irqsave(&stats_flush_lock, flag))
 -		return;
 -
 -	flush_next_time = jiffies_64 + 2*FLUSH_TIME;
 -	cgroup_rstat_flush_irqsafe(root_mem_cgroup->css.cgroup);
 -	atomic_set(&stats_flush_threshold, 0);
 -	spin_unlock_irqrestore(&stats_flush_lock, flag);
 -}
 -
 -void mem_cgroup_flush_stats(void)
 -{
 -	if (atomic_read(&stats_flush_threshold) > num_online_cpus())
 -		__mem_cgroup_flush_stats();
 -}
 -
 -void mem_cgroup_flush_stats_delayed(void)
 -{
 -	if (time_after64(jiffies_64, flush_next_time))
 -		mem_cgroup_flush_stats();
 -}
 -
 -static void flush_memcg_stats_dwork(struct work_struct *w)
 -{
 -	__mem_cgroup_flush_stats();
 -	queue_delayed_work(system_unbound_wq, &stats_flush_dwork, FLUSH_TIME);
 +	return memcg;
  }
  
+ struct memcg_vmstats_percpu {
+ 	/* Local (CPU and cgroup) page state & events */
+ 	long			state[MEMCG_NR_STAT];
+ 	unsigned long		events[NR_VM_EVENT_ITEMS];
+ 
+ 	/* Delta calculation for lockless upward propagation */
+ 	long			state_prev[MEMCG_NR_STAT];
+ 	unsigned long		events_prev[NR_VM_EVENT_ITEMS];
+ 
+ 	/* Cgroup1: threshold notifications & softlimit tree updates */
+ 	unsigned long		nr_page_events;
+ 	unsigned long		targets[MEM_CGROUP_NTARGETS];
+ };
+ 
+ struct memcg_vmstats {
+ 	/* Aggregated (CPU and subtree) page state & events */
+ 	long			state[MEMCG_NR_STAT];
+ 	unsigned long		events[NR_VM_EVENT_ITEMS];
+ 
+ 	/* Pending child counts during tree propagation */
+ 	long			state_pending[MEMCG_NR_STAT];
+ 	unsigned long		events_pending[NR_VM_EVENT_ITEMS];
+ };
+ 
+ unsigned long memcg_page_state(struct mem_cgroup *memcg, int idx)
+ {
+ 	long x = READ_ONCE(memcg->vmstats->state[idx]);
+ #ifdef CONFIG_SMP
+ 	if (x < 0)
+ 		x = 0;
+ #endif
+ 	return x;
+ }
+ 
  /**
   * __mod_memcg_state - update cgroup memory statistics
   * @memcg: the memory cgroup
@@@ -5196,6 -5204,8 +5230,11 @@@ static void __mem_cgroup_free(struct me
  
  	for_each_node(node)
  		free_mem_cgroup_per_node_info(memcg, node);
++<<<<<<< HEAD
++=======
+ 	kfree(memcg->vmstats);
+ 	free_percpu(memcg->vmstats_percpu);
++>>>>>>> 410f8e82689e (memcg: extract memcg_vmstats from struct mem_cgroup)
  	kfree(memcg);
  }
  
@@@ -5525,10 -5491,40 +5568,10 @@@ static void mem_cgroup_css_rstat_flush(
  		if (!delta)
  			continue;
  
- 		memcg->vmstats.events[i] += delta;
+ 		memcg->vmstats->events[i] += delta;
  		if (parent)
- 			parent->vmstats.events_pending[i] += delta;
+ 			parent->vmstats->events_pending[i] += delta;
  	}
 -
 -	for_each_node_state(nid, N_MEMORY) {
 -		struct mem_cgroup_per_node *pn = memcg->nodeinfo[nid];
 -		struct mem_cgroup_per_node *ppn = NULL;
 -		struct lruvec_stats_percpu *lstatc;
 -
 -		if (parent)
 -			ppn = parent->nodeinfo[nid];
 -
 -		lstatc = per_cpu_ptr(pn->lruvec_stats_percpu, cpu);
 -
 -		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
 -			delta = pn->lruvec_stats.state_pending[i];
 -			if (delta)
 -				pn->lruvec_stats.state_pending[i] = 0;
 -
 -			v = READ_ONCE(lstatc->state[i]);
 -			if (v != lstatc->state_prev[i]) {
 -				delta += v - lstatc->state_prev[i];
 -				lstatc->state_prev[i] = v;
 -			}
 -
 -			if (!delta)
 -				continue;
 -
 -			pn->lruvec_stats.state[i] += delta;
 -			if (ppn)
 -				ppn->lruvec_stats.state_pending[i] += delta;
 -		}
 -	}
  }
  
  #ifdef CONFIG_MMU
* Unmerged path include/linux/memcontrol.h
* Unmerged path mm/memcontrol.c
