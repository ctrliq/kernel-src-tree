memcg: rearrange code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit d396def5d86dbeb4ceb4a9dca92611ce206dc66a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/d396def5.failed

This is a preparatory patch for easing the review of the follow up patch
which will reduce the memory overhead of memory cgroups.

Link: https://lkml.kernel.org/r/20220907043537.3457014-3-shakeelb@google.com
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: Roman Gushchin <roman.gushchin@linux.dev>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit d396def5d86dbeb4ceb4a9dca92611ce206dc66a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index 413502fb22c9,78fd7cfb4f92..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -640,27 -566,164 +640,118 @@@ mem_cgroup_largest_soft_limit_node(stru
  }
  
  /*
 - * memcg and lruvec stats flushing
 - *
 - * Many codepaths leading to stats update or read are performance sensitive and
 - * adding stats flushing in such codepaths is not desirable. So, to optimize the
 - * flushing the kernel does:
 + * Return the active percpu stats memcg and optionally mem_cgroup_per_node.
   *
 - * 1) Periodically and asynchronously flush the stats every 2 seconds to not let
 - *    rstat update tree grow unbounded.
 - *
 - * 2) Flush the stats synchronously on reader side only when there are more than
 - *    (MEMCG_CHARGE_BATCH * nr_cpus) update events. Though this optimization
 - *    will let stats be out of sync by atmost (MEMCG_CHARGE_BATCH * nr_cpus) but
 - *    only for 2 seconds due to (1).
 - */
 -static void flush_memcg_stats_dwork(struct work_struct *w);
 -static DECLARE_DEFERRABLE_WORK(stats_flush_dwork, flush_memcg_stats_dwork);
 -static DEFINE_SPINLOCK(stats_flush_lock);
 -static DEFINE_PER_CPU(unsigned int, stats_updates);
 -static atomic_t stats_flush_threshold = ATOMIC_INIT(0);
 -static u64 flush_next_time;
 -
 -#define FLUSH_TIME (2UL*HZ)
 -
 -/*
 - * Accessors to ensure that preemption is disabled on PREEMPT_RT because it can
 - * not rely on this as part of an acquired spinlock_t lock. These functions are
 - * never used in hardirq context on PREEMPT_RT and therefore disabling preemtion
 - * is sufficient.
 + * When percpu_stats_disabled, the percpu stats update is transferred to
 + * its parent.
   */
 -static void memcg_stats_lock(void)
 -{
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_disable();
 -#else
 -      VM_BUG_ON(!irqs_disabled());
 -#endif
 -}
 -
 -static void __memcg_stats_lock(void)
 -{
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_disable();
 -#endif
 -}
 -
 -static void memcg_stats_unlock(void)
 +static inline struct mem_cgroup *
 +percpu_stats_memcg(struct mem_cgroup *memcg, struct mem_cgroup_per_node **pn)
  {
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_enable();
 -#endif
 -}
 +	if (likely(!memcg->percpu_stats_disabled))
 +		return memcg;
  
 -static inline void memcg_rstat_updated(struct mem_cgroup *memcg, int val)
 -{
 -	unsigned int x;
 +	do {
 +		memcg = parent_mem_cgroup(memcg);
 +	} while (memcg->percpu_stats_disabled);
  
 -	cgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());
 +	if (pn) {
 +		unsigned int nid = (*pn)->nid;
  
 -	x = __this_cpu_add_return(stats_updates, abs(val));
 -	if (x > MEMCG_CHARGE_BATCH) {
 -		/*
 -		 * If stats_flush_threshold exceeds the threshold
 -		 * (>num_online_cpus()), cgroup stats update will be triggered
 -		 * in __mem_cgroup_flush_stats(). Increasing this var further
 -		 * is redundant and simply adds overhead in atomic update.
 -		 */
 -		if (atomic_read(&stats_flush_threshold) <= num_online_cpus())
 -			atomic_add(x / MEMCG_CHARGE_BATCH, &stats_flush_threshold);
 -		__this_cpu_write(stats_updates, 0);
 +		*pn = memcg->nodeinfo[nid];
  	}
++<<<<<<< HEAD
 +	return memcg;
++=======
+ }
+ 
+ static void __mem_cgroup_flush_stats(void)
+ {
+ 	unsigned long flag;
+ 
+ 	if (!spin_trylock_irqsave(&stats_flush_lock, flag))
+ 		return;
+ 
+ 	flush_next_time = jiffies_64 + 2*FLUSH_TIME;
+ 	cgroup_rstat_flush_irqsafe(root_mem_cgroup->css.cgroup);
+ 	atomic_set(&stats_flush_threshold, 0);
+ 	spin_unlock_irqrestore(&stats_flush_lock, flag);
+ }
+ 
+ void mem_cgroup_flush_stats(void)
+ {
+ 	if (atomic_read(&stats_flush_threshold) > num_online_cpus())
+ 		__mem_cgroup_flush_stats();
+ }
+ 
+ void mem_cgroup_flush_stats_delayed(void)
+ {
+ 	if (time_after64(jiffies_64, flush_next_time))
+ 		mem_cgroup_flush_stats();
+ }
+ 
+ static void flush_memcg_stats_dwork(struct work_struct *w)
+ {
+ 	__mem_cgroup_flush_stats();
+ 	queue_delayed_work(system_unbound_wq, &stats_flush_dwork, FLUSH_TIME);
+ }
+ 
+ /* Subset of vm_event_item to report for memcg event stats */
+ static const unsigned int memcg_vm_event_stat[] = {
+ 	PGSCAN_KSWAPD,
+ 	PGSCAN_DIRECT,
+ 	PGSTEAL_KSWAPD,
+ 	PGSTEAL_DIRECT,
+ 	PGFAULT,
+ 	PGMAJFAULT,
+ 	PGREFILL,
+ 	PGACTIVATE,
+ 	PGDEACTIVATE,
+ 	PGLAZYFREE,
+ 	PGLAZYFREED,
+ #if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)
+ 	ZSWPIN,
+ 	ZSWPOUT,
+ #endif
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	THP_FAULT_ALLOC,
+ 	THP_COLLAPSE_ALLOC,
+ #endif
+ };
+ 
+ struct memcg_vmstats_percpu {
+ 	/* Local (CPU and cgroup) page state & events */
+ 	long			state[MEMCG_NR_STAT];
+ 	unsigned long		events[NR_VM_EVENT_ITEMS];
+ 
+ 	/* Delta calculation for lockless upward propagation */
+ 	long			state_prev[MEMCG_NR_STAT];
+ 	unsigned long		events_prev[NR_VM_EVENT_ITEMS];
+ 
+ 	/* Cgroup1: threshold notifications & softlimit tree updates */
+ 	unsigned long		nr_page_events;
+ 	unsigned long		targets[MEM_CGROUP_NTARGETS];
+ };
+ 
+ struct memcg_vmstats {
+ 	/* Aggregated (CPU and subtree) page state & events */
+ 	long			state[MEMCG_NR_STAT];
+ 	unsigned long		events[NR_VM_EVENT_ITEMS];
+ 
+ 	/* Pending child counts during tree propagation */
+ 	long			state_pending[MEMCG_NR_STAT];
+ 	unsigned long		events_pending[NR_VM_EVENT_ITEMS];
+ };
+ 
+ unsigned long memcg_page_state(struct mem_cgroup *memcg, int idx)
+ {
+ 	long x = READ_ONCE(memcg->vmstats->state[idx]);
+ #ifdef CONFIG_SMP
+ 	if (x < 0)
+ 		x = 0;
+ #endif
+ 	return x;
++>>>>>>> d396def5d86d (memcg: rearrange code)
  }
  
  /**
@@@ -1442,7 -1524,7 +1533,11 @@@ static inline unsigned long memcg_page_
  	return memcg_page_state(memcg, item) * memcg_page_state_unit(item);
  }
  
++<<<<<<< HEAD
 +static char *memory_stat_format(struct mem_cgroup *memcg)
++=======
+ static void memory_stat_format(struct mem_cgroup *memcg, char *buf, int bufsize)
++>>>>>>> d396def5d86d (memcg: rearrange code)
  {
  	struct seq_buf s;
  	int i;
* Unmerged path mm/memcontrol.c
