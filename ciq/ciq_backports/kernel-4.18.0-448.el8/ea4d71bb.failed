iommu/iova: Consolidate flush queue code

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit ea4d71bb5e3fc5c2b7b856bc7506439298f47a23
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/ea4d71bb.failed

Squash and simplify some of the freeing code, and move the init
and free routines down into the rest of the flush queue code to
obviate the forward declarations.

	Reviewed-by: John Garry <john.garry@huawei.com>
	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
Link: https://lore.kernel.org/r/b0dd4565e6646b6489599d7a1eaa362c75f53c95.1639753638.git.robin.murphy@arm.com
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit ea4d71bb5e3fc5c2b7b856bc7506439298f47a23)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/iova.c
diff --cc drivers/iommu/iova.c
index 1c18cdc4d102,e368fd3da0d2..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -85,63 -71,6 +83,66 @@@ init_iova_domain(struct iova_domain *io
  }
  EXPORT_SYMBOL_GPL(init_iova_domain);
  
++<<<<<<< HEAD
 +static bool has_iova_flush_queue(struct iova_domain *iovad)
 +{
 +	return !!iovad->fq;
 +}
 +
 +static void free_iova_flush_queue(struct iova_domain *iovad)
 +{
 +	if (!has_iova_flush_queue(iovad))
 +		return;
 +
 +	del_timer_sync(&iovad->fq_timer);
 +
 +	fq_destroy_all_entries(iovad);
 +
 +	free_percpu(iovad->fq);
 +
 +	iovad->fq         = NULL;
 +	iovad->flush_cb   = NULL;
 +	iovad->entry_dtor = NULL;
 +}
 +
 +int init_iova_flush_queue(struct iova_domain *iovad,
 +			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor)
 +{
 +	struct iova_fq __percpu *queue;
 +	int cpu;
 +
 +	atomic64_set(&iovad->fq_flush_start_cnt,  0);
 +	atomic64_set(&iovad->fq_flush_finish_cnt, 0);
 +
 +	queue = alloc_percpu(struct iova_fq);
 +	if (!queue)
 +		return -ENOMEM;
 +
 +	iovad->flush_cb   = flush_cb;
 +	iovad->entry_dtor = entry_dtor;
 +
 +	for_each_possible_cpu(cpu) {
 +		struct iova_fq *fq;
 +
 +		fq = per_cpu_ptr(queue, cpu);
 +		fq->head = 0;
 +		fq->tail = 0;
 +
 +		spin_lock_init(&fq->lock);
 +	}
 +
 +	smp_wmb();
 +
 +	iovad->fq = queue;
 +
 +	timer_setup(&iovad->fq_timer, fq_flush_timeout, 0);
 +	atomic_set(&iovad->fq_timer_on, 0);
 +
 +	return 0;
 +}
 +
++=======
++>>>>>>> ea4d71bb5e3f (iommu/iova: Consolidate flush queue code)
  static struct rb_node *
  __get_cached_rbnode(struct iova_domain *iovad, unsigned long limit_pfn)
  {
@@@ -602,27 -538,6 +603,30 @@@ static void iova_domain_flush(struct io
  	atomic64_inc(&iovad->fq_flush_finish_cnt);
  }
  
++<<<<<<< HEAD
 +static void fq_destroy_all_entries(struct iova_domain *iovad)
 +{
 +	int cpu;
 +
 +	/*
 +	 * This code runs when the iova_domain is being detroyed, so don't
 +	 * bother to free iovas, just call the entry_dtor on all remaining
 +	 * entries.
 +	 */
 +	if (!iovad->entry_dtor)
 +		return;
 +
 +	for_each_possible_cpu(cpu) {
 +		struct iova_fq *fq = per_cpu_ptr(iovad->fq, cpu);
 +		int idx;
 +
 +		fq_ring_for_each(idx, fq)
 +			iovad->entry_dtor(fq->entries[idx].data);
 +	}
 +}
 +
++=======
++>>>>>>> ea4d71bb5e3f (iommu/iova: Consolidate flush queue code)
  static void fq_flush_timeout(struct timer_list *t)
  {
  	struct iova_domain *iovad = from_timer(iovad, t, fq_timer);
* Unmerged path drivers/iommu/iova.c
