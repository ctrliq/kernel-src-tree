net/mlx5e: Fix broken SKB allocation in HW-GRO

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Khalid Manaa <khalidm@nvidia.com>
commit 7957837b816f11eecb9146235bb0715478f4c81f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/7957837b.failed

In case the HW doesn't perform header-data split, it will write the whole
packet into the data buffer in the WQ, in this case the SHAMPO CQE handler
couldn't use the header entry to build the SKB, instead it should allocate
a new memory to build the SKB using the function:
mlx5e_skb_from_cqe_mpwrq_nonlinear.

Fixes: f97d5c2a453e ("net/mlx5e: Add handle SHAMPO cqe support")
	Signed-off-by: Khalid Manaa <khalidm@nvidia.com>
	Reviewed-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 7957837b816f11eecb9146235bb0715478f4c81f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 19d50353de25,ee0a8f5206e3..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -1536,6 -1919,40 +1536,43 @@@ mlx5e_skb_from_cqe_shampo(struct mlx5e_
  		skb->len  += head_size;
  	}
  	return skb;
++<<<<<<< HEAD
++=======
+ }
+ 
+ static void
+ mlx5e_shampo_align_fragment(struct sk_buff *skb, u8 log_stride_sz)
+ {
+ 	skb_frag_t *last_frag = &skb_shinfo(skb)->frags[skb_shinfo(skb)->nr_frags - 1];
+ 	unsigned int frag_size = skb_frag_size(last_frag);
+ 	unsigned int frag_truesize;
+ 
+ 	frag_truesize = ALIGN(frag_size, BIT(log_stride_sz));
+ 	skb->truesize += frag_truesize - frag_size;
+ }
+ 
+ static void
+ mlx5e_shampo_flush_skb(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe, bool match)
+ {
+ 	struct sk_buff *skb = rq->hw_gro_data->skb;
+ 	struct mlx5e_rq_stats *stats = rq->stats;
+ 
+ 	stats->gro_skbs++;
+ 	if (likely(skb_shinfo(skb)->nr_frags))
+ 		mlx5e_shampo_align_fragment(skb, rq->mpwqe.log_stride_sz);
+ 	if (NAPI_GRO_CB(skb)->count > 1)
+ 		mlx5e_shampo_update_hdr(rq, cqe, match);
+ 	napi_gro_receive(rq->cq.napi, skb);
+ 	rq->hw_gro_data->skb = NULL;
+ }
+ 
+ static bool
+ mlx5e_hw_gro_skb_has_enough_space(struct sk_buff *skb, u16 data_bcnt)
+ {
+ 	int nr_frags = skb_shinfo(skb)->nr_frags;
+ 
+ 	return PAGE_SIZE * nr_frags + data_bcnt <= GSO_MAX_SIZE;
++>>>>>>> 7957837b816f (net/mlx5e: Fix broken SKB allocation in HW-GRO)
  }
  
  static void
@@@ -1561,8 -1978,12 +1598,16 @@@ static void mlx5e_handle_rx_cqe_mpwrq_s
  	u32 cqe_bcnt		= mpwrq_get_cqe_byte_cnt(cqe);
  	u16 wqe_id		= be16_to_cpu(cqe->wqe_id);
  	u32 page_idx		= wqe_offset >> PAGE_SHIFT;
++<<<<<<< HEAD
++=======
+ 	u16 head_size		= cqe->shampo.header_size;
+ 	struct sk_buff **skb	= &rq->hw_gro_data->skb;
+ 	bool flush		= cqe->shampo.flush;
+ 	bool match		= cqe->shampo.match;
+ 	struct mlx5e_rq_stats *stats = rq->stats;
++>>>>>>> 7957837b816f (net/mlx5e: Fix broken SKB allocation in HW-GRO)
  	struct mlx5e_rx_wqe_ll *wqe;
 +	struct sk_buff *skb = NULL;
  	struct mlx5e_dma_info *di;
  	struct mlx5e_mpw_info *wi;
  	struct mlx5_wq_ll *wq;
@@@ -1584,16 -2002,45 +1629,52 @@@
  		goto mpwrq_cqe_out;
  	}
  
 -	stats->gro_match_packets += match;
 +	skb = mlx5e_skb_from_cqe_shampo(rq, wi, cqe, header_index);
 +
++<<<<<<< HEAD
 +	if (unlikely(!skb))
 +		goto free_hd_entry;
  
 +	di = &wi->umr.dma_info[page_idx];
 +	mlx5e_fill_skb_data(skb, rq, di, data_bcnt, data_offset);
++=======
+ 	if (*skb && (!match || !(mlx5e_hw_gro_skb_has_enough_space(*skb, data_bcnt)))) {
+ 		match = false;
+ 		mlx5e_shampo_flush_skb(rq, cqe, match);
+ 	}
+ 
+ 	if (!*skb) {
+ 		if (likely(head_size))
+ 			*skb = mlx5e_skb_from_cqe_shampo(rq, wi, cqe, header_index);
+ 		else
+ 			*skb = mlx5e_skb_from_cqe_mpwrq_nonlinear(rq, wi, cqe_bcnt, data_offset,
+ 								  page_idx);
+ 		if (unlikely(!*skb))
+ 			goto free_hd_entry;
+ 
+ 		NAPI_GRO_CB(*skb)->count = 1;
+ 		skb_shinfo(*skb)->gso_size = cqe_bcnt - head_size;
+ 	} else {
+ 		NAPI_GRO_CB(*skb)->count++;
+ 		if (NAPI_GRO_CB(*skb)->count == 2 &&
+ 		    rq->hw_gro_data->fk.basic.n_proto == htons(ETH_P_IP)) {
+ 			void *hd_addr = mlx5e_shampo_get_packet_hd(rq, header_index);
+ 			int nhoff = ETH_HLEN + rq->hw_gro_data->fk.control.thoff -
+ 				    sizeof(struct iphdr);
+ 			struct iphdr *iph = (struct iphdr *)(hd_addr + nhoff);
+ 
+ 			rq->hw_gro_data->second_ip_id = ntohs(iph->id);
+ 		}
+ 	}
+ 
+ 	if (likely(head_size)) {
+ 		di = &wi->umr.dma_info[page_idx];
+ 		mlx5e_fill_skb_data(*skb, rq, di, data_bcnt, data_offset);
+ 	}
++>>>>>>> 7957837b816f (net/mlx5e: Fix broken SKB allocation in HW-GRO)
  
 -	mlx5e_shampo_complete_rx_cqe(rq, cqe, cqe_bcnt, *skb);
 -	if (flush)
 -		mlx5e_shampo_flush_skb(rq, cqe, match);
 +	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
 +	napi_gro_receive(rq->cq.napi, skb);
  free_hd_entry:
  	mlx5e_free_rx_shampo_hd_entry(rq, header_index);
  mpwrq_cqe_out:
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
