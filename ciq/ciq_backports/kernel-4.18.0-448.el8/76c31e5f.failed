net/mlx5e: Use FW limitation for max MPW WQEBBs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Aya Levin <ayal@nvidia.com>
commit 76c31e5f758596509fbab120b8d055bf927ed165
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/76c31e5f.failed

Calculate maximal count of MPW WQEBBs on SQ's creation and store it
there. Remove MLX5E_TX_MPW_MAX_NUM_DS and MLX5E_TX_MPW_MAX_WQEBBS.
Update mlx5e_tx_mpwqe_is_full() and mlx5e_xdp_mpqwe_is_full() .

	Signed-off-by: Aya Levin <ayal@nvidia.com>
	Reviewed-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 76c31e5f758596509fbab120b8d055bf927ed165)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en.h
#	drivers/net/ethernet/mellanox/mlx5/core/en/params.c
#	drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en.h
index 534c35c7d233,99529e238fc4..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@@ -168,7 -173,8 +168,12 @@@ struct page_pool
  	ALIGN_DOWN(MLX5E_KLM_MAX_ENTRIES_PER_WQE(wqe_size), MLX5_UMR_KLM_ALIGNMENT)
  
  #define MLX5E_MAX_KLM_PER_WQE(mdev) \
++<<<<<<< HEAD
 +	MLX5E_KLM_ENTRIES_PER_WQE(MLX5E_TX_MPW_MAX_NUM_DS << MLX5_MKEY_BSF_OCTO_SIZE)
++=======
+ 	MLX5E_KLM_ENTRIES_PER_WQE(mlx5e_get_sw_max_sq_mpw_wqebbs(mlx5e_get_max_sq_wqebbs(mdev)) \
+ 				   << MLX5_MKEY_BSF_OCTO_SIZE)
++>>>>>>> 76c31e5f7585 (net/mlx5e: Use FW limitation for max MPW WQEBBs)
  
  #define MLX5E_MSG_LEVEL			NETIF_MSG_LINK
  
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en/params.c
index b2dee648e10a,d41936d65483..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
@@@ -620,17 -668,80 +620,82 @@@ static u8 mlx5e_get_rq_log_wq_sz(void *
  	return MLX5_GET(wq, wq, log_wq_sz);
  }
  
++<<<<<<< HEAD
 +static u8 mlx5e_build_icosq_log_wq_sz(struct mlx5e_params *params,
++=======
+ /* This function calculates the maximum number of headers entries that are needed
+  * per WQE, the formula is based on the size of the reservations and the
+  * restriction we have about max packets for reservation that is equal to max
+  * headers per reservation.
+  */
+ u32 mlx5e_shampo_hd_per_wqe(struct mlx5_core_dev *mdev,
+ 			    struct mlx5e_params *params,
+ 			    struct mlx5e_rq_param *rq_param)
+ {
+ 	int resv_size = BIT(mlx5e_shampo_get_log_rsrv_size(mdev, params)) * PAGE_SIZE;
+ 	u16 num_strides = BIT(mlx5e_mpwqe_get_log_num_strides(mdev, params, NULL));
+ 	int pkt_per_resv = BIT(mlx5e_shampo_get_log_pkt_per_rsrv(mdev, params));
+ 	u8 log_stride_sz = mlx5e_mpwqe_get_log_stride_size(mdev, params, NULL);
+ 	int wqe_size = BIT(log_stride_sz) * num_strides;
+ 	u32 hd_per_wqe;
+ 
+ 	/* Assumption: hd_per_wqe % 8 == 0. */
+ 	hd_per_wqe = (wqe_size / resv_size) * pkt_per_resv;
+ 	mlx5_core_dbg(mdev, "%s hd_per_wqe = %d rsrv_size = %d wqe_size = %d pkt_per_resv = %d\n",
+ 		      __func__, hd_per_wqe, resv_size, wqe_size, pkt_per_resv);
+ 	return hd_per_wqe;
+ }
+ 
+ /* This function calculates the maximum number of headers entries that are needed
+  * for the WQ, this value is uesed to allocate the header buffer in HW, thus
+  * must be a pow of 2.
+  */
+ u32 mlx5e_shampo_hd_per_wq(struct mlx5_core_dev *mdev,
+ 			   struct mlx5e_params *params,
+ 			   struct mlx5e_rq_param *rq_param)
+ {
+ 	void *wqc = MLX5_ADDR_OF(rqc, rq_param->rqc, wq);
+ 	int wq_size = BIT(MLX5_GET(wq, wqc, log_wq_sz));
+ 	u32 hd_per_wqe, hd_per_wq;
+ 
+ 	hd_per_wqe = mlx5e_shampo_hd_per_wqe(mdev, params, rq_param);
+ 	hd_per_wq = roundup_pow_of_two(hd_per_wqe * wq_size);
+ 	return hd_per_wq;
+ }
+ 
+ static u32 mlx5e_shampo_icosq_sz(struct mlx5_core_dev *mdev,
+ 				 struct mlx5e_params *params,
+ 				 struct mlx5e_rq_param *rq_param)
+ {
+ 	int max_num_of_umr_per_wqe, max_hd_per_wqe, max_klm_per_umr, rest;
+ 	void *wqc = MLX5_ADDR_OF(rqc, rq_param->rqc, wq);
+ 	int wq_size = BIT(MLX5_GET(wq, wqc, log_wq_sz));
+ 	u32 wqebbs;
+ 
+ 	max_klm_per_umr = MLX5E_MAX_KLM_PER_WQE(mdev);
+ 	max_hd_per_wqe = mlx5e_shampo_hd_per_wqe(mdev, params, rq_param);
+ 	max_num_of_umr_per_wqe = max_hd_per_wqe / max_klm_per_umr;
+ 	rest = max_hd_per_wqe % max_klm_per_umr;
+ 	wqebbs = MLX5E_KLM_UMR_WQEBBS(max_klm_per_umr) * max_num_of_umr_per_wqe;
+ 	if (rest)
+ 		wqebbs += MLX5E_KLM_UMR_WQEBBS(rest);
+ 	wqebbs *= wq_size;
+ 	return wqebbs;
+ }
+ 
+ static u8 mlx5e_build_icosq_log_wq_sz(struct mlx5_core_dev *mdev,
+ 				      struct mlx5e_params *params,
++>>>>>>> 76c31e5f7585 (net/mlx5e: Use FW limitation for max MPW WQEBBs)
  				      struct mlx5e_rq_param *rqp)
  {
 -	u32 wqebbs;
 -
 -	/* MLX5_WQ_TYPE_CYCLIC */
 -	if (params->rq_wq_type != MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ)
 +	switch (params->rq_wq_type) {
 +	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
 +		return max_t(u8, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE,
 +			     order_base_2(MLX5E_UMR_WQEBBS) +
 +			     mlx5e_get_rq_log_wq_sz(rqp->rqc));
 +	default: /* MLX5_WQ_TYPE_CYCLIC */
  		return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
 -
 -	wqebbs = MLX5E_UMR_WQEBBS * BIT(mlx5e_get_rq_log_wq_sz(rqp->rqc));
 -	if (params->packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO)
 -		wqebbs += mlx5e_shampo_icosq_sz(mdev, params, rqp);
 -	return max_t(u8, MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE, order_base_2(wqebbs));
 +	}
  }
  
  static u8 mlx5e_build_async_icosq_log_wq_sz(struct mlx5_core_dev *mdev)
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index 19d50353de25,91fdf957cd7c..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@@ -500,6 -503,155 +500,158 @@@ static void mlx5e_post_rx_mpwqe(struct 
  	mlx5_wq_ll_update_db_record(wq);
  }
  
++<<<<<<< HEAD
++=======
+ /* This function returns the size of the continuous free space inside a bitmap
+  * that starts from first and no longer than len including circular ones.
+  */
+ static int bitmap_find_window(unsigned long *bitmap, int len,
+ 			      int bitmap_size, int first)
+ {
+ 	int next_one, count;
+ 
+ 	next_one = find_next_bit(bitmap, bitmap_size, first);
+ 	if (next_one == bitmap_size) {
+ 		if (bitmap_size - first >= len)
+ 			return len;
+ 		next_one = find_next_bit(bitmap, bitmap_size, 0);
+ 		count = next_one + bitmap_size - first;
+ 	} else {
+ 		count = next_one - first;
+ 	}
+ 
+ 	return min(len, count);
+ }
+ 
+ static void build_klm_umr(struct mlx5e_icosq *sq, struct mlx5e_umr_wqe *umr_wqe,
+ 			  __be32 key, u16 offset, u16 klm_len, u16 wqe_bbs)
+ {
+ 	memset(umr_wqe, 0, offsetof(struct mlx5e_umr_wqe, inline_klms));
+ 	umr_wqe->ctrl.opmod_idx_opcode =
+ 		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
+ 			     MLX5_OPCODE_UMR);
+ 	umr_wqe->ctrl.umr_mkey = key;
+ 	umr_wqe->ctrl.qpn_ds = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT)
+ 					    | MLX5E_KLM_UMR_DS_CNT(klm_len));
+ 	umr_wqe->uctrl.flags = MLX5_UMR_TRANSLATION_OFFSET_EN | MLX5_UMR_INLINE;
+ 	umr_wqe->uctrl.xlt_offset = cpu_to_be16(offset);
+ 	umr_wqe->uctrl.xlt_octowords = cpu_to_be16(klm_len);
+ 	umr_wqe->uctrl.mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);
+ }
+ 
+ static int mlx5e_build_shampo_hd_umr(struct mlx5e_rq *rq,
+ 				     struct mlx5e_icosq *sq,
+ 				     u16 klm_entries, u16 index)
+ {
+ 	struct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;
+ 	u16 entries, pi, header_offset, err, wqe_bbs, new_entries;
+ 	u32 lkey = rq->mdev->mlx5e_res.hw_objs.mkey;
+ 	struct page *page = shampo->last_page;
+ 	u64 addr = shampo->last_addr;
+ 	struct mlx5e_dma_info *dma_info;
+ 	struct mlx5e_umr_wqe *umr_wqe;
+ 	int headroom, i;
+ 
+ 	headroom = rq->buff.headroom;
+ 	new_entries = klm_entries - (shampo->pi & (MLX5_UMR_KLM_ALIGNMENT - 1));
+ 	entries = ALIGN(klm_entries, MLX5_UMR_KLM_ALIGNMENT);
+ 	wqe_bbs = MLX5E_KLM_UMR_WQEBBS(entries);
+ 	pi = mlx5e_icosq_get_next_pi(sq, wqe_bbs);
+ 	umr_wqe = mlx5_wq_cyc_get_wqe(&sq->wq, pi);
+ 	build_klm_umr(sq, umr_wqe, shampo->key, index, entries, wqe_bbs);
+ 
+ 	for (i = 0; i < entries; i++, index++) {
+ 		dma_info = &shampo->info[index];
+ 		if (i >= klm_entries || (index < shampo->pi && shampo->pi - index <
+ 					 MLX5_UMR_KLM_ALIGNMENT))
+ 			goto update_klm;
+ 		header_offset = (index & (MLX5E_SHAMPO_WQ_HEADER_PER_PAGE - 1)) <<
+ 			MLX5E_SHAMPO_LOG_MAX_HEADER_ENTRY_SIZE;
+ 		if (!(header_offset & (PAGE_SIZE - 1))) {
+ 			err = mlx5e_page_alloc(rq, dma_info);
+ 			if (unlikely(err))
+ 				goto err_unmap;
+ 			addr = dma_info->addr;
+ 			page = dma_info->page;
+ 		} else {
+ 			dma_info->addr = addr + header_offset;
+ 			dma_info->page = page;
+ 		}
+ 
+ update_klm:
+ 		umr_wqe->inline_klms[i].bcount =
+ 			cpu_to_be32(MLX5E_RX_MAX_HEAD);
+ 		umr_wqe->inline_klms[i].key    = cpu_to_be32(lkey);
+ 		umr_wqe->inline_klms[i].va     =
+ 			cpu_to_be64(dma_info->addr + headroom);
+ 	}
+ 
+ 	sq->db.wqe_info[pi] = (struct mlx5e_icosq_wqe_info) {
+ 		.wqe_type	= MLX5E_ICOSQ_WQE_SHAMPO_HD_UMR,
+ 		.num_wqebbs	= wqe_bbs,
+ 		.shampo.len	= new_entries,
+ 	};
+ 
+ 	shampo->pi = (shampo->pi + new_entries) & (shampo->hd_per_wq - 1);
+ 	shampo->last_page = page;
+ 	shampo->last_addr = addr;
+ 	sq->pc += wqe_bbs;
+ 	sq->doorbell_cseg = &umr_wqe->ctrl;
+ 
+ 	return 0;
+ 
+ err_unmap:
+ 	while (--i >= 0) {
+ 		dma_info = &shampo->info[--index];
+ 		if (!(i & (MLX5E_SHAMPO_WQ_HEADER_PER_PAGE - 1))) {
+ 			dma_info->addr = ALIGN_DOWN(dma_info->addr, PAGE_SIZE);
+ 			mlx5e_page_release(rq, dma_info, true);
+ 		}
+ 	}
+ 	rq->stats->buff_alloc_err++;
+ 	return err;
+ }
+ 
+ static int mlx5e_alloc_rx_hd_mpwqe(struct mlx5e_rq *rq)
+ {
+ 	struct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;
+ 	u16 klm_entries, num_wqe, index, entries_before;
+ 	struct mlx5e_icosq *sq = rq->icosq;
+ 	int i, err, max_klm_entries, len;
+ 
+ 	max_klm_entries = MLX5E_MAX_KLM_PER_WQE(rq->mdev);
+ 	klm_entries = bitmap_find_window(shampo->bitmap,
+ 					 shampo->hd_per_wqe,
+ 					 shampo->hd_per_wq, shampo->pi);
+ 	if (!klm_entries)
+ 		return 0;
+ 
+ 	klm_entries += (shampo->pi & (MLX5_UMR_KLM_ALIGNMENT - 1));
+ 	index = ALIGN_DOWN(shampo->pi, MLX5_UMR_KLM_ALIGNMENT);
+ 	entries_before = shampo->hd_per_wq - index;
+ 
+ 	if (unlikely(entries_before < klm_entries))
+ 		num_wqe = DIV_ROUND_UP(entries_before, max_klm_entries) +
+ 			  DIV_ROUND_UP(klm_entries - entries_before, max_klm_entries);
+ 	else
+ 		num_wqe = DIV_ROUND_UP(klm_entries, max_klm_entries);
+ 
+ 	for (i = 0; i < num_wqe; i++) {
+ 		len = (klm_entries > max_klm_entries) ? max_klm_entries :
+ 							klm_entries;
+ 		if (unlikely(index + len > shampo->hd_per_wq))
+ 			len = shampo->hd_per_wq - index;
+ 		err = mlx5e_build_shampo_hd_umr(rq, sq, len, index);
+ 		if (unlikely(err))
+ 			return err;
+ 		index = (index + len) & (rq->mpwqe.shampo->hd_per_wq - 1);
+ 		klm_entries -= len;
+ 	}
+ 
+ 	return 0;
+ }
+ 
++>>>>>>> 76c31e5f7585 (net/mlx5e: Use FW limitation for max MPW WQEBBs)
  static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
  {
  	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[ix];
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en.h
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en/params.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h b/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h
index ca7fd21c0dda..89caabc69361 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h
@@ -9,19 +9,6 @@
 
 #define MLX5E_TX_WQE_EMPTY_DS_COUNT (sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS)
 
-/* The mult of MLX5_SEND_WQE_MAX_WQEBBS * MLX5_SEND_WQEBB_NUM_DS
- * (16 * 4 == 64) does not fit in the 6-bit DS field of Ctrl Segment.
- * We use a bound lower that MLX5_SEND_WQE_MAX_WQEBBS to let a
- * full-session WQE be cache-aligned.
- */
-#if L1_CACHE_BYTES < 128
-#define MLX5E_TX_MPW_MAX_WQEBBS (MLX5_SEND_WQE_MAX_WQEBBS - 1)
-#else
-#define MLX5E_TX_MPW_MAX_WQEBBS (MLX5_SEND_WQE_MAX_WQEBBS - 2)
-#endif
-
-#define MLX5E_TX_MPW_MAX_NUM_DS (MLX5E_TX_MPW_MAX_WQEBBS * MLX5_SEND_WQEBB_NUM_DS)
-
 #define INL_HDR_START_SZ (sizeof(((struct mlx5_wqe_eth_seg *)NULL)->inline_hdr.start))
 
 #define MLX5E_RX_ERR_CQE(cqe) (get_cqe_opcode(cqe) != MLX5_CQE_RESP_SEND)
@@ -298,9 +285,9 @@ mlx5e_tx_dma_unmap(struct device *pdev, struct mlx5e_sq_dma *dma)
 void mlx5e_sq_xmit_simple(struct mlx5e_txqsq *sq, struct sk_buff *skb, bool xmit_more);
 void mlx5e_tx_mpwqe_ensure_complete(struct mlx5e_txqsq *sq);
 
-static inline bool mlx5e_tx_mpwqe_is_full(struct mlx5e_tx_mpwqe *session)
+static inline bool mlx5e_tx_mpwqe_is_full(struct mlx5e_tx_mpwqe *session, u8 max_sq_mpw_wqebbs)
 {
-	return session->ds_count == MLX5E_TX_MPW_MAX_NUM_DS;
+	return session->ds_count == max_sq_mpw_wqebbs * MLX5_SEND_WQEBB_NUM_DS;
 }
 
 static inline void mlx5e_rqwq_reset(struct mlx5e_rq *rq)
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
index 2daa4d0e5e21..b9209ad7f2a8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
@@ -199,7 +199,7 @@ static void mlx5e_xdp_mpwqe_session_start(struct mlx5e_xdpsq *sq)
 	struct mlx5e_tx_wqe *wqe;
 	u16 pi;
 
-	pi = mlx5e_xdpsq_get_next_pi(sq, MLX5E_TX_MPW_MAX_WQEBBS);
+	pi = mlx5e_xdpsq_get_next_pi(sq, sq->max_sq_mpw_wqebbs);
 	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
 	net_prefetchw(wqe->data);
 
@@ -286,7 +286,7 @@ mlx5e_xmit_xdp_frame_mpwqe(struct mlx5e_xdpsq *sq, struct mlx5e_xmit_data *xdptx
 
 	mlx5e_xdp_mpwqe_add_dseg(sq, xdptxd, stats);
 
-	if (unlikely(mlx5e_xdp_mpqwe_is_full(session)))
+	if (unlikely(mlx5e_xdp_mpqwe_is_full(session, sq->max_sq_mpw_wqebbs)))
 		mlx5e_xdp_mpwqe_complete(sq);
 
 	mlx5e_xdpi_fifo_push(&sq->db.xdpi_fifo, xdpi);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 8d991c3b7a50..c62f11d7ef6a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -123,12 +123,13 @@ static inline bool mlx5e_xdp_get_inline_state(struct mlx5e_xdpsq *sq, bool cur)
 	return cur;
 }
 
-static inline bool mlx5e_xdp_mpqwe_is_full(struct mlx5e_tx_mpwqe *session)
+static inline bool mlx5e_xdp_mpqwe_is_full(struct mlx5e_tx_mpwqe *session, u8 max_sq_mpw_wqebbs)
 {
 	if (session->inline_on)
 		return session->ds_count + MLX5E_XDP_INLINE_WQE_MAX_DS_CNT >
-		       MLX5E_TX_MPW_MAX_NUM_DS;
-	return mlx5e_tx_mpwqe_is_full(session);
+		       max_sq_mpw_wqebbs * MLX5_SEND_WQEBB_NUM_DS;
+
+	return mlx5e_tx_mpwqe_is_full(session, max_sq_mpw_wqebbs);
 }
 
 struct mlx5e_xdp_wqe_info {
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
index 24f78650b69e..5b3e4754c0f4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c
@@ -991,6 +991,7 @@ static int mlx5e_alloc_xdpsq(struct mlx5e_channel *c,
 			&c->priv->channel_stats[c->ix]->rq_xdpsq;
 	sq->max_sq_wqebbs = mlx5e_get_max_sq_wqebbs(mdev);
 	sq->stop_room = MLX5E_STOP_ROOM(sq->max_sq_wqebbs);
+	sq->max_sq_mpw_wqebbs = mlx5e_get_sw_max_sq_mpw_wqebbs(sq->max_sq_wqebbs);
 
 	param->wq.db_numa_node = cpu_to_node(c->cpu);
 	err = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, wq, &sq->wq_ctrl);
@@ -1152,6 +1153,7 @@ static int mlx5e_alloc_txqsq(struct mlx5e_channel *c,
 	sq->min_inline_mode = params->tx_min_inline_mode;
 	sq->hw_mtu    = MLX5E_SW2HW_MTU(params, params->sw_mtu);
 	sq->max_sq_wqebbs = mlx5e_get_max_sq_wqebbs(mdev);
+	sq->max_sq_mpw_wqebbs = mlx5e_get_sw_max_sq_mpw_wqebbs(sq->max_sq_wqebbs);
 	INIT_WORK(&sq->recover_work, mlx5e_tx_err_cqe_work);
 	if (!MLX5_CAP_ETH(mdev, wqe_vlan_insert))
 		set_bit(MLX5E_SQ_STATE_VLAN_NEED_L2_INLINE, &sq->state);
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index 5e9914c46862..432d9544e61a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -545,7 +545,7 @@ static void mlx5e_tx_mpwqe_session_start(struct mlx5e_txqsq *sq,
 	struct mlx5e_tx_wqe *wqe;
 	u16 pi;
 
-	pi = mlx5e_txqsq_get_next_pi(sq, MLX5E_TX_MPW_MAX_WQEBBS);
+	pi = mlx5e_txqsq_get_next_pi(sq, sq->max_sq_mpw_wqebbs);
 	wqe = MLX5E_TX_FETCH_WQE(sq, pi);
 	net_prefetchw(wqe->data);
 
@@ -646,7 +646,7 @@ mlx5e_sq_xmit_mpwqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 
 	mlx5e_tx_skb_update_hwts_flags(skb);
 
-	if (unlikely(mlx5e_tx_mpwqe_is_full(&sq->mpwqe))) {
+	if (unlikely(mlx5e_tx_mpwqe_is_full(&sq->mpwqe, sq->max_sq_mpw_wqebbs))) {
 		/* Might stop the queue and affect the retval of __netdev_tx_sent_queue. */
 		cseg = mlx5e_tx_mpwqe_session_complete(sq);
 
