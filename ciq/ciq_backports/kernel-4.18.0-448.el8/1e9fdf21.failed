mmu_gather: Remove per arch tlb_{start,end}_vma()

jira LE-1907
cve {CVE-2022-39188
cve [2130166]
cve Long)
cve (Waiman
cve start,end_vma()
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 1e9fdf21a4339b102539f476a9842e7526c01939
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/1e9fdf21.failed

Scattered across the archs are 3 basic forms of tlb_{start,end}_vma().
Provide two new MMU_GATHER_knobs to enumerate them and remove the per
arch tlb_{start,end}_vma() implementations.

 - MMU_GATHER_NO_FLUSH_CACHE indicates the arch has flush_cache_range()
   but does *NOT* want to call it for each VMA.

 - MMU_GATHER_MERGE_VMAS indicates the arch wants to merge the
   invalidate across multiple VMAs if possible.

With these it is possible to capture the three forms:

  1) empty stubs;
     select MMU_GATHER_NO_FLUSH_CACHE and MMU_GATHER_MERGE_VMAS

  2) start: flush_cache_range(), end: empty;
     select MMU_GATHER_MERGE_VMAS

  3) start: flush_cache_range(), end: flush_tlb_range();
     default

Obviously, if the architecture does not have flush_cache_range() then
it also doesn't need to select MMU_GATHER_NO_FLUSH_CACHE.

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Will Deacon <will@kernel.org>
	Cc: David Miller <davem@davemloft.net>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 1e9fdf21a4339b102539f476a9842e7526c01939)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/csky/include/asm/tlb.h
#	arch/loongarch/Kconfig
#	arch/loongarch/include/asm/tlb.h
#	arch/powerpc/Kconfig
#	arch/s390/Kconfig
#	arch/sparc/Kconfig
#	arch/x86/Kconfig
#	include/asm-generic/tlb.h
diff --cc arch/powerpc/Kconfig
index 068c558b1ac4,c235648fae23..000000000000
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@@ -227,14 -254,22 +227,20 @@@ config PP
  	select IOMMU_HELPER			if PPC64
  	select IRQ_DOMAIN
  	select IRQ_FORCED_THREADING
++<<<<<<< HEAD
++=======
+ 	select MMU_GATHER_PAGE_SIZE
+ 	select MMU_GATHER_RCU_TABLE_FREE
+ 	select MMU_GATHER_MERGE_VMAS
++>>>>>>> 1e9fdf21a433 (mmu_gather: Remove per arch tlb_{start,end}_vma())
  	select MODULES_USE_ELF_RELA
  	select NEED_DMA_MAP_STATE		if PPC64 || NOT_COHERENT_CACHE
 -	select NEED_PER_CPU_EMBED_FIRST_CHUNK	if PPC64
 -	select NEED_PER_CPU_PAGE_FIRST_CHUNK	if PPC64
  	select NEED_SG_DMA_LENGTH
  	select OF
 -	select OF_DMA_DEFAULT_COHERENT		if !NOT_COHERENT_CACHE
  	select OF_EARLY_FLATTREE
 +	select OF_RESERVED_MEM
  	select OLD_SIGACTION			if PPC32
  	select OLD_SIGSUSPEND
 -	select PCI_DOMAINS			if PCI
 -	select PCI_MSI_ARCH_FALLBACKS		if PCI_MSI
 -	select PCI_SYSCALL			if PCI
  	select PPC_DAWR				if PPC64
  	select RTC_LIB
  	select SPARSE_IRQ
diff --cc arch/s390/Kconfig
index 53b3712d90ef,5a1a8dfda6f8..000000000000
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@@ -156,33 -183,45 +156,42 @@@ config S39
  	select HAVE_KRETPROBES
  	select HAVE_KVM
  	select HAVE_LIVEPATCH
 +	select HAVE_PERF_REGS
 +	select HAVE_PERF_USER_STACK_DUMP
  	select HAVE_MEMBLOCK_PHYS_MAP
 +	select HAVE_MMU_GATHER_NO_GATHER
  	select HAVE_MOD_ARCH_SPECIFIC
 -	select HAVE_NMI
 -	select HAVE_NOP_MCOUNT
 -	select HAVE_PCI
 +	select HAVE_OPROFILE
  	select HAVE_PERF_EVENTS
 -	select HAVE_PERF_REGS
 -	select HAVE_PERF_USER_STACK_DUMP
 +	select HAVE_RCU_TABLE_FREE
  	select HAVE_REGS_AND_STACK_ACCESS_API
 -	select HAVE_RELIABLE_STACKTRACE
  	select HAVE_RSEQ
 -	select HAVE_SAMPLE_FTRACE_DIRECT
 -	select HAVE_SAMPLE_FTRACE_DIRECT_MULTI
 -	select HAVE_SOFTIRQ_ON_OWN_STACK
  	select HAVE_SYSCALL_TRACEPOINTS
  	select HAVE_VIRT_CPU_ACCOUNTING
++<<<<<<< HEAD
++=======
+ 	select HAVE_VIRT_CPU_ACCOUNTING_IDLE
+ 	select IOMMU_HELPER		if PCI
+ 	select IOMMU_SUPPORT		if PCI
+ 	select MMU_GATHER_NO_GATHER
+ 	select MMU_GATHER_RCU_TABLE_FREE
+ 	select MMU_GATHER_MERGE_VMAS
++>>>>>>> 1e9fdf21a433 (mmu_gather: Remove per arch tlb_{start,end}_vma())
  	select MODULES_USE_ELF_RELA
 -	select NEED_DMA_MAP_STATE	if PCI
 -	select NEED_SG_DMA_LENGTH	if PCI
  	select OLD_SIGACTION
  	select OLD_SIGSUSPEND3
 -	select PCI_DOMAINS		if PCI
 -	select PCI_MSI			if PCI
 -	select PCI_MSI_ARCH_FALLBACKS	if PCI_MSI
  	select SPARSE_IRQ
 -	select SWIOTLB
  	select SYSCTL_EXCEPTION_TRACE
  	select THREAD_INFO_IN_TASK
 -	select TRACE_IRQFLAGS_SUPPORT
  	select TTY
  	select VIRT_CPU_ACCOUNTING
 -	select ZONE_DMA
 -	# Note: keep the above list sorted alphabetically
 +	select ARCH_HAS_SCALED_CPUTIME
 +	select VIRT_TO_BUS
 +	select HAVE_NMI
 +	select ARCH_HAS_FORCE_DMA_UNENCRYPTED
 +	select SWIOTLB
 +	select GENERIC_ALLOCATOR
 +
  
  config SCHED_OMIT_FRAME_POINTER
  	def_bool y
diff --cc arch/sparc/Kconfig
index 12d490d796fe,4f7d1dfbc608..000000000000
--- a/arch/sparc/Kconfig
+++ b/arch/sparc/Kconfig
@@@ -59,8 -66,9 +59,14 @@@ config SPARC6
  	select HAVE_FUNCTION_GRAPH_TRACER
  	select HAVE_KRETPROBES
  	select HAVE_KPROBES
++<<<<<<< HEAD
 +	select HAVE_RCU_TABLE_FREE if SMP
 +	select HAVE_MEMBLOCK_NODE_MAP
++=======
+ 	select MMU_GATHER_RCU_TABLE_FREE if SMP
+ 	select MMU_GATHER_MERGE_VMAS
+ 	select MMU_GATHER_NO_FLUSH_CACHE
++>>>>>>> 1e9fdf21a433 (mmu_gather: Remove per arch tlb_{start,end}_vma())
  	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
  	select HAVE_DYNAMIC_FTRACE
  	select HAVE_FTRACE_MCOUNT_RECORD
diff --cc arch/x86/Kconfig
index 23844bb111f3,7fff10e15969..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -201,16 -241,25 +201,22 @@@ config X8
  	select HAVE_PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
  	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI
 -	select HAVE_PCI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
++<<<<<<< HEAD
 +	select HAVE_RCU_TABLE_FREE		if PARAVIRT
 +	select HAVE_RCU_TABLE_INVALIDATE	if HAVE_RCU_TABLE_FREE
++=======
+ 	select MMU_GATHER_RCU_TABLE_FREE	if PARAVIRT
+ 	select MMU_GATHER_MERGE_VMAS
+ 	select HAVE_POSIX_CPU_TIMERS_TASK_WORK
++>>>>>>> 1e9fdf21a433 (mmu_gather: Remove per arch tlb_{start,end}_vma())
  	select HAVE_REGS_AND_STACK_ACCESS_API
 -	select HAVE_RELIABLE_STACKTRACE		if UNWINDER_ORC || STACK_VALIDATION
 -	select HAVE_FUNCTION_ARG_ACCESS_API
 -	select HAVE_SETUP_PER_CPU_AREA
 -	select HAVE_SOFTIRQ_ON_OWN_STACK
 +	select HAVE_RELIABLE_STACKTRACE		if X86_64 && (UNWINDER_FRAME_POINTER || UNWINDER_ORC) && STACK_VALIDATION
  	select HAVE_STACKPROTECTOR		if CC_HAS_SANE_STACKPROTECTOR
 -	select HAVE_STACK_VALIDATION		if HAVE_OBJTOOL
 -	select HAVE_STATIC_CALL
 -	select HAVE_STATIC_CALL_INLINE		if HAVE_OBJTOOL
 -	select HAVE_PREEMPT_DYNAMIC_CALL
 +	select HAVE_STACK_VALIDATION		if X86_64
  	select HAVE_RSEQ
  	select HAVE_SYSCALL_TRACEPOINTS
 -	select HAVE_UACCESS_VALIDATION		if HAVE_OBJTOOL
  	select HAVE_UNSTABLE_SCHED_CLOCK
  	select HAVE_USER_RETURN_NOTIFIER
  	select HAVE_GENERIC_VDSO
diff --cc include/asm-generic/tlb.h
index 27ac8002fb01,c1f03c1acbfc..000000000000
--- a/include/asm-generic/tlb.h
+++ b/include/asm-generic/tlb.h
@@@ -141,50 -150,52 +141,79 @@@
   *  When used, an architecture is expected to provide __tlb_remove_table()
   *  which does the actual freeing of these pages.
   *
 - *  MMU_GATHER_RCU_TABLE_FREE
 + *  HAVE_RCU_TABLE_INVALIDATE
   *
 - *  Like MMU_GATHER_TABLE_FREE, and adds semi-RCU semantics to the free (see
 - *  comment below).
 - *
 - *  Useful if your architecture doesn't use IPIs for remote TLB invalidates
 - *  and therefore doesn't naturally serialize with software page-table walkers.
 + *  This makes HAVE_RCU_TABLE_FREE call tlb_flush_mmu_tlbonly() before freeing
 + *  the page-table pages. Required if you use HAVE_RCU_TABLE_FREE and your
 + *  architecture uses the Linux page-tables natively.
   *
+  *  MMU_GATHER_NO_FLUSH_CACHE
+  *
+  *  Indicates the architecture has flush_cache_range() but it needs *NOT* be called
+  *  before unmapping a VMA.
+  *
+  *  NOTE: strictly speaking we shouldn't have this knob and instead rely on
+  *	  flush_cache_range() being a NOP, except Sparc64 seems to be
+  *	  different here.
+  *
+  *  MMU_GATHER_MERGE_VMAS
+  *
+  *  Indicates the architecture wants to merge ranges over VMAs; typical when
+  *  multiple range invalidates are more expensive than a full invalidate.
+  *
   *  MMU_GATHER_NO_RANGE
   *
++<<<<<<< HEAD
 + *  Use this if your architecture lacks an efficient flush_tlb_range().
++=======
+  *  Use this if your architecture lacks an efficient flush_tlb_range(). This
+  *  option implies MMU_GATHER_MERGE_VMAS above.
+  *
+  *  MMU_GATHER_NO_GATHER
+  *
+  *  If the option is set the mmu_gather will not track individual pages for
+  *  delayed page free anymore. A platform that enables the option needs to
+  *  provide its own implementation of the __tlb_remove_page_size() function to
+  *  free pages.
+  *
+  *  This is useful if your architecture already flushes TLB entries in the
+  *  various ptep_get_and_clear() functions.
++>>>>>>> 1e9fdf21a433 (mmu_gather: Remove per arch tlb_{start,end}_vma())
   */
  
 -#ifdef CONFIG_MMU_GATHER_TABLE_FREE
 -
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +/*
 + * Semi RCU freeing of the page directories.
 + *
 + * This is needed by some architectures to implement software pagetable walkers.
 + *
 + * gup_fast() and other software pagetable walkers do a lockless page-table
 + * walk and therefore needs some synchronization with the freeing of the page
 + * directories. The chosen means to accomplish that is by disabling IRQs over
 + * the walk.
 + *
 + * Architectures that use IPIs to flush TLBs will then automagically DTRT,
 + * since we unlink the page, flush TLBs, free the page. Since the disabling of
 + * IRQs delays the completion of the TLB flush we can never observe an already
 + * freed page.
 + *
 + * Architectures that do not have this (PPC) need to delay the freeing by some
 + * other means, this is that means.
 + *
 + * What we do is batch the freed directory pages (tables) and RCU free them.
 + * We use the sched RCU variant, as that guarantees that IRQ/preempt disabling
 + * holds off grace periods.
 + *
 + * However, in order to batch these pages we need to allocate storage, this
 + * allocation is deep inside the MM code and can thus easily fail on memory
 + * pressure. To guarantee progress we fall back to single table freeing, see
 + * the implementation of tlb_remove_table_one().
 + *
 + */
  struct mmu_table_batch {
 -#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
  	struct rcu_head		rcu;
 -#endif
  	unsigned int		nr;
 -	void			*tables[];
 +	void			*tables[0];
  };
  
  #define MAX_TABLE_BATCH		\
* Unmerged path arch/csky/include/asm/tlb.h
* Unmerged path arch/loongarch/Kconfig
* Unmerged path arch/loongarch/include/asm/tlb.h
diff --git a/arch/Kconfig b/arch/Kconfig
index 1e486125c2eb..b55991cb1926 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -381,6 +381,13 @@ config HAVE_MMU_GATHER_PAGE_SIZE
 
 config MMU_GATHER_NO_RANGE
 	bool
+	select MMU_GATHER_MERGE_VMAS
+
+config MMU_GATHER_NO_FLUSH_CACHE
+	bool
+
+config MMU_GATHER_MERGE_VMAS
+	bool
 
 config HAVE_MMU_GATHER_NO_GATHER
 	bool
* Unmerged path arch/csky/include/asm/tlb.h
* Unmerged path arch/loongarch/Kconfig
* Unmerged path arch/loongarch/include/asm/tlb.h
* Unmerged path arch/powerpc/Kconfig
diff --git a/arch/powerpc/include/asm/tlb.h b/arch/powerpc/include/asm/tlb.h
index 6a7bb648464a..9ada35785706 100644
--- a/arch/powerpc/include/asm/tlb.h
+++ b/arch/powerpc/include/asm/tlb.h
@@ -25,8 +25,6 @@
 
 #include <linux/pagemap.h>
 
-#define tlb_start_vma(tlb, vma)	do { } while (0)
-#define tlb_end_vma(tlb, vma)	do { } while (0)
 #define __tlb_remove_tlb_entry	__tlb_remove_tlb_entry
 
 #define tlb_flush tlb_flush
* Unmerged path arch/s390/Kconfig
diff --git a/arch/s390/include/asm/tlb.h b/arch/s390/include/asm/tlb.h
index b885edd7b750..e57e4287cbad 100644
--- a/arch/s390/include/asm/tlb.h
+++ b/arch/s390/include/asm/tlb.h
@@ -27,9 +27,6 @@ static inline void tlb_flush(struct mmu_gather *tlb);
 static inline bool __tlb_remove_page_size(struct mmu_gather *tlb,
 					  struct page *page, int page_size);
 
-#define tlb_start_vma(tlb, vma)			do { } while (0)
-#define tlb_end_vma(tlb, vma)			do { } while (0)
-
 #define tlb_flush tlb_flush
 #define pte_free_tlb pte_free_tlb
 #define pmd_free_tlb pmd_free_tlb
* Unmerged path arch/sparc/Kconfig
diff --git a/arch/sparc/include/asm/tlb_64.h b/arch/sparc/include/asm/tlb_64.h
index 8eefdd0c9193..cab3bfcd8b66 100644
--- a/arch/sparc/include/asm/tlb_64.h
+++ b/arch/sparc/include/asm/tlb_64.h
@@ -23,8 +23,6 @@ void smp_flush_tlb_mm(struct mm_struct *mm);
 void __flush_tlb_pending(unsigned long, unsigned long, unsigned long *);
 void flush_tlb_pending(void);
 
-#define tlb_start_vma(tlb, vma) do { } while (0)
-#define tlb_end_vma(tlb, vma)	do { } while (0)
 #define tlb_flush(tlb)	flush_tlb_pending()
 
 /*
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/include/asm/tlb.h b/arch/x86/include/asm/tlb.h
index 4ea8699cf31d..b37c74be1a67 100644
--- a/arch/x86/include/asm/tlb.h
+++ b/arch/x86/include/asm/tlb.h
@@ -2,9 +2,6 @@
 #ifndef _ASM_X86_TLB_H
 #define _ASM_X86_TLB_H
 
-#define tlb_start_vma(tlb, vma) do { } while (0)
-#define tlb_end_vma(tlb, vma) do { } while (0)
-
 #define tlb_flush tlb_flush
 static inline void tlb_flush(struct mmu_gather *tlb);
 
* Unmerged path include/asm-generic/tlb.h
