drivers/base/memory: determine and store zone for single-zone memory blocks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author David Hildenbrand <david@redhat.com>
commit 395f6081bad49f9c54abafebab49ee23aa985bbd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/395f6081.failed

test_pages_in_a_zone() is just another nasty PFN walker that can easily
stumble over ZONE_DEVICE memory ranges falling into the same memory block
as ordinary system RAM: the memmap of parts of these ranges might possibly
be uninitialized.  In fact, we observed (on an older kernel) with UBSAN:

  UBSAN: Undefined behaviour in ./include/linux/mm.h:1133:50
  index 7 is out of range for type 'zone [5]'
  CPU: 121 PID: 35603 Comm: read_all Kdump: loaded Tainted: [...]
  Hardware name: Dell Inc. PowerEdge R7425/08V001, BIOS 1.12.2 11/15/2019
  Call Trace:
   dump_stack+0x9a/0xf0
   ubsan_epilogue+0x9/0x7a
   __ubsan_handle_out_of_bounds+0x13a/0x181
   test_pages_in_a_zone+0x3c4/0x500
   show_valid_zones+0x1fa/0x380
   dev_attr_show+0x43/0xb0
   sysfs_kf_seq_show+0x1c5/0x440
   seq_read+0x49d/0x1190
   vfs_read+0xff/0x300
   ksys_read+0xb8/0x170
   do_syscall_64+0xa5/0x4b0
   entry_SYSCALL_64_after_hwframe+0x6a/0xdf
  RIP: 0033:0x7f01f4439b52

We seem to stumble over a memmap that contains a garbage zone id.  While
we could try inserting pfn_to_online_page() calls, it will just make
memory offlining slower, because we use test_pages_in_a_zone() to make
sure we're offlining pages that all belong to the same zone.

Let's just get rid of this PFN walker and determine the single zone of a
memory block -- if any -- for early memory blocks during boot.  For memory
onlining, we know the single zone already.  Let's avoid any additional
memmap scanning and just rely on the zone information available during
boot.

For memory hot(un)plug, we only really care about memory blocks that:
* span a single zone (and, thereby, a single node)
* are completely System RAM (IOW, no holes, no ZONE_DEVICE)
If one of these conditions is not met, we reject memory offlining.
Hotplugged memory blocks (starting out offline), always meet both
conditions.

There are three scenarios to handle:

(1) Memory hot(un)plug

A memory block with zone == NULL cannot be offlined, corresponding to
our previous test_pages_in_a_zone() check.

After successful memory onlining/offlining, we simply set the zone
accordingly.
* Memory onlining: set the zone we just used for onlining
* Memory offlining: set zone = NULL

So a hotplugged memory block starts with zone = NULL. Once memory
onlining is done, we set the proper zone.

(2) Boot memory with !CONFIG_NUMA

We know that there is just a single pgdat, so we simply scan all zones
of that pgdat for an intersection with our memory block PFN range when
adding the memory block. If more than one zone intersects (e.g., DMA and
DMA32 on x86 for the first memory block) we set zone = NULL and
consequently mimic what test_pages_in_a_zone() used to do.

(3) Boot memory with CONFIG_NUMA

At the point in time we create the memory block devices during boot, we
don't know yet which nodes *actually* span a memory block. While we could
scan all zones of all nodes for intersections, overlapping nodes complicate
the situation and scanning all nodes is possibly expensive. But that
problem has already been solved by the code that sets the node of a memory
block and creates the link in the sysfs --
do_register_memory_block_under_node().

So, we hook into the code that sets the node id for a memory block. If
we already have a different node id set for the memory block, we know
that multiple nodes *actually* have PFNs falling into our memory block:
we set zone = NULL and consequently mimic what test_pages_in_a_zone() used
to do. If there is no node id set, we do the same as (2) for the given
node.

Note that the call order in driver_init() is:
-> memory_dev_init(): create memory block devices
-> node_dev_init(): link memory block devices to the node and set the
		    node id

So in summary, we detect if there is a single zone responsible for this
memory block and we consequently store the zone in that case in the
memory block, updating it during memory onlining/offlining.

Link: https://lkml.kernel.org/r/20220210184359.235565-3-david@redhat.com
	Signed-off-by: David Hildenbrand <david@redhat.com>
	Reported-by: Rafael Parra <rparrazo@redhat.com>
	Reviewed-by: Oscar Salvador <osalvador@suse.de>
	Cc: "Rafael J. Wysocki" <rafael@kernel.org>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Rafael Parra <rparrazo@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 395f6081bad49f9c54abafebab49ee23aa985bbd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/base/memory.c
#	include/linux/memory.h
#	include/linux/memory_hotplug.h
#	mm/memory_hotplug.c
diff --cc drivers/base/memory.c
index 7e464620a1b2,f75e3467cb59..000000000000
--- a/drivers/base/memory.c
+++ b/drivers/base/memory.c
@@@ -172,16 -179,79 +172,82 @@@ static int memory_block_online(struct m
  {
  	unsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);
  	unsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;
 -	unsigned long nr_vmemmap_pages = mem->nr_vmemmap_pages;
 -	struct zone *zone;
 -	int ret;
  
++<<<<<<< HEAD
 +	return online_pages(start_pfn, nr_pages, mem->online_type, mem->nid);
++=======
+ 	zone = zone_for_pfn_range(mem->online_type, mem->nid, mem->group,
+ 				  start_pfn, nr_pages);
+ 
+ 	/*
+ 	 * Although vmemmap pages have a different lifecycle than the pages
+ 	 * they describe (they remain until the memory is unplugged), doing
+ 	 * their initialization and accounting at memory onlining/offlining
+ 	 * stage helps to keep accounting easier to follow - e.g vmemmaps
+ 	 * belong to the same zone as the memory they backed.
+ 	 */
+ 	if (nr_vmemmap_pages) {
+ 		ret = mhp_init_memmap_on_memory(start_pfn, nr_vmemmap_pages, zone);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+ 	ret = online_pages(start_pfn + nr_vmemmap_pages,
+ 			   nr_pages - nr_vmemmap_pages, zone, mem->group);
+ 	if (ret) {
+ 		if (nr_vmemmap_pages)
+ 			mhp_deinit_memmap_on_memory(start_pfn, nr_vmemmap_pages);
+ 		return ret;
+ 	}
+ 
+ 	/*
+ 	 * Account once onlining succeeded. If the zone was unpopulated, it is
+ 	 * now already properly populated.
+ 	 */
+ 	if (nr_vmemmap_pages)
+ 		adjust_present_page_count(pfn_to_page(start_pfn), mem->group,
+ 					  nr_vmemmap_pages);
+ 
+ 	mem->zone = zone;
+ 	return ret;
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  }
  
  static int memory_block_offline(struct memory_block *mem)
  {
  	unsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);
  	unsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;
 -	unsigned long nr_vmemmap_pages = mem->nr_vmemmap_pages;
 -	int ret;
  
++<<<<<<< HEAD
 +	return offline_pages(start_pfn, nr_pages);
++=======
+ 	if (!mem->zone)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * Unaccount before offlining, such that unpopulated zone and kthreads
+ 	 * can properly be torn down in offline_pages().
+ 	 */
+ 	if (nr_vmemmap_pages)
+ 		adjust_present_page_count(pfn_to_page(start_pfn), mem->group,
+ 					  -nr_vmemmap_pages);
+ 
+ 	ret = offline_pages(start_pfn + nr_vmemmap_pages,
+ 			    nr_pages - nr_vmemmap_pages, mem->zone, mem->group);
+ 	if (ret) {
+ 		/* offline_pages() failed. Account back. */
+ 		if (nr_vmemmap_pages)
+ 			adjust_present_page_count(pfn_to_page(start_pfn),
+ 						  mem->group, nr_vmemmap_pages);
+ 		return ret;
+ 	}
+ 
+ 	if (nr_vmemmap_pages)
+ 		mhp_deinit_memmap_on_memory(start_pfn, nr_vmemmap_pages);
+ 
+ 	mem->zone = NULL;
+ 	return ret;
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  }
  
  /*
@@@ -574,8 -647,85 +639,90 @@@ int register_memory(struct memory_bloc
  	return ret;
  }
  
++<<<<<<< HEAD
 +static int init_memory_block(struct memory_block **memory,
 +			     unsigned long block_id, unsigned long state)
++=======
+ static struct zone *early_node_zone_for_memory_block(struct memory_block *mem,
+ 						     int nid)
+ {
+ 	const unsigned long start_pfn = section_nr_to_pfn(mem->start_section_nr);
+ 	const unsigned long nr_pages = PAGES_PER_SECTION * sections_per_block;
+ 	struct zone *zone, *matching_zone = NULL;
+ 	pg_data_t *pgdat = NODE_DATA(nid);
+ 	int i;
+ 
+ 	/*
+ 	 * This logic only works for early memory, when the applicable zones
+ 	 * already span the memory block. We don't expect overlapping zones on
+ 	 * a single node for early memory. So if we're told that some PFNs
+ 	 * of a node fall into this memory block, we can assume that all node
+ 	 * zones that intersect with the memory block are actually applicable.
+ 	 * No need to look at the memmap.
+ 	 */
+ 	for (i = 0; i < MAX_NR_ZONES; i++) {
+ 		zone = pgdat->node_zones + i;
+ 		if (!populated_zone(zone))
+ 			continue;
+ 		if (!zone_intersects(zone, start_pfn, nr_pages))
+ 			continue;
+ 		if (!matching_zone) {
+ 			matching_zone = zone;
+ 			continue;
+ 		}
+ 		/* Spans multiple zones ... */
+ 		matching_zone = NULL;
+ 		break;
+ 	}
+ 	return matching_zone;
+ }
+ 
+ #ifdef CONFIG_NUMA
+ /**
+  * memory_block_add_nid() - Indicate that system RAM falling into this memory
+  *			    block device (partially) belongs to the given node.
+  * @mem: The memory block device.
+  * @nid: The node id.
+  * @context: The memory initialization context.
+  *
+  * Indicate that system RAM falling into this memory block (partially) belongs
+  * to the given node. If the context indicates ("early") that we are adding the
+  * node during node device subsystem initialization, this will also properly
+  * set/adjust mem->zone based on the zone ranges of the given node.
+  */
+ void memory_block_add_nid(struct memory_block *mem, int nid,
+ 			  enum meminit_context context)
+ {
+ 	if (context == MEMINIT_EARLY && mem->nid != nid) {
+ 		/*
+ 		 * For early memory we have to determine the zone when setting
+ 		 * the node id and handle multiple nodes spanning a single
+ 		 * memory block by indicate via zone == NULL that we're not
+ 		 * dealing with a single zone. So if we're setting the node id
+ 		 * the first time, determine if there is a single zone. If we're
+ 		 * setting the node id a second time to a different node,
+ 		 * invalidate the single detected zone.
+ 		 */
+ 		if (mem->nid == NUMA_NO_NODE)
+ 			mem->zone = early_node_zone_for_memory_block(mem, nid);
+ 		else
+ 			mem->zone = NULL;
+ 	}
+ 
+ 	/*
+ 	 * If this memory block spans multiple nodes, we only indicate
+ 	 * the last processed node. If we span multiple nodes (not applicable
+ 	 * to hotplugged memory), zone == NULL will prohibit memory offlining
+ 	 * and consequently unplug.
+ 	 */
+ 	mem->nid = nid;
+ }
+ #endif
+ 
+ static int init_memory_block(unsigned long block_id, unsigned long state,
+ 			     unsigned long nr_vmemmap_pages,
+ 			     struct memory_group *group)
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  {
  	struct memory_block *mem;
  	int ret = 0;
@@@ -592,11 -742,30 +739,22 @@@
  	mem->start_section_nr = block_id * sections_per_block;
  	mem->state = state;
  	mem->nid = NUMA_NO_NODE;
 -	mem->nr_vmemmap_pages = nr_vmemmap_pages;
 -	INIT_LIST_HEAD(&mem->group_next);
  
+ #ifndef CONFIG_NUMA
+ 	if (state == MEM_ONLINE)
+ 		/*
+ 		 * MEM_ONLINE at this point implies early memory. With NUMA,
+ 		 * we'll determine the zone when setting the node id via
+ 		 * memory_block_add_nid(). Memory hotplug updated the zone
+ 		 * manually when memory onlining/offlining succeeds.
+ 		 */
+ 		mem->zone = early_node_zone_for_memory_block(mem, NUMA_NO_NODE);
+ #endif /* CONFIG_NUMA */
+ 
  	ret = register_memory(mem);
 -	if (ret)
 -		return ret;
  
 -	if (group) {
 -		mem->group = group;
 -		list_add(&mem->group_next, &group->memory_blocks);
 -	}
 -
 -	return 0;
 +	*memory = mem;
 +	return ret;
  }
  
  static int add_memory_block(unsigned long base_section_nr)
diff --cc include/linux/memory.h
index b01c2c56d418,aa619464a1df..000000000000
--- a/include/linux/memory.h
+++ b/include/linux/memory.h
@@@ -27,10 -69,22 +27,21 @@@ struct memory_block 
  	unsigned long start_section_nr;
  	unsigned long state;		/* serialized by the dev->lock */
  	int online_type;		/* for passing data to online routine */
++<<<<<<< HEAD
 +	void *hw;			/* optional pointer to fw/hw data */
 +	int (*phys_callback)(struct memory_block *);
++=======
+ 	int nid;			/* NID for this memory block */
+ 	/*
+ 	 * The single zone of this memory block if all PFNs of this memory block
+ 	 * that are System RAM (not a memory hole, not ZONE_DEVICE ranges) are
+ 	 * managed by a single zone. NULL if multiple zones (including nodes)
+ 	 * apply.
+ 	 */
+ 	struct zone *zone;
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  	struct device dev;
 -	/*
 -	 * Number of vmemmap pages. These pages
 -	 * lay at the beginning of the memory block.
 -	 */
 -	unsigned long nr_vmemmap_pages;
 -	struct memory_group *group;	/* group (if any) for this block */
 -	struct list_head group_next;	/* next block inside memory group */
 +	int nid;			/* NID for this memory block */
  };
  
  int arch_get_memory_phys_device(unsigned long start_pfn);
@@@ -102,12 -168,12 +113,21 @@@ extern int for_each_memory_block(void *
  })
  #define register_hotmemory_notifier(nb)		register_memory_notifier(nb)
  #define unregister_hotmemory_notifier(nb) 	unregister_memory_notifier(nb)
++<<<<<<< HEAD
 +#else
 +#define hotplug_memory_notifier(fn, pri)	({ 0; })
 +/* These aren't inline functions due to a GCC bug. */
 +#define register_hotmemory_notifier(nb)    ({ (void)(nb); 0; })
 +#define unregister_hotmemory_notifier(nb)  ({ (void)(nb); })
 +#endif
++=======
+ 
+ #ifdef CONFIG_NUMA
+ void memory_block_add_nid(struct memory_block *mem, int nid,
+ 			  enum meminit_context context);
+ #endif /* CONFIG_NUMA */
+ #endif	/* CONFIG_MEMORY_HOTPLUG */
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  
  /*
   * Kernel text modification mutex, used for code patching. Users of this lock
diff --cc include/linux/memory_hotplug.h
index 712f08928446,1ce6f8044f1e..000000000000
--- a/include/linux/memory_hotplug.h
+++ b/include/linux/memory_hotplug.h
@@@ -142,14 -154,15 +142,18 @@@ static inline void zone_seqlock_init(st
  {
  	seqlock_init(&zone->span_seqlock);
  }
 -extern void adjust_present_page_count(struct page *page,
 -				      struct memory_group *group,
 -				      long nr_pages);
 +extern int zone_grow_free_lists(struct zone *zone, unsigned long new_nr_pages);
 +extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
 +extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
  /* VM interface that may be used by firmware interface */
 -extern int mhp_init_memmap_on_memory(unsigned long pfn, unsigned long nr_pages,
 -				     struct zone *zone);
 -extern void mhp_deinit_memmap_on_memory(unsigned long pfn, unsigned long nr_pages);
  extern int online_pages(unsigned long pfn, unsigned long nr_pages,
++<<<<<<< HEAD
 +			int online_type, int nid);
 +extern struct zone *test_pages_in_a_zone(unsigned long start_pfn,
 +					 unsigned long end_pfn);
++=======
+ 			struct zone *zone, struct memory_group *group);
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  extern void __offline_isolated_pages(unsigned long start_pfn,
  				     unsigned long end_pfn);
  
@@@ -305,14 -290,17 +309,27 @@@ static inline void pgdat_resize_init(st
  #ifdef CONFIG_MEMORY_HOTREMOVE
  
  extern void try_offline_node(int nid);
++<<<<<<< HEAD
 +extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
 +extern int remove_memory(int nid, u64 start, u64 size);
 +extern void __remove_memory(int nid, u64 start, u64 size);
++=======
+ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,
+ 			 struct zone *zone, struct memory_group *group);
+ extern int remove_memory(u64 start, u64 size);
+ extern void __remove_memory(u64 start, u64 size);
+ extern int offline_and_remove_memory(u64 start, u64 size);
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  
  #else
  static inline void try_offline_node(int nid) {}
  
++<<<<<<< HEAD
 +static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages)
++=======
+ static inline int offline_pages(unsigned long start_pfn, unsigned long nr_pages,
+ 				struct zone *zone, struct memory_group *group)
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  {
  	return -EINVAL;
  }
diff --cc mm/memory_hotplug.c
index 73cbc6ea1fee,aee69281dad6..000000000000
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@@ -1210,47 -1492,63 +1210,50 @@@ out_unlock
  }
  EXPORT_SYMBOL_GPL(add_memory_driver_managed);
  
 +#ifdef CONFIG_MEMORY_HOTREMOVE
  /*
 - * Platforms should define arch_get_mappable_range() that provides
 - * maximum possible addressable physical memory range for which the
 - * linear mapping could be created. The platform returned address
 - * range must adhere to these following semantics.
 - *
 - * - range.start <= range.end
 - * - Range includes both end points [range.start..range.end]
 - *
 - * There is also a fallback definition provided here, allowing the
 - * entire possible physical address range in case any platform does
 - * not define arch_get_mappable_range().
++<<<<<<< HEAD
 + * Confirm all pages in a range [start, end) belong to the same zone (skipping
 + * memory holes). When true, return the zone.
   */
 -struct range __weak arch_get_mappable_range(void)
 -{
 -	struct range mhp_range = {
 -		.start = 0UL,
 -		.end = -1ULL,
 -	};
 -	return mhp_range;
 -}
 -
 -struct range mhp_get_pluggable_range(bool need_mapping)
 +struct zone *test_pages_in_a_zone(unsigned long start_pfn,
 +				  unsigned long end_pfn)
  {
 -	const u64 max_phys = (1ULL << MAX_PHYSMEM_BITS) - 1;
 -	struct range mhp_range;
 -
 -	if (need_mapping) {
 -		mhp_range = arch_get_mappable_range();
 -		if (mhp_range.start > max_phys) {
 -			mhp_range.start = 0;
 -			mhp_range.end = 0;
 +	unsigned long pfn, sec_end_pfn;
 +	struct zone *zone = NULL;
 +	struct page *page;
 +	int i;
 +	for (pfn = start_pfn, sec_end_pfn = SECTION_ALIGN_UP(start_pfn + 1);
 +	     pfn < end_pfn;
 +	     pfn = sec_end_pfn, sec_end_pfn += PAGES_PER_SECTION) {
 +		/* Make sure the memory section is present first */
 +		if (!present_section_nr(pfn_to_section_nr(pfn)))
 +			continue;
 +		for (; pfn < sec_end_pfn && pfn < end_pfn;
 +		     pfn += MAX_ORDER_NR_PAGES) {
 +			i = 0;
 +			/* This is just a CONFIG_HOLES_IN_ZONE check.*/
 +			while ((i < MAX_ORDER_NR_PAGES) &&
 +				!pfn_valid_within(pfn + i))
 +				i++;
 +			if (i == MAX_ORDER_NR_PAGES || pfn + i >= end_pfn)
 +				continue;
 +			/* Check if we got outside of the zone */
 +			if (zone && !zone_spans_pfn(zone, pfn + i))
 +				return NULL;
 +			page = pfn_to_page(pfn + i);
 +			if (zone && page_zone(page) != zone)
 +				return NULL;
 +			zone = page_zone(page);
  		}
 -		mhp_range.end = min_t(u64, mhp_range.end, max_phys);
 -	} else {
 -		mhp_range.start = 0;
 -		mhp_range.end = max_phys;
  	}
 -	return mhp_range;
 -}
 -EXPORT_SYMBOL_GPL(mhp_get_pluggable_range);
  
 -bool mhp_range_allowed(u64 start, u64 size, bool need_mapping)
 -{
 -	struct range mhp_range = mhp_get_pluggable_range(need_mapping);
 -	u64 end = start + size;
 -
 -	if (start < end && start >= mhp_range.start && (end - 1) <= mhp_range.end)
 -		return true;
 -
 -	pr_warn("Hotplug memory [%#llx-%#llx] exceeds maximum addressable range [%#llx-%#llx]\n",
 -		start, end, mhp_range.start, mhp_range.end);
 -	return false;
 +	return zone;
  }
  
 -#ifdef CONFIG_MEMORY_HOTREMOVE
  /*
++=======
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
   * Scan pfn range [start,end) to find movable/migratable pages (LRU pages,
   * non-lru movable pages and hugepages). Will skip over most unmovable
   * pages (esp., pages that can be skipped when offlining), but bail out on
@@@ -1487,19 -1770,27 +1490,24 @@@ static int count_system_ram_pages_cb(un
  	return 0;
  }
  
++<<<<<<< HEAD
 +int __ref offline_pages(unsigned long start_pfn, unsigned long nr_pages)
++=======
+ int __ref offline_pages(unsigned long start_pfn, unsigned long nr_pages,
+ 			struct zone *zone, struct memory_group *group)
++>>>>>>> 395f6081bad4 (drivers/base/memory: determine and store zone for single-zone memory blocks)
  {
  	const unsigned long end_pfn = start_pfn + nr_pages;
  	unsigned long pfn, system_ram_pages = 0;
+ 	const int node = zone_to_nid(zone);
  	unsigned long flags;
- 	struct zone *zone;
  	struct memory_notify arg;
- 	int ret, node;
  	char *reason;
+ 	int ret;
  
 -	/*
 -	 * {on,off}lining is constrained to full memory sections (or more
 -	 * precisely to memory blocks from the user space POV).
 -	 * memmap_on_memory is an exception because it reserves initial part
 -	 * of the physical memory space for vmemmaps. That space is pageblock
 -	 * aligned.
 -	 */
 +	/* We can only offline full sections (e.g., SECTION_IS_ONLINE) */
  	if (WARN_ON_ONCE(!nr_pages ||
 -			 !IS_ALIGNED(start_pfn, pageblock_nr_pages) ||
 -			 !IS_ALIGNED(start_pfn + nr_pages, PAGES_PER_SECTION)))
 +			 !IS_ALIGNED(start_pfn | nr_pages, PAGES_PER_SECTION)))
  		return -EINVAL;
  
  	mem_hotplug_begin();
* Unmerged path drivers/base/memory.c
diff --git a/drivers/base/node.c b/drivers/base/node.c
index c69b28522c92..7b9b447e126e 100644
--- a/drivers/base/node.c
+++ b/drivers/base/node.c
@@ -797,15 +797,12 @@ static int __ref get_nid_for_pfn(unsigned long pfn)
 }
 
 static void do_register_memory_block_under_node(int nid,
-						struct memory_block *mem_blk)
+						struct memory_block *mem_blk,
+						enum meminit_context context)
 {
 	int ret;
 
-	/*
-	 * If this memory block spans multiple nodes, we only indicate
-	 * the last processed node.
-	 */
-	mem_blk->nid = nid;
+	memory_block_add_nid(mem_blk, nid, context);
 
 	ret = sysfs_create_link_nowarn(&node_devices[nid]->dev.kobj,
 				       &mem_blk->dev.kobj,
@@ -858,7 +855,7 @@ static int register_mem_block_under_node_early(struct memory_block *mem_blk,
 		if (page_nid != nid)
 			continue;
 
-		do_register_memory_block_under_node(nid, mem_blk);
+		do_register_memory_block_under_node(nid, mem_blk, MEMINIT_EARLY);
 		return 0;
 	}
 	/* mem section does not span the specified node */
@@ -874,7 +871,7 @@ static int register_mem_block_under_node_hotplug(struct memory_block *mem_blk,
 {
 	int nid = *(int *)arg;
 
-	do_register_memory_block_under_node(nid, mem_blk);
+	do_register_memory_block_under_node(nid, mem_blk, MEMINIT_HOTPLUG);
 	return 0;
 }
 
* Unmerged path include/linux/memory.h
* Unmerged path include/linux/memory_hotplug.h
* Unmerged path mm/memory_hotplug.c
