sbitmap: fix batched wait_cnt accounting

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Keith Busch <kbusch@kernel.org>
commit 16ede66973c84f890c03584f79158dd5b2d725f5
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/16ede669.failed

Batched completions can clear multiple bits, but we're only decrementing
the wait_cnt by one each time. This can cause waiters to never be woken,
stalling IO. Use the batched count instead.

Link: https://bugzilla.kernel.org/show_bug.cgi?id=215679
	Signed-off-by: Keith Busch <kbusch@kernel.org>
Link: https://lore.kernel.org/r/20220825145312.1217900-1-kbusch@fb.com
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit 16ede66973c84f890c03584f79158dd5b2d725f5)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	lib/sbitmap.c
diff --cc lib/sbitmap.c
index e476dc00dc50,2fedf07a9db5..000000000000
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@@ -551,41 -599,33 +551,59 @@@ static struct sbq_wait_state *sbq_wake_
  	return NULL;
  }
  
- static bool __sbq_wake_up(struct sbitmap_queue *sbq)
+ static bool __sbq_wake_up(struct sbitmap_queue *sbq, int nr)
  {
  	struct sbq_wait_state *ws;
++<<<<<<< HEAD
 +	unsigned int wake_batch;
 +	int wait_cnt;
 +	bool ret;
++=======
+ 	int wake_batch, wait_cnt, cur;
++>>>>>>> 16ede66973c8 (sbitmap: fix batched wait_cnt accounting)
  
  	ws = sbq_wake_ptr(sbq);
- 	if (!ws)
+ 	if (!ws || !nr)
  		return false;
  
- 	wait_cnt = atomic_dec_return(&ws->wait_cnt);
+ 	wake_batch = READ_ONCE(sbq->wake_batch);
+ 	cur = atomic_read(&ws->wait_cnt);
+ 	do {
+ 		if (cur <= 0)
+ 			return true;
+ 		wait_cnt = cur - nr;
+ 	} while (!atomic_try_cmpxchg(&ws->wait_cnt, &cur, wait_cnt));
+ 
  	/*
  	 * For concurrent callers of this, callers should call this function
  	 * again to wakeup a new batch on a different 'ws'.
  	 */
++<<<<<<< HEAD
 +	if (wait_cnt < 0)
++=======
+ 	if (!waitqueue_active(&ws->wait))
++>>>>>>> 16ede66973c8 (sbitmap: fix batched wait_cnt accounting)
  		return true;
  
 +	/*
 +	 * If we decremented queue without waiters, retry to avoid lost
 +	 * wakeups.
 +	 */
  	if (wait_cnt > 0)
 -		return false;
 +		return !waitqueue_active(&ws->wait);
 +
++<<<<<<< HEAD
 +	/*
 +	 * When wait_cnt == 0, we have to be particularly careful as we are
 +	 * responsible to reset wait_cnt regardless whether we've actually
 +	 * woken up anybody. But in case we didn't wakeup anybody, we still
 +	 * need to retry.
 +	 */
 +	ret = !waitqueue_active(&ws->wait);
 +	wake_batch = READ_ONCE(sbq->wake_batch);
  
++=======
++>>>>>>> 16ede66973c8 (sbitmap: fix batched wait_cnt accounting)
  	/*
  	 * Wake up first in case that concurrent callers decrease wait_cnt
  	 * while waitqueue is empty.
@@@ -612,16 -652,55 +630,58 @@@
  	sbq_index_atomic_inc(&sbq->wake_index);
  	atomic_set(&ws->wait_cnt, wake_batch);
  
 -	return false;
 +	return ret;
  }
  
- void sbitmap_queue_wake_up(struct sbitmap_queue *sbq)
+ void sbitmap_queue_wake_up(struct sbitmap_queue *sbq, int nr)
  {
- 	while (__sbq_wake_up(sbq))
+ 	while (__sbq_wake_up(sbq, nr))
  		;
  }
- EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
  
++<<<<<<< HEAD
++=======
+ static inline void sbitmap_update_cpu_hint(struct sbitmap *sb, int cpu, int tag)
+ {
+ 	if (likely(!sb->round_robin && tag < sb->depth))
+ 		data_race(*per_cpu_ptr(sb->alloc_hint, cpu) = tag);
+ }
+ 
+ void sbitmap_queue_clear_batch(struct sbitmap_queue *sbq, int offset,
+ 				int *tags, int nr_tags)
+ {
+ 	struct sbitmap *sb = &sbq->sb;
+ 	unsigned long *addr = NULL;
+ 	unsigned long mask = 0;
+ 	int i;
+ 
+ 	smp_mb__before_atomic();
+ 	for (i = 0; i < nr_tags; i++) {
+ 		const int tag = tags[i] - offset;
+ 		unsigned long *this_addr;
+ 
+ 		/* since we're clearing a batch, skip the deferred map */
+ 		this_addr = &sb->map[SB_NR_TO_INDEX(sb, tag)].word;
+ 		if (!addr) {
+ 			addr = this_addr;
+ 		} else if (addr != this_addr) {
+ 			atomic_long_andnot(mask, (atomic_long_t *) addr);
+ 			mask = 0;
+ 			addr = this_addr;
+ 		}
+ 		mask |= (1UL << SB_NR_TO_BIT(sb, tag));
+ 	}
+ 
+ 	if (mask)
+ 		atomic_long_andnot(mask, (atomic_long_t *) addr);
+ 
+ 	smp_mb__after_atomic();
+ 	sbitmap_queue_wake_up(sbq, nr_tags);
+ 	sbitmap_update_cpu_hint(&sbq->sb, raw_smp_processor_id(),
+ 					tags[nr_tags - 1] - offset);
+ }
+ 
++>>>>>>> 16ede66973c8 (sbitmap: fix batched wait_cnt accounting)
  void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
  			 unsigned int cpu)
  {
@@@ -647,10 -724,8 +707,15 @@@
  	 * waiter. See the comment on waitqueue_active().
  	 */
  	smp_mb__after_atomic();
++<<<<<<< HEAD
 +	sbitmap_queue_wake_up(sbq);
 +
 +	if (likely(!sbq->sb.round_robin && nr < sbq->sb.depth))
 +		*per_cpu_ptr(alloc_hint, cpu) = nr;
++=======
+ 	sbitmap_queue_wake_up(sbq, 1);
+ 	sbitmap_update_cpu_hint(&sbq->sb, cpu, nr);
++>>>>>>> 16ede66973c8 (sbitmap: fix batched wait_cnt accounting)
  }
  EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
  
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 3a5ef3dc8628..21f98a7c6b8a 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -159,7 +159,7 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		 * other allocations on previous queue won't be starved.
 		 */
 		if (bt != bt_prev)
-			sbitmap_queue_wake_up(bt_prev);
+			sbitmap_queue_wake_up(bt_prev, 1);
 
 		ws = bt_wait_ptr(bt, data->hctx);
 	} while (1);
diff --git a/include/linux/sbitmap.h b/include/linux/sbitmap.h
index 3a3d03813153..f5ba8a2967bb 100644
--- a/include/linux/sbitmap.h
+++ b/include/linux/sbitmap.h
@@ -590,8 +590,9 @@ void sbitmap_queue_wake_all(struct sbitmap_queue *sbq);
  * sbitmap_queue_wake_up() - Wake up some of waiters in one waitqueue
  * on a &struct sbitmap_queue.
  * @sbq: Bitmap queue to wake up.
+ * @nr: Number of bits cleared.
  */
-void sbitmap_queue_wake_up(struct sbitmap_queue *sbq);
+void sbitmap_queue_wake_up(struct sbitmap_queue *sbq, int nr);
 
 /**
  * sbitmap_queue_show() - Dump &struct sbitmap_queue information to a &struct
* Unmerged path lib/sbitmap.c
