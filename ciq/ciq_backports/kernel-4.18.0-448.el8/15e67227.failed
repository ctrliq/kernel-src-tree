x86: Undo return-thunk damage

jira LE-1907
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23825
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 15e67227c49a57837108acfe1c80570e1bd9f962
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/15e67227.failed

Introduce X86_FEATURE_RETHUNK for those afflicted with needing this.

  [ bp: Do only INT3 padding - simpler. ]

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Josh Poimboeuf <jpoimboe@kernel.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
(cherry picked from commit 15e67227c49a57837108acfe1c80570e1bd9f962)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/alternative.h
#	arch/x86/include/asm/disabled-features.h
#	arch/x86/kernel/alternative.c
#	arch/x86/kernel/module.c
#	arch/x86/kernel/vmlinux.lds.S
diff --cc arch/x86/include/asm/alternative.h
index 0e1856c8f446,9542c582d546..000000000000
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@@ -72,6 -75,9 +72,12 @@@ extern int alternatives_patched
  
  extern void alternative_instructions(void);
  extern void apply_alternatives(struct alt_instr *start, struct alt_instr *end);
++<<<<<<< HEAD
++=======
+ extern void apply_retpolines(s32 *start, s32 *end);
+ extern void apply_returns(s32 *start, s32 *end);
+ extern void apply_ibt_endbr(s32 *start, s32 *end);
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  
  struct module;
  
diff --cc arch/x86/include/asm/disabled-features.h
index f9dead97567f,641c479cca17..000000000000
--- a/arch/x86/include/asm/disabled-features.h
+++ b/arch/x86/include/asm/disabled-features.h
@@@ -56,6 -50,14 +56,17 @@@
  # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
  #endif
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_RETPOLINE
+ # define DISABLE_RETPOLINE	0
+ #else
+ # define DISABLE_RETPOLINE	((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETPOLINE_LFENCE & 31)) | \
+ 				 (1 << (X86_FEATURE_RETHUNK & 31)))
+ #endif
+ 
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  #ifdef CONFIG_INTEL_IOMMU_SVM
  # define DISABLE_ENQCMD		0
  #else
diff --cc arch/x86/kernel/alternative.c
index 0b65a0cb501d,76b745921509..000000000000
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@@ -261,6 -114,9 +261,12 @@@ static void __init_or_module add_nops(v
  	}
  }
  
++<<<<<<< HEAD
++=======
+ extern s32 __retpoline_sites[], __retpoline_sites_end[];
+ extern s32 __return_sites[], __return_sites_end[];
+ extern s32 __ibt_endbr_seal[], __ibt_endbr_seal_end[];
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
  extern s32 __smp_locks[], __smp_locks_end[];
  void text_poke_early(void *addr, const void *opcode, size_t len);
@@@ -432,6 -508,106 +438,109 @@@ void __init_or_module noinline apply_al
  	}
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Rewrite the compiler generated return thunk tail-calls.
+  *
+  * For example, convert:
+  *
+  *   JMP __x86_return_thunk
+  *
+  * into:
+  *
+  *   RET
+  */
+ static int patch_return(void *addr, struct insn *insn, u8 *bytes)
+ {
+ 	int i = 0;
+ 
+ 	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+ 		return -1;
+ 
+ 	bytes[i++] = RET_INSN_OPCODE;
+ 
+ 	for (; i < insn->length;)
+ 		bytes[i++] = INT3_INSN_OPCODE;
+ 
+ 	return i;
+ }
+ 
+ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+ {
+ 	s32 *s;
+ 
+ 	for (s = start; s < end; s++) {
+ 		void *addr = (void *)s + *s;
+ 		struct insn insn;
+ 		int len, ret;
+ 		u8 bytes[16];
+ 		u8 op1;
+ 
+ 		ret = insn_decode_kernel(&insn, addr);
+ 		if (WARN_ON_ONCE(ret < 0))
+ 			continue;
+ 
+ 		op1 = insn.opcode.bytes[0];
+ 		if (WARN_ON_ONCE(op1 != JMP32_INSN_OPCODE))
+ 			continue;
+ 
+ 		DPRINTK("return thunk at: %pS (%px) len: %d to: %pS",
+ 			addr, addr, insn.length,
+ 			addr + insn.length + insn.immediate.value);
+ 
+ 		len = patch_return(addr, &insn, bytes);
+ 		if (len == insn.length) {
+ 			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
+ 			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
+ 			text_poke_early(addr, bytes, len);
+ 		}
+ 	}
+ }
+ #else /* !CONFIG_RETPOLINE || !CONFIG_OBJTOOL */
+ 
+ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end) { }
+ void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
+ 
+ #endif /* CONFIG_RETPOLINE && CONFIG_OBJTOOL */
+ 
+ #ifdef CONFIG_X86_KERNEL_IBT
+ 
+ /*
+  * Generated by: objtool --ibt
+  */
+ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end)
+ {
+ 	s32 *s;
+ 
+ 	for (s = start; s < end; s++) {
+ 		u32 endbr, poison = gen_endbr_poison();
+ 		void *addr = (void *)s + *s;
+ 
+ 		if (WARN_ON_ONCE(get_kernel_nofault(endbr, addr)))
+ 			continue;
+ 
+ 		if (WARN_ON_ONCE(!is_endbr(endbr)))
+ 			continue;
+ 
+ 		DPRINTK("ENDBR at: %pS (%px)", addr, addr);
+ 
+ 		/*
+ 		 * When we have IBT, the lack of ENDBR will trigger #CP
+ 		 */
+ 		DUMP_BYTES(((u8*)addr), 4, "%px: orig: ", addr);
+ 		DUMP_BYTES(((u8*)&poison), 4, "%px: repl: ", addr);
+ 		text_poke_early(addr, &poison, 4);
+ 	}
+ }
+ 
+ #else
+ 
+ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end) { }
+ 
+ #endif /* CONFIG_X86_KERNEL_IBT */
+ 
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  #ifdef CONFIG_SMP
  static void alternatives_smp_lock(const s32 *start, const s32 *end,
  				  u8 *text, u8 *text_end)
@@@ -630,8 -891,44 +739,45 @@@ void __init alternative_instructions(vo
  	 * patching.
  	 */
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * Paravirt patching and alternative patching can be combined to
+ 	 * replace a function call with a short direct code sequence (e.g.
+ 	 * by setting a constant return value instead of doing that in an
+ 	 * external function).
+ 	 * In order to make this work the following sequence is required:
+ 	 * 1. set (artificial) features depending on used paravirt
+ 	 *    functions which can later influence alternative patching
+ 	 * 2. apply paravirt patching (generally replacing an indirect
+ 	 *    function call with a direct one)
+ 	 * 3. apply alternative patching (e.g. replacing a direct function
+ 	 *    call with a custom code sequence)
+ 	 * Doing paravirt patching after alternative patching would clobber
+ 	 * the optimization of the custom code with a function call again.
+ 	 */
+ 	paravirt_set_cap();
+ 
+ 	/*
+ 	 * First patch paravirt functions, such that we overwrite the indirect
+ 	 * call with the direct call.
+ 	 */
+ 	apply_paravirt(__parainstructions, __parainstructions_end);
+ 
+ 	/*
+ 	 * Rewrite the retpolines, must be done before alternatives since
+ 	 * those can rewrite the retpoline thunks.
+ 	 */
+ 	apply_retpolines(__retpoline_sites, __retpoline_sites_end);
+ 	apply_returns(__return_sites, __return_sites_end);
+ 
+ 	/*
+ 	 * Then patch alternatives, such that those paravirt calls that are in
+ 	 * alternatives can be overwritten by their immediate fragments.
+ 	 */
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	apply_alternatives(__alt_instructions, __alt_instructions_end);
  
 -	apply_ibt_endbr(__ibt_endbr_seal, __ibt_endbr_seal_end);
 -
  #ifdef CONFIG_SMP
  	/* Patch to UP if other cpus not imminent. */
  	if (!noreplace_smp && (num_present_cpus() == 1 || setup_max_cpus <= 1)) {
diff --cc arch/x86/kernel/module.c
index 6645f123419c,67828d973389..000000000000
--- a/arch/x86/kernel/module.c
+++ b/arch/x86/kernel/module.c
@@@ -228,7 -252,8 +228,12 @@@ int module_finalize(const Elf_Ehdr *hdr
  		    struct module *me)
  {
  	const Elf_Shdr *s, *text = NULL, *alt = NULL, *locks = NULL,
++<<<<<<< HEAD
 +		*para = NULL, *orc = NULL, *orc_ip = NULL;
++=======
+ 		*para = NULL, *orc = NULL, *orc_ip = NULL,
+ 		*retpolines = NULL, *returns = NULL, *ibt_endbr = NULL;
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
  
  	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
@@@ -244,8 -269,30 +249,35 @@@
  			orc = s;
  		if (!strcmp(".orc_unwind_ip", secstrings + s->sh_name))
  			orc_ip = s;
++<<<<<<< HEAD
 +	}
 +
++=======
+ 		if (!strcmp(".retpoline_sites", secstrings + s->sh_name))
+ 			retpolines = s;
+ 		if (!strcmp(".return_sites", secstrings + s->sh_name))
+ 			returns = s;
+ 		if (!strcmp(".ibt_endbr_seal", secstrings + s->sh_name))
+ 			ibt_endbr = s;
+ 	}
+ 
+ 	/*
+ 	 * See alternative_instructions() for the ordering rules between the
+ 	 * various patching types.
+ 	 */
+ 	if (para) {
+ 		void *pseg = (void *)para->sh_addr;
+ 		apply_paravirt(pseg, pseg + para->sh_size);
+ 	}
+ 	if (retpolines) {
+ 		void *rseg = (void *)retpolines->sh_addr;
+ 		apply_retpolines(rseg, rseg + retpolines->sh_size);
+ 	}
+ 	if (returns) {
+ 		void *rseg = (void *)returns->sh_addr;
+ 		apply_returns(rseg, rseg + returns->sh_size);
+ 	}
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	if (alt) {
  		/* patch .altinstructions */
  		void *aseg = (void *)alt->sh_addr;
diff --cc arch/x86/kernel/vmlinux.lds.S
index 45d0dd83ad4d,ada7eb738113..000000000000
--- a/arch/x86/kernel/vmlinux.lds.S
+++ b/arch/x86/kernel/vmlinux.lds.S
@@@ -270,6 -271,36 +270,39 @@@ SECTION
  		__parainstructions_end = .;
  	}
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_RETPOLINE
+ 	/*
+ 	 * List of instructions that call/jmp/jcc to retpoline thunks
+ 	 * __x86_indirect_thunk_*(). These instructions can be patched along
+ 	 * with alternatives, after which the section can be freed.
+ 	 */
+ 	. = ALIGN(8);
+ 	.retpoline_sites : AT(ADDR(.retpoline_sites) - LOAD_OFFSET) {
+ 		__retpoline_sites = .;
+ 		*(.retpoline_sites)
+ 		__retpoline_sites_end = .;
+ 	}
+ 
+ 	. = ALIGN(8);
+ 	.return_sites : AT(ADDR(.return_sites) - LOAD_OFFSET) {
+ 		__return_sites = .;
+ 		*(.return_sites)
+ 		__return_sites_end = .;
+ 	}
+ #endif
+ 
+ #ifdef CONFIG_X86_KERNEL_IBT
+ 	. = ALIGN(8);
+ 	.ibt_endbr_seal : AT(ADDR(.ibt_endbr_seal) - LOAD_OFFSET) {
+ 		__ibt_endbr_seal = .;
+ 		*(.ibt_endbr_seal)
+ 		__ibt_endbr_seal_end = .;
+ 	}
+ #endif
+ 
++>>>>>>> 15e67227c49a (x86: Undo return-thunk damage)
  	/*
  	 * struct alt_inst entries. From the header (alternative.h):
  	 * "Alternative instructions for different CPU types or capabilities"
* Unmerged path arch/x86/include/asm/alternative.h
diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h
index 05564117439b..a567e42dcb6c 100644
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@ -296,6 +296,7 @@
 /* FREE!				(11*32+11) */
 #define X86_FEATURE_RETPOLINE		(11*32+12) /* "" Generic Retpoline mitigation for Spectre variant 2 */
 #define X86_FEATURE_RETPOLINE_LFENCE	(11*32+13) /* "" Use LFENCE for Spectre variant 2 */
+#define X86_FEATURE_RETHUNK		(11*32+14) /* "" Use REturn THUNK */
 
 /* Intel-defined CPU features, CPUID level 0x00000007:1 (EAX), word 12 */
 #define X86_FEATURE_AVX_VNNI		(12*32+ 4) /* AVX VNNI instructions */
* Unmerged path arch/x86/include/asm/disabled-features.h
* Unmerged path arch/x86/kernel/alternative.c
* Unmerged path arch/x86/kernel/module.c
* Unmerged path arch/x86/kernel/vmlinux.lds.S
