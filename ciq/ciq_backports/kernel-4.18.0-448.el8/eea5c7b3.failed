drm/amdgpu/pm: fix the null pointer while the smu is disabled

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Huang Rui <ray.huang@amd.com>
commit eea5c7b3390c6e006ba4cbd906447dd8cea8cfbf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/eea5c7b3.failed

It needs to check if the pp_funcs is initialized while release the
context, otherwise it will trigger null pointer panic while the software
smu is not enabled.

[ 1109.404555] BUG: kernel NULL pointer dereference, address: 0000000000000078
[ 1109.404609] #PF: supervisor read access in kernel mode
[ 1109.404638] #PF: error_code(0x0000) - not-present page
[ 1109.404657] PGD 0 P4D 0
[ 1109.404672] Oops: 0000 [#1] PREEMPT SMP NOPTI
[ 1109.404701] CPU: 7 PID: 9150 Comm: amdgpu_test Tainted: G           OEL    5.16.0-custom #1
[ 1109.404732] Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
[ 1109.404765] RIP: 0010:amdgpu_dpm_force_performance_level+0x1d/0x170 [amdgpu]
[ 1109.405109] Code: 5d c3 44 8b a3 f0 80 00 00 eb e5 66 90 0f 1f 44 00 00 55 48 89 e5 41 57 41 56 41 55 41 54 53 48 83 ec 08 4c 8b b7 f0 7d 00 00 <49> 83 7e 78 00 0f 84 f2 00 00 00 80 bf 87 80 00 00 00 48 89 fb 0f
[ 1109.405176] RSP: 0018:ffffaf3083ad7c20 EFLAGS: 00010282
[ 1109.405203] RAX: 0000000000000000 RBX: ffff9796b1c14600 RCX: 0000000002862007
[ 1109.405229] RDX: ffff97968591c8c0 RSI: 0000000000000001 RDI: ffff9796a3700000
[ 1109.405260] RBP: ffffaf3083ad7c50 R08: ffffffff9897de00 R09: ffff979688d9db60
[ 1109.405286] R10: 0000000000000000 R11: ffff979688d9db90 R12: 0000000000000001
[ 1109.405316] R13: ffff9796a3700000 R14: 0000000000000000 R15: ffff9796a3708fc0
[ 1109.405345] FS:  00007ff055cff180(0000) GS:ffff9796bfdc0000(0000) knlGS:0000000000000000
[ 1109.405378] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[ 1109.405400] CR2: 0000000000000078 CR3: 000000000a394000 CR4: 00000000000506e0
[ 1109.405434] Call Trace:
[ 1109.405445]  <TASK>
[ 1109.405456]  ? delete_object_full+0x1d/0x20
[ 1109.405480]  amdgpu_ctx_set_stable_pstate+0x7c/0xa0 [amdgpu]
[ 1109.405698]  amdgpu_ctx_fini.part.0+0xcb/0x100 [amdgpu]
[ 1109.405911]  amdgpu_ctx_do_release+0x71/0x80 [amdgpu]
[ 1109.406121]  amdgpu_ctx_ioctl+0x52d/0x550 [amdgpu]
[ 1109.406327]  ? _raw_spin_unlock+0x1a/0x30
[ 1109.406354]  ? drm_gem_handle_delete+0x81/0xb0 [drm]
[ 1109.406400]  ? amdgpu_ctx_get_entity+0x2c0/0x2c0 [amdgpu]
[ 1109.406609]  drm_ioctl_kernel+0xb6/0x140 [drm]

	Signed-off-by: Huang Rui <ray.huang@amd.com>
	Reviewed-by: Aaron Liu <aaron.liu@amd.com>
	Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
(cherry picked from commit eea5c7b3390c6e006ba4cbd906447dd8cea8cfbf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/amd/pm/amdgpu_dpm.c
diff --cc drivers/gpu/drm/amd/pm/amdgpu_dpm.c
index 08362d506534,6caf48cec9f3..000000000000
--- a/drivers/gpu/drm/amd/pm/amdgpu_dpm.c
+++ b/drivers/gpu/drm/amd/pm/amdgpu_dpm.c
@@@ -1249,367 -431,604 +1249,665 @@@ void amdgpu_dpm_thermal_work_handler(st
  	if (!adev->pm.dpm_enabled)
  		return;
  
 -	if (!pp_funcs->pm_compute_clocks)
 -		return;
 -
 +	if (!amdgpu_dpm_read_sensor(adev, AMDGPU_PP_SENSOR_GPU_TEMP,
 +				    (void *)&temp, &size)) {
 +		if (temp < adev->pm.dpm.thermal.min_temp)
 +			/* switch back the user state */
 +			dpm_state = adev->pm.dpm.user_state;
 +	} else {
 +		if (adev->pm.dpm.thermal.high_to_low)
 +			/* switch back the user state */
 +			dpm_state = adev->pm.dpm.user_state;
 +	}
  	mutex_lock(&adev->pm.mutex);
 -	pp_funcs->pm_compute_clocks(adev->powerplay.pp_handle);
 +	if (dpm_state == POWER_STATE_TYPE_INTERNAL_THERMAL)
 +		adev->pm.dpm.thermal_active = true;
 +	else
 +		adev->pm.dpm.thermal_active = false;
 +	adev->pm.dpm.state = dpm_state;
  	mutex_unlock(&adev->pm.mutex);
 -}
 -
 -void amdgpu_dpm_enable_uvd(struct amdgpu_device *adev, bool enable)
 -{
 -	int ret = 0;
  
 -	ret = amdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_UVD, !enable);
 -	if (ret)
 -		DRM_ERROR("Dpm %s uvd failed, ret = %d. \n",
 -			  enable ? "enable" : "disable", ret);
 +	amdgpu_pm_compute_clocks(adev);
  }
  
 -void amdgpu_dpm_enable_vce(struct amdgpu_device *adev, bool enable)
 +static struct amdgpu_ps *amdgpu_dpm_pick_power_state(struct amdgpu_device *adev,
 +						     enum amd_pm_state_type dpm_state)
  {
 -	int ret = 0;
 +	int i;
 +	struct amdgpu_ps *ps;
 +	u32 ui_class;
 +	bool single_display = (adev->pm.dpm.new_active_crtc_count < 2) ?
 +		true : false;
  
 -	ret = amdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_VCE, !enable);
 -	if (ret)
 -		DRM_ERROR("Dpm %s vce failed, ret = %d. \n",
 -			  enable ? "enable" : "disable", ret);
 -}
 +	/* check if the vblank period is too short to adjust the mclk */
 +	if (single_display && adev->powerplay.pp_funcs->vblank_too_short) {
 +		if (amdgpu_dpm_vblank_too_short(adev))
 +			single_display = false;
 +	}
  
 -void amdgpu_dpm_enable_jpeg(struct amdgpu_device *adev, bool enable)
 -{
 -	int ret = 0;
 +	/* certain older asics have a separare 3D performance state,
 +	 * so try that first if the user selected performance
 +	 */
 +	if (dpm_state == POWER_STATE_TYPE_PERFORMANCE)
 +		dpm_state = POWER_STATE_TYPE_INTERNAL_3DPERF;
 +	/* balanced states don't exist at the moment */
 +	if (dpm_state == POWER_STATE_TYPE_BALANCED)
 +		dpm_state = POWER_STATE_TYPE_PERFORMANCE;
 +
 +restart_search:
 +	/* Pick the best power state based on current conditions */
 +	for (i = 0; i < adev->pm.dpm.num_ps; i++) {
 +		ps = &adev->pm.dpm.ps[i];
 +		ui_class = ps->class & ATOM_PPLIB_CLASSIFICATION_UI_MASK;
 +		switch (dpm_state) {
 +		/* user states */
 +		case POWER_STATE_TYPE_BATTERY:
 +			if (ui_class == ATOM_PPLIB_CLASSIFICATION_UI_BATTERY) {
 +				if (ps->caps & ATOM_PPLIB_SINGLE_DISPLAY_ONLY) {
 +					if (single_display)
 +						return ps;
 +				} else
 +					return ps;
 +			}
 +			break;
 +		case POWER_STATE_TYPE_BALANCED:
 +			if (ui_class == ATOM_PPLIB_CLASSIFICATION_UI_BALANCED) {
 +				if (ps->caps & ATOM_PPLIB_SINGLE_DISPLAY_ONLY) {
 +					if (single_display)
 +						return ps;
 +				} else
 +					return ps;
 +			}
 +			break;
 +		case POWER_STATE_TYPE_PERFORMANCE:
 +			if (ui_class == ATOM_PPLIB_CLASSIFICATION_UI_PERFORMANCE) {
 +				if (ps->caps & ATOM_PPLIB_SINGLE_DISPLAY_ONLY) {
 +					if (single_display)
 +						return ps;
 +				} else
 +					return ps;
 +			}
 +			break;
 +		/* internal states */
 +		case POWER_STATE_TYPE_INTERNAL_UVD:
 +			if (adev->pm.dpm.uvd_ps)
 +				return adev->pm.dpm.uvd_ps;
 +			else
 +				break;
 +		case POWER_STATE_TYPE_INTERNAL_UVD_SD:
 +			if (ps->class & ATOM_PPLIB_CLASSIFICATION_SDSTATE)
 +				return ps;
 +			break;
 +		case POWER_STATE_TYPE_INTERNAL_UVD_HD:
 +			if (ps->class & ATOM_PPLIB_CLASSIFICATION_HDSTATE)
 +				return ps;
 +			break;
 +		case POWER_STATE_TYPE_INTERNAL_UVD_HD2:
 +			if (ps->class & ATOM_PPLIB_CLASSIFICATION_HD2STATE)
 +				return ps;
 +			break;
 +		case POWER_STATE_TYPE_INTERNAL_UVD_MVC:
 +			if (ps->class2 & ATOM_PPLIB_CLASSIFICATION2_MVC)
 +				return ps;
 +			break;
 +		case POWER_STATE_TYPE_INTERNAL_BOOT:
 +			return adev->pm.dpm.boot_ps;
 +		case POWER_STATE_TYPE_INTERNAL_THERMAL:
 +			if (ps->class & ATOM_PPLIB_CLASSIFICATION_THERMAL)
 +				return ps;
 +			break;
 +		case POWER_STATE_TYPE_INTERNAL_ACPI:
 +			if (ps->class & ATOM_PPLIB_CLASSIFICATION_ACPI)
 +				return ps;
 +			break;
 +		case POWER_STATE_TYPE_INTERNAL_ULV:
 +			if (ps->class2 & ATOM_PPLIB_CLASSIFICATION2_ULV)
 +				return ps;
 +			break;
 +		case POWER_STATE_TYPE_INTERNAL_3DPERF:
 +			if (ps->class & ATOM_PPLIB_CLASSIFICATION_3DPERFORMANCE)
 +				return ps;
 +			break;
 +		default:
 +			break;
 +		}
 +	}
 +	/* use a fallback state if we didn't match */
 +	switch (dpm_state) {
 +	case POWER_STATE_TYPE_INTERNAL_UVD_SD:
 +		dpm_state = POWER_STATE_TYPE_INTERNAL_UVD_HD;
 +		goto restart_search;
 +	case POWER_STATE_TYPE_INTERNAL_UVD_HD:
 +	case POWER_STATE_TYPE_INTERNAL_UVD_HD2:
 +	case POWER_STATE_TYPE_INTERNAL_UVD_MVC:
 +		if (adev->pm.dpm.uvd_ps) {
 +			return adev->pm.dpm.uvd_ps;
 +		} else {
 +			dpm_state = POWER_STATE_TYPE_PERFORMANCE;
 +			goto restart_search;
 +		}
 +	case POWER_STATE_TYPE_INTERNAL_THERMAL:
 +		dpm_state = POWER_STATE_TYPE_INTERNAL_ACPI;
 +		goto restart_search;
 +	case POWER_STATE_TYPE_INTERNAL_ACPI:
 +		dpm_state = POWER_STATE_TYPE_BATTERY;
 +		goto restart_search;
 +	case POWER_STATE_TYPE_BATTERY:
 +	case POWER_STATE_TYPE_BALANCED:
 +	case POWER_STATE_TYPE_INTERNAL_3DPERF:
 +		dpm_state = POWER_STATE_TYPE_PERFORMANCE;
 +		goto restart_search;
 +	default:
 +		break;
 +	}
  
 -	ret = amdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_JPEG, !enable);
 -	if (ret)
 -		DRM_ERROR("Dpm %s jpeg failed, ret = %d. \n",
 -			  enable ? "enable" : "disable", ret);
 +	return NULL;
  }
  
 -int amdgpu_pm_load_smu_firmware(struct amdgpu_device *adev, uint32_t *smu_version)
 +static void amdgpu_dpm_change_power_state_locked(struct amdgpu_device *adev)
  {
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int r = 0;
 +	struct amdgpu_ps *ps;
 +	enum amd_pm_state_type dpm_state;
 +	int ret;
 +	bool equal = false;
  
 -	if (!pp_funcs || !pp_funcs->load_firmware)
 -		return 0;
 +	/* if dpm init failed */
 +	if (!adev->pm.dpm_enabled)
 +		return;
  
 -	mutex_lock(&adev->pm.mutex);
 -	r = pp_funcs->load_firmware(adev->powerplay.pp_handle);
 -	if (r) {
 -		pr_err("smu firmware loading failed\n");
 -		goto out;
 +	if (adev->pm.dpm.user_state != adev->pm.dpm.state) {
 +		/* add other state override checks here */
 +		if ((!adev->pm.dpm.thermal_active) &&
 +		    (!adev->pm.dpm.uvd_active))
 +			adev->pm.dpm.state = adev->pm.dpm.user_state;
  	}
 +	dpm_state = adev->pm.dpm.state;
  
 -	if (smu_version)
 -		*smu_version = adev->pm.fw_version;
 +	ps = amdgpu_dpm_pick_power_state(adev, dpm_state);
 +	if (ps)
 +		adev->pm.dpm.requested_ps = ps;
 +	else
 +		return;
  
 -out:
 -	mutex_unlock(&adev->pm.mutex);
 -	return r;
 -}
 +	if (amdgpu_dpm == 1 && adev->powerplay.pp_funcs->print_power_state) {
 +		printk("switching from power state:\n");
 +		amdgpu_dpm_print_power_state(adev, adev->pm.dpm.current_ps);
 +		printk("switching to power state:\n");
 +		amdgpu_dpm_print_power_state(adev, adev->pm.dpm.requested_ps);
 +	}
  
 -int amdgpu_dpm_handle_passthrough_sbr(struct amdgpu_device *adev, bool enable)
 -{
 -	int ret = 0;
 +	/* update whether vce is active */
 +	ps->vce_active = adev->pm.dpm.vce_active;
 +	if (adev->powerplay.pp_funcs->display_configuration_changed)
 +		amdgpu_dpm_display_configuration_changed(adev);
 +
 +	ret = amdgpu_dpm_pre_set_power_state(adev);
 +	if (ret)
 +		return;
 +
 +	if (adev->powerplay.pp_funcs->check_state_equal) {
 +		if (0 != amdgpu_dpm_check_state_equal(adev, adev->pm.dpm.current_ps, adev->pm.dpm.requested_ps, &equal))
 +			equal = false;
 +	}
 +
 +	if (equal)
 +		return;
 +
 +	amdgpu_dpm_set_power_state(adev);
 +	amdgpu_dpm_post_set_power_state(adev);
 +
 +	adev->pm.dpm.current_active_crtcs = adev->pm.dpm.new_active_crtcs;
 +	adev->pm.dpm.current_active_crtc_count = adev->pm.dpm.new_active_crtc_count;
 +
 +	if (adev->powerplay.pp_funcs->force_performance_level) {
 +		if (adev->pm.dpm.thermal_active) {
 +			enum amd_dpm_forced_level level = adev->pm.dpm.forced_level;
 +			/* force low perf level for thermal */
 +			amdgpu_dpm_force_performance_level(adev, AMD_DPM_FORCED_LEVEL_LOW);
 +			/* save the user's level */
 +			adev->pm.dpm.forced_level = level;
 +		} else {
 +			/* otherwise, user selected level */
 +			amdgpu_dpm_force_performance_level(adev, adev->pm.dpm.forced_level);
 +		}
 +	}
 +}
 +
 +void amdgpu_pm_compute_clocks(struct amdgpu_device *adev)
 +{
 +	int i = 0;
 +
 +	if (!adev->pm.dpm_enabled)
 +		return;
 +
 +	if (adev->mode_info.num_crtc)
 +		amdgpu_display_bandwidth_update(adev);
 +
 +	for (i = 0; i < AMDGPU_MAX_RINGS; i++) {
 +		struct amdgpu_ring *ring = adev->rings[i];
 +		if (ring && ring->sched.ready)
 +			amdgpu_fence_wait_empty(ring);
 +	}
 +
 +	if (adev->powerplay.pp_funcs->dispatch_tasks) {
 +		if (!amdgpu_device_has_dc_support(adev)) {
 +			mutex_lock(&adev->pm.mutex);
 +			amdgpu_dpm_get_active_displays(adev);
 +			adev->pm.pm_display_cfg.num_display = adev->pm.dpm.new_active_crtc_count;
 +			adev->pm.pm_display_cfg.vrefresh = amdgpu_dpm_get_vrefresh(adev);
 +			adev->pm.pm_display_cfg.min_vblank_time = amdgpu_dpm_get_vblank_time(adev);
 +			/* we have issues with mclk switching with
 +			 * refresh rates over 120 hz on the non-DC code.
 +			 */
 +			if (adev->pm.pm_display_cfg.vrefresh > 120)
 +				adev->pm.pm_display_cfg.min_vblank_time = 0;
 +			if (adev->powerplay.pp_funcs->display_configuration_change)
 +				adev->powerplay.pp_funcs->display_configuration_change(
 +							adev->powerplay.pp_handle,
 +							&adev->pm.pm_display_cfg);
 +			mutex_unlock(&adev->pm.mutex);
 +		}
 +		amdgpu_dpm_dispatch_task(adev, AMD_PP_TASK_DISPLAY_CONFIG_CHANGE, NULL);
 +	} else {
 +		mutex_lock(&adev->pm.mutex);
 +		amdgpu_dpm_get_active_displays(adev);
 +		amdgpu_dpm_change_power_state_locked(adev);
 +		mutex_unlock(&adev->pm.mutex);
 +	}
 +}
 +
 +void amdgpu_dpm_enable_uvd(struct amdgpu_device *adev, bool enable)
 +{
 +	int ret = 0;
 +
 +	if (adev->family == AMDGPU_FAMILY_SI) {
 +		mutex_lock(&adev->pm.mutex);
 +		if (enable) {
 +			adev->pm.dpm.uvd_active = true;
 +			adev->pm.dpm.state = POWER_STATE_TYPE_INTERNAL_UVD;
 +		} else {
 +			adev->pm.dpm.uvd_active = false;
 +		}
 +		mutex_unlock(&adev->pm.mutex);
 +
 +		amdgpu_pm_compute_clocks(adev);
 +	} else {
 +		ret = amdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_UVD, !enable);
 +		if (ret)
 +			DRM_ERROR("Dpm %s uvd failed, ret = %d. \n",
 +				  enable ? "enable" : "disable", ret);
 +
 +		/* enable/disable Low Memory PState for UVD (4k videos) */
 +		if (adev->asic_type == CHIP_STONEY &&
 +			adev->uvd.decode_image_width >= WIDTH_4K) {
 +			struct pp_hwmgr *hwmgr = adev->powerplay.pp_handle;
 +
 +			if (hwmgr && hwmgr->hwmgr_func &&
 +			    hwmgr->hwmgr_func->update_nbdpm_pstate)
 +				hwmgr->hwmgr_func->update_nbdpm_pstate(hwmgr,
 +								       !enable,
 +								       true);
 +		}
 +	}
 +}
 +
 +void amdgpu_dpm_enable_vce(struct amdgpu_device *adev, bool enable)
 +{
 +	int ret = 0;
 +
 +	if (adev->family == AMDGPU_FAMILY_SI) {
 +		mutex_lock(&adev->pm.mutex);
 +		if (enable) {
 +			adev->pm.dpm.vce_active = true;
 +			/* XXX select vce level based on ring/task */
 +			adev->pm.dpm.vce_level = AMD_VCE_LEVEL_AC_ALL;
 +		} else {
 +			adev->pm.dpm.vce_active = false;
 +		}
 +		mutex_unlock(&adev->pm.mutex);
 +
 +		amdgpu_pm_compute_clocks(adev);
 +	} else {
 +		ret = amdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_VCE, !enable);
 +		if (ret)
 +			DRM_ERROR("Dpm %s vce failed, ret = %d. \n",
 +				  enable ? "enable" : "disable", ret);
 +	}
 +}
 +
 +void amdgpu_pm_print_power_states(struct amdgpu_device *adev)
 +{
 +	int i;
 +
 +	if (adev->powerplay.pp_funcs->print_power_state == NULL)
 +		return;
 +
 +	for (i = 0; i < adev->pm.dpm.num_ps; i++)
 +		amdgpu_dpm_print_power_state(adev, &adev->pm.dpm.ps[i]);
 +
 +}
 +
 +void amdgpu_dpm_enable_jpeg(struct amdgpu_device *adev, bool enable)
 +{
 +	int ret = 0;
 +
 +	ret = amdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_JPEG, !enable);
 +	if (ret)
 +		DRM_ERROR("Dpm %s jpeg failed, ret = %d. \n",
 +			  enable ? "enable" : "disable", ret);
 +}
 +
 +int amdgpu_pm_load_smu_firmware(struct amdgpu_device *adev, uint32_t *smu_version)
 +{
 +	int r;
 +
++<<<<<<< HEAD
 +	if (adev->powerplay.pp_funcs && adev->powerplay.pp_funcs->load_firmware) {
 +		r = adev->powerplay.pp_funcs->load_firmware(adev->powerplay.pp_handle);
 +		if (r) {
 +			pr_err("smu firmware loading failed\n");
 +			return r;
++=======
++	if (!pp_funcs || !pp_funcs->load_firmware)
++		return 0;
++
++	mutex_lock(&adev->pm.mutex);
++	r = pp_funcs->load_firmware(adev->powerplay.pp_handle);
++	if (r) {
++		pr_err("smu firmware loading failed\n");
++		goto out;
++	}
++
++	if (smu_version)
++		*smu_version = adev->pm.fw_version;
++
++out:
++	mutex_unlock(&adev->pm.mutex);
++	return r;
++}
++
++int amdgpu_dpm_handle_passthrough_sbr(struct amdgpu_device *adev, bool enable)
++{
++	int ret = 0;
+ 
+ 	if (is_support_sw_smu(adev)) {
+ 		mutex_lock(&adev->pm.mutex);
+ 		ret = smu_handle_passthrough_sbr(adev->powerplay.pp_handle,
+ 						 enable);
+ 		mutex_unlock(&adev->pm.mutex);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ int amdgpu_dpm_send_hbm_bad_pages_num(struct amdgpu_device *adev, uint32_t size)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 	int ret = 0;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return -EOPNOTSUPP;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_send_hbm_bad_pages_num(smu, size);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ int amdgpu_dpm_send_hbm_bad_channel_flag(struct amdgpu_device *adev, uint32_t size)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 	int ret = 0;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return -EOPNOTSUPP;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_send_hbm_bad_channel_flag(smu, size);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ int amdgpu_dpm_get_dpm_freq_range(struct amdgpu_device *adev,
+ 				  enum pp_clock_type type,
+ 				  uint32_t *min,
+ 				  uint32_t *max)
+ {
+ 	int ret = 0;
+ 
+ 	if (type != PP_SCLK)
+ 		return -EINVAL;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return -EOPNOTSUPP;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_get_dpm_freq_range(adev->powerplay.pp_handle,
+ 				     SMU_SCLK,
+ 				     min,
+ 				     max);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ int amdgpu_dpm_set_soft_freq_range(struct amdgpu_device *adev,
+ 				   enum pp_clock_type type,
+ 				   uint32_t min,
+ 				   uint32_t max)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 	int ret = 0;
+ 
+ 	if (type != PP_SCLK)
+ 		return -EINVAL;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return -EOPNOTSUPP;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_set_soft_freq_range(smu,
+ 				      SMU_SCLK,
+ 				      min,
+ 				      max);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ int amdgpu_dpm_write_watermarks_table(struct amdgpu_device *adev)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 	int ret = 0;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return 0;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_write_watermarks_table(smu);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ int amdgpu_dpm_wait_for_event(struct amdgpu_device *adev,
+ 			      enum smu_event_type event,
+ 			      uint64_t event_arg)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 	int ret = 0;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return -EOPNOTSUPP;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_wait_for_event(smu, event, event_arg);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ int amdgpu_dpm_get_status_gfxoff(struct amdgpu_device *adev, uint32_t *value)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 	int ret = 0;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return -EOPNOTSUPP;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_get_status_gfxoff(smu, value);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ uint64_t amdgpu_dpm_get_thermal_throttling_counter(struct amdgpu_device *adev)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return 0;
+ 
+ 	return atomic64_read(&smu->throttle_int_counter);
+ }
+ 
+ /* amdgpu_dpm_gfx_state_change - Handle gfx power state change set
+  * @adev: amdgpu_device pointer
+  * @state: gfx power state(1 -sGpuChangeState_D0Entry and 2 -sGpuChangeState_D3Entry)
+  *
+  */
+ void amdgpu_dpm_gfx_state_change(struct amdgpu_device *adev,
+ 				 enum gfx_change_state state)
+ {
+ 	mutex_lock(&adev->pm.mutex);
+ 	if (adev->powerplay.pp_funcs &&
+ 	    adev->powerplay.pp_funcs->gfx_state_change_set)
+ 		((adev)->powerplay.pp_funcs->gfx_state_change_set(
+ 			(adev)->powerplay.pp_handle, state));
+ 	mutex_unlock(&adev->pm.mutex);
+ }
+ 
+ int amdgpu_dpm_get_ecc_info(struct amdgpu_device *adev,
+ 			    void *umc_ecc)
+ {
+ 	struct smu_context *smu = adev->powerplay.pp_handle;
+ 	int ret = 0;
+ 
+ 	if (!is_support_sw_smu(adev))
+ 		return -EOPNOTSUPP;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	ret = smu_get_ecc_info(smu, umc_ecc);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return ret;
+ }
+ 
+ struct amd_vce_state *amdgpu_dpm_get_vce_clock_state(struct amdgpu_device *adev,
+ 						     uint32_t idx)
+ {
+ 	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
+ 	struct amd_vce_state *vstate = NULL;
+ 
+ 	if (!pp_funcs->get_vce_clock_state)
+ 		return NULL;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	vstate = pp_funcs->get_vce_clock_state(adev->powerplay.pp_handle,
+ 					       idx);
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return vstate;
+ }
+ 
+ void amdgpu_dpm_get_current_power_state(struct amdgpu_device *adev,
+ 					enum amd_pm_state_type *state)
+ {
+ 	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 
+ 	if (!pp_funcs->get_current_power_state) {
+ 		*state = adev->pm.dpm.user_state;
+ 		goto out;
+ 	}
+ 
+ 	*state = pp_funcs->get_current_power_state(adev->powerplay.pp_handle);
+ 	if (*state < POWER_STATE_TYPE_DEFAULT ||
+ 	    *state > POWER_STATE_TYPE_INTERNAL_3DPERF)
+ 		*state = adev->pm.dpm.user_state;
+ 
+ out:
+ 	mutex_unlock(&adev->pm.mutex);
+ }
+ 
+ void amdgpu_dpm_set_power_state(struct amdgpu_device *adev,
+ 				enum amd_pm_state_type state)
+ {
+ 	mutex_lock(&adev->pm.mutex);
+ 	adev->pm.dpm.user_state = state;
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	if (is_support_sw_smu(adev))
+ 		return;
+ 
+ 	if (amdgpu_dpm_dispatch_task(adev,
+ 				     AMD_PP_TASK_ENABLE_USER_STATE,
+ 				     &state) == -EOPNOTSUPP)
+ 		amdgpu_dpm_compute_clocks(adev);
+ }
+ 
+ enum amd_dpm_forced_level amdgpu_dpm_get_performance_level(struct amdgpu_device *adev)
+ {
+ 	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
+ 	enum amd_dpm_forced_level level;
+ 
+ 	mutex_lock(&adev->pm.mutex);
+ 	if (pp_funcs->get_performance_level)
+ 		level = pp_funcs->get_performance_level(adev->powerplay.pp_handle);
+ 	else
+ 		level = adev->pm.dpm.forced_level;
+ 	mutex_unlock(&adev->pm.mutex);
+ 
+ 	return level;
+ }
+ 
+ int amdgpu_dpm_force_performance_level(struct amdgpu_device *adev,
+ 				       enum amd_dpm_forced_level level)
+ {
+ 	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
+ 	enum amd_dpm_forced_level current_level;
+ 	uint32_t profile_mode_mask = AMD_DPM_FORCED_LEVEL_PROFILE_STANDARD |
+ 					AMD_DPM_FORCED_LEVEL_PROFILE_MIN_SCLK |
+ 					AMD_DPM_FORCED_LEVEL_PROFILE_MIN_MCLK |
+ 					AMD_DPM_FORCED_LEVEL_PROFILE_PEAK;
+ 
+ 	if (!pp_funcs || !pp_funcs->force_performance_level)
+ 		return 0;
+ 
+ 	if (adev->pm.dpm.thermal_active)
+ 		return -EINVAL;
+ 
+ 	current_level = amdgpu_dpm_get_performance_level(adev);
+ 	if (current_level == level)
+ 		return 0;
+ 
+ 	if (adev->asic_type == CHIP_RAVEN) {
+ 		if (!(adev->apu_flags & AMD_APU_IS_RAVEN2)) {
+ 			if (current_level != AMD_DPM_FORCED_LEVEL_MANUAL &&
+ 			    level == AMD_DPM_FORCED_LEVEL_MANUAL)
+ 				amdgpu_gfx_off_ctrl(adev, false);
+ 			else if (current_level == AMD_DPM_FORCED_LEVEL_MANUAL &&
+ 				 level != AMD_DPM_FORCED_LEVEL_MANUAL)
+ 				amdgpu_gfx_off_ctrl(adev, true);
++>>>>>>> eea5c7b3390c (drm/amdgpu/pm: fix the null pointer while the smu is disabled)
  		}
 -	}
 -
 -	if (!(current_level & profile_mode_mask) &&
 -	    (level == AMD_DPM_FORCED_LEVEL_PROFILE_EXIT))
 -		return -EINVAL;
 -
 -	if (!(current_level & profile_mode_mask) &&
 -	      (level & profile_mode_mask)) {
 -		/* enter UMD Pstate */
 -		amdgpu_device_ip_set_powergating_state(adev,
 -						       AMD_IP_BLOCK_TYPE_GFX,
 -						       AMD_PG_STATE_UNGATE);
 -		amdgpu_device_ip_set_clockgating_state(adev,
 -						       AMD_IP_BLOCK_TYPE_GFX,
 -						       AMD_CG_STATE_UNGATE);
 -	} else if ((current_level & profile_mode_mask) &&
 -		    !(level & profile_mode_mask)) {
 -		/* exit UMD Pstate */
 -		amdgpu_device_ip_set_clockgating_state(adev,
 -						       AMD_IP_BLOCK_TYPE_GFX,
 -						       AMD_CG_STATE_GATE);
 -		amdgpu_device_ip_set_powergating_state(adev,
 -						       AMD_IP_BLOCK_TYPE_GFX,
 -						       AMD_PG_STATE_GATE);
 -	}
 -
 -	mutex_lock(&adev->pm.mutex);
 -
 -	if (pp_funcs->force_performance_level(adev->powerplay.pp_handle,
 -					      level)) {
 -		mutex_unlock(&adev->pm.mutex);
 -		return -EINVAL;
 -	}
 -
 -	adev->pm.dpm.forced_level = level;
 -
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return 0;
 -}
 -
 -int amdgpu_dpm_get_pp_num_states(struct amdgpu_device *adev,
 -				 struct pp_states_info *states)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->get_pp_num_states)
 -		return -EOPNOTSUPP;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->get_pp_num_states(adev->powerplay.pp_handle,
 -					  states);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_dispatch_task(struct amdgpu_device *adev,
 -			      enum amd_pp_task task_id,
 -			      enum amd_pm_state_type *user_state)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->dispatch_tasks)
 -		return -EOPNOTSUPP;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->dispatch_tasks(adev->powerplay.pp_handle,
 -				       task_id,
 -				       user_state);
 -	mutex_unlock(&adev->pm.mutex);
  
 -	return ret;
 -}
 -
 -int amdgpu_dpm_get_pp_table(struct amdgpu_device *adev, char **table)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->get_pp_table)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->get_pp_table(adev->powerplay.pp_handle,
 -				     table);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_set_fine_grain_clk_vol(struct amdgpu_device *adev,
 -				      uint32_t type,
 -				      long *input,
 -				      uint32_t size)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->set_fine_grain_clk_vol)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->set_fine_grain_clk_vol(adev->powerplay.pp_handle,
 -					       type,
 -					       input,
 -					       size);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_odn_edit_dpm_table(struct amdgpu_device *adev,
 -				  uint32_t type,
 -				  long *input,
 -				  uint32_t size)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->odn_edit_dpm_table)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->odn_edit_dpm_table(adev->powerplay.pp_handle,
 -					   type,
 -					   input,
 -					   size);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_print_clock_levels(struct amdgpu_device *adev,
 -				  enum pp_clock_type type,
 -				  char *buf)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->print_clock_levels)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->print_clock_levels(adev->powerplay.pp_handle,
 -					   type,
 -					   buf);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_emit_clock_levels(struct amdgpu_device *adev,
 -				  enum pp_clock_type type,
 -				  char *buf,
 -				  int *offset)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->emit_clock_levels)
 -		return -ENOENT;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->emit_clock_levels(adev->powerplay.pp_handle,
 -					   type,
 -					   buf,
 -					   offset);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_set_ppfeature_status(struct amdgpu_device *adev,
 -				    uint64_t ppfeature_masks)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->set_ppfeature_status)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->set_ppfeature_status(adev->powerplay.pp_handle,
 -					     ppfeature_masks);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_get_ppfeature_status(struct amdgpu_device *adev, char *buf)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->get_ppfeature_status)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->get_ppfeature_status(adev->powerplay.pp_handle,
 -					     buf);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_force_clock_level(struct amdgpu_device *adev,
 -				 enum pp_clock_type type,
 -				 uint32_t mask)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->force_clock_level)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->force_clock_level(adev->powerplay.pp_handle,
 -					  type,
 -					  mask);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_get_sclk_od(struct amdgpu_device *adev)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -	int ret = 0;
 -
 -	if (!pp_funcs->get_sclk_od)
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	ret = pp_funcs->get_sclk_od(adev->powerplay.pp_handle);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	return ret;
 -}
 -
 -int amdgpu_dpm_set_sclk_od(struct amdgpu_device *adev, uint32_t value)
 -{
 -	const struct amd_pm_funcs *pp_funcs = adev->powerplay.pp_funcs;
 -
 -	if (is_support_sw_smu(adev))
 -		return 0;
 -
 -	mutex_lock(&adev->pm.mutex);
 -	if (pp_funcs->set_sclk_od)
 -		pp_funcs->set_sclk_od(adev->powerplay.pp_handle, value);
 -	mutex_unlock(&adev->pm.mutex);
 -
 -	if (amdgpu_dpm_dispatch_task(adev,
 -				     AMD_PP_TASK_READJUST_POWER_STATE,
 -				     NULL) == -EOPNOTSUPP) {
 -		adev->pm.dpm.current_ps = adev->pm.dpm.boot_ps;
 -		amdgpu_dpm_compute_clocks(adev);
 +		if (smu_version)
 +			*smu_version = adev->pm.fw_version;
  	}
  
  	return 0;
* Unmerged path drivers/gpu/drm/amd/pm/amdgpu_dpm.c
