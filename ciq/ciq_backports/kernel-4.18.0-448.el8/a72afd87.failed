tlb: mmu_gather: Remove start/end arguments from tlb_gather_mmu()

jira LE-1907
cve CVE-2022-39188
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Will Deacon <will@kernel.org>
commit a72afd873089c697053e9daa85ff343b3140d2e7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/a72afd87.failed

The 'start' and 'end' arguments to tlb_gather_mmu() are no longer
needed now that there is a separate function for 'fullmm' flushing.

Remove the unused arguments and update all callers.

	Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
	Signed-off-by: Will Deacon <will@kernel.org>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Yu Zhao <yuzhao@google.com>
	Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
Link: https://lore.kernel.org/r/CAHk-=wjQWa14_4UpfDf=fiineNP+RH74kZeDMo_f1D35xNzq9w@mail.gmail.com
(cherry picked from commit a72afd873089c697053e9daa85ff343b3140d2e7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/mm_types.h
#	mm/mmu_gather.c
diff --cc include/linux/mm_types.h
index 5c430882072e,0974ad501a47..000000000000
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@@ -629,10 -588,9 +629,16 @@@ static inline cpumask_t *mm_cpumask(str
  }
  
  struct mmu_gather;
++<<<<<<< HEAD
 +extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 +				unsigned long start, unsigned long end);
 +extern void tlb_finish_mmu(struct mmu_gather *tlb,
 +				unsigned long start, unsigned long end);
++=======
+ extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm);
+ extern void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm);
+ extern void tlb_finish_mmu(struct mmu_gather *tlb);
++>>>>>>> a72afd873089 (tlb: mmu_gather: Remove start/end arguments from tlb_gather_mmu())
  
  static inline void init_tlb_flush_pending(struct mm_struct *mm)
  {
diff --cc mm/mmu_gather.c
index b84402038601,0dc7149b0c61..000000000000
--- a/mm/mmu_gather.c
+++ b/mm/mmu_gather.c
@@@ -195,23 -253,19 +195,24 @@@ void tlb_flush_mmu(struct mmu_gather *t
   * tlb_gather_mmu - initialize an mmu_gather structure for page-table tear-down
   * @tlb: the mmu_gather structure to initialize
   * @mm: the mm_struct of the target address space
-  * @start: start of the region that will be removed from the page-table
-  * @end: end of the region that will be removed from the page-table
+  * @fullmm: @mm is without users and we're going to destroy the full address
+  *	    space (exit/execve)
   *
   * Called to initialize an (on-stack) mmu_gather structure for page-table
-  * tear-down from @mm. The @start and @end are set to 0 and -1
-  * respectively when @mm is without users and we're going to destroy
-  * the full address space (exit/execve).
+  * tear-down from @mm.
   */
++<<<<<<< HEAD
 +void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 +			unsigned long start, unsigned long end)
++=======
+ static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
+ 			     bool fullmm)
++>>>>>>> a72afd873089 (tlb: mmu_gather: Remove start/end arguments from tlb_gather_mmu())
  {
  	tlb->mm = mm;
- 
- 	/* Is it from 0 to ~0? */
- 	tlb->fullmm     = !(start | (end+1));
+ 	tlb->fullmm = fullmm;
  
 -#ifndef CONFIG_MMU_GATHER_NO_GATHER
 +#ifndef CONFIG_HAVE_MMU_GATHER_NO_GATHER
  	tlb->need_flush_all = 0;
  	tlb->local.next = NULL;
  	tlb->local.nr   = 0;
@@@ -231,6 -283,16 +232,19 @@@
  	inc_tlb_flush_pending(tlb->mm);
  }
  
++<<<<<<< HEAD
++=======
+ void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm)
+ {
+ 	__tlb_gather_mmu(tlb, mm, false);
+ }
+ 
+ void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)
+ {
+ 	__tlb_gather_mmu(tlb, mm, true);
+ }
+ 
++>>>>>>> a72afd873089 (tlb: mmu_gather: Remove start/end arguments from tlb_gather_mmu())
  /**
   * tlb_finish_mmu - finish an mmu_gather structure
   * @tlb: the mmu_gather structure to finish
diff --git a/arch/ia64/include/asm/tlb.h b/arch/ia64/include/asm/tlb.h
index bf8985f5f876..9efaedbf0097 100644
--- a/arch/ia64/include/asm/tlb.h
+++ b/arch/ia64/include/asm/tlb.h
@@ -23,7 +23,7 @@
  * unmapping a portion of the virtual address space, these hooks are called according to
  * the following template:
  *
- *	tlb <- tlb_gather_mmu(mm, start, end);		// start unmap for address space MM
+ *	tlb <- tlb_gather_mmu(mm);			// start unmap for address space MM
  *	{
  *	  for each vma that needs a shootdown do {
  *	    tlb_start_vma(tlb, vma);
diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c
index 01b612e1e600..2d12e0b259c3 100644
--- a/arch/x86/kernel/ldt.c
+++ b/arch/x86/kernel/ldt.c
@@ -314,7 +314,7 @@ static void free_ldt_pgtables(struct mm_struct *mm)
 	if (!boot_cpu_has(X86_FEATURE_PTI))
 		return;
 
-	tlb_gather_mmu(&tlb, mm, start, end);
+	tlb_gather_mmu(&tlb, mm);
 	free_pgd_range(&tlb, start, end, start, end);
 	tlb_finish_mmu(&tlb, start, end);
 #endif
diff --git a/fs/exec.c b/fs/exec.c
index 0d8591fc3f9f..c38532fba925 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -653,7 +653,7 @@ static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)
 		return -ENOMEM;
 
 	lru_add_drain();
-	tlb_gather_mmu(&tlb, mm, old_start, old_end);
+	tlb_gather_mmu(&tlb, mm);
 	if (new_end > old_start) {
 		/*
 		 * when the old and new regions overlap clear from new_end.
* Unmerged path include/linux/mm_types.h
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index e6275b8d1c47..1725453eebe8 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -4075,23 +4075,9 @@ void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 void unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,
 			  unsigned long end, struct page *ref_page)
 {
-	struct mm_struct *mm;
 	struct mmu_gather tlb;
-	unsigned long tlb_start = start;
-	unsigned long tlb_end = end;
 
-	/*
-	 * If shared PMDs were possibly used within this vma range, adjust
-	 * start/end for worst case tlb flushing.
-	 * Note that we can not be sure if PMDs are shared until we try to
-	 * unmap pages.  However, we want to make sure TLB flushing covers
-	 * the largest possible range.
-	 */
-	adjust_range_if_pmd_sharing_possible(vma, &tlb_start, &tlb_end);
-
-	mm = vma->vm_mm;
-
-	tlb_gather_mmu(&tlb, mm, tlb_start, tlb_end);
+	tlb_gather_mmu(&tlb, vma->vm_mm);
 	__unmap_hugepage_range(&tlb, vma, start, end, ref_page);
 	tlb_finish_mmu(&tlb, tlb_start, tlb_end);
 }
diff --git a/mm/madvise.c b/mm/madvise.c
index 1186d88f8308..daa765ebe62b 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -522,7 +522,7 @@ static long madvise_cold(struct vm_area_struct *vma,
 		return -EINVAL;
 
 	lru_add_drain();
-	tlb_gather_mmu(&tlb, mm, start_addr, end_addr);
+	tlb_gather_mmu(&tlb, mm);
 	madvise_cold_page_range(&tlb, vma, start_addr, end_addr);
 	tlb_finish_mmu(&tlb, start_addr, end_addr);
 
@@ -574,7 +574,7 @@ static long madvise_pageout(struct vm_area_struct *vma,
 		return 0;
 
 	lru_add_drain();
-	tlb_gather_mmu(&tlb, mm, start_addr, end_addr);
+	tlb_gather_mmu(&tlb, mm);
 	madvise_pageout_page_range(&tlb, vma, start_addr, end_addr);
 	tlb_finish_mmu(&tlb, start_addr, end_addr);
 
@@ -739,7 +739,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,
 				range.start, range.end);
 
 	lru_add_drain();
-	tlb_gather_mmu(&tlb, mm, range.start, range.end);
+	tlb_gather_mmu(&tlb, mm);
 	update_hiwater_rss(mm);
 
 	mmu_notifier_invalidate_range_start(&range);
diff --git a/mm/memory.c b/mm/memory.c
index d3790173a23c..f5b35b7e2130 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1501,7 +1501,7 @@ void zap_page_range(struct vm_area_struct *vma, unsigned long start,
 	lru_add_drain();
 	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
 				start, start + size);
-	tlb_gather_mmu(&tlb, vma->vm_mm, start, range.end);
+	tlb_gather_mmu(&tlb, vma->vm_mm);
 	update_hiwater_rss(vma->vm_mm);
 	mmu_notifier_invalidate_range_start(&range);
 	for ( ; vma && vma->vm_start < range.end; vma = vma->vm_next)
@@ -1528,7 +1528,7 @@ static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr
 	lru_add_drain();
 	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
 				address, address + size);
-	tlb_gather_mmu(&tlb, vma->vm_mm, address, range.end);
+	tlb_gather_mmu(&tlb, vma->vm_mm);
 	update_hiwater_rss(vma->vm_mm);
 	mmu_notifier_invalidate_range_start(&range);
 	unmap_single_vma(&tlb, vma, address, range.end, details);
diff --git a/mm/mmap.c b/mm/mmap.c
index 878265a8a575..abddc2cd2c2e 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -2597,7 +2597,7 @@ static void unmap_region(struct mm_struct *mm,
 	struct mmu_gather tlb;
 
 	lru_add_drain();
-	tlb_gather_mmu(&tlb, mm, start, end);
+	tlb_gather_mmu(&tlb, mm);
 	update_hiwater_rss(mm);
 	unmap_vmas(&tlb, vma, start, end);
 	free_pgtables(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,
* Unmerged path mm/mmu_gather.c
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index f47de78a8f1c..df5af83619c0 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -535,7 +535,7 @@ bool __oom_reap_task_mm(struct mm_struct *mm)
 			mmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0,
 						vma, mm, vma->vm_start,
 						vma->vm_end);
-			tlb_gather_mmu(&tlb, mm, range.start, range.end);
+			tlb_gather_mmu(&tlb, mm);
 			if (mmu_notifier_invalidate_range_start_nonblock(&range)) {
 				ret = false;
 				continue;
