mm: remove the extra ZONE_DEVICE struct page refcount

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
Rebuild_CHGLOG: - Revert "mm: remove the extra ZONE_DEVICE struct page refcount" (Michel DÃ¤nzer) [2041811]
Rebuild_FUZZ: 92.17%
commit-author Christoph Hellwig <hch@lst.de>
commit 27674ef6c73f0c9096a9827dc5d6ba9fc7808422
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/27674ef6.failed

ZONE_DEVICE struct pages have an extra reference count that complicates
the code for put_page() and several places in the kernel that need to
check the reference count to see that a page is not being used (gup,
compaction, migration, etc.). Clean up the code so the reference count
doesn't need to be treated specially for ZONE_DEVICE pages.

Note that this excludes the special idle page wakeup for fsdax pages,
which still happens at refcount 1.  This is a separate issue and will
be sorted out later.  Given that only fsdax pages require the
notifiacation when the refcount hits 1 now, the PAGEMAP_OPS Kconfig
symbol can go away and be replaced with a FS_DAX check for this hook
in the put_page fastpath.

Based on an earlier patch from Ralph Campbell <rcampbell@nvidia.com>.

Link: https://lkml.kernel.org/r/20220210072828.2930359-8-hch@lst.de
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
	Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
	Reviewed-by: Dan Williams <dan.j.williams@intel.com>
	Acked-by: Felix Kuehling <Felix.Kuehling@amd.com>
	Tested-by: "Sierra Guiza, Alejandro (Alex)" <alex.sierra@amd.com>

	Cc: Alex Deucher <alexander.deucher@amd.com>
	Cc: Alistair Popple <apopple@nvidia.com>
	Cc: Ben Skeggs <bskeggs@redhat.com>
	Cc: Chaitanya Kulkarni <kch@nvidia.com>
	Cc: Christian Knig <christian.koenig@amd.com>
	Cc: Karol Herbst <kherbst@redhat.com>
	Cc: Lyude Paul <lyude@redhat.com>
	Cc: Miaohe Lin <linmiaohe@huawei.com>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Cc: "Pan, Xinhui" <Xinhui.Pan@amd.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
(cherry picked from commit 27674ef6c73f0c9096a9827dc5d6ba9fc7808422)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memremap.h
#	include/linux/mm.h
#	mm/memremap.c
#	mm/migrate.c
diff --cc include/linux/memremap.h
index 34d0e54d38c4,d6a114dd5ea8..000000000000
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@@ -67,25 -61,16 +67,25 @@@ struct vmem_altmap 
  enum memory_type {
  	/* 0 is reserved to catch uninitialized type fields */
  	MEMORY_DEVICE_PRIVATE = 1,
 +	MEMORY_DEVICE_PUBLIC,
  	MEMORY_DEVICE_FS_DAX,
 -	MEMORY_DEVICE_GENERIC,
  	MEMORY_DEVICE_PCI_P2PDMA,
 +#ifndef __GENKSYMS__
 +	MEMORY_DEVICE_GENERIC,
 +#endif /* __GENKSYMS__ */
  };
  
 +typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,
 +				unsigned long addr,
 +				const struct page *page,
 +				unsigned int flags,
 +				pmd_t *pmdp);
 +typedef void (*dev_page_free_t)(struct page *page, void *data);
  struct dev_pagemap_ops {
  	/*
- 	 * Called once the page refcount reaches 1.  (ZONE_DEVICE pages never
- 	 * reach 0 refcount unless there is a refcount bug. This allows the
- 	 * device driver to implement its own memory management.)
+ 	 * Called once the page refcount reaches 0.  The reference count will be
+ 	 * reset to one by the core code after the method is called to prepare
+ 	 * for handing out the page again.
  	 */
  	void (*page_free)(struct page *page);
  
@@@ -156,6 -126,25 +156,28 @@@ static inline struct vmem_altmap *pgmap
  	return NULL;
  }
  
++<<<<<<< HEAD
++=======
+ static inline unsigned long pgmap_vmemmap_nr(struct dev_pagemap *pgmap)
+ {
+ 	return 1 << pgmap->vmemmap_shift;
+ }
+ 
+ static inline bool is_device_private_page(const struct page *page)
+ {
+ 	return IS_ENABLED(CONFIG_DEVICE_PRIVATE) &&
+ 		is_zone_device_page(page) &&
+ 		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
+ }
+ 
+ static inline bool is_pci_p2pdma_page(const struct page *page)
+ {
+ 	return IS_ENABLED(CONFIG_PCI_P2PDMA) &&
+ 		is_zone_device_page(page) &&
+ 		page->pgmap->type == MEMORY_DEVICE_PCI_P2PDMA;
+ }
+ 
++>>>>>>> 27674ef6c73f (mm: remove the extra ZONE_DEVICE struct page refcount)
  #ifdef CONFIG_ZONE_DEVICE
  void *memremap_pages(struct dev_pagemap *pgmap, int nid);
  void memunmap_pages(struct dev_pagemap *pgmap);
diff --cc include/linux/mm.h
index e3b1a16638a0,0201d258c646..000000000000
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@@ -1003,71 -1085,48 +1003,85 @@@ static inline bool is_zone_device_page(
  }
  #endif
  
++<<<<<<< HEAD
 +#ifdef CONFIG_DEV_PAGEMAP_OPS
 +void free_devmap_managed_page(struct page *page);
++=======
+ static inline bool is_zone_movable_page(const struct page *page)
+ {
+ 	return page_zonenum(page) == ZONE_MOVABLE;
+ }
+ 
+ #if defined(CONFIG_ZONE_DEVICE) && defined(CONFIG_FS_DAX)
++>>>>>>> 27674ef6c73f (mm: remove the extra ZONE_DEVICE struct page refcount)
  DECLARE_STATIC_KEY_FALSE(devmap_managed_key);
  
 -bool __put_devmap_managed_page(struct page *page);
 -static inline bool put_devmap_managed_page(struct page *page)
 +static inline bool page_is_devmap_managed(struct page *page)
  {
  	if (!static_branch_unlikely(&devmap_managed_key))
  		return false;
  	if (!is_zone_device_page(page))
  		return false;
 -	return __put_devmap_managed_page(page);
 +	switch (page->pgmap->type) {
 +	case MEMORY_DEVICE_PRIVATE:
 +	case MEMORY_DEVICE_FS_DAX:
 +		return true;
 +	default:
 +		break;
 +	}
 +	return false;
 +}
 +
++<<<<<<< HEAD
 +static inline bool is_device_private_page(const struct page *page)
 +{
 +	return is_zone_device_page(page) &&
 +		page->pgmap->type == MEMORY_DEVICE_PRIVATE;
 +}
 +
 +#ifdef CONFIG_PCI_P2PDMA
 +static inline bool is_pci_p2pdma_page(const struct page *page)
 +{
 +	return is_zone_device_page(page) &&
 +		page->pgmap->type == MEMORY_DEVICE_PCI_P2PDMA;
 +}
 +#else /* CONFIG_PCI_P2PDMA */
 +static inline bool is_pci_p2pdma_page(const struct page *page)
 +{
 +	return false;
 +}
 +#endif /* CONFIG_PCI_P2PDMA */
 +
 +void put_devmap_managed_page(struct page *page);
 +
 +#else /* CONFIG_DEV_PAGEMAP_OPS */
 +static inline bool page_is_devmap_managed(struct page *page)
 +{
 +	return false;
  }
  
 +static inline void put_devmap_managed_page(struct page *page)
 +{
 +}
 +
 +static inline bool is_device_private_page(const struct page *page)
 +{
 +	return false;
 +}
 +
 +static inline bool is_pci_p2pdma_page(const struct page *page)
++=======
+ #else /* CONFIG_ZONE_DEVICE && CONFIG_FS_DAX */
+ static inline bool put_devmap_managed_page(struct page *page)
++>>>>>>> 27674ef6c73f (mm: remove the extra ZONE_DEVICE struct page refcount)
  {
  	return false;
  }
- #endif /* CONFIG_DEV_PAGEMAP_OPS */
+ #endif /* CONFIG_ZONE_DEVICE && CONFIG_FS_DAX */
  
  /* 127: arbitrary random number, small enough to assemble well */
 -#define folio_ref_zero_or_close_to_overflow(folio) \
 -	((unsigned int) folio_ref_count(folio) + 127u <= 127u)
 -
 -/**
 - * folio_get - Increment the reference count on a folio.
 - * @folio: The folio.
 - *
 - * Context: May be called in any context, as long as you know that
 - * you have a refcount on the folio.  If you do not already have one,
 - * folio_try_get() may be the right interface for you to use.
 - */
 -static inline void folio_get(struct folio *folio)
 -{
 -	VM_BUG_ON_FOLIO(folio_ref_zero_or_close_to_overflow(folio), folio);
 -	folio_ref_inc(folio);
 -}
 +#define page_ref_zero_or_close_to_overflow(page) \
 +	((unsigned int) page_ref_count(page) + 127u <= 127u)
  
  static inline void get_page(struct page *page)
  {
diff --cc mm/memremap.c
index 2455bac89506,fef5734d5e49..000000000000
--- a/mm/memremap.c
+++ b/mm/memremap.c
@@@ -102,40 -101,12 +101,49 @@@ static unsigned long pfn_end(struct dev
  	return (range->start + range_len(range)) >> PAGE_SHIFT;
  }
  
++<<<<<<< HEAD
 +static unsigned long pfn_next(unsigned long pfn)
 +{
 +	if (pfn % 1024 == 0)
 +		cond_resched();
 +	return pfn + 1;
 +}
 +
 +#define for_each_device_pfn(pfn, map, i) \
 +	for (pfn = pfn_first(map, i); pfn < pfn_end(map, i); pfn = pfn_next(pfn))
 +
 +static void dev_pagemap_kill(struct dev_pagemap *pgmap)
 +{
 +	if (pgmap->ops && pgmap->ops->kill)
 +		pgmap->ops->kill(pgmap);
 +	else
 +		percpu_ref_kill(pgmap->ref);
 +}
 +
 +static void dev_pagemap_cleanup(struct dev_pagemap *pgmap)
 +{
 +	if (pgmap->ops && pgmap->ops->cleanup) {
 +		pgmap->ops->cleanup(pgmap);
 +	} else {
 +		wait_for_completion(&pgmap->done);
 +		percpu_ref_exit(pgmap->ref);
 +	}
 +	/*
 +	 * Undo the pgmap ref assignment for the internal case as the
 +	 * caller may re-enable the same pgmap.
 +	 */
 +	if (pgmap->ref == &pgmap->internal_ref)
 +		pgmap->ref = NULL;
 +}
 +
++=======
+ static unsigned long pfn_len(struct dev_pagemap *pgmap, unsigned long range_id)
+ {
+ 	return (pfn_end(pgmap, range_id) -
+ 		pfn_first(pgmap, range_id)) >> pgmap->vmemmap_shift;
+ }
+ 
++>>>>>>> 27674ef6c73f (mm: remove the extra ZONE_DEVICE struct page refcount)
  static void pageunmap_range(struct dev_pagemap *pgmap, int range_id)
  {
  	struct range *range = &pgmap->ranges[range_id];
@@@ -167,14 -135,13 +175,19 @@@
  
  void memunmap_pages(struct dev_pagemap *pgmap)
  {
- 	unsigned long pfn;
  	int i;
  
 -	percpu_ref_kill(&pgmap->ref);
 +	dev_pagemap_kill(pgmap);
  	for (i = 0; i < pgmap->nr_range; i++)
++<<<<<<< HEAD
 +		for_each_device_pfn(pfn, pgmap, i)
 +			put_page(pfn_to_page(pfn));
 +	dev_pagemap_cleanup(pgmap);
++=======
+ 		percpu_ref_put_many(&pgmap->ref, pfn_len(pgmap, i));
+ 	wait_for_completion(&pgmap->done);
+ 	percpu_ref_exit(&pgmap->ref);
++>>>>>>> 27674ef6c73f (mm: remove the extra ZONE_DEVICE struct page refcount)
  
  	for (i = 0; i < pgmap->nr_range; i++)
  		pageunmap_range(pgmap, i);
@@@ -527,5 -482,27 +536,31 @@@ void free_zone_device_page(struct page 
  	 */
  	page->mapping = NULL;
  	page->pgmap->ops->page_free(page);
+ 
+ 	/*
+ 	 * Reset the page count to 1 to prepare for handing out the page again.
+ 	 */
+ 	set_page_count(page, 1);
  }
++<<<<<<< HEAD
 +#endif /* CONFIG_DEV_PAGEMAP_OPS */
++=======
+ 
+ #ifdef CONFIG_FS_DAX
+ bool __put_devmap_managed_page(struct page *page)
+ {
+ 	if (page->pgmap->type != MEMORY_DEVICE_FS_DAX)
+ 		return false;
+ 
+ 	/*
+ 	 * fsdax page refcounts are 1-based, rather than 0-based: if
+ 	 * refcount is 1, then the page is free and the refcount is
+ 	 * stable because nobody holds a reference on the page.
+ 	 */
+ 	if (page_ref_dec_return(page) == 1)
+ 		wake_up_var(&page->_refcount);
+ 	return true;
+ }
+ EXPORT_SYMBOL(__put_devmap_managed_page);
+ #endif /* CONFIG_FS_DAX */
++>>>>>>> 27674ef6c73f (mm: remove the extra ZONE_DEVICE struct page refcount)
diff --cc mm/migrate.c
index 635206a18d08,af0534de618a..000000000000
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@@ -381,14 -338,8 +381,19 @@@ static int expected_page_refs(struct ad
  {
  	int expected_count = 1;
  
++<<<<<<< HEAD
 +	/*
 +	 * Device public or private pages have an extra refcount as they are
 +	 * ZONE_DEVICE pages.
 +	 */
 +	expected_count += is_device_private_page(page);
 +	if (mapping)
 +		expected_count += thp_nr_pages(page) + page_has_private(page);
 +
++=======
+ 	if (mapping)
+ 		expected_count += compound_nr(page) + page_has_private(page);
++>>>>>>> 27674ef6c73f (mm: remove the extra ZONE_DEVICE struct page refcount)
  	return expected_count;
  }
  
diff --git a/arch/powerpc/kvm/book3s_hv_uvmem.c b/arch/powerpc/kvm/book3s_hv_uvmem.c
index e371ee1644c1..695c4998ea81 100644
--- a/arch/powerpc/kvm/book3s_hv_uvmem.c
+++ b/arch/powerpc/kvm/book3s_hv_uvmem.c
@@ -711,7 +711,6 @@ static struct page *kvmppc_uvmem_get_page(unsigned long gpa, struct kvm *kvm)
 
 	dpage = pfn_to_page(uvmem_pfn);
 	dpage->zone_device_data = pvt;
-	get_page(dpage);
 	lock_page(dpage);
 	return dpage;
 out_clear:
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_migrate.c b/drivers/gpu/drm/amd/amdkfd/kfd_migrate.c
index 4a16e3c257b9..cacf2ea12242 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_migrate.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_migrate.c
@@ -220,7 +220,6 @@ svm_migrate_get_vram_page(struct svm_range *prange, unsigned long pfn)
 	page = pfn_to_page(pfn);
 	svm_range_bo_ref(prange->svm_bo);
 	page->zone_device_data = prange->svm_bo;
-	get_page(page);
 	lock_page(page);
 }
 
diff --git a/drivers/gpu/drm/nouveau/nouveau_dmem.c b/drivers/gpu/drm/nouveau/nouveau_dmem.c
index 92987daa5e17..4a98f997a565 100644
--- a/drivers/gpu/drm/nouveau/nouveau_dmem.c
+++ b/drivers/gpu/drm/nouveau/nouveau_dmem.c
@@ -324,7 +324,6 @@ nouveau_dmem_page_alloc_locked(struct nouveau_drm *drm)
 			return NULL;
 	}
 
-	get_page(page);
 	lock_page(page);
 	return page;
 }
diff --git a/fs/Kconfig b/fs/Kconfig
index b2698a30593e..eda34a3a72bd 100644
--- a/fs/Kconfig
+++ b/fs/Kconfig
@@ -44,7 +44,6 @@ config FS_DAX
 	bool "Direct Access (DAX) support"
 	depends on MMU
 	depends on !(ARM || MIPS || SPARC)
-	select DEV_PAGEMAP_OPS if (ZONE_DEVICE && !FS_DAX_LIMITED)
 	select FS_IOMAP
 	select DAX
 	help
* Unmerged path include/linux/memremap.h
* Unmerged path include/linux/mm.h
diff --git a/lib/test_hmm.c b/lib/test_hmm.c
index 0996246ccb7b..5cdb09d750b3 100644
--- a/lib/test_hmm.c
+++ b/lib/test_hmm.c
@@ -561,7 +561,6 @@ static struct page *dmirror_devmem_alloc_page(struct dmirror_device *mdevice)
 	}
 
 	dpage->zone_device_data = rpage;
-	get_page(dpage);
 	lock_page(dpage);
 	return dpage;
 
diff --git a/mm/Kconfig b/mm/Kconfig
index 2665edbbccf1..e9de4e0be01a 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -681,9 +681,6 @@ config ZONE_DEVICE
 
 	  If FS_DAX is enabled, then say Y.
 
-config DEV_PAGEMAP_OPS
-	bool
-
 #
 # Helpers to mirror range of the CPU page tables of a process into device page
 # tables.
@@ -696,7 +693,6 @@ config HMM_MIRROR
 config DEVICE_PRIVATE
 	bool "Unaddressable device memory (GPU memory, ...)"
 	depends on ZONE_DEVICE
-	select DEV_PAGEMAP_OPS
 
 	help
 	  Allows creation of struct pages to represent unaddressable device
diff --git a/mm/internal.h b/mm/internal.h
index 544ab31e6706..7c66b7f33b58 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -650,4 +650,6 @@ struct migration_target_control {
 
 DECLARE_PER_CPU(struct per_cpu_nodestat, boot_nodestats);
 
+void free_zone_device_page(struct page *page);
+
 #endif	/* __MM_INTERNAL_H */
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 2c5c2381d3ee..26dbe7d64f4d 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -5598,17 +5598,12 @@ static struct page *mc_handle_swap_pte(struct vm_area_struct *vma,
 		return NULL;
 
 	/*
-	 * Handle MEMORY_DEVICE_PRIVATE which are ZONE_DEVICE page belonging to
-	 * a device and because they are not accessible by CPU they are store
-	 * as special swap entry in the CPU page table.
+	 * Handle device private pages that are not accessible by the CPU, but
+	 * stored as special swap entries in the page table.
 	 */
 	if (is_device_private_entry(ent)) {
 		page = pfn_swap_entry_to_page(ent);
-		/*
-		 * MEMORY_DEVICE_PRIVATE means ZONE_DEVICE page and which have
-		 * a refcount of 1 when free (unlike normal page)
-		 */
-		if (!page_ref_add_unless(page, 1, 1))
+		if (!get_page_unless_zero(page))
 			return NULL;
 		return page;
 	}
* Unmerged path mm/memremap.c
* Unmerged path mm/migrate.c
diff --git a/mm/swap.c b/mm/swap.c
index c9fe61e3a5e4..68061925febc 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -111,17 +111,9 @@ static void __put_compound_page(struct page *page)
 
 void __put_page(struct page *page)
 {
-	if (is_zone_device_page(page)) {
-		put_dev_pagemap(page->pgmap);
-
-		/*
-		 * The page belongs to the device that created pgmap. Do
-		 * not return it to page allocator.
-		 */
-		return;
-	}
-
-	if (unlikely(PageCompound(page)))
+	if (unlikely(is_zone_device_page(page)))
+		free_zone_device_page(page);
+	else if (unlikely(PageCompound(page)))
 		__put_compound_page(page);
 	else
 		__put_single_page(page);
@@ -823,7 +815,7 @@ void release_pages(struct page **pages, int nr)
 				continue;
 			}
 			if (put_page_testzero(page))
-				put_dev_pagemap(page->pgmap);
+				free_zone_device_page(page);
 			continue;
 		}
 
