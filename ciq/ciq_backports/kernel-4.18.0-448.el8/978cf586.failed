drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
Rebuild_CHGLOG: - Revert "drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev" (Jocelyn Falempe) [2115880]
Rebuild_FUZZ: 93.13%
commit-author Christoph Hellwig <hch@lst.de>
commit 978cf586ac35f34604e2d252a51b71192c39f1e4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/978cf586.failed

This is straightforward conversion, the intel_vgpu already has a pointer
to the vfio_dev, which can be replaced with the embedded structure and
we can replace all the mdev_get_drvdata() with a simple container_of().

Based on an patch from Jason Gunthorpe.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
Link: http://patchwork.freedesktop.org/patch/msgid/20220411141403.86980-29-hch@lst.de
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
	Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
(cherry picked from commit 978cf586ac35f34604e2d252a51b71192c39f1e4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/gvt/gvt.h
#	drivers/gpu/drm/i915/gvt/kvmgt.c
diff --cc drivers/gpu/drm/i915/gvt/gvt.h
index 0c0615602343,c32e8eb199f1..000000000000
--- a/drivers/gpu/drm/i915/gvt/gvt.h
+++ b/drivers/gpu/drm/i915/gvt/gvt.h
@@@ -218,13 -210,35 +218,43 @@@ struct intel_vgpu 
  	struct intel_vgpu_vblank_timer vblank_timer;
  
  	u32 scan_nonprivbb;
++<<<<<<< HEAD
++=======
+ 
+ 	struct vfio_device vfio_device;
+ 	struct vfio_region *region;
+ 	int num_regions;
+ 	struct eventfd_ctx *intx_trigger;
+ 	struct eventfd_ctx *msi_trigger;
+ 
+ 	/*
+ 	 * Two caches are used to avoid mapping duplicated pages (eg.
+ 	 * scratch pages). This help to reduce dma setup overhead.
+ 	 */
+ 	struct rb_root gfn_cache;
+ 	struct rb_root dma_addr_cache;
+ 	unsigned long nr_cache_entries;
+ 	struct mutex cache_lock;
+ 
+ 	struct notifier_block iommu_notifier;
+ 	struct notifier_block group_notifier;
+ 	struct kvm *kvm;
+ 	struct work_struct release_work;
+ 	atomic_t released;
+ 	struct vfio_group *vfio_group;
+ 
+ 	struct kvm_page_track_notifier_node track_node;
+ #define NR_BKT (1 << 18)
+ 	struct hlist_head ptable[NR_BKT];
+ #undef NR_BKT
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  };
  
 +static inline void *intel_vgpu_vdev(struct intel_vgpu *vgpu)
 +{
 +	return vgpu->vdev;
 +}
 +
  /* validating GM healthy status*/
  #define vgpu_is_vm_unhealthy(ret_val) \
  	(((ret_val) == -EBADRQC) || ((ret_val) == -EFAULT))
diff --cc drivers/gpu/drm/i915/gvt/kvmgt.c
index 5bd0c74fc4ad,4113f850bf92..000000000000
--- a/drivers/gpu/drm/i915/gvt/kvmgt.c
+++ b/drivers/gpu/drm/i915/gvt/kvmgt.c
@@@ -108,55 -100,25 +108,67 @@@ struct gvt_dma 
  	struct kref ref;
  };
  
++<<<<<<< HEAD
 +struct kvmgt_vdev {
 +	struct intel_vgpu *vgpu;
 +	struct mdev_device *mdev;
 +	struct vfio_region *region;
 +	int num_regions;
 +	struct eventfd_ctx *intx_trigger;
 +	struct eventfd_ctx *msi_trigger;
++=======
+ #define vfio_dev_to_vgpu(vfio_dev) \
+ 	container_of((vfio_dev), struct intel_vgpu, vfio_device)
+ 
+ static void kvmgt_page_track_write(struct kvm_vcpu *vcpu, gpa_t gpa,
+ 		const u8 *val, int len,
+ 		struct kvm_page_track_notifier_node *node);
+ static void kvmgt_page_track_flush_slot(struct kvm *kvm,
+ 		struct kvm_memory_slot *slot,
+ 		struct kvm_page_track_notifier_node *node);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
 +
 +	/*
 +	 * Two caches are used to avoid mapping duplicated pages (eg.
 +	 * scratch pages). This help to reduce dma setup overhead.
 +	 */
 +	struct rb_root gfn_cache;
 +	struct rb_root dma_addr_cache;
 +	unsigned long nr_cache_entries;
 +	struct mutex cache_lock;
 +
 +	struct notifier_block iommu_notifier;
 +	struct notifier_block group_notifier;
 +	struct kvm *kvm;
 +	struct work_struct release_work;
 +	atomic_t released;
 +	struct vfio_device *vfio_device;
 +	struct vfio_group *vfio_group;
 +};
 +
 +static inline struct kvmgt_vdev *kvmgt_vdev(struct intel_vgpu *vgpu)
 +{
 +	return intel_vgpu_vdev(vgpu);
 +}
 +
 +static inline bool handle_valid(unsigned long handle)
 +{
 +	return !!(handle & ~0xff);
 +}
  
 -static ssize_t available_instances_show(struct mdev_type *mtype,
 -					struct mdev_type_attribute *attr,
 +static ssize_t available_instances_show(struct kobject *kobj,
 +					struct device *dev,
  					char *buf)
  {
 -	struct intel_vgpu_type *type;
  	unsigned int num = 0;
 -	struct intel_gvt *gvt = kdev_to_i915(mtype_get_parent_dev(mtype))->gvt;
 +	struct intel_vgpu_type *type;
 +	struct intel_gvt *gvt = kdev_to_i915(dev)->gvt;
 +	int id = mdev_type_kobj_to_group_id(kobj);
  
 -	type = &gvt->types[mtype_get_type_group_id(mtype)];
 +	if (id < 0 || id >= NR_MAX_INTEL_VGPU_TYPES)
 +		return 0;
 +
 +	type = &gvt->types[id];
  	if (!type)
  		num = 0;
  	else
@@@ -787,70 -726,6 +799,73 @@@ static int kvmgt_set_edid(void *p_vgpu
  	return ret;
  }
  
++<<<<<<< HEAD
 +static void kvmgt_put_vfio_device(void *vgpu)
 +{
 +	struct kvmgt_vdev *vdev = kvmgt_vdev((struct intel_vgpu *)vgpu);
 +
 +	if (WARN_ON(!vdev->vfio_device))
 +		return;
 +
 +	vfio_device_put(vdev->vfio_device);
 +}
 +
 +static int intel_vgpu_create(struct kobject *kobj, struct mdev_device *mdev)
 +{
 +	struct intel_vgpu *vgpu = NULL;
 +	struct intel_vgpu_type *type;
 +	struct device *pdev;
 +	struct intel_gvt *gvt;
 +	int ret, id;
 +
 +	pdev = mdev_parent_dev(mdev);
 +	gvt = kdev_to_i915(pdev)->gvt;
 +
 +	id = mdev_type_kobj_to_group_id(kobj);
 +	if (id < 0 || id >= NR_MAX_INTEL_VGPU_TYPES) {
 +		ret = -EINVAL;
 +		goto out;
 +	}
 +
 +	type = &gvt->types[id];
 +	if (!type) {
 +		ret = -EINVAL;
 +		goto out;
 +	}
 +
 +	vgpu = intel_gvt_ops->vgpu_create(gvt, type);
 +	if (IS_ERR_OR_NULL(vgpu)) {
 +		ret = vgpu == NULL ? -EFAULT : PTR_ERR(vgpu);
 +		gvt_err("failed to create intel vgpu: %d\n", ret);
 +		goto out;
 +	}
 +
 +	INIT_WORK(&kvmgt_vdev(vgpu)->release_work, intel_vgpu_release_work);
 +
 +	kvmgt_vdev(vgpu)->mdev = mdev;
 +	mdev_set_drvdata(mdev, vgpu);
 +
 +	gvt_dbg_core("intel_vgpu_create succeeded for mdev: %s\n",
 +		     dev_name(mdev_dev(mdev)));
 +	ret = 0;
 +
 +out:
 +	return ret;
 +}
 +
 +static int intel_vgpu_remove(struct mdev_device *mdev)
 +{
 +	struct intel_vgpu *vgpu = mdev_get_drvdata(mdev);
 +
 +	if (handle_valid(vgpu->handle))
 +		return -EBUSY;
 +
 +	intel_gvt_ops->vgpu_destroy(vgpu);
 +	return 0;
 +}
 +
++=======
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  static int intel_vgpu_iommu_notifier(struct notifier_block *nb,
  				     unsigned long action, void *data)
  {
@@@ -901,20 -773,40 +916,52 @@@ static int intel_vgpu_group_notifier(st
  	return NOTIFY_OK;
  }
  
++<<<<<<< HEAD
 +static int intel_vgpu_open(struct mdev_device *mdev)
 +{
 +	struct intel_vgpu *vgpu = mdev_get_drvdata(mdev);
 +	struct kvmgt_vdev *vdev = kvmgt_vdev(vgpu);
++=======
+ static bool __kvmgt_vgpu_exist(struct intel_vgpu *vgpu)
+ {
+ 	struct intel_vgpu *itr;
+ 	int id;
+ 	bool ret = false;
+ 
+ 	mutex_lock(&vgpu->gvt->lock);
+ 	for_each_active_vgpu(vgpu->gvt, itr, id) {
+ 		if (!itr->attached)
+ 			continue;
+ 
+ 		if (vgpu->kvm == itr->kvm) {
+ 			ret = true;
+ 			goto out;
+ 		}
+ 	}
+ out:
+ 	mutex_unlock(&vgpu->gvt->lock);
+ 	return ret;
+ }
+ 
+ static int intel_vgpu_open_device(struct vfio_device *vfio_dev)
+ {
+ 	struct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  	unsigned long events;
  	int ret;
  	struct vfio_group *vfio_group;
  
 -	vgpu->iommu_notifier.notifier_call = intel_vgpu_iommu_notifier;
 -	vgpu->group_notifier.notifier_call = intel_vgpu_group_notifier;
 +	vdev->iommu_notifier.notifier_call = intel_vgpu_iommu_notifier;
 +	vdev->group_notifier.notifier_call = intel_vgpu_group_notifier;
  
  	events = VFIO_IOMMU_NOTIFY_DMA_UNMAP;
++<<<<<<< HEAD
 +	ret = vfio_register_notifier(mdev_dev(mdev), VFIO_IOMMU_NOTIFY, &events,
 +				&vdev->iommu_notifier);
++=======
+ 	ret = vfio_register_notifier(vfio_dev->dev, VFIO_IOMMU_NOTIFY, &events,
+ 				&vgpu->iommu_notifier);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  	if (ret != 0) {
  		gvt_vgpu_err("vfio_register_notifier for iommu failed: %d\n",
  			ret);
@@@ -922,8 -814,8 +969,13 @@@
  	}
  
  	events = VFIO_GROUP_NOTIFY_SET_KVM;
++<<<<<<< HEAD
 +	ret = vfio_register_notifier(mdev_dev(mdev), VFIO_GROUP_NOTIFY, &events,
 +				&vdev->group_notifier);
++=======
+ 	ret = vfio_register_notifier(vfio_dev->dev, VFIO_GROUP_NOTIFY, &events,
+ 				&vgpu->group_notifier);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  	if (ret != 0) {
  		gvt_vgpu_err("vfio_register_notifier for group failed: %d\n",
  			ret);
@@@ -936,36 -829,51 +989,50 @@@
  		gvt_vgpu_err("vfio_group_get_external_user_from_dev failed\n");
  		goto undo_register;
  	}
 -	vgpu->vfio_group = vfio_group;
 -
 -	ret = -EEXIST;
 -	if (vgpu->attached)
 -		goto undo_group;
 +	vdev->vfio_group = vfio_group;
  
 -	ret = -ESRCH;
 -	if (!vgpu->kvm || vgpu->kvm->mm != current->mm) {
 -		gvt_vgpu_err("KVM is required to use Intel vGPU\n");
++<<<<<<< HEAD
 +	/* Take a module reference as mdev core doesn't take
 +	 * a reference for vendor driver.
 +	 */
 +	if (!try_module_get(THIS_MODULE)) {
 +		ret = -ENODEV;
  		goto undo_group;
  	}
  
 +	ret = kvmgt_guest_init(mdev);
 +	if (ret)
++=======
+ 	ret = -EEXIST;
 -	if (__kvmgt_vgpu_exist(vgpu))
++	if (vgpu->attached)
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  		goto undo_group;
  
 -	vgpu->attached = true;
 -	kvm_get_kvm(vgpu->kvm);
 -
 -	kvmgt_protect_table_init(vgpu);
 -	gvt_cache_init(vgpu);
 -
 -	vgpu->track_node.track_write = kvmgt_page_track_write;
 -	vgpu->track_node.track_flush_slot = kvmgt_page_track_flush_slot;
 -	kvm_page_track_register_notifier(vgpu->kvm, &vgpu->track_node);
 -
 -	debugfs_create_ulong(KVMGT_DEBUGFS_FILENAME, 0444, vgpu->debugfs,
 -			     &vgpu->nr_cache_entries);
 +	intel_gvt_ops->vgpu_activate(vgpu);
  
 -	intel_gvt_activate_vgpu(vgpu);
 -
 -	atomic_set(&vgpu->released, 0);
 -	return 0;
 +	atomic_set(&vdev->released, 0);
 +	return ret;
  
  undo_group:
 -	vfio_group_put_external_user(vgpu->vfio_group);
 -	vgpu->vfio_group = NULL;
 +	vfio_group_put_external_user(vdev->vfio_group);
 +	vdev->vfio_group = NULL;
  
  undo_register:
++<<<<<<< HEAD
 +	vfio_unregister_notifier(mdev_dev(mdev), VFIO_GROUP_NOTIFY,
 +					&vdev->group_notifier);
 +
 +undo_iommu:
 +	vfio_unregister_notifier(mdev_dev(mdev), VFIO_IOMMU_NOTIFY,
 +					&vdev->iommu_notifier);
++=======
+ 	vfio_unregister_notifier(vfio_dev->dev, VFIO_GROUP_NOTIFY,
+ 					&vgpu->group_notifier);
+ 
+ undo_iommu:
+ 	vfio_unregister_notifier(vfio_dev->dev, VFIO_IOMMU_NOTIFY,
+ 					&vgpu->iommu_notifier);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  out:
  	return ret;
  }
@@@ -984,47 -891,44 +1051,68 @@@ static void intel_vgpu_release_msi_even
  
  static void __intel_vgpu_release(struct intel_vgpu *vgpu)
  {
 +	struct kvmgt_vdev *vdev = kvmgt_vdev(vgpu);
  	struct drm_i915_private *i915 = vgpu->gvt->gt->i915;
 +	struct kvmgt_guest_info *info;
  	int ret;
  
 -	if (!vgpu->attached)
 +	if (!handle_valid(vgpu->handle))
  		return;
  
 -	if (atomic_cmpxchg(&vgpu->released, 0, 1))
 +	if (atomic_cmpxchg(&vdev->released, 0, 1))
  		return;
  
 -	intel_gvt_release_vgpu(vgpu);
 +	intel_gvt_ops->vgpu_release(vgpu);
 +
++<<<<<<< HEAD
 +	ret = vfio_unregister_notifier(mdev_dev(vdev->mdev), VFIO_IOMMU_NOTIFY,
 +					&vdev->iommu_notifier);
 +	drm_WARN(&i915->drm, ret,
 +		 "vfio_unregister_notifier for iommu failed: %d\n", ret);
  
 +	ret = vfio_unregister_notifier(mdev_dev(vdev->mdev), VFIO_GROUP_NOTIFY,
 +					&vdev->group_notifier);
 +	drm_WARN(&i915->drm, ret,
 +		 "vfio_unregister_notifier for group failed: %d\n", ret);
 +
 +	/* dereference module reference taken at open */
 +	module_put(THIS_MODULE);
 +
 +	info = (struct kvmgt_guest_info *)vgpu->handle;
 +	kvmgt_guest_exit(info);
++=======
+ 	ret = vfio_unregister_notifier(vgpu->vfio_device.dev, VFIO_IOMMU_NOTIFY,
+ 					&vgpu->iommu_notifier);
+ 	drm_WARN(&i915->drm, ret,
+ 		 "vfio_unregister_notifier for iommu failed: %d\n", ret);
+ 
+ 	ret = vfio_unregister_notifier(vgpu->vfio_device.dev, VFIO_GROUP_NOTIFY,
+ 					&vgpu->group_notifier);
+ 	drm_WARN(&i915->drm, ret,
+ 		 "vfio_unregister_notifier for group failed: %d\n", ret);
+ 
+ 	debugfs_remove(debugfs_lookup(KVMGT_DEBUGFS_FILENAME, vgpu->debugfs));
+ 
+ 	kvm_page_track_unregister_notifier(vgpu->kvm, &vgpu->track_node);
+ 	kvm_put_kvm(vgpu->kvm);
+ 	kvmgt_protect_table_destroy(vgpu);
+ 	gvt_cache_destroy(vgpu);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  
  	intel_vgpu_release_msi_eventfd_ctx(vgpu);
 -	vfio_group_put_external_user(vgpu->vfio_group);
 +	vfio_group_put_external_user(vdev->vfio_group);
  
 -	vgpu->kvm = NULL;
 -	vgpu->attached = false;
 +	vdev->kvm = NULL;
 +	vgpu->handle = 0;
  }
  
++<<<<<<< HEAD
 +static void intel_vgpu_release(struct mdev_device *mdev)
++=======
+ static void intel_vgpu_close_device(struct vfio_device *vfio_dev)
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  {
- 	struct intel_vgpu *vgpu = mdev_get_drvdata(mdev);
- 
- 	__intel_vgpu_release(vgpu);
+ 	__intel_vgpu_release(vfio_dev_to_vgpu(vfio_dev));
  }
  
  static void intel_vgpu_release_work(struct work_struct *work)
@@@ -1179,9 -1080,10 +1267,13 @@@ static bool gtt_entry(struct mdev_devic
  			true : false;
  }
  
- static ssize_t intel_vgpu_read(struct mdev_device *mdev, char __user *buf,
+ static ssize_t intel_vgpu_read(struct vfio_device *vfio_dev, char __user *buf,
  			size_t count, loff_t *ppos)
  {
++<<<<<<< HEAD
++=======
+ 	struct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  	unsigned int done = 0;
  	int ret;
  
@@@ -1256,6 -1158,7 +1348,10 @@@ static ssize_t intel_vgpu_write(struct 
  				const char __user *buf,
  				size_t count, loff_t *ppos)
  {
++<<<<<<< HEAD
++=======
+ 	struct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  	unsigned int done = 0;
  	int ret;
  
@@@ -1454,11 -1358,10 +1551,15 @@@ static int intel_vgpu_set_irqs(struct i
  	return func(vgpu, index, start, count, flags, data);
  }
  
- static long intel_vgpu_ioctl(struct mdev_device *mdev, unsigned int cmd,
+ static long intel_vgpu_ioctl(struct vfio_device *vfio_dev, unsigned int cmd,
  			     unsigned long arg)
  {
++<<<<<<< HEAD
 +	struct intel_vgpu *vgpu = mdev_get_drvdata(mdev);
 +	struct kvmgt_vdev *vdev = kvmgt_vdev(vgpu);
++=======
+ 	struct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  	unsigned long minsz;
  
  	gvt_dbg_core("vgpu%d ioctl, cmd: %d\n", vgpu->id, cmd);
@@@ -1764,48 -1658,77 +1860,117 @@@ static const struct attribute_group *in
  	NULL,
  };
  
++<<<<<<< HEAD
 +static struct mdev_parent_ops intel_vgpu_ops = {
 +	.mdev_attr_groups       = intel_vgpu_groups,
 +	.create			= intel_vgpu_create,
 +	.remove			= intel_vgpu_remove,
 +
 +	.open			= intel_vgpu_open,
 +	.release		= intel_vgpu_release,
 +
 +	.read			= intel_vgpu_read,
 +	.write			= intel_vgpu_write,
 +	.mmap			= intel_vgpu_mmap,
 +	.ioctl			= intel_vgpu_ioctl,
++=======
+ static const struct vfio_device_ops intel_vgpu_dev_ops = {
+ 	.open_device	= intel_vgpu_open_device,
+ 	.close_device	= intel_vgpu_close_device,
+ 	.read		= intel_vgpu_read,
+ 	.write		= intel_vgpu_write,
+ 	.mmap		= intel_vgpu_mmap,
+ 	.ioctl		= intel_vgpu_ioctl,
+ };
+ 
+ static int intel_vgpu_probe(struct mdev_device *mdev)
+ {
+ 	struct device *pdev = mdev_parent_dev(mdev);
+ 	struct intel_gvt *gvt = kdev_to_i915(pdev)->gvt;
+ 	struct intel_vgpu_type *type;
+ 	struct intel_vgpu *vgpu;
+ 	int ret;
+ 
+ 	type = &gvt->types[mdev_get_type_group_id(mdev)];
+ 	if (!type)
+ 		return -EINVAL;
+ 
+ 	vgpu = intel_gvt_create_vgpu(gvt, type);
+ 	if (IS_ERR(vgpu)) {
+ 		gvt_err("failed to create intel vgpu: %ld\n", PTR_ERR(vgpu));
+ 		return PTR_ERR(vgpu);
+ 	}
+ 
+ 	INIT_WORK(&vgpu->release_work, intel_vgpu_release_work);
+ 	vfio_init_group_dev(&vgpu->vfio_device, &mdev->dev,
+ 			    &intel_vgpu_dev_ops);
+ 
+ 	dev_set_drvdata(&mdev->dev, vgpu);
+ 	ret = vfio_register_emulated_iommu_dev(&vgpu->vfio_device);
+ 	if (ret) {
+ 		intel_gvt_destroy_vgpu(vgpu);
+ 		return ret;
+ 	}
+ 
+ 	gvt_dbg_core("intel_vgpu_create succeeded for mdev: %s\n",
+ 		     dev_name(mdev_dev(mdev)));
+ 	return 0;
+ }
+ 
+ static void intel_vgpu_remove(struct mdev_device *mdev)
+ {
+ 	struct intel_vgpu *vgpu = dev_get_drvdata(&mdev->dev);
+ 
+ 	if (WARN_ON_ONCE(vgpu->attached))
+ 		return;
+ 	intel_gvt_destroy_vgpu(vgpu);
+ }
+ 
+ static struct mdev_driver intel_vgpu_mdev_driver = {
+ 	.driver = {
+ 		.name		= "intel_vgpu_mdev",
+ 		.owner		= THIS_MODULE,
+ 		.dev_groups	= intel_vgpu_groups,
+ 	},
+ 	.probe		= intel_vgpu_probe,
+ 	.remove		= intel_vgpu_remove,
+ };
+ 
+ const struct mdev_parent_ops intel_vgpu_mdev_ops = {
+ 	.owner			= THIS_MODULE,
+ 	.supported_type_groups	= gvt_vgpu_type_groups,
+ 	.device_driver		= &intel_vgpu_mdev_driver,
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  };
  
 -int intel_gvt_page_track_add(struct intel_vgpu *info, u64 gfn)
 +static int kvmgt_host_init(struct device *dev, void *gvt, const void *ops)
 +{
 +	int ret;
 +
 +	ret = intel_gvt_init_vgpu_type_groups((struct intel_gvt *)gvt);
 +	if (ret)
 +		return ret;
 +
 +	intel_gvt_ops = ops;
 +	intel_vgpu_ops.supported_type_groups = gvt_vgpu_type_groups;
 +
 +	ret = mdev_register_device(dev, &intel_vgpu_ops);
 +	if (ret)
 +		intel_gvt_cleanup_vgpu_type_groups((struct intel_gvt *)gvt);
 +
 +	return ret;
 +}
 +
 +static void kvmgt_host_exit(struct device *dev, void *gvt)
  {
 -	struct kvm *kvm = info->kvm;
 +	mdev_unregister_device(dev);
 +	intel_gvt_cleanup_vgpu_type_groups((struct intel_gvt *)gvt);
 +}
 +
 +static int kvmgt_page_track_add(unsigned long handle, u64 gfn)
 +{
 +	struct kvmgt_guest_info *info;
 +	struct kvm *kvm;
  	struct kvm_memory_slot *slot;
  	int idx;
  
@@@ -2165,93 -1922,27 +2330,111 @@@ static void kvmgt_dma_unmap_guest_page(
  	entry = __gvt_cache_find_dma_addr(vgpu, dma_addr);
  	if (entry)
  		kref_put(&entry->ref, __gvt_dma_release);
 -	mutex_unlock(&vgpu->cache_lock);
 +	mutex_unlock(&vdev->cache_lock);
 +}
 +
 +static int kvmgt_rw_gpa(unsigned long handle, unsigned long gpa,
 +			void *buf, unsigned long len, bool write)
 +{
 +	struct kvmgt_guest_info *info;
 +
 +	if (!handle_valid(handle))
 +		return -ESRCH;
 +
 +	info = (struct kvmgt_guest_info *)handle;
 +
 +	return vfio_dma_rw(kvmgt_vdev(info->vgpu)->vfio_group,
 +			   gpa, buf, len, write);
  }
  
 +static int kvmgt_read_gpa(unsigned long handle, unsigned long gpa,
 +			void *buf, unsigned long len)
 +{
 +	return kvmgt_rw_gpa(handle, gpa, buf, len, false);
 +}
 +
 +static int kvmgt_write_gpa(unsigned long handle, unsigned long gpa,
 +			void *buf, unsigned long len)
 +{
 +	return kvmgt_rw_gpa(handle, gpa, buf, len, true);
 +}
 +
 +static unsigned long kvmgt_virt_to_pfn(void *addr)
 +{
 +	return PFN_DOWN(__pa(addr));
 +}
 +
 +static bool kvmgt_is_valid_gfn(unsigned long handle, unsigned long gfn)
 +{
 +	struct kvmgt_guest_info *info;
 +	struct kvm *kvm;
 +	int idx;
 +	bool ret;
 +
 +	if (!handle_valid(handle))
 +		return false;
 +
 +	info = (struct kvmgt_guest_info *)handle;
 +	kvm = info->kvm;
 +
 +	idx = srcu_read_lock(&kvm->srcu);
 +	ret = kvm_is_visible_gfn(kvm, gfn);
 +	srcu_read_unlock(&kvm->srcu, idx);
 +
 +	return ret;
 +}
 +
 +static const struct intel_gvt_mpt kvmgt_mpt = {
 +	.type = INTEL_GVT_HYPERVISOR_KVM,
 +	.host_init = kvmgt_host_init,
 +	.host_exit = kvmgt_host_exit,
 +	.attach_vgpu = kvmgt_attach_vgpu,
 +	.detach_vgpu = kvmgt_detach_vgpu,
 +	.inject_msi = kvmgt_inject_msi,
 +	.from_virt_to_mfn = kvmgt_virt_to_pfn,
 +	.enable_page_track = kvmgt_page_track_add,
 +	.disable_page_track = kvmgt_page_track_remove,
 +	.read_gpa = kvmgt_read_gpa,
 +	.write_gpa = kvmgt_write_gpa,
 +	.gfn_to_mfn = kvmgt_gfn_to_pfn,
 +	.dma_map_guest_page = kvmgt_dma_map_guest_page,
 +	.dma_unmap_guest_page = kvmgt_dma_unmap_guest_page,
 +	.dma_pin_guest_page = kvmgt_dma_pin_guest_page,
 +	.set_opregion = kvmgt_set_opregion,
 +	.set_edid = kvmgt_set_edid,
 +	.get_vfio_device = kvmgt_get_vfio_device,
 +	.put_vfio_device = kvmgt_put_vfio_device,
 +	.is_valid_gfn = kvmgt_is_valid_gfn,
 +};
 +
  static int __init kvmgt_init(void)
  {
++<<<<<<< HEAD
 +	if (intel_gvt_register_hypervisor(&kvmgt_mpt) < 0)
 +		return -ENODEV;
 +	return 0;
++=======
+ 	int ret;
+ 
+ 	ret = intel_gvt_set_ops(&intel_gvt_vgpu_ops);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = mdev_register_driver(&intel_vgpu_mdev_driver);
+ 	if (ret)
+ 		intel_gvt_clear_ops(&intel_gvt_vgpu_ops);
+ 	return ret;
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  }
  
  static void __exit kvmgt_exit(void)
  {
++<<<<<<< HEAD
 +	intel_gvt_unregister_hypervisor();
++=======
+ 	mdev_unregister_driver(&intel_vgpu_mdev_driver);
+ 	intel_gvt_clear_ops(&intel_gvt_vgpu_ops);
++>>>>>>> 978cf586ac35 (drm/i915/gvt: convert to use vfio_register_emulated_iommu_dev)
  }
  
  module_init(kvmgt_init);
* Unmerged path drivers/gpu/drm/i915/gvt/gvt.h
* Unmerged path drivers/gpu/drm/i915/gvt/kvmgt.c
