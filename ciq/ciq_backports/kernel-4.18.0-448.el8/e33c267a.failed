mm: shrinkers: provide shrinkers with names

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
Rebuild_CHGLOG: - Revert "mm: shrinkers: provide shrinkers with names" (Jocelyn Falempe) [2115880]
Rebuild_FUZZ: 90.53%
commit-author Roman Gushchin <roman.gushchin@linux.dev>
commit e33c267ab70de4249d22d7eab1cc7d68a889bac2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/e33c267a.failed

Currently shrinkers are anonymous objects.  For debugging purposes they
can be identified by count/scan function names, but it's not always
useful: e.g.  for superblock's shrinkers it's nice to have at least an
idea of to which superblock the shrinker belongs.

This commit adds names to shrinkers.  register_shrinker() and
prealloc_shrinker() functions are extended to take a format and arguments
to master a name.

In some cases it's not possible to determine a good name at the time when
a shrinker is allocated.  For such cases shrinker_debugfs_rename() is
provided.

The expected format is:
    <subsystem>-<shrinker_type>[:<instance>]-<id>
For some shrinkers an instance can be encoded as (MAJOR:MINOR) pair.

After this change the shrinker debugfs directory looks like:
  $ cd /sys/kernel/debug/shrinker/
  $ ls
    dquota-cache-16     sb-devpts-28     sb-proc-47       sb-tmpfs-42
    mm-shadow-18        sb-devtmpfs-5    sb-proc-48       sb-tmpfs-43
    mm-zspool:zram0-34  sb-hugetlbfs-17  sb-pstore-31     sb-tmpfs-44
    rcu-kfree-0         sb-hugetlbfs-33  sb-rootfs-2      sb-tmpfs-49
    sb-aio-20           sb-iomem-12      sb-securityfs-6  sb-tracefs-13
    sb-anon_inodefs-15  sb-mqueue-21     sb-selinuxfs-22  sb-xfs:vda1-36
    sb-bdev-3           sb-nsfs-4        sb-sockfs-8      sb-zsmalloc-19
    sb-bpf-32           sb-pipefs-14     sb-sysfs-26      thp-deferred_split-10
    sb-btrfs:vda2-24    sb-proc-25       sb-tmpfs-1       thp-zero-9
    sb-cgroup2-30       sb-proc-39       sb-tmpfs-27      xfs-buf:vda1-37
    sb-configfs-23      sb-proc-41       sb-tmpfs-29      xfs-inodegc:vda1-38
    sb-dax-11           sb-proc-45       sb-tmpfs-35
    sb-debugfs-7        sb-proc-46       sb-tmpfs-40

[roman.gushchin@linux.dev: fix build warnings]
  Link: https://lkml.kernel.org/r/Yr+ZTnLb9lJk6fJO@castle
  Reported-by: kernel test robot <lkp@intel.com>
Link: https://lkml.kernel.org/r/20220601032227.4076670-4-roman.gushchin@linux.dev
	Signed-off-by: Roman Gushchin <roman.gushchin@linux.dev>
	Cc: Christophe JAILLET <christophe.jaillet@wanadoo.fr>
	Cc: Dave Chinner <dchinner@redhat.com>
	Cc: Hillf Danton <hdanton@sina.com>
	Cc: Kent Overstreet <kent.overstreet@gmail.com>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit e33c267ab70de4249d22d7eab1cc7d68a889bac2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/panfrost/panfrost_gem_shrinker.c
#	drivers/md/bcache/btree.c
#	drivers/misc/vmw_balloon.c
#	drivers/xen/xenbus/xenbus_probe_backend.c
#	fs/erofs/utils.c
#	fs/f2fs/super.c
#	fs/jbd2/journal.c
#	fs/nfsd/filecache.c
#	fs/xfs/xfs_icache.c
#	include/linux/shrinker.h
#	mm/shrinker_debug.c
diff --cc drivers/md/bcache/btree.c
index 547c9eedc2f4,147c493a989a..000000000000
--- a/drivers/md/bcache/btree.c
+++ b/drivers/md/bcache/btree.c
@@@ -812,8 -812,8 +812,13 @@@ int bch_btree_cache_alloc(struct cache_
  	c->shrink.seeks = 4;
  	c->shrink.batch = c->btree_pages * 2;
  
++<<<<<<< HEAD
 +	if (register_shrinker(&c->shrink))
 +		pr_warn("bcache: %s: could not register shrinker",
++=======
+ 	if (register_shrinker(&c->shrink, "md-bcache:%pU", c->set_uuid))
+ 		pr_warn("bcache: %s: could not register shrinker\n",
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
  				__func__);
  
  	return 0;
diff --cc drivers/misc/vmw_balloon.c
index 0341406a8eb6,c2d2fa114e65..000000000000
--- a/drivers/misc/vmw_balloon.c
+++ b/drivers/misc/vmw_balloon.c
@@@ -1135,6 -1508,91 +1135,94 @@@ static void vmballoon_work(struct work_
  	 */
  	queue_delayed_work(system_freezable_wq,
  			   dwork, round_jiffies_relative(HZ));
++<<<<<<< HEAD
++=======
+ 
+ }
+ 
+ /**
+  * vmballoon_shrinker_scan() - deflate the balloon due to memory pressure.
+  * @shrinker: pointer to the balloon shrinker.
+  * @sc: page reclaim information.
+  *
+  * Returns: number of pages that were freed during deflation.
+  */
+ static unsigned long vmballoon_shrinker_scan(struct shrinker *shrinker,
+ 					     struct shrink_control *sc)
+ {
+ 	struct vmballoon *b = &balloon;
+ 	unsigned long deflated_frames;
+ 
+ 	pr_debug("%s - size: %llu", __func__, atomic64_read(&b->size));
+ 
+ 	vmballoon_stats_gen_inc(b, VMW_BALLOON_STAT_SHRINK);
+ 
+ 	/*
+ 	 * If the lock is also contended for read, we cannot easily reclaim and
+ 	 * we bail out.
+ 	 */
+ 	if (!down_read_trylock(&b->conf_sem))
+ 		return 0;
+ 
+ 	deflated_frames = vmballoon_deflate(b, sc->nr_to_scan, true);
+ 
+ 	vmballoon_stats_gen_add(b, VMW_BALLOON_STAT_SHRINK_FREE,
+ 				deflated_frames);
+ 
+ 	/*
+ 	 * Delay future inflation for some time to mitigate the situations in
+ 	 * which balloon continuously grows and shrinks. Use WRITE_ONCE() since
+ 	 * the access is asynchronous.
+ 	 */
+ 	WRITE_ONCE(b->shrink_timeout, jiffies + HZ * VMBALLOON_SHRINK_DELAY);
+ 
+ 	up_read(&b->conf_sem);
+ 
+ 	return deflated_frames;
+ }
+ 
+ /**
+  * vmballoon_shrinker_count() - return the number of ballooned pages.
+  * @shrinker: pointer to the balloon shrinker.
+  * @sc: page reclaim information.
+  *
+  * Returns: number of 4k pages that are allocated for the balloon and can
+  *	    therefore be reclaimed under pressure.
+  */
+ static unsigned long vmballoon_shrinker_count(struct shrinker *shrinker,
+ 					      struct shrink_control *sc)
+ {
+ 	struct vmballoon *b = &balloon;
+ 
+ 	return atomic64_read(&b->size);
+ }
+ 
+ static void vmballoon_unregister_shrinker(struct vmballoon *b)
+ {
+ 	if (b->shrinker_registered)
+ 		unregister_shrinker(&b->shrinker);
+ 	b->shrinker_registered = false;
+ }
+ 
+ static int vmballoon_register_shrinker(struct vmballoon *b)
+ {
+ 	int r;
+ 
+ 	/* Do nothing if the shrinker is not enabled */
+ 	if (!vmwballoon_shrinker_enable)
+ 		return 0;
+ 
+ 	b->shrinker.scan_objects = vmballoon_shrinker_scan;
+ 	b->shrinker.count_objects = vmballoon_shrinker_count;
+ 	b->shrinker.seeks = DEFAULT_SEEKS;
+ 
+ 	r = register_shrinker(&b->shrinker, "vmw-balloon");
+ 
+ 	if (r == 0)
+ 		b->shrinker_registered = true;
+ 
+ 	return r;
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
  }
  
  /*
diff --cc drivers/xen/xenbus/xenbus_probe_backend.c
index b0bed4faf44c,9c09f89d8278..000000000000
--- a/drivers/xen/xenbus/xenbus_probe_backend.c
+++ b/drivers/xen/xenbus/xenbus_probe_backend.c
@@@ -264,6 -305,9 +264,12 @@@ static int __init xenbus_probe_backend_
  
  	register_xenstore_notifier(&xenstore_notifier);
  
++<<<<<<< HEAD
++=======
+ 	if (register_shrinker(&backend_memory_shrinker, "xen-backend"))
+ 		pr_warn("shrinker registration failed\n");
+ 
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
  	return 0;
  }
  subsys_initcall(xenbus_probe_backend_init);
diff --cc fs/f2fs/super.c
index 07cb96e441bc,bce02306f7a0..000000000000
--- a/fs/f2fs/super.c
+++ b/fs/f2fs/super.c
@@@ -3154,13 -4567,19 +3154,18 @@@ static int __init init_f2fs_fs(void
  	err = f2fs_create_checkpoint_caches();
  	if (err)
  		goto free_segment_manager_caches;
 -	err = f2fs_create_recovery_cache();
 -	if (err)
 -		goto free_checkpoint_caches;
  	err = f2fs_create_extent_cache();
  	if (err)
 -		goto free_recovery_cache;
 -	err = f2fs_create_garbage_collection_cache();
 -	if (err)
 -		goto free_extent_cache;
 +		goto free_checkpoint_caches;
  	err = f2fs_init_sysfs();
  	if (err)
++<<<<<<< HEAD
 +		goto free_extent_cache;
 +	err = register_shrinker(&f2fs_shrinker_info);
++=======
+ 		goto free_garbage_collection_cache;
+ 	err = register_shrinker(&f2fs_shrinker_info, "f2fs-shrinker");
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
  	if (err)
  		goto free_sysfs;
  	err = register_filesystem(&f2fs_fs_type);
diff --cc fs/jbd2/journal.c
index bd9eb1f9952d,45e4655c8033..000000000000
--- a/fs/jbd2/journal.c
+++ b/fs/jbd2/journal.c
@@@ -1190,6 -1409,20 +1190,23 @@@ static journal_t *journal_init_common(s
  	journal->j_sb_buffer = bh;
  	journal->j_superblock = (journal_superblock_t *)bh->b_data;
  
++<<<<<<< HEAD
++=======
+ 	journal->j_shrink_transaction = NULL;
+ 	journal->j_shrinker.scan_objects = jbd2_journal_shrink_scan;
+ 	journal->j_shrinker.count_objects = jbd2_journal_shrink_count;
+ 	journal->j_shrinker.seeks = DEFAULT_SEEKS;
+ 	journal->j_shrinker.batch = journal->j_max_transaction_buffers;
+ 
+ 	if (percpu_counter_init(&journal->j_checkpoint_jh_count, 0, GFP_KERNEL))
+ 		goto err_cleanup;
+ 
+ 	if (register_shrinker(&journal->j_shrinker, "jbd2-journal:(%u:%u)",
+ 			      MAJOR(bdev->bd_dev), MINOR(bdev->bd_dev))) {
+ 		percpu_counter_destroy(&journal->j_checkpoint_jh_count);
+ 		goto err_cleanup;
+ 	}
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
  	return journal;
  
  err_cleanup:
diff --cc fs/xfs/xfs_icache.c
index 2ee8fefb589b,a1941c8b8630..000000000000
--- a/fs/xfs/xfs_icache.c
+++ b/fs/xfs/xfs_icache.c
@@@ -1871,5 -1764,442 +1871,445 @@@ xfs_icwalk
  		}
  	}
  	return last_error;
++<<<<<<< HEAD
 +	BUILD_BUG_ON(XFS_ICWALK_PRIVATE_FLAGS & XFS_EOF_FLAGS_VALID);
++=======
+ 	BUILD_BUG_ON(XFS_ICWALK_PRIVATE_FLAGS & XFS_ICWALK_FLAGS_VALID);
+ }
+ 
+ #ifdef DEBUG
+ static void
+ xfs_check_delalloc(
+ 	struct xfs_inode	*ip,
+ 	int			whichfork)
+ {
+ 	struct xfs_ifork	*ifp = XFS_IFORK_PTR(ip, whichfork);
+ 	struct xfs_bmbt_irec	got;
+ 	struct xfs_iext_cursor	icur;
+ 
+ 	if (!ifp || !xfs_iext_lookup_extent(ip, ifp, 0, &icur, &got))
+ 		return;
+ 	do {
+ 		if (isnullstartblock(got.br_startblock)) {
+ 			xfs_warn(ip->i_mount,
+ 	"ino %llx %s fork has delalloc extent at [0x%llx:0x%llx]",
+ 				ip->i_ino,
+ 				whichfork == XFS_DATA_FORK ? "data" : "cow",
+ 				got.br_startoff, got.br_blockcount);
+ 		}
+ 	} while (xfs_iext_next_extent(ifp, &icur, &got));
+ }
+ #else
+ #define xfs_check_delalloc(ip, whichfork)	do { } while (0)
+ #endif
+ 
+ /* Schedule the inode for reclaim. */
+ static void
+ xfs_inodegc_set_reclaimable(
+ 	struct xfs_inode	*ip)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_perag	*pag;
+ 
+ 	if (!xfs_is_shutdown(mp) && ip->i_delayed_blks) {
+ 		xfs_check_delalloc(ip, XFS_DATA_FORK);
+ 		xfs_check_delalloc(ip, XFS_COW_FORK);
+ 		ASSERT(0);
+ 	}
+ 
+ 	pag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));
+ 	spin_lock(&pag->pag_ici_lock);
+ 	spin_lock(&ip->i_flags_lock);
+ 
+ 	trace_xfs_inode_set_reclaimable(ip);
+ 	ip->i_flags &= ~(XFS_NEED_INACTIVE | XFS_INACTIVATING);
+ 	ip->i_flags |= XFS_IRECLAIMABLE;
+ 	xfs_perag_set_inode_tag(pag, XFS_INO_TO_AGINO(mp, ip->i_ino),
+ 			XFS_ICI_RECLAIM_TAG);
+ 
+ 	spin_unlock(&ip->i_flags_lock);
+ 	spin_unlock(&pag->pag_ici_lock);
+ 	xfs_perag_put(pag);
+ }
+ 
+ /*
+  * Free all speculative preallocations and possibly even the inode itself.
+  * This is the last chance to make changes to an otherwise unreferenced file
+  * before incore reclamation happens.
+  */
+ static void
+ xfs_inodegc_inactivate(
+ 	struct xfs_inode	*ip)
+ {
+ 	trace_xfs_inode_inactivating(ip);
+ 	xfs_inactive(ip);
+ 	xfs_inodegc_set_reclaimable(ip);
+ }
+ 
+ void
+ xfs_inodegc_worker(
+ 	struct work_struct	*work)
+ {
+ 	struct xfs_inodegc	*gc = container_of(work, struct xfs_inodegc,
+ 							work);
+ 	struct llist_node	*node = llist_del_all(&gc->list);
+ 	struct xfs_inode	*ip, *n;
+ 
+ 	WRITE_ONCE(gc->items, 0);
+ 
+ 	if (!node)
+ 		return;
+ 
+ 	ip = llist_entry(node, struct xfs_inode, i_gclist);
+ 	trace_xfs_inodegc_worker(ip->i_mount, READ_ONCE(gc->shrinker_hits));
+ 
+ 	WRITE_ONCE(gc->shrinker_hits, 0);
+ 	llist_for_each_entry_safe(ip, n, node, i_gclist) {
+ 		xfs_iflags_set(ip, XFS_INACTIVATING);
+ 		xfs_inodegc_inactivate(ip);
+ 	}
+ }
+ 
+ /*
+  * Force all currently queued inode inactivation work to run immediately and
+  * wait for the work to finish.
+  */
+ void
+ xfs_inodegc_flush(
+ 	struct xfs_mount	*mp)
+ {
+ 	if (!xfs_is_inodegc_enabled(mp))
+ 		return;
+ 
+ 	trace_xfs_inodegc_flush(mp, __return_address);
+ 
+ 	xfs_inodegc_queue_all(mp);
+ 	flush_workqueue(mp->m_inodegc_wq);
+ }
+ 
+ /*
+  * Flush all the pending work and then disable the inode inactivation background
+  * workers and wait for them to stop.
+  */
+ void
+ xfs_inodegc_stop(
+ 	struct xfs_mount	*mp)
+ {
+ 	if (!xfs_clear_inodegc_enabled(mp))
+ 		return;
+ 
+ 	xfs_inodegc_queue_all(mp);
+ 	drain_workqueue(mp->m_inodegc_wq);
+ 
+ 	trace_xfs_inodegc_stop(mp, __return_address);
+ }
+ 
+ /*
+  * Enable the inode inactivation background workers and schedule deferred inode
+  * inactivation work if there is any.
+  */
+ void
+ xfs_inodegc_start(
+ 	struct xfs_mount	*mp)
+ {
+ 	if (xfs_set_inodegc_enabled(mp))
+ 		return;
+ 
+ 	trace_xfs_inodegc_start(mp, __return_address);
+ 	xfs_inodegc_queue_all(mp);
+ }
+ 
+ #ifdef CONFIG_XFS_RT
+ static inline bool
+ xfs_inodegc_want_queue_rt_file(
+ 	struct xfs_inode	*ip)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 
+ 	if (!XFS_IS_REALTIME_INODE(ip))
+ 		return false;
+ 
+ 	if (__percpu_counter_compare(&mp->m_frextents,
+ 				mp->m_low_rtexts[XFS_LOWSP_5_PCNT],
+ 				XFS_FDBLOCKS_BATCH) < 0)
+ 		return true;
+ 
+ 	return false;
+ }
+ #else
+ # define xfs_inodegc_want_queue_rt_file(ip)	(false)
+ #endif /* CONFIG_XFS_RT */
+ 
+ /*
+  * Schedule the inactivation worker when:
+  *
+  *  - We've accumulated more than one inode cluster buffer's worth of inodes.
+  *  - There is less than 5% free space left.
+  *  - Any of the quotas for this inode are near an enforcement limit.
+  */
+ static inline bool
+ xfs_inodegc_want_queue_work(
+ 	struct xfs_inode	*ip,
+ 	unsigned int		items)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 
+ 	if (items > mp->m_ino_geo.inodes_per_cluster)
+ 		return true;
+ 
+ 	if (__percpu_counter_compare(&mp->m_fdblocks,
+ 				mp->m_low_space[XFS_LOWSP_5_PCNT],
+ 				XFS_FDBLOCKS_BATCH) < 0)
+ 		return true;
+ 
+ 	if (xfs_inodegc_want_queue_rt_file(ip))
+ 		return true;
+ 
+ 	if (xfs_inode_near_dquot_enforcement(ip, XFS_DQTYPE_USER))
+ 		return true;
+ 
+ 	if (xfs_inode_near_dquot_enforcement(ip, XFS_DQTYPE_GROUP))
+ 		return true;
+ 
+ 	if (xfs_inode_near_dquot_enforcement(ip, XFS_DQTYPE_PROJ))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Upper bound on the number of inodes in each AG that can be queued for
+  * inactivation at any given time, to avoid monopolizing the workqueue.
+  */
+ #define XFS_INODEGC_MAX_BACKLOG		(4 * XFS_INODES_PER_CHUNK)
+ 
+ /*
+  * Make the frontend wait for inactivations when:
+  *
+  *  - Memory shrinkers queued the inactivation worker and it hasn't finished.
+  *  - The queue depth exceeds the maximum allowable percpu backlog.
+  *
+  * Note: If the current thread is running a transaction, we don't ever want to
+  * wait for other transactions because that could introduce a deadlock.
+  */
+ static inline bool
+ xfs_inodegc_want_flush_work(
+ 	struct xfs_inode	*ip,
+ 	unsigned int		items,
+ 	unsigned int		shrinker_hits)
+ {
+ 	if (current->journal_info)
+ 		return false;
+ 
+ 	if (shrinker_hits > 0)
+ 		return true;
+ 
+ 	if (items > XFS_INODEGC_MAX_BACKLOG)
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ /*
+  * Queue a background inactivation worker if there are inodes that need to be
+  * inactivated and higher level xfs code hasn't disabled the background
+  * workers.
+  */
+ static void
+ xfs_inodegc_queue(
+ 	struct xfs_inode	*ip)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	struct xfs_inodegc	*gc;
+ 	int			items;
+ 	unsigned int		shrinker_hits;
+ 
+ 	trace_xfs_inode_set_need_inactive(ip);
+ 	spin_lock(&ip->i_flags_lock);
+ 	ip->i_flags |= XFS_NEED_INACTIVE;
+ 	spin_unlock(&ip->i_flags_lock);
+ 
+ 	gc = get_cpu_ptr(mp->m_inodegc);
+ 	llist_add(&ip->i_gclist, &gc->list);
+ 	items = READ_ONCE(gc->items);
+ 	WRITE_ONCE(gc->items, items + 1);
+ 	shrinker_hits = READ_ONCE(gc->shrinker_hits);
+ 	put_cpu_ptr(gc);
+ 
+ 	if (!xfs_is_inodegc_enabled(mp))
+ 		return;
+ 
+ 	if (xfs_inodegc_want_queue_work(ip, items)) {
+ 		trace_xfs_inodegc_queue(mp, __return_address);
+ 		queue_work(mp->m_inodegc_wq, &gc->work);
+ 	}
+ 
+ 	if (xfs_inodegc_want_flush_work(ip, items, shrinker_hits)) {
+ 		trace_xfs_inodegc_throttle(mp, __return_address);
+ 		flush_work(&gc->work);
+ 	}
+ }
+ 
+ /*
+  * Fold the dead CPU inodegc queue into the current CPUs queue.
+  */
+ void
+ xfs_inodegc_cpu_dead(
+ 	struct xfs_mount	*mp,
+ 	unsigned int		dead_cpu)
+ {
+ 	struct xfs_inodegc	*dead_gc, *gc;
+ 	struct llist_node	*first, *last;
+ 	unsigned int		count = 0;
+ 
+ 	dead_gc = per_cpu_ptr(mp->m_inodegc, dead_cpu);
+ 	cancel_work_sync(&dead_gc->work);
+ 
+ 	if (llist_empty(&dead_gc->list))
+ 		return;
+ 
+ 	first = dead_gc->list.first;
+ 	last = first;
+ 	while (last->next) {
+ 		last = last->next;
+ 		count++;
+ 	}
+ 	dead_gc->list.first = NULL;
+ 	dead_gc->items = 0;
+ 
+ 	/* Add pending work to current CPU */
+ 	gc = get_cpu_ptr(mp->m_inodegc);
+ 	llist_add_batch(first, last, &gc->list);
+ 	count += READ_ONCE(gc->items);
+ 	WRITE_ONCE(gc->items, count);
+ 	put_cpu_ptr(gc);
+ 
+ 	if (xfs_is_inodegc_enabled(mp)) {
+ 		trace_xfs_inodegc_queue(mp, __return_address);
+ 		queue_work(mp->m_inodegc_wq, &gc->work);
+ 	}
+ }
+ 
+ /*
+  * We set the inode flag atomically with the radix tree tag.  Once we get tag
+  * lookups on the radix tree, this inode flag can go away.
+  *
+  * We always use background reclaim here because even if the inode is clean, it
+  * still may be under IO and hence we have wait for IO completion to occur
+  * before we can reclaim the inode. The background reclaim path handles this
+  * more efficiently than we can here, so simply let background reclaim tear down
+  * all inodes.
+  */
+ void
+ xfs_inode_mark_reclaimable(
+ 	struct xfs_inode	*ip)
+ {
+ 	struct xfs_mount	*mp = ip->i_mount;
+ 	bool			need_inactive;
+ 
+ 	XFS_STATS_INC(mp, vn_reclaim);
+ 
+ 	/*
+ 	 * We should never get here with any of the reclaim flags already set.
+ 	 */
+ 	ASSERT_ALWAYS(!xfs_iflags_test(ip, XFS_ALL_IRECLAIM_FLAGS));
+ 
+ 	need_inactive = xfs_inode_needs_inactive(ip);
+ 	if (need_inactive) {
+ 		xfs_inodegc_queue(ip);
+ 		return;
+ 	}
+ 
+ 	/* Going straight to reclaim, so drop the dquots. */
+ 	xfs_qm_dqdetach(ip);
+ 	xfs_inodegc_set_reclaimable(ip);
+ }
+ 
+ /*
+  * Register a phony shrinker so that we can run background inodegc sooner when
+  * there's memory pressure.  Inactivation does not itself free any memory but
+  * it does make inodes reclaimable, which eventually frees memory.
+  *
+  * The count function, seek value, and batch value are crafted to trigger the
+  * scan function during the second round of scanning.  Hopefully this means
+  * that we reclaimed enough memory that initiating metadata transactions won't
+  * make things worse.
+  */
+ #define XFS_INODEGC_SHRINKER_COUNT	(1UL << DEF_PRIORITY)
+ #define XFS_INODEGC_SHRINKER_BATCH	((XFS_INODEGC_SHRINKER_COUNT / 2) + 1)
+ 
+ static unsigned long
+ xfs_inodegc_shrinker_count(
+ 	struct shrinker		*shrink,
+ 	struct shrink_control	*sc)
+ {
+ 	struct xfs_mount	*mp = container_of(shrink, struct xfs_mount,
+ 						   m_inodegc_shrinker);
+ 	struct xfs_inodegc	*gc;
+ 	int			cpu;
+ 
+ 	if (!xfs_is_inodegc_enabled(mp))
+ 		return 0;
+ 
+ 	for_each_online_cpu(cpu) {
+ 		gc = per_cpu_ptr(mp->m_inodegc, cpu);
+ 		if (!llist_empty(&gc->list))
+ 			return XFS_INODEGC_SHRINKER_COUNT;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static unsigned long
+ xfs_inodegc_shrinker_scan(
+ 	struct shrinker		*shrink,
+ 	struct shrink_control	*sc)
+ {
+ 	struct xfs_mount	*mp = container_of(shrink, struct xfs_mount,
+ 						   m_inodegc_shrinker);
+ 	struct xfs_inodegc	*gc;
+ 	int			cpu;
+ 	bool			no_items = true;
+ 
+ 	if (!xfs_is_inodegc_enabled(mp))
+ 		return SHRINK_STOP;
+ 
+ 	trace_xfs_inodegc_shrinker_scan(mp, sc, __return_address);
+ 
+ 	for_each_online_cpu(cpu) {
+ 		gc = per_cpu_ptr(mp->m_inodegc, cpu);
+ 		if (!llist_empty(&gc->list)) {
+ 			unsigned int	h = READ_ONCE(gc->shrinker_hits);
+ 
+ 			WRITE_ONCE(gc->shrinker_hits, h + 1);
+ 			queue_work_on(cpu, mp->m_inodegc_wq, &gc->work);
+ 			no_items = false;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * If there are no inodes to inactivate, we don't want the shrinker
+ 	 * to think there's deferred work to call us back about.
+ 	 */
+ 	if (no_items)
+ 		return LONG_MAX;
+ 
+ 	return SHRINK_STOP;
+ }
+ 
+ /* Register a shrinker so we can accelerate inodegc and throttle queuing. */
+ int
+ xfs_inodegc_register_shrinker(
+ 	struct xfs_mount	*mp)
+ {
+ 	struct shrinker		*shrink = &mp->m_inodegc_shrinker;
+ 
+ 	shrink->count_objects = xfs_inodegc_shrinker_count;
+ 	shrink->scan_objects = xfs_inodegc_shrinker_scan;
+ 	shrink->seeks = 0;
+ 	shrink->flags = SHRINKER_NONSLAB;
+ 	shrink->batch = XFS_INODEGC_SHRINKER_BATCH;
+ 
+ 	return register_shrinker(shrink, "xfs-inodegc:%s", mp->m_super->s_id);
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
  }
diff --cc include/linux/shrinker.h
index 2de2b9717b12,08e6054e061f..000000000000
--- a/include/linux/shrinker.h
+++ b/include/linux/shrinker.h
@@@ -71,6 -69,15 +71,18 @@@ struct shrinker 
  
  	/* These are for internal use */
  	struct list_head list;
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_MEMCG
+ 	/* ID in shrinker_idr */
+ 	int id;
+ #endif
+ #ifdef CONFIG_SHRINKER_DEBUG
+ 	int debugfs_id;
+ 	const char *name;
+ 	struct dentry *debugfs_entry;
+ #endif
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
  	/* objs pending delete, per node */
  	atomic_long_t *nr_deferred;
  };
@@@ -92,4 -101,24 +106,28 @@@ extern int __printf(2, 3) register_shri
  extern void unregister_shrinker(struct shrinker *shrinker);
  extern void free_prealloced_shrinker(struct shrinker *shrinker);
  extern void synchronize_shrinkers(void);
++<<<<<<< HEAD
 +#endif
++=======
+ 
+ #ifdef CONFIG_SHRINKER_DEBUG
+ extern int shrinker_debugfs_add(struct shrinker *shrinker);
+ extern void shrinker_debugfs_remove(struct shrinker *shrinker);
+ extern int __printf(2, 3) shrinker_debugfs_rename(struct shrinker *shrinker,
+ 						  const char *fmt, ...);
+ #else /* CONFIG_SHRINKER_DEBUG */
+ static inline int shrinker_debugfs_add(struct shrinker *shrinker)
+ {
+ 	return 0;
+ }
+ static inline void shrinker_debugfs_remove(struct shrinker *shrinker)
+ {
+ }
+ static inline __printf(2, 3)
+ int shrinker_debugfs_rename(struct shrinker *shrinker, const char *fmt, ...)
+ {
+ 	return 0;
+ }
+ #endif /* CONFIG_SHRINKER_DEBUG */
+ #endif /* _LINUX_SHRINKER_H */
++>>>>>>> e33c267ab70d (mm: shrinkers: provide shrinkers with names)
* Unmerged path drivers/gpu/drm/panfrost/panfrost_gem_shrinker.c
* Unmerged path fs/erofs/utils.c
* Unmerged path fs/nfsd/filecache.c
* Unmerged path mm/shrinker_debug.c
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 982bacb196ff..34ecd423877a 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -6302,7 +6302,7 @@ int kvm_mmu_vendor_module_init(void)
 	if (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))
 		goto out;
 
-	ret = register_shrinker(&mmu_shrinker);
+	ret = register_shrinker(&mmu_shrinker, "x86-mmu");
 	if (ret)
 		goto out;
 
diff --git a/drivers/android/binder_alloc.c b/drivers/android/binder_alloc.c
index 2628806c64a2..4a8891754965 100644
--- a/drivers/android/binder_alloc.c
+++ b/drivers/android/binder_alloc.c
@@ -1012,7 +1012,7 @@ int binder_alloc_shrinker_init(void)
 	int ret = list_lru_init(&binder_alloc_lru);
 
 	if (ret == 0) {
-		ret = register_shrinker(&binder_shrinker);
+		ret = register_shrinker(&binder_shrinker, "android-binder");
 		if (ret)
 			list_lru_destroy(&binder_alloc_lru);
 	}
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c b/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c
index 5ab136ffdeb2..7214573c2333 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c
@@ -423,7 +423,8 @@ void i915_gem_driver_register__shrinker(struct drm_i915_private *i915)
 	i915->mm.shrinker.count_objects = i915_gem_shrinker_count;
 	i915->mm.shrinker.seeks = DEFAULT_SEEKS;
 	i915->mm.shrinker.batch = 4096;
-	drm_WARN_ON(&i915->drm, register_shrinker(&i915->mm.shrinker));
+	drm_WARN_ON(&i915->drm, register_shrinker(&i915->mm.shrinker,
+						  "drm-i915_gem"));
 
 	i915->mm.oom_notifier.notifier_call = i915_gem_shrinker_oom;
 	drm_WARN_ON(&i915->drm, register_oom_notifier(&i915->mm.oom_notifier));
diff --git a/drivers/gpu/drm/msm/msm_gem_shrinker.c b/drivers/gpu/drm/msm/msm_gem_shrinker.c
index b72d8e6cd51d..816419ec7cea 100644
--- a/drivers/gpu/drm/msm/msm_gem_shrinker.c
+++ b/drivers/gpu/drm/msm/msm_gem_shrinker.c
@@ -151,7 +151,7 @@ void msm_gem_shrinker_init(struct drm_device *dev)
 	priv->shrinker.count_objects = msm_gem_shrinker_count;
 	priv->shrinker.scan_objects = msm_gem_shrinker_scan;
 	priv->shrinker.seeks = DEFAULT_SEEKS;
-	WARN_ON(register_shrinker(&priv->shrinker));
+	WARN_ON(register_shrinker(&priv->shrinker, "drm-msm_gem"));
 
 	priv->vmap_notifier.notifier_call = msm_gem_shrinker_vmap;
 	WARN_ON(register_vmap_purge_notifier(&priv->vmap_notifier));
* Unmerged path drivers/gpu/drm/panfrost/panfrost_gem_shrinker.c
diff --git a/drivers/gpu/drm/ttm/ttm_pool.c b/drivers/gpu/drm/ttm/ttm_pool.c
index 82cbb29a05aa..e34aae6ed4da 100644
--- a/drivers/gpu/drm/ttm/ttm_pool.c
+++ b/drivers/gpu/drm/ttm/ttm_pool.c
@@ -718,7 +718,7 @@ int ttm_pool_mgr_init(unsigned long num_pages)
 	mm_shrinker.count_objects = ttm_pool_shrinker_count;
 	mm_shrinker.scan_objects = ttm_pool_shrinker_scan;
 	mm_shrinker.seeks = 1;
-	return register_shrinker(&mm_shrinker);
+	return register_shrinker(&mm_shrinker, "drm-ttm_pool");
 }
 
 /**
* Unmerged path drivers/md/bcache/btree.c
diff --git a/drivers/md/dm-bufio.c b/drivers/md/dm-bufio.c
index 2433b40fb098..962e4624890c 100644
--- a/drivers/md/dm-bufio.c
+++ b/drivers/md/dm-bufio.c
@@ -1749,7 +1749,8 @@ struct dm_bufio_client *dm_bufio_client_create(struct block_device *bdev, unsign
 	c->shrinker.scan_objects = dm_bufio_shrink_scan;
 	c->shrinker.seeks = 1;
 	c->shrinker.batch = 0;
-	r = register_shrinker(&c->shrinker);
+	r = register_shrinker(&c->shrinker, "md-%s:(%u:%u)", slab_name,
+			      MAJOR(bdev->bd_dev), MINOR(bdev->bd_dev));
 	if (r)
 		goto bad;
 
diff --git a/drivers/md/dm-zoned-metadata.c b/drivers/md/dm-zoned-metadata.c
index b9ca8c031266..70745f046594 100644
--- a/drivers/md/dm-zoned-metadata.c
+++ b/drivers/md/dm-zoned-metadata.c
@@ -2394,7 +2394,9 @@ int dmz_ctr_metadata(struct dmz_dev *dev, struct dmz_metadata **metadata)
 	zmd->mblk_shrinker.seeks = DEFAULT_SEEKS;
 
 	/* Metadata cache shrinker */
-	ret = register_shrinker(&zmd->mblk_shrinker);
+	ret = register_shrinker(&zmd->mblk_shrinker, "md-meta:(%u:%u)",
+				MAJOR(dev->bdev->bd_dev),
+				MINOR(dev->bdev->bd_dev));
 	if (ret) {
 		dmz_dev_err(dev, "Register metadata cache shrinker failed");
 		goto err;
diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index e383ff90328c..a0e19a1bead6 100644
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@ -7639,7 +7639,7 @@ static struct r5conf *setup_conf(struct mddev *mddev)
 	conf->shrinker.count_objects = raid5_cache_count;
 	conf->shrinker.batch = 128;
 	conf->shrinker.flags = 0;
-	ret = register_shrinker(&conf->shrinker);
+	ret = register_shrinker(&conf->shrinker, "md-raid5:%s", mdname(mddev));
 	if (ret) {
 		pr_warn("md/raid:%s: couldn't register shrinker.\n",
 			mdname(mddev));
* Unmerged path drivers/misc/vmw_balloon.c
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index e4bb66f06f19..4a377c434e09 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -896,7 +896,7 @@ static int virtio_balloon_register_shrinker(struct virtio_balloon *vb)
 	vb->shrinker.count_objects = virtio_balloon_shrinker_count;
 	vb->shrinker.seeks = DEFAULT_SEEKS;
 
-	return register_shrinker(&vb->shrinker);
+	return register_shrinker(&vb->shrinker, "virtio-balloon");
 }
 
 static int virtballoon_probe(struct virtio_device *vdev)
* Unmerged path drivers/xen/xenbus/xenbus_probe_backend.c
diff --git a/fs/btrfs/super.c b/fs/btrfs/super.c
index 8f0c882f8d82..cdba01bae056 100644
--- a/fs/btrfs/super.c
+++ b/fs/btrfs/super.c
@@ -1589,6 +1589,8 @@ static struct dentry *btrfs_mount_root(struct file_system_type *fs_type,
 			error = -EBUSY;
 	} else {
 		snprintf(s->s_id, sizeof(s->s_id), "%pg", bdev);
+		shrinker_debugfs_rename(&s->s_shrink, "sb-%s:%s", fs_type->name,
+					s->s_id);
 		btrfs_sb(s)->bdev_holder = fs_type;
 		error = btrfs_fill_super(s, fs_devices, data);
 	}
* Unmerged path fs/erofs/utils.c
diff --git a/fs/ext4/extents_status.c b/fs/ext4/extents_status.c
index 5b24c4852e03..83e1794284fc 100644
--- a/fs/ext4/extents_status.c
+++ b/fs/ext4/extents_status.c
@@ -1276,7 +1276,8 @@ int ext4_es_register_shrinker(struct ext4_sb_info *sbi)
 	sbi->s_es_shrinker.scan_objects = ext4_es_scan;
 	sbi->s_es_shrinker.count_objects = ext4_es_count;
 	sbi->s_es_shrinker.seeks = DEFAULT_SEEKS;
-	err = register_shrinker(&sbi->s_es_shrinker);
+	err = register_shrinker(&sbi->s_es_shrinker, "ext4-es:%s",
+				sbi->s_sb->s_id);
 	if (err)
 		goto err2;
 
* Unmerged path fs/f2fs/super.c
diff --git a/fs/gfs2/glock.c b/fs/gfs2/glock.c
index 10e195ae396c..9f6456147a33 100644
--- a/fs/gfs2/glock.c
+++ b/fs/gfs2/glock.c
@@ -2506,7 +2506,7 @@ int __init gfs2_glock_init(void)
 		return -ENOMEM;
 	}
 
-	ret = register_shrinker(&glock_shrinker);
+	ret = register_shrinker(&glock_shrinker, "gfs2-glock");
 	if (ret) {
 		destroy_workqueue(gfs2_delete_workqueue);
 		destroy_workqueue(glock_workqueue);
diff --git a/fs/gfs2/main.c b/fs/gfs2/main.c
index 5498f4668fa6..10b13ff7b862 100644
--- a/fs/gfs2/main.c
+++ b/fs/gfs2/main.c
@@ -152,7 +152,7 @@ static int __init init_gfs2_fs(void)
 	if (!gfs2_trans_cachep)
 		goto fail_cachep8;
 
-	error = register_shrinker(&gfs2_qd_shrinker);
+	error = register_shrinker(&gfs2_qd_shrinker, "gfs2-qd");
 	if (error)
 		goto fail_shrinker;
 
* Unmerged path fs/jbd2/journal.c
diff --git a/fs/mbcache.c b/fs/mbcache.c
index 081ccf0caee3..82fd94613663 100644
--- a/fs/mbcache.c
+++ b/fs/mbcache.c
@@ -366,7 +366,7 @@ struct mb_cache *mb_cache_create(int bucket_bits)
 	cache->c_shrink.count_objects = mb_cache_count;
 	cache->c_shrink.scan_objects = mb_cache_scan;
 	cache->c_shrink.seeks = DEFAULT_SEEKS;
-	if (register_shrinker(&cache->c_shrink)) {
+	if (register_shrinker(&cache->c_shrink, "mbcache-shrinker")) {
 		kfree(cache->c_hash);
 		kfree(cache);
 		goto err_out;
diff --git a/fs/nfs/nfs42xattr.c b/fs/nfs/nfs42xattr.c
index 1c4d2a05b401..3b741f295bee 100644
--- a/fs/nfs/nfs42xattr.c
+++ b/fs/nfs/nfs42xattr.c
@@ -1018,15 +1018,16 @@ int __init nfs4_xattr_cache_init(void)
 	if (ret)
 		goto out2;
 
-	ret = register_shrinker(&nfs4_xattr_cache_shrinker);
+	ret = register_shrinker(&nfs4_xattr_cache_shrinker, "nfs-xattr_cache");
 	if (ret)
 		goto out1;
 
-	ret = register_shrinker(&nfs4_xattr_entry_shrinker);
+	ret = register_shrinker(&nfs4_xattr_entry_shrinker, "nfs-xattr_entry");
 	if (ret)
 		goto out;
 
-	ret = register_shrinker(&nfs4_xattr_large_entry_shrinker);
+	ret = register_shrinker(&nfs4_xattr_large_entry_shrinker,
+				"nfs-xattr_large_entry");
 	if (!ret)
 		return 0;
 
diff --git a/fs/nfs/super.c b/fs/nfs/super.c
index baa13525501e..9a1bf3d88d04 100644
--- a/fs/nfs/super.c
+++ b/fs/nfs/super.c
@@ -129,7 +129,7 @@ int __init register_nfs_fs(void)
 	ret = nfs_register_sysctl();
 	if (ret < 0)
 		goto error_2;
-	ret = register_shrinker(&acl_shrinker);
+	ret = register_shrinker(&acl_shrinker, "nfs-acl");
 	if (ret < 0)
 		goto error_3;
 	return 0;
* Unmerged path fs/nfsd/filecache.c
diff --git a/fs/nfsd/nfscache.c b/fs/nfsd/nfscache.c
index 5d094982cd07..01bea365bcae 100644
--- a/fs/nfsd/nfscache.c
+++ b/fs/nfsd/nfscache.c
@@ -168,7 +168,8 @@ int nfsd_reply_cache_init(struct nfsd_net *nn)
 	nn->nfsd_reply_cache_shrinker.scan_objects = nfsd_reply_cache_scan;
 	nn->nfsd_reply_cache_shrinker.count_objects = nfsd_reply_cache_count;
 	nn->nfsd_reply_cache_shrinker.seeks = 1;
-	status = register_shrinker(&nn->nfsd_reply_cache_shrinker);
+	status = register_shrinker(&nn->nfsd_reply_cache_shrinker,
+				   "nfsd-reply:%s", nn->nfsd_name);
 	if (status)
 		goto out_nomem;
 
diff --git a/fs/quota/dquot.c b/fs/quota/dquot.c
index d8742b338a35..736b3c4699e5 100644
--- a/fs/quota/dquot.c
+++ b/fs/quota/dquot.c
@@ -2995,7 +2995,7 @@ static int __init dquot_init(void)
 	pr_info("VFS: Dquot-cache hash table entries: %ld (order %ld,"
 		" %ld bytes)\n", nr_hash, order, (PAGE_SIZE << order));
 
-	if (register_shrinker(&dqcache_shrinker))
+	if (register_shrinker(&dqcache_shrinker, "dquota-cache"))
 		panic("Cannot register dquot shrinker");
 
 	return 0;
diff --git a/fs/super.c b/fs/super.c
index 1b312487dfdb..a7be54811280 100644
--- a/fs/super.c
+++ b/fs/super.c
@@ -266,7 +266,7 @@ static struct super_block *alloc_super(struct file_system_type *type, int flags,
 	s->s_shrink.count_objects = super_cache_count;
 	s->s_shrink.batch = 1024;
 	s->s_shrink.flags = SHRINKER_NUMA_AWARE | SHRINKER_MEMCG_AWARE;
-	if (prealloc_shrinker(&s->s_shrink))
+	if (prealloc_shrinker(&s->s_shrink, "sb-%s", type->name))
 		goto fail;
 	if (list_lru_init_memcg(&s->s_dentry_lru, &s->s_shrink))
 		goto fail;
@@ -1339,6 +1339,8 @@ int get_tree_bdev(struct fs_context *fc,
 	} else {
 		s->s_mode = mode;
 		snprintf(s->s_id, sizeof(s->s_id), "%pg", bdev);
+		shrinker_debugfs_rename(&s->s_shrink, "sb-%s:%s",
+					fc->fs_type->name, s->s_id);
 		sb_set_blocksize(s, block_size(bdev));
 		error = fill_super(s, fc);
 		if (error) {
@@ -1414,6 +1416,8 @@ struct dentry *mount_bdev(struct file_system_type *fs_type,
 	} else {
 		s->s_mode = mode;
 		snprintf(s->s_id, sizeof(s->s_id), "%pg", bdev);
+		shrinker_debugfs_rename(&s->s_shrink, "sb-%s:%s",
+					fs_type->name, s->s_id);
 		sb_set_blocksize(s, block_size(bdev));
 		error = fill_super(s, data, flags & SB_SILENT ? 1 : 0);
 		if (error) {
diff --git a/fs/ubifs/super.c b/fs/ubifs/super.c
index 741fe7401b21..f79e9e41118d 100644
--- a/fs/ubifs/super.c
+++ b/fs/ubifs/super.c
@@ -2271,7 +2271,7 @@ static int __init ubifs_init(void)
 	if (!ubifs_inode_slab)
 		return -ENOMEM;
 
-	err = register_shrinker(&ubifs_shrinker_info);
+	err = register_shrinker(&ubifs_shrinker_info, "ubifs-slab");
 	if (err)
 		goto out_slab;
 
diff --git a/fs/xfs/xfs_buf.c b/fs/xfs/xfs_buf.c
index 71c577113d02..978facd877e5 100644
--- a/fs/xfs/xfs_buf.c
+++ b/fs/xfs/xfs_buf.c
@@ -2040,7 +2040,8 @@ xfs_alloc_buftarg(
 	btp->bt_shrinker.scan_objects = xfs_buftarg_shrink_scan;
 	btp->bt_shrinker.seeks = DEFAULT_SEEKS;
 	btp->bt_shrinker.flags = SHRINKER_NUMA_AWARE;
-	if (register_shrinker(&btp->bt_shrinker))
+	if (register_shrinker(&btp->bt_shrinker, "xfs-buf:%s",
+			      mp->m_super->s_id))
 		goto error_pcpu;
 	return btp;
 
* Unmerged path fs/xfs/xfs_icache.c
diff --git a/fs/xfs/xfs_qm.c b/fs/xfs/xfs_qm.c
index 99eb92d3c88e..94eb8e7c97bf 100644
--- a/fs/xfs/xfs_qm.c
+++ b/fs/xfs/xfs_qm.c
@@ -690,7 +690,8 @@ xfs_qm_init_quotainfo(
 	qinf->qi_shrinker.seeks = DEFAULT_SEEKS;
 	qinf->qi_shrinker.flags = SHRINKER_NUMA_AWARE;
 
-	error = register_shrinker(&qinf->qi_shrinker);
+	error = register_shrinker(&qinf->qi_shrinker, "xfs-qm:%s",
+				  mp->m_super->s_id);
 	if (error)
 		goto out_free_inos;
 
* Unmerged path include/linux/shrinker.h
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 4ec6602375c5..bc8d7d656922 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -4704,7 +4704,7 @@ static void __init kfree_rcu_batch_init(void)
 		INIT_DELAYED_WORK(&krcp->page_cache_work, fill_page_cache_func);
 		krcp->initialized = true;
 	}
-	if (register_shrinker(&kfree_rcu_shrinker))
+	if (register_shrinker(&kfree_rcu_shrinker, "rcu-kfree"))
 		pr_err("Failed to register kfree_rcu() shrinker!\n");
 }
 
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 8619d1410f06..1eb73a5ebe2b 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -408,10 +408,10 @@ static int __init hugepage_init(void)
 	if (err)
 		goto err_slab;
 
-	err = register_shrinker(&huge_zero_page_shrinker);
+	err = register_shrinker(&huge_zero_page_shrinker, "thp-zero");
 	if (err)
 		goto err_hzp_shrinker;
-	err = register_shrinker(&deferred_split_shrinker);
+	err = register_shrinker(&deferred_split_shrinker, "thp-deferred_split");
 	if (err)
 		goto err_split_shrinker;
 
* Unmerged path mm/shrinker_debug.c
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 6c6c594da66f..aca67fb6027f 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -572,7 +572,7 @@ static unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,
 /*
  * Add a shrinker callback to be called from the vm.
  */
-int prealloc_shrinker(struct shrinker *shrinker)
+static int __prealloc_shrinker(struct shrinker *shrinker)
 {
 	unsigned int size;
 	int err;
@@ -596,8 +596,36 @@ int prealloc_shrinker(struct shrinker *shrinker)
 	return 0;
 }
 
+#ifdef CONFIG_SHRINKER_DEBUG
+int prealloc_shrinker(struct shrinker *shrinker, const char *fmt, ...)
+{
+	va_list ap;
+	int err;
+
+	va_start(ap, fmt);
+	shrinker->name = kvasprintf_const(GFP_KERNEL, fmt, ap);
+	va_end(ap);
+	if (!shrinker->name)
+		return -ENOMEM;
+
+	err = __prealloc_shrinker(shrinker);
+	if (err)
+		kfree_const(shrinker->name);
+
+	return err;
+}
+#else
+int prealloc_shrinker(struct shrinker *shrinker, const char *fmt, ...)
+{
+	return __prealloc_shrinker(shrinker);
+}
+#endif
+
 void free_prealloced_shrinker(struct shrinker *shrinker)
 {
+#ifdef CONFIG_SHRINKER_DEBUG
+	kfree_const(shrinker->name);
+#endif
 	if (shrinker->flags & SHRINKER_MEMCG_AWARE) {
 		down_write(&shrinker_rwsem);
 		unregister_memcg_shrinker(shrinker);
@@ -617,15 +645,39 @@ void register_shrinker_prepared(struct shrinker *shrinker)
 	up_write(&shrinker_rwsem);
 }
 
-int register_shrinker(struct shrinker *shrinker)
+static int __register_shrinker(struct shrinker *shrinker)
 {
-	int err = prealloc_shrinker(shrinker);
+	int err = __prealloc_shrinker(shrinker);
 
 	if (err)
 		return err;
 	register_shrinker_prepared(shrinker);
 	return 0;
 }
+
+#ifdef CONFIG_SHRINKER_DEBUG
+int register_shrinker(struct shrinker *shrinker, const char *fmt, ...)
+{
+	va_list ap;
+	int err;
+
+	va_start(ap, fmt);
+	shrinker->name = kvasprintf_const(GFP_KERNEL, fmt, ap);
+	va_end(ap);
+	if (!shrinker->name)
+		return -ENOMEM;
+
+	err = __register_shrinker(shrinker);
+	if (err)
+		kfree_const(shrinker->name);
+	return err;
+}
+#else
+int register_shrinker(struct shrinker *shrinker, const char *fmt, ...)
+{
+	return __register_shrinker(shrinker);
+}
+#endif
 EXPORT_SYMBOL(register_shrinker);
 
 /*
diff --git a/mm/workingset.c b/mm/workingset.c
index 2ad3de738d84..254e78c1a02b 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -613,7 +613,7 @@ static int __init workingset_init(void)
 	pr_info("workingset: timestamp_bits=%d max_order=%d bucket_order=%u\n",
 	       timestamp_bits, max_order, bucket_order);
 
-	ret = prealloc_shrinker(&workingset_shadow_shrinker);
+	ret = prealloc_shrinker(&workingset_shadow_shrinker, "mm-shadow");
 	if (ret)
 		goto err;
 	ret = __list_lru_init(&shadow_nodes, true, &shadow_nodes_key,
diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
index a925d2bfd8b1..03fdbc5cb59c 100644
--- a/mm/zsmalloc.c
+++ b/mm/zsmalloc.c
@@ -2414,7 +2414,8 @@ static int zs_register_shrinker(struct zs_pool *pool)
 	pool->shrinker.batch = 0;
 	pool->shrinker.seeks = DEFAULT_SEEKS;
 
-	return register_shrinker(&pool->shrinker);
+	return register_shrinker(&pool->shrinker, "mm-zspool:%s",
+				 pool->name);
 }
 
 /**
diff --git a/net/sunrpc/auth.c b/net/sunrpc/auth.c
index 68c1b801d281..29a40552273e 100644
--- a/net/sunrpc/auth.c
+++ b/net/sunrpc/auth.c
@@ -869,7 +869,7 @@ int __init rpcauth_init_module(void)
 	err = rpc_init_authunix();
 	if (err < 0)
 		goto out1;
-	err = register_shrinker(&rpc_cred_shrinker);
+	err = register_shrinker(&rpc_cred_shrinker, "sunrpc_cred");
 	if (err < 0)
 		goto out2;
 	return 0;
