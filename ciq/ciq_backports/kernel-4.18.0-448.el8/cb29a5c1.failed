sched/numa: Apply imbalance limitations consistently

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Mel Gorman <mgorman@techsingularity.net>
commit cb29a5c19d2d68afc641fb1949e1a1c565b582ea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/cb29a5c1.failed

The imbalance limitations are applied inconsistently at fork time
and at runtime. At fork, a new task can remain local until there are
too many running tasks even if the degree of imbalance is larger than
NUMA_IMBALANCE_MIN which is different to runtime. Secondly, the imbalance
figure used during load balancing is different to the one used at NUMA
placement. Load balancing uses the number of tasks that must move to
restore imbalance where as NUMA balancing uses the total imbalance.

In combination, it is possible for a parallel workload that uses a small
number of CPUs without applying scheduler policies to have very variable
run-to-run performance.

[lkp@intel.com: Fix build breakage for arc-allyesconfig]

	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Tested-by: K Prateek Nayak <kprateek.nayak@amd.com>
Link: https://lore.kernel.org/r/20220520103519.1863-4-mgorman@techsingularity.net
(cherry picked from commit cb29a5c19d2d68afc641fb1949e1a1c565b582ea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/fair.c
diff --cc kernel/sched/fair.c
index 7dfae91de29b,166f5f9bdb4f..000000000000
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@@ -1552,9 -1575,6 +1579,12 @@@ struct task_numa_env 
  
  static unsigned long cpu_load(struct rq *rq);
  static unsigned long cpu_runnable(struct rq *rq);
++<<<<<<< HEAD
 +static unsigned long cpu_util(int cpu);
 +static inline long adjust_numa_imbalance(int imbalance,
 +					int dst_running, int imb_numa_nr);
++=======
++>>>>>>> cb29a5c19d2d (sched/numa: Apply imbalance limitations consistently)
  
  static inline enum
  numa_type numa_classify(unsigned int imbalance_pct,
@@@ -9241,28 -9322,8 +9267,10 @@@ next_group
  		WRITE_ONCE(rd->overutilized, SG_OVERUTILIZED);
  		trace_sched_overutilized_tp(rd, SG_OVERUTILIZED);
  	}
 +
 +	update_idle_cpu_scan(env, sum_util);
  }
  
- #define NUMA_IMBALANCE_MIN 2
- 
- static inline long adjust_numa_imbalance(int imbalance,
- 				int dst_running, int imb_numa_nr)
- {
- 	if (!allow_numa_imbalance(dst_running, imb_numa_nr))
- 		return imbalance;
- 
- 	/*
- 	 * Allow a small imbalance based on a simple pair of communicating
- 	 * tasks that remain local when the destination is lightly loaded.
- 	 */
- 	if (imbalance <= NUMA_IMBALANCE_MIN)
- 		return 0;
- 
- 	return imbalance;
- }
- 
  /**
   * calculate_imbalance - Calculate the amount of imbalance present within the
   *			 groups of a given sched_domain during load balance.
* Unmerged path kernel/sched/fair.c
