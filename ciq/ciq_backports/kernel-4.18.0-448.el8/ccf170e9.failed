perf/x86/intel: Fix PEBS data source encoding for ADL

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Kan Liang <kan.liang@linux.intel.com>
commit ccf170e9d8fdacfe435bbe3749c897c7d86d32f8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/ccf170e9.failed

The PEBS data source encoding for the e-core is different from the
p-core.

Add the pebs_data_source[] in the struct x86_hybrid_pmu to store the
data source encoding for each type of the core.

Add intel_pmu_pebs_data_source_grt() for the e-core.
There is nothing changed for the data source encoding of the p-core,
which still reuse the intel_pmu_pebs_data_source_skl().

Fixes: f83d2f91d259 ("perf/x86/intel: Add Alder Lake Hybrid support")
	Signed-off-by: Kan Liang <kan.liang@linux.intel.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Andi Kleen <ak@linux.intel.com>
Link: https://lkml.kernel.org/r/20220629150840.2235741-2-kan.liang@linux.intel.com
(cherry picked from commit ccf170e9d8fdacfe435bbe3749c897c7d86d32f8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/intel/core.c
#	arch/x86/events/intel/ds.c
#	arch/x86/events/perf_event.h
diff --cc arch/x86/events/intel/core.c
index 0b9951a3cc49,bd8b98857609..000000000000
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@@ -6155,7 -6241,8 +6155,12 @@@ __init int intel_pmu_init(void
  		x86_pmu.flags |= PMU_FL_INSTR_LATENCY;
  		x86_pmu.flags |= PMU_FL_MEM_LOADS_AUX;
  		x86_pmu.lbr_pt_coexist = true;
++<<<<<<< HEAD
 +		intel_pmu_pebs_data_source_skl(false);
++=======
+ 		intel_pmu_pebs_data_source_adl();
+ 		x86_pmu.pebs_latency_data = adl_latency_data_small;
++>>>>>>> ccf170e9d8fd (perf/x86/intel: Fix PEBS data source encoding for ADL)
  		x86_pmu.num_topdown_events = 8;
  		x86_pmu.update_topdown_event = adl_update_topdown_event;
  		x86_pmu.set_topdown_event_period = adl_set_topdown_event_period;
diff --cc arch/x86/events/intel/ds.c
index 381e759e2a07,ba60427caa6d..000000000000
--- a/arch/x86/events/intel/ds.c
+++ b/arch/x86/events/intel/ds.c
@@@ -171,7 -196,50 +196,54 @@@ static u64 precise_datala_hsw(struct pe
  	return dse.val;
  }
  
++<<<<<<< HEAD
 +static u64 load_latency_data(u64 status)
++=======
+ static inline void pebs_set_tlb_lock(u64 *val, bool tlb, bool lock)
+ {
+ 	/*
+ 	 * TLB access
+ 	 * 0 = did not miss 2nd level TLB
+ 	 * 1 = missed 2nd level TLB
+ 	 */
+ 	if (tlb)
+ 		*val |= P(TLB, MISS) | P(TLB, L2);
+ 	else
+ 		*val |= P(TLB, HIT) | P(TLB, L1) | P(TLB, L2);
+ 
+ 	/* locked prefix */
+ 	if (lock)
+ 		*val |= P(LOCK, LOCKED);
+ }
+ 
+ /* Retrieve the latency data for e-core of ADL */
+ u64 adl_latency_data_small(struct perf_event *event, u64 status)
+ {
+ 	union intel_x86_pebs_dse dse;
+ 	u64 val;
+ 
+ 	WARN_ON_ONCE(hybrid_pmu(event->pmu)->cpu_type == hybrid_big);
+ 
+ 	dse.val = status;
+ 
+ 	val = hybrid_var(event->pmu, pebs_data_source)[dse.ld_dse];
+ 
+ 	/*
+ 	 * For the atom core on ADL,
+ 	 * bit 4: lock, bit 5: TLB access.
+ 	 */
+ 	pebs_set_tlb_lock(&val, dse.ld_locked, dse.ld_stlb_miss);
+ 
+ 	if (dse.ld_data_blk)
+ 		val |= P(BLK, DATA);
+ 	else
+ 		val |= P(BLK, NA);
+ 
+ 	return val;
+ }
+ 
+ static u64 load_latency_data(struct perf_event *event, u64 status)
++>>>>>>> ccf170e9d8fd (perf/x86/intel: Fix PEBS data source encoding for ADL)
  {
  	union intel_x86_pebs_dse dse;
  	u64 val;
@@@ -243,23 -298,9 +315,23 @@@ static u64 store_latency_data(struct pe
  	/*
  	 * use the mapping table for bit 0-3
  	 */
- 	val = pebs_data_source[dse.st_lat_dse];
+ 	val = hybrid_var(event->pmu, pebs_data_source)[dse.st_lat_dse];
  
 -	pebs_set_tlb_lock(&val, dse.st_lat_stlb_miss, dse.st_lat_locked);
 +	/*
 +	 * bit 4: TLB access
 +	 * 0 = did not miss 2nd level TLB
 +	 * 1 = missed 2nd level TLB
 +	 */
 +	if (dse.st_lat_stlb_miss)
 +		val |= P(TLB, MISS) | P(TLB, L2);
 +	else
 +		val |= P(TLB, HIT) | P(TLB, L1) | P(TLB, L2);
 +
 +	/*
 +	 * bit 5: locked prefix
 +	 */
 +	if (dse.st_lat_locked)
 +		val |= P(LOCK, LOCKED);
  
  	val |= P(BLK, NA);
  
@@@ -1435,9 -1484,11 +1507,15 @@@ static u64 get_data_src(struct perf_eve
  	bool fst = fl & (PERF_X86_EVENT_PEBS_ST | PERF_X86_EVENT_PEBS_HSW_PREC);
  
  	if (fl & PERF_X86_EVENT_PEBS_LDLAT)
- 		val = load_latency_data(aux);
+ 		val = load_latency_data(event, aux);
  	else if (fl & PERF_X86_EVENT_PEBS_STLAT)
++<<<<<<< HEAD
 +		val = store_latency_data(aux);
++=======
+ 		val = store_latency_data(event, aux);
+ 	else if (fl & PERF_X86_EVENT_PEBS_LAT_HYBRID)
+ 		val = x86_pmu.pebs_latency_data(event, aux);
++>>>>>>> ccf170e9d8fd (perf/x86/intel: Fix PEBS data source encoding for ADL)
  	else if (fst && (fl & PERF_X86_EVENT_PEBS_HSW_PREC))
  		val = precise_datala_hsw(event, aux);
  	else if (fst)
diff --cc arch/x86/events/perf_event.h
index ec6b1d551e32,ca2f8bfe6ff1..000000000000
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@@ -660,7 -671,10 +662,14 @@@ struct x86_hybrid_pmu 
  	struct extra_reg		*extra_regs;
  
  	unsigned int			late_ack	:1,
++<<<<<<< HEAD
 +					mid_ack		:1;
++=======
+ 					mid_ack		:1,
+ 					enabled_ack	:1;
+ 
+ 	u64				pebs_data_source[PERF_PEBS_DATA_SOURCE_MAX];
++>>>>>>> ccf170e9d8fd (perf/x86/intel: Fix PEBS data source encoding for ADL)
  };
  
  static __always_inline struct x86_hybrid_pmu *hybrid_pmu(struct pmu *pmu)
* Unmerged path arch/x86/events/intel/core.c
* Unmerged path arch/x86/events/intel/ds.c
* Unmerged path arch/x86/events/perf_event.h
