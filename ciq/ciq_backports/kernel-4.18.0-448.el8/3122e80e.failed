mm/vma: make vma_is_accessible() available for general use

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Anshuman Khandual <anshuman.khandual@arm.com>
commit 3122e80efc0faf4a2accba7a46c7ed795edbfded
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/3122e80e.failed

Lets move vma_is_accessible() helper to include/linux/mm.h which makes it
available for general use.  While here, this replaces all remaining open
encodings for VMA access check with vma_is_accessible().

	Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
	Acked-by: Guo Ren <guoren@kernel.org>
	Acked-by: Vlastimil Babka <vbabka@suse.cz>
	Cc: Guo Ren <guoren@kernel.org>
	Cc: Geert Uytterhoeven <geert@linux-m68k.org>
	Cc: Ralf Baechle <ralf@linux-mips.org>
	Cc: Paul Burton <paulburton@kernel.org>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Paul Mackerras <paulus@samba.org>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
	Cc: Rich Felker <dalias@libc.org>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Steven Rostedt <rostedt@goodmis.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Alexander Viro <viro@zeniv.linux.org.uk>
	Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
	Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
	Cc: Arnd Bergmann <arnd@arndb.de>
	Cc: Nick Piggin <npiggin@gmail.com>
	Cc: Paul Mackerras <paulus@ozlabs.org>
	Cc: Will Deacon <will@kernel.org>
Link: http://lkml.kernel.org/r/1582520593-30704-3-git-send-email-anshuman.khandual@arm.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 3122e80efc0faf4a2accba7a46c7ed795edbfded)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/csky/mm/fault.c
* Unmerged path arch/csky/mm/fault.c
* Unmerged path arch/csky/mm/fault.c
diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c
index 9b6163c05a75..5f3b0d3e91fd 100644
--- a/arch/m68k/mm/fault.c
+++ b/arch/m68k/mm/fault.c
@@ -125,7 +125,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,
 		case 1:		/* read, present */
 			goto acc_err;
 		case 0:		/* read, not present */
-			if (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))
+			if (!vma_is_accessible(vma))
 				goto acc_err;
 	}
 
diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
index c6c649690877..ec2580047674 100644
--- a/arch/mips/mm/fault.c
+++ b/arch/mips/mm/fault.c
@@ -142,7 +142,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,
 				goto bad_area;
 			}
 		} else {
-			if (!(vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))
+			if (!vma_is_accessible(vma))
 				goto bad_area;
 		}
 	}
diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
index 8dce601f125c..177df782f667 100644
--- a/arch/powerpc/mm/fault.c
+++ b/arch/powerpc/mm/fault.c
@@ -357,7 +357,7 @@ static bool access_error(bool is_write, bool is_exec, struct vm_area_struct *vma
 		return false;
 	}
 
-	if (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))
+	if (unlikely(!vma_is_accessible(vma)))
 		return true;
 	/*
 	 * We should ideally do the vma pkey access check here. But in the
diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c
index 6defd2c6d9b1..a6204b0b1a3e 100644
--- a/arch/sh/mm/fault.c
+++ b/arch/sh/mm/fault.c
@@ -372,7 +372,7 @@ static inline int access_error(int error_code, struct vm_area_struct *vma)
 		return 1;
 
 	/* read, not present: */
-	if (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))
+	if (unlikely(!vma_is_accessible(vma)))
 		return 1;
 
 	return 0;
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index b63f70b6ee4c..3e281d77a888 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1197,7 +1197,7 @@ access_error(unsigned long error_code, struct vm_area_struct *vma)
 		return 1;
 
 	/* read, not present: */
-	if (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))
+	if (unlikely(!vma_is_accessible(vma)))
 		return 1;
 
 	return 0;
diff --git a/include/linux/mm.h b/include/linux/mm.h
index e3b1a16638a0..92561b91b63a 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -611,6 +611,12 @@ static inline bool vma_is_foreign(struct vm_area_struct *vma)
 
 	return false;
 }
+
+static inline bool vma_is_accessible(struct vm_area_struct *vma)
+{
+	return vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC);
+}
+
 #ifdef CONFIG_SHMEM
 /*
  * The vma_is_shmem is not inline because it is used only by slow
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 672d98723c8b..7f91cfb3bf1e 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2817,7 +2817,7 @@ static void task_numa_work(struct callback_head *work)
 		 * Skip inaccessible VMAs to avoid any confusion between
 		 * PROT_NONE and NUMA hinting ptes
 		 */
-		if (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))
+		if (!vma_is_accessible(vma))
 			continue;
 
 		do {
diff --git a/mm/gup.c b/mm/gup.c
index b6ca92e3db6b..a10546ce81cb 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1663,7 +1663,7 @@ long populate_vma_page_range(struct vm_area_struct *vma,
 	 * We want mlock to succeed for regions that have any permissions
 	 * other than PROT_NONE.
 	 */
-	if (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))
+	if (vma_is_accessible(vma))
 		gup_flags |= FOLL_FORCE;
 
 	/*
diff --git a/mm/memory.c b/mm/memory.c
index d3790173a23c..16ee97a8959d 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4160,11 +4160,6 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)
 	return VM_FAULT_FALLBACK;
 }
 
-static inline bool vma_is_accessible(struct vm_area_struct *vma)
-{
-	return vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE);
-}
-
 static vm_fault_t create_huge_pud(struct vm_fault *vmf)
 {
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &&			\
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 5a02d3a08bf2..6ec10bb138a8 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -679,8 +679,7 @@ static int queue_pages_test_walk(unsigned long start, unsigned long end,
 
 	if (flags & MPOL_MF_LAZY) {
 		/* Similar to task_numa_work, skip inaccessible VMAs */
-		if (!is_vm_hugetlb_page(vma) &&
-			(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)) &&
+		if (!is_vm_hugetlb_page(vma) && vma_is_accessible(vma) &&
 			!(vma->vm_flags & VM_MIXEDMAP))
 			change_prot_numa(vma, start, endvma);
 		return 1;
diff --git a/mm/mmap.c b/mm/mmap.c
index 878265a8a575..462c2a7fd7db 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -2347,8 +2347,7 @@ int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 		gap_addr = TASK_SIZE;
 
 	next = vma->vm_next;
-	if (next && next->vm_start < gap_addr &&
-			(next->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {
+	if (next && next->vm_start < gap_addr && vma_is_accessible(next)) {
 		if (!(next->vm_flags & VM_GROWSUP))
 			return -ENOMEM;
 		/* Check that both stack segments have the same anon_vma? */
@@ -2429,7 +2428,7 @@ int expand_downwards(struct vm_area_struct *vma,
 	prev = vma->vm_prev;
 	/* Check that both stack segments have the same anon_vma? */
 	if (prev && !(prev->vm_flags & VM_GROWSDOWN) &&
-			(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {
+			vma_is_accessible(prev)) {
 		if (address - prev->vm_end < stack_guard_gap)
 			return -ENOMEM;
 	}
