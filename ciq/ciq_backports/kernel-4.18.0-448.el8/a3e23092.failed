x86: centralize setting SWIOTLB_FORCE when guest memory encryption is enabled

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Christoph Hellwig <hch@lst.de>
commit a3e230926708125205ffd06d3dc2175a8263ae7e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/a3e23092.failed

Move enabling SWIOTLB_FORCE for guest memory encryption into common code.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
	Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
(cherry picked from commit a3e230926708125205ffd06d3dc2175a8263ae7e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/pci-dma.c
#	arch/x86/mm/mem_encrypt_amd.c
diff --cc arch/x86/kernel/pci-dma.c
index 228aa8ada80f,04140e20ef1a..000000000000
--- a/arch/x86/kernel/pci-dma.c
+++ b/arch/x86/kernel/pci-dma.c
@@@ -35,7 -37,78 +35,82 @@@ int no_iommu __read_mostly
  /* Set this to 1 if there is a HW IOMMU in the system */
  int iommu_detected __read_mostly = 0;
  
++<<<<<<< HEAD
 +extern struct iommu_table_entry __iommu_table[], __iommu_table_end[];
++=======
+ #ifdef CONFIG_SWIOTLB
+ bool x86_swiotlb_enable;
+ 
+ static void __init pci_swiotlb_detect(void)
+ {
+ 	/* don't initialize swiotlb if iommu=off (no_iommu=1) */
+ 	if (!no_iommu && max_possible_pfn > MAX_DMA32_PFN)
+ 		x86_swiotlb_enable = true;
+ 
+ 	/*
+ 	 * Set swiotlb to 1 so that bounce buffers are allocated and used for
+ 	 * devices that can't support DMA to encrypted memory.
+ 	 */
+ 	if (cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT))
+ 		x86_swiotlb_enable = true;
+ 
+ 	/*
+ 	 * Guest with guest memory encryption currently perform all DMA through
+ 	 * bounce buffers as the hypervisor can't access arbitrary VM memory
+ 	 * that is not explicitly shared with it.
+ 	 */
+ 	if (cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT))
+ 		swiotlb_force = SWIOTLB_FORCE;
+ 
+ 	if (swiotlb_force == SWIOTLB_FORCE)
+ 		x86_swiotlb_enable = true;
+ }
+ #else
+ static inline void __init pci_swiotlb_detect(void)
+ {
+ }
+ #endif /* CONFIG_SWIOTLB */
+ 
+ #ifdef CONFIG_SWIOTLB_XEN
+ static bool xen_swiotlb;
+ 
+ static void __init pci_xen_swiotlb_init(void)
+ {
+ 	if (!xen_initial_domain() && !x86_swiotlb_enable &&
+ 	    swiotlb_force != SWIOTLB_FORCE)
+ 		return;
+ 	x86_swiotlb_enable = true;
+ 	xen_swiotlb = true;
+ 	xen_swiotlb_init_early();
+ 	dma_ops = &xen_swiotlb_dma_ops;
+ 	if (IS_ENABLED(CONFIG_PCI))
+ 		pci_request_acs();
+ }
+ 
+ int pci_xen_swiotlb_init_late(void)
+ {
+ 	int rc;
+ 
+ 	if (xen_swiotlb)
+ 		return 0;
+ 
+ 	rc = xen_swiotlb_init();
+ 	if (rc)
+ 		return rc;
+ 
+ 	/* XXX: this switches the dma ops under live devices! */
+ 	dma_ops = &xen_swiotlb_dma_ops;
+ 	if (IS_ENABLED(CONFIG_PCI))
+ 		pci_request_acs();
+ 	return 0;
+ }
+ EXPORT_SYMBOL_GPL(pci_xen_swiotlb_init_late);
+ #else
+ static inline void __init pci_xen_swiotlb_init(void)
+ {
+ }
+ #endif /* CONFIG_SWIOTLB_XEN */
++>>>>>>> a3e230926708 (x86: centralize setting SWIOTLB_FORCE when guest memory encryption is enabled)
  
  void __init pci_iommu_alloc(void)
  {
* Unmerged path arch/x86/mm/mem_encrypt_amd.c
diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c
index 3be07ac0ced7..f2196227e101 100644
--- a/arch/x86/kernel/cpu/mshyperv.c
+++ b/arch/x86/kernel/cpu/mshyperv.c
@@ -319,14 +319,6 @@ static void __init ms_hyperv_init_platform(void)
 			swiotlb_unencrypted_base = ms_hyperv.shared_gpa_boundary;
 #endif
 		}
-
-#ifdef CONFIG_SWIOTLB
-		/*
-		 * Enable swiotlb force mode in Isolation VM to
-		 * use swiotlb bounce buffer for dma transaction.
-		 */
-		swiotlb_force = SWIOTLB_FORCE;
-#endif
 		/* Isolation VMs are unenlightened SEV-based VMs, thus this check: */
 		if (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT)) {
 			if (hv_get_isolation_type() != HV_ISOLATION_TYPE_NONE)
* Unmerged path arch/x86/kernel/pci-dma.c
* Unmerged path arch/x86/mm/mem_encrypt_amd.c
