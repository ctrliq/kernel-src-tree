gve: Implement packet continuation for RX.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author David Awogbemila <awogbemila@google.com>
commit 37149e9374bf7271563f7477ace9014ebc65a8af
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/37149e93.failed

This enables the driver to receive RX packets spread across multiple
buffers:

For a given multi-fragment packet the "packet continuation" bit is set
on all descriptors except the last one. These descriptors' payloads are
combined into a single SKB before the SKB is handed to the
networking stack.

This change adds a "packet buffer size" notion for RX queues. The
CreateRxQueue AdminQueue command sent to the device now includes the
packet_buffer_size.

We opt for a packet_buffer_size of PAGE_SIZE / 2 to give the
driver the opportunity to flip pages where we can instead of copying.

	Signed-off-by: David Awogbemila <awogbemila@google.com>
	Signed-off-by: Jeroen de Borst <jeroendb@google.com>
	Reviewed-by: Catherine Sullivan <csully@google.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 37149e9374bf7271563f7477ace9014ebc65a8af)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/google/gve/gve.h
#	drivers/net/ethernet/google/gve/gve_adminq.c
#	drivers/net/ethernet/google/gve/gve_main.c
#	drivers/net/ethernet/google/gve/gve_rx.c
#	drivers/net/ethernet/google/gve/gve_rx_dqo.c
diff --cc drivers/net/ethernet/google/gve/gve.h
index 4775214c4d98,b719f72281c4..000000000000
--- a/drivers/net/ethernet/google/gve/gve.h
+++ b/drivers/net/ethernet/google/gve/gve.h
@@@ -135,6 -142,19 +135,22 @@@ struct gve_index_list 
  	s16 tail;
  };
  
++<<<<<<< HEAD
++=======
+ /* A single received packet split across multiple buffers may be
+  * reconstructed using the information in this structure.
+  */
+ struct gve_rx_ctx {
+ 	/* head and tail of skb chain for the current packet or NULL if none */
+ 	struct sk_buff *skb_head;
+ 	struct sk_buff *skb_tail;
+ 	u16 total_expected_size;
+ 	u8 expected_frag_cnt;
+ 	u8 curr_frag_cnt;
+ 	u8 reuse_frags;
+ };
+ 
++>>>>>>> 37149e9374bf (gve: Implement packet continuation for RX.)
  /* Contains datapath state used to represent an RX queue. */
  struct gve_rx_ring {
  	struct gve_priv *gve;
diff --cc drivers/net/ethernet/google/gve/gve_adminq.c
index 69a76038cb46,3dfda6da6a96..000000000000
--- a/drivers/net/ethernet/google/gve/gve_adminq.c
+++ b/drivers/net/ethernet/google/gve/gve_adminq.c
@@@ -506,20 -516,37 +506,46 @@@ static int gve_adminq_create_rx_queue(s
  	cmd.opcode = cpu_to_be32(GVE_ADMINQ_CREATE_RX_QUEUE);
  	cmd.create_rx_queue = (struct gve_adminq_create_rx_queue) {
  		.queue_id = cpu_to_be32(queue_index),
 +		.index = cpu_to_be32(queue_index),
 +		.reserved = 0,
  		.ntfy_id = cpu_to_be32(rx->ntfy_id),
  		.queue_resources_addr = cpu_to_be64(rx->q_resources_bus),
 +		.rx_desc_ring_addr = cpu_to_be64(rx->desc.bus),
 +		.rx_data_ring_addr = cpu_to_be64(rx->data.data_bus),
 +		.queue_page_list_id = cpu_to_be32(qpl_id),
  	};
  
 -	if (gve_is_gqi(priv)) {
 -		u32 qpl_id = priv->queue_format == GVE_GQI_RDA_FORMAT ?
 -			GVE_RAW_ADDRESSING_QPL_ID : rx->data.qpl->id;
 +	err = gve_adminq_issue_cmd(priv, &cmd);
 +	if (err)
 +		return err;
  
++<<<<<<< HEAD
 +	return 0;
++=======
+ 		cmd.create_rx_queue.rx_desc_ring_addr =
+ 			cpu_to_be64(rx->desc.bus),
+ 		cmd.create_rx_queue.rx_data_ring_addr =
+ 			cpu_to_be64(rx->data.data_bus),
+ 		cmd.create_rx_queue.index = cpu_to_be32(queue_index);
+ 		cmd.create_rx_queue.queue_page_list_id = cpu_to_be32(qpl_id);
+ 		cmd.create_rx_queue.packet_buffer_size = cpu_to_be16(rx->packet_buffer_size);
+ 	} else {
+ 		cmd.create_rx_queue.rx_ring_size =
+ 			cpu_to_be16(priv->rx_desc_cnt);
+ 		cmd.create_rx_queue.rx_desc_ring_addr =
+ 			cpu_to_be64(rx->dqo.complq.bus);
+ 		cmd.create_rx_queue.rx_data_ring_addr =
+ 			cpu_to_be64(rx->dqo.bufq.bus);
+ 		cmd.create_rx_queue.packet_buffer_size =
+ 			cpu_to_be16(priv->data_buffer_size_dqo);
+ 		cmd.create_rx_queue.rx_buff_ring_size =
+ 			cpu_to_be16(priv->options_dqo_rda.rx_buff_ring_entries);
+ 		cmd.create_rx_queue.enable_rsc =
+ 			!!(priv->dev->features & NETIF_F_LRO);
+ 	}
+ 
+ 	return gve_adminq_issue_cmd(priv, &cmd);
++>>>>>>> 37149e9374bf (gve: Implement packet continuation for RX.)
  }
  
  int gve_adminq_create_rx_queues(struct gve_priv *priv, u32 num_queues)
diff --cc drivers/net/ethernet/google/gve/gve_main.c
index 13afa7a04d60,6b02ef432eda..000000000000
--- a/drivers/net/ethernet/google/gve/gve_main.c
+++ b/drivers/net/ethernet/google/gve/gve_main.c
@@@ -1155,14 -1371,6 +1155,17 @@@ static int gve_init_priv(struct gve_pri
  			"Could not get device information: err=%d\n", err);
  		goto err;
  	}
++<<<<<<< HEAD
 +	if (priv->dev->max_mtu > PAGE_SIZE) {
 +		priv->dev->max_mtu = PAGE_SIZE;
 +		err = gve_adminq_set_mtu(priv, priv->dev->mtu);
 +		if (err) {
 +			dev_err(&priv->pdev->dev, "Could not set mtu");
 +			goto err;
 +		}
 +	}
++=======
++>>>>>>> 37149e9374bf (gve: Implement packet continuation for RX.)
  	priv->dev->mtu = priv->dev->max_mtu;
  	num_ntfy = pci_msix_vec_count(priv->pdev);
  	if (num_ntfy <= 0) {
diff --cc drivers/net/ethernet/google/gve/gve_rx.c
index d81effa5cf87,c8500babbd1d..000000000000
--- a/drivers/net/ethernet/google/gve/gve_rx.c
+++ b/drivers/net/ethernet/google/gve/gve_rx.c
@@@ -471,10 -611,24 +611,24 @@@ static bool gve_rx(struct gve_rx_ring *
  		napi_gro_frags(napi);
  	else
  		napi_gro_receive(napi, skb);
+ 
+ 	gve_rx_ctx_clear(ctx);
  	return true;
+ 
+ skb_alloc_fail:
+ 	if (napi->skb)
+ 		napi_free_frags(napi);
+ 	*packet_size_bytes = 0;
+ 	*work_done = ctx->expected_frag_cnt;
+ 	while (ctx->curr_frag_cnt < ctx->expected_frag_cnt) {
+ 		rx->cnt++;
+ 		ctx->curr_frag_cnt++;
+ 	}
+ 	gve_rx_ctx_clear(ctx);
+ 	return false;
  }
  
 -bool gve_rx_work_pending(struct gve_rx_ring *rx)
 +static bool gve_rx_work_pending(struct gve_rx_ring *rx)
  {
  	struct gve_rx_desc *desc;
  	__be16 flags_seq;
@@@ -547,19 -700,20 +700,20 @@@ static bool gve_rx_refill_buffers(struc
  	return true;
  }
  
 -static int gve_clean_rx_done(struct gve_rx_ring *rx, int budget,
 -			     netdev_features_t feat)
 +bool gve_clean_rx_done(struct gve_rx_ring *rx, int budget,
 +		       netdev_features_t feat)
  {
+ 	u32 work_done = 0, total_packet_cnt = 0, ok_packet_cnt = 0;
  	struct gve_priv *priv = rx->gve;
- 	u32 work_done = 0, packets = 0;
+ 	u32 idx = rx->cnt & rx->mask;
  	struct gve_rx_desc *desc;
- 	u32 cnt = rx->cnt;
- 	u32 idx = cnt & rx->mask;
  	u64 bytes = 0;
  
- 	desc = rx->desc.desc_ring + idx;
+ 	desc = &rx->desc.desc_ring[idx];
  	while ((GVE_SEQNO(desc->flags_seq) == rx->desc.seqno) &&
  	       work_done < budget) {
+ 		u64 packet_size_bytes = 0;
+ 		u32 work_cnt = 0;
  		bool dropped;
  
  		netif_info(priv, rx_status, priv->dev,
@@@ -570,29 -724,26 +724,37 @@@
  			   rx->q_num, GVE_SEQNO(desc->flags_seq),
  			   rx->desc.seqno);
  
- 		/* prefetch two descriptors ahead */
- 		prefetch(rx->desc.desc_ring + ((cnt + 2) & rx->mask));
- 
- 		dropped = !gve_rx(rx, desc, feat, idx);
+ 		dropped = !gve_rx(rx, feat, &packet_size_bytes, &work_cnt);
  		if (!dropped) {
- 			bytes += be16_to_cpu(desc->len) - GVE_RX_PAD;
- 			packets++;
+ 			bytes += packet_size_bytes;
+ 			ok_packet_cnt++;
  		}
- 		cnt++;
- 		idx = cnt & rx->mask;
- 		desc = rx->desc.desc_ring + idx;
- 		rx->desc.seqno = gve_next_seqno(rx->desc.seqno);
- 		work_done++;
+ 		total_packet_cnt++;
+ 		idx = rx->cnt & rx->mask;
+ 		desc = &rx->desc.desc_ring[idx];
+ 		work_done += work_cnt;
  	}
  
++<<<<<<< HEAD
 +	if (!work_done && rx->fill_cnt - cnt > rx->db_threshold)
 +		return false;
 +
 +	u64_stats_update_begin(&rx->statss);
 +	rx->rpackets += packets;
 +	rx->rbytes += bytes;
 +	u64_stats_update_end(&rx->statss);
 +	rx->cnt = cnt;
++=======
+ 	if (!work_done && rx->fill_cnt - rx->cnt > rx->db_threshold)
+ 		return 0;
+ 
+ 	if (work_done) {
+ 		u64_stats_update_begin(&rx->statss);
+ 		rx->rpackets += ok_packet_cnt;
+ 		rx->rbytes += bytes;
+ 		u64_stats_update_end(&rx->statss);
+ 	}
++>>>>>>> 37149e9374bf (gve: Implement packet continuation for RX.)
  
  	/* restock ring slots */
  	if (!rx->data.raw_addressing) {
@@@ -608,19 -759,17 +770,23 @@@
  		/* If we were not able to completely refill buffers, we'll want
  		 * to schedule this queue for work again to refill buffers.
  		 */
- 		if (rx->fill_cnt - cnt <= rx->db_threshold) {
+ 		if (rx->fill_cnt - rx->cnt <= rx->db_threshold) {
  			gve_rx_write_doorbell(priv, rx);
 -			return budget;
 +			return true;
  		}
  	}
  
 +	/* restock desc ring slots */
 +	dma_wmb();	/* Ensure descs are visible before ringing doorbell */
  	gve_rx_write_doorbell(priv, rx);
++<<<<<<< HEAD
 +	return gve_rx_work_pending(rx);
++=======
+ 	return total_packet_cnt;
++>>>>>>> 37149e9374bf (gve: Implement packet continuation for RX.)
  }
  
 -int gve_rx_poll(struct gve_notify_block *block, int budget)
 +bool gve_rx_poll(struct gve_notify_block *block, int budget)
  {
  	struct gve_rx_ring *rx = block->rx;
  	netdev_features_t feat;
* Unmerged path drivers/net/ethernet/google/gve/gve_rx_dqo.c
* Unmerged path drivers/net/ethernet/google/gve/gve.h
* Unmerged path drivers/net/ethernet/google/gve/gve_adminq.c
diff --git a/drivers/net/ethernet/google/gve/gve_desc.h b/drivers/net/ethernet/google/gve/gve_desc.h
index a1c0aaa60139..470313af210f 100644
--- a/drivers/net/ethernet/google/gve/gve_desc.h
+++ b/drivers/net/ethernet/google/gve/gve_desc.h
@@ -88,12 +88,13 @@ union gve_rx_data_slot {
 
 /* GVE Recive Packet Descriptor Flags */
 #define GVE_RXFLG(x)	cpu_to_be16(1 << (3 + (x)))
-#define	GVE_RXF_FRAG	GVE_RXFLG(3)	/* IP Fragment			*/
-#define	GVE_RXF_IPV4	GVE_RXFLG(4)	/* IPv4				*/
-#define	GVE_RXF_IPV6	GVE_RXFLG(5)	/* IPv6				*/
-#define	GVE_RXF_TCP	GVE_RXFLG(6)	/* TCP Packet			*/
-#define	GVE_RXF_UDP	GVE_RXFLG(7)	/* UDP Packet			*/
-#define	GVE_RXF_ERR	GVE_RXFLG(8)	/* Packet Error Detected	*/
+#define	GVE_RXF_FRAG		GVE_RXFLG(3)	/* IP Fragment			*/
+#define	GVE_RXF_IPV4		GVE_RXFLG(4)	/* IPv4				*/
+#define	GVE_RXF_IPV6		GVE_RXFLG(5)	/* IPv6				*/
+#define	GVE_RXF_TCP		GVE_RXFLG(6)	/* TCP Packet			*/
+#define	GVE_RXF_UDP		GVE_RXFLG(7)	/* UDP Packet			*/
+#define	GVE_RXF_ERR		GVE_RXFLG(8)	/* Packet Error Detected	*/
+#define	GVE_RXF_PKT_CONT	GVE_RXFLG(10)	/* Multi Fragment RX packet	*/
 
 /* GVE IRQ */
 #define GVE_IRQ_ACK	BIT(31)
diff --git a/drivers/net/ethernet/google/gve/gve_ethtool.c b/drivers/net/ethernet/google/gve/gve_ethtool.c
index e8a09d3163e6..c45d2725800b 100644
--- a/drivers/net/ethernet/google/gve/gve_ethtool.c
+++ b/drivers/net/ethernet/google/gve/gve_ethtool.c
@@ -42,6 +42,7 @@ static const char gve_gstrings_main_stats[][ETH_GSTRING_LEN] = {
 
 static const char gve_gstrings_rx_stats[][ETH_GSTRING_LEN] = {
 	"rx_posted_desc[%u]", "rx_completed_desc[%u]", "rx_bytes[%u]",
+	"rx_cont_packet_cnt[%u]", "rx_frag_flip_cnt[%u]", "rx_frag_copy_cnt[%u]",
 	"rx_dropped_pkt[%u]", "rx_copybreak_pkt[%u]", "rx_copied_pkt[%u]",
 	"rx_queue_drop_cnt[%u]", "rx_no_buffers_posted[%u]",
 	"rx_drops_packet_over_mru[%u]", "rx_drops_invalid_checksum[%u]",
@@ -263,6 +264,9 @@ gve_get_ethtool_stats(struct net_device *netdev,
 			} while (u64_stats_fetch_retry(&priv->rx[ring].statss,
 						       start));
 			data[i++] = tmp_rx_bytes;
+			data[i++] = rx->rx_cont_packet_cnt;
+			data[i++] = rx->rx_frag_flip_cnt;
+			data[i++] = rx->rx_frag_copy_cnt;
 			/* rx dropped packets */
 			data[i++] = tmp_rx_skb_alloc_fail +
 				tmp_rx_buf_alloc_fail +
* Unmerged path drivers/net/ethernet/google/gve/gve_main.c
* Unmerged path drivers/net/ethernet/google/gve/gve_rx.c
* Unmerged path drivers/net/ethernet/google/gve/gve_rx_dqo.c
diff --git a/drivers/net/ethernet/google/gve/gve_utils.c b/drivers/net/ethernet/google/gve/gve_utils.c
index aef57c448a98..6cb34ebbac09 100644
--- a/drivers/net/ethernet/google/gve/gve_utils.c
+++ b/drivers/net/ethernet/google/gve/gve_utils.c
@@ -50,20 +50,31 @@ void gve_rx_add_to_block(struct gve_priv *priv, int queue_idx)
 
 struct sk_buff *gve_rx_copy(struct net_device *dev, struct napi_struct *napi,
 			    struct gve_rx_slot_page_info *page_info, u16 len,
-			    u16 pad)
+			    u16 padding, struct gve_rx_ctx *ctx)
 {
-	struct sk_buff *skb = napi_alloc_skb(napi, len);
-	void *va = page_info->page_address + pad +
-		   page_info->page_offset;
-
-	if (unlikely(!skb))
-		return NULL;
-
+	void *va = page_info->page_address + padding + page_info->page_offset;
+	int skb_linear_offset = 0;
+	bool set_protocol = false;
+	struct sk_buff *skb;
+
+	if (ctx) {
+		if (!ctx->skb_head)
+			ctx->skb_head = napi_alloc_skb(napi, ctx->total_expected_size);
+
+		if (unlikely(!ctx->skb_head))
+			return NULL;
+		skb = ctx->skb_head;
+		skb_linear_offset = skb->len;
+		set_protocol = ctx->curr_frag_cnt == ctx->expected_frag_cnt - 1;
+	} else {
+		skb = napi_alloc_skb(napi, len);
+		set_protocol = true;
+	}
 	__skb_put(skb, len);
+	skb_copy_to_linear_data_offset(skb, skb_linear_offset, va, len);
 
-	skb_copy_to_linear_data(skb, va, len);
-
-	skb->protocol = eth_type_trans(skb, dev);
+	if (set_protocol)
+		skb->protocol = eth_type_trans(skb, dev);
 
 	return skb;
 }
diff --git a/drivers/net/ethernet/google/gve/gve_utils.h b/drivers/net/ethernet/google/gve/gve_utils.h
index 8fb39b990bbc..a6e60310e0b7 100644
--- a/drivers/net/ethernet/google/gve/gve_utils.h
+++ b/drivers/net/ethernet/google/gve/gve_utils.h
@@ -19,7 +19,7 @@ void gve_rx_add_to_block(struct gve_priv *priv, int queue_idx);
 
 struct sk_buff *gve_rx_copy(struct net_device *dev, struct napi_struct *napi,
 			    struct gve_rx_slot_page_info *page_info, u16 len,
-			    u16 pad);
+			    u16 pad, struct gve_rx_ctx *ctx);
 
 #endif /* _GVE_UTILS_H */
 
