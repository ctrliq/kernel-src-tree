memcg: reduce size of memcg vmstats structures

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit 8278f1c7b4920105f2f30a8df9b8212b378101d2
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/8278f1c7.failed

The struct memcg_vmstats and struct memcg_vmstats_percpu contains two
arrays each for events of size NR_VM_EVENT_ITEMS which can be as large as
110.  However the memcg v1 only uses 4 of those while memcg v2 uses 15. 
The union of both is 17.  On a 64 bit system, we are wasting approximately
((110 - 17) * 8 * 2) * (nr_cpus + 1) bytes which is significant on large
machines.

This patch reduces the size of the given structures by adding one
indirection and only stores array of events which are actually used by the
memcg code.  With this patch, the size of memcg_vmstats has reduced from
2544 bytes to 1056 bytes while the size of memcg_vmstats_percpu has
reduced from 2568 bytes to 1080 bytes.

[akpm@linux-foundation.org: fix memcg_events_local() array index, per Shakeel]
  Link: https://lkml.kernel.org/r/CALvZod70Mvxr+Nzb6k0yiU2RFYjTD=0NFhKK-Eyp+5ejd1PSFw@mail.gmail.com
Link: https://lkml.kernel.org/r/20220907043537.3457014-4-shakeelb@google.com
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Acked-by: Roman Gushchin <roman.gushchin@linux.dev>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 8278f1c7b4920105f2f30a8df9b8212b378101d2)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/memcontrol.c
diff --cc mm/memcontrol.c
index 413502fb22c9,1f204a262054..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -640,27 -566,182 +640,136 @@@ mem_cgroup_largest_soft_limit_node(stru
  }
  
  /*
 - * memcg and lruvec stats flushing
 - *
 - * Many codepaths leading to stats update or read are performance sensitive and
 - * adding stats flushing in such codepaths is not desirable. So, to optimize the
 - * flushing the kernel does:
 + * Return the active percpu stats memcg and optionally mem_cgroup_per_node.
   *
 - * 1) Periodically and asynchronously flush the stats every 2 seconds to not let
 - *    rstat update tree grow unbounded.
 - *
 - * 2) Flush the stats synchronously on reader side only when there are more than
 - *    (MEMCG_CHARGE_BATCH * nr_cpus) update events. Though this optimization
 - *    will let stats be out of sync by atmost (MEMCG_CHARGE_BATCH * nr_cpus) but
 - *    only for 2 seconds due to (1).
 - */
 -static void flush_memcg_stats_dwork(struct work_struct *w);
 -static DECLARE_DEFERRABLE_WORK(stats_flush_dwork, flush_memcg_stats_dwork);
 -static DEFINE_SPINLOCK(stats_flush_lock);
 -static DEFINE_PER_CPU(unsigned int, stats_updates);
 -static atomic_t stats_flush_threshold = ATOMIC_INIT(0);
 -static u64 flush_next_time;
 -
 -#define FLUSH_TIME (2UL*HZ)
 -
 -/*
 - * Accessors to ensure that preemption is disabled on PREEMPT_RT because it can
 - * not rely on this as part of an acquired spinlock_t lock. These functions are
 - * never used in hardirq context on PREEMPT_RT and therefore disabling preemtion
 - * is sufficient.
 + * When percpu_stats_disabled, the percpu stats update is transferred to
 + * its parent.
   */
 -static void memcg_stats_lock(void)
 -{
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_disable();
 -#else
 -      VM_BUG_ON(!irqs_disabled());
 -#endif
 -}
 -
 -static void __memcg_stats_lock(void)
 -{
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_disable();
 -#endif
 -}
 -
 -static void memcg_stats_unlock(void)
 +static inline struct mem_cgroup *
 +percpu_stats_memcg(struct mem_cgroup *memcg, struct mem_cgroup_per_node **pn)
  {
 -#ifdef CONFIG_PREEMPT_RT
 -      preempt_enable();
 -#endif
 -}
 +	if (likely(!memcg->percpu_stats_disabled))
 +		return memcg;
  
 -static inline void memcg_rstat_updated(struct mem_cgroup *memcg, int val)
 -{
 -	unsigned int x;
 +	do {
 +		memcg = parent_mem_cgroup(memcg);
 +	} while (memcg->percpu_stats_disabled);
  
 -	cgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());
 +	if (pn) {
 +		unsigned int nid = (*pn)->nid;
  
 -	x = __this_cpu_add_return(stats_updates, abs(val));
 -	if (x > MEMCG_CHARGE_BATCH) {
 -		/*
 -		 * If stats_flush_threshold exceeds the threshold
 -		 * (>num_online_cpus()), cgroup stats update will be triggered
 -		 * in __mem_cgroup_flush_stats(). Increasing this var further
 -		 * is redundant and simply adds overhead in atomic update.
 -		 */
 -		if (atomic_read(&stats_flush_threshold) <= num_online_cpus())
 -			atomic_add(x / MEMCG_CHARGE_BATCH, &stats_flush_threshold);
 -		__this_cpu_write(stats_updates, 0);
 +		*pn = memcg->nodeinfo[nid];
  	}
++<<<<<<< HEAD
 +	return memcg;
++=======
+ }
+ 
+ static void __mem_cgroup_flush_stats(void)
+ {
+ 	unsigned long flag;
+ 
+ 	if (!spin_trylock_irqsave(&stats_flush_lock, flag))
+ 		return;
+ 
+ 	flush_next_time = jiffies_64 + 2*FLUSH_TIME;
+ 	cgroup_rstat_flush_irqsafe(root_mem_cgroup->css.cgroup);
+ 	atomic_set(&stats_flush_threshold, 0);
+ 	spin_unlock_irqrestore(&stats_flush_lock, flag);
+ }
+ 
+ void mem_cgroup_flush_stats(void)
+ {
+ 	if (atomic_read(&stats_flush_threshold) > num_online_cpus())
+ 		__mem_cgroup_flush_stats();
+ }
+ 
+ void mem_cgroup_flush_stats_delayed(void)
+ {
+ 	if (time_after64(jiffies_64, flush_next_time))
+ 		mem_cgroup_flush_stats();
+ }
+ 
+ static void flush_memcg_stats_dwork(struct work_struct *w)
+ {
+ 	__mem_cgroup_flush_stats();
+ 	queue_delayed_work(system_unbound_wq, &stats_flush_dwork, FLUSH_TIME);
+ }
+ 
+ /* Subset of vm_event_item to report for memcg event stats */
+ static const unsigned int memcg_vm_event_stat[] = {
+ 	PGPGIN,
+ 	PGPGOUT,
+ 	PGSCAN_KSWAPD,
+ 	PGSCAN_DIRECT,
+ 	PGSTEAL_KSWAPD,
+ 	PGSTEAL_DIRECT,
+ 	PGFAULT,
+ 	PGMAJFAULT,
+ 	PGREFILL,
+ 	PGACTIVATE,
+ 	PGDEACTIVATE,
+ 	PGLAZYFREE,
+ 	PGLAZYFREED,
+ #if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)
+ 	ZSWPIN,
+ 	ZSWPOUT,
+ #endif
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ 	THP_FAULT_ALLOC,
+ 	THP_COLLAPSE_ALLOC,
+ #endif
+ };
+ 
+ #define NR_MEMCG_EVENTS ARRAY_SIZE(memcg_vm_event_stat)
+ static int mem_cgroup_events_index[NR_VM_EVENT_ITEMS] __read_mostly;
+ 
+ static void init_memcg_events(void)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < NR_MEMCG_EVENTS; ++i)
+ 		mem_cgroup_events_index[memcg_vm_event_stat[i]] = i + 1;
+ }
+ 
+ static inline int memcg_events_index(enum vm_event_item idx)
+ {
+ 	return mem_cgroup_events_index[idx] - 1;
+ }
+ 
+ struct memcg_vmstats_percpu {
+ 	/* Local (CPU and cgroup) page state & events */
+ 	long			state[MEMCG_NR_STAT];
+ 	unsigned long		events[NR_MEMCG_EVENTS];
+ 
+ 	/* Delta calculation for lockless upward propagation */
+ 	long			state_prev[MEMCG_NR_STAT];
+ 	unsigned long		events_prev[NR_MEMCG_EVENTS];
+ 
+ 	/* Cgroup1: threshold notifications & softlimit tree updates */
+ 	unsigned long		nr_page_events;
+ 	unsigned long		targets[MEM_CGROUP_NTARGETS];
+ };
+ 
+ struct memcg_vmstats {
+ 	/* Aggregated (CPU and subtree) page state & events */
+ 	long			state[MEMCG_NR_STAT];
+ 	unsigned long		events[NR_MEMCG_EVENTS];
+ 
+ 	/* Pending child counts during tree propagation */
+ 	long			state_pending[MEMCG_NR_STAT];
+ 	unsigned long		events_pending[NR_MEMCG_EVENTS];
+ };
+ 
+ unsigned long memcg_page_state(struct mem_cgroup *memcg, int idx)
+ {
+ 	long x = READ_ONCE(memcg->vmstats->state[idx]);
+ #ifdef CONFIG_SMP
+ 	if (x < 0)
+ 		x = 0;
+ #endif
+ 	return x;
++>>>>>>> 8278f1c7b492 (memcg: reduce size of memcg vmstats structures)
  }
  
  /**
@@@ -828,29 -891,37 +937,50 @@@ void __mod_lruvec_kmem_state(void *p, e
  void __count_memcg_events(struct mem_cgroup *memcg, enum vm_event_item idx,
  			  unsigned long count)
  {
- 	if (mem_cgroup_disabled())
+ 	int index = memcg_events_index(idx);
+ 
+ 	if (mem_cgroup_disabled() || index < 0)
  		return;
 +	memcg = percpu_stats_memcg(memcg, NULL);
  
++<<<<<<< HEAD
 +	__this_cpu_add(memcg->vmstats_percpu->events[idx], count);
 +	cgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());
++=======
+ 	memcg_stats_lock();
+ 	__this_cpu_add(memcg->vmstats_percpu->events[index], count);
+ 	memcg_rstat_updated(memcg, count);
+ 	memcg_stats_unlock();
++>>>>>>> 8278f1c7b492 (memcg: reduce size of memcg vmstats structures)
  }
  
  static unsigned long memcg_events(struct mem_cgroup *memcg, int event)
  {
++<<<<<<< HEAD
 +	return READ_ONCE(memcg->vmstats.events[event]);
++=======
+ 	int index = memcg_events_index(event);
+ 
+ 	if (index < 0)
+ 		return 0;
+ 	return READ_ONCE(memcg->vmstats->events[index]);
++>>>>>>> 8278f1c7b492 (memcg: reduce size of memcg vmstats structures)
  }
  
  static unsigned long memcg_events_local(struct mem_cgroup *memcg, int event)
  {
  	long x = 0;
  	int cpu;
+ 	int index = memcg_events_index(event);
+ 
+ 	if (index < 0)
+ 		return 0;
  
 +	if (unlikely(memcg->percpu_stats_disabled))
 +		return 0;
 +
  	for_each_possible_cpu(cpu)
- 		x += per_cpu(memcg->vmstats_percpu->events[event], cpu);
+ 		x += per_cpu(memcg->vmstats_percpu->events[index], cpu);
  	return x;
  }
  
@@@ -1490,21 -1591,16 +1620,33 @@@ static char *memory_stat_format(struct 
  	seq_buf_printf(&s, "pgsteal %lu\n",
  		       memcg_events(memcg, PGSTEAL_KSWAPD) +
  		       memcg_events(memcg, PGSTEAL_DIRECT));
 -
 +	seq_buf_printf(&s, "%s %lu\n", vm_event_name(PGACTIVATE),
 +		       memcg_events(memcg, PGACTIVATE));
 +	seq_buf_printf(&s, "%s %lu\n", vm_event_name(PGDEACTIVATE),
 +		       memcg_events(memcg, PGDEACTIVATE));
 +	seq_buf_printf(&s, "%s %lu\n", vm_event_name(PGLAZYFREE),
 +		       memcg_events(memcg, PGLAZYFREE));
 +	seq_buf_printf(&s, "%s %lu\n", vm_event_name(PGLAZYFREED),
 +		       memcg_events(memcg, PGLAZYFREED));
 +
++<<<<<<< HEAD
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +	seq_buf_printf(&s, "%s %lu\n", vm_event_name(THP_FAULT_ALLOC),
 +		       memcg_events(memcg, THP_FAULT_ALLOC));
 +	seq_buf_printf(&s, "%s %lu\n", vm_event_name(THP_COLLAPSE_ALLOC),
 +		       memcg_events(memcg, THP_COLLAPSE_ALLOC));
 +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++=======
+ 	for (i = 0; i < ARRAY_SIZE(memcg_vm_event_stat); i++) {
+ 		if (memcg_vm_event_stat[i] == PGPGIN ||
+ 		    memcg_vm_event_stat[i] == PGPGOUT)
+ 			continue;
+ 
+ 		seq_buf_printf(&s, "%s %lu\n",
+ 			       vm_event_name(memcg_vm_event_stat[i]),
+ 			       memcg_events(memcg, memcg_vm_event_stat[i]));
+ 	}
++>>>>>>> 8278f1c7b492 (memcg: reduce size of memcg vmstats structures)
  
  	/* The above should easily fit into one page */
  	WARN_ON_ONCE(seq_buf_has_overflowed(&s));
@@@ -5506,15 -5506,15 +5649,20 @@@ static void mem_cgroup_css_rstat_flush(
  			continue;
  
  		/* Aggregate counts on this level and propagate upwards */
 -		memcg->vmstats->state[i] += delta;
 +		memcg->vmstats.state[i] += delta;
  		if (parent)
 -			parent->vmstats->state_pending[i] += delta;
 +			parent->vmstats.state_pending[i] += delta;
  	}
  
++<<<<<<< HEAD
 +	for (i = 0; i < NR_VM_EVENT_ITEMS; i++) {
 +		delta = memcg->vmstats.events_pending[i];
++=======
+ 	for (i = 0; i < NR_MEMCG_EVENTS; i++) {
+ 		delta = memcg->vmstats->events_pending[i];
++>>>>>>> 8278f1c7b492 (memcg: reduce size of memcg vmstats structures)
  		if (delta)
 -			memcg->vmstats->events_pending[i] = 0;
 +			memcg->vmstats.events_pending[i] = 0;
  
  		v = READ_ONCE(statc->events[i]);
  		if (v != statc->events_prev[i]) {
* Unmerged path mm/memcontrol.c
