mm: honor PF_MEMALLOC_PIN for all movable pages

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Pavel Tatashin <pasha.tatashin@soleen.com>
commit 8e3560d963d22ba41857f48e4114ce80373144ea
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/8e3560d9.failed

PF_MEMALLOC_PIN is only honored for CMA pages, extend this flag to work
for any allocations from ZONE_MOVABLE by removing __GFP_MOVABLE from
gfp_mask when this flag is passed in the current context.

Add is_pinnable_page() to return true if page is in a pinnable page.  A
pinnable page is not in ZONE_MOVABLE and not of MIGRATE_CMA type.

Link: https://lkml.kernel.org/r/20210215161349.246722-8-pasha.tatashin@soleen.com
	Signed-off-by: Pavel Tatashin <pasha.tatashin@soleen.com>
	Acked-by: Michal Hocko <mhocko@suse.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: James Morris <jmorris@namei.org>
	Cc: Jason Gunthorpe <jgg@nvidia.com>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Oscar Salvador <osalvador@suse.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Sasha Levin <sashal@kernel.org>
	Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
	Cc: Tyler Hicks <tyhicks@linux.microsoft.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 8e3560d963d22ba41857f48e4114ce80373144ea)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/sched/mm.h
#	mm/hugetlb.c
#	mm/page_alloc.c
diff --cc include/linux/sched/mm.h
index 0b21b5736e43,e24b1fe348e3..000000000000
--- a/include/linux/sched/mm.h
+++ b/include/linux/sched/mm.h
@@@ -176,24 -151,24 +176,40 @@@ static inline bool in_vfork(struct task
   * Applies per-task gfp context to the given allocation flags.
   * PF_MEMALLOC_NOIO implies GFP_NOIO
   * PF_MEMALLOC_NOFS implies GFP_NOFS
++<<<<<<< HEAD
 + * PF_MEMALLOC_NOCMA implies no allocation from CMA region.
 + */
 +static inline gfp_t current_gfp_context(gfp_t flags)
 +{
 +	if (unlikely(current->flags &
 +		     (PF_MEMALLOC_NOIO | PF_MEMALLOC_NOFS | PF_MEMALLOC_NOCMA))) {
++=======
+  * PF_MEMALLOC_PIN  implies !GFP_MOVABLE
+  */
+ static inline gfp_t current_gfp_context(gfp_t flags)
+ {
+ 	unsigned int pflags = READ_ONCE(current->flags);
+ 
+ 	if (unlikely(pflags & (PF_MEMALLOC_NOIO | PF_MEMALLOC_NOFS | PF_MEMALLOC_PIN))) {
++>>>>>>> 8e3560d963d2 (mm: honor PF_MEMALLOC_PIN for all movable pages)
  		/*
  		 * NOIO implies both NOIO and NOFS and it is a weaker context
  		 * so always make sure it makes precedence
  		 */
 -		if (pflags & PF_MEMALLOC_NOIO)
 +		if (current->flags & PF_MEMALLOC_NOIO)
  			flags &= ~(__GFP_IO | __GFP_FS);
 -		else if (pflags & PF_MEMALLOC_NOFS)
 +		else if (current->flags & PF_MEMALLOC_NOFS)
  			flags &= ~__GFP_FS;
++<<<<<<< HEAD
 +#ifdef CONFIG_CMA
 +		if (current->flags & PF_MEMALLOC_NOCMA)
 +			flags &= ~__GFP_MOVABLE;
 +#endif
++=======
+ 
+ 		if (pflags & PF_MEMALLOC_PIN)
+ 			flags &= ~__GFP_MOVABLE;
++>>>>>>> 8e3560d963d2 (mm: honor PF_MEMALLOC_PIN for all movable pages)
  	}
  	return flags;
  }
diff --cc mm/hugetlb.c
index b420ddf66759,629aa4c2259c..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -1057,29 -1079,25 +1057,33 @@@ static void enqueue_huge_page(struct hs
  static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)
  {
  	struct page *page;
 -	bool pin = !!(current->flags & PF_MEMALLOC_PIN);
 +	bool nocma = !!(current->flags & PF_MEMALLOC_NOCMA);
  
 -	lockdep_assert_held(&hugetlb_lock);
  	list_for_each_entry(page, &h->hugepage_freelists[nid], lru) {
++<<<<<<< HEAD
 +		if (nocma && is_migrate_cma_page(page))
++=======
+ 		if (pin && !is_pinnable_page(page))
++>>>>>>> 8e3560d963d2 (mm: honor PF_MEMALLOC_PIN for all movable pages)
  			continue;
  
 -		if (PageHWPoison(page))
 -			continue;
 +		if (!PageHWPoison(page))
 +			break;
  
 -		list_move(&page->lru, &h->hugepage_activelist);
 -		set_page_refcounted(page);
  		ClearHPageFreed(page);
 -		h->free_huge_pages--;
 -		h->free_huge_pages_node[nid]--;
 -		return page;
  	}
  
 -	return NULL;
 +	/*
 +	 * if 'non-isolated free hugepage' not found on the list,
 +	 * the allocation fails.
 +	 */
 +	if (&h->hugepage_freelists[nid] == &page->lru)
 +		return NULL;
 +	list_move(&page->lru, &h->hugepage_activelist);
 +	set_page_refcounted(page);
 +	h->free_huge_pages--;
 +	h->free_huge_pages_node[nid]--;
 +	return page;
  }
  
  static struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask, int nid,
diff --cc mm/page_alloc.c
index dc61c5b711ae,81db38926266..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -3778,6 -3859,17 +3778,20 @@@ alloc_flags_nofragment(struct zone *zon
  	return alloc_flags;
  }
  
++<<<<<<< HEAD
++=======
+ /* Must be called after current_gfp_context() which can change gfp_mask */
+ static inline unsigned int gfp_to_alloc_flags_cma(gfp_t gfp_mask,
+ 						  unsigned int alloc_flags)
+ {
+ #ifdef CONFIG_CMA
+ 	if (gfp_migratetype(gfp_mask) == MIGRATE_MOVABLE)
+ 		alloc_flags |= ALLOC_CMA;
+ #endif
+ 	return alloc_flags;
+ }
+ 
++>>>>>>> 8e3560d963d2 (mm: honor PF_MEMALLOC_PIN for all movable pages)
  /*
   * get_page_from_freelist goes through the zonelist trying to allocate
   * a page.
@@@ -4435,13 -4523,8 +4449,17 @@@ gfp_to_alloc_flags(gfp_t gfp_mask
  	} else if (unlikely(rt_task(current)) && !in_interrupt())
  		alloc_flags |= ALLOC_HARDER;
  
++<<<<<<< HEAD
 +	if (gfp_mask & __GFP_KSWAPD_RECLAIM)
 +		alloc_flags |= ALLOC_KSWAPD;
++=======
+ 	alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);
++>>>>>>> 8e3560d963d2 (mm: honor PF_MEMALLOC_PIN for all movable pages)
  
 +#ifdef CONFIG_CMA
 +	if (gfpflags_to_migratetype(gfp_mask) == MIGRATE_MOVABLE)
 +		alloc_flags |= ALLOC_CMA;
 +#endif
  	return alloc_flags;
  }
  
@@@ -4745,7 -4825,7 +4763,11 @@@ retry
  
  	reserve_flags = __gfp_pfmemalloc_flags(gfp_mask);
  	if (reserve_flags)
++<<<<<<< HEAD
 +		alloc_flags = reserve_flags;
++=======
+ 		alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, reserve_flags);
++>>>>>>> 8e3560d963d2 (mm: honor PF_MEMALLOC_PIN for all movable pages)
  
  	/*
  	 * Reset the nodemask and zonelist iterators if memory policies can be
@@@ -4910,15 -4994,8 +4932,19 @@@ static inline bool prepare_alloc_pages(
  	if (should_fail_alloc_page(gfp_mask, order))
  		return false;
  
++<<<<<<< HEAD
 +	if (IS_ENABLED(CONFIG_CMA) && ac->migratetype == MIGRATE_MOVABLE)
 +		*alloc_flags |= ALLOC_CMA;
++=======
+ 	*alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags);
++>>>>>>> 8e3560d963d2 (mm: honor PF_MEMALLOC_PIN for all movable pages)
  
 +	return true;
 +}
 +
 +/* Determine whether to spread dirty pages and what the first usable zone */
 +static inline void finalise_ac(gfp_t gfp_mask, struct alloc_context *ac)
 +{
  	/* Dirty zone balancing only done in the fast path */
  	ac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);
  
@@@ -4970,13 -5201,7 +4996,14 @@@ __alloc_pages_nodemask(gfp_t gfp_mask, 
  	if (likely(page))
  		goto out;
  
 -	alloc_gfp = gfp;
 +	/*
 +	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
 +	 * resp. GFP_NOIO which has to be inherited for all allocation requests
 +	 * from a particular context which has been marked by
- 	 * memalloc_no{fs,io}_{save,restore}.
++	 * memalloc_no{fs,io}_{save,restore}. And PF_MEMALLOC_PIN which ensures
++	 * movable zones are not used during allocation.
 +	 */
 +	alloc_mask = current_gfp_context(gfp_mask);
  	ac.spread_dirty_pages = false;
  
  	/*
diff --git a/include/linux/mm.h b/include/linux/mm.h
index e3b1a16638a0..2670246c3b64 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1003,6 +1003,11 @@ static inline bool is_zone_device_page(const struct page *page)
 }
 #endif
 
+static inline bool is_zone_movable_page(const struct page *page)
+{
+	return page_zonenum(page) == ZONE_MOVABLE;
+}
+
 #ifdef CONFIG_DEV_PAGEMAP_OPS
 void free_devmap_managed_page(struct page *page);
 DECLARE_STATIC_KEY_FALSE(devmap_managed_key);
@@ -1429,6 +1434,19 @@ static inline unsigned long page_to_section(const struct page *page)
 }
 #endif
 
+/* MIGRATE_CMA and ZONE_MOVABLE do not allow pin pages */
+#ifdef CONFIG_MIGRATION
+static inline bool is_pinnable_page(struct page *page)
+{
+	return !is_zone_movable_page(page) && !is_migrate_cma_page(page);
+}
+#else
+static inline bool is_pinnable_page(struct page *page)
+{
+	return true;
+}
+#endif
+
 static inline void set_page_zone(struct page *page, enum zone_type zone)
 {
 	page->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);
* Unmerged path include/linux/sched/mm.h
* Unmerged path mm/hugetlb.c
* Unmerged path mm/page_alloc.c
