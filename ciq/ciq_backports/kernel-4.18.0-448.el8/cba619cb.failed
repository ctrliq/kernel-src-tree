drm/i915/gvt: merge gvt.c into kvmgvt.c

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
Rebuild_CHGLOG: - Revert "drm/i915/gvt: merge gvt.c into kvmgvt.c" (Jocelyn Falempe) [2115880]
Rebuild_FUZZ: 89.66%
commit-author Christoph Hellwig <hch@lst.de>
commit cba619cb0d4d66c743cf001c6b13c171a769a65f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/cba619cb.failed

The code in both files is deeply interconnected, so merge it and
keep a bunch of structures and functions static.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
Link: http://patchwork.freedesktop.org/patch/msgid/20220411141403.86980-30-hch@lst.de
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
	Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
(cherry picked from commit cba619cb0d4d66c743cf001c6b13c171a769a65f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/gpu/drm/i915/gvt/Makefile
#	drivers/gpu/drm/i915/gvt/gvt.c
#	drivers/gpu/drm/i915/gvt/gvt.h
#	drivers/gpu/drm/i915/gvt/kvmgt.c
diff --cc drivers/gpu/drm/i915/gvt/Makefile
index ea8324abc784,1699f644298e..000000000000
--- a/drivers/gpu/drm/i915/gvt/Makefile
+++ b/drivers/gpu/drm/i915/gvt/Makefile
@@@ -1,9 -1,25 +1,35 @@@
  # SPDX-License-Identifier: GPL-2.0
 +GVT_DIR := gvt
 +GVT_SOURCE := gvt.o aperture_gm.o handlers.o vgpu.o trace_points.o firmware.o \
 +	interrupt.o gtt.o cfg_space.o opregion.o mmio.o display.o edid.o \
 +	execlist.o scheduler.o sched_policy.o mmio_context.o cmd_parser.o debugfs.o \
 +	fb_decoder.o dmabuf.o page_track.o
  
++<<<<<<< HEAD
 +ccflags-y				+= -I $(srctree)/$(src) -I $(srctree)/$(src)/$(GVT_DIR)/
 +i915-y					+= $(addprefix $(GVT_DIR)/, $(GVT_SOURCE))
++=======
+ kvmgt-$(CONFIG_DRM_I915_GVT) += \
+ 	gvt/aperture_gm.o \
+ 	gvt/cfg_space.o \
+ 	gvt/cmd_parser.o \
+ 	gvt/debugfs.o \
+ 	gvt/display.o \
+ 	gvt/dmabuf.o \
+ 	gvt/edid.o \
+ 	gvt/execlist.o \
+ 	gvt/fb_decoder.o \
+ 	gvt/firmware.o \
+ 	gvt/gtt.o \
+ 	gvt/handlers.o \
+ 	gvt/interrupt.o \
+ 	gvt/kvmgt.o \
+ 	gvt/mmio.o \
+ 	gvt/mmio_context.o \
+ 	gvt/opregion.o \
+ 	gvt/page_track.o \
+ 	gvt/sched_policy.o \
+ 	gvt/scheduler.o \
+ 	gvt/trace_points.o \
+ 	gvt/vgpu.o
++>>>>>>> cba619cb0d4d (drm/i915/gvt: merge gvt.c into kvmgvt.c)
diff --cc drivers/gpu/drm/i915/gvt/gvt.h
index 0c0615602343,03ecffc2ba56..000000000000
--- a/drivers/gpu/drm/i915/gvt/gvt.h
+++ b/drivers/gpu/drm/i915/gvt/gvt.h
@@@ -728,9 -757,14 +728,21 @@@ void intel_gvt_debugfs_remove_vgpu(stru
  void intel_gvt_debugfs_init(struct intel_gvt *gvt);
  void intel_gvt_debugfs_clean(struct intel_gvt *gvt);
  
++<<<<<<< HEAD
 +int intel_gvt_pm_resume(struct intel_gvt *gvt);
 +
 +#include "trace.h"
 +#include "mpt.h"
++=======
+ int intel_gvt_page_track_add(struct intel_vgpu *info, u64 gfn);
+ int intel_gvt_page_track_remove(struct intel_vgpu *info, u64 gfn);
+ int intel_gvt_dma_pin_guest_page(struct intel_vgpu *vgpu, dma_addr_t dma_addr);
+ int intel_gvt_dma_map_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,
+ 		unsigned long size, dma_addr_t *dma_addr);
+ void intel_gvt_dma_unmap_guest_page(struct intel_vgpu *vgpu,
+ 		dma_addr_t dma_addr);
+ 
+ #include "trace.h"
++>>>>>>> cba619cb0d4d (drm/i915/gvt: merge gvt.c into kvmgvt.c)
  
  #endif
diff --cc drivers/gpu/drm/i915/gvt/kvmgt.c
index 5bd0c74fc4ad,f033031c676d..000000000000
--- a/drivers/gpu/drm/i915/gvt/kvmgt.c
+++ b/drivers/gpu/drm/i915/gvt/kvmgt.c
@@@ -1764,48 -1663,77 +1769,68 @@@ static const struct attribute_group *in
  	NULL,
  };
  
 -static const struct vfio_device_ops intel_vgpu_dev_ops = {
 -	.open_device	= intel_vgpu_open_device,
 -	.close_device	= intel_vgpu_close_device,
 -	.read		= intel_vgpu_read,
 -	.write		= intel_vgpu_write,
 -	.mmap		= intel_vgpu_mmap,
 -	.ioctl		= intel_vgpu_ioctl,
 +static struct mdev_parent_ops intel_vgpu_ops = {
 +	.mdev_attr_groups       = intel_vgpu_groups,
 +	.create			= intel_vgpu_create,
 +	.remove			= intel_vgpu_remove,
 +
 +	.open			= intel_vgpu_open,
 +	.release		= intel_vgpu_release,
 +
 +	.read			= intel_vgpu_read,
 +	.write			= intel_vgpu_write,
 +	.mmap			= intel_vgpu_mmap,
 +	.ioctl			= intel_vgpu_ioctl,
  };
  
 -static int intel_vgpu_probe(struct mdev_device *mdev)
 +static int kvmgt_host_init(struct device *dev, void *gvt, const void *ops)
  {
 -	struct device *pdev = mdev_parent_dev(mdev);
 -	struct intel_gvt *gvt = kdev_to_i915(pdev)->gvt;
 -	struct intel_vgpu_type *type;
 -	struct intel_vgpu *vgpu;
  	int ret;
  
 -	type = &gvt->types[mdev_get_type_group_id(mdev)];
 -	if (!type)
 -		return -EINVAL;
 -
 -	vgpu = intel_gvt_create_vgpu(gvt, type);
 -	if (IS_ERR(vgpu)) {
 -		gvt_err("failed to create intel vgpu: %ld\n", PTR_ERR(vgpu));
 -		return PTR_ERR(vgpu);
 -	}
 +	ret = intel_gvt_init_vgpu_type_groups((struct intel_gvt *)gvt);
 +	if (ret)
 +		return ret;
  
 -	INIT_WORK(&vgpu->release_work, intel_vgpu_release_work);
 -	vfio_init_group_dev(&vgpu->vfio_device, &mdev->dev,
 -			    &intel_vgpu_dev_ops);
 +	intel_gvt_ops = ops;
 +	intel_vgpu_ops.supported_type_groups = gvt_vgpu_type_groups;
  
 -	dev_set_drvdata(&mdev->dev, vgpu);
 -	ret = vfio_register_emulated_iommu_dev(&vgpu->vfio_device);
 -	if (ret) {
 -		intel_gvt_destroy_vgpu(vgpu);
 -		return ret;
 -	}
 +	ret = mdev_register_device(dev, &intel_vgpu_ops);
 +	if (ret)
 +		intel_gvt_cleanup_vgpu_type_groups((struct intel_gvt *)gvt);
  
 -	gvt_dbg_core("intel_vgpu_create succeeded for mdev: %s\n",
 -		     dev_name(mdev_dev(mdev)));
 -	return 0;
 +	return ret;
  }
  
 -static void intel_vgpu_remove(struct mdev_device *mdev)
 +static void kvmgt_host_exit(struct device *dev, void *gvt)
  {
 -	struct intel_vgpu *vgpu = dev_get_drvdata(&mdev->dev);
 -
 -	if (WARN_ON_ONCE(vgpu->attached))
 -		return;
 -	intel_gvt_destroy_vgpu(vgpu);
 +	mdev_unregister_device(dev);
 +	intel_gvt_cleanup_vgpu_type_groups((struct intel_gvt *)gvt);
  }
  
++<<<<<<< HEAD
 +static int kvmgt_page_track_add(unsigned long handle, u64 gfn)
++=======
+ static struct mdev_driver intel_vgpu_mdev_driver = {
+ 	.driver = {
+ 		.name		= "intel_vgpu_mdev",
+ 		.owner		= THIS_MODULE,
+ 		.dev_groups	= intel_vgpu_groups,
+ 	},
+ 	.probe		= intel_vgpu_probe,
+ 	.remove		= intel_vgpu_remove,
+ };
+ 
+ static const struct mdev_parent_ops intel_vgpu_mdev_ops = {
+ 	.owner			= THIS_MODULE,
+ 	.supported_type_groups	= gvt_vgpu_type_groups,
+ 	.device_driver		= &intel_vgpu_mdev_driver,
+ };
+ 
+ int intel_gvt_page_track_add(struct intel_vgpu *info, u64 gfn)
++>>>>>>> cba619cb0d4d (drm/i915/gvt: merge gvt.c into kvmgvt.c)
  {
 -	struct kvm *kvm = info->kvm;
 +	struct kvmgt_guest_info *info;
 +	struct kvm *kvm;
  	struct kvm_memory_slot *slot;
  	int idx;
  
@@@ -2165,81 -1927,258 +2190,333 @@@ static void kvmgt_dma_unmap_guest_page(
  	entry = __gvt_cache_find_dma_addr(vgpu, dma_addr);
  	if (entry)
  		kref_put(&entry->ref, __gvt_dma_release);
 -	mutex_unlock(&vgpu->cache_lock);
 +	mutex_unlock(&vdev->cache_lock);
 +}
 +
++<<<<<<< HEAD
 +static int kvmgt_rw_gpa(unsigned long handle, unsigned long gpa,
 +			void *buf, unsigned long len, bool write)
 +{
 +	struct kvmgt_guest_info *info;
 +
 +	if (!handle_valid(handle))
 +		return -ESRCH;
 +
 +	info = (struct kvmgt_guest_info *)handle;
 +
 +	return vfio_dma_rw(kvmgt_vdev(info->vgpu)->vfio_group,
 +			   gpa, buf, len, write);
 +}
 +
 +static int kvmgt_read_gpa(unsigned long handle, unsigned long gpa,
 +			void *buf, unsigned long len)
 +{
 +	return kvmgt_rw_gpa(handle, gpa, buf, len, false);
 +}
 +
 +static int kvmgt_write_gpa(unsigned long handle, unsigned long gpa,
 +			void *buf, unsigned long len)
 +{
 +	return kvmgt_rw_gpa(handle, gpa, buf, len, true);
 +}
 +
 +static unsigned long kvmgt_virt_to_pfn(void *addr)
 +{
 +	return PFN_DOWN(__pa(addr));
 +}
 +
 +static bool kvmgt_is_valid_gfn(unsigned long handle, unsigned long gfn)
 +{
 +	struct kvmgt_guest_info *info;
 +	struct kvm *kvm;
 +	int idx;
 +	bool ret;
 +
 +	if (!handle_valid(handle))
 +		return false;
 +
 +	info = (struct kvmgt_guest_info *)handle;
 +	kvm = info->kvm;
 +
 +	idx = srcu_read_lock(&kvm->srcu);
 +	ret = kvm_is_visible_gfn(kvm, gfn);
 +	srcu_read_unlock(&kvm->srcu, idx);
 +
 +	return ret;
  }
  
 +static const struct intel_gvt_mpt kvmgt_mpt = {
 +	.type = INTEL_GVT_HYPERVISOR_KVM,
 +	.host_init = kvmgt_host_init,
 +	.host_exit = kvmgt_host_exit,
 +	.attach_vgpu = kvmgt_attach_vgpu,
 +	.detach_vgpu = kvmgt_detach_vgpu,
 +	.inject_msi = kvmgt_inject_msi,
 +	.from_virt_to_mfn = kvmgt_virt_to_pfn,
 +	.enable_page_track = kvmgt_page_track_add,
 +	.disable_page_track = kvmgt_page_track_remove,
 +	.read_gpa = kvmgt_read_gpa,
 +	.write_gpa = kvmgt_write_gpa,
 +	.gfn_to_mfn = kvmgt_gfn_to_pfn,
 +	.dma_map_guest_page = kvmgt_dma_map_guest_page,
 +	.dma_unmap_guest_page = kvmgt_dma_unmap_guest_page,
 +	.dma_pin_guest_page = kvmgt_dma_pin_guest_page,
 +	.set_opregion = kvmgt_set_opregion,
 +	.set_edid = kvmgt_set_edid,
 +	.get_vfio_device = kvmgt_get_vfio_device,
 +	.put_vfio_device = kvmgt_put_vfio_device,
 +	.is_valid_gfn = kvmgt_is_valid_gfn,
++=======
+ static void init_device_info(struct intel_gvt *gvt)
+ {
+ 	struct intel_gvt_device_info *info = &gvt->device_info;
+ 	struct pci_dev *pdev = to_pci_dev(gvt->gt->i915->drm.dev);
+ 
+ 	info->max_support_vgpus = 8;
+ 	info->cfg_space_size = PCI_CFG_SPACE_EXP_SIZE;
+ 	info->mmio_size = 2 * 1024 * 1024;
+ 	info->mmio_bar = 0;
+ 	info->gtt_start_offset = 8 * 1024 * 1024;
+ 	info->gtt_entry_size = 8;
+ 	info->gtt_entry_size_shift = 3;
+ 	info->gmadr_bytes_in_cmd = 8;
+ 	info->max_surface_size = 36 * 1024 * 1024;
+ 	info->msi_cap_offset = pdev->msi_cap;
+ }
+ 
+ static void intel_gvt_test_and_emulate_vblank(struct intel_gvt *gvt)
+ {
+ 	struct intel_vgpu *vgpu;
+ 	int id;
+ 
+ 	mutex_lock(&gvt->lock);
+ 	idr_for_each_entry((&(gvt)->vgpu_idr), (vgpu), (id)) {
+ 		if (test_and_clear_bit(INTEL_GVT_REQUEST_EMULATE_VBLANK + id,
+ 				       (void *)&gvt->service_request)) {
+ 			if (vgpu->active)
+ 				intel_vgpu_emulate_vblank(vgpu);
+ 		}
+ 	}
+ 	mutex_unlock(&gvt->lock);
+ }
+ 
+ static int gvt_service_thread(void *data)
+ {
+ 	struct intel_gvt *gvt = (struct intel_gvt *)data;
+ 	int ret;
+ 
+ 	gvt_dbg_core("service thread start\n");
+ 
+ 	while (!kthread_should_stop()) {
+ 		ret = wait_event_interruptible(gvt->service_thread_wq,
+ 				kthread_should_stop() || gvt->service_request);
+ 
+ 		if (kthread_should_stop())
+ 			break;
+ 
+ 		if (WARN_ONCE(ret, "service thread is waken up by signal.\n"))
+ 			continue;
+ 
+ 		intel_gvt_test_and_emulate_vblank(gvt);
+ 
+ 		if (test_bit(INTEL_GVT_REQUEST_SCHED,
+ 				(void *)&gvt->service_request) ||
+ 			test_bit(INTEL_GVT_REQUEST_EVENT_SCHED,
+ 					(void *)&gvt->service_request)) {
+ 			intel_gvt_schedule(gvt);
+ 		}
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void clean_service_thread(struct intel_gvt *gvt)
+ {
+ 	kthread_stop(gvt->service_thread);
+ }
+ 
+ static int init_service_thread(struct intel_gvt *gvt)
+ {
+ 	init_waitqueue_head(&gvt->service_thread_wq);
+ 
+ 	gvt->service_thread = kthread_run(gvt_service_thread,
+ 			gvt, "gvt_service_thread");
+ 	if (IS_ERR(gvt->service_thread)) {
+ 		gvt_err("fail to start service thread.\n");
+ 		return PTR_ERR(gvt->service_thread);
+ 	}
+ 	return 0;
+ }
+ 
+ /**
+  * intel_gvt_clean_device - clean a GVT device
+  * @i915: i915 private
+  *
+  * This function is called at the driver unloading stage, to free the
+  * resources owned by a GVT device.
+  *
+  */
+ static void intel_gvt_clean_device(struct drm_i915_private *i915)
+ {
+ 	struct intel_gvt *gvt = fetch_and_zero(&i915->gvt);
+ 
+ 	if (drm_WARN_ON(&i915->drm, !gvt))
+ 		return;
+ 
+ 	mdev_unregister_device(i915->drm.dev);
+ 	intel_gvt_cleanup_vgpu_type_groups(gvt);
+ 	intel_gvt_destroy_idle_vgpu(gvt->idle_vgpu);
+ 	intel_gvt_clean_vgpu_types(gvt);
+ 
+ 	intel_gvt_debugfs_clean(gvt);
+ 	clean_service_thread(gvt);
+ 	intel_gvt_clean_cmd_parser(gvt);
+ 	intel_gvt_clean_sched_policy(gvt);
+ 	intel_gvt_clean_workload_scheduler(gvt);
+ 	intel_gvt_clean_gtt(gvt);
+ 	intel_gvt_free_firmware(gvt);
+ 	intel_gvt_clean_mmio_info(gvt);
+ 	idr_destroy(&gvt->vgpu_idr);
+ 
+ 	kfree(i915->gvt);
+ }
+ 
+ /**
+  * intel_gvt_init_device - initialize a GVT device
+  * @i915: drm i915 private data
+  *
+  * This function is called at the initialization stage, to initialize
+  * necessary GVT components.
+  *
+  * Returns:
+  * Zero on success, negative error code if failed.
+  *
+  */
+ static int intel_gvt_init_device(struct drm_i915_private *i915)
+ {
+ 	struct intel_gvt *gvt;
+ 	struct intel_vgpu *vgpu;
+ 	int ret;
+ 
+ 	if (drm_WARN_ON(&i915->drm, i915->gvt))
+ 		return -EEXIST;
+ 
+ 	gvt = kzalloc(sizeof(struct intel_gvt), GFP_KERNEL);
+ 	if (!gvt)
+ 		return -ENOMEM;
+ 
+ 	gvt_dbg_core("init gvt device\n");
+ 
+ 	idr_init_base(&gvt->vgpu_idr, 1);
+ 	spin_lock_init(&gvt->scheduler.mmio_context_lock);
+ 	mutex_init(&gvt->lock);
+ 	mutex_init(&gvt->sched_lock);
+ 	gvt->gt = to_gt(i915);
+ 	i915->gvt = gvt;
+ 
+ 	init_device_info(gvt);
+ 
+ 	ret = intel_gvt_setup_mmio_info(gvt);
+ 	if (ret)
+ 		goto out_clean_idr;
+ 
+ 	intel_gvt_init_engine_mmio_context(gvt);
+ 
+ 	ret = intel_gvt_load_firmware(gvt);
+ 	if (ret)
+ 		goto out_clean_mmio_info;
+ 
+ 	ret = intel_gvt_init_irq(gvt);
+ 	if (ret)
+ 		goto out_free_firmware;
+ 
+ 	ret = intel_gvt_init_gtt(gvt);
+ 	if (ret)
+ 		goto out_free_firmware;
+ 
+ 	ret = intel_gvt_init_workload_scheduler(gvt);
+ 	if (ret)
+ 		goto out_clean_gtt;
+ 
+ 	ret = intel_gvt_init_sched_policy(gvt);
+ 	if (ret)
+ 		goto out_clean_workload_scheduler;
+ 
+ 	ret = intel_gvt_init_cmd_parser(gvt);
+ 	if (ret)
+ 		goto out_clean_sched_policy;
+ 
+ 	ret = init_service_thread(gvt);
+ 	if (ret)
+ 		goto out_clean_cmd_parser;
+ 
+ 	ret = intel_gvt_init_vgpu_types(gvt);
+ 	if (ret)
+ 		goto out_clean_thread;
+ 
+ 	vgpu = intel_gvt_create_idle_vgpu(gvt);
+ 	if (IS_ERR(vgpu)) {
+ 		ret = PTR_ERR(vgpu);
+ 		gvt_err("failed to create idle vgpu\n");
+ 		goto out_clean_types;
+ 	}
+ 	gvt->idle_vgpu = vgpu;
+ 
+ 	intel_gvt_debugfs_init(gvt);
+ 
+ 	ret = intel_gvt_init_vgpu_type_groups(gvt);
+ 	if (ret)
+ 		goto out_destroy_idle_vgpu;
+ 
+ 	ret = mdev_register_device(i915->drm.dev, &intel_vgpu_mdev_ops);
+ 	if (ret)
+ 		goto out_cleanup_vgpu_type_groups;
+ 
+ 	gvt_dbg_core("gvt device initialization is done\n");
+ 	return 0;
+ 
+ out_cleanup_vgpu_type_groups:
+ 	intel_gvt_cleanup_vgpu_type_groups(gvt);
+ out_destroy_idle_vgpu:
+ 	intel_gvt_destroy_idle_vgpu(gvt->idle_vgpu);
+ 	intel_gvt_debugfs_clean(gvt);
+ out_clean_types:
+ 	intel_gvt_clean_vgpu_types(gvt);
+ out_clean_thread:
+ 	clean_service_thread(gvt);
+ out_clean_cmd_parser:
+ 	intel_gvt_clean_cmd_parser(gvt);
+ out_clean_sched_policy:
+ 	intel_gvt_clean_sched_policy(gvt);
+ out_clean_workload_scheduler:
+ 	intel_gvt_clean_workload_scheduler(gvt);
+ out_clean_gtt:
+ 	intel_gvt_clean_gtt(gvt);
+ out_free_firmware:
+ 	intel_gvt_free_firmware(gvt);
+ out_clean_mmio_info:
+ 	intel_gvt_clean_mmio_info(gvt);
+ out_clean_idr:
+ 	idr_destroy(&gvt->vgpu_idr);
+ 	kfree(gvt);
+ 	i915->gvt = NULL;
+ 	return ret;
+ }
+ 
+ static void intel_gvt_pm_resume(struct drm_i915_private *i915)
+ {
+ 	struct intel_gvt *gvt = i915->gvt;
+ 
+ 	intel_gvt_restore_fence(gvt);
+ 	intel_gvt_restore_mmio(gvt);
+ 	intel_gvt_restore_ggtt(gvt);
+ }
+ 
+ static const struct intel_vgpu_ops intel_gvt_vgpu_ops = {
+ 	.init_device	= intel_gvt_init_device,
+ 	.clean_device	= intel_gvt_clean_device,
+ 	.pm_resume	= intel_gvt_pm_resume,
++>>>>>>> cba619cb0d4d (drm/i915/gvt: merge gvt.c into kvmgvt.c)
  };
  
  static int __init kvmgt_init(void)
* Unmerged path drivers/gpu/drm/i915/gvt/gvt.c
* Unmerged path drivers/gpu/drm/i915/gvt/Makefile
* Unmerged path drivers/gpu/drm/i915/gvt/gvt.c
* Unmerged path drivers/gpu/drm/i915/gvt/gvt.h
* Unmerged path drivers/gpu/drm/i915/gvt/kvmgt.c
