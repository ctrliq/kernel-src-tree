swiotlb: fix setting ->force_bounce

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Christoph Hellwig <hch@lst.de>
commit e15db62bc5648ab459a570862f654e787c498faf
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/e15db62b.failed

The swiotlb_init refactor messed up assigning ->force_bounce by doing
it in different places based on what caused the setting of the flag.

Fix this by passing the SWIOTLB_* flags to swiotlb_init_io_tlb_mem
and just setting it there.

Fixes: c6af2aa9ffc9 ("swiotlb: make the swiotlb_init interface more useful")
	Reported-by: Nathan Chancellor <nathan@kernel.org>
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Tested-by: Nathan Chancellor <nathan@kernel.org>
(cherry picked from commit e15db62bc5648ab459a570862f654e787c498faf)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index eb8c08292910,cb50f8d38360..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -206,8 -203,7 +206,12 @@@ static void swiotlb_init_io_tlb_mem(str
  	mem->index = 0;
  	mem->late_alloc = late_alloc;
  
++<<<<<<< HEAD
 +	if (swiotlb_force == SWIOTLB_FORCE)
 +		mem->force_bounce = true;
++=======
+ 	mem->force_bounce = swiotlb_force_bounce || (flags & SWIOTLB_FORCE);
++>>>>>>> e15db62bc564 (swiotlb: fix setting ->force_bounce)
  
  	spin_lock_init(&mem->lock);
  	for (i = 0; i < mem->nslabs; i++) {
@@@ -246,38 -274,15 +250,42 @@@ int __init swiotlb_init_with_tbl(char *
  		panic("%s: Failed to allocate %zu bytes align=0x%lx\n",
  		      __func__, alloc_size, PAGE_SIZE);
  
++<<<<<<< HEAD
 +	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
++=======
+ 	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, flags, false);
++>>>>>>> e15db62bc564 (swiotlb: fix setting ->force_bounce)
  
 -	if (flags & SWIOTLB_VERBOSE)
 +	if (verbose)
  		swiotlb_print_info();
 +	return 0;
  }
  
 -void __init swiotlb_init(bool addressing_limit, unsigned int flags)
 +/*
 + * Statically reserve bounce buffer space and initialize bounce buffer data
 + * structures for the software IO TLB used to implement the DMA API.
 + */
 +void  __init
 +swiotlb_init(int verbose)
  {
 -	return swiotlb_init_remap(addressing_limit, flags, NULL);
 +	size_t bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
 +	void *tlb;
 +
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
 +		return;
 +
 +	/* Get IO TLB memory from the low pages */
 +	tlb = memblock_alloc_low_nopanic(bytes, PAGE_SIZE);
 +	if (!tlb)
 +		goto fail;
 +	if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
 +		goto fail_free_mem;
 +	return;
 +
 +fail_free_mem:
 +	memblock_free_early(__pa(tlb), bytes);
 +fail:
 +	pr_warn("Cannot allocate buffer");
  }
  
  /*
@@@ -338,11 -339,14 +346,17 @@@ swiotlb_late_init_with_tbl(char *tlb, u
  
  	mem->slots = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,
  		get_order(array_size(sizeof(*mem->slots), nslabs)));
 -	if (!mem->slots) {
 -		free_pages((unsigned long)vstart, order);
 +	if (!mem->slots)
  		return -ENOMEM;
 -	}
  
++<<<<<<< HEAD
 +	set_memory_decrypted((unsigned long)tlb, bytes >> PAGE_SHIFT);
 +	swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
++=======
+ 	set_memory_decrypted((unsigned long)vstart,
+ 			     (nslabs << IO_TLB_SHIFT) >> PAGE_SHIFT);
+ 	swiotlb_init_io_tlb_mem(mem, virt_to_phys(vstart), nslabs, 0, true);
++>>>>>>> e15db62bc564 (swiotlb: fix setting ->force_bounce)
  
  	swiotlb_print_info();
  	return 0;
* Unmerged path kernel/dma/swiotlb.c
