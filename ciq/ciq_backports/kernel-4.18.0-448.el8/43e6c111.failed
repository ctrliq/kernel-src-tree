dm: change from DMWARN to DMERR or DMCRIT for fatal errors

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit 43e6c111824c75940a586cd7d3fe6a5ff1d5104f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/43e6c111.failed

Change DMWARN to DMERR in cases when there is an unrecoverable error.
Change DMWARN to DMCRIT when handling of a case is unimplemented.

	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Mike Snitzer <snitzer@kernel.org>
(cherry picked from commit 43e6c111824c75940a586cd7d3fe6a5ff1d5104f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-ioctl.c
#	drivers/md/dm-table.c
diff --cc drivers/md/dm-ioctl.c
index 7c87b145fb80,6b3f867d0b70..000000000000
--- a/drivers/md/dm-ioctl.c
+++ b/drivers/md/dm-ioctl.c
@@@ -2101,3 -2144,108 +2101,111 @@@ out
  	return r;
  }
  EXPORT_SYMBOL_GPL(dm_copy_name_and_uuid);
++<<<<<<< HEAD
++=======
+ 
+ /**
+  * dm_early_create - create a mapped device in early boot.
+  *
+  * @dmi: Contains main information of the device mapping to be created.
+  * @spec_array: array of pointers to struct dm_target_spec. Describes the
+  * mapping table of the device.
+  * @target_params_array: array of strings with the parameters to a specific
+  * target.
+  *
+  * Instead of having the struct dm_target_spec and the parameters for every
+  * target embedded at the end of struct dm_ioctl (as performed in a normal
+  * ioctl), pass them as arguments, so the caller doesn't need to serialize them.
+  * The size of the spec_array and target_params_array is given by
+  * @dmi->target_count.
+  * This function is supposed to be called in early boot, so locking mechanisms
+  * to protect against concurrent loads are not required.
+  */
+ int __init dm_early_create(struct dm_ioctl *dmi,
+ 			   struct dm_target_spec **spec_array,
+ 			   char **target_params_array)
+ {
+ 	int r, m = DM_ANY_MINOR;
+ 	struct dm_table *t, *old_map;
+ 	struct mapped_device *md;
+ 	unsigned int i;
+ 
+ 	if (!dmi->target_count)
+ 		return -EINVAL;
+ 
+ 	r = check_name(dmi->name);
+ 	if (r)
+ 		return r;
+ 
+ 	if (dmi->flags & DM_PERSISTENT_DEV_FLAG)
+ 		m = MINOR(huge_decode_dev(dmi->dev));
+ 
+ 	/* alloc dm device */
+ 	r = dm_create(m, &md);
+ 	if (r)
+ 		return r;
+ 
+ 	/* hash insert */
+ 	r = dm_hash_insert(dmi->name, *dmi->uuid ? dmi->uuid : NULL, md);
+ 	if (r)
+ 		goto err_destroy_dm;
+ 
+ 	/* alloc table */
+ 	r = dm_table_create(&t, get_mode(dmi), dmi->target_count, md);
+ 	if (r)
+ 		goto err_hash_remove;
+ 
+ 	/* add targets */
+ 	for (i = 0; i < dmi->target_count; i++) {
+ 		r = dm_table_add_target(t, spec_array[i]->target_type,
+ 					(sector_t) spec_array[i]->sector_start,
+ 					(sector_t) spec_array[i]->length,
+ 					target_params_array[i]);
+ 		if (r) {
+ 			DMERR("error adding target to table");
+ 			goto err_destroy_table;
+ 		}
+ 	}
+ 
+ 	/* finish table */
+ 	r = dm_table_complete(t);
+ 	if (r)
+ 		goto err_destroy_table;
+ 
+ 	/* setup md->queue to reflect md's type (may block) */
+ 	r = dm_setup_md_queue(md, t);
+ 	if (r) {
+ 		DMERR("unable to set up device queue for new table.");
+ 		goto err_destroy_table;
+ 	}
+ 
+ 	/* Set new map */
+ 	dm_suspend(md, 0);
+ 	old_map = dm_swap_table(md, t);
+ 	if (IS_ERR(old_map)) {
+ 		r = PTR_ERR(old_map);
+ 		goto err_destroy_table;
+ 	}
+ 	set_disk_ro(dm_disk(md), !!(dmi->flags & DM_READONLY_FLAG));
+ 
+ 	/* resume device */
+ 	r = dm_resume(md);
+ 	if (r)
+ 		goto err_destroy_table;
+ 
+ 	DMINFO("%s (%s) is ready", md->disk->disk_name, dmi->name);
+ 	dm_put(md);
+ 	return 0;
+ 
+ err_destroy_table:
+ 	dm_table_destroy(t);
+ err_hash_remove:
+ 	(void) __hash_remove(__get_name_cell(dmi->name));
+ 	/* release reference from __get_name_cell */
+ 	dm_put(md);
+ err_destroy_dm:
+ 	dm_put(md);
+ 	dm_destroy(md);
+ 	return r;
+ }
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
diff --cc drivers/md/dm-table.c
index 09c684df4adc,078da18bb86d..000000000000
--- a/drivers/md/dm-table.c
+++ b/drivers/md/dm-table.c
@@@ -233,12 -234,12 +233,21 @@@ static int device_area_is_invalid(struc
  		return 0;
  
  	if ((start >= dev_size) || (start + len > dev_size)) {
++<<<<<<< HEAD
 +		DMWARN("%s: %s too small for target: "
 +		       "start=%llu, len=%llu, dev_size=%llu",
 +		       dm_device_name(ti->table->md), bdevname(bdev, b),
 +		       (unsigned long long)start,
 +		       (unsigned long long)len,
 +		       (unsigned long long)dev_size);
++=======
+ 		DMERR("%s: %pg too small for target: "
+ 		      "start=%llu, len=%llu, dev_size=%llu",
+ 		      dm_device_name(ti->table->md), bdev,
+ 		      (unsigned long long)start,
+ 		      (unsigned long long)len,
+ 		      (unsigned long long)dev_size);
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
  		return 1;
  	}
  
@@@ -250,10 -251,10 +259,17 @@@
  		unsigned int zone_sectors = bdev_zone_sectors(bdev);
  
  		if (start & (zone_sectors - 1)) {
++<<<<<<< HEAD
 +			DMWARN("%s: start=%llu not aligned to h/w zone size %u of %s",
 +			       dm_device_name(ti->table->md),
 +			       (unsigned long long)start,
 +			       zone_sectors, bdevname(bdev, b));
++=======
+ 			DMERR("%s: start=%llu not aligned to h/w zone size %u of %pg",
+ 			      dm_device_name(ti->table->md),
+ 			      (unsigned long long)start,
+ 			      zone_sectors, bdev);
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
  			return 1;
  		}
  
@@@ -267,10 -268,10 +283,17 @@@
  		 * the sector range.
  		 */
  		if (len & (zone_sectors - 1)) {
++<<<<<<< HEAD
 +			DMWARN("%s: len=%llu not aligned to h/w zone size %u of %s",
 +			       dm_device_name(ti->table->md),
 +			       (unsigned long long)len,
 +			       zone_sectors, bdevname(bdev, b));
++=======
+ 			DMERR("%s: len=%llu not aligned to h/w zone size %u of %pg",
+ 			      dm_device_name(ti->table->md),
+ 			      (unsigned long long)len,
+ 			      zone_sectors, bdev);
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
  			return 1;
  		}
  	}
@@@ -279,20 -280,20 +302,36 @@@
  		return 0;
  
  	if (start & (logical_block_size_sectors - 1)) {
++<<<<<<< HEAD
 +		DMWARN("%s: start=%llu not aligned to h/w "
 +		       "logical block size %u of %s",
 +		       dm_device_name(ti->table->md),
 +		       (unsigned long long)start,
 +		       limits->logical_block_size, bdevname(bdev, b));
++=======
+ 		DMERR("%s: start=%llu not aligned to h/w "
+ 		      "logical block size %u of %pg",
+ 		      dm_device_name(ti->table->md),
+ 		      (unsigned long long)start,
+ 		      limits->logical_block_size, bdev);
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
  		return 1;
  	}
  
  	if (len & (logical_block_size_sectors - 1)) {
++<<<<<<< HEAD
 +		DMWARN("%s: len=%llu not aligned to h/w "
 +		       "logical block size %u of %s",
 +		       dm_device_name(ti->table->md),
 +		       (unsigned long long)len,
 +		       limits->logical_block_size, bdevname(bdev, b));
++=======
+ 		DMERR("%s: len=%llu not aligned to h/w "
+ 		      "logical block size %u of %pg",
+ 		      dm_device_name(ti->table->md),
+ 		      (unsigned long long)len,
+ 		      limits->logical_block_size, bdev);
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
  		return 1;
  	}
  
@@@ -627,12 -618,12 +666,21 @@@ static int validate_hardware_logical_bl
  	}
  
  	if (remaining) {
++<<<<<<< HEAD
 +		DMWARN("%s: table line %u (start sect %llu len %llu) "
 +		       "not aligned to h/w logical block size %u",
 +		       dm_device_name(table->md), i,
 +		       (unsigned long long) ti->begin,
 +		       (unsigned long long) ti->len,
 +		       limits->logical_block_size);
++=======
+ 		DMERR("%s: table line %u (start sect %llu len %llu) "
+ 		      "not aligned to h/w logical block size %u",
+ 		      dm_device_name(t->md), i,
+ 		      (unsigned long long) ti->begin,
+ 		      (unsigned long long) ti->len,
+ 		      limits->logical_block_size);
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
  		return -EINVAL;
  	}
  
@@@ -1020,13 -1001,14 +1068,13 @@@ bool dm_table_request_based(struct dm_t
  static int dm_table_alloc_md_mempools(struct dm_table *t, struct mapped_device *md)
  {
  	enum dm_queue_mode type = dm_table_get_type(t);
 -	unsigned int per_io_data_size = 0, front_pad, io_front_pad;
 -	unsigned int min_pool_size = 0, pool_size;
 -	struct dm_md_mempools *pools;
 +	unsigned per_io_data_size = 0;
 +	unsigned min_pool_size = 0;
 +	struct dm_target *ti;
 +	unsigned i;
  
  	if (unlikely(type == DM_TYPE_NONE)) {
- 		DMWARN("no table type is set, can't allocate mempools");
+ 		DMERR("no table type is set, can't allocate mempools");
  		return -EINVAL;
  	}
  
@@@ -1187,6 -1197,206 +1235,209 @@@ static int dm_table_register_integrity(
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_BLK_INLINE_ENCRYPTION
+ 
+ struct dm_crypto_profile {
+ 	struct blk_crypto_profile profile;
+ 	struct mapped_device *md;
+ };
+ 
+ struct dm_keyslot_evict_args {
+ 	const struct blk_crypto_key *key;
+ 	int err;
+ };
+ 
+ static int dm_keyslot_evict_callback(struct dm_target *ti, struct dm_dev *dev,
+ 				     sector_t start, sector_t len, void *data)
+ {
+ 	struct dm_keyslot_evict_args *args = data;
+ 	int err;
+ 
+ 	err = blk_crypto_evict_key(bdev_get_queue(dev->bdev), args->key);
+ 	if (!args->err)
+ 		args->err = err;
+ 	/* Always try to evict the key from all devices. */
+ 	return 0;
+ }
+ 
+ /*
+  * When an inline encryption key is evicted from a device-mapper device, evict
+  * it from all the underlying devices.
+  */
+ static int dm_keyslot_evict(struct blk_crypto_profile *profile,
+ 			    const struct blk_crypto_key *key, unsigned int slot)
+ {
+ 	struct mapped_device *md =
+ 		container_of(profile, struct dm_crypto_profile, profile)->md;
+ 	struct dm_keyslot_evict_args args = { key };
+ 	struct dm_table *t;
+ 	int srcu_idx;
+ 
+ 	t = dm_get_live_table(md, &srcu_idx);
+ 	if (!t)
+ 		return 0;
+ 
+ 	for (unsigned int i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
+ 
+ 		if (!ti->type->iterate_devices)
+ 			continue;
+ 		ti->type->iterate_devices(ti, dm_keyslot_evict_callback, &args);
+ 	}
+ 
+ 	dm_put_live_table(md, srcu_idx);
+ 	return args.err;
+ }
+ 
+ static int
+ device_intersect_crypto_capabilities(struct dm_target *ti, struct dm_dev *dev,
+ 				     sector_t start, sector_t len, void *data)
+ {
+ 	struct blk_crypto_profile *parent = data;
+ 	struct blk_crypto_profile *child =
+ 		bdev_get_queue(dev->bdev)->crypto_profile;
+ 
+ 	blk_crypto_intersect_capabilities(parent, child);
+ 	return 0;
+ }
+ 
+ void dm_destroy_crypto_profile(struct blk_crypto_profile *profile)
+ {
+ 	struct dm_crypto_profile *dmcp = container_of(profile,
+ 						      struct dm_crypto_profile,
+ 						      profile);
+ 
+ 	if (!profile)
+ 		return;
+ 
+ 	blk_crypto_profile_destroy(profile);
+ 	kfree(dmcp);
+ }
+ 
+ static void dm_table_destroy_crypto_profile(struct dm_table *t)
+ {
+ 	dm_destroy_crypto_profile(t->crypto_profile);
+ 	t->crypto_profile = NULL;
+ }
+ 
+ /*
+  * Constructs and initializes t->crypto_profile with a crypto profile that
+  * represents the common set of crypto capabilities of the devices described by
+  * the dm_table.  However, if the constructed crypto profile doesn't support all
+  * crypto capabilities that are supported by the current mapped_device, it
+  * returns an error instead, since we don't support removing crypto capabilities
+  * on table changes.  Finally, if the constructed crypto profile is "empty" (has
+  * no crypto capabilities at all), it just sets t->crypto_profile to NULL.
+  */
+ static int dm_table_construct_crypto_profile(struct dm_table *t)
+ {
+ 	struct dm_crypto_profile *dmcp;
+ 	struct blk_crypto_profile *profile;
+ 	unsigned int i;
+ 	bool empty_profile = true;
+ 
+ 	dmcp = kmalloc(sizeof(*dmcp), GFP_KERNEL);
+ 	if (!dmcp)
+ 		return -ENOMEM;
+ 	dmcp->md = t->md;
+ 
+ 	profile = &dmcp->profile;
+ 	blk_crypto_profile_init(profile, 0);
+ 	profile->ll_ops.keyslot_evict = dm_keyslot_evict;
+ 	profile->max_dun_bytes_supported = UINT_MAX;
+ 	memset(profile->modes_supported, 0xFF,
+ 	       sizeof(profile->modes_supported));
+ 
+ 	for (i = 0; i < t->num_targets; i++) {
+ 		struct dm_target *ti = dm_table_get_target(t, i);
+ 
+ 		if (!dm_target_passes_crypto(ti->type)) {
+ 			blk_crypto_intersect_capabilities(profile, NULL);
+ 			break;
+ 		}
+ 		if (!ti->type->iterate_devices)
+ 			continue;
+ 		ti->type->iterate_devices(ti,
+ 					  device_intersect_crypto_capabilities,
+ 					  profile);
+ 	}
+ 
+ 	if (t->md->queue &&
+ 	    !blk_crypto_has_capabilities(profile,
+ 					 t->md->queue->crypto_profile)) {
+ 		DMERR("Inline encryption capabilities of new DM table were more restrictive than the old table's. This is not supported!");
+ 		dm_destroy_crypto_profile(profile);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/*
+ 	 * If the new profile doesn't actually support any crypto capabilities,
+ 	 * we may as well represent it with a NULL profile.
+ 	 */
+ 	for (i = 0; i < ARRAY_SIZE(profile->modes_supported); i++) {
+ 		if (profile->modes_supported[i]) {
+ 			empty_profile = false;
+ 			break;
+ 		}
+ 	}
+ 
+ 	if (empty_profile) {
+ 		dm_destroy_crypto_profile(profile);
+ 		profile = NULL;
+ 	}
+ 
+ 	/*
+ 	 * t->crypto_profile is only set temporarily while the table is being
+ 	 * set up, and it gets set to NULL after the profile has been
+ 	 * transferred to the request_queue.
+ 	 */
+ 	t->crypto_profile = profile;
+ 
+ 	return 0;
+ }
+ 
+ static void dm_update_crypto_profile(struct request_queue *q,
+ 				     struct dm_table *t)
+ {
+ 	if (!t->crypto_profile)
+ 		return;
+ 
+ 	/* Make the crypto profile less restrictive. */
+ 	if (!q->crypto_profile) {
+ 		blk_crypto_register(t->crypto_profile, q);
+ 	} else {
+ 		blk_crypto_update_capabilities(q->crypto_profile,
+ 					       t->crypto_profile);
+ 		dm_destroy_crypto_profile(t->crypto_profile);
+ 	}
+ 	t->crypto_profile = NULL;
+ }
+ 
+ #else /* CONFIG_BLK_INLINE_ENCRYPTION */
+ 
+ static int dm_table_construct_crypto_profile(struct dm_table *t)
+ {
+ 	return 0;
+ }
+ 
+ void dm_destroy_crypto_profile(struct blk_crypto_profile *profile)
+ {
+ }
+ 
+ static void dm_table_destroy_crypto_profile(struct dm_table *t)
+ {
+ }
+ 
+ static void dm_update_crypto_profile(struct request_queue *q,
+ 				     struct dm_table *t)
+ {
+ }
+ 
+ #endif /* !CONFIG_BLK_INLINE_ENCRYPTION */
+ 
++>>>>>>> 43e6c111824c (dm: change from DMWARN to DMERR or DMCRIT for fatal errors)
  /*
   * Prepares the table for use by building the indices,
   * setting the type, and allocating mempools.
* Unmerged path drivers/md/dm-ioctl.c
diff --git a/drivers/md/dm-rq.c b/drivers/md/dm-rq.c
index 5ab340c56885..39a22d96984d 100644
--- a/drivers/md/dm-rq.c
+++ b/drivers/md/dm-rq.c
@@ -243,7 +243,7 @@ static void dm_done(struct request *clone, blk_status_t error, bool mapped)
 		dm_requeue_original_request(tio, true);
 		break;
 	default:
-		DMWARN("unimplemented target endio return value: %d", r);
+		DMCRIT("unimplemented target endio return value: %d", r);
 		BUG();
 	}
 }
@@ -420,7 +420,7 @@ static int map_request(struct dm_rq_target_io *tio)
 		dm_kill_unmapped_request(rq, BLK_STS_IOERR);
 		break;
 	default:
-		DMWARN("unimplemented target map return value: %d", r);
+		DMCRIT("unimplemented target map return value: %d", r);
 		BUG();
 	}
 
diff --git a/drivers/md/dm-stats.c b/drivers/md/dm-stats.c
index 8326f9fe0e91..f105a71915ab 100644
--- a/drivers/md/dm-stats.c
+++ b/drivers/md/dm-stats.c
@@ -1220,7 +1220,7 @@ int dm_stats_message(struct mapped_device *md, unsigned argc, char **argv,
 		return 2; /* this wasn't a stats message */
 
 	if (r == -EINVAL)
-		DMWARN("Invalid parameters for message %s", argv[0]);
+		DMCRIT("Invalid parameters for message %s", argv[0]);
 
 	return r;
 }
* Unmerged path drivers/md/dm-table.c
diff --git a/drivers/md/dm.c b/drivers/md/dm.c
index ba3a15b86e8a..0f3731769036 100644
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -903,7 +903,7 @@ int dm_set_geometry(struct mapped_device *md, struct hd_geometry *geo)
 	sector_t sz = (sector_t)geo->cylinders * geo->heads * geo->sectors;
 
 	if (geo->start > sz) {
-		DMWARN("Start sector is beyond the geometry limits.");
+		DMERR("Start sector is beyond the geometry limits.");
 		return -EINVAL;
 	}
 
@@ -1084,7 +1084,7 @@ static void clone_endio(struct bio *bio)
 			/* The target will handle the io */
 			return;
 		default:
-			DMWARN("unimplemented target endio return value: %d", r);
+			DMCRIT("unimplemented target endio return value: %d", r);
 			BUG();
 		}
 	}
@@ -1428,7 +1428,7 @@ static blk_qc_t __map_bio(struct bio *clone)
 			dm_io_dec_pending(io, BLK_STS_DM_REQUEUE);
 		break;
 	default:
-		DMWARN("unimplemented target map return value: %d", r);
+		DMCRIT("unimplemented target map return value: %d", r);
 		BUG();
 	}
 
@@ -1902,7 +1902,7 @@ static struct mapped_device *alloc_dev(int minor)
 
 	md = kvzalloc_node(sizeof(*md), GFP_KERNEL, numa_node_id);
 	if (!md) {
-		DMWARN("unable to allocate device, out of memory.");
+		DMERR("unable to allocate device, out of memory.");
 		return NULL;
 	}
 
