provide arch_test_bit_acquire for architectures that define test_bit

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit d6ffe6067a54972564552ea45d320fb98db1ac5e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/d6ffe606.failed

Some architectures define their own arch_test_bit and they also need
arch_test_bit_acquire, otherwise they won't compile.  We also clean up
the code by using the generic test_bit if that is equivalent to the
arch-specific version.

	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Cc: stable@vger.kernel.org
Fixes: 8238b4579866 ("wait_on_bit: add an acquire memory barrier")
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d6ffe6067a54972564552ea45d320fb98db1ac5e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/alpha/include/asm/bitops.h
#	arch/hexagon/include/asm/bitops.h
#	arch/ia64/include/asm/bitops.h
#	arch/m68k/include/asm/bitops.h
#	arch/s390/include/asm/bitops.h
#	arch/sh/include/asm/bitops-op32.h
diff --cc arch/alpha/include/asm/bitops.h
index ca43f4d0b937,bafb1c1f0fdc..000000000000
--- a/arch/alpha/include/asm/bitops.h
+++ b/arch/alpha/include/asm/bitops.h
@@@ -283,11 -283,8 +283,16 @@@ __test_and_change_bit(unsigned long nr
  	return (old & mask) != 0;
  }
  
++<<<<<<< HEAD
 +static inline int
 +test_bit(int nr, const volatile void * addr)
 +{
 +	return (1UL & (((const int *) addr)[nr >> 5] >> (nr & 31))) != 0UL;
 +}
++=======
+ #define arch_test_bit generic_test_bit
+ #define arch_test_bit_acquire generic_test_bit_acquire
++>>>>>>> d6ffe6067a54 (provide arch_test_bit_acquire for architectures that define test_bit)
  
  /*
   * ffz = Find First Zero in word. Undefined if no zero exists,
diff --cc arch/hexagon/include/asm/bitops.h
index 5e4a59b3ec1b,160d8f37fa1a..000000000000
--- a/arch/hexagon/include/asm/bitops.h
+++ b/arch/hexagon/include/asm/bitops.h
@@@ -186,7 -179,20 +186,24 @@@ static inline int __test_bit(int nr, co
  	return retval;
  }
  
++<<<<<<< HEAD
 +#define test_bit(nr, addr) __test_bit(nr, addr)
++=======
+ static __always_inline bool
+ arch_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
+ {
+ 	int retval;
+ 
+ 	asm volatile(
+ 	"{P0 = tstbit(%1,%2); if (P0.new) %0 = #1; if (!P0.new) %0 = #0;}\n"
+ 	: "=&r" (retval)
+ 	: "r" (addr[BIT_WORD(nr)]), "r" (nr % BITS_PER_LONG)
+ 	: "p0", "memory"
+ 	);
+ 
+ 	return retval;
+ }
++>>>>>>> d6ffe6067a54 (provide arch_test_bit_acquire for architectures that define test_bit)
  
  /*
   * ffz - find first zero in word.
diff --cc arch/ia64/include/asm/bitops.h
index 56a774bf13fa,1accb7842f58..000000000000
--- a/arch/ia64/include/asm/bitops.h
+++ b/arch/ia64/include/asm/bitops.h
@@@ -331,11 -331,8 +331,16 @@@ __test_and_change_bit (int nr, void *ad
  	return (old & bit) != 0;
  }
  
++<<<<<<< HEAD
 +static __inline__ int
 +test_bit (int nr, const volatile void *addr)
 +{
 +	return 1 & (((const volatile __u32 *) addr)[nr >> 5] >> (nr & 31));
 +}
++=======
+ #define arch_test_bit generic_test_bit
+ #define arch_test_bit_acquire generic_test_bit_acquire
++>>>>>>> d6ffe6067a54 (provide arch_test_bit_acquire for architectures that define test_bit)
  
  /**
   * ffz - find the first zero bit in a long word
diff --cc arch/m68k/include/asm/bitops.h
index 93b47b1f6fb4,e984af71df6b..000000000000
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@@ -145,14 -151,14 +145,19 @@@ static inline void bfchg_mem_change_bit
  				bfchg_mem_change_bit(nr, vaddr))
  #endif
  
 -static __always_inline void
 -arch___change_bit(unsigned long nr, volatile unsigned long *addr)
 +#define __change_bit(nr, vaddr)	change_bit(nr, vaddr)
 +
 +
 +static inline int test_bit(int nr, const volatile unsigned long *vaddr)
  {
 -	change_bit(nr, addr);
 +	return (vaddr[nr >> 5] & (1UL << (nr & 31))) != 0;
  }
  
++<<<<<<< HEAD
++=======
+ #define arch_test_bit generic_test_bit
+ #define arch_test_bit_acquire generic_test_bit_acquire
++>>>>>>> d6ffe6067a54 (provide arch_test_bit_acquire for architectures that define test_bit)
  
  static inline int bset_reg_test_and_set_bit(int nr,
  					    volatile unsigned long *vaddr)
diff --cc arch/s390/include/asm/bitops.h
index 86e5b2fdee3c,2de74fcd0578..000000000000
--- a/arch/s390/include/asm/bitops.h
+++ b/arch/s390/include/asm/bitops.h
@@@ -49,194 -50,161 +49,199 @@@ __bitops_word(unsigned long nr, volatil
  	return (unsigned long *)addr;
  }
  
 -static inline unsigned long __bitops_mask(unsigned long nr)
 +static inline unsigned char *
 +__bitops_byte(unsigned long nr, volatile unsigned long *ptr)
  {
 -	return 1UL << (nr & (BITS_PER_LONG - 1));
 +	return ((unsigned char *)ptr) + ((nr ^ (BITS_PER_LONG - 8)) >> 3);
  }
  
 -static __always_inline void arch_set_bit(unsigned long nr, volatile unsigned long *ptr)
 +static inline void set_bit(unsigned long nr, volatile unsigned long *ptr)
  {
  	unsigned long *addr = __bitops_word(nr, ptr);
 -	unsigned long mask = __bitops_mask(nr);
 +	unsigned long mask;
  
 -	__atomic64_or(mask, (long *)addr);
 +#ifdef CONFIG_HAVE_MARCH_ZEC12_FEATURES
 +	if (__builtin_constant_p(nr)) {
 +		unsigned char *caddr = __bitops_byte(nr, ptr);
 +
 +		asm volatile(
 +			"oi	%0,%b1\n"
 +			: "+Q" (*caddr)
 +			: "i" (1 << (nr & 7))
 +			: "cc", "memory");
 +		return;
 +	}
 +#endif
 +	mask = 1UL << (nr & (BITS_PER_LONG - 1));
 +	__atomic64_or(mask, addr);
  }
  
 -static __always_inline void arch_clear_bit(unsigned long nr, volatile unsigned long *ptr)
 +static inline void clear_bit(unsigned long nr, volatile unsigned long *ptr)
  {
  	unsigned long *addr = __bitops_word(nr, ptr);
 -	unsigned long mask = __bitops_mask(nr);
 +	unsigned long mask;
 +
 +#ifdef CONFIG_HAVE_MARCH_ZEC12_FEATURES
 +	if (__builtin_constant_p(nr)) {
 +		unsigned char *caddr = __bitops_byte(nr, ptr);
  
 -	__atomic64_and(~mask, (long *)addr);
 +		asm volatile(
 +			"ni	%0,%b1\n"
 +			: "+Q" (*caddr)
 +			: "i" (~(1 << (nr & 7)))
 +			: "cc", "memory");
 +		return;
 +	}
 +#endif
 +	mask = ~(1UL << (nr & (BITS_PER_LONG - 1)));
 +	__atomic64_and(mask, addr);
  }
  
 -static __always_inline void arch_change_bit(unsigned long nr,
 -					    volatile unsigned long *ptr)
 +static inline void change_bit(unsigned long nr, volatile unsigned long *ptr)
  {
  	unsigned long *addr = __bitops_word(nr, ptr);
 -	unsigned long mask = __bitops_mask(nr);
 +	unsigned long mask;
 +
 +#ifdef CONFIG_HAVE_MARCH_ZEC12_FEATURES
 +	if (__builtin_constant_p(nr)) {
 +		unsigned char *caddr = __bitops_byte(nr, ptr);
  
 -	__atomic64_xor(mask, (long *)addr);
 +		asm volatile(
 +			"xi	%0,%b1\n"
 +			: "+Q" (*caddr)
 +			: "i" (1 << (nr & 7))
 +			: "cc", "memory");
 +		return;
 +	}
 +#endif
 +	mask = 1UL << (nr & (BITS_PER_LONG - 1));
 +	__atomic64_xor(mask, addr);
  }
  
 -static inline bool arch_test_and_set_bit(unsigned long nr,
 -					 volatile unsigned long *ptr)
 +static inline int
 +test_and_set_bit(unsigned long nr, volatile unsigned long *ptr)
  {
  	unsigned long *addr = __bitops_word(nr, ptr);
 -	unsigned long mask = __bitops_mask(nr);
 -	unsigned long old;
 +	unsigned long old, mask;
  
 -	old = __atomic64_or_barrier(mask, (long *)addr);
 -	return old & mask;
 +	mask = 1UL << (nr & (BITS_PER_LONG - 1));
 +	old = __atomic64_or_barrier(mask, addr);
 +	return (old & mask) != 0;
  }
  
 -static inline bool arch_test_and_clear_bit(unsigned long nr,
 -					   volatile unsigned long *ptr)
 +static inline int
 +test_and_clear_bit(unsigned long nr, volatile unsigned long *ptr)
  {
  	unsigned long *addr = __bitops_word(nr, ptr);
 -	unsigned long mask = __bitops_mask(nr);
 -	unsigned long old;
 +	unsigned long old, mask;
  
 -	old = __atomic64_and_barrier(~mask, (long *)addr);
 -	return old & mask;
 +	mask = ~(1UL << (nr & (BITS_PER_LONG - 1)));
 +	old = __atomic64_and_barrier(mask, addr);
 +	return (old & ~mask) != 0;
  }
  
 -static inline bool arch_test_and_change_bit(unsigned long nr,
 -					    volatile unsigned long *ptr)
 +static inline int
 +test_and_change_bit(unsigned long nr, volatile unsigned long *ptr)
  {
  	unsigned long *addr = __bitops_word(nr, ptr);
 -	unsigned long mask = __bitops_mask(nr);
 -	unsigned long old;
 +	unsigned long old, mask;
  
 -	old = __atomic64_xor_barrier(mask, (long *)addr);
 -	return old & mask;
 +	mask = 1UL << (nr & (BITS_PER_LONG - 1));
 +	old = __atomic64_xor_barrier(mask, addr);
 +	return (old & mask) != 0;
  }
  
 -static __always_inline void
 -arch___set_bit(unsigned long nr, volatile unsigned long *addr)
 +static inline void __set_bit(unsigned long nr, volatile unsigned long *ptr)
  {
 -	unsigned long *p = __bitops_word(nr, addr);
 -	unsigned long mask = __bitops_mask(nr);
 +	unsigned char *addr = __bitops_byte(nr, ptr);
  
 -	*p |= mask;
 +	*addr |= 1 << (nr & 7);
  }
  
 -static __always_inline void
 -arch___clear_bit(unsigned long nr, volatile unsigned long *addr)
 +static inline void 
 +__clear_bit(unsigned long nr, volatile unsigned long *ptr)
  {
 -	unsigned long *p = __bitops_word(nr, addr);
 -	unsigned long mask = __bitops_mask(nr);
 +	unsigned char *addr = __bitops_byte(nr, ptr);
  
 -	*p &= ~mask;
 +	*addr &= ~(1 << (nr & 7));
  }
  
 -static __always_inline void
 -arch___change_bit(unsigned long nr, volatile unsigned long *addr)
 +static inline void __change_bit(unsigned long nr, volatile unsigned long *ptr)
  {
 -	unsigned long *p = __bitops_word(nr, addr);
 -	unsigned long mask = __bitops_mask(nr);
 +	unsigned char *addr = __bitops_byte(nr, ptr);
  
 -	*p ^= mask;
 +	*addr ^= 1 << (nr & 7);
  }
  
 -static __always_inline bool
 -arch___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
 +static inline int
 +__test_and_set_bit(unsigned long nr, volatile unsigned long *ptr)
  {
 -	unsigned long *p = __bitops_word(nr, addr);
 -	unsigned long mask = __bitops_mask(nr);
 -	unsigned long old;
 +	unsigned char *addr = __bitops_byte(nr, ptr);
 +	unsigned char ch;
  
 -	old = *p;
 -	*p |= mask;
 -	return old & mask;
 +	ch = *addr;
 +	*addr |= 1 << (nr & 7);
 +	return (ch >> (nr & 7)) & 1;
  }
  
 -static __always_inline bool
 -arch___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
 +static inline int
 +__test_and_clear_bit(unsigned long nr, volatile unsigned long *ptr)
  {
 -	unsigned long *p = __bitops_word(nr, addr);
 -	unsigned long mask = __bitops_mask(nr);
 -	unsigned long old;
 +	unsigned char *addr = __bitops_byte(nr, ptr);
 +	unsigned char ch;
  
 -	old = *p;
 -	*p &= ~mask;
 -	return old & mask;
 +	ch = *addr;
 +	*addr &= ~(1 << (nr & 7));
 +	return (ch >> (nr & 7)) & 1;
  }
  
 -static __always_inline bool
 -arch___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
 +static inline int
 +__test_and_change_bit(unsigned long nr, volatile unsigned long *ptr)
  {
 -	unsigned long *p = __bitops_word(nr, addr);
 -	unsigned long mask = __bitops_mask(nr);
 -	unsigned long old;
 +	unsigned char *addr = __bitops_byte(nr, ptr);
 +	unsigned char ch;
  
 -	old = *p;
 -	*p ^= mask;
 -	return old & mask;
 +	ch = *addr;
 +	*addr ^= 1 << (nr & 7);
 +	return (ch >> (nr & 7)) & 1;
  }
  
++<<<<<<< HEAD
 +static inline int test_bit(unsigned long nr, const volatile unsigned long *ptr)
 +{
 +	const volatile unsigned char *addr;
 +
 +	addr = ((const volatile unsigned char *)ptr);
 +	addr += (nr ^ (BITS_PER_LONG - 8)) >> 3;
 +	return (*addr >> (nr & 7)) & 1;
 +}
++=======
+ #define arch_test_bit generic_test_bit
+ #define arch_test_bit_acquire generic_test_bit_acquire
++>>>>>>> d6ffe6067a54 (provide arch_test_bit_acquire for architectures that define test_bit)
  
 -static inline bool arch_test_and_set_bit_lock(unsigned long nr,
 -					      volatile unsigned long *ptr)
 +static inline int test_and_set_bit_lock(unsigned long nr,
 +					volatile unsigned long *ptr)
  {
 -	if (arch_test_bit(nr, ptr))
 -		return true;
 -	return arch_test_and_set_bit(nr, ptr);
 +	if (test_bit(nr, ptr))
 +		return 1;
 +	return test_and_set_bit(nr, ptr);
  }
  
 -static inline void arch_clear_bit_unlock(unsigned long nr,
 -					 volatile unsigned long *ptr)
 +static inline void clear_bit_unlock(unsigned long nr,
 +				    volatile unsigned long *ptr)
  {
  	smp_mb__before_atomic();
 -	arch_clear_bit(nr, ptr);
 +	clear_bit(nr, ptr);
  }
  
 -static inline void arch___clear_bit_unlock(unsigned long nr,
 -					   volatile unsigned long *ptr)
 +static inline void __clear_bit_unlock(unsigned long nr,
 +				      volatile unsigned long *ptr)
  {
  	smp_mb();
 -	arch___clear_bit(nr, ptr);
 +	__clear_bit(nr, ptr);
  }
  
 -#include <asm-generic/bitops/instrumented-atomic.h>
 -#include <asm-generic/bitops/instrumented-non-atomic.h>
 -#include <asm-generic/bitops/instrumented-lock.h>
 -
  /*
   * Functions which use MSB0 bit numbering.
   * The bits are numbered:
diff --cc arch/sh/include/asm/bitops-op32.h
index 466880362ad1,5ace89b46507..000000000000
--- a/arch/sh/include/asm/bitops-op32.h
+++ b/arch/sh/include/asm/bitops-op32.h
@@@ -130,14 -135,9 +130,19 @@@ static inline int __test_and_change_bit
  	return (old & mask) != 0;
  }
  
++<<<<<<< HEAD
 +/**
 + * test_bit - Determine whether a bit is set
 + * @nr: bit number to test
 + * @addr: Address to start counting from
 + */
 +static inline int test_bit(int nr, const volatile unsigned long *addr)
 +{
 +	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
 +}
++=======
+ #define arch_test_bit generic_test_bit
+ #define arch_test_bit_acquire generic_test_bit_acquire
 -
 -#include <asm-generic/bitops/non-instrumented-non-atomic.h>
++>>>>>>> d6ffe6067a54 (provide arch_test_bit_acquire for architectures that define test_bit)
  
  #endif /* __ASM_SH_BITOPS_OP32_H */
* Unmerged path arch/alpha/include/asm/bitops.h
* Unmerged path arch/hexagon/include/asm/bitops.h
* Unmerged path arch/ia64/include/asm/bitops.h
* Unmerged path arch/m68k/include/asm/bitops.h
* Unmerged path arch/s390/include/asm/bitops.h
* Unmerged path arch/sh/include/asm/bitops-op32.h
