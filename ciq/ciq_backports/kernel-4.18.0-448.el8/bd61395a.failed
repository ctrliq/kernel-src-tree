KVM: arm64: nvhe: Eliminate kernel-doc warnings

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Randy Dunlap <rdunlap@infradead.org>
commit bd61395ae8393f28f4b084702acd6f5f02b1f7c0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/bd61395a.failed

Don't use begin-kernel-doc notation (/**) for comments that are not in
kernel-doc format.

This prevents these kernel-doc warnings:

arch/arm64/kvm/hyp/nvhe/switch.c:126: warning: This comment starts with '/**', but isn't a kernel-doc comment. Refer Documentation/doc-guide/kernel-doc.rst
 * Disable host events, enable guest events
arch/arm64/kvm/hyp/nvhe/switch.c:146: warning: This comment starts with '/**', but isn't a kernel-doc comment. Refer Documentation/doc-guide/kernel-doc.rst
 * Disable guest events, enable host events
arch/arm64/kvm/hyp/nvhe/switch.c:164: warning: This comment starts with '/**', but isn't a kernel-doc comment. Refer Documentation/doc-guide/kernel-doc.rst
 * Handler for protected VM restricted exceptions.
arch/arm64/kvm/hyp/nvhe/switch.c:176: warning: This comment starts with '/**', but isn't a kernel-doc comment. Refer Documentation/doc-guide/kernel-doc.rst
 * Handler for protected VM MSR, MRS or System instruction execution in AArch64.
arch/arm64/kvm/hyp/nvhe/switch.c:196: warning: Function parameter or member 'vcpu' not described in 'kvm_handle_pvm_fpsimd'
arch/arm64/kvm/hyp/nvhe/switch.c:196: warning: Function parameter or member 'exit_code' not described in 'kvm_handle_pvm_fpsimd'
arch/arm64/kvm/hyp/nvhe/switch.c:196: warning: expecting prototype for Handler for protected floating(). Prototype was for kvm_handle_pvm_fpsimd() instead

Fixes: 09cf57eba304 ("KVM: arm64: Split hyp/switch.c to VHE/nVHE")
Fixes: 1423afcb4117 ("KVM: arm64: Trap access to pVM restricted features")
	Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
	Reported-by: kernel test robot <lkp@intel.com>
	Cc: Fuad Tabba <tabba@google.com>
	Cc: Marc Zyngier <maz@kernel.org>
	Cc: David Brazdil <dbrazdil@google.com>
	Cc: James Morse <james.morse@arm.com>
	Cc: Alexandru Elisei <alexandru.elisei@arm.com>
	Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
	Cc: linux-arm-kernel@lists.infradead.org
	Cc: kvmarm@lists.cs.columbia.edu
	Signed-off-by: Marc Zyngier <maz@kernel.org>
Link: https://lore.kernel.org/r/20220430050123.2844-1-rdunlap@infradead.org
(cherry picked from commit bd61395ae8393f28f4b084702acd6f5f02b1f7c0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/kvm/hyp/nvhe/switch.c
diff --cc arch/arm64/kvm/hyp/nvhe/switch.c
index 6859759d63ed,476196ea90c4..000000000000
--- a/arch/arm64/kvm/hyp/nvhe/switch.c
+++ b/arch/arm64/kvm/hyp/nvhe/switch.c
@@@ -154,6 -158,98 +154,101 @@@ static void __pmu_switch_to_host(struc
  		write_sysreg(pmu->events_host, pmcntenset_el0);
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Handler for protected VM MSR, MRS or System instruction execution in AArch64.
+  *
+  * Returns true if the hypervisor has handled the exit, and control should go
+  * back to the guest, or false if it hasn't.
+  */
+ static bool kvm_handle_pvm_sys64(struct kvm_vcpu *vcpu, u64 *exit_code)
+ {
+ 	/*
+ 	 * Make sure we handle the exit for workarounds and ptrauth
+ 	 * before the pKVM handling, as the latter could decide to
+ 	 * UNDEF.
+ 	 */
+ 	return (kvm_hyp_handle_sysreg(vcpu, exit_code) ||
+ 		kvm_handle_pvm_sysreg(vcpu, exit_code));
+ }
+ 
+ /*
+  * Handler for protected floating-point and Advanced SIMD accesses.
+  *
+  * Returns true if the hypervisor has handled the exit, and control should go
+  * back to the guest, or false if it hasn't.
+  */
+ static bool kvm_handle_pvm_fpsimd(struct kvm_vcpu *vcpu, u64 *exit_code)
+ {
+ 	/* Linux guests assume support for floating-point and Advanced SIMD. */
+ 	BUILD_BUG_ON(!FIELD_GET(ARM64_FEATURE_MASK(ID_AA64PFR0_FP),
+ 				PVM_ID_AA64PFR0_ALLOW));
+ 	BUILD_BUG_ON(!FIELD_GET(ARM64_FEATURE_MASK(ID_AA64PFR0_ASIMD),
+ 				PVM_ID_AA64PFR0_ALLOW));
+ 
+ 	return kvm_hyp_handle_fpsimd(vcpu, exit_code);
+ }
+ 
+ static const exit_handler_fn hyp_exit_handlers[] = {
+ 	[0 ... ESR_ELx_EC_MAX]		= NULL,
+ 	[ESR_ELx_EC_CP15_32]		= kvm_hyp_handle_cp15_32,
+ 	[ESR_ELx_EC_SYS64]		= kvm_hyp_handle_sysreg,
+ 	[ESR_ELx_EC_SVE]		= kvm_hyp_handle_fpsimd,
+ 	[ESR_ELx_EC_FP_ASIMD]		= kvm_hyp_handle_fpsimd,
+ 	[ESR_ELx_EC_IABT_LOW]		= kvm_hyp_handle_iabt_low,
+ 	[ESR_ELx_EC_DABT_LOW]		= kvm_hyp_handle_dabt_low,
+ 	[ESR_ELx_EC_PAC]		= kvm_hyp_handle_ptrauth,
+ };
+ 
+ static const exit_handler_fn pvm_exit_handlers[] = {
+ 	[0 ... ESR_ELx_EC_MAX]		= NULL,
+ 	[ESR_ELx_EC_SYS64]		= kvm_handle_pvm_sys64,
+ 	[ESR_ELx_EC_SVE]		= kvm_handle_pvm_restricted,
+ 	[ESR_ELx_EC_FP_ASIMD]		= kvm_handle_pvm_fpsimd,
+ 	[ESR_ELx_EC_IABT_LOW]		= kvm_hyp_handle_iabt_low,
+ 	[ESR_ELx_EC_DABT_LOW]		= kvm_hyp_handle_dabt_low,
+ 	[ESR_ELx_EC_PAC]		= kvm_hyp_handle_ptrauth,
+ };
+ 
+ static const exit_handler_fn *kvm_get_exit_handler_array(struct kvm_vcpu *vcpu)
+ {
+ 	if (unlikely(kvm_vm_is_protected(kern_hyp_va(vcpu->kvm))))
+ 		return pvm_exit_handlers;
+ 
+ 	return hyp_exit_handlers;
+ }
+ 
+ /*
+  * Some guests (e.g., protected VMs) are not be allowed to run in AArch32.
+  * The ARMv8 architecture does not give the hypervisor a mechanism to prevent a
+  * guest from dropping to AArch32 EL0 if implemented by the CPU. If the
+  * hypervisor spots a guest in such a state ensure it is handled, and don't
+  * trust the host to spot or fix it.  The check below is based on the one in
+  * kvm_arch_vcpu_ioctl_run().
+  *
+  * Returns false if the guest ran in AArch32 when it shouldn't have, and
+  * thus should exit to the host, or true if a the guest run loop can continue.
+  */
+ static void early_exit_filter(struct kvm_vcpu *vcpu, u64 *exit_code)
+ {
+ 	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
+ 
+ 	if (kvm_vm_is_protected(kvm) && vcpu_mode_is_32bit(vcpu)) {
+ 		/*
+ 		 * As we have caught the guest red-handed, decide that it isn't
+ 		 * fit for purpose anymore by making the vcpu invalid. The VMM
+ 		 * can try and fix it by re-initializing the vcpu with
+ 		 * KVM_ARM_VCPU_INIT, however, this is likely not possible for
+ 		 * protected VMs.
+ 		 */
+ 		vcpu->arch.target = -1;
+ 		*exit_code &= BIT(ARM_EXIT_WITH_SERROR_BIT);
+ 		*exit_code |= ARM_EXCEPTION_IL;
+ 	}
+ }
+ 
++>>>>>>> bd61395ae839 (KVM: arm64: nvhe: Eliminate kernel-doc warnings)
  /* Switch to the guest for legacy non-VHE systems */
  int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
  {
* Unmerged path arch/arm64/kvm/hyp/nvhe/switch.c
