mm/gup: longterm pin migration cleanup

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Pavel Tatashin <pasha.tatashin@soleen.com>
commit f68749ec342b5f2c18b3af3435714d9f653736c3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/f68749ec.failed

When pages are longterm pinned, we must migrated them out of movable zone.
The function that migrates them has a hidden loop with goto.  The loop is
to retry on isolation failures, and after successful migration.

Make this code better by moving this loop to the caller.

Link: https://lkml.kernel.org/r/20210215161349.246722-13-pasha.tatashin@soleen.com
	Signed-off-by: Pavel Tatashin <pasha.tatashin@soleen.com>
	Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
	Cc: Dan Williams <dan.j.williams@intel.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: David Rientjes <rientjes@google.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: James Morris <jmorris@namei.org>
	Cc: Jason Gunthorpe <jgg@ziepe.ca>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Mel Gorman <mgorman@suse.de>
	Cc: Michal Hocko <mhocko@kernel.org>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: Oscar Salvador <osalvador@suse.de>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: Sasha Levin <sashal@kernel.org>
	Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
	Cc: Tyler Hicks <tyhicks@linux.microsoft.com>
	Cc: Vlastimil Babka <vbabka@suse.cz>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit f68749ec342b5f2c18b3af3435714d9f653736c3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/gup.c
diff --cc mm/gup.c
index 9f322889c06f,aa09535cf4d4..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -1809,156 -1566,59 +1809,177 @@@ finish_or_fault
  #endif /* !CONFIG_MMU */
  
  /**
 - * get_dump_page() - pin user page in memory while writing it to core dump
 - * @addr: user address
 + * fault_in_writeable - fault in userspace address range for writing
 + * @uaddr: start of address range
 + * @size: size of address range
   *
 - * Returns struct page pointer of user page pinned for dump,
 - * to be freed afterwards by put_page().
 + * Returns the number of bytes not faulted in (like copy_to_user() and
 + * copy_from_user()).
 + */
 +size_t fault_in_writeable(char __user *uaddr, size_t size)
 +{
 +	char __user *start = uaddr, *end;
 +
 +	if (unlikely(size == 0))
 +		return 0;
 +	if (!PAGE_ALIGNED(uaddr)) {
 +		if (unlikely(__put_user(0, uaddr) != 0))
 +			return size;
 +		uaddr = (char __user *)PAGE_ALIGN((unsigned long)uaddr);
 +	}
 +	end = (char __user *)PAGE_ALIGN((unsigned long)start + size);
 +	if (unlikely(end < start))
 +		end = NULL;
 +	while (uaddr != end) {
 +		if (unlikely(__put_user(0, uaddr) != 0))
 +			goto out;
 +		uaddr += PAGE_SIZE;
 +	}
 +
 +out:
 +	if (size > uaddr - start)
 +		return size - (uaddr - start);
 +	return 0;
 +}
 +EXPORT_SYMBOL(fault_in_writeable);
 +
++<<<<<<< HEAD
 +/*
 + * fault_in_safe_writeable - fault in an address range for writing
 + * @uaddr: start of address range
 + * @size: length of address range
   *
 - * Returns NULL on any kind of failure - a hole must then be inserted into
 - * the corefile, to preserve alignment with its headers; and also returns
 - * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -
 - * allowing a hole to be left in the corefile to save diskspace.
 + * Faults in an address range for writing.  This is primarily useful when we
 + * already know that some or all of the pages in the address range aren't in
 + * memory.
 + *
 + * Unlike fault_in_writeable(), this function is non-destructive.
 + *
 + * Note that we don't pin or otherwise hold the pages referenced that we fault
 + * in.  There's no guarantee that they'll stay in memory for any duration of
 + * time.
   *
 - * Called without mmap_lock (takes and releases the mmap_lock by itself).
 + * Returns the number of bytes not faulted in, like copy_to_user() and
 + * copy_from_user().
   */
 -#ifdef CONFIG_ELF_CORE
 -struct page *get_dump_page(unsigned long addr)
 +size_t fault_in_safe_writeable(const char __user *uaddr, size_t size)
  {
 +	unsigned long start = (unsigned long)uaddr, end;
  	struct mm_struct *mm = current->mm;
 -	struct page *page;
 -	int locked = 1;
 -	int ret;
 +	bool unlocked = false;
  
 -	if (mmap_read_lock_killable(mm))
 -		return NULL;
 -	ret = __get_user_pages_locked(mm, addr, 1, &page, NULL, &locked,
 -				      FOLL_FORCE | FOLL_DUMP | FOLL_GET);
 -	if (locked)
 -		mmap_read_unlock(mm);
 +	if (unlikely(size == 0))
 +		return 0;
 +	end = PAGE_ALIGN(start + size);
 +	if (end < start)
 +		end = 0;
  
 -	if (ret == 1 && is_page_poisoned(page))
 -		return NULL;
 +	mmap_read_lock(mm);
 +	do {
 +		if (fixup_user_fault(current, mm, start, FAULT_FLAG_WRITE,
 +				     &unlocked))
 +			break;
 +		start = (start + PAGE_SIZE) & PAGE_MASK;
 +	} while (start != end);
 +	mmap_read_unlock(mm);
  
 -	return (ret == 1) ? page : NULL;
 +	if (size > (unsigned long)uaddr - start)
 +		return size - ((unsigned long)uaddr - start);
 +	return 0;
  }
 -#endif /* CONFIG_ELF_CORE */
 +EXPORT_SYMBOL(fault_in_safe_writeable);
  
 +/**
 + * fault_in_readable - fault in userspace address range for reading
 + * @uaddr: start of user address range
 + * @size: size of user address range
 + *
 + * Returns the number of bytes not faulted in (like copy_to_user() and
 + * copy_from_user()).
 + */
 +size_t fault_in_readable(const char __user *uaddr, size_t size)
 +{
 +	const char __user *start = uaddr, *end;
 +	volatile char c;
 +
 +	if (unlikely(size == 0))
 +		return 0;
 +	if (!PAGE_ALIGNED(uaddr)) {
 +		if (unlikely(__get_user(c, uaddr) != 0))
 +			return size;
 +		uaddr = (const char __user *)PAGE_ALIGN((unsigned long)uaddr);
 +	}
 +	end = (const char __user *)PAGE_ALIGN((unsigned long)start + size);
 +	if (unlikely(end < start))
 +		end = NULL;
 +	while (uaddr != end) {
 +		if (unlikely(__get_user(c, uaddr) != 0))
 +			goto out;
 +		uaddr += PAGE_SIZE;
 +	}
 +
 +out:
 +	(void)c;
 +	if (size > uaddr - start)
 +		return size - (uaddr - start);
 +	return 0;
 +}
 +EXPORT_SYMBOL(fault_in_readable);
 +
 +#if defined(CONFIG_FS_DAX) || defined (CONFIG_CMA)
 +static bool check_dax_vmas(struct vm_area_struct **vmas, long nr_pages)
 +{
 +	long i;
 +	struct vm_area_struct *vma_prev = NULL;
 +
 +	for (i = 0; i < nr_pages; i++) {
 +		struct vm_area_struct *vma = vmas[i];
 +
 +		if (vma == vma_prev)
 +			continue;
 +
 +		vma_prev = vma;
 +
 +		if (vma_is_fsdax(vma))
 +			return true;
 +	}
 +	return false;
 +}
 +
 +#ifdef CONFIG_CMA
 +static long check_and_migrate_cma_pages(struct task_struct *tsk,
 +					struct mm_struct *mm,
 +					unsigned long start,
 +					unsigned long nr_pages,
 +					struct page **pages,
 +					struct vm_area_struct **vmas,
 +					unsigned int gup_flags)
 +{
 +	unsigned long i, isolation_error_count;
 +	bool drain_allow;
 +	LIST_HEAD(cma_page_list);
 +	long ret = nr_pages;
 +	struct page *prev_head, *head;
++=======
+ #ifdef CONFIG_MIGRATION
+ /*
+  * Check whether all pages are pinnable, if so return number of pages.  If some
+  * pages are not pinnable, migrate them, and unpin all pages. Return zero if
+  * pages were migrated, or if some pages were not successfully isolated.
+  * Return negative error if migration fails.
+  */
+ static long check_and_migrate_movable_pages(unsigned long nr_pages,
+ 					    struct page **pages,
+ 					    unsigned int gup_flags)
+ {
+ 	unsigned long i;
+ 	unsigned long isolation_error_count = 0;
+ 	bool drain_allow = true;
+ 	LIST_HEAD(movable_page_list);
+ 	long ret = 0;
+ 	struct page *prev_head = NULL;
+ 	struct page *head;
++>>>>>>> f68749ec342b (mm/gup: longterm pin migration cleanup)
  	struct migration_target_control mtc = {
  		.nid = NUMA_NO_NODE,
  		.gfp_mask = GFP_USER | __GFP_NOWARN,
@@@ -2005,50 -1660,29 +2022,70 @@@
  	 * If list is empty, and no isolation errors, means that all pages are
  	 * in the correct zone.
  	 */
++<<<<<<< HEAD
 +	if (list_empty(&cma_page_list) && !isolation_error_count)
 +		return ret;
 +
 +	if (!list_empty(&cma_page_list)) {
 +		/*
 +		 * drop the above get_user_pages reference.
 +		 */
 +		if (gup_flags & FOLL_PIN)
 +			unpin_user_pages(pages, nr_pages);
 +		else
 +			for (i = 0; i < nr_pages; i++)
 +				put_page(pages[i]);
 +
 +		ret = migrate_pages(&cma_page_list, alloc_migration_target,
 +				    NULL, (unsigned long)&mtc, MIGRATE_SYNC,
 +				    MR_CONTIG_RANGE);
 +		if (ret) {
 +			if (!list_empty(&cma_page_list))
 +				putback_movable_pages(&cma_page_list);
 +			return ret > 0 ? -ENOMEM : ret;
 +		}
 +
 +		/* We unpinned pages before migration, pin them again */
 +		ret = __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
 +					      NULL, gup_flags);
 +		if (ret <= 0)
 +			return ret;
 +		nr_pages = ret;
++=======
+ 	if (list_empty(&movable_page_list) && !isolation_error_count)
+ 		return nr_pages;
+ 
+ 	if (gup_flags & FOLL_PIN) {
+ 		unpin_user_pages(pages, nr_pages);
+ 	} else {
+ 		for (i = 0; i < nr_pages; i++)
+ 			put_page(pages[i]);
+ 	}
+ 	if (!list_empty(&movable_page_list)) {
+ 		ret = migrate_pages(&movable_page_list, alloc_migration_target,
+ 				    NULL, (unsigned long)&mtc, MIGRATE_SYNC,
+ 				    MR_LONGTERM_PIN);
+ 		if (ret && !list_empty(&movable_page_list))
+ 			putback_movable_pages(&movable_page_list);
++>>>>>>> f68749ec342b (mm/gup: longterm pin migration cleanup)
  	}
  
- 	/*
- 	 * check again because pages were unpinned, and we also might have
- 	 * had isolation errors and need more pages to migrate.
- 	 */
- 	goto check_again;
+ 	return ret > 0 ? -ENOMEM : ret;
  }
  #else
++<<<<<<< HEAD
 +static long check_and_migrate_cma_pages(struct task_struct *tsk,
 +					struct mm_struct *mm,
 +					unsigned long start,
 +					unsigned long nr_pages,
 +					struct page **pages,
 +					struct vm_area_struct **vmas,
 +					unsigned int gup_flags)
++=======
+ static long check_and_migrate_movable_pages(unsigned long nr_pages,
+ 					    struct page **pages,
+ 					    unsigned int gup_flags)
++>>>>>>> f68749ec342b (mm/gup: longterm pin migration cleanup)
  {
  	return nr_pages;
  }
@@@ -2066,64 -1699,24 +2103,83 @@@ static long __gup_longterm_locked(struc
  				  struct vm_area_struct **vmas,
  				  unsigned int gup_flags)
  {
++<<<<<<< HEAD
 +	struct vm_area_struct **vmas_tmp = vmas;
 +	unsigned long flags = 0;
 +	long rc, i;
 +
 +	if (gup_flags & FOLL_LONGTERM) {
 +		if (!pages)
 +			return -EINVAL;
 +
 +		if (!vmas_tmp) {
 +			vmas_tmp = kcalloc(nr_pages,
 +					   sizeof(struct vm_area_struct *),
 +					   GFP_KERNEL);
 +			if (!vmas_tmp)
 +				return -ENOMEM;
 +		}
 +		flags = memalloc_nocma_save();
 +	}
 +
 +	rc = __get_user_pages_locked(tsk, mm, start, nr_pages, pages,
 +				     vmas_tmp, NULL, gup_flags);
 +
 +	if (gup_flags & FOLL_LONGTERM) {
 +		if (rc < 0)
 +			goto out;
 +
 +		if (check_dax_vmas(vmas_tmp, rc)) {
 +			if (gup_flags & FOLL_PIN)
 +				unpin_user_pages(pages, rc);
 +			else
 +				for (i = 0; i < rc; i++)
 +					put_page(pages[i]);
 +			rc = -EOPNOTSUPP;
 +			goto out;
 +		}
 +
 +		rc = check_and_migrate_cma_pages(tsk, mm, start, rc, pages,
 +						 vmas_tmp, gup_flags);
 +out:
 +		memalloc_nocma_restore(flags);
 +	}
 +
 +	if (vmas_tmp != vmas)
 +		kfree(vmas_tmp);
++=======
+ 	unsigned int flags;
+ 	long rc;
+ 
+ 	if (!(gup_flags & FOLL_LONGTERM))
+ 		return __get_user_pages_locked(mm, start, nr_pages, pages, vmas,
+ 					       NULL, gup_flags);
+ 	flags = memalloc_pin_save();
+ 	do {
+ 		rc = __get_user_pages_locked(mm, start, nr_pages, pages, vmas,
+ 					     NULL, gup_flags);
+ 		if (rc <= 0)
+ 			break;
+ 		rc = check_and_migrate_movable_pages(rc, pages, gup_flags);
+ 	} while (!rc);
+ 	memalloc_pin_restore(flags);
+ 
++>>>>>>> f68749ec342b (mm/gup: longterm pin migration cleanup)
  	return rc;
  }
 +#else /* !CONFIG_FS_DAX && !CONFIG_CMA */
 +static __always_inline long __gup_longterm_locked(struct task_struct *tsk,
 +						  struct mm_struct *mm,
 +						  unsigned long start,
 +						  unsigned long nr_pages,
 +						  struct page **pages,
 +						  struct vm_area_struct **vmas,
 +						  unsigned int flags)
 +{
 +	return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
 +				       NULL, flags);
 +}
 +#endif /* CONFIG_FS_DAX || CONFIG_CMA */
  
  static bool is_valid_gup_flags(unsigned int gup_flags)
  {
* Unmerged path mm/gup.c
