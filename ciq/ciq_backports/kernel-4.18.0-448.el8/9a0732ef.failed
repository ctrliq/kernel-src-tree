arm64: kasan: don't populate vmalloc area for CONFIG_KASAN_VMALLOC

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Lecopzer Chen <lecopzer.chen@mediatek.com>
commit 9a0732efa77418fc85b1bdc5ddee619e62f59545
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/9a0732ef.failed

Linux support KAsan for VMALLOC since commit 3c5c3cfb9ef4da9
("kasan: support backing vmalloc space with real shadow memory")

Like how the MODULES_VADDR does now, just not to early populate
the VMALLOC_START between VMALLOC_END.

Before:

MODULE_VADDR: no mapping, no zero shadow at init
VMALLOC_VADDR: backed with zero shadow at init

After:

MODULE_VADDR: no mapping, no zero shadow at init
VMALLOC_VADDR: no mapping, no zero shadow at init

Thus the mapping will get allocated on demand by the core function
of KASAN_VMALLOC.

  -----------  vmalloc_shadow_start
 |           |
 |           |
 |           | <= non-mapping
 |           |
 |           |
 |-----------|
 |///////////|<- kimage shadow with page table mapping.
 |-----------|
 |           |
 |           | <= non-mapping
 |           |
 ------------- vmalloc_shadow_end
 |00000000000|
 |00000000000| <= Zero shadow
 |00000000000|
 ------------- KASAN_SHADOW_END

	Signed-off-by: Lecopzer Chen <lecopzer.chen@mediatek.com>
	Acked-by: Andrey Konovalov <andreyknvl@gmail.com>
	Tested-by: Andrey Konovalov <andreyknvl@gmail.com>
	Tested-by: Ard Biesheuvel <ardb@kernel.org>
Link: https://lore.kernel.org/r/20210324040522.15548-2-lecopzer.chen@mediatek.com
[catalin.marinas@arm.com: add a build check on VMALLOC_START != MODULES_END]
	Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
(cherry picked from commit 9a0732efa77418fc85b1bdc5ddee619e62f59545)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/mm/kasan_init.c
diff --cc arch/arm64/mm/kasan_init.c
index 6b7a8f8f5289,7598b0a96b64..000000000000
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@@ -207,8 -214,9 +207,14 @@@ static void __init kasan_init_shadow(vo
  {
  	u64 kimg_shadow_start, kimg_shadow_end;
  	u64 mod_shadow_start, mod_shadow_end;
++<<<<<<< HEAD
 +	struct memblock_region *reg;
 +	int i;
++=======
+ 	u64 vmalloc_shadow_end;
+ 	phys_addr_t pa_start, pa_end;
+ 	u64 i;
++>>>>>>> 9a0732efa774 (arm64: kasan: don't populate vmalloc area for CONFIG_KASAN_VMALLOC)
  
  	kimg_shadow_start = (u64)kasan_mem_to_shadow(_text) & PAGE_MASK;
  	kimg_shadow_end = PAGE_ALIGN((u64)kasan_mem_to_shadow(_end));
@@@ -234,16 -244,22 +242,22 @@@
  
  	kasan_populate_early_shadow(kasan_mem_to_shadow((void *)PAGE_END),
  				   (void *)mod_shadow_start);
- 	kasan_populate_early_shadow((void *)kimg_shadow_end,
- 				   (void *)KASAN_SHADOW_END);
  
- 	if (kimg_shadow_start > mod_shadow_end)
- 		kasan_populate_early_shadow((void *)mod_shadow_end,
- 					    (void *)kimg_shadow_start);
+ 	if (IS_ENABLED(CONFIG_KASAN_VMALLOC)) {
+ 		BUILD_BUG_ON(VMALLOC_START != MODULES_END);
+ 		kasan_populate_early_shadow((void *)vmalloc_shadow_end,
+ 					    (void *)KASAN_SHADOW_END);
+ 	} else {
+ 		kasan_populate_early_shadow((void *)kimg_shadow_end,
+ 					    (void *)KASAN_SHADOW_END);
+ 		if (kimg_shadow_start > mod_shadow_end)
+ 			kasan_populate_early_shadow((void *)mod_shadow_end,
+ 						    (void *)kimg_shadow_start);
+ 	}
  
 -	for_each_mem_range(i, &pa_start, &pa_end) {
 -		void *start = (void *)__phys_to_virt(pa_start);
 -		void *end = (void *)__phys_to_virt(pa_end);
 +	for_each_memblock(memory, reg) {
 +		void *start = (void *)__phys_to_virt(reg->base);
 +		void *end = (void *)__phys_to_virt(reg->base + reg->size);
  
  		if (start >= end)
  			break;
* Unmerged path arch/arm64/mm/kasan_init.c
