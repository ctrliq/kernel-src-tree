swiotlb: use the right nslabs-derived sizes in swiotlb_init_late

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 1b8e5d1a53696d92374acce2b19a649427f1ec1e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/1b8e5d1a.failed

nslabs can shrink when allocations or the remap don't succeed, so make
sure to use it for all sizing.  For that remove the bytes value that
can get stale and replace it with local calculations and a boolean to
indicate if the originally requested size could not be allocated.

Fixes: 6424e31b1c05 ("swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl")
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>
(cherry picked from commit 1b8e5d1a53696d92374acce2b19a649427f1ec1e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index eb8c08292910,d6e62a6a42ce..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -285,20 -292,22 +285,19 @@@ fail
   * initialize the swiotlb later using the slab allocator if needed.
   * This should be just like above, but with some error catching.
   */
 -int swiotlb_init_late(size_t size, gfp_t gfp_mask,
 -		int (*remap)(void *tlb, unsigned long nslabs))
 +int swiotlb_init_late(size_t size, gfp_t gfp_mask)
  {
 -	struct io_tlb_mem *mem = &io_tlb_default_mem;
  	unsigned long nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);
- 	unsigned long bytes;
  	unsigned char *vstart = NULL;
  	unsigned int order;
+ 	bool retried = false;
  	int rc = 0;
  
 -	if (swiotlb_force_disable)
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
  		return 0;
  
 -retry:
  	order = get_order(nslabs << IO_TLB_SHIFT);
  	nslabs = SLABS_PER_PAGE << order;
- 	bytes = nslabs << IO_TLB_SHIFT;
  
  	while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
  		vstart = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,
@@@ -311,38 -322,33 +312,63 @@@
  	if (!vstart)
  		return -ENOMEM;
  
++<<<<<<< HEAD
 +	if (order != get_order(bytes)) {
 +		pr_warn("only able to allocate %ld MB\n",
 +			(PAGE_SIZE << order) >> 20);
 +		nslabs = SLABS_PER_PAGE << order;
 +	}
 +	rc = swiotlb_late_init_with_tbl(vstart, nslabs);
 +	if (rc)
 +		free_pages((unsigned long)vstart, order);
 +
 +	return rc;
 +}
 +
 +int
 +swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 +{
 +	struct io_tlb_mem *mem = &io_tlb_default_mem;
 +	unsigned long bytes = nslabs << IO_TLB_SHIFT;
 +
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
 +		return 0;
 +
 +	/* protect against double initialization */
 +	if (WARN_ON_ONCE(mem->nslabs))
 +		return -ENOMEM;
++=======
+ 	if (remap)
+ 		rc = remap(vstart, nslabs);
+ 	if (rc) {
+ 		free_pages((unsigned long)vstart, order);
+ 
+ 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
+ 		if (nslabs < IO_TLB_MIN_SLABS)
+ 			return rc;
+ 		retried = true;
+ 		goto retry;
+ 	}
++>>>>>>> 1b8e5d1a5369 (swiotlb: use the right nslabs-derived sizes in swiotlb_init_late)
+ 
+ 	if (retried) {
+ 		pr_warn("only able to allocate %ld MB\n",
+ 			(PAGE_SIZE << order) >> 20);
+ 	}
  
  	mem->slots = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,
  		get_order(array_size(sizeof(*mem->slots), nslabs)));
 -	if (!mem->slots) {
 -		free_pages((unsigned long)vstart, order);
 +	if (!mem->slots)
  		return -ENOMEM;
 -	}
  
++<<<<<<< HEAD
 +	set_memory_decrypted((unsigned long)tlb, bytes >> PAGE_SHIFT);
 +	swiotlb_init_io_tlb_mem(mem, virt_to_phys(tlb), nslabs, true);
++=======
+ 	set_memory_decrypted((unsigned long)vstart,
+ 			     (nslabs << IO_TLB_SHIFT) >> PAGE_SHIFT);
+ 	swiotlb_init_io_tlb_mem(mem, virt_to_phys(vstart), nslabs, true);
++>>>>>>> 1b8e5d1a5369 (swiotlb: use the right nslabs-derived sizes in swiotlb_init_late)
  
  	swiotlb_print_info();
  	return 0;
* Unmerged path kernel/dma/swiotlb.c
