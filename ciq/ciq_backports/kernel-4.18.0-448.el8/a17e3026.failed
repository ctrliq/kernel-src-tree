iommu: Move flush queue data into iommu_dma_cookie

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Robin Murphy <robin.murphy@arm.com>
commit a17e3026bc4da9135ca9a42ec0b1fa67f95172e3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/a17e3026.failed

Complete the move into iommu-dma by refactoring the flush queues
themselves to belong to the DMA cookie rather than the IOVA domain.

The refactoring may as well extend to some minor cosmetic aspects
too, to help us stay one step ahead of the style police.

	Signed-off-by: Robin Murphy <robin.murphy@arm.com>
Link: https://lore.kernel.org/r/24304722005bc6f144e2a1fdd865d1465722fc2e.1639753638.git.robin.murphy@arm.com
	Signed-off-by: Joerg Roedel <jroedel@suse.de>
(cherry picked from commit a17e3026bc4da9135ca9a42ec0b1fa67f95172e3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/iommu/dma-iommu.c
#	drivers/iommu/iova.c
#	include/linux/iova.h
diff --cc drivers/iommu/dma-iommu.c
index 223a46c79116,d85d54f2b549..000000000000
--- a/drivers/iommu/dma-iommu.c
+++ b/drivers/iommu/dma-iommu.c
@@@ -20,10 -9,12 +20,17 @@@
   */
  
  #include <linux/acpi_iort.h>
+ #include <linux/atomic.h>
+ #include <linux/crash_dump.h>
  #include <linux/device.h>
++<<<<<<< HEAD
 +#include <linux/dma-map-ops.h>
 +#include <linux/dma-contiguous.h>
++=======
+ #include <linux/dma-direct.h>
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  #include <linux/dma-iommu.h>
+ #include <linux/dma-map-ops.h>
  #include <linux/gfp.h>
  #include <linux/huge_mm.h>
  #include <linux/iommu.h>
@@@ -76,18 -78,205 +94,218 @@@ static int __init iommu_dma_forcedac_se
  }
  early_param("iommu.forcedac", iommu_dma_forcedac_setup);
  
++<<<<<<< HEAD
 +static void iommu_dma_entry_dtor(unsigned long data)
++=======
+ /* Number of entries per flush queue */
+ #define IOVA_FQ_SIZE	256
+ 
+ /* Timeout (in ms) after which entries are flushed from the queue */
+ #define IOVA_FQ_TIMEOUT	10
+ 
+ /* Flush queue entry for deferred flushing */
+ struct iova_fq_entry {
+ 	unsigned long iova_pfn;
+ 	unsigned long pages;
+ 	struct list_head freelist;
+ 	u64 counter; /* Flush counter when this entry was added */
+ };
+ 
+ /* Per-CPU flush queue structure */
+ struct iova_fq {
+ 	struct iova_fq_entry entries[IOVA_FQ_SIZE];
+ 	unsigned int head, tail;
+ 	spinlock_t lock;
+ };
+ 
+ #define fq_ring_for_each(i, fq) \
+ 	for ((i) = (fq)->head; (i) != (fq)->tail; (i) = ((i) + 1) % IOVA_FQ_SIZE)
+ 
+ static inline bool fq_full(struct iova_fq *fq)
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  {
 -	assert_spin_locked(&fq->lock);
 -	return (((fq->tail + 1) % IOVA_FQ_SIZE) == fq->head);
 +	struct page *freelist = (struct page *)data;
 +
++<<<<<<< HEAD
 +	while (freelist) {
 +		unsigned long p = (unsigned long)page_address(freelist);
 +
 +		freelist = freelist->freelist;
 +		free_page(p);
 +	}
  }
  
++=======
+ static inline unsigned int fq_ring_add(struct iova_fq *fq)
+ {
+ 	unsigned int idx = fq->tail;
+ 
+ 	assert_spin_locked(&fq->lock);
+ 
+ 	fq->tail = (idx + 1) % IOVA_FQ_SIZE;
+ 
+ 	return idx;
+ }
+ 
+ static void fq_ring_free(struct iommu_dma_cookie *cookie, struct iova_fq *fq)
+ {
+ 	u64 counter = atomic64_read(&cookie->fq_flush_finish_cnt);
+ 	unsigned int idx;
+ 
+ 	assert_spin_locked(&fq->lock);
+ 
+ 	fq_ring_for_each(idx, fq) {
+ 
+ 		if (fq->entries[idx].counter >= counter)
+ 			break;
+ 
+ 		put_pages_list(&fq->entries[idx].freelist);
+ 		free_iova_fast(&cookie->iovad,
+ 			       fq->entries[idx].iova_pfn,
+ 			       fq->entries[idx].pages);
+ 
+ 		fq->head = (fq->head + 1) % IOVA_FQ_SIZE;
+ 	}
+ }
+ 
+ static void fq_flush_iotlb(struct iommu_dma_cookie *cookie)
+ {
+ 	atomic64_inc(&cookie->fq_flush_start_cnt);
+ 	cookie->fq_domain->ops->flush_iotlb_all(cookie->fq_domain);
+ 	atomic64_inc(&cookie->fq_flush_finish_cnt);
+ }
+ 
+ static void fq_flush_timeout(struct timer_list *t)
+ {
+ 	struct iommu_dma_cookie *cookie = from_timer(cookie, t, fq_timer);
+ 	int cpu;
+ 
+ 	atomic_set(&cookie->fq_timer_on, 0);
+ 	fq_flush_iotlb(cookie);
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		unsigned long flags;
+ 		struct iova_fq *fq;
+ 
+ 		fq = per_cpu_ptr(cookie->fq, cpu);
+ 		spin_lock_irqsave(&fq->lock, flags);
+ 		fq_ring_free(cookie, fq);
+ 		spin_unlock_irqrestore(&fq->lock, flags);
+ 	}
+ }
+ 
+ static void queue_iova(struct iommu_dma_cookie *cookie,
+ 		unsigned long pfn, unsigned long pages,
+ 		struct list_head *freelist)
+ {
+ 	struct iova_fq *fq;
+ 	unsigned long flags;
+ 	unsigned int idx;
+ 
+ 	/*
+ 	 * Order against the IOMMU driver's pagetable update from unmapping
+ 	 * @pte, to guarantee that fq_flush_iotlb() observes that if called
+ 	 * from a different CPU before we release the lock below. Full barrier
+ 	 * so it also pairs with iommu_dma_init_fq() to avoid seeing partially
+ 	 * written fq state here.
+ 	 */
+ 	smp_mb();
+ 
+ 	fq = raw_cpu_ptr(cookie->fq);
+ 	spin_lock_irqsave(&fq->lock, flags);
+ 
+ 	/*
+ 	 * First remove all entries from the flush queue that have already been
+ 	 * flushed out on another CPU. This makes the fq_full() check below less
+ 	 * likely to be true.
+ 	 */
+ 	fq_ring_free(cookie, fq);
+ 
+ 	if (fq_full(fq)) {
+ 		fq_flush_iotlb(cookie);
+ 		fq_ring_free(cookie, fq);
+ 	}
+ 
+ 	idx = fq_ring_add(fq);
+ 
+ 	fq->entries[idx].iova_pfn = pfn;
+ 	fq->entries[idx].pages    = pages;
+ 	fq->entries[idx].counter  = atomic64_read(&cookie->fq_flush_start_cnt);
+ 	list_splice(freelist, &fq->entries[idx].freelist);
+ 
+ 	spin_unlock_irqrestore(&fq->lock, flags);
+ 
+ 	/* Avoid false sharing as much as possible. */
+ 	if (!atomic_read(&cookie->fq_timer_on) &&
+ 	    !atomic_xchg(&cookie->fq_timer_on, 1))
+ 		mod_timer(&cookie->fq_timer,
+ 			  jiffies + msecs_to_jiffies(IOVA_FQ_TIMEOUT));
+ }
+ 
+ static void iommu_dma_free_fq(struct iommu_dma_cookie *cookie)
+ {
+ 	int cpu, idx;
+ 
+ 	if (!cookie->fq)
+ 		return;
+ 
+ 	del_timer_sync(&cookie->fq_timer);
+ 	/* The IOVAs will be torn down separately, so just free our queued pages */
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq = per_cpu_ptr(cookie->fq, cpu);
+ 
+ 		fq_ring_for_each(idx, fq)
+ 			put_pages_list(&fq->entries[idx].freelist);
+ 	}
+ 
+ 	free_percpu(cookie->fq);
+ }
+ 
+ /* sysfs updates are serialised by the mutex of the group owning @domain */
+ int iommu_dma_init_fq(struct iommu_domain *domain)
+ {
+ 	struct iommu_dma_cookie *cookie = domain->iova_cookie;
+ 	struct iova_fq __percpu *queue;
+ 	int i, cpu;
+ 
+ 	if (cookie->fq_domain)
+ 		return 0;
+ 
+ 	atomic64_set(&cookie->fq_flush_start_cnt,  0);
+ 	atomic64_set(&cookie->fq_flush_finish_cnt, 0);
+ 
+ 	queue = alloc_percpu(struct iova_fq);
+ 	if (!queue) {
+ 		pr_warn("iova flush queue initialization failed\n");
+ 		return -ENOMEM;
+ 	}
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		struct iova_fq *fq = per_cpu_ptr(queue, cpu);
+ 
+ 		fq->head = 0;
+ 		fq->tail = 0;
+ 
+ 		spin_lock_init(&fq->lock);
+ 
+ 		for (i = 0; i < IOVA_FQ_SIZE; i++)
+ 			INIT_LIST_HEAD(&fq->entries[i].freelist);
+ 	}
+ 
+ 	cookie->fq = queue;
+ 
+ 	timer_setup(&cookie->fq_timer, fq_flush_timeout, 0);
+ 	atomic_set(&cookie->fq_timer_on, 0);
+ 	/*
+ 	 * Prevent incomplete fq state being observable. Pairs with path from
+ 	 * __iommu_dma_unmap() through iommu_dma_free_iova() to queue_iova()
+ 	 */
+ 	smp_wmb();
+ 	WRITE_ONCE(cookie->fq_domain, domain);
+ 	return 0;
+ }
+ 
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  static inline size_t cookie_msi_granule(struct iommu_dma_cookie *cookie)
  {
  	if (cookie->type == IOMMU_DMA_IOVA_COOKIE)
@@@ -168,8 -357,10 +386,13 @@@ void iommu_put_dma_cookie(struct iommu_
  	if (!cookie)
  		return;
  
++<<<<<<< HEAD
 +	if (cookie->type == IOMMU_DMA_IOVA_COOKIE && cookie->iovad.granule)
++=======
+ 	if (cookie->type == IOMMU_DMA_IOVA_COOKIE && cookie->iovad.granule) {
+ 		iommu_dma_free_fq(cookie);
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  		put_iova_domain(&cookie->iovad);
 -	}
  
  	list_for_each_entry_safe(msi, tmp, &cookie->msi_page_list, list) {
  		list_del(&msi->list);
@@@ -469,10 -636,10 +692,15 @@@ static void iommu_dma_free_iova(struct 
  	/* The MSI case is only ever cleaning up its most recent allocation */
  	if (cookie->type == IOMMU_DMA_MSI_COOKIE)
  		cookie->msi_iova -= size;
++<<<<<<< HEAD
 +	else if (cookie->fq_domain)	/* non-strict mode */
 +		queue_iova(iovad, iova_pfn(iovad, iova),
++=======
+ 	else if (gather && gather->queued)
+ 		queue_iova(cookie, iova_pfn(iovad, iova),
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  				size >> iova_shift(iovad),
 -				&gather->freelist);
 +				(unsigned long)freelist);
  	else
  		free_iova_fast(iovad, iova_pfn(iovad, iova),
  				size >> iova_shift(iovad));
diff --cc drivers/iommu/iova.c
index 1c18cdc4d102,b28c9435b898..000000000000
--- a/drivers/iommu/iova.c
+++ b/drivers/iommu/iova.c
@@@ -75,8 -61,6 +75,11 @@@ init_iova_domain(struct iova_domain *io
  	iovad->start_pfn = start_pfn;
  	iovad->dma_32bit_pfn = 1UL << (32 - iova_shift(iovad));
  	iovad->max32_alloc_size = iovad->dma_32bit_pfn;
++<<<<<<< HEAD
 +	iovad->flush_cb = NULL;
 +	iovad->fq = NULL;
++=======
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  	iovad->anchor.pfn_lo = iovad->anchor.pfn_hi = IOVA_ANCHOR;
  	rb_link_node(&iovad->anchor.node, NULL, &iovad->rbroot.rb_node);
  	rb_insert_color(&iovad->anchor.node, &iovad->rbroot);
diff --cc include/linux/iova.h
index 4512ea4f00b8,0abd48c5e622..000000000000
--- a/include/linux/iova.h
+++ b/include/linux/iova.h
@@@ -14,8 -12,6 +14,11 @@@
  #include <linux/types.h>
  #include <linux/kernel.h>
  #include <linux/rbtree.h>
++<<<<<<< HEAD
 +#include <linux/atomic.h>
 +#include <linux/dma-mapping.h>
++=======
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  
  /* iova structure */
  struct iova {
@@@ -37,35 -33,6 +40,38 @@@ struct iova_rcache 
  	struct iova_cpu_rcache __percpu *cpu_rcaches;
  };
  
++<<<<<<< HEAD
 +struct iova_domain;
 +
 +/* Call-Back from IOVA code into IOMMU drivers */
 +typedef void (* iova_flush_cb)(struct iova_domain *domain);
 +
 +/* Destructor for per-entry data */
 +typedef void (* iova_entry_dtor)(unsigned long data);
 +
 +/* Number of entries per Flush Queue */
 +#define IOVA_FQ_SIZE	256
 +
 +/* Timeout (in ms) after which entries are flushed from the Flush-Queue */
 +#define IOVA_FQ_TIMEOUT	10
 +
 +/* Flush Queue entry for defered flushing */
 +struct iova_fq_entry {
 +	unsigned long iova_pfn;
 +	unsigned long pages;
 +	unsigned long data;
 +	u64 counter; /* Flush counter when this entrie was added */
 +};
 +
 +/* Per-CPU Flush Queue structure */
 +struct iova_fq {
 +	struct iova_fq_entry entries[IOVA_FQ_SIZE];
 +	unsigned head, tail;
 +	spinlock_t lock;
 +};
 +
++=======
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  /* holds all the iova translations for a domain */
  struct iova_domain {
  	spinlock_t	iova_rbtree_lock; /* Lock to protect update of rbtree */
@@@ -76,27 -43,9 +82,23 @@@
  	unsigned long	start_pfn;	/* Lower limit for this domain */
  	unsigned long	dma_32bit_pfn;
  	unsigned long	max32_alloc_size; /* Size of last failed allocation */
- 	struct iova_fq __percpu *fq;	/* Flush Queue */
- 
- 	atomic64_t	fq_flush_start_cnt;	/* Number of TLB flushes that
- 						   have been started */
- 
- 	atomic64_t	fq_flush_finish_cnt;	/* Number of TLB flushes that
- 						   have been finished */
- 
  	struct iova	anchor;		/* rbtree lookup anchor */
+ 
  	struct iova_rcache rcaches[IOVA_RANGE_CACHE_MAX_SIZE];	/* IOVA range caches */
++<<<<<<< HEAD
 +
 +	iova_flush_cb	flush_cb;	/* Call-Back function to flush IOMMU
 +					   TLBs */
 +
 +	iova_entry_dtor entry_dtor;	/* IOMMU driver specific destructor for
 +					   iova entry */
 +
 +	struct timer_list fq_timer;		/* Timer to regularily empty the
 +						   flush-queues */
 +	atomic_t fq_timer_on;			/* 1 when timer is active, 0
 +						   when not */
++=======
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  	struct hlist_node	cpuhp_dead;
  };
  
@@@ -146,17 -95,12 +148,23 @@@ struct iova *alloc_iova(struct iova_dom
  	bool size_aligned);
  void free_iova_fast(struct iova_domain *iovad, unsigned long pfn,
  		    unsigned long size);
++<<<<<<< HEAD
 +void queue_iova(struct iova_domain *iovad,
 +		unsigned long pfn, unsigned long pages,
 +		unsigned long data);
++=======
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  unsigned long alloc_iova_fast(struct iova_domain *iovad, unsigned long size,
  			      unsigned long limit_pfn, bool flush_rcache);
  struct iova *reserve_iova(struct iova_domain *iovad, unsigned long pfn_lo,
  	unsigned long pfn_hi);
  void init_iova_domain(struct iova_domain *iovad, unsigned long granule,
  	unsigned long start_pfn);
++<<<<<<< HEAD
 +int init_iova_flush_queue(struct iova_domain *iovad,
 +			  iova_flush_cb flush_cb, iova_entry_dtor entry_dtor);
++=======
++>>>>>>> a17e3026bc4d (iommu: Move flush queue data into iommu_dma_cookie)
  struct iova *find_iova(struct iova_domain *iovad, unsigned long pfn);
  void put_iova_domain(struct iova_domain *iovad);
  #else
* Unmerged path drivers/iommu/dma-iommu.c
* Unmerged path drivers/iommu/iova.c
* Unmerged path include/linux/iova.h
