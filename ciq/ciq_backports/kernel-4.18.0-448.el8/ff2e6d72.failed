asm-generic/tlb: rename HAVE_RCU_TABLE_FREE

jira LE-1907
cve CVE-2022-39188
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit ff2e6d7259f82ccc9a5aaa7f41194161d9262392
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/ff2e6d72.failed

Towards a more consistent naming scheme.

[akpm@linux-foundation.org: fix sparc64 Kconfig]
Link: http://lkml.kernel.org/r/20200116064531.483522-7-aneesh.kumar@linux.ibm.com
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit ff2e6d7259f82ccc9a5aaa7f41194161d9262392)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm/Kconfig
#	arch/arm/include/asm/tlb.h
#	arch/x86/Kconfig
diff --cc arch/arm/Kconfig
index bca60701e7a0,497e0e4702b7..000000000000
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@@ -88,7 -102,7 +88,11 @@@ config AR
  	select HAVE_PERF_EVENTS
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
++<<<<<<< HEAD
 +	select HAVE_RCU_TABLE_FREE if (SMP && ARM_LPAE)
++=======
+ 	select MMU_GATHER_RCU_TABLE_FREE if SMP && ARM_LPAE
++>>>>>>> ff2e6d7259f8 (asm-generic/tlb: rename HAVE_RCU_TABLE_FREE)
  	select HAVE_REGS_AND_STACK_ACCESS_API
  	select HAVE_RSEQ
  	select HAVE_STACKPROTECTOR
diff --cc arch/arm/include/asm/tlb.h
index 27d6bf4347d5,46a21cee3442..000000000000
--- a/arch/arm/include/asm/tlb.h
+++ b/arch/arm/include/asm/tlb.h
@@@ -41,218 -35,18 +41,223 @@@ static inline void __tlb_remove_table(v
  	free_page_and_swap_cache((struct page *)_table);
  }
  
 -#include <asm-generic/tlb.h>
 +struct mmu_table_batch {
 +	struct rcu_head		rcu;
 +	unsigned int		nr;
 +	void			*tables[0];
 +};
 +
++<<<<<<< HEAD
 +#define MAX_TABLE_BATCH		\
 +	((PAGE_SIZE - sizeof(struct mmu_table_batch)) / sizeof(void *))
 +
 +extern void tlb_table_flush(struct mmu_gather *tlb);
 +extern void tlb_remove_table(struct mmu_gather *tlb, void *table);
 +
 +#define tlb_remove_entry(tlb, entry)	tlb_remove_table(tlb, entry)
 +#else
 +#define tlb_remove_entry(tlb, entry)	tlb_remove_page(tlb, entry)
 +#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
  
 +/*
 + * TLB handling.  This allows us to remove pages from the page
 + * tables, and efficiently handle the TLB issues.
 + */
 +struct mmu_gather {
 +	struct mm_struct	*mm;
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	struct mmu_table_batch	*batch;
 +	unsigned int		need_flush;
++=======
+ #ifndef CONFIG_MMU_GATHER_RCU_TABLE_FREE
+ #define tlb_remove_table(tlb, entry) tlb_remove_page(tlb, entry)
++>>>>>>> ff2e6d7259f8 (asm-generic/tlb: rename HAVE_RCU_TABLE_FREE)
 +#endif
 +	unsigned int		fullmm;
 +	struct vm_area_struct	*vma;
 +	unsigned long		start, end;
 +	unsigned long		range_start;
 +	unsigned long		range_end;
 +	unsigned int		nr;
 +	unsigned int		max;
 +	struct page		**pages;
 +	struct page		*local[MMU_GATHER_BUNDLE];
 +};
 +
 +DECLARE_PER_CPU(struct mmu_gather, mmu_gathers);
 +
 +/*
 + * This is unnecessarily complex.  There's three ways the TLB shootdown
 + * code is used:
 + *  1. Unmapping a range of vmas.  See zap_page_range(), unmap_region().
 + *     tlb->fullmm = 0, and tlb_start_vma/tlb_end_vma will be called.
 + *     tlb->vma will be non-NULL.
 + *  2. Unmapping all vmas.  See exit_mmap().
 + *     tlb->fullmm = 1, and tlb_start_vma/tlb_end_vma will be called.
 + *     tlb->vma will be non-NULL.  Additionally, page tables will be freed.
 + *  3. Unmapping argument pages.  See shift_arg_pages().
 + *     tlb->fullmm = 0, but tlb_start_vma/tlb_end_vma will not be called.
 + *     tlb->vma will be NULL.
 + */
 +static inline void tlb_flush(struct mmu_gather *tlb)
 +{
 +	if (tlb->fullmm || !tlb->vma)
 +		flush_tlb_mm(tlb->mm);
 +	else if (tlb->range_end > 0) {
 +		flush_tlb_range(tlb->vma, tlb->range_start, tlb->range_end);
 +		tlb->range_start = TASK_SIZE;
 +		tlb->range_end = 0;
 +	}
 +}
 +
 +static inline void tlb_add_flush(struct mmu_gather *tlb, unsigned long addr)
 +{
 +	if (!tlb->fullmm) {
 +		if (addr < tlb->range_start)
 +			tlb->range_start = addr;
 +		if (addr + PAGE_SIZE > tlb->range_end)
 +			tlb->range_end = addr + PAGE_SIZE;
 +	}
 +}
 +
 +static inline void __tlb_alloc_page(struct mmu_gather *tlb)
 +{
 +	unsigned long addr = __get_free_pages(GFP_NOWAIT | __GFP_NOWARN, 0);
 +
 +	if (addr) {
 +		tlb->pages = (void *)addr;
 +		tlb->max = PAGE_SIZE / sizeof(struct page *);
 +	}
 +}
 +
 +static inline void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
 +{
 +	tlb_flush(tlb);
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	tlb_table_flush(tlb);
  #endif
 +}
 +
 +static inline void tlb_flush_mmu_free(struct mmu_gather *tlb)
 +{
 +	free_pages_and_swap_cache(tlb->pages, tlb->nr);
 +	tlb->nr = 0;
 +	if (tlb->pages == tlb->local)
 +		__tlb_alloc_page(tlb);
 +}
 +
 +static inline void tlb_flush_mmu(struct mmu_gather *tlb)
 +{
 +	tlb_flush_mmu_tlbonly(tlb);
 +	tlb_flush_mmu_free(tlb);
 +}
  
  static inline void
 -__pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte, unsigned long addr)
 +arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 +			unsigned long start, unsigned long end)
  {
 -	pgtable_pte_page_dtor(pte);
 +	tlb->mm = mm;
 +	tlb->fullmm = !(start | (end+1));
 +	tlb->start = start;
 +	tlb->end = end;
 +	tlb->vma = NULL;
 +	tlb->max = ARRAY_SIZE(tlb->local);
 +	tlb->pages = tlb->local;
 +	tlb->nr = 0;
 +	__tlb_alloc_page(tlb);
  
 -#ifndef CONFIG_ARM_LPAE
 +#ifdef CONFIG_HAVE_RCU_TABLE_FREE
 +	tlb->batch = NULL;
 +#endif
 +}
 +
 +static inline void
 +arch_tlb_finish_mmu(struct mmu_gather *tlb,
 +			unsigned long start, unsigned long end, bool force)
 +{
 +	if (force) {
 +		tlb->range_start = start;
 +		tlb->range_end = end;
 +	}
 +
 +	tlb_flush_mmu(tlb);
 +
 +	/* keep the page table cache within bounds */
 +	check_pgt_cache();
 +
 +	if (tlb->pages != tlb->local)
 +		free_pages((unsigned long)tlb->pages, 0);
 +}
 +
 +/*
 + * Memorize the range for the TLB flush.
 + */
 +static inline void
 +tlb_remove_tlb_entry(struct mmu_gather *tlb, pte_t *ptep, unsigned long addr)
 +{
 +	tlb_add_flush(tlb, addr);
 +}
 +
 +#define tlb_remove_huge_tlb_entry(h, tlb, ptep, address)	\
 +	tlb_remove_tlb_entry(tlb, ptep, address)
 +/*
 + * In the case of tlb vma handling, we can optimise these away in the
 + * case where we're doing a full MM flush.  When we're doing a munmap,
 + * the vmas are adjusted to only cover the region to be torn down.
 + */
 +static inline void
 +tlb_start_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
 +{
 +	if (!tlb->fullmm) {
 +		flush_cache_range(vma, vma->vm_start, vma->vm_end);
 +		tlb->vma = vma;
 +		tlb->range_start = TASK_SIZE;
 +		tlb->range_end = 0;
 +	}
 +}
 +
 +static inline void
 +tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
 +{
 +	if (!tlb->fullmm)
 +		tlb_flush(tlb);
 +}
 +
 +static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 +{
 +	tlb->pages[tlb->nr++] = page;
 +	VM_WARN_ON(tlb->nr > tlb->max);
 +	if (tlb->nr == tlb->max)
 +		return true;
 +	return false;
 +}
 +
 +static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 +{
 +	if (__tlb_remove_page(tlb, page))
 +		tlb_flush_mmu(tlb);
 +}
 +
 +static inline bool __tlb_remove_page_size(struct mmu_gather *tlb,
 +					  struct page *page, int page_size)
 +{
 +	return __tlb_remove_page(tlb, page);
 +}
 +
 +static inline void tlb_remove_page_size(struct mmu_gather *tlb,
 +					struct page *page, int page_size)
 +{
 +	return tlb_remove_page(tlb, page);
 +}
 +
 +static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte,
 +	unsigned long addr)
 +{
 +	pgtable_page_dtor(pte);
 +
 +#ifdef CONFIG_ARM_LPAE
 +	tlb_add_flush(tlb, addr);
 +#else
  	/*
  	 * With the classic ARM MMU, a pte page has two corresponding pmd
  	 * entries, each covering 1MB.
diff --cc arch/x86/Kconfig
index 23844bb111f3,beea77046f9b..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -201,12 -200,13 +201,16 @@@ config X8
  	select HAVE_PERF_EVENTS
  	select HAVE_PERF_EVENTS_NMI
  	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI
 -	select HAVE_PCI
  	select HAVE_PERF_REGS
  	select HAVE_PERF_USER_STACK_DUMP
++<<<<<<< HEAD
 +	select HAVE_RCU_TABLE_FREE		if PARAVIRT
 +	select HAVE_RCU_TABLE_INVALIDATE	if HAVE_RCU_TABLE_FREE
++=======
+ 	select MMU_GATHER_RCU_TABLE_FREE		if PARAVIRT
++>>>>>>> ff2e6d7259f8 (asm-generic/tlb: rename HAVE_RCU_TABLE_FREE)
  	select HAVE_REGS_AND_STACK_ACCESS_API
  	select HAVE_RELIABLE_STACKTRACE		if X86_64 && (UNWINDER_FRAME_POINTER || UNWINDER_ORC) && STACK_VALIDATION
 -	select HAVE_FUNCTION_ARG_ACCESS_API
  	select HAVE_STACKPROTECTOR		if CC_HAS_SANE_STACKPROTECTOR
  	select HAVE_STACK_VALIDATION		if X86_64
  	select HAVE_RSEQ
diff --git a/arch/Kconfig b/arch/Kconfig
index 1e486125c2eb..a5306f27f909 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -373,7 +373,7 @@ config HAVE_ARCH_JUMP_LABEL
 config HAVE_ARCH_JUMP_LABEL_RELATIVE
 	bool
 
-config HAVE_RCU_TABLE_FREE
+config MMU_GATHER_RCU_TABLE_FREE
 	bool
 
 config HAVE_MMU_GATHER_PAGE_SIZE
* Unmerged path arch/arm/Kconfig
* Unmerged path arch/arm/include/asm/tlb.h
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index e45cdd35a6d6..53821be1fd7f 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -157,7 +157,7 @@ config ARM64
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_FUNCTION_ARG_ACCESS_API
 	select HAVE_FUTEX_CMPXCHG if FUTEX
-	select HAVE_RCU_TABLE_FREE
+	select MMU_GATHER_RCU_TABLE_FREE
 	select HAVE_RSEQ
 	select HAVE_STACKPROTECTOR
 	select HAVE_SYSCALL_TRACEPOINTS
diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig
index 068c558b1ac4..ee8f6c4bf228 100644
--- a/arch/powerpc/Kconfig
+++ b/arch/powerpc/Kconfig
@@ -216,7 +216,7 @@ config PPC
 	select HAVE_HARDLOCKUP_DETECTOR_PERF	if PERF_EVENTS && HAVE_PERF_EVENTS_NMI && !HAVE_HARDLOCKUP_DETECTOR_ARCH
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
-	select HAVE_RCU_TABLE_FREE
+	select MMU_GATHER_RCU_TABLE_FREE
 	select HAVE_MMU_GATHER_PAGE_SIZE
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_RELIABLE_STACKTRACE		if PPC_BOOK3S_64 && CPU_LITTLE_ENDIAN
diff --git a/arch/s390/Kconfig b/arch/s390/Kconfig
index 53b3712d90ef..879ada31581b 100644
--- a/arch/s390/Kconfig
+++ b/arch/s390/Kconfig
@@ -163,7 +163,7 @@ config S390
 	select HAVE_MOD_ARCH_SPECIFIC
 	select HAVE_OPROFILE
 	select HAVE_PERF_EVENTS
-	select HAVE_RCU_TABLE_FREE
+	select MMU_GATHER_RCU_TABLE_FREE
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_RSEQ
 	select HAVE_SYSCALL_TRACEPOINTS
diff --git a/arch/sparc/Kconfig b/arch/sparc/Kconfig
index 12d490d796fe..b4350afbb6b7 100644
--- a/arch/sparc/Kconfig
+++ b/arch/sparc/Kconfig
@@ -59,7 +59,7 @@ config SPARC64
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_KRETPROBES
 	select HAVE_KPROBES
-	select HAVE_RCU_TABLE_FREE if SMP
+	select MMU_GATHER_RCU_TABLE_FREE if SMP
 	select HAVE_MEMBLOCK_NODE_MAP
 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 	select HAVE_DYNAMIC_FTRACE
diff --git a/arch/sparc/include/asm/tlb_64.h b/arch/sparc/include/asm/tlb_64.h
index 8cb8f3833239..6820d357581c 100644
--- a/arch/sparc/include/asm/tlb_64.h
+++ b/arch/sparc/include/asm/tlb_64.h
@@ -33,7 +33,7 @@ void flush_tlb_pending(void);
  * and therefore we don't need a TLBI when freeing page-table pages.
  */
 
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
 #define tlb_needs_table_invalidate()	(false)
 #endif
 
* Unmerged path arch/x86/Kconfig
diff --git a/arch/x86/include/asm/tlb.h b/arch/x86/include/asm/tlb.h
index f23e7aaff4cd..820082bd6880 100644
--- a/arch/x86/include/asm/tlb.h
+++ b/arch/x86/include/asm/tlb.h
@@ -29,8 +29,8 @@ static inline void tlb_flush(struct mmu_gather *tlb)
  * shootdown, enablement code for several hypervisors overrides
  * .flush_tlb_others hook in pv_mmu_ops and implements it by issuing
  * a hypercall. To keep software pagetable walkers safe in this case we
- * switch to RCU based table free (HAVE_RCU_TABLE_FREE). See the comment
- * below 'ifdef CONFIG_HAVE_RCU_TABLE_FREE' in include/asm-generic/tlb.h
+ * switch to RCU based table free (MMU_GATHER_RCU_TABLE_FREE). See the comment
+ * below 'ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE' in include/asm-generic/tlb.h
  * for more details.
  */
 static inline void __tlb_remove_table(void *table)
diff --git a/include/asm-generic/tlb.h b/include/asm-generic/tlb.h
index 27ac8002fb01..683ef7efc309 100644
--- a/include/asm-generic/tlb.h
+++ b/include/asm-generic/tlb.h
@@ -130,7 +130,7 @@
  *  This ensures we call tlb_flush() every time tlb_change_page_size() actually
  *  changes the size and provides mmu_gather::page_size to tlb_flush().
  *
- *  HAVE_RCU_TABLE_FREE
+ *  MMU_GATHER_RCU_TABLE_FREE
  *
  *  This provides tlb_remove_table(), to be used instead of tlb_remove_page()
  *  for page directores (__p*_free_tlb()). This provides separate freeing of
@@ -152,7 +152,7 @@
  *  Use this if your architecture lacks an efficient flush_tlb_range().
  */
 
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
 /*
  * Semi RCU freeing of the page directories.
  *
@@ -203,10 +203,10 @@ extern void tlb_remove_table(struct mmu_gather *tlb, void *table);
 #else
 
 #ifdef tlb_needs_table_invalidate
-#error tlb_needs_table_invalidate() requires HAVE_RCU_TABLE_FREE
+#error tlb_needs_table_invalidate() requires MMU_GATHER_RCU_TABLE_FREE
 #endif
 
-#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
+#endif /* CONFIG_MMU_GATHER_RCU_TABLE_FREE */
 
 
 #ifndef CONFIG_HAVE_MMU_GATHER_NO_GATHER
@@ -245,7 +245,7 @@ extern bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,
 struct mmu_gather {
 	struct mm_struct	*mm;
 
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
 	struct mmu_table_batch	*batch;
 #endif
 
diff --git a/mm/gup.c b/mm/gup.c
index b6ca92e3db6b..140eab116ccb 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -2360,7 +2360,7 @@ EXPORT_SYMBOL(get_user_pages_unlocked);
  * Before activating this code, please be aware that the following assumptions
  * are currently made:
  *
- *  *) Either HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table() is used to
+ *  *) Either MMU_GATHER_RCU_TABLE_FREE is enabled, and tlb_remove_table() is used to
  *  free pages containing page tables or TLB flushing requires IPI broadcast.
  *
  *  *) ptes can be read atomically by the architecture.
diff --git a/mm/mmu_gather.c b/mm/mmu_gather.c
index b84402038601..181b387f90e0 100644
--- a/mm/mmu_gather.c
+++ b/mm/mmu_gather.c
@@ -91,7 +91,7 @@ bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page, int page_
 
 #endif /* HAVE_MMU_GATHER_NO_GATHER */
 
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
 
 /*
  * See the comment near struct mmu_table_batch.
@@ -173,11 +173,11 @@ void tlb_remove_table(struct mmu_gather *tlb, void *table)
 		tlb_table_flush(tlb);
 }
 
-#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
+#endif /* CONFIG_MMU_GATHER_RCU_TABLE_FREE */
 
 static void tlb_flush_mmu_free(struct mmu_gather *tlb)
 {
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
 	tlb_table_flush(tlb);
 #endif
 #ifndef CONFIG_HAVE_MMU_GATHER_NO_GATHER
@@ -220,7 +220,7 @@ void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 	tlb->batch_count = 0;
 #endif
 
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+#ifdef CONFIG_MMU_GATHER_RCU_TABLE_FREE
 	tlb->batch = NULL;
 #endif
 #ifdef CONFIG_HAVE_MMU_GATHER_PAGE_SIZE
