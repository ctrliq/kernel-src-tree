RDMA/mlx5: Attach ndescs to mlx5_ib_mkey

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit ae0579acde812bc1efd074086ae3bc5eae170f20
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/ae0579ac.failed

Generalize the use of ndescs by adding it to mlx5_ib_mkey.

	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Shay Drory <shayd@nvidia.com>
	Acked-by: Michael S. Tsirkin <mst@redhat.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
(cherry picked from commit ae0579acde812bc1efd074086ae3bc5eae170f20)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/devx.c
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/odp.c
diff --cc drivers/infiniband/hw/mlx5/devx.c
index abfde896b9aa,08b7f6bc56c3..000000000000
--- a/drivers/infiniband/hw/mlx5/devx.c
+++ b/drivers/infiniband/hw/mlx5/devx.c
@@@ -1292,8 -1292,7 +1292,12 @@@ static int devx_handle_mkey_indirect(st
  				     struct mlx5_ib_dev *dev,
  				     void *in, void *out)
  {
++<<<<<<< HEAD
 +	struct mlx5_ib_devx_mr *devx_mr = &obj->devx_mr;
 +	struct mlx5_core_mkey *mkey;
++=======
+ 	struct mlx5_ib_mkey *mkey = &obj->mkey;
++>>>>>>> ae0579acde81 (RDMA/mlx5: Attach ndescs to mlx5_ib_mkey)
  	void *mkc;
  	u8 key;
  
@@@ -1303,10 -1301,7 +1306,14 @@@
  	mkey->key = mlx5_idx_to_mkey(
  			MLX5_GET(create_mkey_out, out, mkey_index)) | key;
  	mkey->type = MLX5_MKEY_INDIRECT_DEVX;
++<<<<<<< HEAD
 +	mkey->iova = MLX5_GET64(mkc, mkc, start_addr);
 +	mkey->size = MLX5_GET64(mkc, mkc, len);
 +	mkey->pd = MLX5_GET(mkc, mkc, pd);
 +	devx_mr->ndescs = MLX5_GET(mkc, mkc, translations_octword_size);
++=======
+ 	mkey->ndescs = MLX5_GET(mkc, mkc, translations_octword_size);
++>>>>>>> ae0579acde81 (RDMA/mlx5: Attach ndescs to mlx5_ib_mkey)
  	init_waitqueue_head(&mkey->wait);
  
  	return mlx5r_store_odp_mkey(dev, mkey);
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3f728de54a6b,e462e368c353..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -627,6 -619,20 +627,23 @@@ struct mlx5_user_mmap_entry 
  	u32 page_idx;
  };
  
++<<<<<<< HEAD
++=======
+ enum mlx5_mkey_type {
+ 	MLX5_MKEY_MR = 1,
+ 	MLX5_MKEY_MW,
+ 	MLX5_MKEY_INDIRECT_DEVX,
+ };
+ 
+ struct mlx5_ib_mkey {
+ 	u32 key;
+ 	enum mlx5_mkey_type type;
+ 	unsigned int ndescs;
+ 	struct wait_queue_head wait;
+ 	refcount_t usecount;
+ };
+ 
++>>>>>>> ae0579acde81 (RDMA/mlx5: Attach ndescs to mlx5_ib_mkey)
  #define MLX5_IB_MTT_PRESENT (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE)
  
  #define MLX5_IB_DM_MEMIC_ALLOWED_ACCESS (IB_ACCESS_LOCAL_WRITE   |\
@@@ -713,15 -718,15 +729,19 @@@ static inline bool is_odp_mr(struct mlx
  	       mr->umem->is_odp;
  }
  
 -static inline bool is_dmabuf_mr(struct mlx5_ib_mr *mr)
 -{
 -	return IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) && mr->umem &&
 -	       mr->umem->is_dmabuf;
 -}
 -
  struct mlx5_ib_mw {
  	struct ib_mw		ibmw;
++<<<<<<< HEAD
 +	struct mlx5_core_mkey	mmkey;
 +	int			ndescs;
 +};
 +
 +struct mlx5_ib_devx_mr {
 +	struct mlx5_core_mkey	mmkey;
 +	int			ndescs;
++=======
+ 	struct mlx5_ib_mkey	mmkey;
++>>>>>>> ae0579acde81 (RDMA/mlx5: Attach ndescs to mlx5_ib_mkey)
  };
  
  struct mlx5_ib_umr_context {
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 677d9275aa78,086ffb2cb5db..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -740,21 -797,6 +740,24 @@@ static bool mkey_is_eq(struct mlx5_core
  	return mmkey->key == key;
  }
  
++<<<<<<< HEAD
 +static int get_indirect_num_descs(struct mlx5_core_mkey *mmkey)
 +{
 +	struct mlx5_ib_mw *mw;
 +	struct mlx5_ib_devx_mr *devx_mr;
 +
 +	if (mmkey->type == MLX5_MKEY_MW) {
 +		mw = container_of(mmkey, struct mlx5_ib_mw, mmkey);
 +		return mw->ndescs;
 +	}
 +
 +	devx_mr = container_of(mmkey, struct mlx5_ib_devx_mr,
 +			       mmkey);
 +	return devx_mr->ndescs;
 +}
 +
++=======
++>>>>>>> ae0579acde81 (RDMA/mlx5: Attach ndescs to mlx5_ib_mkey)
  /*
   * Handle a single data segment in a page-fault WQE or RDMA region.
   *
* Unmerged path drivers/infiniband/hw/mlx5/devx.c
diff --git a/drivers/infiniband/hw/mlx5/devx.h b/drivers/infiniband/hw/mlx5/devx.h
index 1f69866aed16..ee2213275fd6 100644
--- a/drivers/infiniband/hw/mlx5/devx.h
+++ b/drivers/infiniband/hw/mlx5/devx.h
@@ -16,7 +16,7 @@ struct devx_obj {
 	u32			dinbox[MLX5_MAX_DESTROY_INBOX_SIZE_DW];
 	u32			flags;
 	union {
-		struct mlx5_ib_devx_mr	devx_mr;
+		struct mlx5_ib_mkey	mkey;
 		struct mlx5_core_dct	core_dct;
 		struct mlx5_core_cq	core_cq;
 		u32			flow_counter_bulk_size;
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
diff --git a/drivers/infiniband/hw/mlx5/mr.c b/drivers/infiniband/hw/mlx5/mr.c
index 4263bf96852f..c7b733034c61 100644
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -2104,9 +2104,9 @@ int mlx5_ib_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)
 	struct mlx5_ib_dev *dev = to_mdev(ibmw->device);
 	int inlen = MLX5_ST_SZ_BYTES(create_mkey_in);
 	struct mlx5_ib_mw *mw = to_mmw(ibmw);
+	unsigned int ndescs;
 	u32 *in = NULL;
 	void *mkc;
-	int ndescs;
 	int err;
 	struct mlx5_ib_alloc_mw req = {};
 	struct {
@@ -2151,7 +2151,7 @@ int mlx5_ib_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata)
 
 	mw->mmkey.type = MLX5_MKEY_MW;
 	ibmw->rkey = mw->mmkey.key;
-	mw->ndescs = ndescs;
+	mw->mmkey.ndescs = ndescs;
 
 	resp.response_length =
 		min(offsetofend(typeof(resp), response_length), udata->outlen);
@@ -2247,7 +2247,7 @@ mlx5_ib_map_pa_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,
 	mr->meta_length = 0;
 	if (data_sg_nents == 1) {
 		n++;
-		mr->ndescs = 1;
+		mr->mmkey.ndescs = 1;
 		if (data_sg_offset)
 			sg_offset = *data_sg_offset;
 		mr->data_length = sg_dma_len(data_sg) - sg_offset;
@@ -2300,7 +2300,7 @@ mlx5_ib_sg_to_klms(struct mlx5_ib_mr *mr,
 	if (sg_offset_p)
 		*sg_offset_p = sg_offset;
 
-	mr->ndescs = i;
+	mr->mmkey.ndescs = i;
 	mr->data_length = mr->ibmr.length;
 
 	if (meta_sg_nents) {
@@ -2333,11 +2333,11 @@ static int mlx5_set_page(struct ib_mr *ibmr, u64 addr)
 	struct mlx5_ib_mr *mr = to_mmr(ibmr);
 	__be64 *descs;
 
-	if (unlikely(mr->ndescs == mr->max_descs))
+	if (unlikely(mr->mmkey.ndescs == mr->max_descs))
 		return -ENOMEM;
 
 	descs = mr->descs;
-	descs[mr->ndescs++] = cpu_to_be64(addr | MLX5_EN_RD | MLX5_EN_WR);
+	descs[mr->mmkey.ndescs++] = cpu_to_be64(addr | MLX5_EN_RD | MLX5_EN_WR);
 
 	return 0;
 }
@@ -2347,11 +2347,11 @@ static int mlx5_set_page_pi(struct ib_mr *ibmr, u64 addr)
 	struct mlx5_ib_mr *mr = to_mmr(ibmr);
 	__be64 *descs;
 
-	if (unlikely(mr->ndescs + mr->meta_ndescs == mr->max_descs))
+	if (unlikely(mr->mmkey.ndescs + mr->meta_ndescs == mr->max_descs))
 		return -ENOMEM;
 
 	descs = mr->descs;
-	descs[mr->ndescs + mr->meta_ndescs++] =
+	descs[mr->mmkey.ndescs + mr->meta_ndescs++] =
 		cpu_to_be64(addr | MLX5_EN_RD | MLX5_EN_WR);
 
 	return 0;
@@ -2367,7 +2367,7 @@ mlx5_ib_map_mtt_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,
 	struct mlx5_ib_mr *pi_mr = mr->mtt_mr;
 	int n;
 
-	pi_mr->ndescs = 0;
+	pi_mr->mmkey.ndescs = 0;
 	pi_mr->meta_ndescs = 0;
 	pi_mr->meta_length = 0;
 
@@ -2401,7 +2401,7 @@ mlx5_ib_map_mtt_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,
 		 * metadata offset at the first metadata page
 		 */
 		pi_mr->pi_iova = (iova & page_mask) +
-				 pi_mr->ndescs * ibmr->page_size +
+				 pi_mr->mmkey.ndescs * ibmr->page_size +
 				 (pi_mr->ibmr.iova & ~page_mask);
 		/*
 		 * In order to use one MTT MR for data and metadata, we register
@@ -2432,7 +2432,7 @@ mlx5_ib_map_klm_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,
 	struct mlx5_ib_mr *pi_mr = mr->klm_mr;
 	int n;
 
-	pi_mr->ndescs = 0;
+	pi_mr->mmkey.ndescs = 0;
 	pi_mr->meta_ndescs = 0;
 	pi_mr->meta_length = 0;
 
@@ -2467,7 +2467,7 @@ int mlx5_ib_map_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,
 
 	WARN_ON(ibmr->type != IB_MR_TYPE_INTEGRITY);
 
-	mr->ndescs = 0;
+	mr->mmkey.ndescs = 0;
 	mr->data_length = 0;
 	mr->data_iova = 0;
 	mr->meta_ndescs = 0;
@@ -2523,7 +2523,7 @@ int mlx5_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 	struct mlx5_ib_mr *mr = to_mmr(ibmr);
 	int n;
 
-	mr->ndescs = 0;
+	mr->mmkey.ndescs = 0;
 
 	ib_dma_sync_single_for_cpu(ibmr->device, mr->desc_map,
 				   mr->desc_size * mr->max_descs,
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
diff --git a/drivers/infiniband/hw/mlx5/wr.c b/drivers/infiniband/hw/mlx5/wr.c
index 381e00a21c53..c12e0d1b7ee5 100644
--- a/drivers/infiniband/hw/mlx5/wr.c
+++ b/drivers/infiniband/hw/mlx5/wr.c
@@ -217,7 +217,7 @@ static __be64 sig_mkey_mask(void)
 static void set_reg_umr_seg(struct mlx5_wqe_umr_ctrl_seg *umr,
 			    struct mlx5_ib_mr *mr, u8 flags, bool atomic)
 {
-	int size = (mr->ndescs + mr->meta_ndescs) * mr->desc_size;
+	int size = (mr->mmkey.ndescs + mr->meta_ndescs) * mr->desc_size;
 
 	memset(umr, 0, sizeof(*umr));
 
@@ -374,7 +374,7 @@ static void set_reg_mkey_seg(struct mlx5_mkey_seg *seg,
 			     struct mlx5_ib_mr *mr,
 			     u32 key, int access)
 {
-	int ndescs = ALIGN(mr->ndescs + mr->meta_ndescs, 8) >> 1;
+	int ndescs = ALIGN(mr->mmkey.ndescs + mr->meta_ndescs, 8) >> 1;
 
 	memset(seg, 0, sizeof(*seg));
 
@@ -439,7 +439,7 @@ static void set_reg_data_seg(struct mlx5_wqe_data_seg *dseg,
 			     struct mlx5_ib_mr *mr,
 			     struct mlx5_ib_pd *pd)
 {
-	int bcount = mr->desc_size * (mr->ndescs + mr->meta_ndescs);
+	int bcount = mr->desc_size * (mr->mmkey.ndescs + mr->meta_ndescs);
 
 	dseg->addr = cpu_to_be64(mr->desc_map);
 	dseg->byte_count = cpu_to_be32(ALIGN(bcount, 64));
@@ -861,7 +861,7 @@ static int set_reg_wr(struct mlx5_ib_qp *qp,
 	struct mlx5_ib_mr *mr = to_mmr(wr->mr);
 	struct mlx5_ib_pd *pd = to_mpd(qp->ibqp.pd);
 	struct mlx5_ib_dev *dev = to_mdev(pd->ibpd.device);
-	int mr_list_size = (mr->ndescs + mr->meta_ndescs) * mr->desc_size;
+	int mr_list_size = (mr->mmkey.ndescs + mr->meta_ndescs) * mr->desc_size;
 	bool umr_inline = mr_list_size <= MLX5_IB_SQ_UMR_INLINE_THRESHOLD;
 	bool atomic = wr->access & IB_ACCESS_REMOTE_ATOMIC;
 	u8 flags = 0;
@@ -1111,7 +1111,7 @@ static int handle_reg_mr_integrity(struct mlx5_ib_dev *dev,
 		memset(&pa_pi_mr, 0, sizeof(struct mlx5_ib_mr));
 		/* No UMR, use local_dma_lkey */
 		pa_pi_mr.ibmr.lkey = mr->ibmr.pd->local_dma_lkey;
-		pa_pi_mr.ndescs = mr->ndescs;
+		pa_pi_mr.mmkey.ndescs = mr->mmkey.ndescs;
 		pa_pi_mr.data_length = mr->data_length;
 		pa_pi_mr.data_iova = mr->data_iova;
 		if (mr->meta_ndescs) {
