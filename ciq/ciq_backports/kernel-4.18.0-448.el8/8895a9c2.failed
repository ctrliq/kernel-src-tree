ipv4: Fix data-races around sysctl_fib_multipath_hash_fields.

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Kuniyuki Iwashima <kuniyu@amazon.com>
commit 8895a9c2ac76fb9d3922fed4fe092c8ec5e5cccc
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/8895a9c2.failed

While reading sysctl_fib_multipath_hash_fields, it can be changed
concurrently.  Thus, we need to add READ_ONCE() to its readers.

Fixes: ce5c9c20d364 ("ipv4: Add a sysctl to control multipath hash fields")
	Signed-off-by: Kuniyuki Iwashima <kuniyu@amazon.com>
	Reviewed-by: Ido Schimmel <idosch@nvidia.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 8895a9c2ac76fb9d3922fed4fe092c8ec5e5cccc)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
#	net/ipv4/route.c
diff --cc drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
index c39544dd56e6,85aa1c468cd4..000000000000
--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
@@@ -8492,55 -10227,230 +8492,95 @@@ static void mlxsw_sp_router_fib_dump_fl
  }
  
  #ifdef CONFIG_IP_ROUTE_MULTIPATH
 -struct mlxsw_sp_mp_hash_config {
 -	DECLARE_BITMAP(headers, __MLXSW_REG_RECR2_HEADER_CNT);
 -	DECLARE_BITMAP(fields, __MLXSW_REG_RECR2_FIELD_CNT);
 -	DECLARE_BITMAP(inner_headers, __MLXSW_REG_RECR2_HEADER_CNT);
 -	DECLARE_BITMAP(inner_fields, __MLXSW_REG_RECR2_INNER_FIELD_CNT);
 -	bool inc_parsing_depth;
 -};
 -
 -#define MLXSW_SP_MP_HASH_HEADER_SET(_headers, _header) \
 -	bitmap_set(_headers, MLXSW_REG_RECR2_##_header, 1)
 -
 -#define MLXSW_SP_MP_HASH_FIELD_SET(_fields, _field) \
 -	bitmap_set(_fields, MLXSW_REG_RECR2_##_field, 1)
 -
 -#define MLXSW_SP_MP_HASH_FIELD_RANGE_SET(_fields, _field, _nr) \
 -	bitmap_set(_fields, MLXSW_REG_RECR2_##_field, _nr)
 -
 -static void mlxsw_sp_mp_hash_inner_l3(struct mlxsw_sp_mp_hash_config *config)
 +static void mlxsw_sp_mp_hash_header_set(char *recr2_pl, int header)
  {
 -	unsigned long *inner_headers = config->inner_headers;
 -	unsigned long *inner_fields = config->inner_fields;
 -
 -	/* IPv4 inner */
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV4_EN_NOT_TCP_NOT_UDP);
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV4_EN_TCP_UDP);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV4_SIP0, 4);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV4_DIP0, 4);
 -	/* IPv6 inner */
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV6_EN_NOT_TCP_NOT_UDP);
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV6_EN_TCP_UDP);
 -	MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_SIP0_7);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV6_SIP8, 8);
 -	MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_DIP0_7);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV6_DIP8, 8);
 -	MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_NEXT_HEADER);
 -	MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_FLOW_LABEL);
 +	mlxsw_reg_recr2_outer_header_enables_set(recr2_pl, header, true);
  }
  
 -static void mlxsw_sp_mp4_hash_outer_addr(struct mlxsw_sp_mp_hash_config *config)
 +static void mlxsw_sp_mp_hash_field_set(char *recr2_pl, int field)
  {
 -	unsigned long *headers = config->headers;
 -	unsigned long *fields = config->fields;
 -
 -	MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV4_EN_NOT_TCP_NOT_UDP);
 -	MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV4_EN_TCP_UDP);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV4_SIP0, 4);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV4_DIP0, 4);
 +	mlxsw_reg_recr2_outer_header_fields_enable_set(recr2_pl, field, true);
  }
  
 -static void
 -mlxsw_sp_mp_hash_inner_custom(struct mlxsw_sp_mp_hash_config *config,
 -			      u32 hash_fields)
 -{
 -	unsigned long *inner_headers = config->inner_headers;
 -	unsigned long *inner_fields = config->inner_fields;
 -
 -	/* IPv4 Inner */
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV4_EN_NOT_TCP_NOT_UDP);
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV4_EN_TCP_UDP);
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_SRC_IP)
 -		MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV4_SIP0, 4);
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_DST_IP)
 -		MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV4_DIP0, 4);
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_IP_PROTO)
 -		MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV4_PROTOCOL);
 -	/* IPv6 inner */
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV6_EN_NOT_TCP_NOT_UDP);
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, IPV6_EN_TCP_UDP);
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_SRC_IP) {
 -		MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_SIP0_7);
 -		MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV6_SIP8, 8);
 -	}
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_DST_IP) {
 -		MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_DIP0_7);
 -		MLXSW_SP_MP_HASH_FIELD_RANGE_SET(inner_fields, INNER_IPV6_DIP8, 8);
 -	}
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_IP_PROTO)
 -		MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_NEXT_HEADER);
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_FLOWLABEL)
 -		MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_IPV6_FLOW_LABEL);
 -	/* L4 inner */
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, TCP_UDP_EN_IPV4);
 -	MLXSW_SP_MP_HASH_HEADER_SET(inner_headers, TCP_UDP_EN_IPV6);
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_SRC_PORT)
 -		MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_TCP_UDP_SPORT);
 -	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_DST_PORT)
 -		MLXSW_SP_MP_HASH_FIELD_SET(inner_fields, INNER_TCP_UDP_DPORT);
 -}
 -
 -static void mlxsw_sp_mp4_hash_init(struct mlxsw_sp *mlxsw_sp,
 -				   struct mlxsw_sp_mp_hash_config *config)
 +static void mlxsw_sp_mp4_hash_init(struct mlxsw_sp *mlxsw_sp, char *recr2_pl)
  {
  	struct net *net = mlxsw_sp_net(mlxsw_sp);
 -	unsigned long *headers = config->headers;
 -	unsigned long *fields = config->fields;
 -	u32 hash_fields;
 -
 +	bool only_l3 = !net->ipv4.sysctl_fib_multipath_hash_policy;
 +
++<<<<<<< HEAD
 +	mlxsw_sp_mp_hash_header_set(recr2_pl,
 +				    MLXSW_REG_RECR2_IPV4_EN_NOT_TCP_NOT_UDP);
 +	mlxsw_sp_mp_hash_header_set(recr2_pl, MLXSW_REG_RECR2_IPV4_EN_TCP_UDP);
 +	mlxsw_reg_recr2_ipv4_sip_enable(recr2_pl);
 +	mlxsw_reg_recr2_ipv4_dip_enable(recr2_pl);
 +	if (only_l3)
 +		return;
 +	mlxsw_sp_mp_hash_header_set(recr2_pl, MLXSW_REG_RECR2_TCP_UDP_EN_IPV4);
 +	mlxsw_sp_mp_hash_field_set(recr2_pl, MLXSW_REG_RECR2_IPV4_PROTOCOL);
 +	mlxsw_sp_mp_hash_field_set(recr2_pl, MLXSW_REG_RECR2_TCP_UDP_SPORT);
 +	mlxsw_sp_mp_hash_field_set(recr2_pl, MLXSW_REG_RECR2_TCP_UDP_DPORT);
++=======
+ 	switch (READ_ONCE(net->ipv4.sysctl_fib_multipath_hash_policy)) {
+ 	case 0:
+ 		mlxsw_sp_mp4_hash_outer_addr(config);
+ 		break;
+ 	case 1:
+ 		mlxsw_sp_mp4_hash_outer_addr(config);
+ 		MLXSW_SP_MP_HASH_HEADER_SET(headers, TCP_UDP_EN_IPV4);
+ 		MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV4_PROTOCOL);
+ 		MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_SPORT);
+ 		MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_DPORT);
+ 		break;
+ 	case 2:
+ 		/* Outer */
+ 		mlxsw_sp_mp4_hash_outer_addr(config);
+ 		/* Inner */
+ 		mlxsw_sp_mp_hash_inner_l3(config);
+ 		break;
+ 	case 3:
+ 		hash_fields = READ_ONCE(net->ipv4.sysctl_fib_multipath_hash_fields);
+ 		/* Outer */
+ 		MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV4_EN_NOT_TCP_NOT_UDP);
+ 		MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV4_EN_TCP_UDP);
+ 		MLXSW_SP_MP_HASH_HEADER_SET(headers, TCP_UDP_EN_IPV4);
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_IP)
+ 			MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV4_SIP0, 4);
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_IP)
+ 			MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV4_DIP0, 4);
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_IP_PROTO)
+ 			MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV4_PROTOCOL);
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_PORT)
+ 			MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_SPORT);
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_PORT)
+ 			MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_DPORT);
+ 		/* Inner */
+ 		mlxsw_sp_mp_hash_inner_custom(config, hash_fields);
+ 		break;
+ 	}
++>>>>>>> 8895a9c2ac76 (ipv4: Fix data-races around sysctl_fib_multipath_hash_fields.)
  }
  
 -static void mlxsw_sp_mp6_hash_outer_addr(struct mlxsw_sp_mp_hash_config *config)
 -{
 -	unsigned long *headers = config->headers;
 -	unsigned long *fields = config->fields;
 -
 -	MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV6_EN_NOT_TCP_NOT_UDP);
 -	MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV6_EN_TCP_UDP);
 -	MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_SIP0_7);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV6_SIP8, 8);
 -	MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_DIP0_7);
 -	MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV6_DIP8, 8);
 -}
 -
 -static void mlxsw_sp_mp6_hash_init(struct mlxsw_sp *mlxsw_sp,
 -				   struct mlxsw_sp_mp_hash_config *config)
 -{
 -	u32 hash_fields = ip6_multipath_hash_fields(mlxsw_sp_net(mlxsw_sp));
 -	unsigned long *headers = config->headers;
 -	unsigned long *fields = config->fields;
 -
 -	switch (ip6_multipath_hash_policy(mlxsw_sp_net(mlxsw_sp))) {
 -	case 0:
 -		mlxsw_sp_mp6_hash_outer_addr(config);
 -		MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_NEXT_HEADER);
 -		MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_FLOW_LABEL);
 -		break;
 -	case 1:
 -		mlxsw_sp_mp6_hash_outer_addr(config);
 -		MLXSW_SP_MP_HASH_HEADER_SET(headers, TCP_UDP_EN_IPV6);
 -		MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_NEXT_HEADER);
 -		MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_SPORT);
 -		MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_DPORT);
 -		break;
 -	case 2:
 -		/* Outer */
 -		mlxsw_sp_mp6_hash_outer_addr(config);
 -		MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_NEXT_HEADER);
 -		MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_FLOW_LABEL);
 -		/* Inner */
 -		mlxsw_sp_mp_hash_inner_l3(config);
 -		config->inc_parsing_depth = true;
 -		break;
 -	case 3:
 -		/* Outer */
 -		MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV6_EN_NOT_TCP_NOT_UDP);
 -		MLXSW_SP_MP_HASH_HEADER_SET(headers, IPV6_EN_TCP_UDP);
 -		MLXSW_SP_MP_HASH_HEADER_SET(headers, TCP_UDP_EN_IPV6);
 -		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_IP) {
 -			MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_SIP0_7);
 -			MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV6_SIP8, 8);
 -		}
 -		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_IP) {
 -			MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_DIP0_7);
 -			MLXSW_SP_MP_HASH_FIELD_RANGE_SET(fields, IPV6_DIP8, 8);
 -		}
 -		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_IP_PROTO)
 -			MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_NEXT_HEADER);
 -		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_FLOWLABEL)
 -			MLXSW_SP_MP_HASH_FIELD_SET(fields, IPV6_FLOW_LABEL);
 -		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_PORT)
 -			MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_SPORT);
 -		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_PORT)
 -			MLXSW_SP_MP_HASH_FIELD_SET(fields, TCP_UDP_DPORT);
 -		/* Inner */
 -		mlxsw_sp_mp_hash_inner_custom(config, hash_fields);
 -		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_MASK)
 -			config->inc_parsing_depth = true;
 -		break;
 -	}
 -}
 -
 -static int mlxsw_sp_mp_hash_parsing_depth_adjust(struct mlxsw_sp *mlxsw_sp,
 -						 bool old_inc_parsing_depth,
 -						 bool new_inc_parsing_depth)
 +static void mlxsw_sp_mp6_hash_init(struct mlxsw_sp *mlxsw_sp, char *recr2_pl)
  {
 -	int err;
 +	bool only_l3 = !ip6_multipath_hash_policy(mlxsw_sp_net(mlxsw_sp));
  
 -	if (!old_inc_parsing_depth && new_inc_parsing_depth) {
 -		err = mlxsw_sp_parsing_depth_inc(mlxsw_sp);
 -		if (err)
 -			return err;
 -		mlxsw_sp->router->inc_parsing_depth = true;
 -	} else if (old_inc_parsing_depth && !new_inc_parsing_depth) {
 -		mlxsw_sp_parsing_depth_dec(mlxsw_sp);
 -		mlxsw_sp->router->inc_parsing_depth = false;
 +	mlxsw_sp_mp_hash_header_set(recr2_pl,
 +				    MLXSW_REG_RECR2_IPV6_EN_NOT_TCP_NOT_UDP);
 +	mlxsw_sp_mp_hash_header_set(recr2_pl, MLXSW_REG_RECR2_IPV6_EN_TCP_UDP);
 +	mlxsw_reg_recr2_ipv6_sip_enable(recr2_pl);
 +	mlxsw_reg_recr2_ipv6_dip_enable(recr2_pl);
 +	mlxsw_sp_mp_hash_field_set(recr2_pl, MLXSW_REG_RECR2_IPV6_NEXT_HEADER);
 +	if (only_l3) {
 +		mlxsw_sp_mp_hash_field_set(recr2_pl,
 +					   MLXSW_REG_RECR2_IPV6_FLOW_LABEL);
 +	} else {
 +		mlxsw_sp_mp_hash_header_set(recr2_pl,
 +					    MLXSW_REG_RECR2_TCP_UDP_EN_IPV6);
 +		mlxsw_sp_mp_hash_field_set(recr2_pl,
 +					   MLXSW_REG_RECR2_TCP_UDP_SPORT);
 +		mlxsw_sp_mp_hash_field_set(recr2_pl,
 +					   MLXSW_REG_RECR2_TCP_UDP_DPORT);
  	}
 -
 -	return 0;
  }
  
  static int mlxsw_sp_mp_hash_init(struct mlxsw_sp *mlxsw_sp)
diff --cc net/ipv4/route.c
index 5ad19572d74a,4702c61207a8..000000000000
--- a/net/ipv4/route.c
+++ b/net/ipv4/route.c
@@@ -1894,6 -1925,121 +1894,124 @@@ out
  	hash_keys->addrs.v4addrs.dst = key_iph->daddr;
  }
  
++<<<<<<< HEAD
++=======
+ static u32 fib_multipath_custom_hash_outer(const struct net *net,
+ 					   const struct sk_buff *skb,
+ 					   bool *p_has_inner)
+ {
+ 	u32 hash_fields = READ_ONCE(net->ipv4.sysctl_fib_multipath_hash_fields);
+ 	struct flow_keys keys, hash_keys;
+ 
+ 	if (!(hash_fields & FIB_MULTIPATH_HASH_FIELD_OUTER_MASK))
+ 		return 0;
+ 
+ 	memset(&hash_keys, 0, sizeof(hash_keys));
+ 	skb_flow_dissect_flow_keys(skb, &keys, FLOW_DISSECTOR_F_STOP_AT_ENCAP);
+ 
+ 	hash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_IP)
+ 		hash_keys.addrs.v4addrs.src = keys.addrs.v4addrs.src;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_IP)
+ 		hash_keys.addrs.v4addrs.dst = keys.addrs.v4addrs.dst;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_IP_PROTO)
+ 		hash_keys.basic.ip_proto = keys.basic.ip_proto;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_PORT)
+ 		hash_keys.ports.src = keys.ports.src;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_PORT)
+ 		hash_keys.ports.dst = keys.ports.dst;
+ 
+ 	*p_has_inner = !!(keys.control.flags & FLOW_DIS_ENCAPSULATION);
+ 	return flow_hash_from_keys(&hash_keys);
+ }
+ 
+ static u32 fib_multipath_custom_hash_inner(const struct net *net,
+ 					   const struct sk_buff *skb,
+ 					   bool has_inner)
+ {
+ 	u32 hash_fields = READ_ONCE(net->ipv4.sysctl_fib_multipath_hash_fields);
+ 	struct flow_keys keys, hash_keys;
+ 
+ 	/* We assume the packet carries an encapsulation, but if none was
+ 	 * encountered during dissection of the outer flow, then there is no
+ 	 * point in calling the flow dissector again.
+ 	 */
+ 	if (!has_inner)
+ 		return 0;
+ 
+ 	if (!(hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_MASK))
+ 		return 0;
+ 
+ 	memset(&hash_keys, 0, sizeof(hash_keys));
+ 	skb_flow_dissect_flow_keys(skb, &keys, 0);
+ 
+ 	if (!(keys.control.flags & FLOW_DIS_ENCAPSULATION))
+ 		return 0;
+ 
+ 	if (keys.control.addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {
+ 		hash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_SRC_IP)
+ 			hash_keys.addrs.v4addrs.src = keys.addrs.v4addrs.src;
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_DST_IP)
+ 			hash_keys.addrs.v4addrs.dst = keys.addrs.v4addrs.dst;
+ 	} else if (keys.control.addr_type == FLOW_DISSECTOR_KEY_IPV6_ADDRS) {
+ 		hash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_SRC_IP)
+ 			hash_keys.addrs.v6addrs.src = keys.addrs.v6addrs.src;
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_DST_IP)
+ 			hash_keys.addrs.v6addrs.dst = keys.addrs.v6addrs.dst;
+ 		if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_FLOWLABEL)
+ 			hash_keys.tags.flow_label = keys.tags.flow_label;
+ 	}
+ 
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_IP_PROTO)
+ 		hash_keys.basic.ip_proto = keys.basic.ip_proto;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_SRC_PORT)
+ 		hash_keys.ports.src = keys.ports.src;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_INNER_DST_PORT)
+ 		hash_keys.ports.dst = keys.ports.dst;
+ 
+ 	return flow_hash_from_keys(&hash_keys);
+ }
+ 
+ static u32 fib_multipath_custom_hash_skb(const struct net *net,
+ 					 const struct sk_buff *skb)
+ {
+ 	u32 mhash, mhash_inner;
+ 	bool has_inner = true;
+ 
+ 	mhash = fib_multipath_custom_hash_outer(net, skb, &has_inner);
+ 	mhash_inner = fib_multipath_custom_hash_inner(net, skb, has_inner);
+ 
+ 	return jhash_2words(mhash, mhash_inner, 0);
+ }
+ 
+ static u32 fib_multipath_custom_hash_fl4(const struct net *net,
+ 					 const struct flowi4 *fl4)
+ {
+ 	u32 hash_fields = READ_ONCE(net->ipv4.sysctl_fib_multipath_hash_fields);
+ 	struct flow_keys hash_keys;
+ 
+ 	if (!(hash_fields & FIB_MULTIPATH_HASH_FIELD_OUTER_MASK))
+ 		return 0;
+ 
+ 	memset(&hash_keys, 0, sizeof(hash_keys));
+ 	hash_keys.control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_IP)
+ 		hash_keys.addrs.v4addrs.src = fl4->saddr;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_IP)
+ 		hash_keys.addrs.v4addrs.dst = fl4->daddr;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_IP_PROTO)
+ 		hash_keys.basic.ip_proto = fl4->flowi4_proto;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_SRC_PORT)
+ 		hash_keys.ports.src = fl4->fl4_sport;
+ 	if (hash_fields & FIB_MULTIPATH_HASH_FIELD_DST_PORT)
+ 		hash_keys.ports.dst = fl4->fl4_dport;
+ 
+ 	return flow_hash_from_keys(&hash_keys);
+ }
+ 
++>>>>>>> 8895a9c2ac76 (ipv4: Fix data-races around sysctl_fib_multipath_hash_fields.)
  /* if skb is set it will be used and fl4 can be NULL */
  int fib_multipath_hash(const struct net *net, const struct flowi4 *fl4,
  		       const struct sk_buff *skb, struct flow_keys *flkeys)
* Unmerged path drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c
* Unmerged path net/ipv4/route.c
