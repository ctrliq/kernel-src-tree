swiotlb: use the right nslabs value in swiotlb_init_remap

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-448.el8
commit-author Christoph Hellwig <hch@lst.de>
commit a5e891321a219679d5a2828150a7dda29a47d8a6
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-448.el8/a5e89132.failed

default_nslabs should only be used to initialize nslabs, after that we
need to use the local variable that can shrink when allocations or the
remap don't succeed.

Fixes: 6424e31b1c05 ("swiotlb: remove swiotlb_init_with_tbl and swiotlb_init_late_with_tbl")
	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>
(cherry picked from commit a5e891321a219679d5a2828150a7dda29a47d8a6)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/dma/swiotlb.c
diff --cc kernel/dma/swiotlb.c
index eb8c08292910,113e1e8aaca3..000000000000
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@@ -228,17 -225,49 +228,52 @@@ static void swiotlb_init_io_tlb_mem(str
  	return;
  }
  
 -/*
 - * Statically reserve bounce buffer space and initialize bounce buffer data
 - * structures for the software IO TLB used to implement the DMA API.
 - */
 -void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 -		int (*remap)(void *tlb, unsigned long nslabs))
 +int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  {
  	struct io_tlb_mem *mem = &io_tlb_default_mem;
++<<<<<<< HEAD
 +	size_t alloc_size;
++=======
+ 	unsigned long nslabs = default_nslabs;
+ 	size_t alloc_size;
+ 	size_t bytes;
+ 	void *tlb;
++>>>>>>> a5e891321a21 (swiotlb: use the right nslabs value in swiotlb_init_remap)
  
 -	if (!addressing_limit && !swiotlb_force_bounce)
 -		return;
 -	if (swiotlb_force_disable)
 -		return;
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
 +		return 0;
  
++<<<<<<< HEAD
 +	/* protect against double initialization */
 +	if (WARN_ON_ONCE(mem->nslabs))
 +		return -ENOMEM;
++=======
+ 	/*
+ 	 * By default allocate the bounce buffer memory from low memory, but
+ 	 * allow to pick a location everywhere for hypervisors with guest
+ 	 * memory encryption.
+ 	 */
+ retry:
+ 	bytes = PAGE_ALIGN(nslabs << IO_TLB_SHIFT);
+ 	if (flags & SWIOTLB_ANY)
+ 		tlb = memblock_alloc(bytes, PAGE_SIZE);
+ 	else
+ 		tlb = memblock_alloc_low(bytes, PAGE_SIZE);
+ 	if (!tlb) {
+ 		pr_warn("%s: failed to allocate tlb structure\n", __func__);
+ 		return;
+ 	}
+ 
+ 	if (remap && remap(tlb, nslabs) < 0) {
+ 		memblock_free(tlb, PAGE_ALIGN(bytes));
+ 
+ 		nslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);
+ 		if (nslabs < IO_TLB_MIN_SLABS)
+ 			panic("%s: Failed to remap %zu bytes\n",
+ 			      __func__, bytes);
+ 		goto retry;
+ 	}
++>>>>>>> a5e891321a21 (swiotlb: use the right nslabs value in swiotlb_init_remap)
  
  	alloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));
  	mem->slots = memblock_alloc(alloc_size, PAGE_SIZE);
@@@ -247,37 -276,15 +282,41 @@@
  		      __func__, alloc_size, PAGE_SIZE);
  
  	swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
++<<<<<<< HEAD
++=======
+ 	mem->force_bounce = flags & SWIOTLB_FORCE;
++>>>>>>> a5e891321a21 (swiotlb: use the right nslabs value in swiotlb_init_remap)
  
 -	if (flags & SWIOTLB_VERBOSE)
 +	if (verbose)
  		swiotlb_print_info();
 +	return 0;
  }
  
 -void __init swiotlb_init(bool addressing_limit, unsigned int flags)
 +/*
 + * Statically reserve bounce buffer space and initialize bounce buffer data
 + * structures for the software IO TLB used to implement the DMA API.
 + */
 +void  __init
 +swiotlb_init(int verbose)
  {
 -	return swiotlb_init_remap(addressing_limit, flags, NULL);
 +	size_t bytes = PAGE_ALIGN(default_nslabs << IO_TLB_SHIFT);
 +	void *tlb;
 +
 +	if (swiotlb_force == SWIOTLB_NO_FORCE)
 +		return;
 +
 +	/* Get IO TLB memory from the low pages */
 +	tlb = memblock_alloc_low_nopanic(bytes, PAGE_SIZE);
 +	if (!tlb)
 +		goto fail;
 +	if (swiotlb_init_with_tbl(tlb, default_nslabs, verbose))
 +		goto fail_free_mem;
 +	return;
 +
 +fail_free_mem:
 +	memblock_free_early(__pa(tlb), bytes);
 +fail:
 +	pr_warn("Cannot allocate buffer");
  }
  
  /*
* Unmerged path kernel/dma/swiotlb.c
