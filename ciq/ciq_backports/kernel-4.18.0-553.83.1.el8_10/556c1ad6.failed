x86/vmscape: Enable the mitigation

jira LE-4704
cve CVE-2025-40300
Rebuild_History Non-Buildable kernel-4.18.0-553.83.1.el8_10
commit-author Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
commit 556c1ad666ad90c50ec8fccb930dd5046cfbecfb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.83.1.el8_10/556c1ad6.failed

Enable the previously added mitigation for VMscape. Add the cmdline
vmscape={off|ibpb|force} and sysfs reporting.

	Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
	Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
	Reviewed-by: Borislav Petkov (AMD) <bp@alien8.de>
	Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
(cherry picked from commit 556c1ad666ad90c50ec8fccb930dd5046cfbecfb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/admin-guide/kernel-parameters.txt
#	arch/x86/Kconfig
#	arch/x86/kernel/cpu/bugs.c
#	drivers/base/cpu.c
#	include/linux/cpu.h
diff --cc Documentation/admin-guide/kernel-parameters.txt
index cf789f4181a4,5a7a83c411e9..000000000000
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@@ -6019,7 -8042,17 +6020,21 @@@
  	vmpoff=		[KNL,S390] Perform z/VM CP command after power off.
  			Format: <command>
  
++<<<<<<< HEAD
 +	vsyscall=	[X86-64]
++=======
+ 	vmscape=	[X86] Controls mitigation for VMscape attacks.
+ 			VMscape attacks can leak information from a userspace
+ 			hypervisor to a guest via speculative side-channels.
+ 
+ 			off		- disable the mitigation
+ 			ibpb		- use Indirect Branch Prediction Barrier
+ 					  (IBPB) mitigation (default)
+ 			force		- force vulnerability detection even on
+ 					  unaffected processors
+ 
+ 	vsyscall=	[X86-64,EARLY]
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  			Controls the behavior of vsyscalls (i.e. calls to
  			fixed addresses of 0xffffffffff600x00 from legacy
  			code).  Most statically-linked binaries and older
diff --cc arch/x86/Kconfig
index 92821623048a,52c8910ba2ef..000000000000
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@@ -2626,6 -2579,137 +2626,140 @@@ config MITIGATION_SPECTRE_BH
  	  indirect branches.
  	  See <file:Documentation/admin-guide/hw-vuln/spectre.rst>
  
++<<<<<<< HEAD
++=======
+ config MITIGATION_MDS
+ 	bool "Mitigate Microarchitectural Data Sampling (MDS) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for Microarchitectural Data Sampling (MDS). MDS is
+ 	  a hardware vulnerability which allows unprivileged speculative access
+ 	  to data which is available in various CPU internal buffers.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/mds.rst>
+ 
+ config MITIGATION_TAA
+ 	bool "Mitigate TSX Asynchronous Abort (TAA) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for TSX Asynchronous Abort (TAA). TAA is a hardware
+ 	  vulnerability that allows unprivileged speculative access to data
+ 	  which is available in various CPU internal buffers by using
+ 	  asynchronous aborts within an Intel TSX transactional region.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/tsx_async_abort.rst>
+ 
+ config MITIGATION_MMIO_STALE_DATA
+ 	bool "Mitigate MMIO Stale Data hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for MMIO Stale Data hardware bugs.  Processor MMIO
+ 	  Stale Data Vulnerabilities are a class of memory-mapped I/O (MMIO)
+ 	  vulnerabilities that can expose data. The vulnerabilities require the
+ 	  attacker to have access to MMIO.
+ 	  See also
+ 	  <file:Documentation/admin-guide/hw-vuln/processor_mmio_stale_data.rst>
+ 
+ config MITIGATION_L1TF
+ 	bool "Mitigate L1 Terminal Fault (L1TF) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Mitigate L1 Terminal Fault (L1TF) hardware bug. L1 Terminal Fault is a
+ 	  hardware vulnerability which allows unprivileged speculative access to data
+ 	  available in the Level 1 Data Cache.
+ 	  See <file:Documentation/admin-guide/hw-vuln/l1tf.rst
+ 
+ config MITIGATION_RETBLEED
+ 	bool "Mitigate RETBleed hardware bug"
+ 	depends on (CPU_SUP_INTEL && MITIGATION_SPECTRE_V2) || MITIGATION_UNRET_ENTRY || MITIGATION_IBPB_ENTRY
+ 	default y
+ 	help
+ 	  Enable mitigation for RETBleed (Arbitrary Speculative Code Execution
+ 	  with Return Instructions) vulnerability.  RETBleed is a speculative
+ 	  execution attack which takes advantage of microarchitectural behavior
+ 	  in many modern microprocessors, similar to Spectre v2. An
+ 	  unprivileged attacker can use these flaws to bypass conventional
+ 	  memory security restrictions to gain read access to privileged memory
+ 	  that would otherwise be inaccessible.
+ 
+ config MITIGATION_SPECTRE_V1
+ 	bool "Mitigate SPECTRE V1 hardware bug"
+ 	default y
+ 	help
+ 	  Enable mitigation for Spectre V1 (Bounds Check Bypass). Spectre V1 is a
+ 	  class of side channel attacks that takes advantage of speculative
+ 	  execution that bypasses conditional branch instructions used for
+ 	  memory access bounds check.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/spectre.rst>
+ 
+ config MITIGATION_SPECTRE_V2
+ 	bool "Mitigate SPECTRE V2 hardware bug"
+ 	default y
+ 	help
+ 	  Enable mitigation for Spectre V2 (Branch Target Injection). Spectre
+ 	  V2 is a class of side channel attacks that takes advantage of
+ 	  indirect branch predictors inside the processor. In Spectre variant 2
+ 	  attacks, the attacker can steer speculative indirect branches in the
+ 	  victim to gadget code by poisoning the branch target buffer of a CPU
+ 	  used for predicting indirect branch addresses.
+ 	  See also <file:Documentation/admin-guide/hw-vuln/spectre.rst>
+ 
+ config MITIGATION_SRBDS
+ 	bool "Mitigate Special Register Buffer Data Sampling (SRBDS) hardware bug"
+ 	depends on CPU_SUP_INTEL
+ 	default y
+ 	help
+ 	  Enable mitigation for Special Register Buffer Data Sampling (SRBDS).
+ 	  SRBDS is a hardware vulnerability that allows Microarchitectural Data
+ 	  Sampling (MDS) techniques to infer values returned from special
+ 	  register accesses. An unprivileged user can extract values returned
+ 	  from RDRAND and RDSEED executed on another core or sibling thread
+ 	  using MDS techniques.
+ 	  See also
+ 	  <file:Documentation/admin-guide/hw-vuln/special-register-buffer-data-sampling.rst>
+ 
+ config MITIGATION_SSB
+ 	bool "Mitigate Speculative Store Bypass (SSB) hardware bug"
+ 	default y
+ 	help
+ 	  Enable mitigation for Speculative Store Bypass (SSB). SSB is a
+ 	  hardware security vulnerability and its exploitation takes advantage
+ 	  of speculative execution in a similar way to the Meltdown and Spectre
+ 	  security vulnerabilities.
+ 
+ config MITIGATION_ITS
+ 	bool "Enable Indirect Target Selection mitigation"
+ 	depends on CPU_SUP_INTEL && X86_64
+ 	depends on MITIGATION_RETPOLINE && MITIGATION_RETHUNK
+ 	select EXECMEM
+ 	default y
+ 	help
+ 	  Enable Indirect Target Selection (ITS) mitigation. ITS is a bug in
+ 	  BPU on some Intel CPUs that may allow Spectre V2 style attacks. If
+ 	  disabled, mitigation cannot be enabled via cmdline.
+ 	  See <file:Documentation/admin-guide/hw-vuln/indirect-target-selection.rst>
+ 
+ config MITIGATION_TSA
+ 	bool "Mitigate Transient Scheduler Attacks"
+ 	depends on CPU_SUP_AMD
+ 	default y
+ 	help
+ 	  Enable mitigation for Transient Scheduler Attacks. TSA is a hardware
+ 	  security vulnerability on AMD CPUs which can lead to forwarding of
+ 	  invalid info to subsequent instructions and thus can affect their
+ 	  timing and thereby cause a leakage.
+ 
+ config MITIGATION_VMSCAPE
+ 	bool "Mitigate VMSCAPE"
+ 	depends on KVM
+ 	default y
+ 	help
+ 	  Enable mitigation for VMSCAPE attacks. VMSCAPE is a hardware security
+ 	  vulnerability on Intel and AMD CPUs that may allow a guest to do
+ 	  Spectre v2 style attacks on userspace hypervisor.
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  endif
  
  config ARCH_HAS_ADD_PAGES
diff --cc arch/x86/kernel/cpu/bugs.c
index a556e8ade674,c81024dfc4c8..000000000000
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@@ -37,20 -33,72 +37,35 @@@
  
  #include "cpu.h"
  
 -/*
 - * Speculation Vulnerability Handling
 - *
 - * Each vulnerability is handled with the following functions:
 - *   <vuln>_select_mitigation() -- Selects a mitigation to use.  This should
 - *				   take into account all relevant command line
 - *				   options.
 - *   <vuln>_update_mitigation() -- This is called after all vulnerabilities have
 - *				   selected a mitigation, in case the selection
 - *				   may want to change based on other choices
 - *				   made.  This function is optional.
 - *   <vuln>_apply_mitigation() -- Enable the selected mitigation.
 - *
 - * The compile-time mitigation in all cases should be AUTO.  An explicit
 - * command-line option can override AUTO.  If no such option is
 - * provided, <vuln>_select_mitigation() will override AUTO to the best
 - * mitigation option.
 - */
 -
  static void __init spectre_v1_select_mitigation(void);
 -static void __init spectre_v1_apply_mitigation(void);
  static void __init spectre_v2_select_mitigation(void);
 -static void __init spectre_v2_update_mitigation(void);
 -static void __init spectre_v2_apply_mitigation(void);
  static void __init retbleed_select_mitigation(void);
 -static void __init retbleed_update_mitigation(void);
 -static void __init retbleed_apply_mitigation(void);
  static void __init spectre_v2_user_select_mitigation(void);
 -static void __init spectre_v2_user_update_mitigation(void);
 -static void __init spectre_v2_user_apply_mitigation(void);
  static void __init ssb_select_mitigation(void);
 -static void __init ssb_apply_mitigation(void);
  static void __init l1tf_select_mitigation(void);
 -static void __init l1tf_apply_mitigation(void);
  static void __init mds_select_mitigation(void);
 -static void __init mds_update_mitigation(void);
 -static void __init mds_apply_mitigation(void);
 +static void __init md_clear_update_mitigation(void);
 +static void __init md_clear_select_mitigation(void);
  static void __init taa_select_mitigation(void);
 -static void __init taa_update_mitigation(void);
 -static void __init taa_apply_mitigation(void);
  static void __init mmio_select_mitigation(void);
 -static void __init mmio_update_mitigation(void);
 -static void __init mmio_apply_mitigation(void);
 -static void __init rfds_select_mitigation(void);
 -static void __init rfds_update_mitigation(void);
 -static void __init rfds_apply_mitigation(void);
  static void __init srbds_select_mitigation(void);
 -static void __init srbds_apply_mitigation(void);
 -static void __init l1d_flush_select_mitigation(void);
  static void __init srso_select_mitigation(void);
 -static void __init srso_update_mitigation(void);
 -static void __init srso_apply_mitigation(void);
  static void __init gds_select_mitigation(void);
++<<<<<<< HEAD
++=======
+ static void __init gds_apply_mitigation(void);
+ static void __init bhi_select_mitigation(void);
+ static void __init bhi_update_mitigation(void);
+ static void __init bhi_apply_mitigation(void);
+ static void __init its_select_mitigation(void);
+ static void __init its_update_mitigation(void);
+ static void __init its_apply_mitigation(void);
+ static void __init tsa_select_mitigation(void);
+ static void __init tsa_apply_mitigation(void);
+ static void __init vmscape_select_mitigation(void);
+ static void __init vmscape_update_mitigation(void);
+ static void __init vmscape_apply_mitigation(void);
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  
  /* The base value of the SPEC_CTRL MSR without task-specific bits set */
  u64 x86_spec_ctrl_base;
@@@ -175,48 -262,68 +190,103 @@@ void __init check_bugs(void
  	spectre_v2_user_select_mitigation();
  	ssb_select_mitigation();
  	l1tf_select_mitigation();
 -	mds_select_mitigation();
 -	taa_select_mitigation();
 -	mmio_select_mitigation();
 -	rfds_select_mitigation();
 +	md_clear_select_mitigation();
  	srbds_select_mitigation();
 -	l1d_flush_select_mitigation();
 +
 +	/*
 +	 * srso_select_mitigation() depends and must run after
 +	 * retbleed_select_mitigation().
 +	 */
  	srso_select_mitigation();
  	gds_select_mitigation();
++<<<<<<< HEAD
 +
 +	arch_smt_update();
 +
 +#ifdef CONFIG_X86_32
 +	/*
 +	 * Check whether we are able to run this kernel safely on SMP.
 +	 *
 +	 * - i386 is no longer supported.
 +	 * - In order to run on anything without a TSC, we need to be
 +	 *   compiled for a i486.
 +	 */
 +	if (boot_cpu_data.x86 < 4)
 +		panic("Kernel requires i486+ for 'invlpg' and other features");
 +
 +	init_utsname()->machine[1] =
 +		'0' + (boot_cpu_data.x86 > 6 ? 6 : boot_cpu_data.x86);
 +	alternative_instructions();
 +
 +	fpu__init_check_bugs();
 +#else /* CONFIG_X86_64 */
 +	alternative_instructions();
++=======
+ 	its_select_mitigation();
+ 	bhi_select_mitigation();
+ 	tsa_select_mitigation();
+ 	vmscape_select_mitigation();
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  
  	/*
 -	 * After mitigations are selected, some may need to update their
 -	 * choices.
 +	 * Make sure the first 2MB area is not mapped by huge pages
 +	 * There are typically fixed size MTRRs in there and overlapping
 +	 * MTRRs into large pages causes slow downs.
 +	 *
 +	 * Right now we don't do that with gbpages because there seems
 +	 * very little benefit for that case.
  	 */
++<<<<<<< HEAD
 +	if (!direct_gbpages)
 +		set_memory_4k((unsigned long)__va(0), 1);
 +#endif
++=======
+ 	spectre_v2_update_mitigation();
+ 	/*
+ 	 * retbleed_update_mitigation() relies on the state set by
+ 	 * spectre_v2_update_mitigation(); specifically it wants to know about
+ 	 * spectre_v2=ibrs.
+ 	 */
+ 	retbleed_update_mitigation();
+ 	/*
+ 	 * its_update_mitigation() depends on spectre_v2_update_mitigation()
+ 	 * and retbleed_update_mitigation().
+ 	 */
+ 	its_update_mitigation();
+ 
+ 	/*
+ 	 * spectre_v2_user_update_mitigation() depends on
+ 	 * retbleed_update_mitigation(), specifically the STIBP
+ 	 * selection is forced for UNRET or IBPB.
+ 	 */
+ 	spectre_v2_user_update_mitigation();
+ 	mds_update_mitigation();
+ 	taa_update_mitigation();
+ 	mmio_update_mitigation();
+ 	rfds_update_mitigation();
+ 	bhi_update_mitigation();
+ 	/* srso_update_mitigation() depends on retbleed_update_mitigation(). */
+ 	srso_update_mitigation();
+ 	vmscape_update_mitigation();
+ 
+ 	spectre_v1_apply_mitigation();
+ 	spectre_v2_apply_mitigation();
+ 	retbleed_apply_mitigation();
+ 	spectre_v2_user_apply_mitigation();
+ 	ssb_apply_mitigation();
+ 	l1tf_apply_mitigation();
+ 	mds_apply_mitigation();
+ 	taa_apply_mitigation();
+ 	mmio_apply_mitigation();
+ 	rfds_apply_mitigation();
+ 	srbds_apply_mitigation();
+ 	srso_apply_mitigation();
+ 	gds_apply_mitigation();
+ 	its_apply_mitigation();
+ 	bhi_apply_mitigation();
+ 	tsa_apply_mitigation();
+ 	vmscape_apply_mitigation();
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  }
  
  /*
@@@ -2644,66 -3284,121 +2714,137 @@@ static void __init srso_select_mitigati
  		if (boot_cpu_has(X86_FEATURE_SBPB))
  			x86_pred_cmd = PRED_CMD_SBPB;
  		return;
 -	}
  
 -	switch (srso_mitigation) {
 -	case SRSO_MITIGATION_SAFE_RET:
 -	case SRSO_MITIGATION_SAFE_RET_UCODE_NEEDED:
 -		/*
 -		 * Enable the return thunk for generated code
 -		 * like ftrace, static_call, etc.
 -		 */
 -		setup_force_cpu_cap(X86_FEATURE_RETHUNK);
 -		setup_force_cpu_cap(X86_FEATURE_UNRET);
 +	case SRSO_CMD_MICROCODE:
 +		if (has_microcode) {
 +			srso_mitigation = SRSO_MITIGATION_MICROCODE;
 +			pr_warn(SRSO_NOTICE);
 +		}
 +		break;
  
 -		if (boot_cpu_data.x86 == 0x19) {
 -			setup_force_cpu_cap(X86_FEATURE_SRSO_ALIAS);
 -			set_return_thunk(srso_alias_return_thunk);
 +	case SRSO_CMD_SAFE_RET:
 +		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
 +			/*
 +			 * Enable the return thunk for generated code
 +			 * like ftrace, static_call, etc.
 +			 */
 +			setup_force_cpu_cap(X86_FEATURE_RETHUNK);
 +			setup_force_cpu_cap(X86_FEATURE_UNRET);
 +
 +			if (boot_cpu_data.x86 == 0x19) {
 +				setup_force_cpu_cap(X86_FEATURE_SRSO_ALIAS);
 +				x86_return_thunk = srso_alias_return_thunk;
 +			} else {
 +				setup_force_cpu_cap(X86_FEATURE_SRSO);
 +				x86_return_thunk = srso_return_thunk;
 +			}
 +			if (has_microcode)
 +				srso_mitigation = SRSO_MITIGATION_SAFE_RET;
 +			else
 +				srso_mitigation = SRSO_MITIGATION_SAFE_RET_UCODE_NEEDED;
  		} else {
 -			setup_force_cpu_cap(X86_FEATURE_SRSO);
 -			set_return_thunk(srso_return_thunk);
 +			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
  		}
  		break;
 -	case SRSO_MITIGATION_IBPB:
 -		setup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);
 -		/*
 -		 * IBPB on entry already obviates the need for
 -		 * software-based untraining so clear those in case some
 -		 * other mitigation like Retbleed has selected them.
 -		 */
 -		setup_clear_cpu_cap(X86_FEATURE_UNRET);
 -		setup_clear_cpu_cap(X86_FEATURE_RETHUNK);
 -		fallthrough;
 -	case SRSO_MITIGATION_IBPB_ON_VMEXIT:
 -		setup_force_cpu_cap(X86_FEATURE_IBPB_ON_VMEXIT);
 -		/*
 -		 * There is no need for RSB filling: entry_ibpb() ensures
 -		 * all predictions, including the RSB, are invalidated,
 -		 * regardless of IBPB implementation.
 -		 */
 -		setup_clear_cpu_cap(X86_FEATURE_RSB_VMEXIT);
 +
 +	case SRSO_CMD_IBPB:
 +		if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY)) {
 +			if (has_microcode) {
 +				setup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);
 +				srso_mitigation = SRSO_MITIGATION_IBPB;
 +			}
 +		} else {
 +			pr_err("WARNING: kernel not compiled with CPU_IBPB_ENTRY.\n");
 +		}
  		break;
 -	default:
 +
 +	case SRSO_CMD_IBPB_ON_VMEXIT:
 +		if (IS_ENABLED(CONFIG_CPU_SRSO)) {
 +			if (!boot_cpu_has(X86_FEATURE_ENTRY_IBPB) && has_microcode) {
 +				setup_force_cpu_cap(X86_FEATURE_IBPB_ON_VMEXIT);
 +				srso_mitigation = SRSO_MITIGATION_IBPB_ON_VMEXIT;
 +			}
 +		} else {
 +			pr_err("WARNING: kernel not compiled with CPU_SRSO.\n");
 +                }
  		break;
  	}
 +
 +out:
 +	pr_info("%s\n", srso_strings[srso_mitigation]);
  }
  
+ #undef pr_fmt
+ #define pr_fmt(fmt)	"VMSCAPE: " fmt
+ 
+ enum vmscape_mitigations {
+ 	VMSCAPE_MITIGATION_NONE,
+ 	VMSCAPE_MITIGATION_AUTO,
+ 	VMSCAPE_MITIGATION_IBPB_EXIT_TO_USER,
+ 	VMSCAPE_MITIGATION_IBPB_ON_VMEXIT,
+ };
+ 
+ static const char * const vmscape_strings[] = {
+ 	[VMSCAPE_MITIGATION_NONE]		= "Vulnerable",
+ 	/* [VMSCAPE_MITIGATION_AUTO] */
+ 	[VMSCAPE_MITIGATION_IBPB_EXIT_TO_USER]	= "Mitigation: IBPB before exit to userspace",
+ 	[VMSCAPE_MITIGATION_IBPB_ON_VMEXIT]	= "Mitigation: IBPB on VMEXIT",
+ };
+ 
+ static enum vmscape_mitigations vmscape_mitigation __ro_after_init =
+ 	IS_ENABLED(CONFIG_MITIGATION_VMSCAPE) ? VMSCAPE_MITIGATION_AUTO : VMSCAPE_MITIGATION_NONE;
+ 
+ static int __init vmscape_parse_cmdline(char *str)
+ {
+ 	if (!str)
+ 		return -EINVAL;
+ 
+ 	if (!strcmp(str, "off")) {
+ 		vmscape_mitigation = VMSCAPE_MITIGATION_NONE;
+ 	} else if (!strcmp(str, "ibpb")) {
+ 		vmscape_mitigation = VMSCAPE_MITIGATION_IBPB_EXIT_TO_USER;
+ 	} else if (!strcmp(str, "force")) {
+ 		setup_force_cpu_bug(X86_BUG_VMSCAPE);
+ 		vmscape_mitigation = VMSCAPE_MITIGATION_AUTO;
+ 	} else {
+ 		pr_err("Ignoring unknown vmscape=%s option.\n", str);
+ 	}
+ 
+ 	return 0;
+ }
+ early_param("vmscape", vmscape_parse_cmdline);
+ 
+ static void __init vmscape_select_mitigation(void)
+ {
+ 	if (cpu_mitigations_off() ||
+ 	    !boot_cpu_has_bug(X86_BUG_VMSCAPE) ||
+ 	    !boot_cpu_has(X86_FEATURE_IBPB)) {
+ 		vmscape_mitigation = VMSCAPE_MITIGATION_NONE;
+ 		return;
+ 	}
+ 
+ 	if (vmscape_mitigation == VMSCAPE_MITIGATION_AUTO)
+ 		vmscape_mitigation = VMSCAPE_MITIGATION_IBPB_EXIT_TO_USER;
+ }
+ 
+ static void __init vmscape_update_mitigation(void)
+ {
+ 	if (!boot_cpu_has_bug(X86_BUG_VMSCAPE))
+ 		return;
+ 
+ 	if (retbleed_mitigation == RETBLEED_MITIGATION_IBPB ||
+ 	    srso_mitigation == SRSO_MITIGATION_IBPB_ON_VMEXIT)
+ 		vmscape_mitigation = VMSCAPE_MITIGATION_IBPB_ON_VMEXIT;
+ 
+ 	pr_info("%s\n", vmscape_strings[vmscape_mitigation]);
+ }
+ 
+ static void __init vmscape_apply_mitigation(void)
+ {
+ 	if (vmscape_mitigation == VMSCAPE_MITIGATION_IBPB_EXIT_TO_USER)
+ 		setup_force_cpu_cap(X86_FEATURE_IBPB_EXIT_TO_USER);
+ }
+ 
  #undef pr_fmt
  #define pr_fmt(fmt) fmt
  
@@@ -2940,6 -3642,16 +3081,19 @@@ static ssize_t gds_show_state(char *buf
  	return sysfs_emit(buf, "%s\n", gds_strings[gds_mitigation]);
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t tsa_show_state(char *buf)
+ {
+ 	return sysfs_emit(buf, "%s\n", tsa_strings[tsa_mitigation]);
+ }
+ 
+ static ssize_t vmscape_show_state(char *buf)
+ {
+ 	return sysfs_emit(buf, "%s\n", vmscape_strings[vmscape_mitigation]);
+ }
+ 
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  static ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,
  			       char *buf, unsigned int bug)
  {
@@@ -2998,6 -3709,18 +3152,21 @@@
  	case X86_BUG_RFDS:
  		return rfds_show_state(buf);
  
++<<<<<<< HEAD
++=======
+ 	case X86_BUG_OLD_MICROCODE:
+ 		return old_microcode_show_state(buf);
+ 
+ 	case X86_BUG_ITS:
+ 		return its_show_state(buf);
+ 
+ 	case X86_BUG_TSA:
+ 		return tsa_show_state(buf);
+ 
+ 	case X86_BUG_VMSCAPE:
+ 		return vmscape_show_state(buf);
+ 
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  	default:
  		break;
  	}
@@@ -3077,4 -3797,29 +3246,27 @@@ ssize_t cpu_show_reg_file_data_sampling
  {
  	return cpu_show_common(dev, attr, buf, X86_BUG_RFDS);
  }
++<<<<<<< HEAD
++=======
+ 
+ ssize_t cpu_show_old_microcode(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return cpu_show_common(dev, attr, buf, X86_BUG_OLD_MICROCODE);
+ }
+ 
+ ssize_t cpu_show_indirect_target_selection(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return cpu_show_common(dev, attr, buf, X86_BUG_ITS);
+ }
+ 
+ ssize_t cpu_show_tsa(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return cpu_show_common(dev, attr, buf, X86_BUG_TSA);
+ }
+ 
+ ssize_t cpu_show_vmscape(struct device *dev, struct device_attribute *attr, char *buf)
+ {
+ 	return cpu_show_common(dev, attr, buf, X86_BUG_VMSCAPE);
+ }
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  #endif
 -
 -void __warn_thunk(void)
 -{
 -	WARN_ONCE(1, "Unpatched return thunk in use. This should not happen!\n");
 -}
diff --cc drivers/base/cpu.c
index aae1dde82781,008da0354fba..000000000000
--- a/drivers/base/cpu.c
+++ b/drivers/base/cpu.c
@@@ -551,6 -599,11 +551,14 @@@ CPU_SHOW_VULN_FALLBACK(retbleed)
  CPU_SHOW_VULN_FALLBACK(spec_rstack_overflow);
  CPU_SHOW_VULN_FALLBACK(gds);
  CPU_SHOW_VULN_FALLBACK(reg_file_data_sampling);
++<<<<<<< HEAD
++=======
+ CPU_SHOW_VULN_FALLBACK(ghostwrite);
+ CPU_SHOW_VULN_FALLBACK(old_microcode);
+ CPU_SHOW_VULN_FALLBACK(indirect_target_selection);
+ CPU_SHOW_VULN_FALLBACK(tsa);
+ CPU_SHOW_VULN_FALLBACK(vmscape);
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  
  static DEVICE_ATTR(meltdown, 0444, cpu_show_meltdown, NULL);
  static DEVICE_ATTR(spectre_v1, 0444, cpu_show_spectre_v1, NULL);
@@@ -566,6 -619,11 +574,14 @@@ static DEVICE_ATTR(retbleed, 0444, cpu_
  static DEVICE_ATTR(spec_rstack_overflow, 0444, cpu_show_spec_rstack_overflow, NULL);
  static DEVICE_ATTR(gather_data_sampling, 0444, cpu_show_gds, NULL);
  static DEVICE_ATTR(reg_file_data_sampling, 0444, cpu_show_reg_file_data_sampling, NULL);
++<<<<<<< HEAD
++=======
+ static DEVICE_ATTR(ghostwrite, 0444, cpu_show_ghostwrite, NULL);
+ static DEVICE_ATTR(old_microcode, 0444, cpu_show_old_microcode, NULL);
+ static DEVICE_ATTR(indirect_target_selection, 0444, cpu_show_indirect_target_selection, NULL);
+ static DEVICE_ATTR(tsa, 0444, cpu_show_tsa, NULL);
+ static DEVICE_ATTR(vmscape, 0444, cpu_show_vmscape, NULL);
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  
  static struct attribute *cpu_root_vulnerabilities_attrs[] = {
  	&dev_attr_meltdown.attr,
@@@ -582,6 -640,11 +598,14 @@@
  	&dev_attr_spec_rstack_overflow.attr,
  	&dev_attr_gather_data_sampling.attr,
  	&dev_attr_reg_file_data_sampling.attr,
++<<<<<<< HEAD
++=======
+ 	&dev_attr_ghostwrite.attr,
+ 	&dev_attr_old_microcode.attr,
+ 	&dev_attr_indirect_target_selection.attr,
+ 	&dev_attr_tsa.attr,
+ 	&dev_attr_vmscape.attr,
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  	NULL
  };
  
diff --cc include/linux/cpu.h
index 1e897b9cf3a6,487b3bf2e1ea..000000000000
--- a/include/linux/cpu.h
+++ b/include/linux/cpu.h
@@@ -76,6 -77,13 +76,16 @@@ extern ssize_t cpu_show_gds(struct devi
  			    struct device_attribute *attr, char *buf);
  extern ssize_t cpu_show_reg_file_data_sampling(struct device *dev,
  					       struct device_attribute *attr, char *buf);
++<<<<<<< HEAD
++=======
+ extern ssize_t cpu_show_ghostwrite(struct device *dev, struct device_attribute *attr, char *buf);
+ extern ssize_t cpu_show_old_microcode(struct device *dev,
+ 				      struct device_attribute *attr, char *buf);
+ extern ssize_t cpu_show_indirect_target_selection(struct device *dev,
+ 						  struct device_attribute *attr, char *buf);
+ extern ssize_t cpu_show_tsa(struct device *dev, struct device_attribute *attr, char *buf);
+ extern ssize_t cpu_show_vmscape(struct device *dev, struct device_attribute *attr, char *buf);
++>>>>>>> 556c1ad666ad (x86/vmscape: Enable the mitigation)
  
  extern __printf(4, 5)
  struct device *cpu_device_create(struct device *parent, void *drvdata,
diff --git a/Documentation/ABI/testing/sysfs-devices-system-cpu b/Documentation/ABI/testing/sysfs-devices-system-cpu
index d9415c851cc8..a8fc8134da9e 100644
--- a/Documentation/ABI/testing/sysfs-devices-system-cpu
+++ b/Documentation/ABI/testing/sysfs-devices-system-cpu
@@ -499,6 +499,7 @@ What:		/sys/devices/system/cpu/vulnerabilities
 		/sys/devices/system/cpu/vulnerabilities/spectre_v2
 		/sys/devices/system/cpu/vulnerabilities/srbds
 		/sys/devices/system/cpu/vulnerabilities/tsx_async_abort
+		/sys/devices/system/cpu/vulnerabilities/vmscape
 Date:		January 2018
 Contact:	Linux kernel mailing list <linux-kernel@vger.kernel.org>
 Description:	Information about CPU vulnerabilities
* Unmerged path Documentation/admin-guide/kernel-parameters.txt
* Unmerged path arch/x86/Kconfig
* Unmerged path arch/x86/kernel/cpu/bugs.c
* Unmerged path drivers/base/cpu.c
* Unmerged path include/linux/cpu.h
