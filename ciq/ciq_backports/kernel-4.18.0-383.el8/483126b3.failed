skbuff: make __build_skb_around() return void

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-383.el8
commit-author Alexander Lobakin <alobakin@pm.me>
commit 483126b3b2c649c0ef95f67ac75d3c99390d6cc8
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-383.el8/483126b3.failed

__build_skb_around() can never fail and always returns passed skb.
Make it return void to simplify and optimize the code.

	Signed-off-by: Alexander Lobakin <alobakin@pm.me>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 483126b3b2c649c0ef95f67ac75d3c99390d6cc8)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skbuff.c
diff --cc net/core/skbuff.c
index eb88d35559a5,c7d184e11547..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -124,6 -119,152 +124,155 @@@ static void skb_under_panic(struct sk_b
  	skb_panic(skb, sz, addr, __func__);
  }
  
++<<<<<<< HEAD
++=======
+ /* Caller must provide SKB that is memset cleared */
+ static void __build_skb_around(struct sk_buff *skb, void *data,
+ 			       unsigned int frag_size)
+ {
+ 	struct skb_shared_info *shinfo;
+ 	unsigned int size = frag_size ? : ksize(data);
+ 
+ 	size -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+ 
+ 	/* Assumes caller memset cleared SKB */
+ 	skb->truesize = SKB_TRUESIZE(size);
+ 	refcount_set(&skb->users, 1);
+ 	skb->head = data;
+ 	skb->data = data;
+ 	skb_reset_tail_pointer(skb);
+ 	skb->end = skb->tail + size;
+ 	skb->mac_header = (typeof(skb->mac_header))~0U;
+ 	skb->transport_header = (typeof(skb->transport_header))~0U;
+ 
+ 	/* make sure we initialize shinfo sequentially */
+ 	shinfo = skb_shinfo(skb);
+ 	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+ 	atomic_set(&shinfo->dataref, 1);
+ 
+ 	skb_set_kcov_handle(skb, kcov_common_handle());
+ }
+ 
+ /**
+  * __build_skb - build a network buffer
+  * @data: data buffer provided by caller
+  * @frag_size: size of data, or 0 if head was kmalloced
+  *
+  * Allocate a new &sk_buff. Caller provides space holding head and
+  * skb_shared_info. @data must have been allocated by kmalloc() only if
+  * @frag_size is 0, otherwise data should come from the page allocator
+  *  or vmalloc()
+  * The return is the new skb buffer.
+  * On a failure the return is %NULL, and @data is not freed.
+  * Notes :
+  *  Before IO, driver allocates only data buffer where NIC put incoming frame
+  *  Driver should add room at head (NET_SKB_PAD) and
+  *  MUST add room at tail (SKB_DATA_ALIGN(skb_shared_info))
+  *  After IO, driver calls build_skb(), to allocate sk_buff and populate it
+  *  before giving packet to stack.
+  *  RX rings only contains data buffers, not full skbs.
+  */
+ struct sk_buff *__build_skb(void *data, unsigned int frag_size)
+ {
+ 	struct sk_buff *skb;
+ 
+ 	skb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
+ 	memset(skb, 0, offsetof(struct sk_buff, tail));
+ 	__build_skb_around(skb, data, frag_size);
+ 
+ 	return skb;
+ }
+ 
+ /* build_skb() is wrapper over __build_skb(), that specifically
+  * takes care of skb->head and skb->pfmemalloc
+  * This means that if @frag_size is not zero, then @data must be backed
+  * by a page fragment, not kmalloc() or vmalloc()
+  */
+ struct sk_buff *build_skb(void *data, unsigned int frag_size)
+ {
+ 	struct sk_buff *skb = __build_skb(data, frag_size);
+ 
+ 	if (skb && frag_size) {
+ 		skb->head_frag = 1;
+ 		if (page_is_pfmemalloc(virt_to_head_page(data)))
+ 			skb->pfmemalloc = 1;
+ 	}
+ 	return skb;
+ }
+ EXPORT_SYMBOL(build_skb);
+ 
+ /**
+  * build_skb_around - build a network buffer around provided skb
+  * @skb: sk_buff provide by caller, must be memset cleared
+  * @data: data buffer provided by caller
+  * @frag_size: size of data, or 0 if head was kmalloced
+  */
+ struct sk_buff *build_skb_around(struct sk_buff *skb,
+ 				 void *data, unsigned int frag_size)
+ {
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
+ 	__build_skb_around(skb, data, frag_size);
+ 
+ 	if (frag_size) {
+ 		skb->head_frag = 1;
+ 		if (page_is_pfmemalloc(virt_to_head_page(data)))
+ 			skb->pfmemalloc = 1;
+ 	}
+ 	return skb;
+ }
+ EXPORT_SYMBOL(build_skb_around);
+ 
+ #define NAPI_SKB_CACHE_SIZE	64
+ 
+ struct napi_alloc_cache {
+ 	struct page_frag_cache page;
+ 	unsigned int skb_count;
+ 	void *skb_cache[NAPI_SKB_CACHE_SIZE];
+ };
+ 
+ static DEFINE_PER_CPU(struct page_frag_cache, netdev_alloc_cache);
+ static DEFINE_PER_CPU(struct napi_alloc_cache, napi_alloc_cache);
+ 
+ static void *__alloc_frag_align(unsigned int fragsz, gfp_t gfp_mask,
+ 				unsigned int align_mask)
+ {
+ 	struct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);
+ 
+ 	return page_frag_alloc_align(&nc->page, fragsz, gfp_mask, align_mask);
+ }
+ 
+ void *__napi_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)
+ {
+ 	fragsz = SKB_DATA_ALIGN(fragsz);
+ 
+ 	return __alloc_frag_align(fragsz, GFP_ATOMIC, align_mask);
+ }
+ EXPORT_SYMBOL(__napi_alloc_frag_align);
+ 
+ void *__netdev_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)
+ {
+ 	struct page_frag_cache *nc;
+ 	void *data;
+ 
+ 	fragsz = SKB_DATA_ALIGN(fragsz);
+ 	if (in_irq() || irqs_disabled()) {
+ 		nc = this_cpu_ptr(&netdev_alloc_cache);
+ 		data = page_frag_alloc_align(nc, fragsz, GFP_ATOMIC, align_mask);
+ 	} else {
+ 		local_bh_disable();
+ 		data = __alloc_frag_align(fragsz, GFP_ATOMIC, align_mask);
+ 		local_bh_enable();
+ 	}
+ 	return data;
+ }
+ EXPORT_SYMBOL(__netdev_alloc_frag_align);
+ 
++>>>>>>> 483126b3b2c6 (skbuff: make __build_skb_around() return void)
  /*
   * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells
   * the caller if emergency pfmemalloc reserves are being used. If it is and
* Unmerged path net/core/skbuff.c
