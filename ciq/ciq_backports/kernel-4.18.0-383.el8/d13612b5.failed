skbuff: allow to optionally use NAPI cache from __alloc_skb()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-383.el8
commit-author Alexander Lobakin <alobakin@pm.me>
commit d13612b58e6453fc664f282514fe2bd7b848230f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-383.el8/d13612b5.failed

Reuse the old and forgotten SKB_ALLOC_NAPI to add an option to get
an skbuff_head from the NAPI cache instead of inplace allocation
inside __alloc_skb().
This implies that the function is called from softirq or BH-off
context, not for allocating a clone or from a distant node.

	Cc: Alexander Duyck <alexander.duyck@gmail.com> # Simplified flags check
	Signed-off-by: Alexander Lobakin <alobakin@pm.me>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit d13612b58e6453fc664f282514fe2bd7b848230f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/core/skbuff.c
diff --cc net/core/skbuff.c
index 764873a244e5,a80581eed7fc..000000000000
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@@ -124,145 -119,75 +124,155 @@@ static void skb_under_panic(struct sk_b
  	skb_panic(skb, sz, addr, __func__);
  }
  
 -#define NAPI_SKB_CACHE_SIZE	64
 -#define NAPI_SKB_CACHE_BULK	16
 -#define NAPI_SKB_CACHE_HALF	(NAPI_SKB_CACHE_SIZE / 2)
 +/*
 + * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells
 + * the caller if emergency pfmemalloc reserves are being used. If it is and
 + * the socket is later found to be SOCK_MEMALLOC then PFMEMALLOC reserves
 + * may be used. Otherwise, the packet data may be discarded until enough
 + * memory is free
 + */
 +static void *kmalloc_reserve(size_t size, gfp_t flags, int node,
 +			     bool *pfmemalloc)
 +{
 +	void *obj;
 +	bool ret_pfmemalloc = false;
  
 -struct napi_alloc_cache {
 -	struct page_frag_cache page;
 -	unsigned int skb_count;
 -	void *skb_cache[NAPI_SKB_CACHE_SIZE];
 -};
 +	/*
 +	 * Try a regular allocation, when that fails and we're not entitled
 +	 * to the reserves, fail.
 +	 */
 +	obj = kmalloc_node_track_caller(size,
 +					flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
 +					node);
 +	if (obj || !(gfp_pfmemalloc_allowed(flags)))
 +		goto out;
  
 -static DEFINE_PER_CPU(struct page_frag_cache, netdev_alloc_cache);
 -static DEFINE_PER_CPU(struct napi_alloc_cache, napi_alloc_cache);
 +	/* Try again but now we are using pfmemalloc reserves */
 +	ret_pfmemalloc = true;
 +	obj = kmalloc_node_track_caller(size, flags, node);
  
 -static void *__alloc_frag_align(unsigned int fragsz, gfp_t gfp_mask,
 -				unsigned int align_mask)
 -{
 -	struct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);
 +out:
 +	if (pfmemalloc)
 +		*pfmemalloc = ret_pfmemalloc;
  
 -	return page_frag_alloc_align(&nc->page, fragsz, gfp_mask, align_mask);
 +	return obj;
  }
  
 -void *__napi_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)
 -{
 -	fragsz = SKB_DATA_ALIGN(fragsz);
 -
 -	return __alloc_frag_align(fragsz, GFP_ATOMIC, align_mask);
 -}
 -EXPORT_SYMBOL(__napi_alloc_frag_align);
 +/* 	Allocate a new skbuff. We do this ourselves so we can fill in a few
 + *	'private' fields and also do memory statistics to find all the
 + *	[BEEP] leaks.
 + *
 + */
  
 -void *__netdev_alloc_frag_align(unsigned int fragsz, unsigned int align_mask)
 +/**
 + *	__alloc_skb	-	allocate a network buffer
 + *	@size: size to allocate
 + *	@gfp_mask: allocation mask
 + *	@flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache
 + *		instead of head cache and allocate a cloned (child) skb.
 + *		If SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for
 + *		allocations in case the data is required for writeback
 + *	@node: numa node to allocate memory on
 + *
 + *	Allocate a new &sk_buff. The returned buffer has no headroom and a
 + *	tail room of at least size bytes. The object has a reference count
 + *	of one. The return is the buffer. On a failure the return is %NULL.
 + *
 + *	Buffers may only be allocated from interrupts using a @gfp_mask of
 + *	%GFP_ATOMIC.
 + */
 +struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 +			    int flags, int node)
  {
 -	struct page_frag_cache *nc;
 -	void *data;
 +	struct kmem_cache *cache;
 +	struct skb_shared_info *shinfo;
 +	struct sk_buff *skb;
 +	u8 *data;
 +	bool pfmemalloc;
  
 -	fragsz = SKB_DATA_ALIGN(fragsz);
 -	if (in_irq() || irqs_disabled()) {
 -		nc = this_cpu_ptr(&netdev_alloc_cache);
 -		data = page_frag_alloc_align(nc, fragsz, GFP_ATOMIC, align_mask);
 -	} else {
 -		local_bh_disable();
 -		data = __alloc_frag_align(fragsz, GFP_ATOMIC, align_mask);
 -		local_bh_enable();
 -	}
 -	return data;
 -}
 -EXPORT_SYMBOL(__netdev_alloc_frag_align);
 +	cache = (flags & SKB_ALLOC_FCLONE)
 +		? skbuff_fclone_cache : skbuff_head_cache;
  
 -static struct sk_buff *napi_skb_cache_get(void)
 -{
 -	struct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);
 -	struct sk_buff *skb;
 +	if (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))
 +		gfp_mask |= __GFP_MEMALLOC;
  
 -	if (unlikely(!nc->skb_count))
 -		nc->skb_count = kmem_cache_alloc_bulk(skbuff_head_cache,
 -						      GFP_ATOMIC,
 -						      NAPI_SKB_CACHE_BULK,
 -						      nc->skb_cache);
 -	if (unlikely(!nc->skb_count))
 +	/* Get the HEAD */
++<<<<<<< HEAD
 +	skb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);
 +	if (!skb)
 +		goto out;
++=======
++	if ((flags & (SKB_ALLOC_FCLONE | SKB_ALLOC_NAPI)) == SKB_ALLOC_NAPI &&
++	    likely(node == NUMA_NO_NODE || node == numa_mem_id()))
++		skb = napi_skb_cache_get();
++	else
++		skb = kmem_cache_alloc_node(cache, gfp_mask & ~GFP_DMA, node);
++	if (unlikely(!skb))
+ 		return NULL;
++>>>>>>> d13612b58e64 (skbuff: allow to optionally use NAPI cache from __alloc_skb())
 +	prefetchw(skb);
  
 -	skb = nc->skb_cache[--nc->skb_count];
 -	kasan_unpoison_object_data(skbuff_head_cache, skb);
 +	/* We do our best to align skb_shared_info on a separate cache
 +	 * line. It usually works because kmalloc(X > SMP_CACHE_BYTES) gives
 +	 * aligned memory blocks, unless SLUB/SLAB debug is enabled.
 +	 * Both skb->head and skb_shared_info are cache line aligned.
 +	 */
 +	size = SKB_DATA_ALIGN(size);
 +	size += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 +	data = kmalloc_reserve(size, gfp_mask, node, &pfmemalloc);
 +	if (!data)
 +		goto nodata;
 +	/* kmalloc(size) might give us more room than requested.
 +	 * Put skb_shared_info exactly at the end of allocated zone,
 +	 * to allow max possible filling before reallocation.
 +	 */
 +	size = SKB_WITH_OVERHEAD(ksize(data));
 +	prefetchw(data + size);
  
 +	/*
 +	 * Only clear those fields we need to clear, not those that we will
 +	 * actually initialise below. Hence, don't put any more fields after
 +	 * the tail pointer in struct sk_buff!
 +	 */
 +	memset(skb, 0, offsetof(struct sk_buff, tail));
 +	/* Account for allocated memory : skb + skb->head */
 +	skb->truesize = SKB_TRUESIZE(size);
 +	skb->pfmemalloc = pfmemalloc;
 +	refcount_set(&skb->users, 1);
 +	skb->head = data;
 +	skb->data = data;
 +	skb_reset_tail_pointer(skb);
 +	skb->end = skb->tail + size;
 +	skb->mac_header = (typeof(skb->mac_header))~0U;
 +	skb->transport_header = (typeof(skb->transport_header))~0U;
 +
 +	/* make sure we initialize shinfo sequentially */
 +	shinfo = skb_shinfo(skb);
 +	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
 +	atomic_set(&shinfo->dataref, 1);
 +
 +	if (flags & SKB_ALLOC_FCLONE) {
 +		struct sk_buff_fclones *fclones;
 +
 +		fclones = container_of(skb, struct sk_buff_fclones, skb1);
 +
 +		skb->fclone = SKB_FCLONE_ORIG;
 +		refcount_set(&fclones->fclone_ref, 1);
 +
 +		fclones->skb2.fclone = SKB_FCLONE_CLONE;
 +	}
 +out:
  	return skb;
 +nodata:
 +	kmem_cache_free(cache, skb);
 +	skb = NULL;
 +	goto out;
  }
 +EXPORT_SYMBOL(__alloc_skb);
  
  /* Caller must provide SKB that is memset cleared */
 -static void __build_skb_around(struct sk_buff *skb, void *data,
 -			       unsigned int frag_size)
 +static struct sk_buff *__build_skb_around(struct sk_buff *skb,
 +					  void *data, unsigned int frag_size)
  {
  	struct skb_shared_info *shinfo;
  	unsigned int size = frag_size ? : ksize(data);
* Unmerged path net/core/skbuff.c
