ice: Add tc-flower filter support for channel

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-383.el8
commit-author Kiran Patil <kiran.patil@intel.com>
commit 9fea749856d14c4713a2f5dee6f692aeaa2700b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-383.el8/9fea7498.failed

Add support to add/delete channel specific filter using tc-flower.
For now, only supported action is "skip_sw hw_tc <tc_num>"

Filter criteria is specific to channel and it can be
combination of L3, L3+L4, L2+L4.

Example:
MATCH criteria       Action
---------------------------
src and/or dest IPv4[6]/mask -> Forward to "hw_tc <tc_num>"
dest IPv4[6]/mask + dest L4 port -> Forward to "hw_tc <tc_num>"
dest MAC + dest L4 port -> Forward to "hw_tc <tc_num>"
src IPv4[6]/mask + src L4 port -> Forward to "hw_tc <tc_num>"
src MAC + src L4 port -> Forward to "hw_tc <tc_num>"

Adding tc-flower filter for channel using "hw_tc"
-------------------------------------------------
tc qdisc add dev <ethX> clsact

Above two steps are only needed the first time when adding
tc-flower filter.

tc filter add dev <ethX> protocol ip ingress prio 1 flower \
     dst_ip 192.168.0.1/32 ip_proto tcp dst_port 5001 \
     skip_sw hw_tc 1

tc filter show dev <ethX> ingress
filter protocol ip pref 1 flower chain 0
filter protocol ip pref 1 flower chain 0 handle 0x1 hw_tc 1
  eth_type ipv4
  ip_proto tcp
  dst_ip 192.168.0.1
  dst_port 5001
  skip_sw
  in_hw in_hw_count 1

Delete specific filter:
-------------------------
tc filter del  dev <ethx> ingress pref 1 handle 0x1 flower

Delete All filters:
------------------
tc filter del dev <ethX> ingress

Co-developed-by: Amritha Nambiar <amritha.nambiar@intel.com>
	Signed-off-by: Amritha Nambiar <amritha.nambiar@intel.com>
	Signed-off-by: Kiran Patil <kiran.patil@intel.com>
	Signed-off-by: Sudheer Mogilappagari <sudheer.mogilappagari@intel.com>
	Tested-by: Bharathi Sreenivas <bharathi.sreenivas@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 9fea749856d14c4713a2f5dee6f692aeaa2700b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice.h
#	drivers/net/ethernet/intel/ice/ice_main.c
#	drivers/net/ethernet/intel/ice/ice_tc_lib.c
#	drivers/net/ethernet/intel/ice/ice_tc_lib.h
diff --cc drivers/net/ethernet/intel/ice/ice.h
index ffdeecf46b8e,967a90efcb11..000000000000
--- a/drivers/net/ethernet/intel/ice/ice.h
+++ b/drivers/net/ethernet/intel/ice/ice.h
@@@ -38,6 -38,10 +38,13 @@@
  #include <linux/avf/virtchnl.h>
  #include <linux/cpu_rmap.h>
  #include <linux/dim.h>
++<<<<<<< HEAD
++=======
+ #include <net/pkt_cls.h>
+ #include <net/tc_act/tc_mirred.h>
+ #include <net/tc_act/tc_gact.h>
+ #include <net/ip.h>
++>>>>>>> 9fea749856d1 (ice: Add tc-flower filter support for channel)
  #include <net/devlink.h>
  #include <net/ipv6.h>
  #include <net/xdp_sock.h>
@@@ -354,6 -395,35 +361,38 @@@ struct ice_vsi 
  
  	struct net_device **target_netdevs;
  
++<<<<<<< HEAD
++=======
+ 	struct tc_mqprio_qopt_offload mqprio_qopt; /* queue parameters */
+ 
+ 	/* Channel Specific Fields */
+ 	struct ice_vsi *tc_map_vsi[ICE_CHNL_MAX_TC];
+ 	u16 cnt_q_avail;
+ 	u16 next_base_q;	/* next queue to be used for channel setup */
+ 	struct list_head ch_list;
+ 	u16 num_chnl_rxq;
+ 	u16 num_chnl_txq;
+ 	u16 ch_rss_size;
+ 	u16 num_chnl_fltr;
+ 	/* store away rss size info before configuring ADQ channels so that,
+ 	 * it can be used after tc-qdisc delete, to get back RSS setting as
+ 	 * they were before
+ 	 */
+ 	u16 orig_rss_size;
+ 	/* this keeps tracks of all enabled TC with and without DCB
+ 	 * and inclusive of ADQ, vsi->mqprio_opt keeps track of queue
+ 	 * information
+ 	 */
+ 	u8 all_numtc;
+ 	u16 all_enatc;
+ 
+ 	/* store away TC info, to be used for rebuild logic */
+ 	u8 old_numtc;
+ 	u16 old_ena_tc;
+ 
+ 	struct ice_channel *ch;
+ 
++>>>>>>> 9fea749856d1 (ice: Add tc-flower filter support for channel)
  	/* setup back reference, to which aggregator node this VSI
  	 * corresponds to
  	 */
@@@ -511,6 -585,11 +550,14 @@@ struct ice_pf 
  	struct auxiliary_device *adev;
  	int aux_idx;
  	u32 sw_int_count;
++<<<<<<< HEAD
++=======
+ 	/* count of tc_flower filters specific to channel (aka where filter
+ 	 * action is "hw_tc <tc_num>")
+ 	 */
+ 	u16 num_dmac_chnl_fltrs;
+ 	struct hlist_head tc_flower_fltr_list;
++>>>>>>> 9fea749856d1 (ice: Add tc-flower filter support for channel)
  
  	__le64 nvm_phy_type_lo; /* NVM PHY type low */
  	__le64 nvm_phy_type_hi; /* NVM PHY type high */
diff --cc drivers/net/ethernet/intel/ice/ice_main.c
index 15460d97b024,cb82abd08a40..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_main.c
+++ b/drivers/net/ethernet/intel/ice/ice_main.c
@@@ -5405,6 -5512,18 +5411,21 @@@ ice_set_features(struct net_device *net
  		ice_clear_arfs(vsi);
  	}
  
++<<<<<<< HEAD
++=======
+ 	/* don't turn off hw_tc_offload when ADQ is already enabled */
+ 	if (!(features & NETIF_F_HW_TC) && ice_is_adq_active(pf)) {
+ 		dev_err(ice_pf_to_dev(pf), "ADQ is active, can't turn hw_tc_offload off\n");
+ 		return -EACCES;
+ 	}
+ 
+ 	if ((features & NETIF_F_HW_TC) &&
+ 	    !(netdev->features & NETIF_F_HW_TC))
+ 		set_bit(ICE_FLAG_CLS_FLOWER, pf->flags);
+ 	else
+ 		clear_bit(ICE_FLAG_CLS_FLOWER, pf->flags);
+ 
++>>>>>>> 9fea749856d1 (ice: Add tc-flower filter support for channel)
  	return ret;
  }
  
@@@ -7086,6 -7221,935 +7107,938 @@@ static void ice_tx_timeout(struct net_d
  }
  
  /**
++<<<<<<< HEAD
++=======
+  * ice_setup_tc_cls_flower - flower classifier offloads
+  * @np: net device to configure
+  * @filter_dev: device on which filter is added
+  * @cls_flower: offload data
+  */
+ static int
+ ice_setup_tc_cls_flower(struct ice_netdev_priv *np,
+ 			struct net_device *filter_dev,
+ 			struct flow_cls_offload *cls_flower)
+ {
+ 	struct ice_vsi *vsi = np->vsi;
+ 
+ 	if (cls_flower->common.chain_index)
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (cls_flower->command) {
+ 	case FLOW_CLS_REPLACE:
+ 		return ice_add_cls_flower(filter_dev, vsi, cls_flower);
+ 	case FLOW_CLS_DESTROY:
+ 		return ice_del_cls_flower(vsi, cls_flower);
+ 	default:
+ 		return -EINVAL;
+ 	}
+ }
+ 
+ /**
+  * ice_setup_tc_block_cb - callback handler registered for TC block
+  * @type: TC SETUP type
+  * @type_data: TC flower offload data that contains user input
+  * @cb_priv: netdev private data
+  */
+ static int
+ ice_setup_tc_block_cb(enum tc_setup_type type, void *type_data, void *cb_priv)
+ {
+ 	struct ice_netdev_priv *np = cb_priv;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_CLSFLOWER:
+ 		return ice_setup_tc_cls_flower(np, np->vsi->netdev,
+ 					       type_data);
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ }
+ 
+ /**
+  * ice_validate_mqprio_qopt - Validate TCF input parameters
+  * @vsi: Pointer to VSI
+  * @mqprio_qopt: input parameters for mqprio queue configuration
+  *
+  * This function validates MQPRIO params, such as qcount (power of 2 wherever
+  * needed), and make sure user doesn't specify qcount and BW rate limit
+  * for TCs, which are more than "num_tc"
+  */
+ static int
+ ice_validate_mqprio_qopt(struct ice_vsi *vsi,
+ 			 struct tc_mqprio_qopt_offload *mqprio_qopt)
+ {
+ 	u64 sum_max_rate = 0, sum_min_rate = 0;
+ 	int non_power_of_2_qcount = 0;
+ 	struct ice_pf *pf = vsi->back;
+ 	int max_rss_q_cnt = 0;
+ 	struct device *dev;
+ 	int i, speed;
+ 	u8 num_tc;
+ 
+ 	if (vsi->type != ICE_VSI_PF)
+ 		return -EINVAL;
+ 
+ 	if (mqprio_qopt->qopt.offset[0] != 0 ||
+ 	    mqprio_qopt->qopt.num_tc < 1 ||
+ 	    mqprio_qopt->qopt.num_tc > ICE_CHNL_MAX_TC)
+ 		return -EINVAL;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	vsi->ch_rss_size = 0;
+ 	num_tc = mqprio_qopt->qopt.num_tc;
+ 
+ 	for (i = 0; num_tc; i++) {
+ 		int qcount = mqprio_qopt->qopt.count[i];
+ 		u64 max_rate, min_rate, rem;
+ 
+ 		if (!qcount)
+ 			return -EINVAL;
+ 
+ 		if (is_power_of_2(qcount)) {
+ 			if (non_power_of_2_qcount &&
+ 			    qcount > non_power_of_2_qcount) {
+ 				dev_err(dev, "qcount[%d] cannot be greater than non power of 2 qcount[%d]\n",
+ 					qcount, non_power_of_2_qcount);
+ 				return -EINVAL;
+ 			}
+ 			if (qcount > max_rss_q_cnt)
+ 				max_rss_q_cnt = qcount;
+ 		} else {
+ 			if (non_power_of_2_qcount &&
+ 			    qcount != non_power_of_2_qcount) {
+ 				dev_err(dev, "Only one non power of 2 qcount allowed[%d,%d]\n",
+ 					qcount, non_power_of_2_qcount);
+ 				return -EINVAL;
+ 			}
+ 			if (qcount < max_rss_q_cnt) {
+ 				dev_err(dev, "non power of 2 qcount[%d] cannot be less than other qcount[%d]\n",
+ 					qcount, max_rss_q_cnt);
+ 				return -EINVAL;
+ 			}
+ 			max_rss_q_cnt = qcount;
+ 			non_power_of_2_qcount = qcount;
+ 		}
+ 
+ 		/* TC command takes input in K/N/Gbps or K/M/Gbit etc but
+ 		 * converts the bandwidth rate limit into Bytes/s when
+ 		 * passing it down to the driver. So convert input bandwidth
+ 		 * from Bytes/s to Kbps
+ 		 */
+ 		max_rate = mqprio_qopt->max_rate[i];
+ 		max_rate = div_u64(max_rate, ICE_BW_KBPS_DIVISOR);
+ 		sum_max_rate += max_rate;
+ 
+ 		/* min_rate is minimum guaranteed rate and it can't be zero */
+ 		min_rate = mqprio_qopt->min_rate[i];
+ 		min_rate = div_u64(min_rate, ICE_BW_KBPS_DIVISOR);
+ 		sum_min_rate += min_rate;
+ 
+ 		if (min_rate && min_rate < ICE_MIN_BW_LIMIT) {
+ 			dev_err(dev, "TC%d: min_rate(%llu Kbps) < %u Kbps\n", i,
+ 				min_rate, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		iter_div_u64_rem(min_rate, ICE_MIN_BW_LIMIT, &rem);
+ 		if (rem) {
+ 			dev_err(dev, "TC%d: Min Rate not multiple of %u Kbps",
+ 				i, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		iter_div_u64_rem(max_rate, ICE_MIN_BW_LIMIT, &rem);
+ 		if (rem) {
+ 			dev_err(dev, "TC%d: Max Rate not multiple of %u Kbps",
+ 				i, ICE_MIN_BW_LIMIT);
+ 			return -EINVAL;
+ 		}
+ 
+ 		/* min_rate can't be more than max_rate, except when max_rate
+ 		 * is zero (implies max_rate sought is max line rate). In such
+ 		 * a case min_rate can be more than max.
+ 		 */
+ 		if (max_rate && min_rate > max_rate) {
+ 			dev_err(dev, "min_rate %llu Kbps can't be more than max_rate %llu Kbps\n",
+ 				min_rate, max_rate);
+ 			return -EINVAL;
+ 		}
+ 
+ 		if (i >= mqprio_qopt->qopt.num_tc - 1)
+ 			break;
+ 		if (mqprio_qopt->qopt.offset[i + 1] !=
+ 		    (mqprio_qopt->qopt.offset[i] + qcount))
+ 			return -EINVAL;
+ 	}
+ 	if (vsi->num_rxq <
+ 	    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))
+ 		return -EINVAL;
+ 	if (vsi->num_txq <
+ 	    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))
+ 		return -EINVAL;
+ 
+ 	speed = ice_get_link_speed_kbps(vsi);
+ 	if (sum_max_rate && sum_max_rate > (u64)speed) {
+ 		dev_err(dev, "Invalid max Tx rate(%llu) Kbps > speed(%u) Kbps specified\n",
+ 			sum_max_rate, speed);
+ 		return -EINVAL;
+ 	}
+ 	if (sum_min_rate && sum_min_rate > (u64)speed) {
+ 		dev_err(dev, "Invalid min Tx rate(%llu) Kbps > speed (%u) Kbps specified\n",
+ 			sum_min_rate, speed);
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* make sure vsi->ch_rss_size is set correctly based on TC's qcount */
+ 	vsi->ch_rss_size = max_rss_q_cnt;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_add_channel - add a channel by adding VSI
+  * @pf: ptr to PF device
+  * @sw_id: underlying HW switching element ID
+  * @ch: ptr to channel structure
+  *
+  * Add a channel (VSI) using add_vsi and queue_map
+  */
+ static int ice_add_channel(struct ice_pf *pf, u16 sw_id, struct ice_channel *ch)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vsi *vsi;
+ 
+ 	if (ch->type != ICE_VSI_CHNL) {
+ 		dev_err(dev, "add new VSI failed, ch->type %d\n", ch->type);
+ 		return -EINVAL;
+ 	}
+ 
+ 	vsi = ice_chnl_vsi_setup(pf, pf->hw.port_info, ch);
+ 	if (!vsi || vsi->type != ICE_VSI_CHNL) {
+ 		dev_err(dev, "create chnl VSI failure\n");
+ 		return -EINVAL;
+ 	}
+ 
+ 	ch->sw_id = sw_id;
+ 	ch->vsi_num = vsi->vsi_num;
+ 	ch->info.mapping_flags = vsi->info.mapping_flags;
+ 	ch->ch_vsi = vsi;
+ 	/* set the back pointer of channel for newly created VSI */
+ 	vsi->ch = ch;
+ 
+ 	memcpy(&ch->info.q_mapping, &vsi->info.q_mapping,
+ 	       sizeof(vsi->info.q_mapping));
+ 	memcpy(&ch->info.tc_mapping, vsi->info.tc_mapping,
+ 	       sizeof(vsi->info.tc_mapping));
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_chnl_cfg_res
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  *
+  * Configure channel specific resources such as rings, vector.
+  */
+ static void ice_chnl_cfg_res(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < ch->num_txq; i++) {
+ 		struct ice_q_vector *tx_q_vector, *rx_q_vector;
+ 		struct ice_ring_container *rc;
+ 		struct ice_tx_ring *tx_ring;
+ 		struct ice_rx_ring *rx_ring;
+ 
+ 		tx_ring = vsi->tx_rings[ch->base_q + i];
+ 		rx_ring = vsi->rx_rings[ch->base_q + i];
+ 		if (!tx_ring || !rx_ring)
+ 			continue;
+ 
+ 		/* setup ring being channel enabled */
+ 		tx_ring->ch = ch;
+ 		rx_ring->ch = ch;
+ 
+ 		/* following code block sets up vector specific attributes */
+ 		tx_q_vector = tx_ring->q_vector;
+ 		rx_q_vector = rx_ring->q_vector;
+ 		if (!tx_q_vector && !rx_q_vector)
+ 			continue;
+ 
+ 		if (tx_q_vector) {
+ 			tx_q_vector->ch = ch;
+ 			/* setup Tx and Rx ITR setting if DIM is off */
+ 			rc = &tx_q_vector->tx;
+ 			if (!ITR_IS_DYNAMIC(rc))
+ 				ice_write_itr(rc, rc->itr_setting);
+ 		}
+ 		if (rx_q_vector) {
+ 			rx_q_vector->ch = ch;
+ 			/* setup Tx and Rx ITR setting if DIM is off */
+ 			rc = &rx_q_vector->rx;
+ 			if (!ITR_IS_DYNAMIC(rc))
+ 				ice_write_itr(rc, rc->itr_setting);
+ 		}
+ 	}
+ 
+ 	/* it is safe to assume that, if channel has non-zero num_t[r]xq, then
+ 	 * GLINT_ITR register would have written to perform in-context
+ 	 * update, hence perform flush
+ 	 */
+ 	if (ch->num_txq || ch->num_rxq)
+ 		ice_flush(&vsi->back->hw);
+ }
+ 
+ /**
+  * ice_cfg_chnl_all_res - configure channel resources
+  * @vsi: pte to main_vsi
+  * @ch: ptr to channel structure
+  *
+  * This function configures channel specific resources such as flow-director
+  * counter index, and other resources such as queues, vectors, ITR settings
+  */
+ static void
+ ice_cfg_chnl_all_res(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	/* configure channel (aka ADQ) resources such as queues, vectors,
+ 	 * ITR settings for channel specific vectors and anything else
+ 	 */
+ 	ice_chnl_cfg_res(vsi, ch);
+ }
+ 
+ /**
+  * ice_setup_hw_channel - setup new channel
+  * @pf: ptr to PF device
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  * @sw_id: underlying HW switching element ID
+  * @type: type of channel to be created (VMDq2/VF)
+  *
+  * Setup new channel (VSI) based on specified type (VMDq2/VF)
+  * and configures Tx rings accordingly
+  */
+ static int
+ ice_setup_hw_channel(struct ice_pf *pf, struct ice_vsi *vsi,
+ 		     struct ice_channel *ch, u16 sw_id, u8 type)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int ret;
+ 
+ 	ch->base_q = vsi->next_base_q;
+ 	ch->type = type;
+ 
+ 	ret = ice_add_channel(pf, sw_id, ch);
+ 	if (ret) {
+ 		dev_err(dev, "failed to add_channel using sw_id %u\n", sw_id);
+ 		return ret;
+ 	}
+ 
+ 	/* configure/setup ADQ specific resources */
+ 	ice_cfg_chnl_all_res(vsi, ch);
+ 
+ 	/* make sure to update the next_base_q so that subsequent channel's
+ 	 * (aka ADQ) VSI queue map is correct
+ 	 */
+ 	vsi->next_base_q = vsi->next_base_q + ch->num_rxq;
+ 	dev_dbg(dev, "added channel: vsi_num %u, num_rxq %u\n", ch->vsi_num,
+ 		ch->num_rxq);
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_setup_channel - setup new channel using uplink element
+  * @pf: ptr to PF device
+  * @vsi: the VSI being setup
+  * @ch: ptr to channel structure
+  *
+  * Setup new channel (VSI) based on specified type (VMDq2/VF)
+  * and uplink switching element
+  */
+ static bool
+ ice_setup_channel(struct ice_pf *pf, struct ice_vsi *vsi,
+ 		  struct ice_channel *ch)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	u16 sw_id;
+ 	int ret;
+ 
+ 	if (vsi->type != ICE_VSI_PF) {
+ 		dev_err(dev, "unsupported parent VSI type(%d)\n", vsi->type);
+ 		return false;
+ 	}
+ 
+ 	sw_id = pf->first_sw->sw_id;
+ 
+ 	/* create channel (VSI) */
+ 	ret = ice_setup_hw_channel(pf, vsi, ch, sw_id, ICE_VSI_CHNL);
+ 	if (ret) {
+ 		dev_err(dev, "failed to setup hw_channel\n");
+ 		return false;
+ 	}
+ 	dev_dbg(dev, "successfully created channel()\n");
+ 
+ 	return ch->ch_vsi ? true : false;
+ }
+ 
+ /**
+  * ice_set_bw_limit - setup BW limit for Tx traffic based on max_tx_rate
+  * @vsi: VSI to be configured
+  * @max_tx_rate: max Tx rate in Kbps to be configured as maximum BW limit
+  * @min_tx_rate: min Tx rate in Kbps to be configured as minimum BW limit
+  */
+ static int
+ ice_set_bw_limit(struct ice_vsi *vsi, u64 max_tx_rate, u64 min_tx_rate)
+ {
+ 	int err;
+ 
+ 	err = ice_set_min_bw_limit(vsi, min_tx_rate);
+ 	if (err)
+ 		return err;
+ 
+ 	return ice_set_max_bw_limit(vsi, max_tx_rate);
+ }
+ 
+ /**
+  * ice_create_q_channel - function to create channel
+  * @vsi: VSI to be configured
+  * @ch: ptr to channel (it contains channel specific params)
+  *
+  * This function creates channel (VSI) using num_queues specified by user,
+  * reconfigs RSS if needed.
+  */
+ static int ice_create_q_channel(struct ice_vsi *vsi, struct ice_channel *ch)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct device *dev;
+ 
+ 	if (!ch)
+ 		return -EINVAL;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	if (!ch->num_txq || !ch->num_rxq) {
+ 		dev_err(dev, "Invalid num_queues requested: %d\n", ch->num_rxq);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!vsi->cnt_q_avail || vsi->cnt_q_avail < ch->num_txq) {
+ 		dev_err(dev, "cnt_q_avail (%u) less than num_queues %d\n",
+ 			vsi->cnt_q_avail, ch->num_txq);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (!ice_setup_channel(pf, vsi, ch)) {
+ 		dev_info(dev, "Failed to setup channel\n");
+ 		return -EINVAL;
+ 	}
+ 	/* configure BW rate limit */
+ 	if (ch->ch_vsi && (ch->max_tx_rate || ch->min_tx_rate)) {
+ 		int ret;
+ 
+ 		ret = ice_set_bw_limit(ch->ch_vsi, ch->max_tx_rate,
+ 				       ch->min_tx_rate);
+ 		if (ret)
+ 			dev_err(dev, "failed to set Tx rate of %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->ch_vsi->vsi_num);
+ 		else
+ 			dev_dbg(dev, "set Tx rate of %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->ch_vsi->vsi_num);
+ 	}
+ 
+ 	vsi->cnt_q_avail -= ch->num_txq;
+ 
+ 	return 0;
+ }
+ 
+ /**
+  * ice_rem_all_chnl_fltrs - removes all channel filters
+  * @pf: ptr to PF, TC-flower based filter are tracked at PF level
+  *
+  * Remove all advanced switch filters only if they are channel specific
+  * tc-flower based filter
+  */
+ static void ice_rem_all_chnl_fltrs(struct ice_pf *pf)
+ {
+ 	struct ice_tc_flower_fltr *fltr;
+ 	struct hlist_node *node;
+ 
+ 	/* to remove all channel filters, iterate an ordered list of filters */
+ 	hlist_for_each_entry_safe(fltr, node,
+ 				  &pf->tc_flower_fltr_list,
+ 				  tc_flower_node) {
+ 		struct ice_rule_query_data rule;
+ 		int status;
+ 
+ 		/* for now process only channel specific filters */
+ 		if (!ice_is_chnl_fltr(fltr))
+ 			continue;
+ 
+ 		rule.rid = fltr->rid;
+ 		rule.rule_id = fltr->rule_id;
+ 		rule.vsi_handle = fltr->dest_id;
+ 		status = ice_rem_adv_rule_by_id(&pf->hw, &rule);
+ 		if (status) {
+ 			if (status == -ENOENT)
+ 				dev_dbg(ice_pf_to_dev(pf), "TC flower filter (rule_id %u) does not exist\n",
+ 					rule.rule_id);
+ 			else
+ 				dev_err(ice_pf_to_dev(pf), "failed to delete TC flower filter, status %d\n",
+ 					status);
+ 		} else if (fltr->dest_vsi) {
+ 			/* update advanced switch filter count */
+ 			if (fltr->dest_vsi->type == ICE_VSI_CHNL) {
+ 				u32 flags = fltr->flags;
+ 
+ 				fltr->dest_vsi->num_chnl_fltr--;
+ 				if (flags & (ICE_TC_FLWR_FIELD_DST_MAC |
+ 					     ICE_TC_FLWR_FIELD_ENC_DST_MAC))
+ 					pf->num_dmac_chnl_fltrs--;
+ 			}
+ 		}
+ 
+ 		hlist_del(&fltr->tc_flower_node);
+ 		kfree(fltr);
+ 	}
+ }
+ 
+ /**
+  * ice_remove_q_channels - Remove queue channels for the TCs
+  * @vsi: VSI to be configured
+  * @rem_fltr: delete advanced switch filter or not
+  *
+  * Remove queue channels for the TCs
+  */
+ static void ice_remove_q_channels(struct ice_vsi *vsi, bool rem_fltr)
+ {
+ 	struct ice_channel *ch, *ch_tmp;
+ 	struct ice_pf *pf = vsi->back;
+ 	int i;
+ 
+ 	/* remove all tc-flower based filter if they are channel filters only */
+ 	if (rem_fltr)
+ 		ice_rem_all_chnl_fltrs(pf);
+ 
+ 	/* perform cleanup for channels if they exist */
+ 	list_for_each_entry_safe(ch, ch_tmp, &vsi->ch_list, list) {
+ 		struct ice_vsi *ch_vsi;
+ 
+ 		list_del(&ch->list);
+ 		ch_vsi = ch->ch_vsi;
+ 		if (!ch_vsi) {
+ 			kfree(ch);
+ 			continue;
+ 		}
+ 
+ 		/* Reset queue contexts */
+ 		for (i = 0; i < ch->num_rxq; i++) {
+ 			struct ice_tx_ring *tx_ring;
+ 			struct ice_rx_ring *rx_ring;
+ 
+ 			tx_ring = vsi->tx_rings[ch->base_q + i];
+ 			rx_ring = vsi->rx_rings[ch->base_q + i];
+ 			if (tx_ring) {
+ 				tx_ring->ch = NULL;
+ 				if (tx_ring->q_vector)
+ 					tx_ring->q_vector->ch = NULL;
+ 			}
+ 			if (rx_ring) {
+ 				rx_ring->ch = NULL;
+ 				if (rx_ring->q_vector)
+ 					rx_ring->q_vector->ch = NULL;
+ 			}
+ 		}
+ 
+ 		/* clear the VSI from scheduler tree */
+ 		ice_rm_vsi_lan_cfg(ch->ch_vsi->port_info, ch->ch_vsi->idx);
+ 
+ 		/* Delete VSI from FW */
+ 		ice_vsi_delete(ch->ch_vsi);
+ 
+ 		/* Delete VSI from PF and HW VSI arrays */
+ 		ice_vsi_clear(ch->ch_vsi);
+ 
+ 		/* free the channel */
+ 		kfree(ch);
+ 	}
+ 
+ 	/* clear the channel VSI map which is stored in main VSI */
+ 	ice_for_each_chnl_tc(i)
+ 		vsi->tc_map_vsi[i] = NULL;
+ 
+ 	/* reset main VSI's all TC information */
+ 	vsi->all_enatc = 0;
+ 	vsi->all_numtc = 0;
+ }
+ 
+ /**
+  * ice_rebuild_channels - rebuild channel
+  * @pf: ptr to PF
+  *
+  * Recreate channel VSIs and replay filters
+  */
+ static int ice_rebuild_channels(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vsi *main_vsi;
+ 	bool rem_adv_fltr = true;
+ 	struct ice_channel *ch;
+ 	struct ice_vsi *vsi;
+ 	int tc_idx = 1;
+ 	int i, err;
+ 
+ 	main_vsi = ice_get_main_vsi(pf);
+ 	if (!main_vsi)
+ 		return 0;
+ 
+ 	if (!test_bit(ICE_FLAG_TC_MQPRIO, pf->flags) ||
+ 	    main_vsi->old_numtc == 1)
+ 		return 0; /* nothing to be done */
+ 
+ 	/* reconfigure main VSI based on old value of TC and cached values
+ 	 * for MQPRIO opts
+ 	 */
+ 	err = ice_vsi_cfg_tc(main_vsi, main_vsi->old_ena_tc);
+ 	if (err) {
+ 		dev_err(dev, "failed configuring TC(ena_tc:0x%02x) for HW VSI=%u\n",
+ 			main_vsi->old_ena_tc, main_vsi->vsi_num);
+ 		return err;
+ 	}
+ 
+ 	/* rebuild ADQ VSIs */
+ 	ice_for_each_vsi(pf, i) {
+ 		enum ice_vsi_type type;
+ 
+ 		vsi = pf->vsi[i];
+ 		if (!vsi || vsi->type != ICE_VSI_CHNL)
+ 			continue;
+ 
+ 		type = vsi->type;
+ 
+ 		/* rebuild ADQ VSI */
+ 		err = ice_vsi_rebuild(vsi, true);
+ 		if (err) {
+ 			dev_err(dev, "VSI (type:%s) at index %d rebuild failed, err %d\n",
+ 				ice_vsi_type_str(type), vsi->idx, err);
+ 			goto cleanup;
+ 		}
+ 
+ 		/* Re-map HW VSI number, using VSI handle that has been
+ 		 * previously validated in ice_replay_vsi() call above
+ 		 */
+ 		vsi->vsi_num = ice_get_hw_vsi_num(&pf->hw, vsi->idx);
+ 
+ 		/* replay filters for the VSI */
+ 		err = ice_replay_vsi(&pf->hw, vsi->idx);
+ 		if (err) {
+ 			dev_err(dev, "VSI (type:%s) replay failed, err %d, VSI index %d\n",
+ 				ice_vsi_type_str(type), err, vsi->idx);
+ 			rem_adv_fltr = false;
+ 			goto cleanup;
+ 		}
+ 		dev_info(dev, "VSI (type:%s) at index %d rebuilt successfully\n",
+ 			 ice_vsi_type_str(type), vsi->idx);
+ 
+ 		/* store ADQ VSI at correct TC index in main VSI's
+ 		 * map of TC to VSI
+ 		 */
+ 		main_vsi->tc_map_vsi[tc_idx++] = vsi;
+ 	}
+ 
+ 	/* ADQ VSI(s) has been rebuilt successfully, so setup
+ 	 * channel for main VSI's Tx and Rx rings
+ 	 */
+ 	list_for_each_entry(ch, &main_vsi->ch_list, list) {
+ 		struct ice_vsi *ch_vsi;
+ 
+ 		ch_vsi = ch->ch_vsi;
+ 		if (!ch_vsi)
+ 			continue;
+ 
+ 		/* reconfig channel resources */
+ 		ice_cfg_chnl_all_res(main_vsi, ch);
+ 
+ 		/* replay BW rate limit if it is non-zero */
+ 		if (!ch->max_tx_rate && !ch->min_tx_rate)
+ 			continue;
+ 
+ 		err = ice_set_bw_limit(ch_vsi, ch->max_tx_rate,
+ 				       ch->min_tx_rate);
+ 		if (err)
+ 			dev_err(dev, "failed (err:%d) to rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\n",
+ 				err, ch->max_tx_rate, ch->min_tx_rate,
+ 				ch_vsi->vsi_num);
+ 		else
+ 			dev_dbg(dev, "successfully rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\n",
+ 				ch->max_tx_rate, ch->min_tx_rate,
+ 				ch_vsi->vsi_num);
+ 	}
+ 
+ 	/* reconfig RSS for main VSI */
+ 	if (main_vsi->ch_rss_size)
+ 		ice_vsi_cfg_rss_lut_key(main_vsi);
+ 
+ 	return 0;
+ 
+ cleanup:
+ 	ice_remove_q_channels(main_vsi, rem_adv_fltr);
+ 	return err;
+ }
+ 
+ /**
+  * ice_create_q_channels - Add queue channel for the given TCs
+  * @vsi: VSI to be configured
+  *
+  * Configures queue channel mapping to the given TCs
+  */
+ static int ice_create_q_channels(struct ice_vsi *vsi)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	struct ice_channel *ch;
+ 	int ret = 0, i;
+ 
+ 	ice_for_each_chnl_tc(i) {
+ 		if (!(vsi->all_enatc & BIT(i)))
+ 			continue;
+ 
+ 		ch = kzalloc(sizeof(*ch), GFP_KERNEL);
+ 		if (!ch) {
+ 			ret = -ENOMEM;
+ 			goto err_free;
+ 		}
+ 		INIT_LIST_HEAD(&ch->list);
+ 		ch->num_rxq = vsi->mqprio_qopt.qopt.count[i];
+ 		ch->num_txq = vsi->mqprio_qopt.qopt.count[i];
+ 		ch->base_q = vsi->mqprio_qopt.qopt.offset[i];
+ 		ch->max_tx_rate = vsi->mqprio_qopt.max_rate[i];
+ 		ch->min_tx_rate = vsi->mqprio_qopt.min_rate[i];
+ 
+ 		/* convert to Kbits/s */
+ 		if (ch->max_tx_rate)
+ 			ch->max_tx_rate = div_u64(ch->max_tx_rate,
+ 						  ICE_BW_KBPS_DIVISOR);
+ 		if (ch->min_tx_rate)
+ 			ch->min_tx_rate = div_u64(ch->min_tx_rate,
+ 						  ICE_BW_KBPS_DIVISOR);
+ 
+ 		ret = ice_create_q_channel(vsi, ch);
+ 		if (ret) {
+ 			dev_err(ice_pf_to_dev(pf),
+ 				"failed creating channel TC:%d\n", i);
+ 			kfree(ch);
+ 			goto err_free;
+ 		}
+ 		list_add_tail(&ch->list, &vsi->ch_list);
+ 		vsi->tc_map_vsi[i] = ch->ch_vsi;
+ 		dev_dbg(ice_pf_to_dev(pf),
+ 			"successfully created channel: VSI %pK\n", ch->ch_vsi);
+ 	}
+ 	return 0;
+ 
+ err_free:
+ 	ice_remove_q_channels(vsi, false);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * ice_setup_tc_mqprio_qdisc - configure multiple traffic classes
+  * @netdev: net device to configure
+  * @type_data: TC offload data
+  */
+ static int ice_setup_tc_mqprio_qdisc(struct net_device *netdev, void *type_data)
+ {
+ 	struct tc_mqprio_qopt_offload *mqprio_qopt = type_data;
+ 	struct ice_netdev_priv *np = netdev_priv(netdev);
+ 	struct ice_vsi *vsi = np->vsi;
+ 	struct ice_pf *pf = vsi->back;
+ 	u16 mode, ena_tc_qdisc = 0;
+ 	int cur_txq, cur_rxq;
+ 	u8 hw = 0, num_tcf;
+ 	struct device *dev;
+ 	int ret, i;
+ 
+ 	dev = ice_pf_to_dev(pf);
+ 	num_tcf = mqprio_qopt->qopt.num_tc;
+ 	hw = mqprio_qopt->qopt.hw;
+ 	mode = mqprio_qopt->mode;
+ 	if (!hw) {
+ 		clear_bit(ICE_FLAG_TC_MQPRIO, pf->flags);
+ 		vsi->ch_rss_size = 0;
+ 		memcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));
+ 		goto config_tcf;
+ 	}
+ 
+ 	/* Generate queue region map for number of TCF requested */
+ 	for (i = 0; i < num_tcf; i++)
+ 		ena_tc_qdisc |= BIT(i);
+ 
+ 	switch (mode) {
+ 	case TC_MQPRIO_MODE_CHANNEL:
+ 
+ 		ret = ice_validate_mqprio_qopt(vsi, mqprio_qopt);
+ 		if (ret) {
+ 			netdev_err(netdev, "failed to validate_mqprio_qopt(), ret %d\n",
+ 				   ret);
+ 			return ret;
+ 		}
+ 		memcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));
+ 		set_bit(ICE_FLAG_TC_MQPRIO, pf->flags);
+ 		/* don't assume state of hw_tc_offload during driver load
+ 		 * and set the flag for TC flower filter if hw_tc_offload
+ 		 * already ON
+ 		 */
+ 		if (vsi->netdev->features & NETIF_F_HW_TC)
+ 			set_bit(ICE_FLAG_CLS_FLOWER, pf->flags);
+ 		break;
+ 	default:
+ 		return -EINVAL;
+ 	}
+ 
+ config_tcf:
+ 
+ 	/* Requesting same TCF configuration as already enabled */
+ 	if (ena_tc_qdisc == vsi->tc_cfg.ena_tc &&
+ 	    mode != TC_MQPRIO_MODE_CHANNEL)
+ 		return 0;
+ 
+ 	/* Pause VSI queues */
+ 	ice_dis_vsi(vsi, true);
+ 
+ 	if (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))
+ 		ice_remove_q_channels(vsi, true);
+ 
+ 	if (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {
+ 		vsi->req_txq = min_t(int, ice_get_avail_txq_count(pf),
+ 				     num_online_cpus());
+ 		vsi->req_rxq = min_t(int, ice_get_avail_rxq_count(pf),
+ 				     num_online_cpus());
+ 	} else {
+ 		/* logic to rebuild VSI, same like ethtool -L */
+ 		u16 offset = 0, qcount_tx = 0, qcount_rx = 0;
+ 
+ 		for (i = 0; i < num_tcf; i++) {
+ 			if (!(ena_tc_qdisc & BIT(i)))
+ 				continue;
+ 
+ 			offset = vsi->mqprio_qopt.qopt.offset[i];
+ 			qcount_rx = vsi->mqprio_qopt.qopt.count[i];
+ 			qcount_tx = vsi->mqprio_qopt.qopt.count[i];
+ 		}
+ 		vsi->req_txq = offset + qcount_tx;
+ 		vsi->req_rxq = offset + qcount_rx;
+ 
+ 		/* store away original rss_size info, so that it gets reused
+ 		 * form ice_vsi_rebuild during tc-qdisc delete stage - to
+ 		 * determine, what should be the rss_sizefor main VSI
+ 		 */
+ 		vsi->orig_rss_size = vsi->rss_size;
+ 	}
+ 
+ 	/* save current values of Tx and Rx queues before calling VSI rebuild
+ 	 * for fallback option
+ 	 */
+ 	cur_txq = vsi->num_txq;
+ 	cur_rxq = vsi->num_rxq;
+ 
+ 	/* proceed with rebuild main VSI using correct number of queues */
+ 	ret = ice_vsi_rebuild(vsi, false);
+ 	if (ret) {
+ 		/* fallback to current number of queues */
+ 		dev_info(dev, "Rebuild failed with new queues, try with current number of queues\n");
+ 		vsi->req_txq = cur_txq;
+ 		vsi->req_rxq = cur_rxq;
+ 		clear_bit(ICE_RESET_FAILED, pf->state);
+ 		if (ice_vsi_rebuild(vsi, false)) {
+ 			dev_err(dev, "Rebuild of main VSI failed again\n");
+ 			return ret;
+ 		}
+ 	}
+ 
+ 	vsi->all_numtc = num_tcf;
+ 	vsi->all_enatc = ena_tc_qdisc;
+ 	ret = ice_vsi_cfg_tc(vsi, ena_tc_qdisc);
+ 	if (ret) {
+ 		netdev_err(netdev, "failed configuring TC for VSI id=%d\n",
+ 			   vsi->vsi_num);
+ 		goto exit;
+ 	}
+ 
+ 	if (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {
+ 		u64 max_tx_rate = vsi->mqprio_qopt.max_rate[0];
+ 		u64 min_tx_rate = vsi->mqprio_qopt.min_rate[0];
+ 
+ 		/* set TC0 rate limit if specified */
+ 		if (max_tx_rate || min_tx_rate) {
+ 			/* convert to Kbits/s */
+ 			if (max_tx_rate)
+ 				max_tx_rate = div_u64(max_tx_rate, ICE_BW_KBPS_DIVISOR);
+ 			if (min_tx_rate)
+ 				min_tx_rate = div_u64(min_tx_rate, ICE_BW_KBPS_DIVISOR);
+ 
+ 			ret = ice_set_bw_limit(vsi, max_tx_rate, min_tx_rate);
+ 			if (!ret) {
+ 				dev_dbg(dev, "set Tx rate max %llu min %llu for VSI(%u)\n",
+ 					max_tx_rate, min_tx_rate, vsi->vsi_num);
+ 			} else {
+ 				dev_err(dev, "failed to set Tx rate max %llu min %llu for VSI(%u)\n",
+ 					max_tx_rate, min_tx_rate, vsi->vsi_num);
+ 				goto exit;
+ 			}
+ 		}
+ 		ret = ice_create_q_channels(vsi);
+ 		if (ret) {
+ 			netdev_err(netdev, "failed configuring queue channels\n");
+ 			goto exit;
+ 		} else {
+ 			netdev_dbg(netdev, "successfully configured channels\n");
+ 		}
+ 	}
+ 
+ 	if (vsi->ch_rss_size)
+ 		ice_vsi_cfg_rss_lut_key(vsi);
+ 
+ exit:
+ 	/* if error, reset the all_numtc and all_enatc */
+ 	if (ret) {
+ 		vsi->all_numtc = 0;
+ 		vsi->all_enatc = 0;
+ 	}
+ 	/* resume VSI */
+ 	ice_ena_vsi(vsi, true);
+ 
+ 	return ret;
+ }
+ 
+ static LIST_HEAD(ice_block_cb_list);
+ 
+ static int
+ ice_setup_tc(struct net_device *netdev, enum tc_setup_type type,
+ 	     void *type_data)
+ {
+ 	struct ice_netdev_priv *np = netdev_priv(netdev);
+ 	struct ice_pf *pf = np->vsi->back;
+ 	int err;
+ 
+ 	switch (type) {
+ 	case TC_SETUP_BLOCK:
+ 		return flow_block_cb_setup_simple(type_data,
+ 						  &ice_block_cb_list,
+ 						  ice_setup_tc_block_cb,
+ 						  np, np, true);
+ 	case TC_SETUP_QDISC_MQPRIO:
+ 		/* setup traffic classifier for receive side */
+ 		mutex_lock(&pf->tc_mutex);
+ 		err = ice_setup_tc_mqprio_qdisc(netdev, type_data);
+ 		mutex_unlock(&pf->tc_mutex);
+ 		return err;
+ 	default:
+ 		return -EOPNOTSUPP;
+ 	}
+ 	return -EOPNOTSUPP;
+ }
+ 
+ /**
++>>>>>>> 9fea749856d1 (ice: Add tc-flower filter support for channel)
   * ice_open - Called when a network interface becomes active
   * @netdev: network interface device structure
   *
* Unmerged path drivers/net/ethernet/intel/ice/ice_tc_lib.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_tc_lib.h
* Unmerged path drivers/net/ethernet/intel/ice/ice.h
* Unmerged path drivers/net/ethernet/intel/ice/ice_main.c
diff --git a/drivers/net/ethernet/intel/ice/ice_switch.c b/drivers/net/ethernet/intel/ice/ice_switch.c
index 0d07547b40f1..4f91b6bf15b9 100644
--- a/drivers/net/ethernet/intel/ice/ice_switch.c
+++ b/drivers/net/ethernet/intel/ice/ice_switch.c
@@ -2272,6 +2272,125 @@ ice_remove_rule_internal(struct ice_hw *hw, u8 recp_id,
 	return status;
 }
 
+/**
+ * ice_mac_fltr_exist - does this MAC filter exist for given VSI
+ * @hw: pointer to the hardware structure
+ * @mac: MAC address to be checked (for MAC filter)
+ * @vsi_handle: check MAC filter for this VSI
+ */
+bool ice_mac_fltr_exist(struct ice_hw *hw, u8 *mac, u16 vsi_handle)
+{
+	struct ice_fltr_mgmt_list_entry *entry;
+	struct list_head *rule_head;
+	struct ice_switch_info *sw;
+	struct mutex *rule_lock; /* Lock to protect filter rule list */
+	u16 hw_vsi_id;
+
+	if (!ice_is_vsi_valid(hw, vsi_handle))
+		return false;
+
+	hw_vsi_id = ice_get_hw_vsi_num(hw, vsi_handle);
+	sw = hw->switch_info;
+	rule_head = &sw->recp_list[ICE_SW_LKUP_MAC].filt_rules;
+	if (!rule_head)
+		return false;
+
+	rule_lock = &sw->recp_list[ICE_SW_LKUP_MAC].filt_rule_lock;
+	mutex_lock(rule_lock);
+	list_for_each_entry(entry, rule_head, list_entry) {
+		struct ice_fltr_info *f_info = &entry->fltr_info;
+		u8 *mac_addr = &f_info->l_data.mac.mac_addr[0];
+
+		if (is_zero_ether_addr(mac_addr))
+			continue;
+
+		if (f_info->flag != ICE_FLTR_TX ||
+		    f_info->src_id != ICE_SRC_ID_VSI ||
+		    f_info->lkup_type != ICE_SW_LKUP_MAC ||
+		    f_info->fltr_act != ICE_FWD_TO_VSI ||
+		    hw_vsi_id != f_info->fwd_id.hw_vsi_id)
+			continue;
+
+		if (ether_addr_equal(mac, mac_addr)) {
+			mutex_unlock(rule_lock);
+			return true;
+		}
+	}
+	mutex_unlock(rule_lock);
+	return false;
+}
+
+/**
+ * ice_vlan_fltr_exist - does this VLAN filter exist for given VSI
+ * @hw: pointer to the hardware structure
+ * @vlan_id: VLAN ID
+ * @vsi_handle: check MAC filter for this VSI
+ */
+bool ice_vlan_fltr_exist(struct ice_hw *hw, u16 vlan_id, u16 vsi_handle)
+{
+	struct ice_fltr_mgmt_list_entry *entry;
+	struct list_head *rule_head;
+	struct ice_switch_info *sw;
+	struct mutex *rule_lock; /* Lock to protect filter rule list */
+	u16 hw_vsi_id;
+
+	if (vlan_id > ICE_MAX_VLAN_ID)
+		return false;
+
+	if (!ice_is_vsi_valid(hw, vsi_handle))
+		return false;
+
+	hw_vsi_id = ice_get_hw_vsi_num(hw, vsi_handle);
+	sw = hw->switch_info;
+	rule_head = &sw->recp_list[ICE_SW_LKUP_VLAN].filt_rules;
+	if (!rule_head)
+		return false;
+
+	rule_lock = &sw->recp_list[ICE_SW_LKUP_VLAN].filt_rule_lock;
+	mutex_lock(rule_lock);
+	list_for_each_entry(entry, rule_head, list_entry) {
+		struct ice_fltr_info *f_info = &entry->fltr_info;
+		u16 entry_vlan_id = f_info->l_data.vlan.vlan_id;
+		struct ice_vsi_list_map_info *map_info;
+
+		if (entry_vlan_id > ICE_MAX_VLAN_ID)
+			continue;
+
+		if (f_info->flag != ICE_FLTR_TX ||
+		    f_info->src_id != ICE_SRC_ID_VSI ||
+		    f_info->lkup_type != ICE_SW_LKUP_VLAN)
+			continue;
+
+		/* Only allowed filter action are FWD_TO_VSI/_VSI_LIST */
+		if (f_info->fltr_act != ICE_FWD_TO_VSI &&
+		    f_info->fltr_act != ICE_FWD_TO_VSI_LIST)
+			continue;
+
+		if (f_info->fltr_act == ICE_FWD_TO_VSI) {
+			if (hw_vsi_id != f_info->fwd_id.hw_vsi_id)
+				continue;
+		} else if (f_info->fltr_act == ICE_FWD_TO_VSI_LIST) {
+			/* If filter_action is FWD_TO_VSI_LIST, make sure
+			 * that VSI being checked is part of VSI list
+			 */
+			if (entry->vsi_count == 1 &&
+			    entry->vsi_list_info) {
+				map_info = entry->vsi_list_info;
+				if (!test_bit(vsi_handle, map_info->vsi_map))
+					continue;
+			}
+		}
+
+		if (vlan_id == entry_vlan_id) {
+			mutex_unlock(rule_lock);
+			return true;
+		}
+	}
+	mutex_unlock(rule_lock);
+
+	return false;
+}
+
 /**
  * ice_add_mac - Add a MAC address based filter rule
  * @hw: pointer to the hardware structure
diff --git a/drivers/net/ethernet/intel/ice/ice_switch.h b/drivers/net/ethernet/intel/ice/ice_switch.h
index 34b7f74b1ab8..7f690fe3cdcb 100644
--- a/drivers/net/ethernet/intel/ice/ice_switch.h
+++ b/drivers/net/ethernet/intel/ice/ice_switch.h
@@ -324,6 +324,8 @@ enum ice_status
 ice_remove_eth_mac(struct ice_hw *hw, struct list_head *em_list);
 int
 ice_cfg_rdma_fltr(struct ice_hw *hw, u16 vsi_handle, bool enable);
+bool ice_mac_fltr_exist(struct ice_hw *hw, u8 *mac, u16 vsi_handle);
+bool ice_vlan_fltr_exist(struct ice_hw *hw, u16 vlan_id, u16 vsi_handle);
 void ice_remove_vsi_fltr(struct ice_hw *hw, u16 vsi_handle);
 enum ice_status
 ice_add_vlan(struct ice_hw *hw, struct list_head *m_list);
* Unmerged path drivers/net/ethernet/intel/ice/ice_tc_lib.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_tc_lib.h
