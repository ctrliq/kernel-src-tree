gfs2: Update to the evict / remote delete documentation

jira LE-3201
Rebuild_History Non-Buildable kernel-rt-4.18.0-553.27.1.rt7.368.el8_10
commit-author Andreas Gruenbacher <agruenba@redhat.com>
commit a6033333ccce01ecada39b3ddabc03fd967e60c0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-553.27.1.rt7.368.el8_10/a6033333.failed

Try to be a bit more clear and remove some duplications.  We cannot
actually get rid of the verification step eventually, so remove the
comment saying so.

	Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
(cherry picked from commit a6033333ccce01ecada39b3ddabc03fd967e60c0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/gfs2/glock.c
diff --cc fs/gfs2/glock.c
index 7369f9303901,8fff36846145..000000000000
--- a/fs/gfs2/glock.c
+++ b/fs/gfs2/glock.c
@@@ -992,37 -1028,15 +995,42 @@@ static void delete_work_func(struct wor
  	struct delayed_work *dwork = to_delayed_work(work);
  	struct gfs2_glock *gl = container_of(dwork, struct gfs2_glock, gl_delete);
  	struct gfs2_sbd *sdp = gl->gl_name.ln_sbd;
 -	bool verify_delete = test_and_clear_bit(GLF_VERIFY_DELETE, &gl->gl_flags);
 +	struct inode *inode;
 +	u64 no_addr = gl->gl_name.ln_number;
  
++<<<<<<< HEAD
 +	if (test_and_clear_bit(GLF_TRY_TO_EVICT, &gl->gl_flags)) {
 +		/*
 +		 * If we can evict the inode, give the remote node trying to
 +		 * delete the inode some time before verifying that the delete
 +		 * has happened.  Otherwise, if we cause contention on the inode glock
 +		 * immediately, the remote node will think that we still have
 +		 * the inode in use, and so it will give up waiting.
 +		 *
 +		 * If we can't evict the inode, signal to the remote node that
 +		 * the inode is still in use.  We'll later try to delete the
 +		 * inode locally in gfs2_evict_inode.
 +		 *
 +		 * FIXME: We only need to verify that the remote node has
 +		 * deleted the inode because nodes before this remote delete
 +		 * rework won't cooperate.  At a later time, when we no longer
 +		 * care about compatibility with such nodes, we can skip this
 +		 * step entirely.
 +		 */
 +		if (gfs2_try_evict(gl)) {
 +			if (test_bit(SDF_KILL, &sdp->sd_flags))
 +				goto out;
 +			if (gfs2_queue_verify_delete(gl))
 +				return;
 +		}
 +		goto out;
 +	}
++=======
+ 	if (test_and_clear_bit(GLF_TRY_TO_EVICT, &gl->gl_flags))
+ 		gfs2_try_evict(gl);
++>>>>>>> a6033333ccce (gfs2: Update to the evict / remote delete documentation)
  
 -	if (verify_delete) {
 -		u64 no_addr = gl->gl_name.ln_number;
 -		struct inode *inode;
 -
 +	if (test_and_clear_bit(GLF_VERIFY_DELETE, &gl->gl_flags)) {
  		inode = gfs2_lookup_by_inum(sdp, no_addr, gl->gl_no_formal_ino,
  					    GFS2_BLKST_UNLINKED);
  		if (IS_ERR(inode)) {
* Unmerged path fs/gfs2/glock.c
diff --git a/fs/gfs2/super.c b/fs/gfs2/super.c
index 0857964a90b5..bd7f7213789d 100644
--- a/fs/gfs2/super.c
+++ b/fs/gfs2/super.c
@@ -1301,9 +1301,9 @@ static bool gfs2_upgrade_iopen_glock(struct inode *inode)
 	 * exclusive access to the iopen glock here.
 	 *
 	 * Otherwise, the other nodes holding the lock will be notified about
-	 * our locking request.  If they do not have the inode open, they are
-	 * expected to evict the cached inode and release the lock, allowing us
-	 * to proceed.
+	 * our locking request (see iopen_go_callback()).  If they do not have
+	 * the inode open, they are expected to evict the cached inode and
+	 * release the lock, allowing us to proceed.
 	 *
 	 * Otherwise, if they cannot evict the inode, they are expected to poke
 	 * the inode glock (note: not the iopen glock).  We will notice that
