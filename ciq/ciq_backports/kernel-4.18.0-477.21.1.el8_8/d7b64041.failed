iomap: write iomap validity checks

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-477.21.1.el8_8
commit-author Dave Chinner <dchinner@redhat.com>
commit d7b64041164ca177170191d2ad775da074ab2926
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-477.21.1.el8_8/d7b64041.failed

A recent multithreaded write data corruption has been uncovered in
the iomap write code. The core of the problem is partial folio
writes can be flushed to disk while a new racing write can map it
and fill the rest of the page:

writeback			new write

allocate blocks
  blocks are unwritten
submit IO
.....
				map blocks
				iomap indicates UNWRITTEN range
				loop {
				  lock folio
				  copyin data
.....
IO completes
  runs unwritten extent conv
    blocks are marked written
				  <iomap now stale>
				  get next folio
				}

Now add memory pressure such that memory reclaim evicts the
partially written folio that has already been written to disk.

When the new write finally gets to the last partial page of the new
write, it does not find it in cache, so it instantiates a new page,
sees the iomap is unwritten, and zeros the part of the page that
it does not have data from. This overwrites the data on disk that
was originally written.

The full description of the corruption mechanism can be found here:

https://lore.kernel.org/linux-xfs/20220817093627.GZ3600936@dread.disaster.area/

To solve this problem, we need to check whether the iomap is still
valid after we lock each folio during the write. We have to do it
after we lock the page so that we don't end up with state changes
occurring while we wait for the folio to be locked.

Hence we need a mechanism to be able to check that the cached iomap
is still valid (similar to what we already do in buffered
writeback), and we need a way for ->begin_write to back out and
tell the high level iomap iterator that we need to remap the
remaining write range.

The iomap needs to grow some storage for the validity cookie that
the filesystem provides to travel with the iomap. XFS, in
particular, also needs to know some more information about what the
iomap maps (attribute extents rather than file data extents) to for
the validity cookie to cover all the types of iomaps we might need
to validate.

	Signed-off-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Darrick J. Wong <djwong@kernel.org>
(cherry picked from commit d7b64041164ca177170191d2ad775da074ab2926)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/iomap/buffered-io.c
diff --cc fs/iomap/buffered-io.c
index a9fecbda715a,356193e44cf0..000000000000
--- a/fs/iomap/buffered-io.c
+++ b/fs/iomap/buffered-io.c
@@@ -622,43 -575,29 +622,48 @@@ __iomap_write_begin(struct inode *inode
  	return 0;
  }
  
 -static int iomap_write_begin_inline(const struct iomap_iter *iter,
 -		struct folio *folio)
 +static void __iomap_put_folio(struct iomap *iomap, struct inode *inode,
 +			      loff_t pos, size_t ret, struct page *page)
  {
 -	/* needs more work for the tailpacking case; disable for now */
 -	if (WARN_ON_ONCE(iomap_iter_srcmap(iter)->offset != 0))
 -		return -EIO;
 -	return iomap_read_inline_data(iter, folio);
 +	const struct iomap_page_ops *page_ops = iomap->page_ops;
 +
 +	if (page_ops && page_ops->page_done)
 +		page_ops->page_done(inode, pos, ret, page);
 +	else if (page) {
 +		unlock_page(page);
 +		put_page(page);
 +	}
  }
  
++<<<<<<< HEAD
 +static int iomap_write_begin_inline(struct inode *inode,
 +		struct page *page, struct iomap *srcmap)
++=======
+ static int iomap_write_begin(struct iomap_iter *iter, loff_t pos,
+ 		size_t len, struct folio **foliop)
++>>>>>>> d7b64041164c (iomap: write iomap validity checks)
  {
 -	const struct iomap_page_ops *page_ops = iter->iomap.page_ops;
 -	const struct iomap *srcmap = iomap_iter_srcmap(iter);
 -	struct folio *folio;
 -	unsigned fgp = FGP_LOCK | FGP_WRITE | FGP_CREAT | FGP_STABLE | FGP_NOFS;
 -	int status = 0;
 +	int ret;
 +
 +	/* needs more work for the tailpacking case; disable for now */
 +	if (WARN_ON_ONCE(srcmap->offset != 0))
 +		return -EIO;
 +	ret = iomap_read_inline_data(inode, page, srcmap);
 +	if (ret < 0)
 +		return ret;
 +	return 0;
 +}
  
 -	if (iter->flags & IOMAP_NOWAIT)
 -		fgp |= FGP_NOWAIT;
 +static int
 +iomap_write_begin(struct inode *inode, loff_t pos, unsigned len, unsigned flags,
 +		struct page **pagep, struct iomap *iomap, struct iomap *srcmap)
 +{
 +	const struct iomap_page_ops *page_ops = iomap->page_ops;
 +	struct page *page;
 +	int status = 0;
  
 -	BUG_ON(pos + len > iter->iomap.offset + iter->iomap.length);
 -	if (srcmap != &iter->iomap)
 +	BUG_ON(pos + len > iomap->offset + iomap->length);
 +	if (srcmap != iomap)
  		BUG_ON(pos + len > srcmap->offset + srcmap->length);
  
  	if (fatal_signal_pending(current))
@@@ -670,21 -612,42 +675,47 @@@
  			return status;
  	}
  
 -	folio = __filemap_get_folio(iter->inode->i_mapping, pos >> PAGE_SHIFT,
 -			fgp, mapping_gfp_mask(iter->inode->i_mapping));
 -	if (!folio) {
 -		status = (iter->flags & IOMAP_NOWAIT) ? -EAGAIN : -ENOMEM;
 -		goto out_no_page;
 +	page = grab_cache_page_write_begin(inode->i_mapping, pos >> PAGE_SHIFT,
 +			AOP_FLAG_NOFS);
 +	if (!page) {
 +		status = -ENOMEM;
 +		__iomap_put_folio(iomap, inode, pos, 0, NULL);
 +		return status;
  	}
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Now we have a locked folio, before we do anything with it we need to
+ 	 * check that the iomap we have cached is not stale. The inode extent
+ 	 * mapping can change due to concurrent IO in flight (e.g.
+ 	 * IOMAP_UNWRITTEN state can change and memory reclaim could have
+ 	 * reclaimed a previously partially written page at this index after IO
+ 	 * completion before this write reaches this file offset) and hence we
+ 	 * could do the wrong thing here (zero a page range incorrectly or fail
+ 	 * to zero) and corrupt data.
+ 	 */
+ 	if (page_ops && page_ops->iomap_valid) {
+ 		bool iomap_valid = page_ops->iomap_valid(iter->inode,
+ 							&iter->iomap);
+ 		if (!iomap_valid) {
+ 			iter->iomap.flags |= IOMAP_F_STALE;
+ 			status = 0;
+ 			goto out_unlock;
+ 		}
+ 	}
+ 
+ 	if (pos + len > folio_pos(folio) + folio_size(folio))
+ 		len = folio_pos(folio) + folio_size(folio) - pos;
++>>>>>>> d7b64041164c (iomap: write iomap validity checks)
  
  	if (srcmap->type == IOMAP_INLINE)
 -		status = iomap_write_begin_inline(iter, folio);
 -	else if (srcmap->flags & IOMAP_F_BUFFER_HEAD)
 -		status = __block_write_begin_int(folio, pos, len, NULL, srcmap);
 +		status = iomap_write_begin_inline(inode, page, srcmap);
 +	else if (iomap->flags & IOMAP_F_BUFFER_HEAD)
 +		status = __block_write_begin_int(page, pos, len, NULL, srcmap);
  	else
 -		status = __iomap_write_begin(iter, pos, len, folio);
 +		status = __iomap_write_begin(inode, pos, len, flags, page,
 +				srcmap);
  
  	if (unlikely(status))
  		goto out_unlock;
@@@ -833,21 -791,24 +864,23 @@@ again
  			break;
  		}
  
 -		status = iomap_write_begin(iter, pos, bytes, &folio);
 +		status = iomap_write_begin(iter->inode, pos, bytes, 0, &page,
 +					   iomap, srcmap);
  		if (unlikely(status))
  			break;
+ 		if (iter->iomap.flags & IOMAP_F_STALE)
+ 			break;
  
 -		page = folio_file_page(folio, pos >> PAGE_SHIFT);
 -		if (mapping_writably_mapped(mapping))
 +		if (mapping_writably_mapped(iter->inode->i_mapping))
  			flush_dcache_page(page);
  
 -		copied = copy_page_from_iter_atomic(page, offset, bytes, i);
 -
 -		status = iomap_write_end(iter, pos, bytes, copied, folio);
 +		copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);
  
 -		if (unlikely(copied != status))
 -			iov_iter_revert(i, copied - status);
 +		status = iomap_write_end(iter->inode, pos, bytes, copied, page,
 +					 iomap, srcmap);
  
  		cond_resched();
 +
  		if (unlikely(status == 0)) {
  			/*
  			 * A short copy made iomap_write_end() reject the
@@@ -910,15 -1099,15 +943,17 @@@ static loff_t iomap_unshare_iter(struc
  	do {
  		unsigned long offset = offset_in_page(pos);
  		unsigned long bytes = min_t(loff_t, PAGE_SIZE - offset, length);
 -		struct folio *folio;
 +		struct page *page;
  
 -		status = iomap_write_begin(iter, pos, bytes, &folio);
 +		status = iomap_write_begin(iter->inode, pos, bytes,
 +				IOMAP_WRITE_F_UNSHARE, &page, iomap, srcmap);
  		if (unlikely(status))
  			return status;
+ 		if (iter->iomap.flags & IOMAP_F_STALE)
+ 			break;
  
 -		status = iomap_write_end(iter, pos, bytes, bytes, folio);
 +		status = iomap_write_end(iter->inode, pos, bytes, bytes, page, iomap,
 +				srcmap);
  		if (WARN_ON_ONCE(status == 0))
  			return -EIO;
  
@@@ -983,15 -1153,27 +1018,34 @@@ static loff_t iomap_zero_iter(struct io
  		return length;
  
  	do {
 -		struct folio *folio;
 -		int status;
 -		size_t offset;
 -		size_t bytes = min_t(u64, SIZE_MAX, length);
 +		s64 bytes;
  
++<<<<<<< HEAD
 +		if (IS_DAX(iter->inode))
 +			bytes = dax_iomap_zero(pos, length, iomap);
 +		else
 +			bytes = iomap_zero(iter->inode, pos, length, iomap,
 +					   srcmap);
 +		if (bytes < 0)
 +			return bytes;
++=======
+ 		status = iomap_write_begin(iter, pos, bytes, &folio);
+ 		if (status)
+ 			return status;
+ 		if (iter->iomap.flags & IOMAP_F_STALE)
+ 			break;
+ 
+ 		offset = offset_in_folio(folio, pos);
+ 		if (bytes > folio_size(folio) - offset)
+ 			bytes = folio_size(folio) - offset;
+ 
+ 		folio_zero_range(folio, offset, bytes);
+ 		folio_mark_accessed(folio);
+ 
+ 		bytes = iomap_write_end(iter, pos, bytes, bytes, folio);
+ 		if (WARN_ON_ONCE(bytes == 0))
+ 			return -EIO;
++>>>>>>> d7b64041164c (iomap: write iomap validity checks)
  
  		pos += bytes;
  		length -= bytes;
diff --git a/fs/iomap/apply.c b/fs/iomap/apply.c
index a1c7592d2ade..79a0614eaab7 100644
--- a/fs/iomap/apply.c
+++ b/fs/iomap/apply.c
@@ -7,12 +7,28 @@
 #include <linux/iomap.h>
 #include "trace.h"
 
+/*
+ * Advance to the next range we need to map.
+ *
+ * If the iomap is marked IOMAP_F_STALE, it means the existing map was not fully
+ * processed - it was aborted because the extent the iomap spanned may have been
+ * changed during the operation. In this case, the iteration behaviour is to
+ * remap the unprocessed range of the iter, and that means we may need to remap
+ * even when we've made no progress (i.e. iter->processed = 0). Hence the
+ * "finished iterating" case needs to distinguish between
+ * (processed = 0) meaning we are done and (processed = 0 && stale) meaning we
+ * need to remap the entire remaining range.
+ */
 static inline int iomap_iter_advance(struct iomap_iter *iter)
 {
+	bool stale = iter->iomap.flags & IOMAP_F_STALE;
+
 	/* handle the previous iteration (if any) */
 	if (iter->iomap.length) {
-		if (iter->processed <= 0)
+		if (iter->processed < 0)
 			return iter->processed;
+		if (!iter->processed && !stale)
+			return 0;
 		if (WARN_ON_ONCE(iter->processed > iomap_length(iter)))
 			return -EIO;
 		iter->pos += iter->processed;
@@ -33,6 +49,7 @@ static inline void iomap_iter_done(struct iomap_iter *iter)
 	WARN_ON_ONCE(iter->iomap.offset > iter->pos);
 	WARN_ON_ONCE(iter->iomap.length == 0);
 	WARN_ON_ONCE(iter->iomap.offset + iter->iomap.length <= iter->pos);
+	WARN_ON_ONCE(iter->iomap.flags & IOMAP_F_STALE);
 
 	trace_iomap_iter_dstmap(iter->inode, &iter->iomap);
 	if (iter->srcmap.type != IOMAP_HOLE)
* Unmerged path fs/iomap/buffered-io.c
diff --git a/include/linux/iomap.h b/include/linux/iomap.h
index 293e72d14e21..0b16921bbbb7 100644
--- a/include/linux/iomap.h
+++ b/include/linux/iomap.h
@@ -51,26 +51,35 @@ struct vm_fault;
  *
  * IOMAP_F_BUFFER_HEAD indicates that the file system requires the use of
  * buffer heads for this mapping.
+ *
+ * IOMAP_F_XATTR indicates that the iomap is for an extended attribute extent
+ * rather than a file data extent.
  */
-#define IOMAP_F_NEW		0x01
-#define IOMAP_F_DIRTY		0x02
-#define IOMAP_F_SHARED		0x04
-#define IOMAP_F_MERGED		0x08
-#define IOMAP_F_BUFFER_HEAD	0x10
-#define IOMAP_F_ZONE_APPEND	0x20
+#define IOMAP_F_NEW		(1U << 0)
+#define IOMAP_F_DIRTY		(1U << 1)
+#define IOMAP_F_SHARED		(1U << 2)
+#define IOMAP_F_MERGED		(1U << 3)
+#define IOMAP_F_BUFFER_HEAD	(1U << 4)
+#define IOMAP_F_ZONE_APPEND	(1U << 5)
+#define IOMAP_F_XATTR		(1U << 6)
 
 /*
  * Flags set by the core iomap code during operations:
  *
  * IOMAP_F_SIZE_CHANGED indicates to the iomap_end method that the file size
  * has changed as the result of this write operation.
+ *
+ * IOMAP_F_STALE indicates that the iomap is not valid any longer and the file
+ * range it covers needs to be remapped by the high level before the operation
+ * can proceed.
  */
-#define IOMAP_F_SIZE_CHANGED	0x100
+#define IOMAP_F_SIZE_CHANGED	(1U << 8)
+#define IOMAP_F_STALE		(1U << 9)
 
 /*
  * Flags from 0x1000 up are for file system specific usage:
  */
-#define IOMAP_F_PRIVATE		0x1000
+#define IOMAP_F_PRIVATE		(1U << 12)
 
 
 /*
@@ -91,6 +100,7 @@ struct iomap {
 	void			*inline_data;
 	void			*private; /* filesystem private */
 	const struct iomap_page_ops *page_ops;
+	u64			validity_cookie; /* used with .iomap_valid() */
 };
 
 static inline sector_t iomap_sector(const struct iomap *iomap, loff_t pos)
@@ -131,6 +141,23 @@ struct iomap_page_ops {
 	int (*page_prepare)(struct inode *inode, loff_t pos, unsigned len);
 	void (*page_done)(struct inode *inode, loff_t pos, unsigned copied,
 			struct page *page);
+
+	/*
+	 * Check that the cached iomap still maps correctly to the filesystem's
+	 * internal extent map. FS internal extent maps can change while iomap
+	 * is iterating a cached iomap, so this hook allows iomap to detect that
+	 * the iomap needs to be refreshed during a long running write
+	 * operation.
+	 *
+	 * The filesystem can store internal state (e.g. a sequence number) in
+	 * iomap->validity_cookie when the iomap is first mapped to be able to
+	 * detect changes between mapping time and whenever .iomap_valid() is
+	 * called.
+	 *
+	 * This is called with the folio over the specified file position held
+	 * locked by the iomap code.
+	 */
+	bool (*iomap_valid)(struct inode *inode, const struct iomap *iomap);
 };
 
 /*
