mm/page_alloc: track range of active PCP lists during bulk free

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-507.el8
commit-author Mel Gorman <mgorman@techsingularity.net>
commit 35b6d770e6334aa470080570f0f81c8b74a07afd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-507.el8/35b6d770.failed

free_pcppages_bulk() frees pages in a round-robin fashion.  Originally,
this was dealing only with migratetypes but storing high-order pages
means that there can be many more empty lists that are uselessly
checked.  Track the minimum and maximum active pindex to reduce the
search space.

Link: https://lkml.kernel.org/r/20220217002227.5739-3-mgorman@techsingularity.net
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
	Tested-by: Aaron Lu <aaron.lu@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Jesper Dangaard Brouer <brouer@redhat.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 35b6d770e6334aa470080570f0f81c8b74a07afd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index c5bf723f114f,63edba38ebc2..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -1374,8 -1446,12 +1374,14 @@@ static inline void prefetch_buddy(struc
  static void free_pcppages_bulk(struct zone *zone, int count,
  					struct per_cpu_pages *pcp)
  {
++<<<<<<< HEAD
 +	int migratetype = 0;
++=======
+ 	int pindex = 0;
+ 	int min_pindex = 0;
+ 	int max_pindex = NR_PCP_LISTS - 1;
++>>>>>>> 35b6d770e633 (mm/page_alloc: track range of active PCP lists during bulk free)
  	int batch_free = 0;
 -	int nr_freed = 0;
 -	unsigned int order;
  	int prefetch_nr = READ_ONCE(pcp->batch);
  	bool isolated_pageblocks;
  	struct page *page, *tmp;
@@@ -1398,15 -1474,24 +1404,32 @@@
  		 */
  		do {
  			batch_free++;
++<<<<<<< HEAD
 +			if (++migratetype == MIGRATE_PCPTYPES)
 +				migratetype = 0;
 +			list = &pcp->lists[migratetype];
 +		} while (list_empty(list));
 +
 +		/* This is the only non-empty list. Free them all. */
 +		if (batch_free == MIGRATE_PCPTYPES)
++=======
+ 			if (++pindex > max_pindex)
+ 				pindex = min_pindex;
+ 			list = &pcp->lists[pindex];
+ 			if (!list_empty(list))
+ 				break;
+ 
+ 			if (pindex == max_pindex)
+ 				max_pindex--;
+ 			if (pindex == min_pindex)
+ 				min_pindex++;
+ 		} while (1);
+ 
+ 		/* This is the only non-empty list. Free them all. */
+ 		if (batch_free >= max_pindex - min_pindex)
++>>>>>>> 35b6d770e633 (mm/page_alloc: track range of active PCP lists during bulk free)
  			batch_free = count;
  
 -		order = pindex_to_order(pindex);
 -		BUILD_BUG_ON(MAX_ORDER >= (1<<NR_PCP_ORDER_WIDTH));
  		do {
  			page = list_last_entry(list, struct page, lru);
  			/* must delete to avoid corrupting pcp list */
* Unmerged path mm/page_alloc.c
