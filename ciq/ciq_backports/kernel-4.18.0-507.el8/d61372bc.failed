mm/page_alloc: drain the requested list first during bulk free

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-507.el8
commit-author Mel Gorman <mgorman@techsingularity.net>
commit d61372bc41cfe91d6170434fc44b6af49cd2c755
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-507.el8/d61372bc.failed

Prior to the series, pindex 0 (order-0 MIGRATE_UNMOVABLE) was always
skipped first and the precise reason is forgotten.  A potential reason
may have been to artificially preserve MIGRATE_UNMOVABLE but there is no
reason why that would be optimal as it depends on the workload.  The
more likely reason is that it was less complicated to do a pre-increment
instead of a post-increment in terms of overall code flow.  As
free_pcppages_bulk() now typically receives the pindex of the PCP list
that exceeded high, always start draining that list.

Link: https://lkml.kernel.org/r/20220217002227.5739-5-mgorman@techsingularity.net
	Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
	Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
	Tested-by: Aaron Lu <aaron.lu@intel.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: Jesper Dangaard Brouer <brouer@redhat.com>
	Cc: Michal Hocko <mhocko@kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit d61372bc41cfe91d6170434fc44b6af49cd2c755)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/page_alloc.c
diff --cc mm/page_alloc.c
index c5bf723f114f,ddb75c78879f..000000000000
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@@ -1386,27 -1460,31 +1386,35 @@@ static void free_pcppages_bulk(struct z
  	 * below while (list_empty(list)) loop.
  	 */
  	count = min(pcp->count, count);
++<<<<<<< HEAD
 +	while (count) {
++=======
+ 
+ 	/* Ensure requested pindex is drained first. */
+ 	pindex = pindex - 1;
+ 
+ 	while (count > 0) {
++>>>>>>> d61372bc41cf (mm/page_alloc: drain the requested list first during bulk free)
  		struct list_head *list;
 -		int nr_pages;
  
 -		/* Remove pages from lists in a round-robin fashion. */
 +		/*
 +		 * Remove pages from lists in a round-robin fashion. A
 +		 * batch_free count is maintained that is incremented when an
 +		 * empty list is encountered.  This is so more pages are freed
 +		 * off fuller lists instead of spinning excessively around empty
 +		 * lists
 +		 */
  		do {
 -			if (++pindex > max_pindex)
 -				pindex = min_pindex;
 -			list = &pcp->lists[pindex];
 -			if (!list_empty(list))
 -				break;
 +			batch_free++;
 +			if (++migratetype == MIGRATE_PCPTYPES)
 +				migratetype = 0;
 +			list = &pcp->lists[migratetype];
 +		} while (list_empty(list));
  
 -			if (pindex == max_pindex)
 -				max_pindex--;
 -			if (pindex == min_pindex)
 -				min_pindex++;
 -		} while (1);
 +		/* This is the only non-empty list. Free them all. */
 +		if (batch_free == MIGRATE_PCPTYPES)
 +			batch_free = count;
  
 -		order = pindex_to_order(pindex);
 -		nr_pages = 1 << order;
 -		BUILD_BUG_ON(MAX_ORDER >= (1<<NR_PCP_ORDER_WIDTH));
  		do {
  			page = list_last_entry(list, struct page, lru);
  			/* must delete to avoid corrupting pcp list */
* Unmerged path mm/page_alloc.c
