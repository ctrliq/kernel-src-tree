idpf: merge singleq and splitq &net_device_ops

jira LE-3467
Rebuild_History Non-Buildable kernel-4.18.0-553.58.1.el8_10
commit-author Alexander Lobakin <aleksander.lobakin@intel.com>
commit 14f662b43bf8c765114f73d184af2702b2280436
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.58.1.el8_10/14f662b4.failed

It makes no sense to have a second &net_device_ops struct (800 bytes of
rodata) with only one difference in .ndo_start_xmit, which can easily
be just one `if`. This `if` is a drop in the ocean and you won't see
any difference.
Define unified idpf_xmit_start(). The preparation for sending is the
same, just call either idpf_tx_splitq_frame() or idpf_tx_singleq_frame()
depending on the active model to actually map and send the skb.

	Reviewed-by: Przemek Kitszel <przemyslaw.kitszel@intel.com>
	Reviewed-by: Jacob Keller <jacob.e.keller@intel.com>
	Signed-off-by: Alexander Lobakin <aleksander.lobakin@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 14f662b43bf8c765114f73d184af2702b2280436)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
#	drivers/net/ethernet/intel/idpf/idpf_txrx.h
diff --cc drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
index 5c3d34d3de8a,8630db24f63a..000000000000
--- a/drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
@@@ -351,8 -351,8 +351,13 @@@ static void idpf_tx_singleq_build_ctx_d
   *
   * Returns NETDEV_TX_OK if sent, else an error code
   */
++<<<<<<< HEAD
 +static netdev_tx_t idpf_tx_singleq_frame(struct sk_buff *skb,
 +					 struct idpf_queue *tx_q)
++=======
+ netdev_tx_t idpf_tx_singleq_frame(struct sk_buff *skb,
+ 				  struct idpf_tx_queue *tx_q)
++>>>>>>> 14f662b43bf8 (idpf: merge singleq and splitq &net_device_ops)
  {
  	struct idpf_tx_offload_params offload = { };
  	struct idpf_tx_buf *first;
@@@ -409,33 -409,6 +414,36 @@@ out_drop
  }
  
  /**
++<<<<<<< HEAD
 + * idpf_tx_singleq_start - Selects the right Tx queue to send buffer
 + * @skb: send buffer
 + * @netdev: network interface device structure
 + *
 + * Returns NETDEV_TX_OK if sent, else an error code
 + */
 +netdev_tx_t idpf_tx_singleq_start(struct sk_buff *skb,
 +				  struct net_device *netdev)
 +{
 +	struct idpf_vport *vport = idpf_netdev_to_vport(netdev);
 +	struct idpf_queue *tx_q;
 +
 +	tx_q = vport->txqs[skb_get_queue_mapping(skb)];
 +
 +	/* hardware can't handle really short frames, hardware padding works
 +	 * beyond this point
 +	 */
 +	if (skb_put_padto(skb, IDPF_TX_MIN_PKT_LEN)) {
 +		idpf_tx_buf_hw_update(tx_q, tx_q->next_to_use, false);
 +
 +		return NETDEV_TX_OK;
 +	}
 +
 +	return idpf_tx_singleq_frame(skb, tx_q);
 +}
 +
 +/**
++=======
++>>>>>>> 14f662b43bf8 (idpf: merge singleq and splitq &net_device_ops)
   * idpf_tx_singleq_clean - Reclaim resources from queue
   * @tx_q: Tx queue to clean
   * @napi_budget: Used to determine if we are in netpoll
diff --cc drivers/net/ethernet/intel/idpf/idpf_txrx.h
index 1669bf01ba1d,b2bf58146484..000000000000
--- a/drivers/net/ethernet/intel/idpf/idpf_txrx.h
+++ b/drivers/net/ethernet/intel/idpf/idpf_txrx.h
@@@ -1031,28 -1187,23 +1031,37 @@@ void idpf_deinit_rss(struct idpf_vport 
  int idpf_rx_bufs_init_all(struct idpf_vport *vport);
  void idpf_rx_add_frag(struct idpf_rx_buf *rx_buf, struct sk_buff *skb,
  		      unsigned int size);
 -struct sk_buff *idpf_rx_construct_skb(const struct idpf_rx_queue *rxq,
 +struct sk_buff *idpf_rx_construct_skb(struct idpf_queue *rxq,
  				      struct idpf_rx_buf *rx_buf,
  				      unsigned int size);
 -void idpf_tx_buf_hw_update(struct idpf_tx_queue *tx_q, u32 val,
 +bool idpf_init_rx_buf_hw_alloc(struct idpf_queue *rxq, struct idpf_rx_buf *buf);
 +void idpf_rx_buf_hw_update(struct idpf_queue *rxq, u32 val);
 +void idpf_tx_buf_hw_update(struct idpf_queue *tx_q, u32 val,
  			   bool xmit_more);
  unsigned int idpf_size_to_txd_count(unsigned int size);
 -netdev_tx_t idpf_tx_drop_skb(struct idpf_tx_queue *tx_q, struct sk_buff *skb);
 -void idpf_tx_dma_map_error(struct idpf_tx_queue *txq, struct sk_buff *skb,
 +netdev_tx_t idpf_tx_drop_skb(struct idpf_queue *tx_q, struct sk_buff *skb);
 +void idpf_tx_dma_map_error(struct idpf_queue *txq, struct sk_buff *skb,
  			   struct idpf_tx_buf *first, u16 ring_idx);
 -unsigned int idpf_tx_desc_count_required(struct idpf_tx_queue *txq,
 +unsigned int idpf_tx_desc_count_required(struct idpf_queue *txq,
  					 struct sk_buff *skb);
++<<<<<<< HEAD
 +bool idpf_chk_linearize(struct sk_buff *skb, unsigned int max_bufs,
 +			unsigned int count);
 +int idpf_tx_maybe_stop_common(struct idpf_queue *tx_q, unsigned int size);
 +void idpf_tx_timeout(struct net_device *netdev, unsigned int txqueue);
 +netdev_tx_t idpf_tx_splitq_start(struct sk_buff *skb,
 +				 struct net_device *netdev);
 +netdev_tx_t idpf_tx_singleq_start(struct sk_buff *skb,
 +				  struct net_device *netdev);
 +bool idpf_rx_singleq_buf_hw_alloc_all(struct idpf_queue *rxq,
++=======
+ int idpf_tx_maybe_stop_common(struct idpf_tx_queue *tx_q, unsigned int size);
+ void idpf_tx_timeout(struct net_device *netdev, unsigned int txqueue);
+ netdev_tx_t idpf_tx_singleq_frame(struct sk_buff *skb,
+ 				  struct idpf_tx_queue *tx_q);
+ netdev_tx_t idpf_tx_start(struct sk_buff *skb, struct net_device *netdev);
+ bool idpf_rx_singleq_buf_hw_alloc_all(struct idpf_rx_queue *rxq,
++>>>>>>> 14f662b43bf8 (idpf: merge singleq and splitq &net_device_ops)
  				      u16 cleaned_count);
  int idpf_tso(struct sk_buff *skb, struct idpf_tx_offload_params *off);
  
diff --git a/drivers/net/ethernet/intel/idpf/idpf_lib.c b/drivers/net/ethernet/intel/idpf/idpf_lib.c
index ae8a48c48070..0964cfb4633c 100644
--- a/drivers/net/ethernet/intel/idpf/idpf_lib.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_lib.c
@@ -4,8 +4,7 @@
 #include "idpf.h"
 #include "idpf_virtchnl.h"
 
-static const struct net_device_ops idpf_netdev_ops_splitq;
-static const struct net_device_ops idpf_netdev_ops_singleq;
+static const struct net_device_ops idpf_netdev_ops;
 
 /**
  * idpf_init_vector_stack - Fill the MSIX vector stack with vector index
@@ -765,10 +764,7 @@ static int idpf_cfg_netdev(struct idpf_vport *vport)
 	}
 
 	/* assign netdev_ops */
-	if (idpf_is_queue_model_split(vport->txq_model))
-		netdev->netdev_ops = &idpf_netdev_ops_splitq;
-	else
-		netdev->netdev_ops = &idpf_netdev_ops_singleq;
+	netdev->netdev_ops = &idpf_netdev_ops;
 
 	/* setup watchdog timeout value to be 5 second */
 	netdev->watchdog_timeo = 5 * HZ;
@@ -2393,24 +2389,10 @@ void idpf_free_dma_mem(struct idpf_hw *hw, struct idpf_dma_mem *mem)
 	mem->pa = 0;
 }
 
-static const struct net_device_ops idpf_netdev_ops_splitq = {
-	.ndo_open = idpf_open,
-	.ndo_stop = idpf_stop,
-	.ndo_start_xmit = idpf_tx_splitq_start,
-	.ndo_features_check = idpf_features_check,
-	.ndo_set_rx_mode = idpf_set_rx_mode,
-	.ndo_validate_addr = eth_validate_addr,
-	.ndo_set_mac_address = idpf_set_mac,
-	.ndo_change_mtu = idpf_change_mtu,
-	.ndo_get_stats64 = idpf_get_stats64,
-	.ndo_set_features = idpf_set_features,
-	.ndo_tx_timeout = idpf_tx_timeout,
-};
-
-static const struct net_device_ops idpf_netdev_ops_singleq = {
+static const struct net_device_ops idpf_netdev_ops = {
 	.ndo_open = idpf_open,
 	.ndo_stop = idpf_stop,
-	.ndo_start_xmit = idpf_tx_singleq_start,
+	.ndo_start_xmit = idpf_tx_start,
 	.ndo_features_check = idpf_features_check,
 	.ndo_set_rx_mode = idpf_set_rx_mode,
 	.ndo_validate_addr = eth_validate_addr,
* Unmerged path drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
diff --git a/drivers/net/ethernet/intel/idpf/idpf_txrx.c b/drivers/net/ethernet/intel/idpf/idpf_txrx.c
index 4a7062c55fcb..1634a59120fd 100644
--- a/drivers/net/ethernet/intel/idpf/idpf_txrx.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_txrx.c
@@ -4,6 +4,9 @@
 #include "idpf.h"
 #include "idpf_virtchnl.h"
 
+static bool idpf_chk_linearize(struct sk_buff *skb, unsigned int max_bufs,
+			       unsigned int count);
+
 /**
  * idpf_buf_lifo_push - push a buffer pointer onto stack
  * @stack: pointer to stack struct
@@ -2527,8 +2530,8 @@ static bool __idpf_chk_linearize(struct sk_buff *skb, unsigned int max_bufs)
  * E.g.: a packet with 7 fragments can require 9 DMA transactions; 1 for TSO
  * header, 1 for segment payload, and then 7 for the fragments.
  */
-bool idpf_chk_linearize(struct sk_buff *skb, unsigned int max_bufs,
-			unsigned int count)
+static bool idpf_chk_linearize(struct sk_buff *skb, unsigned int max_bufs,
+			       unsigned int count)
 {
 	if (likely(count < max_bufs))
 		return false;
@@ -2674,14 +2677,13 @@ static netdev_tx_t idpf_tx_splitq_frame(struct sk_buff *skb,
 }
 
 /**
- * idpf_tx_splitq_start - Selects the right Tx queue to send buffer
+ * idpf_tx_start - Selects the right Tx queue to send buffer
  * @skb: send buffer
  * @netdev: network interface device structure
  *
  * Returns NETDEV_TX_OK if sent, else an error code
  */
-netdev_tx_t idpf_tx_splitq_start(struct sk_buff *skb,
-				 struct net_device *netdev)
+netdev_tx_t idpf_tx_start(struct sk_buff *skb, struct net_device *netdev)
 {
 	struct idpf_vport *vport = idpf_netdev_to_vport(netdev);
 	struct idpf_queue *tx_q;
@@ -2703,7 +2705,10 @@ netdev_tx_t idpf_tx_splitq_start(struct sk_buff *skb,
 		return NETDEV_TX_OK;
 	}
 
-	return idpf_tx_splitq_frame(skb, tx_q);
+	if (idpf_is_queue_model_split(vport->txq_model))
+		return idpf_tx_splitq_frame(skb, tx_q);
+	else
+		return idpf_tx_singleq_frame(skb, tx_q);
 }
 
 /**
* Unmerged path drivers/net/ethernet/intel/idpf/idpf_txrx.h
