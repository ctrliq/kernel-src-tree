net: remove gfp_mask from napi_alloc_skb()

jira LE-3467
Rebuild_History Non-Buildable kernel-4.18.0-553.58.1.el8_10
Rebuild_CHGLOG: - net: remove gfp_mask from napi_alloc_skb() [idpf] (Michal Schmidt) [RHEL-71182]
Rebuild_FUZZ: 92.31%
commit-author Jakub Kicinski <kuba@kernel.org>
commit 6e9b01909a811555ff3326cf80a5847169c57806
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.58.1.el8_10/6e9b0190.failed

__napi_alloc_skb() is napi_alloc_skb() with the added flexibility
of choosing gfp_mask. This is a NAPI function, so GFP_ATOMIC is
implied. The only practical choice the caller has is whether to
set __GFP_NOWARN. But that's a false choice, too, allocation failures
in atomic context will happen, and printing warnings in logs,
effectively for a packet drop, is both too much and very likely
non-actionable.

This leads me to a conclusion that most uses of napi_alloc_skb()
are simply misguided, and should use __GFP_NOWARN in the first
place. We also have a "standard" way of reporting allocation
failures via the queue stat API (qstats::rx-alloc-fail).

The direct motivation for this patch is that one of the drivers
used at Meta calls napi_alloc_skb() (so prior to this patch without
__GFP_NOWARN), and the resulting OOM warning is the top networking
warning in our fleet.

	Reviewed-by: Alexander Lobakin <aleksander.lobakin@intel.com>
	Reviewed-by: Simon Horman <horms@kernel.org>
	Reviewed-by: Eric Dumazet <edumazet@google.com>
Link: https://lore.kernel.org/r/20240327040213.3153864-1-kuba@kernel.org
	Signed-off-by: Jakub Kicinski <kuba@kernel.org>
(cherry picked from commit 6e9b01909a811555ff3326cf80a5847169c57806)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/translations/zh_CN/mm/page_frags.rst
#	Documentation/vm/page_frags.rst
#	drivers/net/ethernet/intel/ice/ice_txrx.c
#	drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
diff --cc Documentation/vm/page_frags.rst
index 637cc49d1b2f,503ca6cdb804..000000000000
--- a/Documentation/vm/page_frags.rst
+++ b/Documentation/vm/page_frags.rst
@@@ -26,8 -24,8 +26,13 @@@ to be disabled when executing the fragm
  
  The network stack uses two separate caches per CPU to handle fragment
  allocation.  The netdev_alloc_cache is used by callers making use of the
++<<<<<<< HEAD:Documentation/vm/page_frags.rst
 +__netdev_alloc_frag and __netdev_alloc_skb calls.  The napi_alloc_cache is
 +used by callers of the __napi_alloc_frag and __napi_alloc_skb calls.  The
++=======
+ netdev_alloc_frag and __netdev_alloc_skb calls.  The napi_alloc_cache is
+ used by callers of the __napi_alloc_frag and napi_alloc_skb calls.  The
++>>>>>>> 6e9b01909a81 (net: remove gfp_mask from napi_alloc_skb()):Documentation/mm/page_frags.rst
  main difference between these two calls is the context in which they may be
  called.  The "netdev" prefixed functions are usable in any context as these
  functions will disable interrupts, while the "napi" prefixed functions are
diff --cc drivers/net/ethernet/intel/ice/ice_txrx.c
index b62c095891da,8bb743f78fcb..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_txrx.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.c
@@@ -990,12 -1043,15 +990,16 @@@ ice_construct_skb(struct ice_rx_ring *r
  	struct sk_buff *skb;
  
  	/* prefetch first cache line of first page */
 -	net_prefetch(xdp->data);
 -
 -	if (unlikely(xdp_buff_has_frags(xdp))) {
 -		sinfo = xdp_get_shared_info_from_buff(xdp);
 -		nr_frags = sinfo->nr_frags;
 -	}
 +	net_prefetch(xdp->data_meta);
  
  	/* allocate a skb to store the frags */
++<<<<<<< HEAD
 +	skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
 +			       ICE_RX_HDR_SIZE + metasize,
 +			       GFP_ATOMIC | __GFP_NOWARN);
++=======
+ 	skb = napi_alloc_skb(&rx_ring->q_vector->napi, ICE_RX_HDR_SIZE);
++>>>>>>> 6e9b01909a81 (net: remove gfp_mask from napi_alloc_skb())
  	if (unlikely(!skb))
  		return NULL;
  
diff --cc drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 34009ae26aed,bcdde68a099a..000000000000
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@@ -4144,6 -4911,513 +4144,516 @@@ static unsigned int stmmac_rx_buf2_len(
  	return plen - len;
  }
  
++<<<<<<< HEAD
++=======
+ static int stmmac_xdp_xmit_xdpf(struct stmmac_priv *priv, int queue,
+ 				struct xdp_frame *xdpf, bool dma_map)
+ {
+ 	struct stmmac_txq_stats *txq_stats = &priv->xstats.txq_stats[queue];
+ 	struct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];
+ 	unsigned int entry = tx_q->cur_tx;
+ 	struct dma_desc *tx_desc;
+ 	dma_addr_t dma_addr;
+ 	bool set_ic;
+ 
+ 	if (stmmac_tx_avail(priv, queue) < STMMAC_TX_THRESH(priv))
+ 		return STMMAC_XDP_CONSUMED;
+ 
+ 	if (priv->plat->est && priv->plat->est->enable &&
+ 	    priv->plat->est->max_sdu[queue] &&
+ 	    xdpf->len > priv->plat->est->max_sdu[queue]) {
+ 		priv->xstats.max_sdu_txq_drop[queue]++;
+ 		return STMMAC_XDP_CONSUMED;
+ 	}
+ 
+ 	if (likely(priv->extend_desc))
+ 		tx_desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+ 	else if (tx_q->tbs & STMMAC_TBS_AVAIL)
+ 		tx_desc = &tx_q->dma_entx[entry].basic;
+ 	else
+ 		tx_desc = tx_q->dma_tx + entry;
+ 
+ 	if (dma_map) {
+ 		dma_addr = dma_map_single(priv->device, xdpf->data,
+ 					  xdpf->len, DMA_TO_DEVICE);
+ 		if (dma_mapping_error(priv->device, dma_addr))
+ 			return STMMAC_XDP_CONSUMED;
+ 
+ 		tx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_XDP_NDO;
+ 	} else {
+ 		struct page *page = virt_to_page(xdpf->data);
+ 
+ 		dma_addr = page_pool_get_dma_addr(page) + sizeof(*xdpf) +
+ 			   xdpf->headroom;
+ 		dma_sync_single_for_device(priv->device, dma_addr,
+ 					   xdpf->len, DMA_BIDIRECTIONAL);
+ 
+ 		tx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_XDP_TX;
+ 	}
+ 
+ 	tx_q->tx_skbuff_dma[entry].buf = dma_addr;
+ 	tx_q->tx_skbuff_dma[entry].map_as_page = false;
+ 	tx_q->tx_skbuff_dma[entry].len = xdpf->len;
+ 	tx_q->tx_skbuff_dma[entry].last_segment = true;
+ 	tx_q->tx_skbuff_dma[entry].is_jumbo = false;
+ 
+ 	tx_q->xdpf[entry] = xdpf;
+ 
+ 	stmmac_set_desc_addr(priv, tx_desc, dma_addr);
+ 
+ 	stmmac_prepare_tx_desc(priv, tx_desc, 1, xdpf->len,
+ 			       true, priv->mode, true, true,
+ 			       xdpf->len);
+ 
+ 	tx_q->tx_count_frames++;
+ 
+ 	if (tx_q->tx_count_frames % priv->tx_coal_frames[queue] == 0)
+ 		set_ic = true;
+ 	else
+ 		set_ic = false;
+ 
+ 	if (set_ic) {
+ 		tx_q->tx_count_frames = 0;
+ 		stmmac_set_tx_ic(priv, tx_desc);
+ 		u64_stats_update_begin(&txq_stats->q_syncp);
+ 		u64_stats_inc(&txq_stats->q.tx_set_ic_bit);
+ 		u64_stats_update_end(&txq_stats->q_syncp);
+ 	}
+ 
+ 	stmmac_enable_dma_transmission(priv, priv->ioaddr);
+ 
+ 	entry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_tx_size);
+ 	tx_q->cur_tx = entry;
+ 
+ 	return STMMAC_XDP_TX;
+ }
+ 
+ static int stmmac_xdp_get_tx_queue(struct stmmac_priv *priv,
+ 				   int cpu)
+ {
+ 	int index = cpu;
+ 
+ 	if (unlikely(index < 0))
+ 		index = 0;
+ 
+ 	while (index >= priv->plat->tx_queues_to_use)
+ 		index -= priv->plat->tx_queues_to_use;
+ 
+ 	return index;
+ }
+ 
+ static int stmmac_xdp_xmit_back(struct stmmac_priv *priv,
+ 				struct xdp_buff *xdp)
+ {
+ 	struct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);
+ 	int cpu = smp_processor_id();
+ 	struct netdev_queue *nq;
+ 	int queue;
+ 	int res;
+ 
+ 	if (unlikely(!xdpf))
+ 		return STMMAC_XDP_CONSUMED;
+ 
+ 	queue = stmmac_xdp_get_tx_queue(priv, cpu);
+ 	nq = netdev_get_tx_queue(priv->dev, queue);
+ 
+ 	__netif_tx_lock(nq, cpu);
+ 	/* Avoids TX time-out as we are sharing with slow path */
+ 	txq_trans_cond_update(nq);
+ 
+ 	res = stmmac_xdp_xmit_xdpf(priv, queue, xdpf, false);
+ 	if (res == STMMAC_XDP_TX)
+ 		stmmac_flush_tx_descriptors(priv, queue);
+ 
+ 	__netif_tx_unlock(nq);
+ 
+ 	return res;
+ }
+ 
+ static int __stmmac_xdp_run_prog(struct stmmac_priv *priv,
+ 				 struct bpf_prog *prog,
+ 				 struct xdp_buff *xdp)
+ {
+ 	u32 act;
+ 	int res;
+ 
+ 	act = bpf_prog_run_xdp(prog, xdp);
+ 	switch (act) {
+ 	case XDP_PASS:
+ 		res = STMMAC_XDP_PASS;
+ 		break;
+ 	case XDP_TX:
+ 		res = stmmac_xdp_xmit_back(priv, xdp);
+ 		break;
+ 	case XDP_REDIRECT:
+ 		if (xdp_do_redirect(priv->dev, xdp, prog) < 0)
+ 			res = STMMAC_XDP_CONSUMED;
+ 		else
+ 			res = STMMAC_XDP_REDIRECT;
+ 		break;
+ 	default:
+ 		bpf_warn_invalid_xdp_action(priv->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_ABORTED:
+ 		trace_xdp_exception(priv->dev, prog, act);
+ 		fallthrough;
+ 	case XDP_DROP:
+ 		res = STMMAC_XDP_CONSUMED;
+ 		break;
+ 	}
+ 
+ 	return res;
+ }
+ 
+ static struct sk_buff *stmmac_xdp_run_prog(struct stmmac_priv *priv,
+ 					   struct xdp_buff *xdp)
+ {
+ 	struct bpf_prog *prog;
+ 	int res;
+ 
+ 	prog = READ_ONCE(priv->xdp_prog);
+ 	if (!prog) {
+ 		res = STMMAC_XDP_PASS;
+ 		goto out;
+ 	}
+ 
+ 	res = __stmmac_xdp_run_prog(priv, prog, xdp);
+ out:
+ 	return ERR_PTR(-res);
+ }
+ 
+ static void stmmac_finalize_xdp_rx(struct stmmac_priv *priv,
+ 				   int xdp_status)
+ {
+ 	int cpu = smp_processor_id();
+ 	int queue;
+ 
+ 	queue = stmmac_xdp_get_tx_queue(priv, cpu);
+ 
+ 	if (xdp_status & STMMAC_XDP_TX)
+ 		stmmac_tx_timer_arm(priv, queue);
+ 
+ 	if (xdp_status & STMMAC_XDP_REDIRECT)
+ 		xdp_do_flush();
+ }
+ 
+ static struct sk_buff *stmmac_construct_skb_zc(struct stmmac_channel *ch,
+ 					       struct xdp_buff *xdp)
+ {
+ 	unsigned int metasize = xdp->data - xdp->data_meta;
+ 	unsigned int datasize = xdp->data_end - xdp->data;
+ 	struct sk_buff *skb;
+ 
+ 	skb = napi_alloc_skb(&ch->rxtx_napi,
+ 			     xdp->data_end - xdp->data_hard_start);
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
+ 	skb_reserve(skb, xdp->data - xdp->data_hard_start);
+ 	memcpy(__skb_put(skb, datasize), xdp->data, datasize);
+ 	if (metasize)
+ 		skb_metadata_set(skb, metasize);
+ 
+ 	return skb;
+ }
+ 
+ static void stmmac_dispatch_skb_zc(struct stmmac_priv *priv, u32 queue,
+ 				   struct dma_desc *p, struct dma_desc *np,
+ 				   struct xdp_buff *xdp)
+ {
+ 	struct stmmac_rxq_stats *rxq_stats = &priv->xstats.rxq_stats[queue];
+ 	struct stmmac_channel *ch = &priv->channel[queue];
+ 	unsigned int len = xdp->data_end - xdp->data;
+ 	enum pkt_hash_types hash_type;
+ 	int coe = priv->hw->rx_csum;
+ 	struct sk_buff *skb;
+ 	u32 hash;
+ 
+ 	skb = stmmac_construct_skb_zc(ch, xdp);
+ 	if (!skb) {
+ 		priv->xstats.rx_dropped++;
+ 		return;
+ 	}
+ 
+ 	stmmac_get_rx_hwtstamp(priv, p, np, skb);
+ 	if (priv->hw->hw_vlan_en)
+ 		/* MAC level stripping. */
+ 		stmmac_rx_hw_vlan(priv, priv->hw, p, skb);
+ 	else
+ 		/* Driver level stripping. */
+ 		stmmac_rx_vlan(priv->dev, skb);
+ 	skb->protocol = eth_type_trans(skb, priv->dev);
+ 
+ 	if (unlikely(!coe) || !stmmac_has_ip_ethertype(skb))
+ 		skb_checksum_none_assert(skb);
+ 	else
+ 		skb->ip_summed = CHECKSUM_UNNECESSARY;
+ 
+ 	if (!stmmac_get_rx_hash(priv, p, &hash, &hash_type))
+ 		skb_set_hash(skb, hash, hash_type);
+ 
+ 	skb_record_rx_queue(skb, queue);
+ 	napi_gro_receive(&ch->rxtx_napi, skb);
+ 
+ 	u64_stats_update_begin(&rxq_stats->napi_syncp);
+ 	u64_stats_inc(&rxq_stats->napi.rx_pkt_n);
+ 	u64_stats_add(&rxq_stats->napi.rx_bytes, len);
+ 	u64_stats_update_end(&rxq_stats->napi_syncp);
+ }
+ 
+ static bool stmmac_rx_refill_zc(struct stmmac_priv *priv, u32 queue, u32 budget)
+ {
+ 	struct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];
+ 	unsigned int entry = rx_q->dirty_rx;
+ 	struct dma_desc *rx_desc = NULL;
+ 	bool ret = true;
+ 
+ 	budget = min(budget, stmmac_rx_dirty(priv, queue));
+ 
+ 	while (budget-- > 0 && entry != rx_q->cur_rx) {
+ 		struct stmmac_rx_buffer *buf = &rx_q->buf_pool[entry];
+ 		dma_addr_t dma_addr;
+ 		bool use_rx_wd;
+ 
+ 		if (!buf->xdp) {
+ 			buf->xdp = xsk_buff_alloc(rx_q->xsk_pool);
+ 			if (!buf->xdp) {
+ 				ret = false;
+ 				break;
+ 			}
+ 		}
+ 
+ 		if (priv->extend_desc)
+ 			rx_desc = (struct dma_desc *)(rx_q->dma_erx + entry);
+ 		else
+ 			rx_desc = rx_q->dma_rx + entry;
+ 
+ 		dma_addr = xsk_buff_xdp_get_dma(buf->xdp);
+ 		stmmac_set_desc_addr(priv, rx_desc, dma_addr);
+ 		stmmac_set_desc_sec_addr(priv, rx_desc, 0, false);
+ 		stmmac_refill_desc3(priv, rx_q, rx_desc);
+ 
+ 		rx_q->rx_count_frames++;
+ 		rx_q->rx_count_frames += priv->rx_coal_frames[queue];
+ 		if (rx_q->rx_count_frames > priv->rx_coal_frames[queue])
+ 			rx_q->rx_count_frames = 0;
+ 
+ 		use_rx_wd = !priv->rx_coal_frames[queue];
+ 		use_rx_wd |= rx_q->rx_count_frames > 0;
+ 		if (!priv->use_riwt)
+ 			use_rx_wd = false;
+ 
+ 		dma_wmb();
+ 		stmmac_set_rx_owner(priv, rx_desc, use_rx_wd);
+ 
+ 		entry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_rx_size);
+ 	}
+ 
+ 	if (rx_desc) {
+ 		rx_q->dirty_rx = entry;
+ 		rx_q->rx_tail_addr = rx_q->dma_rx_phy +
+ 				     (rx_q->dirty_rx * sizeof(struct dma_desc));
+ 		stmmac_set_rx_tail_ptr(priv, priv->ioaddr, rx_q->rx_tail_addr, queue);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static struct stmmac_xdp_buff *xsk_buff_to_stmmac_ctx(struct xdp_buff *xdp)
+ {
+ 	/* In XDP zero copy data path, xdp field in struct xdp_buff_xsk is used
+ 	 * to represent incoming packet, whereas cb field in the same structure
+ 	 * is used to store driver specific info. Thus, struct stmmac_xdp_buff
+ 	 * is laid on top of xdp and cb fields of struct xdp_buff_xsk.
+ 	 */
+ 	return (struct stmmac_xdp_buff *)xdp;
+ }
+ 
+ static int stmmac_rx_zc(struct stmmac_priv *priv, int limit, u32 queue)
+ {
+ 	struct stmmac_rxq_stats *rxq_stats = &priv->xstats.rxq_stats[queue];
+ 	struct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];
+ 	unsigned int count = 0, error = 0, len = 0;
+ 	int dirty = stmmac_rx_dirty(priv, queue);
+ 	unsigned int next_entry = rx_q->cur_rx;
+ 	u32 rx_errors = 0, rx_dropped = 0;
+ 	unsigned int desc_size;
+ 	struct bpf_prog *prog;
+ 	bool failure = false;
+ 	int xdp_status = 0;
+ 	int status = 0;
+ 
+ 	if (netif_msg_rx_status(priv)) {
+ 		void *rx_head;
+ 
+ 		netdev_dbg(priv->dev, "%s: descriptor ring:\n", __func__);
+ 		if (priv->extend_desc) {
+ 			rx_head = (void *)rx_q->dma_erx;
+ 			desc_size = sizeof(struct dma_extended_desc);
+ 		} else {
+ 			rx_head = (void *)rx_q->dma_rx;
+ 			desc_size = sizeof(struct dma_desc);
+ 		}
+ 
+ 		stmmac_display_ring(priv, rx_head, priv->dma_conf.dma_rx_size, true,
+ 				    rx_q->dma_rx_phy, desc_size);
+ 	}
+ 	while (count < limit) {
+ 		struct stmmac_rx_buffer *buf;
+ 		struct stmmac_xdp_buff *ctx;
+ 		unsigned int buf1_len = 0;
+ 		struct dma_desc *np, *p;
+ 		int entry;
+ 		int res;
+ 
+ 		if (!count && rx_q->state_saved) {
+ 			error = rx_q->state.error;
+ 			len = rx_q->state.len;
+ 		} else {
+ 			rx_q->state_saved = false;
+ 			error = 0;
+ 			len = 0;
+ 		}
+ 
+ 		if (count >= limit)
+ 			break;
+ 
+ read_again:
+ 		buf1_len = 0;
+ 		entry = next_entry;
+ 		buf = &rx_q->buf_pool[entry];
+ 
+ 		if (dirty >= STMMAC_RX_FILL_BATCH) {
+ 			failure = failure ||
+ 				  !stmmac_rx_refill_zc(priv, queue, dirty);
+ 			dirty = 0;
+ 		}
+ 
+ 		if (priv->extend_desc)
+ 			p = (struct dma_desc *)(rx_q->dma_erx + entry);
+ 		else
+ 			p = rx_q->dma_rx + entry;
+ 
+ 		/* read the status of the incoming frame */
+ 		status = stmmac_rx_status(priv, &priv->xstats, p);
+ 		/* check if managed by the DMA otherwise go ahead */
+ 		if (unlikely(status & dma_own))
+ 			break;
+ 
+ 		/* Prefetch the next RX descriptor */
+ 		rx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx,
+ 						priv->dma_conf.dma_rx_size);
+ 		next_entry = rx_q->cur_rx;
+ 
+ 		if (priv->extend_desc)
+ 			np = (struct dma_desc *)(rx_q->dma_erx + next_entry);
+ 		else
+ 			np = rx_q->dma_rx + next_entry;
+ 
+ 		prefetch(np);
+ 
+ 		/* Ensure a valid XSK buffer before proceed */
+ 		if (!buf->xdp)
+ 			break;
+ 
+ 		if (priv->extend_desc)
+ 			stmmac_rx_extended_status(priv, &priv->xstats,
+ 						  rx_q->dma_erx + entry);
+ 		if (unlikely(status == discard_frame)) {
+ 			xsk_buff_free(buf->xdp);
+ 			buf->xdp = NULL;
+ 			dirty++;
+ 			error = 1;
+ 			if (!priv->hwts_rx_en)
+ 				rx_errors++;
+ 		}
+ 
+ 		if (unlikely(error && (status & rx_not_ls)))
+ 			goto read_again;
+ 		if (unlikely(error)) {
+ 			count++;
+ 			continue;
+ 		}
+ 
+ 		/* XSK pool expects RX frame 1:1 mapped to XSK buffer */
+ 		if (likely(status & rx_not_ls)) {
+ 			xsk_buff_free(buf->xdp);
+ 			buf->xdp = NULL;
+ 			dirty++;
+ 			count++;
+ 			goto read_again;
+ 		}
+ 
+ 		ctx = xsk_buff_to_stmmac_ctx(buf->xdp);
+ 		ctx->priv = priv;
+ 		ctx->desc = p;
+ 		ctx->ndesc = np;
+ 
+ 		/* XDP ZC Frame only support primary buffers for now */
+ 		buf1_len = stmmac_rx_buf1_len(priv, p, status, len);
+ 		len += buf1_len;
+ 
+ 		/* ACS is disabled; strip manually. */
+ 		if (likely(!(status & rx_not_ls))) {
+ 			buf1_len -= ETH_FCS_LEN;
+ 			len -= ETH_FCS_LEN;
+ 		}
+ 
+ 		/* RX buffer is good and fit into a XSK pool buffer */
+ 		buf->xdp->data_end = buf->xdp->data + buf1_len;
+ 		xsk_buff_dma_sync_for_cpu(buf->xdp, rx_q->xsk_pool);
+ 
+ 		prog = READ_ONCE(priv->xdp_prog);
+ 		res = __stmmac_xdp_run_prog(priv, prog, buf->xdp);
+ 
+ 		switch (res) {
+ 		case STMMAC_XDP_PASS:
+ 			stmmac_dispatch_skb_zc(priv, queue, p, np, buf->xdp);
+ 			xsk_buff_free(buf->xdp);
+ 			break;
+ 		case STMMAC_XDP_CONSUMED:
+ 			xsk_buff_free(buf->xdp);
+ 			rx_dropped++;
+ 			break;
+ 		case STMMAC_XDP_TX:
+ 		case STMMAC_XDP_REDIRECT:
+ 			xdp_status |= res;
+ 			break;
+ 		}
+ 
+ 		buf->xdp = NULL;
+ 		dirty++;
+ 		count++;
+ 	}
+ 
+ 	if (status & rx_not_ls) {
+ 		rx_q->state_saved = true;
+ 		rx_q->state.error = error;
+ 		rx_q->state.len = len;
+ 	}
+ 
+ 	stmmac_finalize_xdp_rx(priv, xdp_status);
+ 
+ 	u64_stats_update_begin(&rxq_stats->napi_syncp);
+ 	u64_stats_add(&rxq_stats->napi.rx_pkt_n, count);
+ 	u64_stats_update_end(&rxq_stats->napi_syncp);
+ 
+ 	priv->xstats.rx_dropped += rx_dropped;
+ 	priv->xstats.rx_errors += rx_errors;
+ 
+ 	if (xsk_uses_need_wakeup(rx_q->xsk_pool)) {
+ 		if (failure || stmmac_rx_dirty(priv, queue) > 0)
+ 			xsk_set_rx_need_wakeup(rx_q->xsk_pool);
+ 		else
+ 			xsk_clear_rx_need_wakeup(rx_q->xsk_pool);
+ 
+ 		return (int)count;
+ 	}
+ 
+ 	return failure ? limit : (int)count;
+ }
+ 
++>>>>>>> 6e9b01909a81 (net: remove gfp_mask from napi_alloc_skb())
  /**
   * stmmac_rx - manage the receive process
   * @priv: driver private structure
* Unmerged path Documentation/translations/zh_CN/mm/page_frags.rst
* Unmerged path Documentation/translations/zh_CN/mm/page_frags.rst
* Unmerged path Documentation/vm/page_frags.rst
diff --git a/drivers/net/ethernet/intel/i40e/i40e_txrx.c b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
index 44c8a3b7fd14..674efe554b7b 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_txrx.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_txrx.c
@@ -2134,9 +2134,7 @@ static struct sk_buff *i40e_construct_skb(struct i40e_ring *rx_ring,
 	 */
 
 	/* allocate a skb to store the frags */
-	skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
-			       I40E_RX_HDR_SIZE,
-			       GFP_ATOMIC | __GFP_NOWARN);
+	skb = napi_alloc_skb(&rx_ring->q_vector->napi, I40E_RX_HDR_SIZE);
 	if (unlikely(!skb))
 		return NULL;
 
diff --git a/drivers/net/ethernet/intel/i40e/i40e_xsk.c b/drivers/net/ethernet/intel/i40e/i40e_xsk.c
index 51ce422d0e5d..9dfbba1dc8eb 100644
--- a/drivers/net/ethernet/intel/i40e/i40e_xsk.c
+++ b/drivers/net/ethernet/intel/i40e/i40e_xsk.c
@@ -302,8 +302,7 @@ static struct sk_buff *i40e_construct_skb_zc(struct i40e_ring *rx_ring,
 	net_prefetch(xdp->data_meta);
 
 	/* allocate a skb to store the frags */
-	skb = __napi_alloc_skb(&rx_ring->q_vector->napi, totalsize,
-			       GFP_ATOMIC | __GFP_NOWARN);
+	skb = napi_alloc_skb(&rx_ring->q_vector->napi, totalsize);
 	if (unlikely(!skb))
 		goto out;
 
diff --git a/drivers/net/ethernet/intel/iavf/iavf_txrx.c b/drivers/net/ethernet/intel/iavf/iavf_txrx.c
index d64c4997136b..ef37886b222a 100644
--- a/drivers/net/ethernet/intel/iavf/iavf_txrx.c
+++ b/drivers/net/ethernet/intel/iavf/iavf_txrx.c
@@ -1335,9 +1335,7 @@ static struct sk_buff *iavf_construct_skb(struct iavf_ring *rx_ring,
 	net_prefetch(va);
 
 	/* allocate a skb to store the frags */
-	skb = __napi_alloc_skb(&rx_ring->q_vector->napi,
-			       IAVF_RX_HDR_SIZE,
-			       GFP_ATOMIC | __GFP_NOWARN);
+	skb = napi_alloc_skb(&rx_ring->q_vector->napi, IAVF_RX_HDR_SIZE);
 	if (unlikely(!skb))
 		return NULL;
 
* Unmerged path drivers/net/ethernet/intel/ice/ice_txrx.c
diff --git a/drivers/net/ethernet/intel/ice/ice_xsk.c b/drivers/net/ethernet/intel/ice/ice_xsk.c
index 360de5b754a7..725c52d6f29e 100644
--- a/drivers/net/ethernet/intel/ice/ice_xsk.c
+++ b/drivers/net/ethernet/intel/ice/ice_xsk.c
@@ -579,8 +579,7 @@ ice_construct_skb_zc(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp)
 
 	net_prefetch(xdp->data_meta);
 
-	skb = __napi_alloc_skb(&rx_ring->q_vector->napi, totalsize,
-			       GFP_ATOMIC | __GFP_NOWARN);
+	skb = napi_alloc_skb(&rx_ring->q_vector->napi, totalsize);
 	if (unlikely(!skb))
 		return NULL;
 
diff --git a/drivers/net/ethernet/intel/idpf/idpf_txrx.c b/drivers/net/ethernet/intel/idpf/idpf_txrx.c
index 4a7062c55fcb..2fd55d454984 100644
--- a/drivers/net/ethernet/intel/idpf/idpf_txrx.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_txrx.c
@@ -3007,8 +3007,7 @@ struct sk_buff *idpf_rx_construct_skb(struct idpf_queue *rxq,
 	/* prefetch first cache line of first page */
 	net_prefetch(va);
 	/* allocate a skb to store the frags */
-	skb = __napi_alloc_skb(&rxq->q_vector->napi, IDPF_RX_HDR_SIZE,
-			       GFP_ATOMIC);
+	skb = napi_alloc_skb(&rxq->q_vector->napi, IDPF_RX_HDR_SIZE);
 	if (unlikely(!skb)) {
 		idpf_rx_put_page(rx_buf);
 
@@ -3062,7 +3061,7 @@ static struct sk_buff *idpf_rx_hdr_construct_skb(struct idpf_queue *rxq,
 	struct sk_buff *skb;
 
 	/* allocate a skb to store the frags */
-	skb = __napi_alloc_skb(&rxq->q_vector->napi, size, GFP_ATOMIC);
+	skb = napi_alloc_skb(&rxq->q_vector->napi, size);
 	if (unlikely(!skb))
 		return NULL;
 
diff --git a/drivers/net/ethernet/intel/igc/igc_main.c b/drivers/net/ethernet/intel/igc/igc_main.c
index eaa4005ef616..409b7f1eebad 100644
--- a/drivers/net/ethernet/intel/igc/igc_main.c
+++ b/drivers/net/ethernet/intel/igc/igc_main.c
@@ -2681,8 +2681,7 @@ static struct sk_buff *igc_construct_skb_zc(struct igc_ring *ring,
 
 	net_prefetch(xdp->data_meta);
 
-	skb = __napi_alloc_skb(&ring->q_vector->napi, totalsize,
-			       GFP_ATOMIC | __GFP_NOWARN);
+	skb = napi_alloc_skb(&ring->q_vector->napi, totalsize);
 	if (unlikely(!skb))
 		return NULL;
 
diff --git a/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c b/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
index 1703c640a434..4a90a1380be4 100644
--- a/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
+++ b/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c
@@ -220,8 +220,7 @@ static struct sk_buff *ixgbe_construct_skb_zc(struct ixgbe_ring *rx_ring,
 	net_prefetch(xdp->data_meta);
 
 	/* allocate a skb to store the frags */
-	skb = __napi_alloc_skb(&rx_ring->q_vector->napi, totalsize,
-			       GFP_ATOMIC | __GFP_NOWARN);
+	skb = napi_alloc_skb(&rx_ring->q_vector->napi, totalsize);
 	if (unlikely(!skb))
 		return NULL;
 
* Unmerged path drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 197c9f9ded5f..d1c9fb9e898d 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -2975,13 +2975,7 @@ static inline void *napi_alloc_frag_align(unsigned int fragsz,
 	return __napi_alloc_frag_align(fragsz, -align);
 }
 
-struct sk_buff *__napi_alloc_skb(struct napi_struct *napi,
-				 unsigned int length, gfp_t gfp_mask);
-static inline struct sk_buff *napi_alloc_skb(struct napi_struct *napi,
-					     unsigned int length)
-{
-	return __napi_alloc_skb(napi, length, GFP_ATOMIC);
-}
+struct sk_buff *napi_alloc_skb(struct napi_struct *napi, unsigned int length);
 void napi_consume_skb(struct sk_buff *skb, int budget);
 
 void napi_skb_free_stolen_head(struct sk_buff *skb);
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index c2c53c1c31aa..aca6313cb05d 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -605,10 +605,9 @@ struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,
 EXPORT_SYMBOL(__netdev_alloc_skb);
 
 /**
- *	__napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance
+ *	napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance
  *	@napi: napi instance this buffer was allocated for
  *	@len: length to allocate
- *	@gfp_mask: get_free_pages mask, passed to alloc_skb and alloc_pages
  *
  *	Allocate a new sk_buff for use in NAPI receive.  This buffer will
  *	attempt to allocate the head from a special reserved region used
@@ -617,9 +616,9 @@ EXPORT_SYMBOL(__netdev_alloc_skb);
  *
  *	%NULL is returned if there is no free memory.
  */
-struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,
-				 gfp_t gfp_mask)
+struct sk_buff *napi_alloc_skb(struct napi_struct *napi, unsigned int len)
 {
+	gfp_t gfp_mask = GFP_ATOMIC | __GFP_NOWARN;
 	struct napi_alloc_cache *nc;
 	struct sk_buff *skb;
 	bool pfmemalloc;
@@ -693,7 +692,7 @@ struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,
 skb_fail:
 	return skb;
 }
-EXPORT_SYMBOL(__napi_alloc_skb);
+EXPORT_SYMBOL(napi_alloc_skb);
 
 void skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,
 		     int size, unsigned int truesize)
