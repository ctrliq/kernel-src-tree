x86/cpu/amd: Move the errata checking functionality up

jira LE-1907
cve CVE-2023-20593
Rebuild_History Non-Buildable kernel-4.18.0-508.el8
commit-author Borislav Petkov (AMD) <bp@alien8.de>
commit 8b6f687743dacce83dbb0c7cfacf88bab00f808a
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-508.el8/8b6f6877.failed

Avoid new and remove old forward declarations.

No functional changes.

	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
(cherry picked from commit 8b6f687743dacce83dbb0c7cfacf88bab00f808a)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/cpu/amd.c
diff --cc arch/x86/kernel/cpu/amd.c
index 717c38189e3a,27e177f7ef43..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -1145,91 -1177,43 +1207,104 @@@ static const struct cpu_dev amd_cpu_de
  
  cpu_dev_register(amd_cpu_dev);
  
++<<<<<<< HEAD
 +/*
 + * AMD errata checking
 + *
 + * Errata are defined as arrays of ints using the AMD_LEGACY_ERRATUM() or
 + * AMD_OSVW_ERRATUM() macros. The latter is intended for newer errata that
 + * have an OSVW id assigned, which it takes as first argument. Both take a
 + * variable number of family-specific model-stepping ranges created by
 + * AMD_MODEL_RANGE().
 + *
 + * Example:
 + *
 + * const int amd_erratum_319[] =
 + *	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0x4, 0x2),
 + *			   AMD_MODEL_RANGE(0x10, 0x8, 0x0, 0x8, 0x0),
 + *			   AMD_MODEL_RANGE(0x10, 0x9, 0x0, 0x9, 0x0));
 + */
 +
 +#define AMD_LEGACY_ERRATUM(...)		{ -1, __VA_ARGS__, 0 }
 +#define AMD_OSVW_ERRATUM(osvw_id, ...)	{ osvw_id, __VA_ARGS__, 0 }
 +#define AMD_MODEL_RANGE(f, m_start, s_start, m_end, s_end) \
 +	((f << 24) | (m_start << 16) | (s_start << 12) | (m_end << 4) | (s_end))
 +#define AMD_MODEL_RANGE_FAMILY(range)	(((range) >> 24) & 0xff)
 +#define AMD_MODEL_RANGE_START(range)	(((range) >> 12) & 0xfff)
 +#define AMD_MODEL_RANGE_END(range)	((range) & 0xfff)
 +
 +static const int amd_erratum_400[] =
 +	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
 +			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
 +
 +static const int amd_erratum_383[] =
 +	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
 +
 +/* #1054: Instructions Retired Performance Counter May Be Inaccurate */
 +static const int amd_erratum_1054[] =
 +	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x17, 0, 0, 0x2f, 0xf));
 +
 +static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
 +{
 +	int osvw_id = *erratum++;
 +	u32 range;
 +	u32 ms;
 +
 +	if (osvw_id >= 0 && osvw_id < 65536 &&
 +	    cpu_has(cpu, X86_FEATURE_OSVW)) {
 +		u64 osvw_len;
 +
 +		rdmsrl(MSR_AMD64_OSVW_ID_LENGTH, osvw_len);
 +		if (osvw_id < osvw_len) {
 +			u64 osvw_bits;
 +
 +			rdmsrl(MSR_AMD64_OSVW_STATUS + (osvw_id >> 6),
 +			    osvw_bits);
 +			return osvw_bits & (1ULL << (osvw_id & 0x3f));
 +		}
 +	}
 +
 +	/* OSVW unavailable or ID unknown, match family-model-stepping range */
 +	ms = (cpu->x86_model << 4) | cpu->x86_stepping;
 +	while ((range = *erratum++))
 +		if ((cpu->x86 == AMD_MODEL_RANGE_FAMILY(range)) &&
 +		    (ms >= AMD_MODEL_RANGE_START(range)) &&
 +		    (ms <= AMD_MODEL_RANGE_END(range)))
 +			return true;
 +
 +	return false;
 +}
 +
 +void set_dr_addr_mask(unsigned long mask, int dr)
++=======
+ static DEFINE_PER_CPU_READ_MOSTLY(unsigned long[4], amd_dr_addr_mask);
+ 
+ static unsigned int amd_msr_dr_addr_masks[] = {
+ 	MSR_F16H_DR0_ADDR_MASK,
+ 	MSR_F16H_DR1_ADDR_MASK,
+ 	MSR_F16H_DR1_ADDR_MASK + 1,
+ 	MSR_F16H_DR1_ADDR_MASK + 2
+ };
+ 
+ void amd_set_dr_addr_mask(unsigned long mask, unsigned int dr)
++>>>>>>> 8b6f687743da (x86/cpu/amd: Move the errata checking functionality up)
  {
 -	int cpu = smp_processor_id();
 -
 -	if (!cpu_feature_enabled(X86_FEATURE_BPEXT))
 -		return;
 -
 -	if (WARN_ON_ONCE(dr >= ARRAY_SIZE(amd_msr_dr_addr_masks)))
 +	if (!boot_cpu_has(X86_FEATURE_BPEXT))
  		return;
  
 -	if (per_cpu(amd_dr_addr_mask, cpu)[dr] == mask)
 -		return;
 -
 -	wrmsr(amd_msr_dr_addr_masks[dr], mask, 0);
 -	per_cpu(amd_dr_addr_mask, cpu)[dr] = mask;
 -}
 -
 -unsigned long amd_get_dr_addr_mask(unsigned int dr)
 -{
 -	if (!cpu_feature_enabled(X86_FEATURE_BPEXT))
 -		return 0;
 -
 -	if (WARN_ON_ONCE(dr >= ARRAY_SIZE(amd_msr_dr_addr_masks)))
 -		return 0;
 -
 -	return per_cpu(amd_dr_addr_mask[dr], smp_processor_id());
 +	switch (dr) {
 +	case 0:
 +		wrmsr(MSR_F16H_DR0_ADDR_MASK, mask, 0);
 +		break;
 +	case 1:
 +	case 2:
 +	case 3:
 +		wrmsr(MSR_F16H_DR1_ADDR_MASK - 1 + dr, mask, 0);
 +		break;
 +	default:
 +		break;
 +	}
  }
 -EXPORT_SYMBOL_GPL(amd_get_dr_addr_mask);
  
  u32 amd_get_highest_perf(void)
  {
* Unmerged path arch/x86/kernel/cpu/amd.c
