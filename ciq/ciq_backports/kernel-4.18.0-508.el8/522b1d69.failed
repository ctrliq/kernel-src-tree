x86/cpu/amd: Add a Zenbleed fix

jira LE-1907
cve CVE-2023-20593
Rebuild_History Non-Buildable kernel-4.18.0-508.el8
commit-author Borislav Petkov (AMD) <bp@alien8.de>
commit 522b1d69219d8f083173819fde04f994aa051a98
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-508.el8/522b1d69.failed

Add a fix for the Zen2 VZEROUPPER data corruption bug where under
certain circumstances executing VZEROUPPER can cause register
corruption or leak data.

The optimal fix is through microcode but in the case the proper
microcode revision has not been applied, enable a fallback fix using
a chicken bit.

	Signed-off-by: Borislav Petkov (AMD) <bp@alien8.de>
(cherry picked from commit 522b1d69219d8f083173819fde04f994aa051a98)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/msr-index.h
#	arch/x86/kernel/cpu/amd.c
diff --cc arch/x86/include/asm/msr-index.h
index bc0f2eb02f5a,a00a53e15ab7..000000000000
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@@ -501,8 -538,15 +501,17 @@@
  #define MSR_AMD64_OSVW_STATUS		0xc0010141
  #define MSR_AMD_PPIN_CTL		0xc00102f0
  #define MSR_AMD_PPIN			0xc00102f1
 -#define MSR_AMD64_CPUID_FN_1		0xc0011004
  #define MSR_AMD64_LS_CFG		0xc0011020
  #define MSR_AMD64_DC_CFG		0xc0011022
++<<<<<<< HEAD
++=======
+ 
+ #define MSR_AMD64_DE_CFG		0xc0011029
+ #define MSR_AMD64_DE_CFG_LFENCE_SERIALIZE_BIT	 1
+ #define MSR_AMD64_DE_CFG_LFENCE_SERIALIZE	BIT_ULL(MSR_AMD64_DE_CFG_LFENCE_SERIALIZE_BIT)
+ #define MSR_AMD64_DE_CFG_ZEN2_FP_BACKUP_FIX_BIT 9
+ 
++>>>>>>> 522b1d69219d (x86/cpu/amd: Add a Zenbleed fix)
  #define MSR_AMD64_BU_CFG2		0xc001102a
  #define MSR_AMD64_IBSFETCHCTL		0xc0011030
  #define MSR_AMD64_IBSFETCHLINAD		0xc0011031
diff --cc arch/x86/kernel/cpu/amd.c
index 717c38189e3a,26ad7ca423e7..000000000000
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@@ -38,6 -34,78 +38,81 @@@ static bool cpu_has_amd_erratum(struct 
   */
  static u32 nodes_per_socket = 1;
  
++<<<<<<< HEAD
++=======
+ /*
+  * AMD errata checking
+  *
+  * Errata are defined as arrays of ints using the AMD_LEGACY_ERRATUM() or
+  * AMD_OSVW_ERRATUM() macros. The latter is intended for newer errata that
+  * have an OSVW id assigned, which it takes as first argument. Both take a
+  * variable number of family-specific model-stepping ranges created by
+  * AMD_MODEL_RANGE().
+  *
+  * Example:
+  *
+  * const int amd_erratum_319[] =
+  *	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0x4, 0x2),
+  *			   AMD_MODEL_RANGE(0x10, 0x8, 0x0, 0x8, 0x0),
+  *			   AMD_MODEL_RANGE(0x10, 0x9, 0x0, 0x9, 0x0));
+  */
+ 
+ #define AMD_LEGACY_ERRATUM(...)		{ -1, __VA_ARGS__, 0 }
+ #define AMD_OSVW_ERRATUM(osvw_id, ...)	{ osvw_id, __VA_ARGS__, 0 }
+ #define AMD_MODEL_RANGE(f, m_start, s_start, m_end, s_end) \
+ 	((f << 24) | (m_start << 16) | (s_start << 12) | (m_end << 4) | (s_end))
+ #define AMD_MODEL_RANGE_FAMILY(range)	(((range) >> 24) & 0xff)
+ #define AMD_MODEL_RANGE_START(range)	(((range) >> 12) & 0xfff)
+ #define AMD_MODEL_RANGE_END(range)	((range) & 0xfff)
+ 
+ static const int amd_erratum_400[] =
+ 	AMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),
+ 			    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));
+ 
+ static const int amd_erratum_383[] =
+ 	AMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));
+ 
+ /* #1054: Instructions Retired Performance Counter May Be Inaccurate */
+ static const int amd_erratum_1054[] =
+ 	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x17, 0, 0, 0x2f, 0xf));
+ 
+ static const int amd_zenbleed[] =
+ 	AMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x17, 0x30, 0x0, 0x4f, 0xf),
+ 			   AMD_MODEL_RANGE(0x17, 0x60, 0x0, 0x7f, 0xf),
+ 			   AMD_MODEL_RANGE(0x17, 0xa0, 0x0, 0xaf, 0xf));
+ 
+ static bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)
+ {
+ 	int osvw_id = *erratum++;
+ 	u32 range;
+ 	u32 ms;
+ 
+ 	if (osvw_id >= 0 && osvw_id < 65536 &&
+ 	    cpu_has(cpu, X86_FEATURE_OSVW)) {
+ 		u64 osvw_len;
+ 
+ 		rdmsrl(MSR_AMD64_OSVW_ID_LENGTH, osvw_len);
+ 		if (osvw_id < osvw_len) {
+ 			u64 osvw_bits;
+ 
+ 			rdmsrl(MSR_AMD64_OSVW_STATUS + (osvw_id >> 6),
+ 			    osvw_bits);
+ 			return osvw_bits & (1ULL << (osvw_id & 0x3f));
+ 		}
+ 	}
+ 
+ 	/* OSVW unavailable or ID unknown, match family-model-stepping range */
+ 	ms = (cpu->x86_model << 4) | cpu->x86_stepping;
+ 	while ((range = *erratum++))
+ 		if ((cpu->x86 == AMD_MODEL_RANGE_FAMILY(range)) &&
+ 		    (ms >= AMD_MODEL_RANGE_START(range)) &&
+ 		    (ms <= AMD_MODEL_RANGE_END(range)))
+ 			return true;
+ 
+ 	return false;
+ }
+ 
++>>>>>>> 522b1d69219d (x86/cpu/amd: Add a Zenbleed fix)
  static inline int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)
  {
  	u32 gprs[8] = { 0 };
@@@ -1050,6 -1117,19 +1166,22 @@@ static void init_amd(struct cpuinfo_x8
  		msr_set_bit(MSR_K7_HWCR, MSR_K7_HWCR_IRPERF_EN_BIT);
  
  	check_null_seg_clears_base(c);
++<<<<<<< HEAD
++=======
+ 
+ 	/*
+ 	 * Make sure EFER[AIBRSE - Automatic IBRS Enable] is set. The APs are brought up
+ 	 * using the trampoline code and as part of it, MSR_EFER gets prepared there in
+ 	 * order to be replicated onto them. Regardless, set it here again, if not set,
+ 	 * to protect against any future refactoring/code reorganization which might
+ 	 * miss setting this important bit.
+ 	 */
+ 	if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ 	    cpu_has(c, X86_FEATURE_AUTOIBRS))
+ 		WARN_ON_ONCE(msr_set_bit(MSR_EFER, _EFER_AUTOIBRS));
+ 
+ 	zenbleed_check(c);
++>>>>>>> 522b1d69219d (x86/cpu/amd: Add a Zenbleed fix)
  }
  
  #ifdef CONFIG_X86_32
diff --git a/arch/x86/include/asm/microcode.h b/arch/x86/include/asm/microcode.h
index 320566a0443d..66dbba181bd9 100644
--- a/arch/x86/include/asm/microcode.h
+++ b/arch/x86/include/asm/microcode.h
@@ -5,6 +5,7 @@
 #include <asm/cpu.h>
 #include <linux/earlycpio.h>
 #include <linux/initrd.h>
+#include <asm/microcode_amd.h>
 
 struct ucode_patch {
 	struct list_head plist;
diff --git a/arch/x86/include/asm/microcode_amd.h b/arch/x86/include/asm/microcode_amd.h
index e6662adf3af4..9675c621c1ca 100644
--- a/arch/x86/include/asm/microcode_amd.h
+++ b/arch/x86/include/asm/microcode_amd.h
@@ -48,11 +48,13 @@ extern void __init load_ucode_amd_bsp(unsigned int family);
 extern void load_ucode_amd_ap(unsigned int family);
 extern int __init save_microcode_in_initrd_amd(unsigned int family);
 void reload_ucode_amd(unsigned int cpu);
+extern void amd_check_microcode(void);
 #else
 static inline void __init load_ucode_amd_bsp(unsigned int family) {}
 static inline void load_ucode_amd_ap(unsigned int family) {}
 static inline int __init
 save_microcode_in_initrd_amd(unsigned int family) { return -EINVAL; }
 static inline void reload_ucode_amd(unsigned int cpu) {}
+static inline void amd_check_microcode(void) {}
 #endif
 #endif /* _ASM_X86_MICROCODE_AMD_H */
* Unmerged path arch/x86/include/asm/msr-index.h
* Unmerged path arch/x86/kernel/cpu/amd.c
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index a9ac066dd8b3..1a08c4174be7 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -2217,6 +2217,8 @@ void microcode_check(struct cpuinfo_x86 *prev_info)
 
 	perf_check_microcode();
 
+	amd_check_microcode();
+
 	store_cpu_caps(&curr_info);
 
 	if (!memcmp(&prev_info->x86_capability, &curr_info.x86_capability,
