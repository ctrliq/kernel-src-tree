dm mpath: replace spin_lock_irqsave with spin_lock_irq

jira LE-3526
Rebuild_History Non-Buildable kernel-6.12.0-55.20.1.el10_0
commit-author Mikulas Patocka <mpatocka@redhat.com>
commit 050a3e71ce24c6f18d70679d68056f76375ff51c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-6.12.0-55.20.1.el10_0/050a3e71.failed

Replace spin_lock_irqsave/spin_unlock_irqrestore with
spin_lock_irq/spin_unlock_irq at places where it is known that interrupts
are enabled.

	Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
	Signed-off-by: Benjamin Marzinski <bmarzins@redhat.com>
(cherry picked from commit 050a3e71ce24c6f18d70679d68056f76375ff51c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/md/dm-mpath.c
diff --cc drivers/md/dm-mpath.c
index 368606afb6f0,81fec2e1e0ef..000000000000
--- a/drivers/md/dm-mpath.c
+++ b/drivers/md/dm-mpath.c
@@@ -1476,11 -1478,15 +1469,11 @@@ static int switch_pg_num(struct multipa
  		if (--pgnum)
  			continue;
  
 -		if (test_bit(MPATHF_DELAY_PG_SWITCH, &m->flags))
 -			set_bit(MPATHF_NEED_PG_SWITCH, &m->flags);
 -		else {
 -			m->current_pgpath = NULL;
 -			m->current_pg = NULL;
 -		}
 +		m->current_pgpath = NULL;
 +		m->current_pg = NULL;
  		m->next_pg = pg;
  	}
- 	spin_unlock_irqrestore(&m->lock, flags);
+ 	spin_unlock_irq(&m->lock);
  
  	schedule_work(&m->trigger_event);
  	return 0;
@@@ -1742,6 -1748,9 +1735,12 @@@ static void multipath_presuspend(struc
  {
  	struct multipath *m = ti->private;
  
++<<<<<<< HEAD
++=======
+ 	spin_lock_irq(&m->lock);
+ 	m->is_suspending = true;
+ 	spin_unlock_irq(&m->lock);
++>>>>>>> 050a3e71ce24 (dm mpath: replace spin_lock_irqsave with spin_lock_irq)
  	/* FIXME: bio-based shouldn't need to always disable queue_if_no_path */
  	if (m->queue_mode == DM_TYPE_BIO_BASED || !dm_noflush_suspending(m->ti))
  		queue_if_no_path(m, false, true, __func__);
@@@ -1762,9 -1771,9 +1761,13 @@@ static void multipath_postsuspend(struc
  static void multipath_resume(struct dm_target *ti)
  {
  	struct multipath *m = ti->private;
- 	unsigned long flags;
  
++<<<<<<< HEAD
 +	spin_lock_irqsave(&m->lock, flags);
++=======
+ 	spin_lock_irq(&m->lock);
+ 	m->is_suspending = false;
++>>>>>>> 050a3e71ce24 (dm mpath: replace spin_lock_irqsave with spin_lock_irq)
  	if (test_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags)) {
  		set_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags);
  		clear_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags);
@@@ -2021,6 -2028,113 +2022,116 @@@ out
  	return r;
  }
  
++<<<<<<< HEAD
++=======
+ /*
+  * Perform a minimal read from the given path to find out whether the
+  * path still works.  If a path error occurs, fail it.
+  */
+ static int probe_path(struct pgpath *pgpath)
+ {
+ 	struct block_device *bdev = pgpath->path.dev->bdev;
+ 	unsigned int read_size = bdev_logical_block_size(bdev);
+ 	struct page *page;
+ 	struct bio *bio;
+ 	blk_status_t status;
+ 	int r = 0;
+ 
+ 	if (WARN_ON_ONCE(read_size > PAGE_SIZE))
+ 		return -EINVAL;
+ 
+ 	page = alloc_page(GFP_KERNEL);
+ 	if (!page)
+ 		return -ENOMEM;
+ 
+ 	/* Perform a minimal read: Sector 0, length read_size */
+ 	bio = bio_alloc(bdev, 1, REQ_OP_READ, GFP_KERNEL);
+ 	if (!bio) {
+ 		r = -ENOMEM;
+ 		goto out;
+ 	}
+ 
+ 	bio->bi_iter.bi_sector = 0;
+ 	__bio_add_page(bio, page, read_size, 0);
+ 	submit_bio_wait(bio);
+ 	status = bio->bi_status;
+ 	bio_put(bio);
+ 
+ 	if (status && blk_path_error(status))
+ 		fail_path(pgpath);
+ 
+ out:
+ 	__free_page(page);
+ 	return r;
+ }
+ 
+ /*
+  * Probe all active paths in current_pg to find out whether they still work.
+  * Fail all paths that do not work.
+  *
+  * Return -ENOTCONN if no valid path is left (even outside of current_pg). We
+  * cannot probe paths in other pgs without switching current_pg, so if valid
+  * paths are only in different pgs, they may or may not work. Additionally
+  * we should not probe paths in a pathgroup that is in the process of
+  * Initializing. Userspace can submit a request and we'll switch and wait
+  * for the pathgroup to be initialized. If the request fails, it may need to
+  * probe again.
+  */
+ static int probe_active_paths(struct multipath *m)
+ {
+ 	struct pgpath *pgpath;
+ 	struct priority_group *pg = NULL;
+ 	int r = 0;
+ 
+ 	spin_lock_irq(&m->lock);
+ 	if (test_bit(MPATHF_DELAY_PG_SWITCH, &m->flags)) {
+ 		wait_event_lock_irq(m->probe_wait,
+ 				    !test_bit(MPATHF_DELAY_PG_SWITCH, &m->flags),
+ 				    m->lock);
+ 		/*
+ 		 * if we waited because a probe was already in progress,
+ 		 * and it probed the current active pathgroup, don't
+ 		 * reprobe. Just return the number of valid paths
+ 		 */
+ 		if (m->current_pg == m->last_probed_pg)
+ 			goto skip_probe;
+ 	}
+ 	if (!m->current_pg || m->is_suspending ||
+ 	    test_bit(MPATHF_QUEUE_IO, &m->flags))
+ 		goto skip_probe;
+ 	set_bit(MPATHF_DELAY_PG_SWITCH, &m->flags);
+ 	pg = m->last_probed_pg = m->current_pg;
+ 	spin_unlock_irq(&m->lock);
+ 
+ 	list_for_each_entry(pgpath, &pg->pgpaths, list) {
+ 		if (pg != READ_ONCE(m->current_pg) ||
+ 		    READ_ONCE(m->is_suspending))
+ 			goto out;
+ 		if (!pgpath->is_active)
+ 			continue;
+ 
+ 		r = probe_path(pgpath);
+ 		if (r < 0)
+ 			goto out;
+ 	}
+ 
+ out:
+ 	spin_lock_irq(&m->lock);
+ 	clear_bit(MPATHF_DELAY_PG_SWITCH, &m->flags);
+ 	if (test_and_clear_bit(MPATHF_NEED_PG_SWITCH, &m->flags)) {
+ 		m->current_pgpath = NULL;
+ 		m->current_pg = NULL;
+ 	}
+ skip_probe:
+ 	if (r == 0 && !atomic_read(&m->nr_valid_paths))
+ 		r = -ENOTCONN;
+ 	spin_unlock_irq(&m->lock);
+ 	if (pg)
+ 		wake_up(&m->probe_wait);
+ 	return r;
+ }
+ 
++>>>>>>> 050a3e71ce24 (dm mpath: replace spin_lock_irqsave with spin_lock_irq)
  static int multipath_prepare_ioctl(struct dm_target *ti,
  				   struct block_device **bdev,
  				   unsigned int cmd, unsigned long arg,
@@@ -2028,9 -2142,18 +2139,8 @@@
  {
  	struct multipath *m = ti->private;
  	struct pgpath *pgpath;
- 	unsigned long flags;
  	int r;
  
 -	if (_IOC_TYPE(cmd) == DM_IOCTL) {
 -		*forward = false;
 -		switch (cmd) {
 -		case DM_MPATH_PROBE_PATHS:
 -			return probe_active_paths(m);
 -		default:
 -			return -ENOTTY;
 -		}
 -	}
 -
  	pgpath = READ_ONCE(m->current_pgpath);
  	if (!pgpath || !mpath_double_check_test_bit(MPATHF_QUEUE_IO, m))
  		pgpath = choose_pgpath(m, 0);
* Unmerged path drivers/md/dm-mpath.c
