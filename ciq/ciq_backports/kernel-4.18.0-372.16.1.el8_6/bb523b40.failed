gup: Turn fault_in_pages_{readable,writeable} into fault_in_{readable,writeable}

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-372.16.1.el8_6
commit-author Andreas Gruenbacher <agruenba@redhat.com>
commit bb523b406c849eef8f265a07cd7f320f1f177743
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-372.16.1.el8_6/bb523b40.failed

Turn fault_in_pages_{readable,writeable} into versions that return the
number of bytes not faulted in, similar to copy_to_user, instead of
returning a non-zero value when any of the requested pages couldn't be
faulted in.  This supports the existing users that require all pages to
be faulted in as well as new users that are happy if any pages can be
faulted in.

Rename the functions to fault_in_{readable,writeable} to make sure
this change doesn't silently break things.

Neither of these functions is entirely trivial and it doesn't seem
useful to inline them, so move them to mm/gup.c.

	Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
(cherry picked from commit bb523b406c849eef8f265a07cd7f320f1f177743)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/fpu/signal.c
#	fs/btrfs/ioctl.c
#	mm/filemap.c
#	mm/gup.c
diff --cc arch/x86/kernel/fpu/signal.c
index cc977da6e128,164c96434704..000000000000
--- a/arch/x86/kernel/fpu/signal.c
+++ b/arch/x86/kernel/fpu/signal.c
@@@ -233,9 -205,9 +233,13 @@@ retry
  	fpregs_unlock();
  
  	if (ret) {
++<<<<<<< HEAD
 +		if (!__clear_user(buf_fx, fpstate->user_size))
++=======
+ 		if (!fault_in_writeable(buf_fx, fpu_user_xstate_size))
++>>>>>>> bb523b406c84 (gup: Turn fault_in_pages_{readable,writeable} into fault_in_{readable,writeable})
  			goto retry;
 -		return -EFAULT;
 +		return false;
  	}
  
  	/* Save the fsave header for the 32-bit frames. */
@@@ -306,12 -275,12 +310,18 @@@ retry
  		fpregs_unlock();
  
  		/* Try to handle #PF, but anything else is fatal. */
 -		if (ret != -EFAULT)
 -			return -EINVAL;
 +		if (ret != X86_TRAP_PF)
 +			return false;
  
++<<<<<<< HEAD
 +		if (!fault_in_pages_readable(buf, size))
 +			goto retry;
 +		return false;
++=======
+ 		if (!fault_in_readable(buf, size))
+ 			goto retry;
+ 		return -EFAULT;
++>>>>>>> bb523b406c84 (gup: Turn fault_in_pages_{readable,writeable} into fault_in_{readable,writeable})
  	}
  
  	/*
diff --cc fs/btrfs/ioctl.c
index 197de3adb8a8,c0739f0af634..000000000000
--- a/fs/btrfs/ioctl.c
+++ b/fs/btrfs/ioctl.c
@@@ -2153,6 -2261,10 +2153,13 @@@ static noinline int search_ioctl(struc
  	key.offset = sk->min_offset;
  
  	while (1) {
++<<<<<<< HEAD
++=======
+ 		ret = -EFAULT;
+ 		if (fault_in_writeable(ubuf + sk_offset, *buf_size - sk_offset))
+ 			break;
+ 
++>>>>>>> bb523b406c84 (gup: Turn fault_in_pages_{readable,writeable} into fault_in_{readable,writeable})
  		ret = btrfs_search_forward(root, &key, path, sk->min_transid);
  		if (ret != 0) {
  			if (ret > 0)
diff --cc mm/filemap.c
index 37e2ef20fa83,ff34f4087f87..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -83,10 -86,11 +83,15 @@@
   *        ->i_pages lock	(arch-dependent flush_dcache_mmap_lock)
   *
   *  ->mmap_lock
 - *    ->invalidate_lock		(filemap_fault)
 - *      ->lock_page		(filemap_fault, access_process_vm)
 + *    ->lock_page		(access_process_vm)
   *
++<<<<<<< HEAD
 + *  ->i_mutex			(generic_perform_write)
 + *    ->mmap_lock		(fault_in_pages_readable->do_page_fault)
++=======
+  *  ->i_rwsem			(generic_perform_write)
+  *    ->mmap_lock		(fault_in_readable->do_page_fault)
++>>>>>>> bb523b406c84 (gup: Turn fault_in_pages_{readable,writeable} into fault_in_{readable,writeable})
   *
   *  bdi->wb.list_lock
   *    sb_lock			(fs/fs-writeback.c)
diff --cc mm/gup.c
index 992475088545,a7efb027d6cf..000000000000
--- a/mm/gup.c
+++ b/mm/gup.c
@@@ -1652,40 -1656,128 +1652,131 @@@ finish_or_fault
  }
  #endif /* !CONFIG_MMU */
  
++<<<<<<< HEAD
 +#if defined(CONFIG_FS_DAX) || defined (CONFIG_CMA)
 +static bool check_dax_vmas(struct vm_area_struct **vmas, long nr_pages)
++=======
+ /**
+  * fault_in_writeable - fault in userspace address range for writing
+  * @uaddr: start of address range
+  * @size: size of address range
+  *
+  * Returns the number of bytes not faulted in (like copy_to_user() and
+  * copy_from_user()).
+  */
+ size_t fault_in_writeable(char __user *uaddr, size_t size)
+ {
+ 	char __user *start = uaddr, *end;
+ 
+ 	if (unlikely(size == 0))
+ 		return 0;
+ 	if (!PAGE_ALIGNED(uaddr)) {
+ 		if (unlikely(__put_user(0, uaddr) != 0))
+ 			return size;
+ 		uaddr = (char __user *)PAGE_ALIGN((unsigned long)uaddr);
+ 	}
+ 	end = (char __user *)PAGE_ALIGN((unsigned long)start + size);
+ 	if (unlikely(end < start))
+ 		end = NULL;
+ 	while (uaddr != end) {
+ 		if (unlikely(__put_user(0, uaddr) != 0))
+ 			goto out;
+ 		uaddr += PAGE_SIZE;
+ 	}
+ 
+ out:
+ 	if (size > uaddr - start)
+ 		return size - (uaddr - start);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(fault_in_writeable);
+ 
+ /**
+  * fault_in_readable - fault in userspace address range for reading
+  * @uaddr: start of user address range
+  * @size: size of user address range
+  *
+  * Returns the number of bytes not faulted in (like copy_to_user() and
+  * copy_from_user()).
+  */
+ size_t fault_in_readable(const char __user *uaddr, size_t size)
+ {
+ 	const char __user *start = uaddr, *end;
+ 	volatile char c;
+ 
+ 	if (unlikely(size == 0))
+ 		return 0;
+ 	if (!PAGE_ALIGNED(uaddr)) {
+ 		if (unlikely(__get_user(c, uaddr) != 0))
+ 			return size;
+ 		uaddr = (const char __user *)PAGE_ALIGN((unsigned long)uaddr);
+ 	}
+ 	end = (const char __user *)PAGE_ALIGN((unsigned long)start + size);
+ 	if (unlikely(end < start))
+ 		end = NULL;
+ 	while (uaddr != end) {
+ 		if (unlikely(__get_user(c, uaddr) != 0))
+ 			goto out;
+ 		uaddr += PAGE_SIZE;
+ 	}
+ 
+ out:
+ 	(void)c;
+ 	if (size > uaddr - start)
+ 		return size - (uaddr - start);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(fault_in_readable);
+ 
+ /**
+  * get_dump_page() - pin user page in memory while writing it to core dump
+  * @addr: user address
+  *
+  * Returns struct page pointer of user page pinned for dump,
+  * to be freed afterwards by put_page().
+  *
+  * Returns NULL on any kind of failure - a hole must then be inserted into
+  * the corefile, to preserve alignment with its headers; and also returns
+  * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -
+  * allowing a hole to be left in the corefile to save disk space.
+  *
+  * Called without mmap_lock (takes and releases the mmap_lock by itself).
+  */
+ #ifdef CONFIG_ELF_CORE
+ struct page *get_dump_page(unsigned long addr)
++>>>>>>> bb523b406c84 (gup: Turn fault_in_pages_{readable,writeable} into fault_in_{readable,writeable})
  {
 -	struct mm_struct *mm = current->mm;
 -	struct page *page;
 -	int locked = 1;
 -	int ret;
 +	long i;
 +	struct vm_area_struct *vma_prev = NULL;
  
 -	if (mmap_read_lock_killable(mm))
 -		return NULL;
 -	ret = __get_user_pages_locked(mm, addr, 1, &page, NULL, &locked,
 -				      FOLL_FORCE | FOLL_DUMP | FOLL_GET);
 -	if (locked)
 -		mmap_read_unlock(mm);
 -	return (ret == 1) ? page : NULL;
 +	for (i = 0; i < nr_pages; i++) {
 +		struct vm_area_struct *vma = vmas[i];
 +
 +		if (vma == vma_prev)
 +			continue;
 +
 +		vma_prev = vma;
 +
 +		if (vma_is_fsdax(vma))
 +			return true;
 +	}
 +	return false;
  }
 -#endif /* CONFIG_ELF_CORE */
  
 -#ifdef CONFIG_MIGRATION
 -/*
 - * Check whether all pages are pinnable, if so return number of pages.  If some
 - * pages are not pinnable, migrate them, and unpin all pages. Return zero if
 - * pages were migrated, or if some pages were not successfully isolated.
 - * Return negative error if migration fails.
 - */
 -static long check_and_migrate_movable_pages(unsigned long nr_pages,
 -					    struct page **pages,
 -					    unsigned int gup_flags)
 -{
 -	unsigned long i;
 -	unsigned long isolation_error_count = 0;
 -	bool drain_allow = true;
 -	LIST_HEAD(movable_page_list);
 -	long ret = 0;
 -	struct page *prev_head = NULL;
 -	struct page *head;
 +#ifdef CONFIG_CMA
 +static long check_and_migrate_cma_pages(struct task_struct *tsk,
 +					struct mm_struct *mm,
 +					unsigned long start,
 +					unsigned long nr_pages,
 +					struct page **pages,
 +					struct vm_area_struct **vmas,
 +					unsigned int gup_flags)
 +{
 +	unsigned long i, isolation_error_count;
 +	bool drain_allow;
 +	LIST_HEAD(cma_page_list);
 +	long ret = nr_pages;
 +	struct page *prev_head, *head;
  	struct migration_target_control mtc = {
  		.nid = NUMA_NO_NODE,
  		.gfp_mask = GFP_USER | __GFP_NOWARN,
diff --git a/arch/powerpc/kernel/kvm.c b/arch/powerpc/kernel/kvm.c
index 2283b9bfd2d1..41740736c95b 100644
--- a/arch/powerpc/kernel/kvm.c
+++ b/arch/powerpc/kernel/kvm.c
@@ -680,7 +680,8 @@ static void kvm_use_magic_page(void)
 	on_each_cpu(kvm_map_magic_page, &features, 1);
 
 	/* Quick self-test to see if the mapping works */
-	if (fault_in_pages_readable((const char *)KVM_MAGIC_PAGE, sizeof(u32))) {
+	if (fault_in_readable((const char __user *)KVM_MAGIC_PAGE,
+			      sizeof(u32))) {
 		kvm_patching_worked = false;
 		return;
 	}
diff --git a/arch/powerpc/kernel/signal_32.c b/arch/powerpc/kernel/signal_32.c
index 164cfe405d30..ad2bf43e0cbe 100644
--- a/arch/powerpc/kernel/signal_32.c
+++ b/arch/powerpc/kernel/signal_32.c
@@ -1125,7 +1125,7 @@ SYSCALL_DEFINE3(swapcontext, struct ucontext __user *, old_ctx,
 	if (new_ctx == NULL)
 		return 0;
 	if (!access_ok(new_ctx, ctx_size) ||
-	    fault_in_pages_readable((u8 __user *)new_ctx, ctx_size))
+	    fault_in_readable((char __user *)new_ctx, ctx_size))
 		return -EFAULT;
 
 	/*
@@ -1315,7 +1315,7 @@ SYSCALL_DEFINE3(debug_setcontext, struct ucontext __user *, ctx,
 #endif
 
 	if (!access_ok(ctx, sizeof(*ctx)) ||
-	    fault_in_pages_readable((u8 __user *)ctx, sizeof(*ctx)))
+	    fault_in_readable((char __user *)ctx, sizeof(*ctx)))
 		return -EFAULT;
 
 	/*
diff --git a/arch/powerpc/kernel/signal_64.c b/arch/powerpc/kernel/signal_64.c
index d379a1126342..a4653f7eb343 100644
--- a/arch/powerpc/kernel/signal_64.c
+++ b/arch/powerpc/kernel/signal_64.c
@@ -672,7 +672,7 @@ SYSCALL_DEFINE3(swapcontext, struct ucontext __user *, old_ctx,
 	if (new_ctx == NULL)
 		return 0;
 	if (!access_ok(new_ctx, ctx_size) ||
-	    fault_in_pages_readable((u8 __user *)new_ctx, ctx_size))
+	    fault_in_readable((char __user *)new_ctx, ctx_size))
 		return -EFAULT;
 
 	/*
* Unmerged path arch/x86/kernel/fpu/signal.c
diff --git a/drivers/gpu/drm/armada/armada_gem.c b/drivers/gpu/drm/armada/armada_gem.c
index 21909642ee4c..8fbb25913327 100644
--- a/drivers/gpu/drm/armada/armada_gem.c
+++ b/drivers/gpu/drm/armada/armada_gem.c
@@ -336,7 +336,7 @@ int armada_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 	struct drm_armada_gem_pwrite *args = data;
 	struct armada_gem_object *dobj;
 	char __user *ptr;
-	int ret;
+	int ret = 0;
 
 	DRM_DEBUG_DRIVER("handle %u off %u size %u ptr 0x%llx\n",
 		args->handle, args->offset, args->size, args->ptr);
@@ -349,9 +349,8 @@ int armada_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 	if (!access_ok(ptr, args->size))
 		return -EFAULT;
 
-	ret = fault_in_pages_readable(ptr, args->size);
-	if (ret)
-		return ret;
+	if (fault_in_readable(ptr, args->size))
+		return -EFAULT;
 
 	dobj = armada_gem_object_lookup(file, args->handle);
 	if (dobj == NULL)
* Unmerged path fs/btrfs/ioctl.c
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index 0871f183968b..f8bb3b608f30 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -548,61 +548,10 @@ void page_endio(struct page *page, bool is_write, int err);
 extern void add_page_wait_queue(struct page *page, wait_queue_entry_t *waiter);
 
 /*
- * Fault everything in given userspace address range in.
+ * Fault in userspace address range.
  */
-static inline int fault_in_pages_writeable(char __user *uaddr, size_t size)
-{
-	char __user *end = uaddr + size - 1;
-
-	if (unlikely(size == 0))
-		return 0;
-
-	if (unlikely(uaddr > end))
-		return -EFAULT;
-	/*
-	 * Writing zeroes into userspace here is OK, because we know that if
-	 * the zero gets there, we'll be overwriting it.
-	 */
-	do {
-		if (unlikely(__put_user(0, uaddr) != 0))
-			return -EFAULT;
-		uaddr += PAGE_SIZE;
-	} while (uaddr <= end);
-
-	/* Check whether the range spilled into the next page. */
-	if (((unsigned long)uaddr & PAGE_MASK) ==
-			((unsigned long)end & PAGE_MASK))
-		return __put_user(0, end);
-
-	return 0;
-}
-
-static inline int fault_in_pages_readable(const char __user *uaddr, size_t size)
-{
-	volatile char c;
-	const char __user *end = uaddr + size - 1;
-
-	if (unlikely(size == 0))
-		return 0;
-
-	if (unlikely(uaddr > end))
-		return -EFAULT;
-
-	do {
-		if (unlikely(__get_user(c, uaddr) != 0))
-			return -EFAULT;
-		uaddr += PAGE_SIZE;
-	} while (uaddr <= end);
-
-	/* Check whether the range spilled into the next page. */
-	if (((unsigned long)uaddr & PAGE_MASK) ==
-			((unsigned long)end & PAGE_MASK)) {
-		return __get_user(c, end);
-	}
-
-	(void)c;
-	return 0;
-}
+size_t fault_in_writeable(char __user *uaddr, size_t size);
+size_t fault_in_readable(const char __user *uaddr, size_t size);
 
 int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
 				pgoff_t index, gfp_t gfp_mask);
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index ca9bcc01dca1..92176e0f1b30 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -174,7 +174,7 @@ static size_t copy_page_to_iter_iovec(struct page *page, size_t offset, size_t b
 	buf = iov->iov_base + skip;
 	copy = min(bytes, iov->iov_len - skip);
 
-	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_writeable(buf, copy)) {
+	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_writeable(buf, copy)) {
 		kaddr = kmap_atomic(page);
 		from = kaddr + offset;
 
@@ -258,7 +258,7 @@ static size_t copy_page_from_iter_iovec(struct page *page, size_t offset, size_t
 	buf = iov->iov_base + skip;
 	copy = min(bytes, iov->iov_len - skip);
 
-	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_pages_readable(buf, copy)) {
+	if (IS_ENABLED(CONFIG_HIGHMEM) && !fault_in_readable(buf, copy)) {
 		kaddr = kmap_atomic(page);
 		to = kaddr + offset;
 
@@ -426,13 +426,11 @@ int iov_iter_fault_in_readable(const struct iov_iter *i, size_t bytes)
 			bytes = i->count;
 		for (p = i->iov, skip = i->iov_offset; bytes; p++, skip = 0) {
 			size_t len = min(bytes, p->iov_len - skip);
-			int err;
 
 			if (unlikely(!len))
 				continue;
-			err = fault_in_pages_readable(p->iov_base + skip, len);
-			if (unlikely(err))
-				return err;
+			if (fault_in_readable(p->iov_base + skip, len))
+				return -EFAULT;
 			bytes -= len;
 		}
 	}
* Unmerged path mm/filemap.c
* Unmerged path mm/gup.c
