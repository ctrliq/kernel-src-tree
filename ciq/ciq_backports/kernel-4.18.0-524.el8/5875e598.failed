mm/slub: simplify __cmpxchg_double_slab() and slab_[un]lock()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-524.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 5875e59828a026e47f37c5e343f4fe8e0ba023b9
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-524.el8/5875e598.failed

The PREEMPT_RT specific disabling of irqs in __cmpxchg_double_slab()
(through slab_[un]lock()) is unnecessary as bit_spin_lock() disables
preemption and that's sufficient on PREEMPT_RT where no allocation/free
operation is performed in hardirq context and so can't interrupt the
current operation.

That means we no longer need the slab_[un]lock() wrappers, so delete
them and rename the current __slab_[un]lock() to slab_[un]lock().

	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Acked-by: David Rientjes <rientjes@google.com>
	Reviewed-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
	Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
(cherry picked from commit 5875e59828a026e47f37c5e343f4fe8e0ba023b9)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 4ece4a8929e7,d58f19502a92..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -413,38 -423,53 +413,51 @@@ static inline unsigned int oo_objects(s
  /*
   * Per slab locking using the pagelock
   */
++<<<<<<< HEAD
 +static __always_inline void __slab_lock(struct page *page)
++=======
+ static __always_inline void slab_lock(struct slab *slab)
++>>>>>>> 5875e59828a0 (mm/slub: simplify __cmpxchg_double_slab() and slab_[un]lock())
  {
 -	struct page *page = slab_page(slab);
 -
  	VM_BUG_ON_PAGE(PageTail(page), page);
  	bit_spin_lock(PG_locked, &page->flags);
  }
  
++<<<<<<< HEAD
 +static __always_inline void __slab_unlock(struct page *page)
++=======
+ static __always_inline void slab_unlock(struct slab *slab)
++>>>>>>> 5875e59828a0 (mm/slub: simplify __cmpxchg_double_slab() and slab_[un]lock())
  {
 -	struct page *page = slab_page(slab);
 -
  	VM_BUG_ON_PAGE(PageTail(page), page);
  	__bit_spin_unlock(PG_locked, &page->flags);
  }
  
++<<<<<<< HEAD
 +static __always_inline void slab_lock(struct page *page, unsigned long *flags)
 +{
 +	if (IS_ENABLED(CONFIG_PREEMPT_RT))
 +		local_irq_save(*flags);
 +	__slab_lock(page);
 +}
 +
 +static __always_inline void slab_unlock(struct page *page, unsigned long *flags)
 +{
 +	__slab_unlock(page);
 +	if (IS_ENABLED(CONFIG_PREEMPT_RT))
 +		local_irq_restore(*flags);
 +}
 +
++=======
++>>>>>>> 5875e59828a0 (mm/slub: simplify __cmpxchg_double_slab() and slab_[un]lock())
  /*
   * Interrupts must be disabled (for the fallback code to work right), typically
-  * by an _irqsave() lock variant. Except on PREEMPT_RT where locks are different
-  * so we disable interrupts as part of slab_[un]lock().
+  * by an _irqsave() lock variant. On PREEMPT_RT the preempt_disable(), which is
+  * part of bit_spin_lock(), is sufficient because the policy is not to allow any
+  * allocation/ free operation in hardirq context. Therefore nothing can
+  * interrupt the operation.
   */
 -static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct slab *slab,
 +static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
  		void *freelist_old, unsigned long counters_old,
  		void *freelist_new, unsigned long counters_new,
  		const char *n)
@@@ -461,18 -486,15 +474,30 @@@
  	} else
  #endif
  	{
++<<<<<<< HEAD
 +		/* init to 0 to prevent spurious warnings */
 +		unsigned long flags = 0;
 +
 +		slab_lock(page, &flags);
 +		if (page->freelist == freelist_old &&
 +					page->counters == counters_old) {
 +			page->freelist = freelist_new;
 +			page->counters = counters_new;
 +			slab_unlock(page, &flags);
 +			return true;
 +		}
 +		slab_unlock(page, &flags);
++=======
+ 		slab_lock(slab);
+ 		if (slab->freelist == freelist_old &&
+ 					slab->counters == counters_old) {
+ 			slab->freelist = freelist_new;
+ 			slab->counters = counters_new;
+ 			slab_unlock(slab);
+ 			return true;
+ 		}
+ 		slab_unlock(slab);
++>>>>>>> 5875e59828a0 (mm/slub: simplify __cmpxchg_double_slab() and slab_[un]lock())
  	}
  
  	cpu_relax();
@@@ -503,16 -525,16 +528,29 @@@ static inline bool cmpxchg_double_slab(
  		unsigned long flags;
  
  		local_irq_save(flags);
++<<<<<<< HEAD
 +		__slab_lock(page);
 +		if (page->freelist == freelist_old &&
 +					page->counters == counters_old) {
 +			page->freelist = freelist_new;
 +			page->counters = counters_new;
 +			__slab_unlock(page);
 +			local_irq_restore(flags);
 +			return true;
 +		}
 +		__slab_unlock(page);
++=======
+ 		slab_lock(slab);
+ 		if (slab->freelist == freelist_old &&
+ 					slab->counters == counters_old) {
+ 			slab->freelist = freelist_new;
+ 			slab->counters = counters_new;
+ 			slab_unlock(slab);
+ 			local_irq_restore(flags);
+ 			return true;
+ 		}
+ 		slab_unlock(slab);
++>>>>>>> 5875e59828a0 (mm/slub: simplify __cmpxchg_double_slab() and slab_[un]lock())
  		local_irq_restore(flags);
  	}
  
* Unmerged path mm/slub.c
