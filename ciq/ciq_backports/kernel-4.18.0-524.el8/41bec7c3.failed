mm/slub: remove slab_lock() usage for debug operations

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-524.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 41bec7c33f37aaae6e3737615e2dfa17a30ea985
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-524.el8/41bec7c3.failed

All alloc and free operations on debug caches are now serialized by
n->list_lock, so we can remove slab_lock() usage in validate_slab()
and list_slab_objects() as those also happen under n->list_lock.

Note the usage in list_slab_objects() could happen even on non-debug
caches, but only during cache shutdown time, so there should not be any
parallel freeing activity anymore. Except for buggy slab users, but in
that case the slab_lock() would not help against the common cmpxchg
based fast paths (in non-debug caches) anyway.

Also adjust documentation comments accordingly.

	Suggested-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Reviewed-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
	Acked-by: David Rientjes <rientjes@google.com>
(cherry picked from commit 41bec7c33f37aaae6e3737615e2dfa17a30ea985)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 4ece4a8929e7,37234e60591c..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -46,7 -50,7 +46,11 @@@
   *   1. slab_mutex (Global Mutex)
   *   2. node->list_lock (Spinlock)
   *   3. kmem_cache->cpu_slab->lock (Local lock)
++<<<<<<< HEAD
 + *   4. slab_lock(page) (Only on some arches or for debugging)
++=======
+  *   4. slab_lock(slab) (Only on some arches)
++>>>>>>> 41bec7c33f37 (mm/slub: remove slab_lock() usage for debug operations)
   *   5. object_map_lock (Only for debugging)
   *
   *   slab_mutex
@@@ -60,12 -64,13 +64,22 @@@
   *   The slab_lock is a wrapper around the page lock, thus it is a bit
   *   spinlock.
   *
++<<<<<<< HEAD
 + *   The slab_lock is only used for debugging and on arches that do not
 + *   have the ability to do a cmpxchg_double. It only protects:
 + *	A. page->freelist	-> List of object free in a page
 + *	B. page->inuse		-> Number of objects in use
 + *	C. page->objects	-> Number of objects in page
 + *	D. page->frozen		-> frozen state
++=======
+  *   The slab_lock is only used on arches that do not have the ability
+  *   to do a cmpxchg_double. It only protects:
+  *
+  *	A. slab->freelist	-> List of free objects in a slab
+  *	B. slab->inuse		-> Number of objects in use
+  *	C. slab->objects	-> Number of objects in slab
+  *	D. slab->frozen		-> frozen state
++>>>>>>> 41bec7c33f37 (mm/slub: remove slab_lock() usage for debug operations)
   *
   *   Frozen slabs
   *
@@@ -4190,11 -4372,10 +4207,15 @@@ static void list_slab_objects(struct km
  	unsigned long *map;
  	void *p;
  
++<<<<<<< HEAD
 +	slab_err(s, page, text, s->name);
 +	slab_lock(page, &flags);
++=======
+ 	slab_err(s, slab, text, s->name);
++>>>>>>> 41bec7c33f37 (mm/slub: remove slab_lock() usage for debug operations)
  
 -	map = get_map(s, slab);
 -	for_each_object(p, s, addr, slab->objects) {
 +	map = get_map(s, page);
 +	for_each_object(p, s, addr, page->objects) {
  
  		if (!test_bit(__obj_to_index(s, addr, p), map)) {
  			pr_err("Object 0x%p @offset=%tu\n", p, p - addr);
@@@ -4202,7 -4383,6 +4223,10 @@@
  		}
  	}
  	put_map(map);
++<<<<<<< HEAD
 +	slab_unlock(page, &flags);
++=======
++>>>>>>> 41bec7c33f37 (mm/slub: remove slab_lock() usage for debug operations)
  #endif
  }
  
@@@ -4917,25 -5109,20 +4941,35 @@@ static void validate_slab(struct kmem_c
  			  unsigned long *obj_map)
  {
  	void *p;
++<<<<<<< HEAD
 +	void *addr = page_address(page);
 +	unsigned long flags;
 +
 +	slab_lock(page, &flags);
 +
 +	if (!check_slab(s, page) || !on_freelist(s, page, NULL))
 +		goto unlock;
++=======
+ 	void *addr = slab_address(slab);
+ 
+ 	if (!check_slab(s, slab) || !on_freelist(s, slab, NULL))
+ 		return;
++>>>>>>> 41bec7c33f37 (mm/slub: remove slab_lock() usage for debug operations)
  
  	/* Now we know that a valid freelist exists */
 -	__fill_map(obj_map, s, slab);
 -	for_each_object(p, s, addr, slab->objects) {
 +	__fill_map(obj_map, s, page);
 +	for_each_object(p, s, addr, page->objects) {
  		u8 val = test_bit(__obj_to_index(s, addr, p), obj_map) ?
  			 SLUB_RED_INACTIVE : SLUB_RED_ACTIVE;
  
 -		if (!check_object(s, slab, p, val))
 +		if (!check_object(s, page, p, val))
  			break;
  	}
++<<<<<<< HEAD
 +unlock:
 +	slab_unlock(page, &flags);
++=======
++>>>>>>> 41bec7c33f37 (mm/slub: remove slab_lock() usage for debug operations)
  }
  
  static int validate_slab_node(struct kmem_cache *s,
* Unmerged path mm/slub.c
