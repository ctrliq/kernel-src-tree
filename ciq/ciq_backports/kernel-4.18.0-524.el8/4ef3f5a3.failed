mm/slub: convert object_map_lock to non-raw spinlock

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-524.el8
commit-author Vlastimil Babka <vbabka@suse.cz>
commit 4ef3f5a32051def596b2d445462a1dccda7af600
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-524.el8/4ef3f5a3.failed

The only remaining user of object_map_lock is list_slab_objects().
Obtaining the lock there used to happen under slab_lock() which implied
disabling irqs on PREEMPT_RT, thus it's a raw_spinlock. With the
slab_lock() removed, we can convert it to a normal spinlock.

Also remove the get_map()/put_map() wrappers as list_slab_objects()
became their only remaining user.

	Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
	Acked-by: David Rientjes <rientjes@google.com>
	Reviewed-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
	Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
(cherry picked from commit 4ef3f5a32051def596b2d445462a1dccda7af600)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/slub.c
diff --cc mm/slub.c
index 4ece4a8929e7,e0759b5fe506..000000000000
--- a/mm/slub.c
+++ b/mm/slub.c
@@@ -528,17 -565,17 +528,17 @@@ static inline bool cmpxchg_double_slab(
  
  #ifdef CONFIG_SLUB_DEBUG
  static unsigned long object_map[BITS_TO_LONGS(MAX_OBJS_PER_PAGE)];
- static DEFINE_RAW_SPINLOCK(object_map_lock);
+ static DEFINE_SPINLOCK(object_map_lock);
  
  static void __fill_map(unsigned long *obj_map, struct kmem_cache *s,
 -		       struct slab *slab)
 +		       struct page *page)
  {
 -	void *addr = slab_address(slab);
 +	void *addr = page_address(page);
  	void *p;
  
 -	bitmap_zero(obj_map, slab->objects);
 +	bitmap_zero(obj_map, page->objects);
  
 -	for (p = slab->freelist; p; p = get_freepointer(s, p))
 +	for (p = page->freelist; p; p = get_freepointer(s, p))
  		set_bit(__obj_to_index(s, addr, p), obj_map);
  }
  
@@@ -563,29 -599,6 +563,32 @@@ static bool slab_add_kunit_errors(void
  static inline bool slab_add_kunit_errors(void) { return false; }
  #endif
  
++<<<<<<< HEAD
 +/*
 + * Determine a map of object in use on a page.
 + *
 + * Node listlock must be held to guarantee that the page does
 + * not vanish from under us.
 + */
 +static unsigned long *get_map(struct kmem_cache *s, struct page *page)
 +{
 +	VM_BUG_ON(!irqs_disabled());
 +
 +	raw_spin_lock(&object_map_lock);
 +
 +	__fill_map(object_map, s, page);
 +
 +	return object_map;
 +}
 +
 +static void put_map(unsigned long *map)
 +{
 +	VM_BUG_ON(map != object_map);
 +	raw_spin_unlock(&object_map_lock);
 +}
 +
++=======
++>>>>>>> 4ef3f5a32051 (mm/slub: convert object_map_lock to non-raw spinlock)
  static inline unsigned int size_from_object(struct kmem_cache *s)
  {
  	if (s->flags & SLAB_RED_ZONE)
@@@ -4185,24 -4343,22 +4188,39 @@@ static void list_slab_objects(struct km
  			      const char *text)
  {
  #ifdef CONFIG_SLUB_DEBUG
++<<<<<<< HEAD
 +	void *addr = page_address(page);
 +	unsigned long flags;
 +	unsigned long *map;
++=======
+ 	void *addr = slab_address(slab);
++>>>>>>> 4ef3f5a32051 (mm/slub: convert object_map_lock to non-raw spinlock)
  	void *p;
  
 -	slab_err(s, slab, text, s->name);
 +	slab_err(s, page, text, s->name);
 +	slab_lock(page, &flags);
  
++<<<<<<< HEAD
 +	map = get_map(s, page);
 +	for_each_object(p, s, addr, page->objects) {
++=======
+ 	spin_lock(&object_map_lock);
+ 	__fill_map(object_map, s, slab);
  
- 		if (!test_bit(__obj_to_index(s, addr, p), map)) {
+ 	for_each_object(p, s, addr, slab->objects) {
++>>>>>>> 4ef3f5a32051 (mm/slub: convert object_map_lock to non-raw spinlock)
+ 
+ 		if (!test_bit(__obj_to_index(s, addr, p), object_map)) {
  			pr_err("Object 0x%p @offset=%tu\n", p, p - addr);
  			print_tracking(s, p);
  		}
  	}
++<<<<<<< HEAD
 +	put_map(map);
 +	slab_unlock(page, &flags);
++=======
+ 	spin_unlock(&object_map_lock);
++>>>>>>> 4ef3f5a32051 (mm/slub: convert object_map_lock to non-raw spinlock)
  #endif
  }
  
* Unmerged path mm/slub.c
