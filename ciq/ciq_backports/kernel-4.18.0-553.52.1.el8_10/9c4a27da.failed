idpf: enable WB_ON_ITR

jira LE-3064
Rebuild_History Non-Buildable kernel-4.18.0-553.52.1.el8_10
commit-author Joshua Hay <joshua.a.hay@intel.com>
commit 9c4a27da0ecc4080dfcd63903dd94f01ba1399dd
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.52.1.el8_10/9c4a27da.failed

Tell hardware to write back completed descriptors even when interrupts
are disabled. Otherwise, descriptors might not be written back until
the hardware can flush a full cacheline of descriptors. This can cause
unnecessary delays when traffic is light (or even trigger Tx queue
timeout).

The example scenario to reproduce the Tx timeout if the fix is not
applied:
  - configure at least 2 Tx queues to be assigned to the same q_vector,
  - generate a huge Tx traffic on the first Tx queue
  - try to send a few packets using the second Tx queue.
In such a case Tx timeout will appear on the second Tx queue because no
completion descriptors are written back for that queue while interrupts
are disabled due to NAPI polling.

Fixes: c2d548cad150 ("idpf: add TX splitq napi poll support")
Fixes: a5ab9ee0df0b ("idpf: add singleq start_xmit and napi poll")
	Signed-off-by: Joshua Hay <joshua.a.hay@intel.com>
Co-developed-by: Michal Kubiak <michal.kubiak@intel.com>
	Signed-off-by: Michal Kubiak <michal.kubiak@intel.com>
	Reviewed-by: Przemek Kitszel <przemyslaw.kitszel@intel.com>
	Signed-off-by: Alexander Lobakin <aleksander.lobakin@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit 9c4a27da0ecc4080dfcd63903dd94f01ba1399dd)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/idpf/idpf_txrx.h
diff --cc drivers/net/ethernet/intel/idpf/idpf_txrx.h
index 387f1af9c33f,f0537826f840..000000000000
--- a/drivers/net/ethernet/intel/idpf/idpf_txrx.h
+++ b/drivers/net/ethernet/intel/idpf/idpf_txrx.h
@@@ -508,12 -376,18 +512,17 @@@ struct idpf_intr_reg 
  /**
   * struct idpf_q_vector
   * @vport: Vport back pointer
 - * @num_rxq: Number of RX queues
 + * @affinity_mask: CPU affinity mask
 + * @napi: napi handler
++<<<<<<< HEAD
 + * @v_idx: Vector index
 + * @intr_reg: See struct idpf_intr_reg
   * @num_txq: Number of TX queues
 - * @num_bufq: Number of buffer queues
 - * @num_complq: number of completion queues
 - * @rx: Array of RX queues to service
   * @tx: Array of TX queues to service
 - * @bufq: Array of buffer queues to service
 - * @complq: array of completion queues
 - * @intr_reg: See struct idpf_intr_reg
 - * @napi: napi handler
++=======
+  * @total_events: Number of interrupts processed
+  * @wb_on_itr: whether WB on ITR is enabled
++>>>>>>> 9c4a27da0ecc (idpf: enable WB_ON_ITR)
   * @tx_dim: Data for TX net_dim algorithm
   * @tx_itr_value: TX interrupt throttling rate
   * @tx_intr_mode: Dynamic ITR or not
@@@ -524,20 -396,30 +533,25 @@@
   * @rx_itr_value: RX interrupt throttling rate
   * @rx_intr_mode: Dynamic ITR or not
   * @rx_itr_idx: RX ITR index
 - * @v_idx: Vector index
 - * @affinity_mask: CPU affinity mask
 + * @num_bufq: Number of buffer queues
 + * @bufq: Array of buffer queues to service
 + * @total_events: Number of interrupts processed
 + * @name: Queue vector name
   */
  struct idpf_q_vector {
 -	__cacheline_group_begin_aligned(read_mostly);
  	struct idpf_vport *vport;
 -
 -	u16 num_rxq;
 -	u16 num_txq;
 -	u16 num_bufq;
 -	u16 num_complq;
 -	struct idpf_rx_queue **rx;
 -	struct idpf_tx_queue **tx;
 -	struct idpf_buf_queue **bufq;
 -	struct idpf_compl_queue **complq;
 -
 -	struct idpf_intr_reg intr_reg;
 -	__cacheline_group_end_aligned(read_mostly);
 -
 -	__cacheline_group_begin_aligned(read_write);
 +	cpumask_t affinity_mask;
  	struct napi_struct napi;
++<<<<<<< HEAD
 +	u16 v_idx;
 +	struct idpf_intr_reg intr_reg;
++=======
+ 	u16 total_events;
+ 	bool wb_on_itr;
++>>>>>>> 9c4a27da0ecc (idpf: enable WB_ON_ITR)
  
 +	u16 num_txq;
 +	struct idpf_queue **tx;
  	struct dim tx_dim;
  	u16 tx_itr_value;
  	bool tx_intr_mode;
@@@ -549,13 -429,17 +563,19 @@@
  	u16 rx_itr_value;
  	bool rx_intr_mode;
  	u32 rx_itr_idx;
 -	__cacheline_group_end_aligned(read_write);
  
 -	__cacheline_group_begin_aligned(cold);
 -	u16 v_idx;
 +	u16 num_bufq;
 +	struct idpf_queue **bufq;
  
 -	cpumask_var_t affinity_mask;
 -	__cacheline_group_end_aligned(cold);
 +	u16 total_events;
 +	char *name;
  };
++<<<<<<< HEAD
++=======
+ libeth_cacheline_set_assert(struct idpf_q_vector, 112,
+ 			    424 + 2 * sizeof(struct dim),
+ 			    8 + sizeof(cpumask_var_t));
++>>>>>>> 9c4a27da0ecc (idpf: enable WB_ON_ITR)
  
  struct idpf_rx_queue_stats {
  	u64_stats_t packets;
@@@ -921,55 -994,22 +941,74 @@@ static inline void idpf_tx_splitq_build
  }
  
  /**
++<<<<<<< HEAD
 + * idpf_alloc_page - Allocate a new RX buffer from the page pool
 + * @pool: page_pool to allocate from
 + * @buf: metadata struct to populate with page info
 + * @buf_size: 2K or 4K
 + *
 + * Returns &dma_addr_t to be passed to HW for Rx, %DMA_MAPPING_ERROR otherwise.
 + */
 +static inline dma_addr_t idpf_alloc_page(struct page_pool *pool,
 +					 struct idpf_rx_buf *buf,
 +					 unsigned int buf_size)
 +{
 +	WARN_ON_ONCE(buf_size != IDPF_RX_BUF_4096);
 +	buf->page = page_pool_dev_alloc_pages(pool);
 +	buf->pp = pool;
 +
 +	if (!buf->page)
 +		return DMA_MAPPING_ERROR;
 +
 +	buf->truesize = buf_size;
 +
 +	return page_pool_get_dma_addr(buf->page) + buf->page_offset +
 +	       pool->p.offset;
 +}
 +
 +/**
 + * idpf_rx_put_page - Return RX buffer page to pool
 + * @rx_buf: RX buffer metadata struct
 + */
 +static inline void idpf_rx_put_page(struct idpf_rx_buf *rx_buf)
 +{
 +	page_pool_put_page(rx_buf->pp, rx_buf->page,
 +			   rx_buf->truesize, true);
 +	rx_buf->page = NULL;
 +}
 +
 +/**
 + * idpf_rx_sync_for_cpu - Synchronize DMA buffer
 + * @rx_buf: RX buffer metadata struct
 + * @len: frame length from descriptor
 + */
 +static inline void idpf_rx_sync_for_cpu(struct idpf_rx_buf *rx_buf, u32 len)
 +{
 +	struct page *page = rx_buf->page;
 +	struct page_pool *pp = rx_buf->pp;
 +
 +	dma_sync_single_range_for_cpu(pp->p.dev,
 +				      page_pool_get_dma_addr(page),
 +				      rx_buf->page_offset + pp->p.offset, len,
 +				      page_pool_get_dma_dir(pp));
++=======
+  * idpf_vport_intr_set_wb_on_itr - enable descriptor writeback on disabled interrupts
+  * @q_vector: pointer to queue vector struct
+  */
+ static inline void idpf_vport_intr_set_wb_on_itr(struct idpf_q_vector *q_vector)
+ {
+ 	struct idpf_intr_reg *reg;
+ 
+ 	if (q_vector->wb_on_itr)
+ 		return;
+ 
+ 	q_vector->wb_on_itr = true;
+ 	reg = &q_vector->intr_reg;
+ 
+ 	writel(reg->dyn_ctl_wb_on_itr_m | reg->dyn_ctl_intena_msk_m |
+ 	       (IDPF_NO_ITR_UPDATE_IDX << reg->dyn_ctl_itridx_s),
+ 	       reg->dyn_ctl);
++>>>>>>> 9c4a27da0ecc (idpf: enable WB_ON_ITR)
  }
  
  int idpf_vport_singleq_napi_poll(struct napi_struct *napi, int budget);
diff --git a/drivers/net/ethernet/intel/idpf/idpf_dev.c b/drivers/net/ethernet/intel/idpf/idpf_dev.c
index 3df9935685e9..6c913a703df6 100644
--- a/drivers/net/ethernet/intel/idpf/idpf_dev.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_dev.c
@@ -97,8 +97,10 @@ static int idpf_intr_reg_init(struct idpf_vport *vport)
 		intr->dyn_ctl = idpf_get_reg_addr(adapter,
 						  reg_vals[vec_id].dyn_ctl_reg);
 		intr->dyn_ctl_intena_m = PF_GLINT_DYN_CTL_INTENA_M;
+		intr->dyn_ctl_intena_msk_m = PF_GLINT_DYN_CTL_INTENA_MSK_M;
 		intr->dyn_ctl_itridx_s = PF_GLINT_DYN_CTL_ITR_INDX_S;
 		intr->dyn_ctl_intrvl_s = PF_GLINT_DYN_CTL_INTERVAL_S;
+		intr->dyn_ctl_wb_on_itr_m = PF_GLINT_DYN_CTL_WB_ON_ITR_M;
 
 		spacing = IDPF_ITR_IDX_SPACING(reg_vals[vec_id].itrn_index_spacing,
 					       IDPF_PF_ITR_IDX_SPACING);
diff --git a/drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c b/drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
index 27b93592c4ba..fe39412fece7 100644
--- a/drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_singleq_txrx.c
@@ -1166,8 +1166,10 @@ int idpf_vport_singleq_napi_poll(struct napi_struct *napi, int budget)
 						    &work_done);
 
 	/* If work not completed, return budget and polling will return */
-	if (!clean_complete)
+	if (!clean_complete) {
+		idpf_vport_intr_set_wb_on_itr(q_vector);
 		return budget;
+	}
 
 	work_done = min_t(int, work_done, budget - 1);
 
@@ -1176,6 +1178,8 @@ int idpf_vport_singleq_napi_poll(struct napi_struct *napi, int budget)
 	 */
 	if (likely(napi_complete_done(napi, work_done)))
 		idpf_vport_intr_update_itr_ena_irq(q_vector);
+	else
+		idpf_vport_intr_set_wb_on_itr(q_vector);
 
 	return work_done;
 }
diff --git a/drivers/net/ethernet/intel/idpf/idpf_txrx.c b/drivers/net/ethernet/intel/idpf/idpf_txrx.c
index 35677fdf6d7a..ddd68c45bd59 100644
--- a/drivers/net/ethernet/intel/idpf/idpf_txrx.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_txrx.c
@@ -3631,6 +3631,7 @@ void idpf_vport_intr_update_itr_ena_irq(struct idpf_q_vector *q_vector)
 	/* net_dim() updates ITR out-of-band using a work item */
 	idpf_net_dim(q_vector);
 
+	q_vector->wb_on_itr = false;
 	intval = idpf_vport_intr_buildreg_itr(q_vector,
 					      IDPF_NO_ITR_UPDATE_IDX, 0);
 
@@ -3926,8 +3927,10 @@ static int idpf_vport_splitq_napi_poll(struct napi_struct *napi, int budget)
 	clean_complete &= idpf_tx_splitq_clean_all(q_vector, budget, &work_done);
 
 	/* If work not completed, return budget and polling will return */
-	if (!clean_complete)
+	if (!clean_complete) {
+		idpf_vport_intr_set_wb_on_itr(q_vector);
 		return budget;
+	}
 
 	work_done = min_t(int, work_done, budget - 1);
 
@@ -3936,6 +3939,8 @@ static int idpf_vport_splitq_napi_poll(struct napi_struct *napi, int budget)
 	 */
 	if (likely(napi_complete_done(napi, work_done)))
 		idpf_vport_intr_update_itr_ena_irq(q_vector);
+	else
+		idpf_vport_intr_set_wb_on_itr(q_vector);
 
 	/* Switch to poll mode in the tear-down path after sending disable
 	 * queues virtchnl message, as the interrupts will be disabled after
* Unmerged path drivers/net/ethernet/intel/idpf/idpf_txrx.h
diff --git a/drivers/net/ethernet/intel/idpf/idpf_vf_dev.c b/drivers/net/ethernet/intel/idpf/idpf_vf_dev.c
index 629cb5cb7c9f..99b8dbaf4225 100644
--- a/drivers/net/ethernet/intel/idpf/idpf_vf_dev.c
+++ b/drivers/net/ethernet/intel/idpf/idpf_vf_dev.c
@@ -97,7 +97,9 @@ static int idpf_vf_intr_reg_init(struct idpf_vport *vport)
 		intr->dyn_ctl = idpf_get_reg_addr(adapter,
 						  reg_vals[vec_id].dyn_ctl_reg);
 		intr->dyn_ctl_intena_m = VF_INT_DYN_CTLN_INTENA_M;
+		intr->dyn_ctl_intena_msk_m = VF_INT_DYN_CTLN_INTENA_MSK_M;
 		intr->dyn_ctl_itridx_s = VF_INT_DYN_CTLN_ITR_INDX_S;
+		intr->dyn_ctl_wb_on_itr_m = VF_INT_DYN_CTLN_WB_ON_ITR_M;
 
 		spacing = IDPF_ITR_IDX_SPACING(reg_vals[vec_id].itrn_index_spacing,
 					       IDPF_VF_ITR_IDX_SPACING);
