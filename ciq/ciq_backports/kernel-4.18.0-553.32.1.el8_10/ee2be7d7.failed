gfs2: Replace gfs2_glock_queue_put with gfs2_glock_put_async

jira LE-2169
Rebuild_History Non-Buildable kernel-4.18.0-553.32.1.el8_10
commit-author Andreas Gruenbacher <agruenba@redhat.com>
commit ee2be7d7c7f32783f60ee5fe59b91548a4571f10
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.32.1.el8_10/ee2be7d7.failed

Function gfs2_glock_queue_put() puts a glock reference by enqueuing
glock work instead of putting the reference directly.  This ensures that
the operation won't sleep, but it is costly and really only necessary
when putting the final glock reference.  Replace it with a new
gfs2_glock_put_async() function that only queues glock work when putting
the last glock reference.

	Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
(cherry picked from commit ee2be7d7c7f32783f60ee5fe59b91548a4571f10)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/gfs2/glock.h
#	fs/gfs2/super.c
diff --cc fs/gfs2/glock.h
index be4bc9edf46d,2c697674a86f..000000000000
--- a/fs/gfs2/glock.h
+++ b/fs/gfs2/glock.h
@@@ -193,33 -167,40 +193,70 @@@ static inline struct address_space *gfs
  	return NULL;
  }
  
++<<<<<<< HEAD
 +extern int gfs2_glock_get(struct gfs2_sbd *sdp, u64 number,
 +			  const struct gfs2_glock_operations *glops,
 +			  int create, struct gfs2_glock **glp);
 +extern void gfs2_glock_hold(struct gfs2_glock *gl);
 +extern void gfs2_glock_put(struct gfs2_glock *gl);
 +extern void gfs2_glock_queue_put(struct gfs2_glock *gl);
 +extern void gfs2_holder_init(struct gfs2_glock *gl, unsigned int state,
 +			     u16 flags, struct gfs2_holder *gh);
 +extern void gfs2_holder_reinit(unsigned int state, u16 flags,
 +			       struct gfs2_holder *gh);
 +extern void gfs2_holder_uninit(struct gfs2_holder *gh);
 +extern int gfs2_glock_nq(struct gfs2_holder *gh);
 +extern int gfs2_glock_poll(struct gfs2_holder *gh);
 +extern int gfs2_instantiate(struct gfs2_holder *gh);
 +extern int gfs2_glock_holder_ready(struct gfs2_holder *gh);
 +extern int gfs2_glock_wait(struct gfs2_holder *gh);
 +extern int gfs2_glock_async_wait(unsigned int num_gh, struct gfs2_holder *ghs);
 +extern void gfs2_glock_dq(struct gfs2_holder *gh);
 +extern void gfs2_glock_dq_wait(struct gfs2_holder *gh);
 +extern void gfs2_glock_dq_uninit(struct gfs2_holder *gh);
 +extern int gfs2_glock_nq_num(struct gfs2_sbd *sdp, u64 number,
 +			     const struct gfs2_glock_operations *glops,
 +			     unsigned int state, u16 flags,
 +			     struct gfs2_holder *gh);
 +extern int gfs2_glock_nq_m(unsigned int num_gh, struct gfs2_holder *ghs);
 +extern void gfs2_glock_dq_m(unsigned int num_gh, struct gfs2_holder *ghs);
 +extern void gfs2_dump_glock(struct seq_file *seq, struct gfs2_glock *gl,
++=======
+ int gfs2_glock_get(struct gfs2_sbd *sdp, u64 number,
+ 		   const struct gfs2_glock_operations *glops,
+ 		   int create, struct gfs2_glock **glp);
+ struct gfs2_glock *gfs2_glock_hold(struct gfs2_glock *gl);
+ void gfs2_glock_put(struct gfs2_glock *gl);
+ void gfs2_glock_put_async(struct gfs2_glock *gl);
+ 
+ void __gfs2_holder_init(struct gfs2_glock *gl, unsigned int state,
+ 		        u16 flags, struct gfs2_holder *gh,
+ 		        unsigned long ip);
+ static inline void gfs2_holder_init(struct gfs2_glock *gl, unsigned int state,
+ 				    u16 flags, struct gfs2_holder *gh) {
+ 	__gfs2_holder_init(gl, state, flags, gh, _RET_IP_);
+ }
+ 
+ void gfs2_holder_reinit(unsigned int state, u16 flags,
+ 		        struct gfs2_holder *gh);
+ void gfs2_holder_uninit(struct gfs2_holder *gh);
+ int gfs2_glock_nq(struct gfs2_holder *gh);
+ int gfs2_glock_poll(struct gfs2_holder *gh);
+ int gfs2_instantiate(struct gfs2_holder *gh);
+ int gfs2_glock_holder_ready(struct gfs2_holder *gh);
+ int gfs2_glock_wait(struct gfs2_holder *gh);
+ int gfs2_glock_async_wait(unsigned int num_gh, struct gfs2_holder *ghs);
+ void gfs2_glock_dq(struct gfs2_holder *gh);
+ void gfs2_glock_dq_wait(struct gfs2_holder *gh);
+ void gfs2_glock_dq_uninit(struct gfs2_holder *gh);
+ int gfs2_glock_nq_num(struct gfs2_sbd *sdp, u64 number,
+ 		      const struct gfs2_glock_operations *glops,
+ 		      unsigned int state, u16 flags,
+ 		      struct gfs2_holder *gh);
+ int gfs2_glock_nq_m(unsigned int num_gh, struct gfs2_holder *ghs);
+ void gfs2_glock_dq_m(unsigned int num_gh, struct gfs2_holder *ghs);
+ void gfs2_dump_glock(struct seq_file *seq, struct gfs2_glock *gl,
++>>>>>>> ee2be7d7c7f3 (gfs2: Replace gfs2_glock_queue_put with gfs2_glock_put_async)
  			    bool fsid);
  #define GLOCK_BUG_ON(gl,x) do { if (unlikely(x)) {		\
  			gfs2_dump_glock(NULL, gl, true);	\
diff --cc fs/gfs2/super.c
index a0ec4e506f04,d481db9510ac..000000000000
--- a/fs/gfs2/super.c
+++ b/fs/gfs2/super.c
@@@ -1070,8 -1048,8 +1070,13 @@@ static int gfs2_drop_inode(struct inod
  		struct gfs2_glock *gl = ip->i_iopen_gh.gh_gl;
  
  		gfs2_glock_hold(gl);
++<<<<<<< HEAD
 +		if (!gfs2_queue_verify_delete(gl))
 +			gfs2_glock_queue_put(gl);
++=======
+ 		if (!gfs2_queue_try_to_evict(gl))
+ 			gfs2_glock_put_async(gl);
++>>>>>>> ee2be7d7c7f3 (gfs2: Replace gfs2_glock_queue_put with gfs2_glock_put_async)
  		return 0;
  	}
  
diff --git a/fs/gfs2/glock.c b/fs/gfs2/glock.c
index 732764d4d899..05c7a5909e21 100644
--- a/fs/gfs2/glock.c
+++ b/fs/gfs2/glock.c
@@ -310,14 +310,6 @@ static void __gfs2_glock_put(struct gfs2_glock *gl)
 	sdp->sd_lockstruct.ls_ops->lm_put_lock(gl);
 }
 
-/*
- * Cause the glock to be put in work queue context.
- */
-void gfs2_glock_queue_put(struct gfs2_glock *gl)
-{
-	gfs2_glock_queue_work(gl, 0);
-}
-
 /**
  * gfs2_glock_put() - Decrement reference count on glock
  * @gl: The glock to put
@@ -332,6 +324,22 @@ void gfs2_glock_put(struct gfs2_glock *gl)
 	__gfs2_glock_put(gl);
 }
 
+/*
+ * gfs2_glock_put_async - Decrement reference count without sleeping
+ * @gl: The glock to put
+ *
+ * Decrement the reference count on glock immediately unless it is the last
+ * reference.  Defer putting the last reference to work queue context.
+ */
+void gfs2_glock_put_async(struct gfs2_glock *gl)
+{
+	if (lockref_put_or_lock(&gl->gl_lockref))
+		return;
+
+	__gfs2_glock_queue_work(gl, 0);
+	spin_unlock(&gl->gl_lockref.lock);
+}
+
 /**
  * may_grant - check if it's ok to grant a new lock
  * @gl: The glock
@@ -2457,8 +2465,7 @@ static void gfs2_glock_iter_next(struct gfs2_glock_iter *gi, loff_t n)
 	if (gl) {
 		if (n == 0)
 			return;
-		if (!lockref_put_not_zero(&gl->gl_lockref))
-			gfs2_glock_queue_put(gl);
+		gfs2_glock_put_async(gl);
 	}
 	for (;;) {
 		gl = rhashtable_walk_next(&gi->hti);
* Unmerged path fs/gfs2/glock.h
diff --git a/fs/gfs2/log.c b/fs/gfs2/log.c
index 41bf7b2cb73e..f8fa525525c0 100644
--- a/fs/gfs2/log.c
+++ b/fs/gfs2/log.c
@@ -752,7 +752,7 @@ void gfs2_glock_remove_revoke(struct gfs2_glock *gl)
 {
 	if (atomic_dec_return(&gl->gl_revokes) == 0) {
 		clear_bit(GLF_LFLUSH, &gl->gl_flags);
-		gfs2_glock_queue_put(gl);
+		gfs2_glock_put_async(gl);
 	}
 }
 
* Unmerged path fs/gfs2/super.c
