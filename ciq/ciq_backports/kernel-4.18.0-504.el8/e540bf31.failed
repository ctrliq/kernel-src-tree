fork: Only cache the VMAP stack in finish_task_switch()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-504.el8
commit-author Sebastian Andrzej Siewior <bigeasy@linutronix.de>
commit e540bf3162e822d7a1f07e69e3bb1b4f925ca368
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-504.el8/e540bf31.failed

The task stack could be deallocated later, but for fork()/exec() kind of
workloads (say a shell script executing several commands) it is important
that the stack is released in finish_task_switch() so that in VMAP_STACK
case it can be cached and reused in the new task.

For PREEMPT_RT it would be good if the wake-up in vfree_atomic() could
be avoided in the scheduling path. Far worse are the other
free_thread_stack() implementations which invoke __free_pages()/
kmem_cache_free() with disabled preemption.

Cache the stack in free_thread_stack() in the VMAP_STACK case and
RCU-delay the free path otherwise. Free the stack in the RCU callback.
In the VMAP_STACK case this is another opportunity to fill the cache.

	Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
	Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
	Acked-by: Andy Lutomirski <luto@kernel.org>
Link: https://lore.kernel.org/r/20220217102406.3697941-8-bigeasy@linutronix.de

(cherry picked from commit e540bf3162e822d7a1f07e69e3bb1b4f925ca368)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/fork.c
diff --cc kernel/fork.c
index 2fc87f72c8b1,177bc64078cd..000000000000
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@@ -255,29 -331,28 +290,51 @@@ static unsigned long *alloc_thread_stac
  
  static void free_thread_stack(struct task_struct *tsk)
  {
++<<<<<<< HEAD
 +	struct vm_struct *vm = task_stack_vm_area(tsk);
 +	int i;
 +
 +	for (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++)
 +		memcg_kmem_uncharge_page(vm->pages[i], 0);
 +
 +	for (i = 0; i < NR_CACHED_STACKS; i++) {
 +		if (this_cpu_cmpxchg(cached_stacks[i], NULL,
 +				     tsk->stack_vm_area) != NULL)
 +			continue;
 +
 +		tsk->stack = NULL;
 +		tsk->stack_vm_area = NULL;
 +		return;
 +	}
 +	vfree_atomic(tsk->stack);
++=======
+ 	if (!try_release_thread_stack_to_cache(tsk->stack_vm_area))
+ 		thread_stack_delayed_free(tsk);
+ 
++>>>>>>> e540bf3162e8 (fork: Only cache the VMAP stack in finish_task_switch())
  	tsk->stack = NULL;
  	tsk->stack_vm_area = NULL;
  }
  
  #  else /* !CONFIG_VMAP_STACK */
  
++<<<<<<< HEAD
 +static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
++=======
+ static void thread_stack_free_rcu(struct rcu_head *rh)
+ {
+ 	__free_pages(virt_to_page(rh), THREAD_SIZE_ORDER);
+ }
+ 
+ static void thread_stack_delayed_free(struct task_struct *tsk)
+ {
+ 	struct rcu_head *rh = tsk->stack;
+ 
+ 	call_rcu(rh, thread_stack_free_rcu);
+ }
+ 
+ static int alloc_thread_stack_node(struct task_struct *tsk, int node)
++>>>>>>> e540bf3162e8 (fork: Only cache the VMAP stack in finish_task_switch())
  {
  	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
  					     THREAD_SIZE_ORDER);
@@@ -300,8 -375,19 +357,24 @@@ static void free_thread_stack(struct ta
  
  static struct kmem_cache *thread_stack_cache;
  
++<<<<<<< HEAD
 +static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
 +						  int node)
++=======
+ static void thread_stack_free_rcu(struct rcu_head *rh)
+ {
+ 	kmem_cache_free(thread_stack_cache, rh);
+ }
+ 
+ static void thread_stack_delayed_free(struct task_struct *tsk)
+ {
+ 	struct rcu_head *rh = tsk->stack;
+ 
+ 	call_rcu(rh, thread_stack_free_rcu);
+ }
+ 
+ static int alloc_thread_stack_node(struct task_struct *tsk, int node)
++>>>>>>> e540bf3162e8 (fork: Only cache the VMAP stack in finish_task_switch())
  {
  	unsigned long *stack;
  	stack = kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
* Unmerged path kernel/fork.c
