RDMA/mlx5: Introduce mlx5_umr_post_send_wait()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit 6f0689fdf19ed3aca3ee3910223ad27216640693
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/6f0689fd.failed

Introduce mlx5_umr_post_send_wait() that uses a UMR adjusted flow for
posting WQEs. The next patches will gradually move UMR operations to use
this flow. Once done, will get rid of mlx5_ib_post_send_wait().

mlx5_umr_post_send_wait gets already written WQE segments and will only
memcpy it to the SQ. This way, we avoid packing all the data in a WR just
to unpack it into the WQE.

Link: https://lore.kernel.org/r/f027dd592fde62402b2d49efded8d1d22229d22b.1649747695.git.leonro@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Michael Guralnik <michaelgur@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 6f0689fdf19ed3aca3ee3910223ad27216640693)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/umr.h
diff --cc drivers/infiniband/hw/mlx5/umr.h
index cb1a2c95aac2,d984213caf60..000000000000
--- a/drivers/infiniband/hw/mlx5/umr.h
+++ b/drivers/infiniband/hw/mlx5/umr.h
@@@ -9,4 -9,86 +9,82 @@@
  int mlx5r_umr_resource_init(struct mlx5_ib_dev *dev);
  void mlx5r_umr_resource_cleanup(struct mlx5_ib_dev *dev);
  
++<<<<<<< HEAD
++=======
+ static inline bool mlx5r_umr_can_load_pas(struct mlx5_ib_dev *dev,
+ 					  size_t length)
+ {
+ 	/*
+ 	 * umr_check_mkey_mask() rejects MLX5_MKEY_MASK_PAGE_SIZE which is
+ 	 * always set if MLX5_IB_SEND_UMR_UPDATE_TRANSLATION (aka
+ 	 * MLX5_IB_UPD_XLT_ADDR and MLX5_IB_UPD_XLT_ENABLE) is set. Thus, a mkey
+ 	 * can never be enabled without this capability. Simplify this weird
+ 	 * quirky hardware by just saying it can't use PAS lists with UMR at
+ 	 * all.
+ 	 */
+ 	if (MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))
+ 		return false;
+ 
+ 	/*
+ 	 * length is the size of the MR in bytes when mlx5_ib_update_xlt() is
+ 	 * used.
+ 	 */
+ 	if (!MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset) &&
+ 	    length >= MLX5_MAX_UMR_PAGES * PAGE_SIZE)
+ 		return false;
+ 	return true;
+ }
+ 
+ /*
+  * true if an existing MR can be reconfigured to new access_flags using UMR.
+  * Older HW cannot use UMR to update certain elements of the MKC. See
+  * get_umr_update_access_mask() and umr_check_mkey_mask()
+  */
+ static inline bool mlx5r_umr_can_reconfig(struct mlx5_ib_dev *dev,
+ 					  unsigned int current_access_flags,
+ 					  unsigned int target_access_flags)
+ {
+ 	unsigned int diffs = current_access_flags ^ target_access_flags;
+ 
+ 	if ((diffs & IB_ACCESS_REMOTE_ATOMIC) &&
+ 	    MLX5_CAP_GEN(dev->mdev, atomic) &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
+ 		return false;
+ 
+ 	if ((diffs & IB_ACCESS_RELAXED_ORDERING) &&
+ 	    MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write) &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))
+ 		return false;
+ 
+ 	if ((diffs & IB_ACCESS_RELAXED_ORDERING) &&
+ 	    MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read) &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static inline u64 mlx5r_umr_get_xlt_octo(u64 bytes)
+ {
+ 	return ALIGN(bytes, MLX5_IB_UMR_XLT_ALIGNMENT) /
+ 	       MLX5_IB_UMR_OCTOWORD;
+ }
+ 
+ int mlx5r_umr_set_umr_ctrl_seg(struct mlx5_ib_dev *dev,
+ 			       struct mlx5_wqe_umr_ctrl_seg *umr,
+ 			       const struct ib_send_wr *wr);
+ 
+ struct mlx5r_umr_context {
+ 	struct ib_cqe cqe;
+ 	enum ib_wc_status status;
+ 	struct completion done;
+ };
+ 
+ struct mlx5r_umr_wqe {
+ 	struct mlx5_wqe_umr_ctrl_seg ctrl_seg;
+ 	struct mlx5_mkey_seg mkey_seg;
+ 	struct mlx5_wqe_data_seg data_seg;
+ };
+ 
++>>>>>>> 6f0689fdf19e (RDMA/mlx5: Introduce mlx5_umr_post_send_wait())
  #endif /* _MLX5_IB_UMR_H */
diff --git a/drivers/infiniband/hw/mlx5/umr.c b/drivers/infiniband/hw/mlx5/umr.c
index 46eaf919eb49..308586d2872a 100644
--- a/drivers/infiniband/hw/mlx5/umr.c
+++ b/drivers/infiniband/hw/mlx5/umr.c
@@ -3,6 +3,7 @@
 
 #include "mlx5_ib.h"
 #include "umr.h"
+#include "wr.h"
 
 enum {
 	MAX_UMR_WR = 128,
@@ -104,3 +105,94 @@ void mlx5r_umr_resource_cleanup(struct mlx5_ib_dev *dev)
 	ib_free_cq(dev->umrc.cq);
 	ib_dealloc_pd(dev->umrc.pd);
 }
+
+static int mlx5r_umr_post_send(struct ib_qp *ibqp, u32 mkey, struct ib_cqe *cqe,
+			       struct mlx5r_umr_wqe *wqe, bool with_data)
+{
+	unsigned int wqe_size =
+		with_data ? sizeof(struct mlx5r_umr_wqe) :
+			    sizeof(struct mlx5r_umr_wqe) -
+				    sizeof(struct mlx5_wqe_data_seg);
+	struct mlx5_ib_dev *dev = to_mdev(ibqp->device);
+	struct mlx5_core_dev *mdev = dev->mdev;
+	struct mlx5_ib_qp *qp = to_mqp(ibqp);
+	struct mlx5_wqe_ctrl_seg *ctrl;
+	union {
+		struct ib_cqe *ib_cqe;
+		u64 wr_id;
+	} id;
+	void *cur_edge, *seg;
+	unsigned long flags;
+	unsigned int idx;
+	int size, err;
+
+	if (unlikely(mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR))
+		return -EIO;
+
+	spin_lock_irqsave(&qp->sq.lock, flags);
+
+	err = mlx5r_begin_wqe(qp, &seg, &ctrl, &idx, &size, &cur_edge, 0,
+			      cpu_to_be32(mkey), false, false);
+	if (WARN_ON(err))
+		goto out;
+
+	qp->sq.wr_data[idx] = MLX5_IB_WR_UMR;
+
+	mlx5r_memcpy_send_wqe(&qp->sq, &cur_edge, &seg, &size, wqe, wqe_size);
+
+	id.ib_cqe = cqe;
+	mlx5r_finish_wqe(qp, ctrl, seg, size, cur_edge, idx, id.wr_id, 0,
+			 MLX5_FENCE_MODE_NONE, MLX5_OPCODE_UMR);
+
+	mlx5r_ring_db(qp, 1, ctrl);
+
+out:
+	spin_unlock_irqrestore(&qp->sq.lock, flags);
+
+	return err;
+}
+
+static void mlx5r_umr_done(struct ib_cq *cq, struct ib_wc *wc)
+{
+	struct mlx5_ib_umr_context *context =
+		container_of(wc->wr_cqe, struct mlx5_ib_umr_context, cqe);
+
+	context->status = wc->status;
+	complete(&context->done);
+}
+
+static inline void mlx5r_umr_init_context(struct mlx5r_umr_context *context)
+{
+	context->cqe.done = mlx5r_umr_done;
+	init_completion(&context->done);
+}
+
+static int mlx5r_umr_post_send_wait(struct mlx5_ib_dev *dev, u32 mkey,
+				   struct mlx5r_umr_wqe *wqe, bool with_data)
+{
+	struct umr_common *umrc = &dev->umrc;
+	struct mlx5r_umr_context umr_context;
+	int err;
+
+	err = umr_check_mkey_mask(dev, be64_to_cpu(wqe->ctrl_seg.mkey_mask));
+	if (WARN_ON(err))
+		return err;
+
+	mlx5r_umr_init_context(&umr_context);
+
+	down(&umrc->sem);
+	err = mlx5r_umr_post_send(umrc->qp, mkey, &umr_context.cqe, wqe,
+				  with_data);
+	if (err)
+		mlx5_ib_warn(dev, "UMR post send failed, err %d\n", err);
+	else {
+		wait_for_completion(&umr_context.done);
+		if (umr_context.status != IB_WC_SUCCESS) {
+			mlx5_ib_warn(dev, "reg umr failed (%u)\n",
+				     umr_context.status);
+			err = -EFAULT;
+		}
+	}
+	up(&umrc->sem);
+	return err;
+}
* Unmerged path drivers/infiniband/hw/mlx5/umr.h
