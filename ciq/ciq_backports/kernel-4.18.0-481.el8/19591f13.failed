RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit 19591f134c59703dfc272356808e6fe2037d0d40
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/19591f13.failed

total_mrs is used only to calculate the number of mkeys currently in
use. To simplify things, replace it with a new member called "in_use" and
directly store the number of mkeys currently in use.

Link: https://lore.kernel.org/r/20220726071911.122765-4-michaelgur@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 19591f134c59703dfc272356808e6fe2037d0d40)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index bc16ce369c59,da9202f4b5f3..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -770,18 -755,11 +770,26 @@@ struct mlx5_cache_ent 
  	u8 fill_to_high_water:1;
  
  	/*
++<<<<<<< HEAD
 +	 * - available_mrs is the length of list head, ie the number of MRs
 +	 *   available for immediate allocation.
 +	 * - total_mrs is available_mrs plus all in use MRs that could be
 +	 *   returned to the cache.
 +	 * - limit is the low water mark for available_mrs, 2* limit is the
++=======
+ 	 * - limit is the low water mark for stored mkeys, 2* limit is the
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  	 *   upper water mark.
 +	 * - pending is the number of MRs currently being created
  	 */
++<<<<<<< HEAD
 +	u32 total_mrs;
 +	u32 available_mrs;
++=======
+ 	u32 in_use;
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  	u32 limit;
 +	u32 pending;
  
  	/* Statistics */
  	u32                     miss;
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 2aee9118633b,26bfdbba24b4..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -183,14 -266,11 +183,19 @@@ static void create_mkey_callback(int st
  
  	WRITE_ONCE(dev->cache.last_add, jiffies);
  
++<<<<<<< HEAD
 +	spin_lock_irqsave(&ent->lock, flags);
 +	list_add_tail(&mr->list, &ent->head);
 +	ent->available_mrs++;
 +	ent->total_mrs++;
++=======
+ 	xa_lock_irqsave(&ent->mkeys, flags);
+ 	push_to_reserved(ent, mr);
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  	/* If we are doing fill_to_high_water then keep going. */
  	queue_adjust_cache_locked(ent);
 -	xa_unlock_irqrestore(&ent->mkeys, flags);
 +	ent->pending--;
 +	spin_unlock_irqrestore(&ent->lock, flags);
  }
  
  static int get_mkc_octo_size(unsigned int access_mode, unsigned int ndescs)
@@@ -308,9 -390,6 +313,12 @@@ static struct mlx5_ib_mr *create_cache_
  	init_waitqueue_head(&mr->mmkey.wait);
  	mr->mmkey.type = MLX5_MKEY_MR;
  	WRITE_ONCE(ent->dev->cache.last_add, jiffies);
++<<<<<<< HEAD
 +	spin_lock_irq(&ent->lock);
 +	ent->total_mrs++;
 +	spin_unlock_irq(&ent->lock);
++=======
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  	kfree(in);
  	return mr;
  free_mr:
@@@ -324,17 -403,14 +332,22 @@@ static void remove_cache_mr_locked(stru
  {
  	struct mlx5_ib_mr *mr;
  
 -	lockdep_assert_held(&ent->mkeys.xa_lock);
 -	if (!ent->stored)
 +	lockdep_assert_held(&ent->lock);
 +	if (list_empty(&ent->head))
  		return;
++<<<<<<< HEAD
 +	mr = list_first_entry(&ent->head, struct mlx5_ib_mr, list);
 +	list_del(&mr->list);
 +	ent->available_mrs--;
 +	ent->total_mrs--;
 +	spin_unlock_irq(&ent->lock);
++=======
+ 	mr = pop_stored_mkey(ent);
+ 	xa_unlock_irq(&ent->mkeys);
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  	mlx5_core_destroy_mkey(ent->dev->mdev, mr->mmkey.key);
  	kfree(mr);
 -	xa_lock_irq(&ent->mkeys);
 +	spin_lock_irq(&ent->lock);
  }
  
  static int resize_available_mrs(struct mlx5_cache_ent *ent, unsigned int target,
@@@ -381,15 -458,15 +394,24 @@@ static ssize_t size_write(struct file *
  
  	/*
  	 * Target is the new value of total_mrs the user requests, however we
 -	 * cannot free MRs that are in use. Compute the target value for stored
 -	 * mkeys.
 +	 * cannot free MRs that are in use. Compute the target value for
 +	 * available_mrs.
  	 */
++<<<<<<< HEAD
 +	spin_lock_irq(&ent->lock);
 +	if (target < ent->total_mrs - ent->available_mrs) {
 +		err = -EINVAL;
 +		goto err_unlock;
 +	}
 +	target = target - (ent->total_mrs - ent->available_mrs);
++=======
+ 	xa_lock_irq(&ent->mkeys);
+ 	if (target < ent->in_use) {
+ 		err = -EINVAL;
+ 		goto err_unlock;
+ 	}
+ 	target = target - ent->in_use;
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  	if (target < ent->limit || target > ent->limit*2) {
  		err = -EINVAL;
  		goto err_unlock;
@@@ -604,23 -680,27 +626,34 @@@ struct mlx5_ib_mr *mlx5_mr_cache_alloc(
  	struct mlx5_ib_mr *mr;
  
  	/* Matches access in alloc_cache_mr() */
 -	if (!mlx5r_umr_can_reconfig(dev, 0, access_flags))
 +	if (!mlx5_ib_can_reconfig_with_umr(dev, 0, access_flags))
  		return ERR_PTR(-EOPNOTSUPP);
  
++<<<<<<< HEAD
 +	spin_lock_irq(&ent->lock);
 +	if (list_empty(&ent->head)) {
++=======
+ 	xa_lock_irq(&ent->mkeys);
+ 	ent->in_use++;
+ 
+ 	if (!ent->stored) {
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  		queue_adjust_cache_locked(ent);
  		ent->miss++;
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  		mr = create_cache_mr(ent);
- 		if (IS_ERR(mr))
+ 		if (IS_ERR(mr)) {
+ 			xa_lock_irq(&ent->mkeys);
+ 			ent->in_use--;
+ 			xa_unlock_irq(&ent->mkeys);
  			return mr;
+ 		}
  	} else {
 -		mr = pop_stored_mkey(ent);
 +		mr = list_first_entry(&ent->head, struct mlx5_ib_mr, list);
 +		list_del(&mr->list);
 +		ent->available_mrs--;
  		queue_adjust_cache_locked(ent);
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  
  		mlx5_clear_mr(mr);
  	}
@@@ -653,29 -711,18 +686,36 @@@ static void clean_keys(struct mlx5_ib_d
  {
  	struct mlx5_mr_cache *cache = &dev->cache;
  	struct mlx5_cache_ent *ent = &cache->ent[c];
 +	struct mlx5_ib_mr *tmp_mr;
  	struct mlx5_ib_mr *mr;
 +	LIST_HEAD(del_list);
  
  	cancel_delayed_work(&ent->dwork);
++<<<<<<< HEAD
 +	while (1) {
 +		spin_lock_irq(&ent->lock);
 +		if (list_empty(&ent->head)) {
 +			spin_unlock_irq(&ent->lock);
 +			break;
 +		}
 +		mr = list_first_entry(&ent->head, struct mlx5_ib_mr, list);
 +		list_move(&mr->list, &del_list);
 +		ent->available_mrs--;
 +		ent->total_mrs--;
 +		spin_unlock_irq(&ent->lock);
++=======
+ 	xa_lock_irq(&ent->mkeys);
+ 	while (ent->stored) {
+ 		mr = pop_stored_mkey(ent);
+ 		xa_unlock_irq(&ent->mkeys);
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  		mlx5_core_destroy_mkey(dev->mdev, mr->mmkey.key);
 +	}
 +
 +	list_for_each_entry_safe(mr, tmp_mr, &del_list, list) {
 +		list_del(&mr->list);
  		kfree(mr);
 -		xa_lock_irq(&ent->mkeys);
  	}
 -	xa_unlock_irq(&ent->mkeys);
  }
  
  static void mlx5_mr_cache_debugfs_cleanup(struct mlx5_ib_dev *dev)
@@@ -1811,12 -1642,13 +1851,20 @@@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr
  
  	/* Stop DMA */
  	if (mr->cache_ent) {
++<<<<<<< HEAD
 +		if (revoke_mr(mr)) {
 +			spin_lock_irq(&mr->cache_ent->lock);
 +			mr->cache_ent->total_mrs--;
 +			spin_unlock_irq(&mr->cache_ent->lock);
++=======
+ 		xa_lock_irq(&mr->cache_ent->mkeys);
+ 		mr->cache_ent->in_use--;
+ 		xa_unlock_irq(&mr->cache_ent->mkeys);
+ 
+ 		if (mlx5r_umr_revoke_mr(mr) ||
+ 		    push_mkey(mr->cache_ent, false, mr))
++>>>>>>> 19591f134c59 (RDMA/mlx5: Store the number of in_use cache mkeys instead of total_mrs)
  			mr->cache_ent = NULL;
- 		}
  	}
  	if (!mr->cache_ent) {
  		rc = destroy_mkey(to_mdev(mr->ibmr.device), mr);
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
