x86/sev: Don't use cc_platform_has() for early SEV-SNP calls

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit cdaa0a407f1acd3a44861e3aea6e3c7349e668f1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/cdaa0a40.failed

When running identity-mapped and depending on the kernel configuration,
it is possible that the compiler uses jump tables when generating code
for cc_platform_has().

This causes a boot failure because the jump table uses un-mapped kernel
virtual addresses, not identity-mapped addresses. This has been seen
with CONFIG_RETPOLINE=n.

Similar to sme_encrypt_kernel(), use an open-coded direct check for the
status of SNP rather than trying to eliminate the jump table. This
preserves any code optimization in cc_platform_has() that can be useful
post boot. It also limits the changes to SEV-specific files so that
future compiler features won't necessarily require possible build changes
just because they are not compatible with running identity-mapped.

  [ bp: Massage commit message. ]

Fixes: 5e5ccff60a29 ("x86/sev: Add helper for validating pages in early enc attribute changes")
	Reported-by: Sean Christopherson <seanjc@google.com>
	Suggested-by: Sean Christopherson <seanjc@google.com>
	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Cc: <stable@vger.kernel.org> # 5.19.x
Link: https://lore.kernel.org/all/YqfabnTRxFSM+LoX@google.com/
(cherry picked from commit cdaa0a407f1acd3a44861e3aea6e3c7349e668f1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/sev.c
diff --cc arch/x86/kernel/sev.c
index 57edd1e021cf,4f84c3f11af5..000000000000
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@@ -539,7 -643,508 +539,512 @@@ static u64 get_jump_table_addr(void
  	return ret;
  }
  
++<<<<<<< HEAD
 +int sev_es_setup_ap_jump_table(struct real_mode_header *rmh)
++=======
+ static void pvalidate_pages(unsigned long vaddr, unsigned int npages, bool validate)
+ {
+ 	unsigned long vaddr_end;
+ 	int rc;
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		rc = pvalidate(vaddr, RMP_PG_SIZE_4K, validate);
+ 		if (WARN(rc, "Failed to validate address 0x%lx ret %d", vaddr, rc))
+ 			sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PVALIDATE);
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 	}
+ }
+ 
+ static void __init early_set_pages_state(unsigned long paddr, unsigned int npages, enum psc_op op)
+ {
+ 	unsigned long paddr_end;
+ 	u64 val;
+ 
+ 	paddr = paddr & PAGE_MASK;
+ 	paddr_end = paddr + (npages << PAGE_SHIFT);
+ 
+ 	while (paddr < paddr_end) {
+ 		/*
+ 		 * Use the MSR protocol because this function can be called before
+ 		 * the GHCB is established.
+ 		 */
+ 		sev_es_wr_ghcb_msr(GHCB_MSR_PSC_REQ_GFN(paddr >> PAGE_SHIFT, op));
+ 		VMGEXIT();
+ 
+ 		val = sev_es_rd_ghcb_msr();
+ 
+ 		if (WARN(GHCB_RESP_CODE(val) != GHCB_MSR_PSC_RESP,
+ 			 "Wrong PSC response code: 0x%x\n",
+ 			 (unsigned int)GHCB_RESP_CODE(val)))
+ 			goto e_term;
+ 
+ 		if (WARN(GHCB_MSR_PSC_RESP_VAL(val),
+ 			 "Failed to change page state to '%s' paddr 0x%lx error 0x%llx\n",
+ 			 op == SNP_PAGE_STATE_PRIVATE ? "private" : "shared",
+ 			 paddr, GHCB_MSR_PSC_RESP_VAL(val)))
+ 			goto e_term;
+ 
+ 		paddr = paddr + PAGE_SIZE;
+ 	}
+ 
+ 	return;
+ 
+ e_term:
+ 	sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ void __init early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr,
+ 					 unsigned int npages)
+ {
+ 	/*
+ 	 * This can be invoked in early boot while running identity mapped, so
+ 	 * use an open coded check for SNP instead of using cc_platform_has().
+ 	 * This eliminates worries about jump tables or checking boot_cpu_data
+ 	 * in the cc_platform_has() function.
+ 	 */
+ 	if (!(sev_status & MSR_AMD64_SEV_SNP_ENABLED))
+ 		return;
+ 
+ 	 /*
+ 	  * Ask the hypervisor to mark the memory pages as private in the RMP
+ 	  * table.
+ 	  */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	/* Validate the memory pages after they've been added in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
+ void __init early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr,
+ 					unsigned int npages)
+ {
+ 	/*
+ 	 * This can be invoked in early boot while running identity mapped, so
+ 	 * use an open coded check for SNP instead of using cc_platform_has().
+ 	 * This eliminates worries about jump tables or checking boot_cpu_data
+ 	 * in the cc_platform_has() function.
+ 	 */
+ 	if (!(sev_status & MSR_AMD64_SEV_SNP_ENABLED))
+ 		return;
+ 
+ 	/* Invalidate the memory pages before they are marked shared in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	 /* Ask hypervisor to mark the memory pages shared in the RMP table. */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op)
+ {
+ 	unsigned long vaddr, npages;
+ 
+ 	vaddr = (unsigned long)__va(paddr);
+ 	npages = PAGE_ALIGN(sz) >> PAGE_SHIFT;
+ 
+ 	if (op == SNP_PAGE_STATE_PRIVATE)
+ 		early_snp_set_memory_private(vaddr, paddr, npages);
+ 	else if (op == SNP_PAGE_STATE_SHARED)
+ 		early_snp_set_memory_shared(vaddr, paddr, npages);
+ 	else
+ 		WARN(1, "invalid memory op %d\n", op);
+ }
+ 
+ static int vmgexit_psc(struct snp_psc_desc *desc)
+ {
+ 	int cur_entry, end_entry, ret = 0;
+ 	struct snp_psc_desc *data;
+ 	struct ghcb_state state;
+ 	struct es_em_ctxt ctxt;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 
+ 	/*
+ 	 * __sev_get_ghcb() needs to run with IRQs disabled because it is using
+ 	 * a per-CPU GHCB.
+ 	 */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 	if (!ghcb) {
+ 		ret = 1;
+ 		goto out_unlock;
+ 	}
+ 
+ 	/* Copy the input desc into GHCB shared buffer */
+ 	data = (struct snp_psc_desc *)ghcb->shared_buffer;
+ 	memcpy(ghcb->shared_buffer, desc, min_t(int, GHCB_SHARED_BUF_SIZE, sizeof(*desc)));
+ 
+ 	/*
+ 	 * As per the GHCB specification, the hypervisor can resume the guest
+ 	 * before processing all the entries. Check whether all the entries
+ 	 * are processed. If not, then keep retrying. Note, the hypervisor
+ 	 * will update the data memory directly to indicate the status, so
+ 	 * reference the data->hdr everywhere.
+ 	 *
+ 	 * The strategy here is to wait for the hypervisor to change the page
+ 	 * state in the RMP table before guest accesses the memory pages. If the
+ 	 * page state change was not successful, then later memory access will
+ 	 * result in a crash.
+ 	 */
+ 	cur_entry = data->hdr.cur_entry;
+ 	end_entry = data->hdr.end_entry;
+ 
+ 	while (data->hdr.cur_entry <= data->hdr.end_entry) {
+ 		ghcb_set_sw_scratch(ghcb, (u64)__pa(data));
+ 
+ 		/* This will advance the shared buffer data points to. */
+ 		ret = sev_es_ghcb_hv_call(ghcb, &ctxt, SVM_VMGEXIT_PSC, 0, 0);
+ 
+ 		/*
+ 		 * Page State Change VMGEXIT can pass error code through
+ 		 * exit_info_2.
+ 		 */
+ 		if (WARN(ret || ghcb->save.sw_exit_info_2,
+ 			 "SNP: PSC failed ret=%d exit_info_2=%llx\n",
+ 			 ret, ghcb->save.sw_exit_info_2)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/* Verify that reserved bit is not set */
+ 		if (WARN(data->hdr.reserved, "Reserved bit is set in the PSC header\n")) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/*
+ 		 * Sanity check that entry processing is not going backwards.
+ 		 * This will happen only if hypervisor is tricking us.
+ 		 */
+ 		if (WARN(data->hdr.end_entry > end_entry || cur_entry > data->hdr.cur_entry,
+ "SNP: PSC processing going backward, end_entry %d (got %d) cur_entry %d (got %d)\n",
+ 			 end_entry, data->hdr.end_entry, cur_entry, data->hdr.cur_entry)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 	}
+ 
+ out:
+ 	__sev_put_ghcb(&state);
+ 
+ out_unlock:
+ 	local_irq_restore(flags);
+ 
+ 	return ret;
+ }
+ 
+ static void __set_pages_state(struct snp_psc_desc *data, unsigned long vaddr,
+ 			      unsigned long vaddr_end, int op)
+ {
+ 	struct psc_hdr *hdr;
+ 	struct psc_entry *e;
+ 	unsigned long pfn;
+ 	int i;
+ 
+ 	hdr = &data->hdr;
+ 	e = data->entries;
+ 
+ 	memset(data, 0, sizeof(*data));
+ 	i = 0;
+ 
+ 	while (vaddr < vaddr_end) {
+ 		if (is_vmalloc_addr((void *)vaddr))
+ 			pfn = vmalloc_to_pfn((void *)vaddr);
+ 		else
+ 			pfn = __pa(vaddr) >> PAGE_SHIFT;
+ 
+ 		e->gfn = pfn;
+ 		e->operation = op;
+ 		hdr->end_entry = i;
+ 
+ 		/*
+ 		 * Current SNP implementation doesn't keep track of the RMP page
+ 		 * size so use 4K for simplicity.
+ 		 */
+ 		e->pagesize = RMP_PG_SIZE_4K;
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 		e++;
+ 		i++;
+ 	}
+ 
+ 	if (vmgexit_psc(data))
+ 		sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ static void set_pages_state(unsigned long vaddr, unsigned int npages, int op)
+ {
+ 	unsigned long vaddr_end, next_vaddr;
+ 	struct snp_psc_desc *desc;
+ 
+ 	desc = kmalloc(sizeof(*desc), GFP_KERNEL_ACCOUNT);
+ 	if (!desc)
+ 		panic("SNP: failed to allocate memory for PSC descriptor\n");
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		/* Calculate the last vaddr that fits in one struct snp_psc_desc. */
+ 		next_vaddr = min_t(unsigned long, vaddr_end,
+ 				   (VMGEXIT_PSC_MAX_ENTRY * PAGE_SIZE) + vaddr);
+ 
+ 		__set_pages_state(desc, vaddr, next_vaddr, op);
+ 
+ 		vaddr = next_vaddr;
+ 	}
+ 
+ 	kfree(desc);
+ }
+ 
+ void snp_set_memory_shared(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void snp_set_memory_private(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
+ static int snp_set_vmsa(void *va, bool vmsa)
+ {
+ 	u64 attrs;
+ 
+ 	/*
+ 	 * Running at VMPL0 allows the kernel to change the VMSA bit for a page
+ 	 * using the RMPADJUST instruction. However, for the instruction to
+ 	 * succeed it must target the permissions of a lesser privileged
+ 	 * (higher numbered) VMPL level, so use VMPL1 (refer to the RMPADJUST
+ 	 * instruction in the AMD64 APM Volume 3).
+ 	 */
+ 	attrs = 1;
+ 	if (vmsa)
+ 		attrs |= RMPADJUST_VMSA_PAGE_BIT;
+ 
+ 	return rmpadjust((unsigned long)va, RMP_PG_SIZE_4K, attrs);
+ }
+ 
+ #define __ATTR_BASE		(SVM_SELECTOR_P_MASK | SVM_SELECTOR_S_MASK)
+ #define INIT_CS_ATTRIBS		(__ATTR_BASE | SVM_SELECTOR_READ_MASK | SVM_SELECTOR_CODE_MASK)
+ #define INIT_DS_ATTRIBS		(__ATTR_BASE | SVM_SELECTOR_WRITE_MASK)
+ 
+ #define INIT_LDTR_ATTRIBS	(SVM_SELECTOR_P_MASK | 2)
+ #define INIT_TR_ATTRIBS		(SVM_SELECTOR_P_MASK | 3)
+ 
+ static void *snp_alloc_vmsa_page(void)
+ {
+ 	struct page *p;
+ 
+ 	/*
+ 	 * Allocate VMSA page to work around the SNP erratum where the CPU will
+ 	 * incorrectly signal an RMP violation #PF if a large page (2MB or 1GB)
+ 	 * collides with the RMP entry of VMSA page. The recommended workaround
+ 	 * is to not use a large page.
+ 	 *
+ 	 * Allocate an 8k page which is also 8k-aligned.
+ 	 */
+ 	p = alloc_pages(GFP_KERNEL_ACCOUNT | __GFP_ZERO, 1);
+ 	if (!p)
+ 		return NULL;
+ 
+ 	split_page(p, 1);
+ 
+ 	/* Free the first 4k. This page may be 2M/1G aligned and cannot be used. */
+ 	__free_page(p);
+ 
+ 	return page_address(p + 1);
+ }
+ 
+ static void snp_cleanup_vmsa(struct sev_es_save_area *vmsa)
+ {
+ 	int err;
+ 
+ 	err = snp_set_vmsa(vmsa, false);
+ 	if (err)
+ 		pr_err("clear VMSA page failed (%u), leaking page\n", err);
+ 	else
+ 		free_page((unsigned long)vmsa);
+ }
+ 
+ static int wakeup_cpu_via_vmgexit(int apic_id, unsigned long start_ip)
+ {
+ 	struct sev_es_save_area *cur_vmsa, *vmsa;
+ 	struct ghcb_state state;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 	u8 sipi_vector;
+ 	int cpu, ret;
+ 	u64 cr4;
+ 
+ 	/*
+ 	 * The hypervisor SNP feature support check has happened earlier, just check
+ 	 * the AP_CREATION one here.
+ 	 */
+ 	if (!(sev_hv_features & GHCB_HV_FT_SNP_AP_CREATION))
+ 		return -EOPNOTSUPP;
+ 
+ 	/*
+ 	 * Verify the desired start IP against the known trampoline start IP
+ 	 * to catch any future new trampolines that may be introduced that
+ 	 * would require a new protected guest entry point.
+ 	 */
+ 	if (WARN_ONCE(start_ip != real_mode_header->trampoline_start,
+ 		      "Unsupported SNP start_ip: %lx\n", start_ip))
+ 		return -EINVAL;
+ 
+ 	/* Override start_ip with known protected guest start IP */
+ 	start_ip = real_mode_header->sev_es_trampoline_start;
+ 
+ 	/* Find the logical CPU for the APIC ID */
+ 	for_each_present_cpu(cpu) {
+ 		if (arch_match_cpu_phys_id(cpu, apic_id))
+ 			break;
+ 	}
+ 	if (cpu >= nr_cpu_ids)
+ 		return -EINVAL;
+ 
+ 	cur_vmsa = per_cpu(sev_vmsa, cpu);
+ 
+ 	/*
+ 	 * A new VMSA is created each time because there is no guarantee that
+ 	 * the current VMSA is the kernels or that the vCPU is not running. If
+ 	 * an attempt was done to use the current VMSA with a running vCPU, a
+ 	 * #VMEXIT of that vCPU would wipe out all of the settings being done
+ 	 * here.
+ 	 */
+ 	vmsa = (struct sev_es_save_area *)snp_alloc_vmsa_page();
+ 	if (!vmsa)
+ 		return -ENOMEM;
+ 
+ 	/* CR4 should maintain the MCE value */
+ 	cr4 = native_read_cr4() & X86_CR4_MCE;
+ 
+ 	/* Set the CS value based on the start_ip converted to a SIPI vector */
+ 	sipi_vector		= (start_ip >> 12);
+ 	vmsa->cs.base		= sipi_vector << 12;
+ 	vmsa->cs.limit		= AP_INIT_CS_LIMIT;
+ 	vmsa->cs.attrib		= INIT_CS_ATTRIBS;
+ 	vmsa->cs.selector	= sipi_vector << 8;
+ 
+ 	/* Set the RIP value based on start_ip */
+ 	vmsa->rip		= start_ip & 0xfff;
+ 
+ 	/* Set AP INIT defaults as documented in the APM */
+ 	vmsa->ds.limit		= AP_INIT_DS_LIMIT;
+ 	vmsa->ds.attrib		= INIT_DS_ATTRIBS;
+ 	vmsa->es		= vmsa->ds;
+ 	vmsa->fs		= vmsa->ds;
+ 	vmsa->gs		= vmsa->ds;
+ 	vmsa->ss		= vmsa->ds;
+ 
+ 	vmsa->gdtr.limit	= AP_INIT_GDTR_LIMIT;
+ 	vmsa->ldtr.limit	= AP_INIT_LDTR_LIMIT;
+ 	vmsa->ldtr.attrib	= INIT_LDTR_ATTRIBS;
+ 	vmsa->idtr.limit	= AP_INIT_IDTR_LIMIT;
+ 	vmsa->tr.limit		= AP_INIT_TR_LIMIT;
+ 	vmsa->tr.attrib		= INIT_TR_ATTRIBS;
+ 
+ 	vmsa->cr4		= cr4;
+ 	vmsa->cr0		= AP_INIT_CR0_DEFAULT;
+ 	vmsa->dr7		= DR7_RESET_VALUE;
+ 	vmsa->dr6		= AP_INIT_DR6_DEFAULT;
+ 	vmsa->rflags		= AP_INIT_RFLAGS_DEFAULT;
+ 	vmsa->g_pat		= AP_INIT_GPAT_DEFAULT;
+ 	vmsa->xcr0		= AP_INIT_XCR0_DEFAULT;
+ 	vmsa->mxcsr		= AP_INIT_MXCSR_DEFAULT;
+ 	vmsa->x87_ftw		= AP_INIT_X87_FTW_DEFAULT;
+ 	vmsa->x87_fcw		= AP_INIT_X87_FCW_DEFAULT;
+ 
+ 	/* SVME must be set. */
+ 	vmsa->efer		= EFER_SVME;
+ 
+ 	/*
+ 	 * Set the SNP-specific fields for this VMSA:
+ 	 *   VMPL level
+ 	 *   SEV_FEATURES (matches the SEV STATUS MSR right shifted 2 bits)
+ 	 */
+ 	vmsa->vmpl		= 0;
+ 	vmsa->sev_features	= sev_status >> 2;
+ 
+ 	/* Switch the page over to a VMSA page now that it is initialized */
+ 	ret = snp_set_vmsa(vmsa, true);
+ 	if (ret) {
+ 		pr_err("set VMSA page failed (%u)\n", ret);
+ 		free_page((unsigned long)vmsa);
+ 
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* Issue VMGEXIT AP Creation NAE event */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 
+ 	vc_ghcb_invalidate(ghcb);
+ 	ghcb_set_rax(ghcb, vmsa->sev_features);
+ 	ghcb_set_sw_exit_code(ghcb, SVM_VMGEXIT_AP_CREATION);
+ 	ghcb_set_sw_exit_info_1(ghcb, ((u64)apic_id << 32) | SVM_VMGEXIT_AP_CREATE);
+ 	ghcb_set_sw_exit_info_2(ghcb, __pa(vmsa));
+ 
+ 	sev_es_wr_ghcb_msr(__pa(ghcb));
+ 	VMGEXIT();
+ 
+ 	if (!ghcb_sw_exit_info_1_is_valid(ghcb) ||
+ 	    lower_32_bits(ghcb->save.sw_exit_info_1)) {
+ 		pr_err("SNP AP Creation error\n");
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	__sev_put_ghcb(&state);
+ 
+ 	local_irq_restore(flags);
+ 
+ 	/* Perform cleanup if there was an error */
+ 	if (ret) {
+ 		snp_cleanup_vmsa(vmsa);
+ 		vmsa = NULL;
+ 	}
+ 
+ 	/* Free up any previous VMSA page */
+ 	if (cur_vmsa)
+ 		snp_cleanup_vmsa(cur_vmsa);
+ 
+ 	/* Record the current VMSA page */
+ 	per_cpu(sev_vmsa, cpu) = vmsa;
+ 
+ 	return ret;
+ }
+ 
+ void snp_set_wakeup_secondary_cpu(void)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	/*
+ 	 * Always set this override if SNP is enabled. This makes it the
+ 	 * required method to start APs under SNP. If the hypervisor does
+ 	 * not support AP creation, then no APs will be started.
+ 	 */
+ 	apic->wakeup_secondary_cpu = wakeup_cpu_via_vmgexit;
+ }
+ 
+ int __init sev_es_setup_ap_jump_table(struct real_mode_header *rmh)
++>>>>>>> cdaa0a407f1a (x86/sev: Don't use cc_platform_has() for early SEV-SNP calls)
  {
  	u16 startup_cs, startup_ip;
  	phys_addr_t jump_table_pa;
* Unmerged path arch/x86/kernel/sev.c
