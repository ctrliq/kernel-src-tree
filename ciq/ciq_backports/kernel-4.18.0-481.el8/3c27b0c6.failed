perf/x86/amd: Fix AMD BRS period adjustment

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Peter Zijlstra <peterz@infradead.org>
commit 3c27b0c6ea48bc61492a138c410e262735d660ab
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/3c27b0c6.failed

There's two problems with the current amd_brs_adjust_period() code:

 - it isn't in fact AMD specific and wil always adjust the period;

 - it adjusts the period, while it should only adjust the event count,
   resulting in repoting a short period.

Fix this by using x86_pmu.limit_period, this makes it specific to the
AMD BRS case and ensures only the event count is adjusted while the
reported period is unmodified.

Fixes: ba2fe7500845 ("perf/x86/amd: Add AMD branch sampling period adjustment")
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
(cherry picked from commit 3c27b0c6ea48bc61492a138c410e262735d660ab)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/amd/core.c
#	arch/x86/events/perf_event.h
diff --cc arch/x86/events/amd/core.c
index fc84e3f05348,3eee59c64daa..000000000000
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@@ -904,6 -1248,25 +904,28 @@@ static ssize_t amd_event_sysfs_show(cha
  	return x86_event_sysfs_show(page, config, event);
  }
  
++<<<<<<< HEAD
++=======
+ static void amd_pmu_sched_task(struct perf_event_context *ctx,
+ 				 bool sched_in)
+ {
+ 	if (sched_in && x86_pmu.lbr_nr)
+ 		amd_pmu_brs_sched_task(ctx, sched_in);
+ }
+ 
+ static u64 amd_pmu_limit_period(struct perf_event *event, u64 left)
+ {
+ 	/*
+ 	 * Decrease period by the depth of the BRS feature to get the last N
+ 	 * taken branches and approximate the desired period
+ 	 */
+ 	if (has_branch_stack(event) && left > x86_pmu.lbr_nr)
+ 		left -= x86_pmu.lbr_nr;
+ 
+ 	return left;
+ }
+ 
++>>>>>>> 3c27b0c6ea48 (perf/x86/amd: Fix AMD BRS period adjustment)
  static __initconst const struct x86_pmu amd_pmu = {
  	.name			= "AMD",
  	.handle_irq		= amd_pmu_handle_irq,
@@@ -988,6 -1421,23 +1010,26 @@@ static int __init amd_core_pmu_init(voi
  		x86_pmu.flags |= PMU_FL_PAIR;
  	}
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * BRS requires special event constraints and flushing on ctxsw.
+ 	 */
+ 	if (boot_cpu_data.x86 >= 0x19 && !amd_brs_init()) {
+ 		x86_pmu.get_event_constraints = amd_get_event_constraints_f19h;
+ 		x86_pmu.sched_task = amd_pmu_sched_task;
+ 		x86_pmu.limit_period = amd_pmu_limit_period;
+ 		/*
+ 		 * put_event_constraints callback same as Fam17h, set above
+ 		 */
+ 
+ 		/* branch sampling must be stopped when entering low power */
+ 		amd_brs_lopwr_init();
+ 	}
+ 
+ 	x86_pmu.attr_update = amd_attr_update;
+ 
++>>>>>>> 3c27b0c6ea48 (perf/x86/amd: Fix AMD BRS period adjustment)
  	pr_cont("core perfctr, ");
  	return 0;
  }
diff --cc arch/x86/events/perf_event.h
index 139832e590d3,21a5482bcf84..000000000000
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@@ -1223,6 -1219,75 +1223,78 @@@ static inline bool fixed_counter_disabl
  
  int amd_pmu_init(void);
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_PERF_EVENTS_AMD_BRS
+ int amd_brs_init(void);
+ void amd_brs_disable(void);
+ void amd_brs_enable(void);
+ void amd_brs_enable_all(void);
+ void amd_brs_disable_all(void);
+ void amd_brs_drain(void);
+ void amd_brs_lopwr_init(void);
+ void amd_brs_disable_all(void);
+ int amd_brs_setup_filter(struct perf_event *event);
+ void amd_brs_reset(void);
+ 
+ static inline void amd_pmu_brs_add(struct perf_event *event)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 
+ 	perf_sched_cb_inc(event->ctx->pmu);
+ 	cpuc->lbr_users++;
+ 	/*
+ 	 * No need to reset BRS because it is reset
+ 	 * on brs_enable() and it is saturating
+ 	 */
+ }
+ 
+ static inline void amd_pmu_brs_del(struct perf_event *event)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 
+ 	cpuc->lbr_users--;
+ 	WARN_ON_ONCE(cpuc->lbr_users < 0);
+ 
+ 	perf_sched_cb_dec(event->ctx->pmu);
+ }
+ 
+ void amd_pmu_brs_sched_task(struct perf_event_context *ctx, bool sched_in);
+ #else
+ static inline int amd_brs_init(void)
+ {
+ 	return 0;
+ }
+ static inline void amd_brs_disable(void) {}
+ static inline void amd_brs_enable(void) {}
+ static inline void amd_brs_drain(void) {}
+ static inline void amd_brs_lopwr_init(void) {}
+ static inline void amd_brs_disable_all(void) {}
+ static inline int amd_brs_setup_filter(struct perf_event *event)
+ {
+ 	return 0;
+ }
+ static inline void amd_brs_reset(void) {}
+ 
+ static inline void amd_pmu_brs_add(struct perf_event *event)
+ {
+ }
+ 
+ static inline void amd_pmu_brs_del(struct perf_event *event)
+ {
+ }
+ 
+ static inline void amd_pmu_brs_sched_task(struct perf_event_context *ctx, bool sched_in)
+ {
+ }
+ 
+ static inline void amd_brs_enable_all(void)
+ {
+ }
+ 
+ #endif
+ 
++>>>>>>> 3c27b0c6ea48 (perf/x86/amd: Fix AMD BRS period adjustment)
  #else /* CONFIG_CPU_SUP_AMD */
  
  static inline int amd_pmu_init(void)
@@@ -1230,6 -1295,22 +1302,25 @@@
  	return 0;
  }
  
++<<<<<<< HEAD
++=======
+ static inline int amd_brs_init(void)
+ {
+ 	return -EOPNOTSUPP;
+ }
+ 
+ static inline void amd_brs_drain(void)
+ {
+ }
+ 
+ static inline void amd_brs_enable_all(void)
+ {
+ }
+ 
+ static inline void amd_brs_disable_all(void)
+ {
+ }
++>>>>>>> 3c27b0c6ea48 (perf/x86/amd: Fix AMD BRS period adjustment)
  #endif /* CONFIG_CPU_SUP_AMD */
  
  static inline int is_pebs_pt(struct perf_event *event)
* Unmerged path arch/x86/events/amd/core.c
* Unmerged path arch/x86/events/perf_event.h
