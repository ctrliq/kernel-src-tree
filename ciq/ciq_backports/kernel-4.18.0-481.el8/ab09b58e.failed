x86/boot/compressed/64: Use TEST %reg,%reg instead of CMP $0,%reg

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
Rebuild_CHGLOG: - x86/boot/compressed/64: Use TEST reg,reg instead of CMP $0,reg (Wander Lairson Costa) [1836977]
Rebuild_FUZZ: 97.64%
commit-author Uros Bizjak <ubizjak@gmail.com>
commit ab09b58e4bdfdbcec425e54ebeaf6e209a96318f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/ab09b58e.failed

Use TEST %reg,%reg which sets the zero flag in the same way as CMP
$0,%reg, but the encoding uses one byte less.

	Signed-off-by: Uros Bizjak <ubizjak@gmail.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
	Reviewed-by: Andy Lutomirski <luto@kernel.org>
Link: https://lkml.kernel.org/r/20201029160258.139216-1-ubizjak@gmail.com
(cherry picked from commit ab09b58e4bdfdbcec425e54ebeaf6e209a96318f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/head_64.S
diff --cc arch/x86/boot/compressed/head_64.S
index f7943d0ad386,e94874f4bbc1..000000000000
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@@ -205,15 -238,30 +205,37 @@@ SYM_FUNC_START(startup_32
  	 * We place all of the values on our mini stack so lret can
  	 * used to perform that far jump.
  	 */
 -	leal	rva(startup_64)(%ebp), %eax
 +	pushl	$__KERNEL_CS
 +	leal	startup_64(%ebp), %eax
  #ifdef CONFIG_EFI_MIXED
++<<<<<<< HEAD
 +	movl	efi32_config(%ebp), %ebx
 +	cmp	$0, %ebx
 +	jz	1f
 +	leal	handover_entry(%ebp), %eax
++=======
+ 	movl	rva(efi32_boot_args)(%ebp), %edi
+ 	testl	%edi, %edi
+ 	jz	1f
+ 	leal	rva(efi64_stub_entry)(%ebp), %eax
+ 	movl	rva(efi32_boot_args+4)(%ebp), %esi
+ 	movl	rva(efi32_boot_args+8)(%ebp), %edx	// saved bootparams pointer
+ 	testl	%edx, %edx
+ 	jnz	1f
+ 	/*
+ 	 * efi_pe_entry uses MS calling convention, which requires 32 bytes of
+ 	 * shadow space on the stack even if all arguments are passed in
+ 	 * registers. We also need an additional 8 bytes for the space that
+ 	 * would be occupied by the return address, and this also results in
+ 	 * the correct stack alignment for entry.
+ 	 */
+ 	subl	$40, %esp
+ 	leal	rva(efi_pe_entry)(%ebp), %eax
+ 	movl	%edi, %ecx			// MS calling convention
+ 	movl	%esi, %edx
++>>>>>>> ab09b58e4bdf (x86/boot/compressed/64: Use TEST %reg,%reg instead of CMP $0,%reg)
  1:
  #endif
 -	pushl	$__KERNEL_CS
  	pushl	%eax
  
  	/* Enter paged protected Mode, activating Long Mode */
* Unmerged path arch/x86/boot/compressed/head_64.S
