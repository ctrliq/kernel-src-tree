RDMA/mlx5: Clean UMR QP type flow from mlx5_ib_post_send()

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit c8a02e38f86fbab30aab6261662076516cfb9ec3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/c8a02e38.failed

No internal UMR operation is using mlx5_ib_post_send(), remove the UMR QP
type logic from this function.

Link: https://lore.kernel.org/r/0b2f368f14bc9266ebdf92a601ca4e1e5b1e1188.1649747695.git.leonro@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Michael Guralnik <michaelgur@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit c8a02e38f86fbab30aab6261662076516cfb9ec3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/umr.c
#	drivers/infiniband/hw/mlx5/umr.h
#	drivers/infiniband/hw/mlx5/wr.c
diff --cc drivers/infiniband/hw/mlx5/umr.c
index 46eaf919eb49,3a48364c0918..000000000000
--- a/drivers/infiniband/hw/mlx5/umr.c
+++ b/drivers/infiniband/hw/mlx5/umr.c
@@@ -1,8 -1,98 +1,100 @@@
  // SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB
  /* Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. */
  
 -#include <rdma/ib_umem_odp.h>
  #include "mlx5_ib.h"
  #include "umr.h"
++<<<<<<< HEAD
++=======
+ #include "wr.h"
+ 
+ /*
+  * We can't use an array for xlt_emergency_page because dma_map_single doesn't
+  * work on kernel modules memory
+  */
+ void *xlt_emergency_page;
+ static DEFINE_MUTEX(xlt_emergency_page_mutex);
+ 
+ static __be64 get_umr_enable_mr_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_KEY |
+ 		 MLX5_MKEY_MASK_FREE;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_disable_mr_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_FREE;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_update_translation_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_LEN |
+ 		 MLX5_MKEY_MASK_PAGE_SIZE |
+ 		 MLX5_MKEY_MASK_START_ADDR;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_update_access_mask(struct mlx5_ib_dev *dev)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_LR |
+ 		 MLX5_MKEY_MASK_LW |
+ 		 MLX5_MKEY_MASK_RR |
+ 		 MLX5_MKEY_MASK_RW;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, atomic))
+ 		result |= MLX5_MKEY_MASK_A;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))
+ 		result |= MLX5_MKEY_MASK_RELAXED_ORDERING_WRITE;
+ 
+ 	if (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))
+ 		result |= MLX5_MKEY_MASK_RELAXED_ORDERING_READ;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static __be64 get_umr_update_pd_mask(void)
+ {
+ 	u64 result;
+ 
+ 	result = MLX5_MKEY_MASK_PD;
+ 
+ 	return cpu_to_be64(result);
+ }
+ 
+ static int umr_check_mkey_mask(struct mlx5_ib_dev *dev, u64 mask)
+ {
+ 	if (mask & MLX5_MKEY_MASK_PAGE_SIZE &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))
+ 		return -EPERM;
+ 
+ 	if (mask & MLX5_MKEY_MASK_A &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
+ 		return -EPERM;
+ 
+ 	if (mask & MLX5_MKEY_MASK_RELAXED_ORDERING_WRITE &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))
+ 		return -EPERM;
+ 
+ 	if (mask & MLX5_MKEY_MASK_RELAXED_ORDERING_READ &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))
+ 		return -EPERM;
+ 
+ 	return 0;
+ }
++>>>>>>> c8a02e38f86f (RDMA/mlx5: Clean UMR QP type flow from mlx5_ib_post_send())
  
  enum {
  	MAX_UMR_WR = 128,
diff --cc drivers/infiniband/hw/mlx5/umr.h
index cb1a2c95aac2,c9d0021381a2..000000000000
--- a/drivers/infiniband/hw/mlx5/umr.h
+++ b/drivers/infiniband/hw/mlx5/umr.h
@@@ -9,4 -9,89 +9,85 @@@
  int mlx5r_umr_resource_init(struct mlx5_ib_dev *dev);
  void mlx5r_umr_resource_cleanup(struct mlx5_ib_dev *dev);
  
++<<<<<<< HEAD
++=======
+ static inline bool mlx5r_umr_can_load_pas(struct mlx5_ib_dev *dev,
+ 					  size_t length)
+ {
+ 	/*
+ 	 * umr_check_mkey_mask() rejects MLX5_MKEY_MASK_PAGE_SIZE which is
+ 	 * always set if MLX5_IB_SEND_UMR_UPDATE_TRANSLATION (aka
+ 	 * MLX5_IB_UPD_XLT_ADDR and MLX5_IB_UPD_XLT_ENABLE) is set. Thus, a mkey
+ 	 * can never be enabled without this capability. Simplify this weird
+ 	 * quirky hardware by just saying it can't use PAS lists with UMR at
+ 	 * all.
+ 	 */
+ 	if (MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))
+ 		return false;
+ 
+ 	/*
+ 	 * length is the size of the MR in bytes when mlx5_ib_update_xlt() is
+ 	 * used.
+ 	 */
+ 	if (!MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset) &&
+ 	    length >= MLX5_MAX_UMR_PAGES * PAGE_SIZE)
+ 		return false;
+ 	return true;
+ }
+ 
+ /*
+  * true if an existing MR can be reconfigured to new access_flags using UMR.
+  * Older HW cannot use UMR to update certain elements of the MKC. See
+  * get_umr_update_access_mask() and umr_check_mkey_mask()
+  */
+ static inline bool mlx5r_umr_can_reconfig(struct mlx5_ib_dev *dev,
+ 					  unsigned int current_access_flags,
+ 					  unsigned int target_access_flags)
+ {
+ 	unsigned int diffs = current_access_flags ^ target_access_flags;
+ 
+ 	if ((diffs & IB_ACCESS_REMOTE_ATOMIC) &&
+ 	    MLX5_CAP_GEN(dev->mdev, atomic) &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
+ 		return false;
+ 
+ 	if ((diffs & IB_ACCESS_RELAXED_ORDERING) &&
+ 	    MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write) &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))
+ 		return false;
+ 
+ 	if ((diffs & IB_ACCESS_RELAXED_ORDERING) &&
+ 	    MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read) &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static inline u64 mlx5r_umr_get_xlt_octo(u64 bytes)
+ {
+ 	return ALIGN(bytes, MLX5_IB_UMR_XLT_ALIGNMENT) /
+ 	       MLX5_IB_UMR_OCTOWORD;
+ }
+ 
+ struct mlx5r_umr_context {
+ 	struct ib_cqe cqe;
+ 	enum ib_wc_status status;
+ 	struct completion done;
+ };
+ 
+ struct mlx5r_umr_wqe {
+ 	struct mlx5_wqe_umr_ctrl_seg ctrl_seg;
+ 	struct mlx5_mkey_seg mkey_seg;
+ 	struct mlx5_wqe_data_seg data_seg;
+ };
+ 
+ int mlx5r_umr_revoke_mr(struct mlx5_ib_mr *mr);
+ int mlx5r_umr_rereg_pd_access(struct mlx5_ib_mr *mr, struct ib_pd *pd,
+ 			      int access_flags);
+ int mlx5r_umr_update_mr_pas(struct mlx5_ib_mr *mr, unsigned int flags);
+ int mlx5r_umr_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
+ 			 int page_shift, int flags);
+ 
++>>>>>>> c8a02e38f86f (RDMA/mlx5: Clean UMR QP type flow from mlx5_ib_post_send())
  #endif /* _MLX5_IB_UMR_H */
diff --cc drivers/infiniband/hw/mlx5/wr.c
index da274281df3e,855f3f4fefad..000000000000
--- a/drivers/infiniband/hw/mlx5/wr.c
+++ b/drivers/infiniband/hw/mlx5/wr.c
@@@ -1192,35 -1022,6 +1155,38 @@@ static void handle_qpt_ud(struct mlx5_i
  	}
  }
  
++<<<<<<< HEAD
 +static int handle_qpt_reg_umr(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp,
 +			      const struct ib_send_wr *wr,
 +			      struct mlx5_wqe_ctrl_seg **ctrl, void **seg,
 +			      int *size, void **cur_edge, unsigned int idx)
 +{
 +	int err = 0;
 +
 +	if (unlikely(wr->opcode != MLX5_IB_WR_UMR)) {
 +		err = -EINVAL;
 +		mlx5_ib_warn(dev, "bad opcode %d\n", wr->opcode);
 +		goto out;
 +	}
 +
 +	qp->sq.wr_data[idx] = MLX5_IB_WR_UMR;
 +	(*ctrl)->imm = cpu_to_be32(umr_wr(wr)->mkey);
 +	err = set_reg_umr_segment(dev, *seg, wr);
 +	if (unlikely(err))
 +		goto out;
 +	*seg += sizeof(struct mlx5_wqe_umr_ctrl_seg);
 +	*size += sizeof(struct mlx5_wqe_umr_ctrl_seg) / 16;
 +	handle_post_send_edge(&qp->sq, seg, *size, cur_edge);
 +	set_reg_mkey_segment(dev, *seg, wr);
 +	*seg += sizeof(struct mlx5_mkey_seg);
 +	*size += sizeof(struct mlx5_mkey_seg) / 16;
 +	handle_post_send_edge(&qp->sq, seg, *size, cur_edge);
 +out:
 +	return err;
 +}
 +
++=======
++>>>>>>> c8a02e38f86f (RDMA/mlx5: Clean UMR QP type flow from mlx5_ib_post_send())
  void mlx5r_ring_db(struct mlx5_ib_qp *qp, unsigned int nreq,
  		   struct mlx5_wqe_ctrl_seg *ctrl)
  {
diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 568671fe4f18..ef6ff62370c5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -290,16 +290,9 @@ struct mlx5_ib_flow_db {
 };
 
 /* Use macros here so that don't have to duplicate
- * enum ib_send_flags and enum ib_qp_type for low-level driver
+ * enum ib_qp_type for low-level driver
  */
 
-#define MLX5_IB_SEND_UMR_ENABLE_MR	       (IB_SEND_RESERVED_START << 0)
-#define MLX5_IB_SEND_UMR_DISABLE_MR	       (IB_SEND_RESERVED_START << 1)
-#define MLX5_IB_SEND_UMR_FAIL_IF_FREE	       (IB_SEND_RESERVED_START << 2)
-#define MLX5_IB_SEND_UMR_UPDATE_XLT	       (IB_SEND_RESERVED_START << 3)
-#define MLX5_IB_SEND_UMR_UPDATE_TRANSLATION    (IB_SEND_RESERVED_START << 4)
-#define MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS       IB_SEND_RESERVED_END
-
 #define MLX5_IB_QPT_REG_UMR	IB_QPT_RESERVED1
 /*
  * IB_QPT_GSI creates the software wrapper around GSI, and MLX5_IB_QPT_HW_GSI
@@ -538,24 +531,6 @@ struct mlx5_ib_cq_buf {
 	int			nent;
 };
 
-struct mlx5_umr_wr {
-	struct ib_send_wr		wr;
-	u64				virt_addr;
-	u64				offset;
-	struct ib_pd		       *pd;
-	unsigned int			page_shift;
-	unsigned int			xlt_size;
-	u64				length;
-	int				access_flags;
-	u32				mkey;
-	u8				ignore_free_state:1;
-};
-
-static inline const struct mlx5_umr_wr *umr_wr(const struct ib_send_wr *wr)
-{
-	return container_of(wr, struct mlx5_umr_wr, wr);
-}
-
 enum mlx5_ib_cq_pr_flags {
 	MLX5_IB_CQ_PR_FLAGS_CQE_128_PAD	= 1 << 0,
 	MLX5_IB_CQ_PR_FLAGS_REAL_TIME_TS = 1 << 1,
* Unmerged path drivers/infiniband/hw/mlx5/umr.c
* Unmerged path drivers/infiniband/hw/mlx5/umr.h
* Unmerged path drivers/infiniband/hw/mlx5/wr.c
