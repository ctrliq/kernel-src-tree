RDMA/mlx5: Replace cache list with Xarray

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit 86457a92df1bebdcd8e20afa286427e4b525aa08
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/86457a92.failed

The Xarray allows us to store the cached mkeys in memory efficient way.

Entries are reserved in the Xarray using xa_cmpxchg before calling to the
upcoming callbacks to avoid allocations in interrupt context.  The
xa_cmpxchg can sleep when using GFP_KERNEL, so we call it in a loop to
ensure one reserved entry for each process trying to reserve.

Link: https://lore.kernel.org/r/20220726071911.122765-3-michaelgur@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 86457a92df1bebdcd8e20afa286427e4b525aa08)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index bc16ce369c59,e0eb666aefa1..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -754,11 -737,13 +752,17 @@@ struct umr_common 
  };
  
  struct mlx5_cache_ent {
++<<<<<<< HEAD
 +	struct list_head	head;
 +	/* sync access to the cahce entry
 +	 */
 +	spinlock_t		lock;
 +
++=======
+ 	struct xarray		mkeys;
+ 	unsigned long		stored;
+ 	unsigned long		reserved;
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  
  	char                    name[4];
  	u32                     order;
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 2aee9118633b,cbb8882c7787..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -168,10 -251,10 +266,15 @@@ static void create_mkey_callback(int st
  	if (status) {
  		create_mkey_warn(dev, status, mr->out);
  		kfree(mr);
++<<<<<<< HEAD
 +		spin_lock_irqsave(&ent->lock, flags);
 +		ent->pending--;
++=======
+ 		xa_lock_irqsave(&ent->mkeys, flags);
+ 		undo_push_reserve_mkey(ent);
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		WRITE_ONCE(dev->fill_delay, 1);
 -		xa_unlock_irqrestore(&ent->mkeys, flags);
 +		spin_unlock_irqrestore(&ent->lock, flags);
  		mod_timer(&dev->delay_timer, jiffies + HZ);
  		return;
  	}
@@@ -183,14 -266,12 +286,23 @@@
  
  	WRITE_ONCE(dev->cache.last_add, jiffies);
  
++<<<<<<< HEAD
 +	spin_lock_irqsave(&ent->lock, flags);
 +	list_add_tail(&mr->list, &ent->head);
 +	ent->available_mrs++;
 +	ent->total_mrs++;
 +	/* If we are doing fill_to_high_water then keep going. */
 +	queue_adjust_cache_locked(ent);
 +	ent->pending--;
 +	spin_unlock_irqrestore(&ent->lock, flags);
++=======
+ 	xa_lock_irqsave(&ent->mkeys, flags);
+ 	push_to_reserved(ent, mr);
+ 	ent->total_mrs++;
+ 	/* If we are doing fill_to_high_water then keep going. */
+ 	queue_adjust_cache_locked(ent);
+ 	xa_unlock_irqrestore(&ent->mkeys, flags);
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  }
  
  static int get_mkc_octo_size(unsigned int access_mode, unsigned int ndescs)
@@@ -252,28 -333,20 +364,38 @@@ static int add_keys(struct mlx5_cache_e
  		mr = alloc_cache_mr(ent, mkc);
  		if (!mr) {
  			err = -ENOMEM;
- 			break;
+ 			goto free_in;
  		}
++<<<<<<< HEAD
 +		spin_lock_irq(&ent->lock);
 +		if (ent->pending >= MAX_PENDING_REG_MR) {
 +			err = -EAGAIN;
 +			spin_unlock_irq(&ent->lock);
 +			kfree(mr);
 +			break;
 +		}
 +		ent->pending++;
 +		spin_unlock_irq(&ent->lock);
++=======
+ 
+ 		err = push_mkey(ent, true, NULL);
+ 		if (err)
+ 			goto free_mr;
+ 
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		err = mlx5_ib_create_mkey_cb(ent->dev, &mr->mmkey,
  					     &ent->dev->async_ctx, in, inlen,
  					     mr->out, sizeof(mr->out),
  					     &mr->cb_work);
  		if (err) {
++<<<<<<< HEAD
 +			spin_lock_irq(&ent->lock);
 +			ent->pending--;
 +			spin_unlock_irq(&ent->lock);
++=======
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  			mlx5_ib_warn(ent->dev, "create mkey failed %d\n", err);
- 			kfree(mr);
- 			break;
+ 			goto err_undo_reserve;
  		}
  	}
  
@@@ -324,17 -407,15 +456,20 @@@ static void remove_cache_mr_locked(stru
  {
  	struct mlx5_ib_mr *mr;
  
++<<<<<<< HEAD
 +	lockdep_assert_held(&ent->lock);
 +	if (list_empty(&ent->head))
++=======
+ 	lockdep_assert_held(&ent->mkeys.xa_lock);
+ 	if (!ent->stored)
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		return;
- 	mr = list_first_entry(&ent->head, struct mlx5_ib_mr, list);
- 	list_del(&mr->list);
- 	ent->available_mrs--;
+ 	mr = pop_stored_mkey(ent);
  	ent->total_mrs--;
 -	xa_unlock_irq(&ent->mkeys);
 +	spin_unlock_irq(&ent->lock);
  	mlx5_core_destroy_mkey(ent->dev->mdev, mr->mmkey.key);
  	kfree(mr);
 -	xa_lock_irq(&ent->mkeys);
 +	spin_lock_irq(&ent->lock);
  }
  
  static int resize_available_mrs(struct mlx5_cache_ent *ent, unsigned int target,
@@@ -347,12 -429,12 +483,12 @@@
  	while (true) {
  		if (limit_fill)
  			target = ent->limit * 2;
- 		if (target == ent->available_mrs + ent->pending)
+ 		if (target == ent->reserved)
  			return 0;
- 		if (target > ent->available_mrs + ent->pending) {
- 			u32 todo = target - (ent->available_mrs + ent->pending);
+ 		if (target > ent->reserved) {
+ 			u32 todo = target - ent->reserved;
  
 -			xa_unlock_irq(&ent->mkeys);
 +			spin_unlock_irq(&ent->lock);
  			err = add_keys(ent, todo);
  			if (err == -EAGAIN)
  				usleep_range(3000, 5000);
@@@ -381,11 -463,11 +517,16 @@@ static ssize_t size_write(struct file *
  
  	/*
  	 * Target is the new value of total_mrs the user requests, however we
- 	 * cannot free MRs that are in use. Compute the target value for
- 	 * available_mrs.
+ 	 * cannot free MRs that are in use. Compute the target value for stored
+ 	 * mkeys.
  	 */
++<<<<<<< HEAD
 +	spin_lock_irq(&ent->lock);
 +	if (target < ent->total_mrs - ent->available_mrs) {
++=======
+ 	xa_lock_irq(&ent->mkeys);
+ 	if (target < ent->total_mrs - ent->stored) {
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		err = -EINVAL;
  		goto err_unlock;
  	}
@@@ -480,9 -562,9 +621,15 @@@ static bool someone_adding(struct mlx5_
  		struct mlx5_cache_ent *ent = &cache->ent[i];
  		bool ret;
  
++<<<<<<< HEAD
 +		spin_lock_irq(&ent->lock);
 +		ret = ent->available_mrs < ent->limit;
 +		spin_unlock_irq(&ent->lock);
++=======
+ 		xa_lock_irq(&ent->mkeys);
+ 		ret = ent->stored < ent->limit;
+ 		xa_unlock_irq(&ent->mkeys);
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		if (ret)
  			return true;
  	}
@@@ -533,12 -615,11 +680,11 @@@ static void __cache_work_func(struct ml
  	if (ent->disabled)
  		goto out;
  
- 	if (ent->fill_to_high_water &&
- 	    ent->available_mrs + ent->pending < 2 * ent->limit &&
+ 	if (ent->fill_to_high_water && ent->reserved < 2 * ent->limit &&
  	    !READ_ONCE(dev->fill_delay)) {
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  		err = add_keys(ent, 1);
 -		xa_lock_irq(&ent->mkeys);
 +		spin_lock_irq(&ent->lock);
  		if (ent->disabled)
  			goto out;
  		if (err) {
@@@ -604,78 -685,44 +750,86 @@@ struct mlx5_ib_mr *mlx5_mr_cache_alloc(
  	struct mlx5_ib_mr *mr;
  
  	/* Matches access in alloc_cache_mr() */
 -	if (!mlx5r_umr_can_reconfig(dev, 0, access_flags))
 +	if (!mlx5_ib_can_reconfig_with_umr(dev, 0, access_flags))
  		return ERR_PTR(-EOPNOTSUPP);
  
++<<<<<<< HEAD
 +	spin_lock_irq(&ent->lock);
 +	if (list_empty(&ent->head)) {
++=======
+ 	xa_lock_irq(&ent->mkeys);
+ 	if (!ent->stored) {
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		queue_adjust_cache_locked(ent);
  		ent->miss++;
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  		mr = create_cache_mr(ent);
  		if (IS_ERR(mr))
  			return mr;
  	} else {
- 		mr = list_first_entry(&ent->head, struct mlx5_ib_mr, list);
- 		list_del(&mr->list);
- 		ent->available_mrs--;
+ 		mr = pop_stored_mkey(ent);
  		queue_adjust_cache_locked(ent);
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  
  		mlx5_clear_mr(mr);
  	}
  	return mr;
  }
  
++<<<<<<< HEAD
 +static void detach_mr_from_cache(struct mlx5_ib_mr *mr)
 +{
 +	struct mlx5_cache_ent *ent = mr->cache_ent;
 +
 +	mr->cache_ent = NULL;
 +	spin_lock_irq(&ent->lock);
 +	ent->total_mrs--;
 +	spin_unlock_irq(&ent->lock);
 +}
 +
 +static void mlx5_mr_cache_free(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
 +{
 +	struct mlx5_cache_ent *ent = mr->cache_ent;
 +
 +	WRITE_ONCE(dev->cache.last_add, jiffies);
 +	spin_lock_irq(&ent->lock);
 +	list_add_tail(&mr->list, &ent->head);
 +	ent->available_mrs++;
 +	queue_adjust_cache_locked(ent);
 +	spin_unlock_irq(&ent->lock);
 +}
 +
++=======
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  static void clean_keys(struct mlx5_ib_dev *dev, int c)
  {
  	struct mlx5_mr_cache *cache = &dev->cache;
  	struct mlx5_cache_ent *ent = &cache->ent[c];
- 	struct mlx5_ib_mr *tmp_mr;
  	struct mlx5_ib_mr *mr;
- 	LIST_HEAD(del_list);
  
  	cancel_delayed_work(&ent->dwork);
++<<<<<<< HEAD
 +	while (1) {
 +		spin_lock_irq(&ent->lock);
 +		if (list_empty(&ent->head)) {
 +			spin_unlock_irq(&ent->lock);
 +			break;
 +		}
 +		mr = list_first_entry(&ent->head, struct mlx5_ib_mr, list);
 +		list_move(&mr->list, &del_list);
 +		ent->available_mrs--;
++=======
+ 	xa_lock_irq(&ent->mkeys);
+ 	while (ent->stored) {
+ 		mr = pop_stored_mkey(ent);
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		ent->total_mrs--;
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  		mlx5_core_destroy_mkey(dev->mdev, mr->mmkey.key);
- 	}
- 
- 	list_for_each_entry_safe(mr, tmp_mr, &del_list, list) {
- 		list_del(&mr->list);
  		kfree(mr);
+ 		xa_lock_irq(&ent->mkeys);
  	}
+ 	xa_unlock_irq(&ent->mkeys);
  }
  
  static void mlx5_mr_cache_debugfs_cleanup(struct mlx5_ib_dev *dev)
@@@ -734,8 -781,7 +888,12 @@@ int mlx5_mr_cache_init(struct mlx5_ib_d
  	timer_setup(&dev->delay_timer, delay_time_func, 0);
  	for (i = 0; i < MAX_MR_CACHE_ENTRIES; i++) {
  		ent = &cache->ent[i];
++<<<<<<< HEAD
 +		INIT_LIST_HEAD(&ent->head);
 +		spin_lock_init(&ent->lock);
++=======
+ 		xa_init_flags(&ent->mkeys, XA_FLAGS_LOCK_IRQ);
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  		ent->order = i + 2;
  		ent->dev = dev;
  		ent->limit = 0;
@@@ -1811,10 -1642,11 +1969,16 @@@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr
  
  	/* Stop DMA */
  	if (mr->cache_ent) {
++<<<<<<< HEAD
 +		if (revoke_mr(mr)) {
 +			spin_lock_irq(&mr->cache_ent->lock);
++=======
+ 		if (mlx5r_umr_revoke_mr(mr) ||
+ 		    push_mkey(mr->cache_ent, false, mr)) {
+ 			xa_lock_irq(&mr->cache_ent->mkeys);
++>>>>>>> 86457a92df1b (RDMA/mlx5: Replace cache list with Xarray)
  			mr->cache_ent->total_mrs--;
 -			xa_unlock_irq(&mr->cache_ent->mkeys);
 +			spin_unlock_irq(&mr->cache_ent->lock);
  			mr->cache_ent = NULL;
  		}
  	}
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
