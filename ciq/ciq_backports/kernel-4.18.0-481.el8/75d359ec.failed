x86/sev: Add missing __init annotations to SEV init routines

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Michael Roth <michael.roth@amd.com>
commit 75d359ec4141b013727022a663762931f69e6510
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/75d359ec.failed

Currently, get_secrets_page() is only reachable from the following call
chain:

  __init snp_init_platform_device():
    get_secrets_page()

so mark it as __init as well. This is also needed since it calls
early_memremap(), which is also an __init routine.

Similarly, get_jump_table_addr() is only reachable from the following
call chain:

  __init setup_real_mode():
    sme_sev_setup_real_mode():
      sev_es_setup_ap_jump_table():
        get_jump_table_addr()

so mark get_jump_table_addr() and everything up that call chain as
__init as well. This is also needed since future patches will add a
call to get_secrets_page(), which needs to be __init due to the reasons
stated above.

	Suggested-by: Borislav Petkov <bp@suse.de>
	Signed-off-by: Michael Roth <michael.roth@amd.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lore.kernel.org/r/20220422135624.114172-2-michael.roth@amd.com
(cherry picked from commit 75d359ec4141b013727022a663762931f69e6510)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/kernel/sev.c
diff --cc arch/x86/kernel/sev.c
index 57edd1e021cf,b7fd1915560d..000000000000
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@@ -506,10 -555,10 +506,10 @@@ void noinstr __sev_es_nmi_complete(void
  	sev_es_wr_ghcb_msr(__pa_nodebug(ghcb));
  	VMGEXIT();
  
 -	__sev_put_ghcb(&state);
 +	sev_es_put_ghcb(&state);
  }
  
- static u64 get_jump_table_addr(void)
+ static u64 __init get_jump_table_addr(void)
  {
  	struct ghcb_state state;
  	unsigned long flags;
@@@ -539,7 -588,496 +539,500 @@@
  	return ret;
  }
  
++<<<<<<< HEAD
 +int sev_es_setup_ap_jump_table(struct real_mode_header *rmh)
++=======
+ static void pvalidate_pages(unsigned long vaddr, unsigned int npages, bool validate)
+ {
+ 	unsigned long vaddr_end;
+ 	int rc;
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		rc = pvalidate(vaddr, RMP_PG_SIZE_4K, validate);
+ 		if (WARN(rc, "Failed to validate address 0x%lx ret %d", vaddr, rc))
+ 			sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PVALIDATE);
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 	}
+ }
+ 
+ static void __init early_set_pages_state(unsigned long paddr, unsigned int npages, enum psc_op op)
+ {
+ 	unsigned long paddr_end;
+ 	u64 val;
+ 
+ 	paddr = paddr & PAGE_MASK;
+ 	paddr_end = paddr + (npages << PAGE_SHIFT);
+ 
+ 	while (paddr < paddr_end) {
+ 		/*
+ 		 * Use the MSR protocol because this function can be called before
+ 		 * the GHCB is established.
+ 		 */
+ 		sev_es_wr_ghcb_msr(GHCB_MSR_PSC_REQ_GFN(paddr >> PAGE_SHIFT, op));
+ 		VMGEXIT();
+ 
+ 		val = sev_es_rd_ghcb_msr();
+ 
+ 		if (WARN(GHCB_RESP_CODE(val) != GHCB_MSR_PSC_RESP,
+ 			 "Wrong PSC response code: 0x%x\n",
+ 			 (unsigned int)GHCB_RESP_CODE(val)))
+ 			goto e_term;
+ 
+ 		if (WARN(GHCB_MSR_PSC_RESP_VAL(val),
+ 			 "Failed to change page state to '%s' paddr 0x%lx error 0x%llx\n",
+ 			 op == SNP_PAGE_STATE_PRIVATE ? "private" : "shared",
+ 			 paddr, GHCB_MSR_PSC_RESP_VAL(val)))
+ 			goto e_term;
+ 
+ 		paddr = paddr + PAGE_SIZE;
+ 	}
+ 
+ 	return;
+ 
+ e_term:
+ 	sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ void __init early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr,
+ 					 unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	 /*
+ 	  * Ask the hypervisor to mark the memory pages as private in the RMP
+ 	  * table.
+ 	  */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	/* Validate the memory pages after they've been added in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
+ void __init early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr,
+ 					unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	/* Invalidate the memory pages before they are marked shared in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	 /* Ask hypervisor to mark the memory pages shared in the RMP table. */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op)
+ {
+ 	unsigned long vaddr, npages;
+ 
+ 	vaddr = (unsigned long)__va(paddr);
+ 	npages = PAGE_ALIGN(sz) >> PAGE_SHIFT;
+ 
+ 	if (op == SNP_PAGE_STATE_PRIVATE)
+ 		early_snp_set_memory_private(vaddr, paddr, npages);
+ 	else if (op == SNP_PAGE_STATE_SHARED)
+ 		early_snp_set_memory_shared(vaddr, paddr, npages);
+ 	else
+ 		WARN(1, "invalid memory op %d\n", op);
+ }
+ 
+ static int vmgexit_psc(struct snp_psc_desc *desc)
+ {
+ 	int cur_entry, end_entry, ret = 0;
+ 	struct snp_psc_desc *data;
+ 	struct ghcb_state state;
+ 	struct es_em_ctxt ctxt;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 
+ 	/*
+ 	 * __sev_get_ghcb() needs to run with IRQs disabled because it is using
+ 	 * a per-CPU GHCB.
+ 	 */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 	if (!ghcb) {
+ 		ret = 1;
+ 		goto out_unlock;
+ 	}
+ 
+ 	/* Copy the input desc into GHCB shared buffer */
+ 	data = (struct snp_psc_desc *)ghcb->shared_buffer;
+ 	memcpy(ghcb->shared_buffer, desc, min_t(int, GHCB_SHARED_BUF_SIZE, sizeof(*desc)));
+ 
+ 	/*
+ 	 * As per the GHCB specification, the hypervisor can resume the guest
+ 	 * before processing all the entries. Check whether all the entries
+ 	 * are processed. If not, then keep retrying. Note, the hypervisor
+ 	 * will update the data memory directly to indicate the status, so
+ 	 * reference the data->hdr everywhere.
+ 	 *
+ 	 * The strategy here is to wait for the hypervisor to change the page
+ 	 * state in the RMP table before guest accesses the memory pages. If the
+ 	 * page state change was not successful, then later memory access will
+ 	 * result in a crash.
+ 	 */
+ 	cur_entry = data->hdr.cur_entry;
+ 	end_entry = data->hdr.end_entry;
+ 
+ 	while (data->hdr.cur_entry <= data->hdr.end_entry) {
+ 		ghcb_set_sw_scratch(ghcb, (u64)__pa(data));
+ 
+ 		/* This will advance the shared buffer data points to. */
+ 		ret = sev_es_ghcb_hv_call(ghcb, true, &ctxt, SVM_VMGEXIT_PSC, 0, 0);
+ 
+ 		/*
+ 		 * Page State Change VMGEXIT can pass error code through
+ 		 * exit_info_2.
+ 		 */
+ 		if (WARN(ret || ghcb->save.sw_exit_info_2,
+ 			 "SNP: PSC failed ret=%d exit_info_2=%llx\n",
+ 			 ret, ghcb->save.sw_exit_info_2)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/* Verify that reserved bit is not set */
+ 		if (WARN(data->hdr.reserved, "Reserved bit is set in the PSC header\n")) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/*
+ 		 * Sanity check that entry processing is not going backwards.
+ 		 * This will happen only if hypervisor is tricking us.
+ 		 */
+ 		if (WARN(data->hdr.end_entry > end_entry || cur_entry > data->hdr.cur_entry,
+ "SNP: PSC processing going backward, end_entry %d (got %d) cur_entry %d (got %d)\n",
+ 			 end_entry, data->hdr.end_entry, cur_entry, data->hdr.cur_entry)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 	}
+ 
+ out:
+ 	__sev_put_ghcb(&state);
+ 
+ out_unlock:
+ 	local_irq_restore(flags);
+ 
+ 	return ret;
+ }
+ 
+ static void __set_pages_state(struct snp_psc_desc *data, unsigned long vaddr,
+ 			      unsigned long vaddr_end, int op)
+ {
+ 	struct psc_hdr *hdr;
+ 	struct psc_entry *e;
+ 	unsigned long pfn;
+ 	int i;
+ 
+ 	hdr = &data->hdr;
+ 	e = data->entries;
+ 
+ 	memset(data, 0, sizeof(*data));
+ 	i = 0;
+ 
+ 	while (vaddr < vaddr_end) {
+ 		if (is_vmalloc_addr((void *)vaddr))
+ 			pfn = vmalloc_to_pfn((void *)vaddr);
+ 		else
+ 			pfn = __pa(vaddr) >> PAGE_SHIFT;
+ 
+ 		e->gfn = pfn;
+ 		e->operation = op;
+ 		hdr->end_entry = i;
+ 
+ 		/*
+ 		 * Current SNP implementation doesn't keep track of the RMP page
+ 		 * size so use 4K for simplicity.
+ 		 */
+ 		e->pagesize = RMP_PG_SIZE_4K;
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 		e++;
+ 		i++;
+ 	}
+ 
+ 	if (vmgexit_psc(data))
+ 		sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ static void set_pages_state(unsigned long vaddr, unsigned int npages, int op)
+ {
+ 	unsigned long vaddr_end, next_vaddr;
+ 	struct snp_psc_desc *desc;
+ 
+ 	desc = kmalloc(sizeof(*desc), GFP_KERNEL_ACCOUNT);
+ 	if (!desc)
+ 		panic("SNP: failed to allocate memory for PSC descriptor\n");
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		/* Calculate the last vaddr that fits in one struct snp_psc_desc. */
+ 		next_vaddr = min_t(unsigned long, vaddr_end,
+ 				   (VMGEXIT_PSC_MAX_ENTRY * PAGE_SIZE) + vaddr);
+ 
+ 		__set_pages_state(desc, vaddr, next_vaddr, op);
+ 
+ 		vaddr = next_vaddr;
+ 	}
+ 
+ 	kfree(desc);
+ }
+ 
+ void snp_set_memory_shared(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void snp_set_memory_private(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
+ static int snp_set_vmsa(void *va, bool vmsa)
+ {
+ 	u64 attrs;
+ 
+ 	/*
+ 	 * Running at VMPL0 allows the kernel to change the VMSA bit for a page
+ 	 * using the RMPADJUST instruction. However, for the instruction to
+ 	 * succeed it must target the permissions of a lesser privileged
+ 	 * (higher numbered) VMPL level, so use VMPL1 (refer to the RMPADJUST
+ 	 * instruction in the AMD64 APM Volume 3).
+ 	 */
+ 	attrs = 1;
+ 	if (vmsa)
+ 		attrs |= RMPADJUST_VMSA_PAGE_BIT;
+ 
+ 	return rmpadjust((unsigned long)va, RMP_PG_SIZE_4K, attrs);
+ }
+ 
+ #define __ATTR_BASE		(SVM_SELECTOR_P_MASK | SVM_SELECTOR_S_MASK)
+ #define INIT_CS_ATTRIBS		(__ATTR_BASE | SVM_SELECTOR_READ_MASK | SVM_SELECTOR_CODE_MASK)
+ #define INIT_DS_ATTRIBS		(__ATTR_BASE | SVM_SELECTOR_WRITE_MASK)
+ 
+ #define INIT_LDTR_ATTRIBS	(SVM_SELECTOR_P_MASK | 2)
+ #define INIT_TR_ATTRIBS		(SVM_SELECTOR_P_MASK | 3)
+ 
+ static void *snp_alloc_vmsa_page(void)
+ {
+ 	struct page *p;
+ 
+ 	/*
+ 	 * Allocate VMSA page to work around the SNP erratum where the CPU will
+ 	 * incorrectly signal an RMP violation #PF if a large page (2MB or 1GB)
+ 	 * collides with the RMP entry of VMSA page. The recommended workaround
+ 	 * is to not use a large page.
+ 	 *
+ 	 * Allocate an 8k page which is also 8k-aligned.
+ 	 */
+ 	p = alloc_pages(GFP_KERNEL_ACCOUNT | __GFP_ZERO, 1);
+ 	if (!p)
+ 		return NULL;
+ 
+ 	split_page(p, 1);
+ 
+ 	/* Free the first 4k. This page may be 2M/1G aligned and cannot be used. */
+ 	__free_page(p);
+ 
+ 	return page_address(p + 1);
+ }
+ 
+ static void snp_cleanup_vmsa(struct sev_es_save_area *vmsa)
+ {
+ 	int err;
+ 
+ 	err = snp_set_vmsa(vmsa, false);
+ 	if (err)
+ 		pr_err("clear VMSA page failed (%u), leaking page\n", err);
+ 	else
+ 		free_page((unsigned long)vmsa);
+ }
+ 
+ static int wakeup_cpu_via_vmgexit(int apic_id, unsigned long start_ip)
+ {
+ 	struct sev_es_save_area *cur_vmsa, *vmsa;
+ 	struct ghcb_state state;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 	u8 sipi_vector;
+ 	int cpu, ret;
+ 	u64 cr4;
+ 
+ 	/*
+ 	 * The hypervisor SNP feature support check has happened earlier, just check
+ 	 * the AP_CREATION one here.
+ 	 */
+ 	if (!(sev_hv_features & GHCB_HV_FT_SNP_AP_CREATION))
+ 		return -EOPNOTSUPP;
+ 
+ 	/*
+ 	 * Verify the desired start IP against the known trampoline start IP
+ 	 * to catch any future new trampolines that may be introduced that
+ 	 * would require a new protected guest entry point.
+ 	 */
+ 	if (WARN_ONCE(start_ip != real_mode_header->trampoline_start,
+ 		      "Unsupported SNP start_ip: %lx\n", start_ip))
+ 		return -EINVAL;
+ 
+ 	/* Override start_ip with known protected guest start IP */
+ 	start_ip = real_mode_header->sev_es_trampoline_start;
+ 
+ 	/* Find the logical CPU for the APIC ID */
+ 	for_each_present_cpu(cpu) {
+ 		if (arch_match_cpu_phys_id(cpu, apic_id))
+ 			break;
+ 	}
+ 	if (cpu >= nr_cpu_ids)
+ 		return -EINVAL;
+ 
+ 	cur_vmsa = per_cpu(sev_vmsa, cpu);
+ 
+ 	/*
+ 	 * A new VMSA is created each time because there is no guarantee that
+ 	 * the current VMSA is the kernels or that the vCPU is not running. If
+ 	 * an attempt was done to use the current VMSA with a running vCPU, a
+ 	 * #VMEXIT of that vCPU would wipe out all of the settings being done
+ 	 * here.
+ 	 */
+ 	vmsa = (struct sev_es_save_area *)snp_alloc_vmsa_page();
+ 	if (!vmsa)
+ 		return -ENOMEM;
+ 
+ 	/* CR4 should maintain the MCE value */
+ 	cr4 = native_read_cr4() & X86_CR4_MCE;
+ 
+ 	/* Set the CS value based on the start_ip converted to a SIPI vector */
+ 	sipi_vector		= (start_ip >> 12);
+ 	vmsa->cs.base		= sipi_vector << 12;
+ 	vmsa->cs.limit		= AP_INIT_CS_LIMIT;
+ 	vmsa->cs.attrib		= INIT_CS_ATTRIBS;
+ 	vmsa->cs.selector	= sipi_vector << 8;
+ 
+ 	/* Set the RIP value based on start_ip */
+ 	vmsa->rip		= start_ip & 0xfff;
+ 
+ 	/* Set AP INIT defaults as documented in the APM */
+ 	vmsa->ds.limit		= AP_INIT_DS_LIMIT;
+ 	vmsa->ds.attrib		= INIT_DS_ATTRIBS;
+ 	vmsa->es		= vmsa->ds;
+ 	vmsa->fs		= vmsa->ds;
+ 	vmsa->gs		= vmsa->ds;
+ 	vmsa->ss		= vmsa->ds;
+ 
+ 	vmsa->gdtr.limit	= AP_INIT_GDTR_LIMIT;
+ 	vmsa->ldtr.limit	= AP_INIT_LDTR_LIMIT;
+ 	vmsa->ldtr.attrib	= INIT_LDTR_ATTRIBS;
+ 	vmsa->idtr.limit	= AP_INIT_IDTR_LIMIT;
+ 	vmsa->tr.limit		= AP_INIT_TR_LIMIT;
+ 	vmsa->tr.attrib		= INIT_TR_ATTRIBS;
+ 
+ 	vmsa->cr4		= cr4;
+ 	vmsa->cr0		= AP_INIT_CR0_DEFAULT;
+ 	vmsa->dr7		= DR7_RESET_VALUE;
+ 	vmsa->dr6		= AP_INIT_DR6_DEFAULT;
+ 	vmsa->rflags		= AP_INIT_RFLAGS_DEFAULT;
+ 	vmsa->g_pat		= AP_INIT_GPAT_DEFAULT;
+ 	vmsa->xcr0		= AP_INIT_XCR0_DEFAULT;
+ 	vmsa->mxcsr		= AP_INIT_MXCSR_DEFAULT;
+ 	vmsa->x87_ftw		= AP_INIT_X87_FTW_DEFAULT;
+ 	vmsa->x87_fcw		= AP_INIT_X87_FCW_DEFAULT;
+ 
+ 	/* SVME must be set. */
+ 	vmsa->efer		= EFER_SVME;
+ 
+ 	/*
+ 	 * Set the SNP-specific fields for this VMSA:
+ 	 *   VMPL level
+ 	 *   SEV_FEATURES (matches the SEV STATUS MSR right shifted 2 bits)
+ 	 */
+ 	vmsa->vmpl		= 0;
+ 	vmsa->sev_features	= sev_status >> 2;
+ 
+ 	/* Switch the page over to a VMSA page now that it is initialized */
+ 	ret = snp_set_vmsa(vmsa, true);
+ 	if (ret) {
+ 		pr_err("set VMSA page failed (%u)\n", ret);
+ 		free_page((unsigned long)vmsa);
+ 
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* Issue VMGEXIT AP Creation NAE event */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 
+ 	vc_ghcb_invalidate(ghcb);
+ 	ghcb_set_rax(ghcb, vmsa->sev_features);
+ 	ghcb_set_sw_exit_code(ghcb, SVM_VMGEXIT_AP_CREATION);
+ 	ghcb_set_sw_exit_info_1(ghcb, ((u64)apic_id << 32) | SVM_VMGEXIT_AP_CREATE);
+ 	ghcb_set_sw_exit_info_2(ghcb, __pa(vmsa));
+ 
+ 	sev_es_wr_ghcb_msr(__pa(ghcb));
+ 	VMGEXIT();
+ 
+ 	if (!ghcb_sw_exit_info_1_is_valid(ghcb) ||
+ 	    lower_32_bits(ghcb->save.sw_exit_info_1)) {
+ 		pr_err("SNP AP Creation error\n");
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	__sev_put_ghcb(&state);
+ 
+ 	local_irq_restore(flags);
+ 
+ 	/* Perform cleanup if there was an error */
+ 	if (ret) {
+ 		snp_cleanup_vmsa(vmsa);
+ 		vmsa = NULL;
+ 	}
+ 
+ 	/* Free up any previous VMSA page */
+ 	if (cur_vmsa)
+ 		snp_cleanup_vmsa(cur_vmsa);
+ 
+ 	/* Record the current VMSA page */
+ 	per_cpu(sev_vmsa, cpu) = vmsa;
+ 
+ 	return ret;
+ }
+ 
+ void snp_set_wakeup_secondary_cpu(void)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	/*
+ 	 * Always set this override if SNP is enabled. This makes it the
+ 	 * required method to start APs under SNP. If the hypervisor does
+ 	 * not support AP creation, then no APs will be started.
+ 	 */
+ 	apic->wakeup_secondary_cpu = wakeup_cpu_via_vmgexit;
+ }
+ 
+ int __init sev_es_setup_ap_jump_table(struct real_mode_header *rmh)
++>>>>>>> 75d359ec4141 (x86/sev: Add missing __init annotations to SEV init routines)
  {
  	u16 startup_cs, startup_ip;
  	phys_addr_t jump_table_pa;
@@@ -1491,3 -1984,237 +1984,240 @@@ fail
  
  	sev_es_terminate(SEV_TERM_SET_GEN, GHCB_SEV_ES_GEN_REQ);
  }
++<<<<<<< HEAD
++=======
+ 
+ /*
+  * Initial set up of SNP relies on information provided by the
+  * Confidential Computing blob, which can be passed to the kernel
+  * in the following ways, depending on how it is booted:
+  *
+  * - when booted via the boot/decompress kernel:
+  *   - via boot_params
+  *
+  * - when booted directly by firmware/bootloader (e.g. CONFIG_PVH):
+  *   - via a setup_data entry, as defined by the Linux Boot Protocol
+  *
+  * Scan for the blob in that order.
+  */
+ static __init struct cc_blob_sev_info *find_cc_blob(struct boot_params *bp)
+ {
+ 	struct cc_blob_sev_info *cc_info;
+ 
+ 	/* Boot kernel would have passed the CC blob via boot_params. */
+ 	if (bp->cc_blob_address) {
+ 		cc_info = (struct cc_blob_sev_info *)(unsigned long)bp->cc_blob_address;
+ 		goto found_cc_info;
+ 	}
+ 
+ 	/*
+ 	 * If kernel was booted directly, without the use of the
+ 	 * boot/decompression kernel, the CC blob may have been passed via
+ 	 * setup_data instead.
+ 	 */
+ 	cc_info = find_cc_blob_setup_data(bp);
+ 	if (!cc_info)
+ 		return NULL;
+ 
+ found_cc_info:
+ 	if (cc_info->magic != CC_BLOB_SEV_HDR_MAGIC)
+ 		snp_abort();
+ 
+ 	return cc_info;
+ }
+ 
+ bool __init snp_init(struct boot_params *bp)
+ {
+ 	struct cc_blob_sev_info *cc_info;
+ 
+ 	if (!bp)
+ 		return false;
+ 
+ 	cc_info = find_cc_blob(bp);
+ 	if (!cc_info)
+ 		return false;
+ 
+ 	setup_cpuid_table(cc_info);
+ 
+ 	/*
+ 	 * The CC blob will be used later to access the secrets page. Cache
+ 	 * it here like the boot kernel does.
+ 	 */
+ 	bp->cc_blob_address = (u32)(unsigned long)cc_info;
+ 
+ 	return true;
+ }
+ 
+ void __init snp_abort(void)
+ {
+ 	sev_es_terminate(SEV_TERM_SET_GEN, GHCB_SNP_UNSUPPORTED);
+ }
+ 
+ static void dump_cpuid_table(void)
+ {
+ 	const struct snp_cpuid_table *cpuid_table = snp_cpuid_get_table();
+ 	int i = 0;
+ 
+ 	pr_info("count=%d reserved=0x%x reserved2=0x%llx\n",
+ 		cpuid_table->count, cpuid_table->__reserved1, cpuid_table->__reserved2);
+ 
+ 	for (i = 0; i < SNP_CPUID_COUNT_MAX; i++) {
+ 		const struct snp_cpuid_fn *fn = &cpuid_table->fn[i];
+ 
+ 		pr_info("index=%3d fn=0x%08x subfn=0x%08x: eax=0x%08x ebx=0x%08x ecx=0x%08x edx=0x%08x xcr0_in=0x%016llx xss_in=0x%016llx reserved=0x%016llx\n",
+ 			i, fn->eax_in, fn->ecx_in, fn->eax, fn->ebx, fn->ecx,
+ 			fn->edx, fn->xcr0_in, fn->xss_in, fn->__reserved);
+ 	}
+ }
+ 
+ /*
+  * It is useful from an auditing/testing perspective to provide an easy way
+  * for the guest owner to know that the CPUID table has been initialized as
+  * expected, but that initialization happens too early in boot to print any
+  * sort of indicator, and there's not really any other good place to do it,
+  * so do it here.
+  */
+ static int __init report_cpuid_table(void)
+ {
+ 	const struct snp_cpuid_table *cpuid_table = snp_cpuid_get_table();
+ 
+ 	if (!cpuid_table->count)
+ 		return 0;
+ 
+ 	pr_info("Using SNP CPUID table, %d entries present.\n",
+ 		cpuid_table->count);
+ 
+ 	if (sev_cfg.debug)
+ 		dump_cpuid_table();
+ 
+ 	return 0;
+ }
+ arch_initcall(report_cpuid_table);
+ 
+ static int __init init_sev_config(char *str)
+ {
+ 	char *s;
+ 
+ 	while ((s = strsep(&str, ","))) {
+ 		if (!strcmp(s, "debug")) {
+ 			sev_cfg.debug = true;
+ 			continue;
+ 		}
+ 
+ 		pr_info("SEV command-line option '%s' was not recognized\n", s);
+ 	}
+ 
+ 	return 1;
+ }
+ __setup("sev=", init_sev_config);
+ 
+ int snp_issue_guest_request(u64 exit_code, struct snp_req_data *input, unsigned long *fw_err)
+ {
+ 	struct ghcb_state state;
+ 	struct es_em_ctxt ctxt;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 	int ret;
+ 
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return -ENODEV;
+ 
+ 	if (!fw_err)
+ 		return -EINVAL;
+ 
+ 	/*
+ 	 * __sev_get_ghcb() needs to run with IRQs disabled because it is using
+ 	 * a per-CPU GHCB.
+ 	 */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 	if (!ghcb) {
+ 		ret = -EIO;
+ 		goto e_restore_irq;
+ 	}
+ 
+ 	vc_ghcb_invalidate(ghcb);
+ 
+ 	if (exit_code == SVM_VMGEXIT_EXT_GUEST_REQUEST) {
+ 		ghcb_set_rax(ghcb, input->data_gpa);
+ 		ghcb_set_rbx(ghcb, input->data_npages);
+ 	}
+ 
+ 	ret = sev_es_ghcb_hv_call(ghcb, true, &ctxt, exit_code, input->req_gpa, input->resp_gpa);
+ 	if (ret)
+ 		goto e_put;
+ 
+ 	if (ghcb->save.sw_exit_info_2) {
+ 		/* Number of expected pages are returned in RBX */
+ 		if (exit_code == SVM_VMGEXIT_EXT_GUEST_REQUEST &&
+ 		    ghcb->save.sw_exit_info_2 == SNP_GUEST_REQ_INVALID_LEN)
+ 			input->data_npages = ghcb_get_rbx(ghcb);
+ 
+ 		*fw_err = ghcb->save.sw_exit_info_2;
+ 
+ 		ret = -EIO;
+ 	}
+ 
+ e_put:
+ 	__sev_put_ghcb(&state);
+ e_restore_irq:
+ 	local_irq_restore(flags);
+ 
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(snp_issue_guest_request);
+ 
+ static struct platform_device sev_guest_device = {
+ 	.name		= "sev-guest",
+ 	.id		= -1,
+ };
+ 
+ static u64 __init get_secrets_page(void)
+ {
+ 	u64 pa_data = boot_params.cc_blob_address;
+ 	struct cc_blob_sev_info info;
+ 	void *map;
+ 
+ 	/*
+ 	 * The CC blob contains the address of the secrets page, check if the
+ 	 * blob is present.
+ 	 */
+ 	if (!pa_data)
+ 		return 0;
+ 
+ 	map = early_memremap(pa_data, sizeof(info));
+ 	memcpy(&info, map, sizeof(info));
+ 	early_memunmap(map, sizeof(info));
+ 
+ 	/* smoke-test the secrets page passed */
+ 	if (!info.secrets_phys || info.secrets_len != PAGE_SIZE)
+ 		return 0;
+ 
+ 	return info.secrets_phys;
+ }
+ 
+ static int __init snp_init_platform_device(void)
+ {
+ 	struct sev_guest_platform_data data;
+ 	u64 gpa;
+ 
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return -ENODEV;
+ 
+ 	gpa = get_secrets_page();
+ 	if (!gpa)
+ 		return -ENODEV;
+ 
+ 	data.secrets_gpa = gpa;
+ 	if (platform_device_add_data(&sev_guest_device, &data, sizeof(data)))
+ 		return -ENODEV;
+ 
+ 	if (platform_device_register(&sev_guest_device))
+ 		return -ENODEV;
+ 
+ 	pr_info("SNP guest platform device initialized.\n");
+ 	return 0;
+ }
+ device_initcall(snp_init_platform_device);
++>>>>>>> 75d359ec4141 (x86/sev: Add missing __init annotations to SEV init routines)
* Unmerged path arch/x86/kernel/sev.c
diff --git a/arch/x86/realmode/init.c b/arch/x86/realmode/init.c
index ca9cdc404cd1..01715c917499 100644
--- a/arch/x86/realmode/init.c
+++ b/arch/x86/realmode/init.c
@@ -41,7 +41,7 @@ void __init reserve_real_mode(void)
 	memblock_reserve(0, SZ_1M);
 }
 
-static void sme_sev_setup_real_mode(struct trampoline_header *th)
+static void __init sme_sev_setup_real_mode(struct trampoline_header *th)
 {
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 	if (cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT))
