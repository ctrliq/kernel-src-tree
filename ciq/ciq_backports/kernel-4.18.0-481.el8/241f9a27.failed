IB: Set IOVA/LENGTH on IB_MR in core/uverbs layers

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Daisuke Matsuda <matsuda-daisuke@fujitsu.com>
commit 241f9a27e0fc0eaf23e3d52c8450f10648cd11f1
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/241f9a27.failed

Set 'iova' and 'length' on ib_mr in ib_uverbs and ib_core layers to let all
drivers have the members filled. Also, this commit removes redundancy in
the respective drivers.

Previously, commit 04c0a5fcfcf65 ("IB/uverbs: Set IOVA on IB MR in uverbs
layer") changed to set 'iova', but seems to have missed 'length' and the
ib_core layer at that time.

Fixes: 04c0a5fcfcf65 ("IB/uverbs: Set IOVA on IB MR in uverbs layer")
	Signed-off-by: Daisuke Matsuda <matsuda-daisuke@fujitsu.com>
Link: https://lore.kernel.org/r/20220921080844.1616883-1-matsuda-daisuke@fujitsu.com
	Signed-off-by: Leon Romanovsky <leon@kernel.org>
(cherry picked from commit 241f9a27e0fc0eaf23e3d52c8450f10648cd11f1)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/hns/hns_roce_mr.c
diff --cc drivers/infiniband/hw/hns/hns_roce_mr.c
index ba1754ca6d84,dedfa56f5773..000000000000
--- a/drivers/infiniband/hw/hns/hns_roce_mr.c
+++ b/drivers/infiniband/hw/hns/hns_roce_mr.c
@@@ -656,13 -140,352 +656,353 @@@ static void hns_roce_mr_free(struct hns
  static int hns_roce_mr_enable(struct hns_roce_dev *hr_dev,
  			      struct hns_roce_mr *mr)
  {
 +	int ret;
  	unsigned long mtpt_idx = key_to_hw_index(mr->key);
 +	struct device *dev = hr_dev->dev;
  	struct hns_roce_cmd_mailbox *mailbox;
++<<<<<<< HEAD
++=======
+ 	struct device *dev = hr_dev->dev;
+ 	int ret;
+ 
+ 	/* Allocate mailbox memory */
+ 	mailbox = hns_roce_alloc_cmd_mailbox(hr_dev);
+ 	if (IS_ERR(mailbox))
+ 		return PTR_ERR(mailbox);
+ 
+ 	if (mr->type != MR_TYPE_FRMR)
+ 		ret = hr_dev->hw->write_mtpt(hr_dev, mailbox->buf, mr);
+ 	else
+ 		ret = hr_dev->hw->frmr_write_mtpt(hr_dev, mailbox->buf, mr);
+ 	if (ret) {
+ 		dev_err(dev, "failed to write mtpt, ret = %d.\n", ret);
+ 		goto err_page;
+ 	}
+ 
+ 	ret = hns_roce_create_hw_ctx(hr_dev, mailbox, HNS_ROCE_CMD_CREATE_MPT,
+ 				     mtpt_idx & (hr_dev->caps.num_mtpts - 1));
+ 	if (ret) {
+ 		dev_err(dev, "failed to create mpt, ret = %d.\n", ret);
+ 		goto err_page;
+ 	}
+ 
+ 	mr->enabled = 1;
+ 
+ err_page:
+ 	hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 
+ 	return ret;
+ }
+ 
+ void hns_roce_init_mr_table(struct hns_roce_dev *hr_dev)
+ {
+ 	struct hns_roce_ida *mtpt_ida = &hr_dev->mr_table.mtpt_ida;
+ 
+ 	ida_init(&mtpt_ida->ida);
+ 	mtpt_ida->max = hr_dev->caps.num_mtpts - 1;
+ 	mtpt_ida->min = hr_dev->caps.reserved_mrws;
+ }
+ 
+ struct ib_mr *hns_roce_get_dma_mr(struct ib_pd *pd, int acc)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(pd->device);
+ 	struct hns_roce_mr *mr;
+ 	int ret;
+ 
+ 	mr = kzalloc(sizeof(*mr), GFP_KERNEL);
+ 	if (mr == NULL)
+ 		return  ERR_PTR(-ENOMEM);
+ 
+ 	mr->type = MR_TYPE_DMA;
+ 	mr->pd = to_hr_pd(pd)->pdn;
+ 	mr->access = acc;
+ 
+ 	/* Allocate memory region key */
+ 	hns_roce_hem_list_init(&mr->pbl_mtr.hem_list);
+ 	ret = alloc_mr_key(hr_dev, mr);
+ 	if (ret)
+ 		goto err_free;
+ 
+ 	ret = hns_roce_mr_enable(hr_dev, mr);
+ 	if (ret)
+ 		goto err_mr;
+ 
+ 	mr->ibmr.rkey = mr->ibmr.lkey = mr->key;
+ 
+ 	return &mr->ibmr;
+ err_mr:
+ 	free_mr_key(hr_dev, mr);
+ 
+ err_free:
+ 	kfree(mr);
+ 	return ERR_PTR(ret);
+ }
+ 
+ struct ib_mr *hns_roce_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
+ 				   u64 virt_addr, int access_flags,
+ 				   struct ib_udata *udata)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(pd->device);
+ 	struct hns_roce_mr *mr;
+ 	int ret;
+ 
+ 	mr = kzalloc(sizeof(*mr), GFP_KERNEL);
+ 	if (!mr)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mr->iova = virt_addr;
+ 	mr->size = length;
+ 	mr->pd = to_hr_pd(pd)->pdn;
+ 	mr->access = access_flags;
+ 	mr->type = MR_TYPE_MR;
+ 
+ 	ret = alloc_mr_key(hr_dev, mr);
+ 	if (ret)
+ 		goto err_alloc_mr;
+ 
+ 	ret = alloc_mr_pbl(hr_dev, mr, udata, start);
+ 	if (ret)
+ 		goto err_alloc_key;
+ 
+ 	ret = hns_roce_mr_enable(hr_dev, mr);
+ 	if (ret)
+ 		goto err_alloc_pbl;
+ 
+ 	mr->ibmr.rkey = mr->ibmr.lkey = mr->key;
+ 
+ 	return &mr->ibmr;
+ 
+ err_alloc_pbl:
+ 	free_mr_pbl(hr_dev, mr);
+ err_alloc_key:
+ 	free_mr_key(hr_dev, mr);
+ err_alloc_mr:
+ 	kfree(mr);
+ 	return ERR_PTR(ret);
+ }
+ 
+ struct ib_mr *hns_roce_rereg_user_mr(struct ib_mr *ibmr, int flags, u64 start,
+ 				     u64 length, u64 virt_addr,
+ 				     int mr_access_flags, struct ib_pd *pd,
+ 				     struct ib_udata *udata)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);
+ 	struct ib_device *ib_dev = &hr_dev->ib_dev;
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 	struct hns_roce_cmd_mailbox *mailbox;
+ 	unsigned long mtpt_idx;
+ 	int ret;
+ 
+ 	if (!mr->enabled)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	mailbox = hns_roce_alloc_cmd_mailbox(hr_dev);
+ 	if (IS_ERR(mailbox))
+ 		return ERR_CAST(mailbox);
+ 
+ 	mtpt_idx = key_to_hw_index(mr->key) & (hr_dev->caps.num_mtpts - 1);
+ 
+ 	ret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, HNS_ROCE_CMD_QUERY_MPT,
+ 				mtpt_idx);
+ 	if (ret)
+ 		goto free_cmd_mbox;
+ 
+ 	ret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_MPT,
+ 				      mtpt_idx);
+ 	if (ret)
+ 		ibdev_warn(ib_dev, "failed to destroy MPT, ret = %d.\n", ret);
+ 
+ 	mr->enabled = 0;
+ 	mr->iova = virt_addr;
+ 	mr->size = length;
+ 
+ 	if (flags & IB_MR_REREG_PD)
+ 		mr->pd = to_hr_pd(pd)->pdn;
+ 
+ 	if (flags & IB_MR_REREG_ACCESS)
+ 		mr->access = mr_access_flags;
+ 
+ 	if (flags & IB_MR_REREG_TRANS) {
+ 		free_mr_pbl(hr_dev, mr);
+ 		ret = alloc_mr_pbl(hr_dev, mr, udata, start);
+ 		if (ret) {
+ 			ibdev_err(ib_dev, "failed to alloc mr PBL, ret = %d.\n",
+ 				  ret);
+ 			goto free_cmd_mbox;
+ 		}
+ 	}
+ 
+ 	ret = hr_dev->hw->rereg_write_mtpt(hr_dev, mr, flags, mailbox->buf);
+ 	if (ret) {
+ 		ibdev_err(ib_dev, "failed to write mtpt, ret = %d.\n", ret);
+ 		goto free_cmd_mbox;
+ 	}
+ 
+ 	ret = hns_roce_create_hw_ctx(hr_dev, mailbox, HNS_ROCE_CMD_CREATE_MPT,
+ 				     mtpt_idx);
+ 	if (ret) {
+ 		ibdev_err(ib_dev, "failed to create MPT, ret = %d.\n", ret);
+ 		goto free_cmd_mbox;
+ 	}
+ 
+ 	mr->enabled = 1;
+ 
+ free_cmd_mbox:
+ 	hns_roce_free_cmd_mailbox(hr_dev, mailbox);
+ 
+ 	if (ret)
+ 		return ERR_PTR(ret);
+ 	return NULL;
+ }
+ 
+ int hns_roce_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 
+ 	if (hr_dev->hw->dereg_mr)
+ 		hr_dev->hw->dereg_mr(hr_dev);
+ 
+ 	hns_roce_mr_free(hr_dev, mr);
+ 	kfree(mr);
+ 
+ 	return 0;
+ }
+ 
+ struct ib_mr *hns_roce_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
+ 				u32 max_num_sg)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(pd->device);
+ 	struct device *dev = hr_dev->dev;
+ 	struct hns_roce_mr *mr;
+ 	int ret;
+ 
+ 	if (mr_type != IB_MR_TYPE_MEM_REG)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	if (max_num_sg > HNS_ROCE_FRMR_MAX_PA) {
+ 		dev_err(dev, "max_num_sg larger than %d\n",
+ 			HNS_ROCE_FRMR_MAX_PA);
+ 		return ERR_PTR(-EINVAL);
+ 	}
+ 
+ 	mr = kzalloc(sizeof(*mr), GFP_KERNEL);
+ 	if (!mr)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	mr->type = MR_TYPE_FRMR;
+ 	mr->pd = to_hr_pd(pd)->pdn;
+ 	mr->size = max_num_sg * (1 << PAGE_SHIFT);
+ 
+ 	/* Allocate memory region key */
+ 	ret = alloc_mr_key(hr_dev, mr);
+ 	if (ret)
+ 		goto err_free;
+ 
+ 	ret = alloc_mr_pbl(hr_dev, mr, NULL, 0);
+ 	if (ret)
+ 		goto err_key;
+ 
+ 	ret = hns_roce_mr_enable(hr_dev, mr);
+ 	if (ret)
+ 		goto err_pbl;
+ 
+ 	mr->ibmr.rkey = mr->ibmr.lkey = mr->key;
+ 	mr->ibmr.length = mr->size;
+ 
+ 	return &mr->ibmr;
+ 
+ err_key:
+ 	free_mr_key(hr_dev, mr);
+ err_pbl:
+ 	free_mr_pbl(hr_dev, mr);
+ err_free:
+ 	kfree(mr);
+ 	return ERR_PTR(ret);
+ }
+ 
+ static int hns_roce_set_page(struct ib_mr *ibmr, u64 addr)
+ {
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 
+ 	if (likely(mr->npages < mr->pbl_mtr.hem_cfg.buf_pg_count)) {
+ 		mr->page_list[mr->npages++] = addr;
+ 		return 0;
+ 	}
+ 
+ 	return -ENOBUFS;
+ }
+ 
+ int hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
+ 		       unsigned int *sg_offset)
+ {
+ 	struct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device);
+ 	struct ib_device *ibdev = &hr_dev->ib_dev;
+ 	struct hns_roce_mr *mr = to_hr_mr(ibmr);
+ 	struct hns_roce_mtr *mtr = &mr->pbl_mtr;
+ 	int ret = 0;
+ 
+ 	mr->npages = 0;
+ 	mr->page_list = kvcalloc(mr->pbl_mtr.hem_cfg.buf_pg_count,
+ 				 sizeof(dma_addr_t), GFP_KERNEL);
+ 	if (!mr->page_list)
+ 		return ret;
+ 
+ 	ret = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, hns_roce_set_page);
+ 	if (ret < 1) {
+ 		ibdev_err(ibdev, "failed to store sg pages %u %u, cnt = %d.\n",
+ 			  mr->npages, mr->pbl_mtr.hem_cfg.buf_pg_count, ret);
+ 		goto err_page_list;
+ 	}
+ 
+ 	mtr->hem_cfg.region[0].offset = 0;
+ 	mtr->hem_cfg.region[0].count = mr->npages;
+ 	mtr->hem_cfg.region[0].hopnum = mr->pbl_hop_num;
+ 	mtr->hem_cfg.region_count = 1;
+ 	ret = hns_roce_mtr_map(hr_dev, mtr, mr->page_list, mr->npages);
+ 	if (ret) {
+ 		ibdev_err(ibdev, "failed to map sg mtr, ret = %d.\n", ret);
+ 		ret = 0;
+ 	} else {
+ 		mr->pbl_mtr.hem_cfg.buf_pg_shift = (u32)ilog2(ibmr->page_size);
+ 		ret = mr->npages;
+ 	}
+ 
+ err_page_list:
+ 	kvfree(mr->page_list);
+ 	mr->page_list = NULL;
+ 
+ 	return ret;
+ }
+ 
+ static void hns_roce_mw_free(struct hns_roce_dev *hr_dev,
+ 			     struct hns_roce_mw *mw)
+ {
+ 	struct device *dev = hr_dev->dev;
+ 	int ret;
+ 
+ 	if (mw->enabled) {
+ 		ret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_MPT,
+ 					      key_to_hw_index(mw->rkey) &
+ 					      (hr_dev->caps.num_mtpts - 1));
+ 		if (ret)
+ 			dev_warn(dev, "MW DESTROY_MPT failed (%d)\n", ret);
+ 
+ 		hns_roce_table_put(hr_dev, &hr_dev->mr_table.mtpt_table,
+ 				   key_to_hw_index(mw->rkey));
+ 	}
+ 
+ 	ida_free(&hr_dev->mr_table.mtpt_ida.ida,
+ 		 (int)key_to_hw_index(mw->rkey));
+ }
+ 
+ static int hns_roce_mw_enable(struct hns_roce_dev *hr_dev,
+ 			      struct hns_roce_mw *mw)
+ {
++>>>>>>> 241f9a27e0fc (IB: Set IOVA/LENGTH on IB_MR in core/uverbs layers)
  	struct hns_roce_mr_table *mr_table = &hr_dev->mr_table;
 -	struct hns_roce_cmd_mailbox *mailbox;
 -	struct device *dev = hr_dev->dev;
 -	unsigned long mtpt_idx = key_to_hw_index(mw->rkey);
 -	int ret;
  
 -	/* prepare HEM entry memory */
 +	/* Prepare HEM entry memory */
  	ret = hns_roce_table_get(hr_dev, &mr_table->mtpt_table, mtpt_idx);
  	if (ret)
  		return ret;
diff --git a/drivers/infiniband/core/uverbs_cmd.c b/drivers/infiniband/core/uverbs_cmd.c
index 4e8101fd5a4b..b9f7780bf608 100644
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@ -741,6 +741,7 @@ static int ib_uverbs_reg_mr(struct uverbs_attr_bundle *attrs)
 	mr->uobject = uobj;
 	atomic_inc(&pd->usecnt);
 	mr->iova = cmd.hca_va;
+	mr->length = cmd.length;
 
 	rdma_restrack_new(&mr->res, RDMA_RESTRACK_MR);
 	rdma_restrack_set_name(&mr->res, NULL);
@@ -863,8 +864,10 @@ static int ib_uverbs_rereg_mr(struct uverbs_attr_bundle *attrs)
 			mr->pd = new_pd;
 			atomic_inc(&new_pd->usecnt);
 		}
-		if (cmd.flags & IB_MR_REREG_TRANS)
+		if (cmd.flags & IB_MR_REREG_TRANS) {
 			mr->iova = cmd.hca_va;
+			mr->length = cmd.length;
+		}
 	}
 
 	memset(&resp, 0, sizeof(resp));
diff --git a/drivers/infiniband/core/verbs.c b/drivers/infiniband/core/verbs.c
index e54b3f1b730e..f8964c8cf0ad 100644
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@ -2149,6 +2149,8 @@ struct ib_mr *ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 	mr->pd = pd;
 	mr->dm = NULL;
 	atomic_inc(&pd->usecnt);
+	mr->iova =  virt_addr;
+	mr->length = length;
 
 	rdma_restrack_new(&mr->res, RDMA_RESTRACK_MR);
 	rdma_restrack_parent_name(&mr->res, &pd->res);
* Unmerged path drivers/infiniband/hw/hns/hns_roce_mr.c
diff --git a/drivers/infiniband/hw/mlx4/mr.c b/drivers/infiniband/hw/mlx4/mr.c
index 032633e71ede..ac0e80ac06b7 100644
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@ -439,7 +439,6 @@ struct ib_mr *mlx4_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 		goto err_mr;
 
 	mr->ibmr.rkey = mr->ibmr.lkey = mr->mmr.key;
-	mr->ibmr.length = length;
 	mr->ibmr.page_size = 1U << shift;
 
 	return &mr->ibmr;
