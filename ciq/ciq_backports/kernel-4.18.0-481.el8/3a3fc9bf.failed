powerpc64/bpf: Store temp registers' bpf to ppc mapping

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Jordan Niethe <jniethe5@gmail.com>
commit 3a3fc9bf103974d9a886fa37087d5d491c806e00
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/3a3fc9bf.failed

In bpf_jit_build_body(), the mapping of TMP_REG_1 and TMP_REG_2's bpf
register to ppc register is evalulated at every use despite not
changing. Instead, determine the ppc register once and store the result.

	Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
[Rebased, converted additional usage sites]
	Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/0944e2f0fa6dd254ea401f1c946fb6c9a5294278.1644834730.git.naveen.n.rao@linux.vnet.ibm.com

(cherry picked from commit 3a3fc9bf103974d9a886fa37087d5d491c806e00)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/net/bpf_jit_comp64.c
diff --cc arch/powerpc/net/bpf_jit_comp64.c
index 3361fea2cdf5,b4de0c35c8a4..000000000000
--- a/arch/powerpc/net/bpf_jit_comp64.c
+++ b/arch/powerpc/net/bpf_jit_comp64.c
@@@ -312,6 -356,9 +312,12 @@@ int bpf_jit_build_body(struct bpf_prog 
  		u32 code = insn[i].code;
  		u32 dst_reg = b2p[insn[i].dst_reg];
  		u32 src_reg = b2p[insn[i].src_reg];
++<<<<<<< HEAD
++=======
+ 		u32 size = BPF_SIZE(code);
+ 		u32 tmp1_reg = b2p[TMP_REG_1];
+ 		u32 tmp2_reg = b2p[TMP_REG_2];
++>>>>>>> 3a3fc9bf1039 (powerpc64/bpf: Store temp registers' bpf to ppc mapping)
  		s16 off = insn[i].off;
  		s32 imm = insn[i].imm;
  		bool func_addr_fixed;
@@@ -626,25 -657,29 +616,43 @@@ bpf_alu32_trunc
  				 * 2 bytes are already in their final position
  				 * -- byte 2 and 4 (of bytes 1, 2, 3 and 4)
  				 */
- 				EMIT(PPC_RAW_RLWINM(b2p[TMP_REG_1], dst_reg, 8, 0, 31));
+ 				EMIT(PPC_RAW_RLWINM(tmp1_reg, dst_reg, 8, 0, 31));
  				/* Rotate 24 bits and insert byte 1 */
- 				EMIT(PPC_RAW_RLWIMI(b2p[TMP_REG_1], dst_reg, 24, 0, 7));
+ 				EMIT(PPC_RAW_RLWIMI(tmp1_reg, dst_reg, 24, 0, 7));
  				/* Rotate 24 bits and insert byte 3 */
- 				EMIT(PPC_RAW_RLWIMI(b2p[TMP_REG_1], dst_reg, 24, 16, 23));
- 				EMIT(PPC_RAW_MR(dst_reg, b2p[TMP_REG_1]));
+ 				EMIT(PPC_RAW_RLWIMI(tmp1_reg, dst_reg, 24, 16, 23));
+ 				EMIT(PPC_RAW_MR(dst_reg, tmp1_reg));
  				break;
  			case 64:
++<<<<<<< HEAD
 +				/*
 +				 * Way easier and faster(?) to store the value
 +				 * into stack and then use ldbrx
 +				 *
 +				 * ctx->seen will be reliable in pass2, but
 +				 * the instructions generated will remain the
 +				 * same across all passes
 +				 */
 +				PPC_BPF_STL(dst_reg, 1, bpf_jit_stack_local(ctx));
 +				EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], 1, bpf_jit_stack_local(ctx)));
 +				EMIT(PPC_RAW_LDBRX(dst_reg, 0, b2p[TMP_REG_1]));
++=======
+ 				/* Store the value to stack and then use byte-reverse loads */
+ 				EMIT(PPC_RAW_STD(dst_reg, _R1, bpf_jit_stack_local(ctx)));
+ 				EMIT(PPC_RAW_ADDI(tmp1_reg, _R1, bpf_jit_stack_local(ctx)));
+ 				if (cpu_has_feature(CPU_FTR_ARCH_206)) {
+ 					EMIT(PPC_RAW_LDBRX(dst_reg, 0, tmp1_reg));
+ 				} else {
+ 					EMIT(PPC_RAW_LWBRX(dst_reg, 0, tmp1_reg));
+ 					if (IS_ENABLED(CONFIG_CPU_LITTLE_ENDIAN))
+ 						EMIT(PPC_RAW_SLDI(dst_reg, dst_reg, 32));
+ 					EMIT(PPC_RAW_LI(tmp2_reg, 4));
+ 					EMIT(PPC_RAW_LWBRX(tmp2_reg, tmp2_reg, tmp1_reg));
+ 					if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+ 						EMIT(PPC_RAW_SLDI(tmp2_reg, tmp2_reg, 32));
+ 					EMIT(PPC_RAW_OR(dst_reg, dst_reg, tmp2_reg));
+ 				}
++>>>>>>> 3a3fc9bf1039 (powerpc64/bpf: Store temp registers' bpf to ppc mapping)
  				break;
  			}
  			break;
@@@ -726,10 -761,15 +734,19 @@@ emit_clear
  		case BPF_STX | BPF_MEM | BPF_DW: /* (u64 *)(dst + off) = src */
  		case BPF_ST | BPF_MEM | BPF_DW: /* *(u64 *)(dst + off) = imm */
  			if (BPF_CLASS(code) == BPF_ST) {
- 				PPC_LI32(b2p[TMP_REG_1], imm);
- 				src_reg = b2p[TMP_REG_1];
+ 				PPC_LI32(tmp1_reg, imm);
+ 				src_reg = tmp1_reg;
  			}
++<<<<<<< HEAD
 +			PPC_BPF_STL(src_reg, dst_reg, off);
++=======
+ 			if (off % 4) {
+ 				EMIT(PPC_RAW_LI(tmp2_reg, off));
+ 				EMIT(PPC_RAW_STDX(src_reg, dst_reg, tmp2_reg));
+ 			} else {
+ 				EMIT(PPC_RAW_STD(src_reg, dst_reg, off));
+ 			}
++>>>>>>> 3a3fc9bf1039 (powerpc64/bpf: Store temp registers' bpf to ppc mapping)
  			break;
  
  		/*
@@@ -779,25 -819,70 +796,83 @@@
  		 */
  		/* dst = *(u8 *)(ul) (src + off) */
  		case BPF_LDX | BPF_MEM | BPF_B:
 -		case BPF_LDX | BPF_PROBE_MEM | BPF_B:
 +			EMIT(PPC_RAW_LBZ(dst_reg, src_reg, off));
 +			if (insn_is_zext(&insn[i + 1]))
 +				addrs[++i] = ctx->idx * 4;
 +			break;
  		/* dst = *(u16 *)(ul) (src + off) */
  		case BPF_LDX | BPF_MEM | BPF_H:
 -		case BPF_LDX | BPF_PROBE_MEM | BPF_H:
 +			EMIT(PPC_RAW_LHZ(dst_reg, src_reg, off));
 +			if (insn_is_zext(&insn[i + 1]))
 +				addrs[++i] = ctx->idx * 4;
 +			break;
  		/* dst = *(u32 *)(ul) (src + off) */
  		case BPF_LDX | BPF_MEM | BPF_W:
 -		case BPF_LDX | BPF_PROBE_MEM | BPF_W:
 +			EMIT(PPC_RAW_LWZ(dst_reg, src_reg, off));
 +			if (insn_is_zext(&insn[i + 1]))
 +				addrs[++i] = ctx->idx * 4;
 +			break;
  		/* dst = *(u64 *)(ul) (src + off) */
  		case BPF_LDX | BPF_MEM | BPF_DW:
++<<<<<<< HEAD
 +			PPC_BPF_LL(dst_reg, src_reg, off);
++=======
+ 		case BPF_LDX | BPF_PROBE_MEM | BPF_DW:
+ 			/*
+ 			 * As PTR_TO_BTF_ID that uses BPF_PROBE_MEM mode could either be a valid
+ 			 * kernel pointer or NULL but not a userspace address, execute BPF_PROBE_MEM
+ 			 * load only if addr is kernel address (see is_kernel_addr()), otherwise
+ 			 * set dst_reg=0 and move on.
+ 			 */
+ 			if (BPF_MODE(code) == BPF_PROBE_MEM) {
+ 				EMIT(PPC_RAW_ADDI(tmp1_reg, src_reg, off));
+ 				if (IS_ENABLED(CONFIG_PPC_BOOK3E_64))
+ 					PPC_LI64(tmp2_reg, 0x8000000000000000ul);
+ 				else /* BOOK3S_64 */
+ 					PPC_LI64(tmp2_reg, PAGE_OFFSET);
+ 				EMIT(PPC_RAW_CMPLD(tmp1_reg, tmp2_reg));
+ 				PPC_BCC_SHORT(COND_GT, (ctx->idx + 3) * 4);
+ 				EMIT(PPC_RAW_LI(dst_reg, 0));
+ 				/*
+ 				 * Check if 'off' is word aligned for BPF_DW, because
+ 				 * we might generate two instructions.
+ 				 */
+ 				if (BPF_SIZE(code) == BPF_DW && (off & 3))
+ 					PPC_JMP((ctx->idx + 3) * 4);
+ 				else
+ 					PPC_JMP((ctx->idx + 2) * 4);
+ 			}
+ 
+ 			switch (size) {
+ 			case BPF_B:
+ 				EMIT(PPC_RAW_LBZ(dst_reg, src_reg, off));
+ 				break;
+ 			case BPF_H:
+ 				EMIT(PPC_RAW_LHZ(dst_reg, src_reg, off));
+ 				break;
+ 			case BPF_W:
+ 				EMIT(PPC_RAW_LWZ(dst_reg, src_reg, off));
+ 				break;
+ 			case BPF_DW:
+ 				if (off % 4) {
+ 					EMIT(PPC_RAW_LI(tmp1_reg, off));
+ 					EMIT(PPC_RAW_LDX(dst_reg, src_reg, tmp1_reg));
+ 				} else {
+ 					EMIT(PPC_RAW_LD(dst_reg, src_reg, off));
+ 				}
+ 				break;
+ 			}
+ 
+ 			if (size != BPF_DW && insn_is_zext(&insn[i + 1]))
+ 				addrs[++i] = ctx->idx * 4;
+ 
+ 			if (BPF_MODE(code) == BPF_PROBE_MEM) {
+ 				ret = bpf_add_extable_entry(fp, image, pass, ctx, ctx->idx - 1,
+ 							    4, dst_reg);
+ 				if (ret)
+ 					return ret;
+ 			}
++>>>>>>> 3a3fc9bf1039 (powerpc64/bpf: Store temp registers' bpf to ppc mapping)
  			break;
  
  		/*
@@@ -825,8 -910,11 +900,16 @@@
  			 * the epilogue. If we _are_ the last instruction,
  			 * we'll just fall through to the epilogue.
  			 */
++<<<<<<< HEAD
 +			if (i != flen - 1)
 +				PPC_JMP(exit_addr);
++=======
+ 			if (i != flen - 1) {
+ 				ret = bpf_jit_emit_exit_insn(image, ctx, tmp1_reg, exit_addr);
+ 				if (ret)
+ 					return ret;
+ 			}
++>>>>>>> 3a3fc9bf1039 (powerpc64/bpf: Store temp registers' bpf to ppc mapping)
  			/* else fall through to the epilogue */
  			break;
  
* Unmerged path arch/powerpc/net/bpf_jit_comp64.c
