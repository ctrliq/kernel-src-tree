powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
commit 49c3af43e65fbcc13860e0cf5fb2507b13e9724c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/49c3af43.failed

Convert bpf_to_ppc() to a macro to help simplify its usage since
codegen_context is available in all places it is used. Adopt it also for
powerpc64 for uniformity and get rid of the global b2p structure.

	Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/09f0540ce3e0cd4120b5b33993b5e73b6ef9e979.1644834730.git.naveen.n.rao@linux.vnet.ibm.com

(cherry picked from commit 49c3af43e65fbcc13860e0cf5fb2507b13e9724c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/net/bpf_jit.h
#	arch/powerpc/net/bpf_jit_comp32.c
#	arch/powerpc/net/bpf_jit_comp64.c
diff --cc arch/powerpc/net/bpf_jit.h
index d22140878361,979701d360da..000000000000
--- a/arch/powerpc/net/bpf_jit.h
+++ b/arch/powerpc/net/bpf_jit.h
@@@ -118,15 -119,6 +118,18 @@@ static inline bool is_nearbranch(int of
  #define SEEN_FUNC	0x20000000 /* might call external helpers */
  #define SEEN_TAILCALL	0x40000000 /* uses tail calls */
  
++<<<<<<< HEAD
 +#define SEEN_VREG_MASK	0x1ff80000 /* Volatile registers r3-r12 */
 +#define SEEN_NVREG_MASK	0x0003ffff /* Non volatile registers r14-r31 */
 +
 +#ifdef CONFIG_PPC64
 +extern const int b2p[MAX_BPF_JIT_REG + 2];
 +#else
 +extern const int b2p[MAX_BPF_JIT_REG + 1];
 +#endif
 +
++=======
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  struct codegen_context {
  	/*
  	 * This is used to track register usage as well
@@@ -140,9 -132,19 +143,25 @@@
  	unsigned int seen;
  	unsigned int idx;
  	unsigned int stack_size;
++<<<<<<< HEAD
 +	int b2p[ARRAY_SIZE(b2p)];
 +};
 +
++=======
+ 	int b2p[MAX_BPF_JIT_REG + 2];
+ 	unsigned int exentry_idx;
+ 	unsigned int alt_exit_addr;
+ };
+ 
+ #define bpf_to_ppc(r)	(ctx->b2p[r])
+ 
+ #ifdef CONFIG_PPC32
+ #define BPF_FIXUP_LEN	3 /* Three instructions => 12 bytes */
+ #else
+ #define BPF_FIXUP_LEN	2 /* Two instructions => 8 bytes */
+ #endif
+ 
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  static inline void bpf_flush_icache(void *start, void *end)
  {
  	smp_wmb();	/* smp write barrier */
@@@ -164,9 -166,10 +183,14 @@@ static inline void bpf_clear_seen_regis
  	ctx->seen &= ~(1 << (31 - i));
  }
  
++<<<<<<< HEAD
 +void bpf_jit_emit_func_call_rel(u32 *image, struct codegen_context *ctx, u64 func);
++=======
+ void bpf_jit_init_reg_mapping(struct codegen_context *ctx);
+ int bpf_jit_emit_func_call_rel(u32 *image, struct codegen_context *ctx, u64 func);
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  int bpf_jit_build_body(struct bpf_prog *fp, u32 *image, struct codegen_context *ctx,
 -		       u32 *addrs, int pass);
 +		       u32 *addrs, bool extra_pass);
  void bpf_jit_build_prologue(u32 *image, struct codegen_context *ctx);
  void bpf_jit_build_epilogue(u32 *image, struct codegen_context *ctx);
  void bpf_jit_realloc_regs(struct codegen_context *ctx);
diff --cc arch/powerpc/net/bpf_jit_comp64.c
index 3361fea2cdf5,585f257da045..000000000000
--- a/arch/powerpc/net/bpf_jit_comp64.c
+++ b/arch/powerpc/net/bpf_jit_comp64.c
@@@ -20,7 -17,60 +20,64 @@@
  #include <linux/bpf.h>
  #include <asm/security_features.h>
  
++<<<<<<< HEAD
 +#include "bpf_jit64.h"
++=======
+ #include "bpf_jit.h"
+ 
+ /*
+  * Stack layout:
+  * Ensure the top half (upto local_tmp_var) stays consistent
+  * with our redzone usage.
+  *
+  *		[	prev sp		] <-------------
+  *		[   nv gpr save area	] 5*8		|
+  *		[    tail_call_cnt	] 8		|
+  *		[    local_tmp_var	] 16		|
+  * fp (r31) -->	[   ebpf stack space	] upto 512	|
+  *		[     frame header	] 32/112	|
+  * sp (r1) --->	[    stack pointer	] --------------
+  */
+ 
+ /* for gpr non volatile registers BPG_REG_6 to 10 */
+ #define BPF_PPC_STACK_SAVE	(5*8)
+ /* for bpf JIT code internal usage */
+ #define BPF_PPC_STACK_LOCALS	24
+ /* stack frame excluding BPF stack, ensure this is quadword aligned */
+ #define BPF_PPC_STACKFRAME	(STACK_FRAME_MIN_SIZE + \
+ 				 BPF_PPC_STACK_LOCALS + BPF_PPC_STACK_SAVE)
+ 
+ /* BPF register usage */
+ #define TMP_REG_1	(MAX_BPF_JIT_REG + 0)
+ #define TMP_REG_2	(MAX_BPF_JIT_REG + 1)
+ 
+ /* BPF to ppc register mappings */
+ void bpf_jit_init_reg_mapping(struct codegen_context *ctx)
+ {
+ 	/* function return value */
+ 	ctx->b2p[BPF_REG_0] = _R8;
+ 	/* function arguments */
+ 	ctx->b2p[BPF_REG_1] = _R3;
+ 	ctx->b2p[BPF_REG_2] = _R4;
+ 	ctx->b2p[BPF_REG_3] = _R5;
+ 	ctx->b2p[BPF_REG_4] = _R6;
+ 	ctx->b2p[BPF_REG_5] = _R7;
+ 	/* non volatile registers */
+ 	ctx->b2p[BPF_REG_6] = _R27;
+ 	ctx->b2p[BPF_REG_7] = _R28;
+ 	ctx->b2p[BPF_REG_8] = _R29;
+ 	ctx->b2p[BPF_REG_9] = _R30;
+ 	/* frame pointer aka BPF_REG_10 */
+ 	ctx->b2p[BPF_REG_FP] = _R31;
+ 	/* eBPF jit internal registers */
+ 	ctx->b2p[BPF_REG_AX] = _R12;
+ 	ctx->b2p[TMP_REG_1] = _R9;
+ 	ctx->b2p[TMP_REG_2] = _R10;
+ }
+ 
+ /* PPC NVR range -- update this if we ever use NVRs below r27 */
+ #define BPF_PPC_NVR_MIN		_R27
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  
  static inline bool bpf_has_stack_frame(struct codegen_context *ctx)
  {
@@@ -85,9 -135,9 +142,13 @@@ void bpf_jit_build_prologue(u32 *image
  	 * invoked through a tail call.
  	 */
  	if (ctx->seen & SEEN_TAILCALL) {
- 		EMIT(PPC_RAW_LI(b2p[TMP_REG_1], 0));
+ 		EMIT(PPC_RAW_LI(bpf_to_ppc(TMP_REG_1), 0));
  		/* this goes in the redzone */
++<<<<<<< HEAD
 +		PPC_BPF_STL(b2p[TMP_REG_1], 1, -(BPF_PPC_STACK_SAVE + 8));
++=======
+ 		EMIT(PPC_RAW_STD(bpf_to_ppc(TMP_REG_1), _R1, -(BPF_PPC_STACK_SAVE + 8)));
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  	} else {
  		EMIT(PPC_RAW_NOP());
  		EMIT(PPC_RAW_NOP());
@@@ -112,12 -162,12 +173,21 @@@
  	 * in the protected zone below the previous stack frame
  	 */
  	for (i = BPF_REG_6; i <= BPF_REG_10; i++)
++<<<<<<< HEAD
 +		if (bpf_is_seen_register(ctx, b2p[i]))
 +			PPC_BPF_STL(b2p[i], 1, bpf_jit_stack_offsetof(ctx, b2p[i]));
 +
 +	/* Setup frame pointer to point to the bpf stack area */
 +	if (bpf_is_seen_register(ctx, b2p[BPF_REG_FP]))
 +		EMIT(PPC_RAW_ADDI(b2p[BPF_REG_FP], 1,
++=======
+ 		if (bpf_is_seen_register(ctx, bpf_to_ppc(i)))
+ 			EMIT(PPC_RAW_STD(bpf_to_ppc(i), _R1, bpf_jit_stack_offsetof(ctx, bpf_to_ppc(i))));
+ 
+ 	/* Setup frame pointer to point to the bpf stack area */
+ 	if (bpf_is_seen_register(ctx, bpf_to_ppc(BPF_REG_FP)))
+ 		EMIT(PPC_RAW_ADDI(bpf_to_ppc(BPF_REG_FP), _R1,
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  				STACK_FRAME_MIN_SIZE + ctx->stack_size));
  }
  
@@@ -127,8 -177,8 +197,13 @@@ static void bpf_jit_emit_common_epilogu
  
  	/* Restore NVRs */
  	for (i = BPF_REG_6; i <= BPF_REG_10; i++)
++<<<<<<< HEAD
 +		if (bpf_is_seen_register(ctx, b2p[i]))
 +			PPC_BPF_LL(b2p[i], 1, bpf_jit_stack_offsetof(ctx, b2p[i]));
++=======
+ 		if (bpf_is_seen_register(ctx, bpf_to_ppc(i)))
+ 			EMIT(PPC_RAW_LD(bpf_to_ppc(i), _R1, bpf_jit_stack_offsetof(ctx, bpf_to_ppc(i))));
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  
  	/* Tear down our stack frame */
  	if (bpf_has_stack_frame(ctx)) {
@@@ -145,7 -195,7 +220,11 @@@ void bpf_jit_build_epilogue(u32 *image
  	bpf_jit_emit_common_epilogue(image, ctx);
  
  	/* Move result to r3 */
++<<<<<<< HEAD
 +	EMIT(PPC_RAW_MR(3, b2p[BPF_REG_0]));
++=======
+ 	EMIT(PPC_RAW_MR(_R3, bpf_to_ppc(BPF_REG_0)));
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  
  	EMIT(PPC_RAW_BLR());
  }
@@@ -230,42 -273,42 +309,72 @@@ static void bpf_jit_emit_tail_call(u32 
  	 * if (index >= array->map.max_entries)
  	 *   goto out;
  	 */
- 	EMIT(PPC_RAW_LWZ(b2p[TMP_REG_1], b2p_bpf_array, offsetof(struct bpf_array, map.max_entries)));
+ 	EMIT(PPC_RAW_LWZ(bpf_to_ppc(TMP_REG_1), b2p_bpf_array, offsetof(struct bpf_array, map.max_entries)));
  	EMIT(PPC_RAW_RLWINM(b2p_index, b2p_index, 0, 0, 31));
++<<<<<<< HEAD
 +	EMIT(PPC_RAW_CMPLW(b2p_index, b2p[TMP_REG_1]));
 +	PPC_BCC(COND_GE, out);
++=======
+ 	EMIT(PPC_RAW_CMPLW(b2p_index, bpf_to_ppc(TMP_REG_1)));
+ 	PPC_BCC_SHORT(COND_GE, out);
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  
  	/*
 -	 * if (tail_call_cnt >= MAX_TAIL_CALL_CNT)
 +	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)
  	 *   goto out;
  	 */
++<<<<<<< HEAD
 +	PPC_BPF_LL(b2p[TMP_REG_1], 1, bpf_jit_stack_tailcallcnt(ctx));
 +	EMIT(PPC_RAW_CMPLWI(b2p[TMP_REG_1], MAX_TAIL_CALL_CNT));
 +	PPC_BCC(COND_GT, out);
++=======
+ 	EMIT(PPC_RAW_LD(bpf_to_ppc(TMP_REG_1), _R1, bpf_jit_stack_tailcallcnt(ctx)));
+ 	EMIT(PPC_RAW_CMPLWI(bpf_to_ppc(TMP_REG_1), MAX_TAIL_CALL_CNT));
+ 	PPC_BCC_SHORT(COND_GE, out);
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  
  	/*
  	 * tail_call_cnt++;
  	 */
++<<<<<<< HEAD
 +	EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], b2p[TMP_REG_1], 1));
 +	PPC_BPF_STL(b2p[TMP_REG_1], 1, bpf_jit_stack_tailcallcnt(ctx));
 +
 +	/* prog = array->ptrs[index]; */
 +	EMIT(PPC_RAW_MULI(b2p[TMP_REG_1], b2p_index, 8));
 +	EMIT(PPC_RAW_ADD(b2p[TMP_REG_1], b2p[TMP_REG_1], b2p_bpf_array));
 +	PPC_BPF_LL(b2p[TMP_REG_1], b2p[TMP_REG_1], offsetof(struct bpf_array, ptrs));
++=======
+ 	EMIT(PPC_RAW_ADDI(bpf_to_ppc(TMP_REG_1), bpf_to_ppc(TMP_REG_1), 1));
+ 	EMIT(PPC_RAW_STD(bpf_to_ppc(TMP_REG_1), _R1, bpf_jit_stack_tailcallcnt(ctx)));
+ 
+ 	/* prog = array->ptrs[index]; */
+ 	EMIT(PPC_RAW_MULI(bpf_to_ppc(TMP_REG_1), b2p_index, 8));
+ 	EMIT(PPC_RAW_ADD(bpf_to_ppc(TMP_REG_1), bpf_to_ppc(TMP_REG_1), b2p_bpf_array));
+ 	EMIT(PPC_RAW_LD(bpf_to_ppc(TMP_REG_1), bpf_to_ppc(TMP_REG_1), offsetof(struct bpf_array, ptrs)));
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  
  	/*
  	 * if (prog == NULL)
  	 *   goto out;
  	 */
++<<<<<<< HEAD
 +	EMIT(PPC_RAW_CMPLDI(b2p[TMP_REG_1], 0));
 +	PPC_BCC(COND_EQ, out);
 +
 +	/* goto *(prog->bpf_func + prologue_size); */
 +	PPC_BPF_LL(b2p[TMP_REG_1], b2p[TMP_REG_1], offsetof(struct bpf_prog, bpf_func));
 +	EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], b2p[TMP_REG_1],
++=======
+ 	EMIT(PPC_RAW_CMPLDI(bpf_to_ppc(TMP_REG_1), 0));
+ 	PPC_BCC_SHORT(COND_EQ, out);
+ 
+ 	/* goto *(prog->bpf_func + prologue_size); */
+ 	EMIT(PPC_RAW_LD(bpf_to_ppc(TMP_REG_1), bpf_to_ppc(TMP_REG_1), offsetof(struct bpf_prog, bpf_func)));
+ 	EMIT(PPC_RAW_ADDI(bpf_to_ppc(TMP_REG_1), bpf_to_ppc(TMP_REG_1),
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  			FUNCTION_DESCR_SIZE + bpf_tailcall_prologue_size));
- 	EMIT(PPC_RAW_MTCTR(b2p[TMP_REG_1]));
+ 	EMIT(PPC_RAW_MTCTR(bpf_to_ppc(TMP_REG_1)));
  
  	/* tear down stack, restore NVRs, ... */
  	bpf_jit_emit_common_epilogue(image, ctx);
@@@ -310,8 -355,11 +419,16 @@@ int bpf_jit_build_body(struct bpf_prog 
  
  	for (i = 0; i < flen; i++) {
  		u32 code = insn[i].code;
++<<<<<<< HEAD
 +		u32 dst_reg = b2p[insn[i].dst_reg];
 +		u32 src_reg = b2p[insn[i].src_reg];
++=======
+ 		u32 dst_reg = bpf_to_ppc(insn[i].dst_reg);
+ 		u32 src_reg = bpf_to_ppc(insn[i].src_reg);
+ 		u32 size = BPF_SIZE(code);
+ 		u32 tmp1_reg = bpf_to_ppc(TMP_REG_1);
+ 		u32 tmp2_reg = bpf_to_ppc(TMP_REG_2);
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  		s16 off = insn[i].off;
  		s32 imm = insn[i].imm;
  		bool func_addr_fixed;
@@@ -842,11 -931,15 +959,15 @@@ emit_clear
  				return ret;
  
  			if (func_addr_fixed)
 -				ret = bpf_jit_emit_func_call_hlp(image, ctx, func_addr);
 +				bpf_jit_emit_func_call_hlp(image, ctx, func_addr);
  			else
 -				ret = bpf_jit_emit_func_call_rel(image, ctx, func_addr);
 -
 -			if (ret)
 -				return ret;
 -
 +				bpf_jit_emit_func_call_rel(image, ctx, func_addr);
  			/* move return value from r3 to BPF_REG_0 */
++<<<<<<< HEAD
 +			EMIT(PPC_RAW_MR(b2p[BPF_REG_0], 3));
++=======
+ 			EMIT(PPC_RAW_MR(bpf_to_ppc(BPF_REG_0), _R3));
++>>>>>>> 49c3af43e65f (powerpc/bpf: Simplify bpf_to_ppc() and adopt it for powerpc64)
  			break;
  
  		/*
* Unmerged path arch/powerpc/net/bpf_jit_comp32.c
* Unmerged path arch/powerpc/net/bpf_jit.h
diff --git a/arch/powerpc/net/bpf_jit_comp.c b/arch/powerpc/net/bpf_jit_comp.c
index 16ef5fa3992f..6e36cba75cee 100644
--- a/arch/powerpc/net/bpf_jit_comp.c
+++ b/arch/powerpc/net/bpf_jit_comp.c
@@ -70,13 +70,13 @@ static int bpf_jit_fixup_addresses(struct bpf_prog *fp, u32 *image,
 			tmp_idx = ctx->idx;
 			ctx->idx = addrs[i] / 4;
 #ifdef CONFIG_PPC32
-			PPC_LI32(ctx->b2p[insn[i].dst_reg] - 1, (u32)insn[i + 1].imm);
-			PPC_LI32(ctx->b2p[insn[i].dst_reg], (u32)insn[i].imm);
+			PPC_LI32(bpf_to_ppc(insn[i].dst_reg) - 1, (u32)insn[i + 1].imm);
+			PPC_LI32(bpf_to_ppc(insn[i].dst_reg), (u32)insn[i].imm);
 			for (j = ctx->idx - addrs[i] / 4; j < 4; j++)
 				EMIT(PPC_RAW_NOP());
 #else
 			func_addr = ((u64)(u32)insn[i].imm) | (((u64)(u32)insn[i + 1].imm) << 32);
-			PPC_LI64(b2p[insn[i].dst_reg], func_addr);
+			PPC_LI64(bpf_to_ppc(insn[i].dst_reg), func_addr);
 			/* overwrite rest with nops */
 			for (j = ctx->idx - addrs[i] / 4; j < 5; j++)
 				EMIT(PPC_RAW_NOP());
@@ -160,7 +160,7 @@ struct bpf_prog *bpf_int_jit_compile(struct bpf_prog *fp)
 	}
 
 	memset(&cgctx, 0, sizeof(struct codegen_context));
-	memcpy(cgctx.b2p, b2p, sizeof(cgctx.b2p));
+	bpf_jit_init_reg_mapping(&cgctx);
 
 	/* Make sure that the stack is quadword aligned. */
 	cgctx.stack_size = round_up(fp->aux->stack_depth, 16);
* Unmerged path arch/powerpc/net/bpf_jit_comp32.c
* Unmerged path arch/powerpc/net/bpf_jit_comp64.c
