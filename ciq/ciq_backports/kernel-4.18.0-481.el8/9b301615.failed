memcg: sync flush only if periodic flush is delayed

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Shakeel Butt <shakeelb@google.com>
commit 9b3016154c913b2e7ec5ae5c9a42eb9e732d86aa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/9b301615.failed

Daniel Dao has reported [1] a regression on workloads that may trigger a
lot of refaults (anon and file).  The underlying issue is that flushing
rstat is expensive.  Although rstat flush are batched with (nr_cpus *
MEMCG_BATCH) stat updates, it seems like there are workloads which
genuinely do stat updates larger than batch value within short amount of
time.  Since the rstat flush can happen in the performance critical
codepaths like page faults, such workload can suffer greatly.

This patch fixes this regression by making the rstat flushing
conditional in the performance critical codepaths.  More specifically,
the kernel relies on the async periodic rstat flusher to flush the stats
and only if the periodic flusher is delayed by more than twice the
amount of its normal time window then the kernel allows rstat flushing
from the performance critical codepaths.

Now the question: what are the side-effects of this change? The worst
that can happen is the refault codepath will see 4sec old lruvec stats
and may cause false (or missed) activations of the refaulted page which
may under-or-overestimate the workingset size.  Though that is not very
concerning as the kernel can already miss or do false activations.

There are two more codepaths whose flushing behavior is not changed by
this patch and we may need to come to them in future.  One is the
writeback stats used by dirty throttling and second is the deactivation
heuristic in the reclaim.  For now keeping an eye on them and if there
is report of regression due to these codepaths, we will reevaluate then.

Link: https://lore.kernel.org/all/CA+wXwBSyO87ZX5PVwdHm-=dBjZYECGmfnydUicUyrQqndgX2MQ@mail.gmail.com [1]
Link: https://lkml.kernel.org/r/20220304184040.1304781-1-shakeelb@google.com
Fixes: 1f828223b799 ("memcg: flush lruvec stats in the refault")
	Signed-off-by: Shakeel Butt <shakeelb@google.com>
	Reported-by: Daniel Dao <dqminh@cloudflare.com>
	Tested-by: Ivan Babrou <ivan@cloudflare.com>
	Cc: Michal Hocko <mhocko@suse.com>
	Cc: Roman Gushchin <roman.gushchin@linux.dev>
	Cc: Johannes Weiner <hannes@cmpxchg.org>
	Cc: Michal Koutn√Ω <mkoutny@suse.com>
	Cc: Frank Hofmann <fhofmann@cloudflare.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 9b3016154c913b2e7ec5ae5c9a42eb9e732d86aa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/memcontrol.h
#	mm/memcontrol.c
#	mm/workingset.c
diff --cc include/linux/memcontrol.h
index 99cbfe3d87a3,89b14729d59f..000000000000
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@@ -1028,6 -1011,9 +1028,12 @@@ static inline unsigned long lruvec_page
  	return x;
  }
  
++<<<<<<< HEAD
++=======
+ void mem_cgroup_flush_stats(void);
+ void mem_cgroup_flush_stats_delayed(void);
+ 
++>>>>>>> 9b3016154c91 (memcg: sync flush only if periodic flush is delayed)
  void __mod_memcg_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,
  			      int val);
  void __mod_lruvec_kmem_state(void *p, enum node_stat_item idx, int val);
@@@ -1438,6 -1452,14 +1444,17 @@@ static inline unsigned long lruvec_page
  	return node_page_state(lruvec_pgdat(lruvec), idx);
  }
  
++<<<<<<< HEAD
++=======
+ static inline void mem_cgroup_flush_stats(void)
+ {
+ }
+ 
+ static inline void mem_cgroup_flush_stats_delayed(void)
+ {
+ }
+ 
++>>>>>>> 9b3016154c91 (memcg: sync flush only if periodic flush is delayed)
  static inline void __mod_memcg_lruvec_state(struct lruvec *lruvec,
  					    enum node_stat_item idx, int val)
  {
diff --cc mm/memcontrol.c
index 6bc1413fddb1,598fece89e2b..000000000000
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@@ -640,6 -568,30 +640,33 @@@ mem_cgroup_largest_soft_limit_node(stru
  }
  
  /*
++<<<<<<< HEAD
++=======
+  * memcg and lruvec stats flushing
+  *
+  * Many codepaths leading to stats update or read are performance sensitive and
+  * adding stats flushing in such codepaths is not desirable. So, to optimize the
+  * flushing the kernel does:
+  *
+  * 1) Periodically and asynchronously flush the stats every 2 seconds to not let
+  *    rstat update tree grow unbounded.
+  *
+  * 2) Flush the stats synchronously on reader side only when there are more than
+  *    (MEMCG_CHARGE_BATCH * nr_cpus) update events. Though this optimization
+  *    will let stats be out of sync by atmost (MEMCG_CHARGE_BATCH * nr_cpus) but
+  *    only for 2 seconds due to (1).
+  */
+ static void flush_memcg_stats_dwork(struct work_struct *w);
+ static DECLARE_DEFERRABLE_WORK(stats_flush_dwork, flush_memcg_stats_dwork);
+ static DEFINE_SPINLOCK(stats_flush_lock);
+ static DEFINE_PER_CPU(unsigned int, stats_updates);
+ static atomic_t stats_flush_threshold = ATOMIC_INIT(0);
+ static u64 flush_next_time;
+ 
+ #define FLUSH_TIME (2UL*HZ)
+ 
+ /*
++>>>>>>> 9b3016154c91 (memcg: sync flush only if periodic flush is delayed)
   * Accessors to ensure that preemption is disabled on PREEMPT_RT because it can
   * not rely on this as part of an acquired spinlock_t lock. These functions are
   * never used in hardirq context on PREEMPT_RT and therefore disabling preemtion
@@@ -668,99 -620,48 +695,122 @@@ static void memcg_stats_unlock(void
  #endif
  }
  
 -static inline void memcg_rstat_updated(struct mem_cgroup *memcg, int val)
 +/*
 + * Return the active percpu stats memcg and optionally mem_cgroup_per_node.
 + *
 + * When percpu_stats_disabled, the percpu stats update is transferred to
 + * its parent.
 + */
 +static inline struct mem_cgroup *
 +percpu_stats_memcg(struct mem_cgroup *memcg, struct mem_cgroup_per_node **pn)
  {
 -	unsigned int x;
 +	if (likely(!memcg->percpu_stats_disabled))
 +		return memcg;
  
 -	cgroup_rstat_updated(memcg->css.cgroup, smp_processor_id());
 +	do {
 +		memcg = parent_mem_cgroup(memcg);
 +	} while (memcg->percpu_stats_disabled);
  
 -	x = __this_cpu_add_return(stats_updates, abs(val));
 -	if (x > MEMCG_CHARGE_BATCH) {
 -		atomic_add(x / MEMCG_CHARGE_BATCH, &stats_flush_threshold);
 -		__this_cpu_write(stats_updates, 0);
 +	if (pn) {
 +		unsigned int nid = (*pn)->nid;
 +
 +		*pn = memcg->nodeinfo[nid];
  	}
 +	return memcg;
  }
  
 -static void __mem_cgroup_flush_stats(void)
 +/* Subset of vm_event_item to report for memcg event stats */
 +static const unsigned int memcg_vm_event_stat[] = {
 +	PGPGIN,
 +	PGPGOUT,
 +	PGSCAN_KSWAPD,
 +	PGSCAN_DIRECT,
 +	PGSTEAL_KSWAPD,
 +	PGSTEAL_DIRECT,
 +	PGFAULT,
 +	PGMAJFAULT,
 +	PGREFILL,
 +	PGACTIVATE,
 +	PGDEACTIVATE,
 +	PGLAZYFREE,
 +	PGLAZYFREED,
 +#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 +	THP_FAULT_ALLOC,
 +	THP_COLLAPSE_ALLOC,
 +#endif
 +};
 +
 +#define NR_MEMCG_EVENTS ARRAY_SIZE(memcg_vm_event_stat)
 +static int mem_cgroup_events_index[NR_VM_EVENT_ITEMS] __read_mostly;
 +
 +static void init_memcg_events(void)
  {
 -	unsigned long flag;
 +	int i;
  
++<<<<<<< HEAD
 +	for (i = 0; i < NR_MEMCG_EVENTS; ++i)
 +		mem_cgroup_events_index[memcg_vm_event_stat[i]] = i + 1;
++=======
+ 	if (!spin_trylock_irqsave(&stats_flush_lock, flag))
+ 		return;
+ 
+ 	flush_next_time = jiffies_64 + 2*FLUSH_TIME;
+ 	cgroup_rstat_flush_irqsafe(root_mem_cgroup->css.cgroup);
+ 	atomic_set(&stats_flush_threshold, 0);
+ 	spin_unlock_irqrestore(&stats_flush_lock, flag);
++>>>>>>> 9b3016154c91 (memcg: sync flush only if periodic flush is delayed)
  }
  
 -void mem_cgroup_flush_stats(void)
 +static inline int memcg_events_index(enum vm_event_item idx)
  {
 -	if (atomic_read(&stats_flush_threshold) > num_online_cpus())
 -		__mem_cgroup_flush_stats();
 +	return mem_cgroup_events_index[idx] - 1;
  }
  
++<<<<<<< HEAD
 +struct memcg_vmstats_percpu {
 +	/* Local (CPU and cgroup) page state & events */
 +	long			state[MEMCG_NR_STAT];
 +	unsigned long		events[NR_MEMCG_EVENTS];
 +
 +	/* Delta calculation for lockless upward propagation */
 +	long			state_prev[MEMCG_NR_STAT];
 +	unsigned long		events_prev[NR_MEMCG_EVENTS];
 +
 +	/* Cgroup1: threshold notifications & softlimit tree updates */
 +	unsigned long		nr_page_events;
 +	unsigned long		targets[MEM_CGROUP_NTARGETS];
 +};
 +
 +struct memcg_vmstats {
 +	/* Aggregated (CPU and subtree) page state & events */
 +	long			state[MEMCG_NR_STAT];
 +	unsigned long		events[NR_MEMCG_EVENTS];
 +
 +	/* Pending child counts during tree propagation */
 +	long			state_pending[MEMCG_NR_STAT];
 +	unsigned long		events_pending[NR_MEMCG_EVENTS];
 +};
 +
 +unsigned long memcg_page_state(struct mem_cgroup *memcg, int idx)
 +{
 +	long x = READ_ONCE(memcg->vmstats->state[idx]);
 +#ifdef CONFIG_SMP
 +	if (x < 0)
 +		x = 0;
 +#endif
 +	return x;
++=======
+ void mem_cgroup_flush_stats_delayed(void)
+ {
+ 	if (time_after64(jiffies_64, flush_next_time))
+ 		mem_cgroup_flush_stats();
+ }
+ 
+ static void flush_memcg_stats_dwork(struct work_struct *w)
+ {
+ 	__mem_cgroup_flush_stats();
+ 	queue_delayed_work(system_unbound_wq, &stats_flush_dwork, FLUSH_TIME);
++>>>>>>> 9b3016154c91 (memcg: sync flush only if periodic flush is delayed)
  }
  
  /**
diff --cc mm/workingset.c
index 2ad3de738d84,592569a8974c..000000000000
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@@ -338,18 -342,20 +338,22 @@@ void workingset_refault(struct page *pa
  	refault_distance = (refault - eviction) & EVICTION_MASK;
  
  	/*
 -	 * The activation decision for this folio is made at the level
 +	 * The activation decision for this page is made at the level
  	 * where the eviction occurred, as that is where the LRU order
 -	 * during folio reclaim is being determined.
 +	 * during page reclaim is being determined.
  	 *
 -	 * However, the cgroup that will own the folio is the one that
 +	 * However, the cgroup that will own the page is the one that
  	 * is actually experiencing the refault event.
  	 */
 -	nr = folio_nr_pages(folio);
 -	memcg = folio_memcg(folio);
 +	memcg = page_memcg(page);
  	lruvec = mem_cgroup_lruvec(memcg, pgdat);
  
 -	mod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);
 +	inc_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file);
  
++<<<<<<< HEAD
++=======
+ 	mem_cgroup_flush_stats_delayed();
++>>>>>>> 9b3016154c91 (memcg: sync flush only if periodic flush is delayed)
  	/*
  	 * Compare the distance to the existing workingset size. We
  	 * don't activate pages that couldn't stay resident even if
* Unmerged path include/linux/memcontrol.h
* Unmerged path mm/memcontrol.c
* Unmerged path mm/workingset.c
