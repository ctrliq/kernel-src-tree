x86/sev: Use SEV-SNP AP creation to start secondary CPUs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Tom Lendacky <thomas.lendacky@amd.com>
commit 0afb6b660a6b58cb336d1175ed687bf9525849a4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/0afb6b66.failed

To provide a more secure way to start APs under SEV-SNP, use the SEV-SNP
AP Creation NAE event. This allows for guest control over the AP register
state rather than trusting the hypervisor with the SEV-ES Jump Table
address.

During native_smp_prepare_cpus(), invoke an SEV-SNP function that, if
SEV-SNP is active, will set/override apic->wakeup_secondary_cpu. This
will allow the SEV-SNP AP Creation NAE event method to be used to boot
the APs. As a result of installing the override when SEV-SNP is active,
this method of starting the APs becomes the required method. The override
function will fail to start the AP if the hypervisor does not have
support for AP creation.

  [ bp: Work in forgotten review comments. ]

	Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
	Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lore.kernel.org/r/20220307213356.2797205-23-brijesh.singh@amd.com
(cherry picked from commit 0afb6b660a6b58cb336d1175ed687bf9525849a4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/include/asm/sev-common.h
#	arch/x86/include/asm/sev.h
#	arch/x86/include/uapi/asm/svm.h
#	arch/x86/kernel/sev.c
#	arch/x86/kernel/smpboot.c
diff --cc arch/x86/include/asm/sev-common.h
index 94f0ea574049,e9b6815b3b3d..000000000000
--- a/arch/x86/include/asm/sev-common.h
+++ b/arch/x86/include/asm/sev-common.h
@@@ -60,6 -60,73 +60,37 @@@
  /* GHCB Hypervisor Feature Request/Response */
  #define GHCB_MSR_HV_FT_REQ		0x080
  #define GHCB_MSR_HV_FT_RESP		0x081
++<<<<<<< HEAD
++=======
+ #define GHCB_MSR_HV_FT_RESP_VAL(v)			\
+ 	/* GHCBData[63:12] */				\
+ 	(((u64)(v) & GENMASK_ULL(63, 12)) >> 12)
+ 
+ #define GHCB_HV_FT_SNP			BIT_ULL(0)
+ #define GHCB_HV_FT_SNP_AP_CREATION	BIT_ULL(1)
+ 
+ /* SNP Page State Change NAE event */
+ #define VMGEXIT_PSC_MAX_ENTRY		253
+ 
+ struct psc_hdr {
+ 	u16 cur_entry;
+ 	u16 end_entry;
+ 	u32 reserved;
+ } __packed;
+ 
+ struct psc_entry {
+ 	u64	cur_page	: 12,
+ 		gfn		: 40,
+ 		operation	: 4,
+ 		pagesize	: 1,
+ 		reserved	: 7;
+ } __packed;
+ 
+ struct snp_psc_desc {
+ 	struct psc_hdr hdr;
+ 	struct psc_entry entries[VMGEXIT_PSC_MAX_ENTRY];
+ } __packed;
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  
  #define GHCB_MSR_TERM_REQ		0x100
  #define GHCB_MSR_TERM_REASON_SET_POS	12
diff --cc arch/x86/include/asm/sev.h
index 45ba3d868cd3,a3203b2caaca..000000000000
--- a/arch/x86/include/asm/sev.h
+++ b/arch/x86/include/asm/sev.h
@@@ -63,6 -63,11 +63,14 @@@ extern bool handle_vc_boot_ghcb(struct 
  /* Software defined (when rFlags.CF = 1) */
  #define PVALIDATE_FAIL_NOUPDATE		255
  
++<<<<<<< HEAD
++=======
+ /* RMP page size */
+ #define RMP_PG_SIZE_4K			0
+ 
+ #define RMPADJUST_VMSA_PAGE_BIT		BIT(16)
+ 
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  #ifdef CONFIG_AMD_MEM_ENCRYPT
  extern struct static_key_false sev_es_enable_key;
  extern void __sev_es_ist_enter(struct pt_regs *regs);
@@@ -107,6 -124,15 +115,18 @@@ static inline int pvalidate(unsigned lo
  
  	return rc;
  }
++<<<<<<< HEAD
++=======
+ void setup_ghcb(void);
+ void __init early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr,
+ 					 unsigned int npages);
+ void __init early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr,
+ 					unsigned int npages);
+ void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op);
+ void snp_set_memory_shared(unsigned long vaddr, unsigned int npages);
+ void snp_set_memory_private(unsigned long vaddr, unsigned int npages);
+ void snp_set_wakeup_secondary_cpu(void);
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  #else
  static inline void sev_es_ist_enter(struct pt_regs *regs) { }
  static inline void sev_es_ist_exit(void) { }
@@@ -114,6 -140,16 +134,19 @@@ static inline int sev_es_setup_ap_jump_
  static inline void sev_es_nmi_complete(void) { }
  static inline int sev_es_efi_map_ghcbs(pgd_t *pgd) { return 0; }
  static inline int pvalidate(unsigned long vaddr, bool rmp_psize, bool validate) { return 0; }
++<<<<<<< HEAD
++=======
+ static inline int rmpadjust(unsigned long vaddr, bool rmp_psize, unsigned long attrs) { return 0; }
+ static inline void setup_ghcb(void) { }
+ static inline void __init
+ early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr, unsigned int npages) { }
+ static inline void __init
+ early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr, unsigned int npages) { }
+ static inline void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op) { }
+ static inline void snp_set_memory_shared(unsigned long vaddr, unsigned int npages) { }
+ static inline void snp_set_memory_private(unsigned long vaddr, unsigned int npages) { }
+ static inline void snp_set_wakeup_secondary_cpu(void) { }
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  #endif
  
  #endif
diff --cc arch/x86/include/uapi/asm/svm.h
index efa969325ede,cfea5e875088..000000000000
--- a/arch/x86/include/uapi/asm/svm.h
+++ b/arch/x86/include/uapi/asm/svm.h
@@@ -108,6 -108,12 +108,15 @@@
  #define SVM_VMGEXIT_AP_JUMP_TABLE		0x80000005
  #define SVM_VMGEXIT_SET_AP_JUMP_TABLE		0
  #define SVM_VMGEXIT_GET_AP_JUMP_TABLE		1
++<<<<<<< HEAD
++=======
+ #define SVM_VMGEXIT_PSC				0x80000010
+ #define SVM_VMGEXIT_AP_CREATION			0x80000013
+ #define SVM_VMGEXIT_AP_CREATE_ON_INIT		0
+ #define SVM_VMGEXIT_AP_CREATE			1
+ #define SVM_VMGEXIT_AP_DESTROY			2
+ #define SVM_VMGEXIT_HV_FEATURES			0x8000fffd
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  #define SVM_VMGEXIT_UNSUPPORTED_EVENT		0x8000ffff
  
  /* Exit code reserved for hypervisor/software use */
@@@ -218,6 -224,9 +227,12 @@@
  	{ SVM_VMGEXIT_NMI_COMPLETE,	"vmgexit_nmi_complete" }, \
  	{ SVM_VMGEXIT_AP_HLT_LOOP,	"vmgexit_ap_hlt_loop" }, \
  	{ SVM_VMGEXIT_AP_JUMP_TABLE,	"vmgexit_ap_jump_table" }, \
++<<<<<<< HEAD
++=======
+ 	{ SVM_VMGEXIT_PSC,		"vmgexit_page_state_change" }, \
+ 	{ SVM_VMGEXIT_AP_CREATION,	"vmgexit_ap_creation" }, \
+ 	{ SVM_VMGEXIT_HV_FEATURES,	"vmgexit_hypervisor_feature" }, \
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  	{ SVM_EXIT_ERR,         "invalid_guest_state" }
  
  
diff --cc arch/x86/kernel/sev.c
index 9f3a4a57b1e6,d7915ae7729d..000000000000
--- a/arch/x86/kernel/sev.c
+++ b/arch/x86/kernel/sev.c
@@@ -539,6 -576,495 +559,498 @@@ static u64 get_jump_table_addr(void
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static void pvalidate_pages(unsigned long vaddr, unsigned int npages, bool validate)
+ {
+ 	unsigned long vaddr_end;
+ 	int rc;
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		rc = pvalidate(vaddr, RMP_PG_SIZE_4K, validate);
+ 		if (WARN(rc, "Failed to validate address 0x%lx ret %d", vaddr, rc))
+ 			sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PVALIDATE);
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 	}
+ }
+ 
+ static void __init early_set_pages_state(unsigned long paddr, unsigned int npages, enum psc_op op)
+ {
+ 	unsigned long paddr_end;
+ 	u64 val;
+ 
+ 	paddr = paddr & PAGE_MASK;
+ 	paddr_end = paddr + (npages << PAGE_SHIFT);
+ 
+ 	while (paddr < paddr_end) {
+ 		/*
+ 		 * Use the MSR protocol because this function can be called before
+ 		 * the GHCB is established.
+ 		 */
+ 		sev_es_wr_ghcb_msr(GHCB_MSR_PSC_REQ_GFN(paddr >> PAGE_SHIFT, op));
+ 		VMGEXIT();
+ 
+ 		val = sev_es_rd_ghcb_msr();
+ 
+ 		if (WARN(GHCB_RESP_CODE(val) != GHCB_MSR_PSC_RESP,
+ 			 "Wrong PSC response code: 0x%x\n",
+ 			 (unsigned int)GHCB_RESP_CODE(val)))
+ 			goto e_term;
+ 
+ 		if (WARN(GHCB_MSR_PSC_RESP_VAL(val),
+ 			 "Failed to change page state to '%s' paddr 0x%lx error 0x%llx\n",
+ 			 op == SNP_PAGE_STATE_PRIVATE ? "private" : "shared",
+ 			 paddr, GHCB_MSR_PSC_RESP_VAL(val)))
+ 			goto e_term;
+ 
+ 		paddr = paddr + PAGE_SIZE;
+ 	}
+ 
+ 	return;
+ 
+ e_term:
+ 	sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ void __init early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr,
+ 					 unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	 /*
+ 	  * Ask the hypervisor to mark the memory pages as private in the RMP
+ 	  * table.
+ 	  */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	/* Validate the memory pages after they've been added in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
+ void __init early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr,
+ 					unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	/* Invalidate the memory pages before they are marked shared in the RMP table. */
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	 /* Ask hypervisor to mark the memory pages shared in the RMP table. */
+ 	early_set_pages_state(paddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op)
+ {
+ 	unsigned long vaddr, npages;
+ 
+ 	vaddr = (unsigned long)__va(paddr);
+ 	npages = PAGE_ALIGN(sz) >> PAGE_SHIFT;
+ 
+ 	if (op == SNP_PAGE_STATE_PRIVATE)
+ 		early_snp_set_memory_private(vaddr, paddr, npages);
+ 	else if (op == SNP_PAGE_STATE_SHARED)
+ 		early_snp_set_memory_shared(vaddr, paddr, npages);
+ 	else
+ 		WARN(1, "invalid memory op %d\n", op);
+ }
+ 
+ static int vmgexit_psc(struct snp_psc_desc *desc)
+ {
+ 	int cur_entry, end_entry, ret = 0;
+ 	struct snp_psc_desc *data;
+ 	struct ghcb_state state;
+ 	struct es_em_ctxt ctxt;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 
+ 	/*
+ 	 * __sev_get_ghcb() needs to run with IRQs disabled because it is using
+ 	 * a per-CPU GHCB.
+ 	 */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 	if (!ghcb) {
+ 		ret = 1;
+ 		goto out_unlock;
+ 	}
+ 
+ 	/* Copy the input desc into GHCB shared buffer */
+ 	data = (struct snp_psc_desc *)ghcb->shared_buffer;
+ 	memcpy(ghcb->shared_buffer, desc, min_t(int, GHCB_SHARED_BUF_SIZE, sizeof(*desc)));
+ 
+ 	/*
+ 	 * As per the GHCB specification, the hypervisor can resume the guest
+ 	 * before processing all the entries. Check whether all the entries
+ 	 * are processed. If not, then keep retrying. Note, the hypervisor
+ 	 * will update the data memory directly to indicate the status, so
+ 	 * reference the data->hdr everywhere.
+ 	 *
+ 	 * The strategy here is to wait for the hypervisor to change the page
+ 	 * state in the RMP table before guest accesses the memory pages. If the
+ 	 * page state change was not successful, then later memory access will
+ 	 * result in a crash.
+ 	 */
+ 	cur_entry = data->hdr.cur_entry;
+ 	end_entry = data->hdr.end_entry;
+ 
+ 	while (data->hdr.cur_entry <= data->hdr.end_entry) {
+ 		ghcb_set_sw_scratch(ghcb, (u64)__pa(data));
+ 
+ 		/* This will advance the shared buffer data points to. */
+ 		ret = sev_es_ghcb_hv_call(ghcb, true, &ctxt, SVM_VMGEXIT_PSC, 0, 0);
+ 
+ 		/*
+ 		 * Page State Change VMGEXIT can pass error code through
+ 		 * exit_info_2.
+ 		 */
+ 		if (WARN(ret || ghcb->save.sw_exit_info_2,
+ 			 "SNP: PSC failed ret=%d exit_info_2=%llx\n",
+ 			 ret, ghcb->save.sw_exit_info_2)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/* Verify that reserved bit is not set */
+ 		if (WARN(data->hdr.reserved, "Reserved bit is set in the PSC header\n")) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 
+ 		/*
+ 		 * Sanity check that entry processing is not going backwards.
+ 		 * This will happen only if hypervisor is tricking us.
+ 		 */
+ 		if (WARN(data->hdr.end_entry > end_entry || cur_entry > data->hdr.cur_entry,
+ "SNP: PSC processing going backward, end_entry %d (got %d) cur_entry %d (got %d)\n",
+ 			 end_entry, data->hdr.end_entry, cur_entry, data->hdr.cur_entry)) {
+ 			ret = 1;
+ 			goto out;
+ 		}
+ 	}
+ 
+ out:
+ 	__sev_put_ghcb(&state);
+ 
+ out_unlock:
+ 	local_irq_restore(flags);
+ 
+ 	return ret;
+ }
+ 
+ static void __set_pages_state(struct snp_psc_desc *data, unsigned long vaddr,
+ 			      unsigned long vaddr_end, int op)
+ {
+ 	struct psc_hdr *hdr;
+ 	struct psc_entry *e;
+ 	unsigned long pfn;
+ 	int i;
+ 
+ 	hdr = &data->hdr;
+ 	e = data->entries;
+ 
+ 	memset(data, 0, sizeof(*data));
+ 	i = 0;
+ 
+ 	while (vaddr < vaddr_end) {
+ 		if (is_vmalloc_addr((void *)vaddr))
+ 			pfn = vmalloc_to_pfn((void *)vaddr);
+ 		else
+ 			pfn = __pa(vaddr) >> PAGE_SHIFT;
+ 
+ 		e->gfn = pfn;
+ 		e->operation = op;
+ 		hdr->end_entry = i;
+ 
+ 		/*
+ 		 * Current SNP implementation doesn't keep track of the RMP page
+ 		 * size so use 4K for simplicity.
+ 		 */
+ 		e->pagesize = RMP_PG_SIZE_4K;
+ 
+ 		vaddr = vaddr + PAGE_SIZE;
+ 		e++;
+ 		i++;
+ 	}
+ 
+ 	if (vmgexit_psc(data))
+ 		sev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);
+ }
+ 
+ static void set_pages_state(unsigned long vaddr, unsigned int npages, int op)
+ {
+ 	unsigned long vaddr_end, next_vaddr;
+ 	struct snp_psc_desc *desc;
+ 
+ 	desc = kmalloc(sizeof(*desc), GFP_KERNEL_ACCOUNT);
+ 	if (!desc)
+ 		panic("SNP: failed to allocate memory for PSC descriptor\n");
+ 
+ 	vaddr = vaddr & PAGE_MASK;
+ 	vaddr_end = vaddr + (npages << PAGE_SHIFT);
+ 
+ 	while (vaddr < vaddr_end) {
+ 		/* Calculate the last vaddr that fits in one struct snp_psc_desc. */
+ 		next_vaddr = min_t(unsigned long, vaddr_end,
+ 				   (VMGEXIT_PSC_MAX_ENTRY * PAGE_SIZE) + vaddr);
+ 
+ 		__set_pages_state(desc, vaddr, next_vaddr, op);
+ 
+ 		vaddr = next_vaddr;
+ 	}
+ 
+ 	kfree(desc);
+ }
+ 
+ void snp_set_memory_shared(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	pvalidate_pages(vaddr, npages, false);
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_SHARED);
+ }
+ 
+ void snp_set_memory_private(unsigned long vaddr, unsigned int npages)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	set_pages_state(vaddr, npages, SNP_PAGE_STATE_PRIVATE);
+ 
+ 	pvalidate_pages(vaddr, npages, true);
+ }
+ 
+ static int snp_set_vmsa(void *va, bool vmsa)
+ {
+ 	u64 attrs;
+ 
+ 	/*
+ 	 * Running at VMPL0 allows the kernel to change the VMSA bit for a page
+ 	 * using the RMPADJUST instruction. However, for the instruction to
+ 	 * succeed it must target the permissions of a lesser privileged
+ 	 * (higher numbered) VMPL level, so use VMPL1 (refer to the RMPADJUST
+ 	 * instruction in the AMD64 APM Volume 3).
+ 	 */
+ 	attrs = 1;
+ 	if (vmsa)
+ 		attrs |= RMPADJUST_VMSA_PAGE_BIT;
+ 
+ 	return rmpadjust((unsigned long)va, RMP_PG_SIZE_4K, attrs);
+ }
+ 
+ #define __ATTR_BASE		(SVM_SELECTOR_P_MASK | SVM_SELECTOR_S_MASK)
+ #define INIT_CS_ATTRIBS		(__ATTR_BASE | SVM_SELECTOR_READ_MASK | SVM_SELECTOR_CODE_MASK)
+ #define INIT_DS_ATTRIBS		(__ATTR_BASE | SVM_SELECTOR_WRITE_MASK)
+ 
+ #define INIT_LDTR_ATTRIBS	(SVM_SELECTOR_P_MASK | 2)
+ #define INIT_TR_ATTRIBS		(SVM_SELECTOR_P_MASK | 3)
+ 
+ static void *snp_alloc_vmsa_page(void)
+ {
+ 	struct page *p;
+ 
+ 	/*
+ 	 * Allocate VMSA page to work around the SNP erratum where the CPU will
+ 	 * incorrectly signal an RMP violation #PF if a large page (2MB or 1GB)
+ 	 * collides with the RMP entry of VMSA page. The recommended workaround
+ 	 * is to not use a large page.
+ 	 *
+ 	 * Allocate an 8k page which is also 8k-aligned.
+ 	 */
+ 	p = alloc_pages(GFP_KERNEL_ACCOUNT | __GFP_ZERO, 1);
+ 	if (!p)
+ 		return NULL;
+ 
+ 	split_page(p, 1);
+ 
+ 	/* Free the first 4k. This page may be 2M/1G aligned and cannot be used. */
+ 	__free_page(p);
+ 
+ 	return page_address(p + 1);
+ }
+ 
+ static void snp_cleanup_vmsa(struct sev_es_save_area *vmsa)
+ {
+ 	int err;
+ 
+ 	err = snp_set_vmsa(vmsa, false);
+ 	if (err)
+ 		pr_err("clear VMSA page failed (%u), leaking page\n", err);
+ 	else
+ 		free_page((unsigned long)vmsa);
+ }
+ 
+ static int wakeup_cpu_via_vmgexit(int apic_id, unsigned long start_ip)
+ {
+ 	struct sev_es_save_area *cur_vmsa, *vmsa;
+ 	struct ghcb_state state;
+ 	unsigned long flags;
+ 	struct ghcb *ghcb;
+ 	u8 sipi_vector;
+ 	int cpu, ret;
+ 	u64 cr4;
+ 
+ 	/*
+ 	 * The hypervisor SNP feature support check has happened earlier, just check
+ 	 * the AP_CREATION one here.
+ 	 */
+ 	if (!(sev_hv_features & GHCB_HV_FT_SNP_AP_CREATION))
+ 		return -EOPNOTSUPP;
+ 
+ 	/*
+ 	 * Verify the desired start IP against the known trampoline start IP
+ 	 * to catch any future new trampolines that may be introduced that
+ 	 * would require a new protected guest entry point.
+ 	 */
+ 	if (WARN_ONCE(start_ip != real_mode_header->trampoline_start,
+ 		      "Unsupported SNP start_ip: %lx\n", start_ip))
+ 		return -EINVAL;
+ 
+ 	/* Override start_ip with known protected guest start IP */
+ 	start_ip = real_mode_header->sev_es_trampoline_start;
+ 
+ 	/* Find the logical CPU for the APIC ID */
+ 	for_each_present_cpu(cpu) {
+ 		if (arch_match_cpu_phys_id(cpu, apic_id))
+ 			break;
+ 	}
+ 	if (cpu >= nr_cpu_ids)
+ 		return -EINVAL;
+ 
+ 	cur_vmsa = per_cpu(sev_vmsa, cpu);
+ 
+ 	/*
+ 	 * A new VMSA is created each time because there is no guarantee that
+ 	 * the current VMSA is the kernels or that the vCPU is not running. If
+ 	 * an attempt was done to use the current VMSA with a running vCPU, a
+ 	 * #VMEXIT of that vCPU would wipe out all of the settings being done
+ 	 * here.
+ 	 */
+ 	vmsa = (struct sev_es_save_area *)snp_alloc_vmsa_page();
+ 	if (!vmsa)
+ 		return -ENOMEM;
+ 
+ 	/* CR4 should maintain the MCE value */
+ 	cr4 = native_read_cr4() & X86_CR4_MCE;
+ 
+ 	/* Set the CS value based on the start_ip converted to a SIPI vector */
+ 	sipi_vector		= (start_ip >> 12);
+ 	vmsa->cs.base		= sipi_vector << 12;
+ 	vmsa->cs.limit		= AP_INIT_CS_LIMIT;
+ 	vmsa->cs.attrib		= INIT_CS_ATTRIBS;
+ 	vmsa->cs.selector	= sipi_vector << 8;
+ 
+ 	/* Set the RIP value based on start_ip */
+ 	vmsa->rip		= start_ip & 0xfff;
+ 
+ 	/* Set AP INIT defaults as documented in the APM */
+ 	vmsa->ds.limit		= AP_INIT_DS_LIMIT;
+ 	vmsa->ds.attrib		= INIT_DS_ATTRIBS;
+ 	vmsa->es		= vmsa->ds;
+ 	vmsa->fs		= vmsa->ds;
+ 	vmsa->gs		= vmsa->ds;
+ 	vmsa->ss		= vmsa->ds;
+ 
+ 	vmsa->gdtr.limit	= AP_INIT_GDTR_LIMIT;
+ 	vmsa->ldtr.limit	= AP_INIT_LDTR_LIMIT;
+ 	vmsa->ldtr.attrib	= INIT_LDTR_ATTRIBS;
+ 	vmsa->idtr.limit	= AP_INIT_IDTR_LIMIT;
+ 	vmsa->tr.limit		= AP_INIT_TR_LIMIT;
+ 	vmsa->tr.attrib		= INIT_TR_ATTRIBS;
+ 
+ 	vmsa->cr4		= cr4;
+ 	vmsa->cr0		= AP_INIT_CR0_DEFAULT;
+ 	vmsa->dr7		= DR7_RESET_VALUE;
+ 	vmsa->dr6		= AP_INIT_DR6_DEFAULT;
+ 	vmsa->rflags		= AP_INIT_RFLAGS_DEFAULT;
+ 	vmsa->g_pat		= AP_INIT_GPAT_DEFAULT;
+ 	vmsa->xcr0		= AP_INIT_XCR0_DEFAULT;
+ 	vmsa->mxcsr		= AP_INIT_MXCSR_DEFAULT;
+ 	vmsa->x87_ftw		= AP_INIT_X87_FTW_DEFAULT;
+ 	vmsa->x87_fcw		= AP_INIT_X87_FCW_DEFAULT;
+ 
+ 	/* SVME must be set. */
+ 	vmsa->efer		= EFER_SVME;
+ 
+ 	/*
+ 	 * Set the SNP-specific fields for this VMSA:
+ 	 *   VMPL level
+ 	 *   SEV_FEATURES (matches the SEV STATUS MSR right shifted 2 bits)
+ 	 */
+ 	vmsa->vmpl		= 0;
+ 	vmsa->sev_features	= sev_status >> 2;
+ 
+ 	/* Switch the page over to a VMSA page now that it is initialized */
+ 	ret = snp_set_vmsa(vmsa, true);
+ 	if (ret) {
+ 		pr_err("set VMSA page failed (%u)\n", ret);
+ 		free_page((unsigned long)vmsa);
+ 
+ 		return -EINVAL;
+ 	}
+ 
+ 	/* Issue VMGEXIT AP Creation NAE event */
+ 	local_irq_save(flags);
+ 
+ 	ghcb = __sev_get_ghcb(&state);
+ 
+ 	vc_ghcb_invalidate(ghcb);
+ 	ghcb_set_rax(ghcb, vmsa->sev_features);
+ 	ghcb_set_sw_exit_code(ghcb, SVM_VMGEXIT_AP_CREATION);
+ 	ghcb_set_sw_exit_info_1(ghcb, ((u64)apic_id << 32) | SVM_VMGEXIT_AP_CREATE);
+ 	ghcb_set_sw_exit_info_2(ghcb, __pa(vmsa));
+ 
+ 	sev_es_wr_ghcb_msr(__pa(ghcb));
+ 	VMGEXIT();
+ 
+ 	if (!ghcb_sw_exit_info_1_is_valid(ghcb) ||
+ 	    lower_32_bits(ghcb->save.sw_exit_info_1)) {
+ 		pr_err("SNP AP Creation error\n");
+ 		ret = -EINVAL;
+ 	}
+ 
+ 	__sev_put_ghcb(&state);
+ 
+ 	local_irq_restore(flags);
+ 
+ 	/* Perform cleanup if there was an error */
+ 	if (ret) {
+ 		snp_cleanup_vmsa(vmsa);
+ 		vmsa = NULL;
+ 	}
+ 
+ 	/* Free up any previous VMSA page */
+ 	if (cur_vmsa)
+ 		snp_cleanup_vmsa(cur_vmsa);
+ 
+ 	/* Record the current VMSA page */
+ 	per_cpu(sev_vmsa, cpu) = vmsa;
+ 
+ 	return ret;
+ }
+ 
+ void snp_set_wakeup_secondary_cpu(void)
+ {
+ 	if (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))
+ 		return;
+ 
+ 	/*
+ 	 * Always set this override if SNP is enabled. This makes it the
+ 	 * required method to start APs under SNP. If the hypervisor does
+ 	 * not support AP creation, then no APs will be started.
+ 	 */
+ 	apic->wakeup_secondary_cpu = wakeup_cpu_via_vmgexit;
+ }
+ 
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  int sev_es_setup_ap_jump_table(struct real_mode_header *rmh)
  {
  	u16 startup_cs, startup_ip;
diff --cc arch/x86/kernel/smpboot.c
index f17d1ac1bdb2,85d7b1c5eb70..000000000000
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@@ -84,10 -81,8 +84,15 @@@
  #include <asm/cpu_device_id.h>
  #include <asm/spec-ctrl.h>
  #include <asm/hw_irq.h>
++<<<<<<< HEAD
 +
 +#ifdef CONFIG_ACPI_CPPC_LIB
 +#include <acpi/cppc_acpi.h>
 +#endif
++=======
+ #include <asm/stackprotector.h>
+ #include <asm/sev.h>
++>>>>>>> 0afb6b660a6b (x86/sev: Use SEV-SNP AP creation to start secondary CPUs)
  
  /* representing HT siblings of each logical CPU */
  DEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_sibling_map);
@@@ -1456,9 -1431,11 +1461,11 @@@ void __init native_smp_prepare_cpus(uns
  	smp_quirk_init_udelay();
  
  	speculative_store_bypass_ht_init();
+ 
+ 	snp_set_wakeup_secondary_cpu();
  }
  
 -void arch_thaw_secondary_cpus_begin(void)
 +void arch_enable_nonboot_cpus_begin(void)
  {
  	set_mtrr_aps_delayed_init();
  }
* Unmerged path arch/x86/include/asm/sev-common.h
* Unmerged path arch/x86/include/asm/sev.h
* Unmerged path arch/x86/include/uapi/asm/svm.h
* Unmerged path arch/x86/kernel/sev.c
* Unmerged path arch/x86/kernel/smpboot.c
