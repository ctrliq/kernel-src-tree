powerpc/bpf: Use _Rn macros for GPRs

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
commit 036d559c0bdea75bf4840ba6780790d08572480c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/036d559c.failed

Use _Rn macros to specify register names to make their usage clear.

	Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/7df626b8cdc6141d4295ac16137c82ad570b6637.1644834730.git.naveen.n.rao@linux.vnet.ibm.com

(cherry picked from commit 036d559c0bdea75bf4840ba6780790d08572480c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/net/bpf_jit_comp32.c
#	arch/powerpc/net/bpf_jit_comp64.c
diff --cc arch/powerpc/net/bpf_jit_comp64.c
index 3361fea2cdf5,ac06efa70223..000000000000
--- a/arch/powerpc/net/bpf_jit_comp64.c
+++ b/arch/powerpc/net/bpf_jit_comp64.c
@@@ -20,7 -17,59 +20,63 @@@
  #include <linux/bpf.h>
  #include <asm/security_features.h>
  
++<<<<<<< HEAD
 +#include "bpf_jit64.h"
++=======
+ #include "bpf_jit.h"
+ 
+ /*
+  * Stack layout:
+  * Ensure the top half (upto local_tmp_var) stays consistent
+  * with our redzone usage.
+  *
+  *		[	prev sp		] <-------------
+  *		[   nv gpr save area	] 5*8		|
+  *		[    tail_call_cnt	] 8		|
+  *		[    local_tmp_var	] 16		|
+  * fp (r31) -->	[   ebpf stack space	] upto 512	|
+  *		[     frame header	] 32/112	|
+  * sp (r1) --->	[    stack pointer	] --------------
+  */
+ 
+ /* for gpr non volatile registers BPG_REG_6 to 10 */
+ #define BPF_PPC_STACK_SAVE	(5*8)
+ /* for bpf JIT code internal usage */
+ #define BPF_PPC_STACK_LOCALS	24
+ /* stack frame excluding BPF stack, ensure this is quadword aligned */
+ #define BPF_PPC_STACKFRAME	(STACK_FRAME_MIN_SIZE + \
+ 				 BPF_PPC_STACK_LOCALS + BPF_PPC_STACK_SAVE)
+ 
+ /* BPF register usage */
+ #define TMP_REG_1	(MAX_BPF_JIT_REG + 0)
+ #define TMP_REG_2	(MAX_BPF_JIT_REG + 1)
+ 
+ /* BPF to ppc register mappings */
+ const int b2p[MAX_BPF_JIT_REG + 2] = {
+ 	/* function return value */
+ 	[BPF_REG_0] = _R8,
+ 	/* function arguments */
+ 	[BPF_REG_1] = _R3,
+ 	[BPF_REG_2] = _R4,
+ 	[BPF_REG_3] = _R5,
+ 	[BPF_REG_4] = _R6,
+ 	[BPF_REG_5] = _R7,
+ 	/* non volatile registers */
+ 	[BPF_REG_6] = _R27,
+ 	[BPF_REG_7] = _R28,
+ 	[BPF_REG_8] = _R29,
+ 	[BPF_REG_9] = _R30,
+ 	/* frame pointer aka BPF_REG_10 */
+ 	[BPF_REG_FP] = _R31,
+ 	/* eBPF jit internal registers */
+ 	[BPF_REG_AX] = _R12,
+ 	[TMP_REG_1] = _R9,
+ 	[TMP_REG_2] = _R10
+ };
+ 
+ /* PPC NVR range -- update this if we ever use NVRs below r27 */
+ #define BPF_PPC_NVR_MIN		_R27
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  
  static inline bool bpf_has_stack_frame(struct codegen_context *ctx)
  {
@@@ -87,7 -136,7 +143,11 @@@ void bpf_jit_build_prologue(u32 *image
  	if (ctx->seen & SEEN_TAILCALL) {
  		EMIT(PPC_RAW_LI(b2p[TMP_REG_1], 0));
  		/* this goes in the redzone */
++<<<<<<< HEAD
 +		PPC_BPF_STL(b2p[TMP_REG_1], 1, -(BPF_PPC_STACK_SAVE + 8));
++=======
+ 		EMIT(PPC_RAW_STD(b2p[TMP_REG_1], _R1, -(BPF_PPC_STACK_SAVE + 8)));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  	} else {
  		EMIT(PPC_RAW_NOP());
  		EMIT(PPC_RAW_NOP());
@@@ -100,10 -149,10 +160,17 @@@
  		 */
  		if (ctx->seen & SEEN_FUNC) {
  			EMIT(PPC_RAW_MFLR(_R0));
++<<<<<<< HEAD
 +			PPC_BPF_STL(0, 1, PPC_LR_STKOFF);
 +		}
 +
 +		PPC_BPF_STLU(1, 1, -(BPF_PPC_STACKFRAME + ctx->stack_size));
++=======
+ 			EMIT(PPC_RAW_STD(_R0, _R1, PPC_LR_STKOFF));
+ 		}
+ 
+ 		EMIT(PPC_RAW_STDU(_R1, _R1, -(BPF_PPC_STACKFRAME + ctx->stack_size)));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  	}
  
  	/*
@@@ -113,7 -162,7 +180,11 @@@
  	 */
  	for (i = BPF_REG_6; i <= BPF_REG_10; i++)
  		if (bpf_is_seen_register(ctx, b2p[i]))
++<<<<<<< HEAD
 +			PPC_BPF_STL(b2p[i], 1, bpf_jit_stack_offsetof(ctx, b2p[i]));
++=======
+ 			EMIT(PPC_RAW_STD(b2p[i], _R1, bpf_jit_stack_offsetof(ctx, b2p[i])));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  
  	/* Setup frame pointer to point to the bpf stack area */
  	if (bpf_is_seen_register(ctx, b2p[BPF_REG_FP]))
@@@ -128,14 -177,14 +199,23 @@@ static void bpf_jit_emit_common_epilogu
  	/* Restore NVRs */
  	for (i = BPF_REG_6; i <= BPF_REG_10; i++)
  		if (bpf_is_seen_register(ctx, b2p[i]))
++<<<<<<< HEAD
 +			PPC_BPF_LL(b2p[i], 1, bpf_jit_stack_offsetof(ctx, b2p[i]));
++=======
+ 			EMIT(PPC_RAW_LD(b2p[i], _R1, bpf_jit_stack_offsetof(ctx, b2p[i])));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  
  	/* Tear down our stack frame */
  	if (bpf_has_stack_frame(ctx)) {
- 		EMIT(PPC_RAW_ADDI(1, 1, BPF_PPC_STACKFRAME + ctx->stack_size));
+ 		EMIT(PPC_RAW_ADDI(_R1, _R1, BPF_PPC_STACKFRAME + ctx->stack_size));
  		if (ctx->seen & SEEN_FUNC) {
++<<<<<<< HEAD
 +			PPC_BPF_LL(0, 1, PPC_LR_STKOFF);
 +			EMIT(PPC_RAW_MTLR(0));
++=======
+ 			EMIT(PPC_RAW_LD(_R0, _R1, PPC_LR_STKOFF));
+ 			EMIT(PPC_RAW_MTLR(_R0));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  		}
  	}
  }
@@@ -179,8 -225,14 +259,8 @@@ void bpf_jit_emit_func_call_rel(u32 *im
  {
  	unsigned int i, ctx_idx = ctx->idx;
  
 -	if (WARN_ON_ONCE(func && is_module_text_address(func)))
 -		return -EINVAL;
 -
 -	/* skip past descriptor if elf v1 */
 -	func += FUNCTION_DESCR_SIZE;
 -
  	/* Load function address into r12 */
- 	PPC_LI64(12, func);
+ 	PPC_LI64(_R12, func);
  
  	/* For bpf-to-bpf function calls, the callee's address is unknown
  	 * until the last extra pass. As seen above, we use PPC_LI64() to
@@@ -195,23 -247,13 +275,27 @@@
  	for (i = ctx->idx - ctx_idx; i < 5; i++)
  		EMIT(PPC_RAW_NOP());
  
++<<<<<<< HEAD
 +#ifdef PPC64_ELF_ABI_v1
 +	/*
 +	 * Load TOC from function descriptor at offset 8.
 +	 * We can clobber r2 since we get called through a
 +	 * function pointer (so caller will save/restore r2)
 +	 * and since we don't use a TOC ourself.
 +	 */
 +	PPC_BPF_LL(2, 12, 8);
 +	/* Load actual entry point from function descriptor */
 +	PPC_BPF_LL(12, 12, 0);
 +#endif
 +
 +	EMIT(PPC_RAW_MTCTR(12));
++=======
+ 	EMIT(PPC_RAW_MTCTR(_R12));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  	EMIT(PPC_RAW_BCTRL());
 -
 -	return 0;
  }
  
 -static int bpf_jit_emit_tail_call(u32 *image, struct codegen_context *ctx, u32 out)
 +static void bpf_jit_emit_tail_call(u32 *image, struct codegen_context *ctx, u32 out)
  {
  	/*
  	 * By now, the eBPF program has already setup parameters in r3, r4 and r5
@@@ -233,21 -275,21 +317,29 @@@
  	EMIT(PPC_RAW_LWZ(b2p[TMP_REG_1], b2p_bpf_array, offsetof(struct bpf_array, map.max_entries)));
  	EMIT(PPC_RAW_RLWINM(b2p_index, b2p_index, 0, 0, 31));
  	EMIT(PPC_RAW_CMPLW(b2p_index, b2p[TMP_REG_1]));
 -	PPC_BCC_SHORT(COND_GE, out);
 +	PPC_BCC(COND_GE, out);
  
  	/*
 -	 * if (tail_call_cnt >= MAX_TAIL_CALL_CNT)
 +	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)
  	 *   goto out;
  	 */
++<<<<<<< HEAD
 +	PPC_BPF_LL(b2p[TMP_REG_1], 1, bpf_jit_stack_tailcallcnt(ctx));
++=======
+ 	EMIT(PPC_RAW_LD(b2p[TMP_REG_1], _R1, bpf_jit_stack_tailcallcnt(ctx)));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  	EMIT(PPC_RAW_CMPLWI(b2p[TMP_REG_1], MAX_TAIL_CALL_CNT));
 -	PPC_BCC_SHORT(COND_GE, out);
 +	PPC_BCC(COND_GT, out);
  
  	/*
  	 * tail_call_cnt++;
  	 */
  	EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], b2p[TMP_REG_1], 1));
++<<<<<<< HEAD
 +	PPC_BPF_STL(b2p[TMP_REG_1], 1, bpf_jit_stack_tailcallcnt(ctx));
++=======
+ 	EMIT(PPC_RAW_STD(b2p[TMP_REG_1], _R1, bpf_jit_stack_tailcallcnt(ctx)));
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  
  	/* prog = array->ptrs[index]; */
  	EMIT(PPC_RAW_MULI(b2p[TMP_REG_1], b2p_index, 8));
@@@ -634,17 -679,21 +726,35 @@@ bpf_alu32_trunc
  				EMIT(PPC_RAW_MR(dst_reg, b2p[TMP_REG_1]));
  				break;
  			case 64:
++<<<<<<< HEAD
 +				/*
 +				 * Way easier and faster(?) to store the value
 +				 * into stack and then use ldbrx
 +				 *
 +				 * ctx->seen will be reliable in pass2, but
 +				 * the instructions generated will remain the
 +				 * same across all passes
 +				 */
 +				PPC_BPF_STL(dst_reg, 1, bpf_jit_stack_local(ctx));
 +				EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], 1, bpf_jit_stack_local(ctx)));
 +				EMIT(PPC_RAW_LDBRX(dst_reg, 0, b2p[TMP_REG_1]));
++=======
+ 				/* Store the value to stack and then use byte-reverse loads */
+ 				EMIT(PPC_RAW_STD(dst_reg, _R1, bpf_jit_stack_local(ctx)));
+ 				EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], _R1, bpf_jit_stack_local(ctx)));
+ 				if (cpu_has_feature(CPU_FTR_ARCH_206)) {
+ 					EMIT(PPC_RAW_LDBRX(dst_reg, 0, b2p[TMP_REG_1]));
+ 				} else {
+ 					EMIT(PPC_RAW_LWBRX(dst_reg, 0, b2p[TMP_REG_1]));
+ 					if (IS_ENABLED(CONFIG_CPU_LITTLE_ENDIAN))
+ 						EMIT(PPC_RAW_SLDI(dst_reg, dst_reg, 32));
+ 					EMIT(PPC_RAW_LI(b2p[TMP_REG_2], 4));
+ 					EMIT(PPC_RAW_LWBRX(b2p[TMP_REG_2], b2p[TMP_REG_2], b2p[TMP_REG_1]));
+ 					if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+ 						EMIT(PPC_RAW_SLDI(b2p[TMP_REG_2], b2p[TMP_REG_2], 32));
+ 					EMIT(PPC_RAW_OR(dst_reg, dst_reg, b2p[TMP_REG_2]));
+ 				}
++>>>>>>> 036d559c0bde (powerpc/bpf: Use _Rn macros for GPRs)
  				break;
  			}
  			break;
@@@ -842,11 -944,15 +952,11 @@@ emit_clear
  				return ret;
  
  			if (func_addr_fixed)
 -				ret = bpf_jit_emit_func_call_hlp(image, ctx, func_addr);
 +				bpf_jit_emit_func_call_hlp(image, ctx, func_addr);
  			else
 -				ret = bpf_jit_emit_func_call_rel(image, ctx, func_addr);
 -
 -			if (ret)
 -				return ret;
 -
 +				bpf_jit_emit_func_call_rel(image, ctx, func_addr);
  			/* move return value from r3 to BPF_REG_0 */
- 			EMIT(PPC_RAW_MR(b2p[BPF_REG_0], 3));
+ 			EMIT(PPC_RAW_MR(b2p[BPF_REG_0], _R3));
  			break;
  
  		/*
* Unmerged path arch/powerpc/net/bpf_jit_comp32.c
* Unmerged path arch/powerpc/net/bpf_jit_comp32.c
* Unmerged path arch/powerpc/net/bpf_jit_comp64.c
