powerpc64/bpf: Convert some of the uses of PPC_BPF_[LL|STL] to PPC_BPF_[LD|STD]

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
Rebuild_CHGLOG: - powerpc64/bpf: Convert some of the uses of PPC_BPF_[LL|STL] to PPC_BPF_[LD|STD] (Mamatha Inamdar) [2113254]
Rebuild_FUZZ: 93.96%
commit-author Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
commit 391c271f4deb7356482d12f962a6fc018b6a3fb0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/391c271f.failed

PPC_BPF_[LL|STL] are macros meant for scenarios where we may have to
deal with a non-word aligned offset. Limit their usage to only those
scenarios by converting the rest to just use PPC_BPF_[LD|STD].

	Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
	Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
Link: https://lore.kernel.org/r/0eb472428165a307f6fdaf22b0c33cbf13a9a635.1644834730.git.naveen.n.rao@linux.vnet.ibm.com

(cherry picked from commit 391c271f4deb7356482d12f962a6fc018b6a3fb0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/net/bpf_jit_comp64.c
diff --cc arch/powerpc/net/bpf_jit_comp64.c
index 3361fea2cdf5,411ac41dba42..000000000000
--- a/arch/powerpc/net/bpf_jit_comp64.c
+++ b/arch/powerpc/net/bpf_jit_comp64.c
@@@ -233,15 -223,15 +233,15 @@@ static void bpf_jit_emit_tail_call(u32 
  	EMIT(PPC_RAW_LWZ(b2p[TMP_REG_1], b2p_bpf_array, offsetof(struct bpf_array, map.max_entries)));
  	EMIT(PPC_RAW_RLWINM(b2p_index, b2p_index, 0, 0, 31));
  	EMIT(PPC_RAW_CMPLW(b2p_index, b2p[TMP_REG_1]));
 -	PPC_BCC_SHORT(COND_GE, out);
 +	PPC_BCC(COND_GE, out);
  
  	/*
 -	 * if (tail_call_cnt >= MAX_TAIL_CALL_CNT)
 +	 * if (tail_call_cnt > MAX_TAIL_CALL_CNT)
  	 *   goto out;
  	 */
- 	PPC_BPF_LL(b2p[TMP_REG_1], 1, bpf_jit_stack_tailcallcnt(ctx));
+ 	EMIT(PPC_RAW_LD(b2p[TMP_REG_1], 1, bpf_jit_stack_tailcallcnt(ctx)));
  	EMIT(PPC_RAW_CMPLWI(b2p[TMP_REG_1], MAX_TAIL_CALL_CNT));
 -	PPC_BCC_SHORT(COND_GE, out);
 +	PPC_BCC(COND_GT, out);
  
  	/*
  	 * tail_call_cnt++;
@@@ -259,10 -249,10 +259,10 @@@
  	 *   goto out;
  	 */
  	EMIT(PPC_RAW_CMPLDI(b2p[TMP_REG_1], 0));
 -	PPC_BCC_SHORT(COND_EQ, out);
 +	PPC_BCC(COND_EQ, out);
  
  	/* goto *(prog->bpf_func + prologue_size); */
- 	PPC_BPF_LL(b2p[TMP_REG_1], b2p[TMP_REG_1], offsetof(struct bpf_prog, bpf_func));
+ 	EMIT(PPC_RAW_LD(b2p[TMP_REG_1], b2p[TMP_REG_1], offsetof(struct bpf_prog, bpf_func)));
  	EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], b2p[TMP_REG_1],
  			FUNCTION_DESCR_SIZE + bpf_tailcall_prologue_size));
  	EMIT(PPC_RAW_MTCTR(b2p[TMP_REG_1]));
@@@ -634,17 -627,21 +634,22 @@@ bpf_alu32_trunc
  				EMIT(PPC_RAW_MR(dst_reg, b2p[TMP_REG_1]));
  				break;
  			case 64:
++<<<<<<< HEAD
 +				/*
 +				 * Way easier and faster(?) to store the value
 +				 * into stack and then use ldbrx
 +				 *
 +				 * ctx->seen will be reliable in pass2, but
 +				 * the instructions generated will remain the
 +				 * same across all passes
 +				 */
 +				PPC_BPF_STL(dst_reg, 1, bpf_jit_stack_local(ctx));
++=======
+ 				/* Store the value to stack and then use byte-reverse loads */
+ 				EMIT(PPC_RAW_STD(dst_reg, 1, bpf_jit_stack_local(ctx)));
++>>>>>>> 391c271f4deb (powerpc64/bpf: Convert some of the uses of PPC_BPF_[LL|STL] to PPC_BPF_[LD|STD])
  				EMIT(PPC_RAW_ADDI(b2p[TMP_REG_1], 1, bpf_jit_stack_local(ctx)));
 -				if (cpu_has_feature(CPU_FTR_ARCH_206)) {
 -					EMIT(PPC_RAW_LDBRX(dst_reg, 0, b2p[TMP_REG_1]));
 -				} else {
 -					EMIT(PPC_RAW_LWBRX(dst_reg, 0, b2p[TMP_REG_1]));
 -					if (IS_ENABLED(CONFIG_CPU_LITTLE_ENDIAN))
 -						EMIT(PPC_RAW_SLDI(dst_reg, dst_reg, 32));
 -					EMIT(PPC_RAW_LI(b2p[TMP_REG_2], 4));
 -					EMIT(PPC_RAW_LWBRX(b2p[TMP_REG_2], b2p[TMP_REG_2], b2p[TMP_REG_1]));
 -					if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
 -						EMIT(PPC_RAW_SLDI(b2p[TMP_REG_2], b2p[TMP_REG_2], 32));
 -					EMIT(PPC_RAW_OR(dst_reg, dst_reg, b2p[TMP_REG_2]));
 -				}
 +				EMIT(PPC_RAW_LDBRX(dst_reg, 0, b2p[TMP_REG_1]));
  				break;
  			}
  			break;
* Unmerged path arch/powerpc/net/bpf_jit_comp64.c
