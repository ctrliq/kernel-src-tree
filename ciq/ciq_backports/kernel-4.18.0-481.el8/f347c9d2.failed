filemap: make the accounting of thrashing more consistent

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-481.el8
commit-author Yang Yang <yang.yang29@zte.com.cn>
commit f347c9d2697fcbbb64e077f7113a3887a181b8c0
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-481.el8/f347c9d2.failed

Once upon a time, we only support accounting thrashing of page cache. 
Then Joonsoo introduced workingset detection for anonymous pages and we
gained the ability to account thrashing of them[1].

So let delayacct account both the thrashing of page cache and anonymous
pages, this could make the codes more consistent and simpler.

[1] commit aae466b0052e ("mm/swap: implement workingset detection for anonymous LRU")

Link: https://lkml.kernel.org/r/20220805033838.1714674-1-yang.yang29@zte.com.cn
	Signed-off-by: Yang Yang <yang.yang29@zte.com.cn>
	Signed-off-by: CGEL ZTE <cgel.zte@gmail.com>
	Acked-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Cc: Balbir Singh <bsingharora@gmail.com>
	Cc: Jonathan Corbet <corbet@lwn.net>
	Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
	Cc: Yang Yang <yang.yang29@zte.com.cn>
	Cc: David Hildenbrand <david@redhat.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit f347c9d2697fcbbb64e077f7113a3887a181b8c0)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	Documentation/accounting/delay-accounting.txt
#	mm/filemap.c
diff --cc Documentation/accounting/delay-accounting.txt
index 042ea59b5853,7103b62ba6d7..000000000000
--- a/Documentation/accounting/delay-accounting.txt
+++ b/Documentation/accounting/delay-accounting.txt
@@@ -12,6 -13,9 +12,12 @@@ a) waiting for a CPU (while being runna
  b) completion of synchronous block I/O initiated by the task
  c) swapping in pages
  d) memory reclaim
++<<<<<<< HEAD:Documentation/accounting/delay-accounting.txt
++=======
+ e) thrashing
+ f) direct compact
+ g) write-protect copy
++>>>>>>> f347c9d2697f (filemap: make the accounting of thrashing more consistent):Documentation/accounting/delay-accounting.rst
  
  and makes these statistics available to userspace through
  the taskstats interface.
diff --cc mm/filemap.c
index 87e41a01a79e,5570d083ec0f..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -1142,20 -1193,39 +1142,25 @@@ enum behavior 
  			 */
  };
  
 -/*
 - * Attempt to check (or get) the folio flag, and mark us done
 - * if successful.
 - */
 -static inline bool folio_trylock_flag(struct folio *folio, int bit_nr,
 -					struct wait_queue_entry *wait)
 -{
 -	if (wait->flags & WQ_FLAG_EXCLUSIVE) {
 -		if (test_and_set_bit(bit_nr, &folio->flags))
 -			return false;
 -	} else if (test_bit(bit_nr, &folio->flags))
 -		return false;
 -
 -	wait->flags |= WQ_FLAG_WOKEN | WQ_FLAG_DONE;
 -	return true;
 -}
 -
 -/* How many times do we accept lock stealing from under a waiter? */
 -int sysctl_page_lock_unfairness = 5;
 -
 -static inline int folio_wait_bit_common(struct folio *folio, int bit_nr,
 -		int state, enum behavior behavior)
 +static inline int wait_on_page_bit_common(wait_queue_head_t *q,
 +	struct page *page, int bit_nr, int state, enum behavior behavior)
  {
 -	wait_queue_head_t *q = folio_waitqueue(folio);
 -	int unfairness = sysctl_page_lock_unfairness;
  	struct wait_page_queue wait_page;
  	wait_queue_entry_t *wait = &wait_page.wait;
 +	bool bit_is_set;
  	bool thrashing = false;
  	unsigned long pflags;
 +	int ret = 0;
  
  	if (bit_nr == PG_locked &&
++<<<<<<< HEAD
 +	    !PageUptodate(page) && PageWorkingset(page)) {
 +		if (!PageSwapBacked(page))
 +			delayacct_thrashing_start();
++=======
+ 	    !folio_test_uptodate(folio) && folio_test_workingset(folio)) {
+ 		delayacct_thrashing_start();
++>>>>>>> f347c9d2697f (filemap: make the accounting of thrashing more consistent)
  		psi_memstall_enter(&pflags);
  		thrashing = true;
  	}
@@@ -1176,90 -1285,181 +1181,217 @@@
  
  		set_current_state(state);
  
 -		/* Loop until we've been woken or interrupted */
 -		flags = smp_load_acquire(&wait->flags);
 -		if (!(flags & WQ_FLAG_WOKEN)) {
 -			if (signal_pending_state(state, current))
 -				break;
 +		spin_unlock_irq(&q->lock);
  
 +		bit_is_set = test_bit(bit_nr, &page->flags);
 +		if (behavior == DROP)
 +			put_page(page);
 +
 +		if (likely(bit_is_set))
  			io_schedule();
 -			continue;
 -		}
  
++<<<<<<< HEAD
 +		if (behavior == EXCLUSIVE) {
 +			if (!test_and_set_bit_lock(bit_nr, &page->flags))
 +				break;
 +		} else if (behavior == SHARED) {
 +			if (!test_bit(bit_nr, &page->flags))
++=======
+ 		/* If we were non-exclusive, we're done */
+ 		if (behavior != EXCLUSIVE)
+ 			break;
+ 
+ 		/* If the waker got the lock for us, we're done */
+ 		if (flags & WQ_FLAG_DONE)
+ 			break;
+ 
+ 		/*
+ 		 * Otherwise, if we're getting the lock, we need to
+ 		 * try to get it ourselves.
+ 		 *
+ 		 * And if that fails, we'll have to retry this all.
+ 		 */
+ 		if (unlikely(test_and_set_bit(bit_nr, folio_flags(folio, 0))))
+ 			goto repeat;
+ 
+ 		wait->flags |= WQ_FLAG_DONE;
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * If a signal happened, this 'finish_wait()' may remove the last
+ 	 * waiter from the wait-queues, but the folio waiters bit will remain
+ 	 * set. That's ok. The next wakeup will take care of it, and trying
+ 	 * to do it here would be difficult and prone to races.
+ 	 */
+ 	finish_wait(q, wait);
+ 
+ 	if (thrashing) {
+ 		delayacct_thrashing_end();
+ 		psi_memstall_leave(&pflags);
+ 	}
+ 
+ 	/*
+ 	 * NOTE! The wait->flags weren't stable until we've done the
+ 	 * 'finish_wait()', and we could have exited the loop above due
+ 	 * to a signal, and had a wakeup event happen after the signal
+ 	 * test but before the 'finish_wait()'.
+ 	 *
+ 	 * So only after the finish_wait() can we reliably determine
+ 	 * if we got woken up or not, so we can now figure out the final
+ 	 * return value based on that state without races.
+ 	 *
+ 	 * Also note that WQ_FLAG_WOKEN is sufficient for a non-exclusive
+ 	 * waiter, but an exclusive one requires WQ_FLAG_DONE.
+ 	 */
+ 	if (behavior == EXCLUSIVE)
+ 		return wait->flags & WQ_FLAG_DONE ? 0 : -EINTR;
+ 
+ 	return wait->flags & WQ_FLAG_WOKEN ? 0 : -EINTR;
+ }
+ 
+ #ifdef CONFIG_MIGRATION
+ /**
+  * migration_entry_wait_on_locked - Wait for a migration entry to be removed
+  * @entry: migration swap entry.
+  * @ptep: mapped pte pointer. Will return with the ptep unmapped. Only required
+  *        for pte entries, pass NULL for pmd entries.
+  * @ptl: already locked ptl. This function will drop the lock.
+  *
+  * Wait for a migration entry referencing the given page to be removed. This is
+  * equivalent to put_and_wait_on_page_locked(page, TASK_UNINTERRUPTIBLE) except
+  * this can be called without taking a reference on the page. Instead this
+  * should be called while holding the ptl for the migration entry referencing
+  * the page.
+  *
+  * Returns after unmapping and unlocking the pte/ptl with pte_unmap_unlock().
+  *
+  * This follows the same logic as folio_wait_bit_common() so see the comments
+  * there.
+  */
+ void migration_entry_wait_on_locked(swp_entry_t entry, pte_t *ptep,
+ 				spinlock_t *ptl)
+ {
+ 	struct wait_page_queue wait_page;
+ 	wait_queue_entry_t *wait = &wait_page.wait;
+ 	bool thrashing = false;
+ 	unsigned long pflags;
+ 	wait_queue_head_t *q;
+ 	struct folio *folio = page_folio(pfn_swap_entry_to_page(entry));
+ 
+ 	q = folio_waitqueue(folio);
+ 	if (!folio_test_uptodate(folio) && folio_test_workingset(folio)) {
+ 		delayacct_thrashing_start();
+ 		psi_memstall_enter(&pflags);
+ 		thrashing = true;
+ 	}
+ 
+ 	init_wait(wait);
+ 	wait->func = wake_page_function;
+ 	wait_page.folio = folio;
+ 	wait_page.bit_nr = PG_locked;
+ 	wait->flags = 0;
+ 
+ 	spin_lock_irq(&q->lock);
+ 	folio_set_waiters(folio);
+ 	if (!folio_trylock_flag(folio, PG_locked, wait))
+ 		__add_wait_queue_entry_tail(q, wait);
+ 	spin_unlock_irq(&q->lock);
+ 
+ 	/*
+ 	 * If a migration entry exists for the page the migration path must hold
+ 	 * a valid reference to the page, and it must take the ptl to remove the
+ 	 * migration entry. So the page is valid until the ptl is dropped.
+ 	 */
+ 	if (ptep)
+ 		pte_unmap_unlock(ptep, ptl);
+ 	else
+ 		spin_unlock(ptl);
+ 
+ 	for (;;) {
+ 		unsigned int flags;
+ 
+ 		set_current_state(TASK_UNINTERRUPTIBLE);
+ 
+ 		/* Loop until we've been woken or interrupted */
+ 		flags = smp_load_acquire(&wait->flags);
+ 		if (!(flags & WQ_FLAG_WOKEN)) {
+ 			if (signal_pending_state(TASK_UNINTERRUPTIBLE, current))
++>>>>>>> f347c9d2697f (filemap: make the accounting of thrashing more consistent)
  				break;
 +		}
  
 -			io_schedule();
 -			continue;
 +		if (signal_pending_state(state, current)) {
 +			ret = -EINTR;
 +			break;
 +		}
 +
 +		if (behavior == DROP) {
 +			/*
 +			 * We can no longer safely access page->flags:
 +			 * even if CONFIG_MEMORY_HOTREMOVE is not enabled,
 +			 * there is a risk of waiting forever on a page reused
 +			 * for something that keeps it locked indefinitely.
 +			 * But best check for -EINTR above before breaking.
 +			 */
 +			break;
  		}
 -		break;
  	}
  
  	finish_wait(q, wait);
  
  	if (thrashing) {
++<<<<<<< HEAD
 +		if (!PageSwapBacked(page))
 +			delayacct_thrashing_end();
++=======
+ 		delayacct_thrashing_end();
++>>>>>>> f347c9d2697f (filemap: make the accounting of thrashing more consistent)
  		psi_memstall_leave(&pflags);
  	}
 +
 +	/*
 +	 * A signal could leave PageWaiters set. Clearing it here if
 +	 * !waitqueue_active would be possible (by open-coding finish_wait),
 +	 * but still fail to catch it in the case of wait hash collision. We
 +	 * already can fail to clear wait hash collision cases, so don't
 +	 * bother with signals either.
 +	 */
 +
 +	return ret;
  }
 -#endif
  
 -void folio_wait_bit(struct folio *folio, int bit_nr)
 +void wait_on_page_bit(struct page *page, int bit_nr)
  {
 -	folio_wait_bit_common(folio, bit_nr, TASK_UNINTERRUPTIBLE, SHARED);
 +	wait_queue_head_t *q = page_waitqueue(page);
 +	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, SHARED);
  }
 -EXPORT_SYMBOL(folio_wait_bit);
 +EXPORT_SYMBOL(wait_on_page_bit);
  
 -int folio_wait_bit_killable(struct folio *folio, int bit_nr)
 +int wait_on_page_bit_killable(struct page *page, int bit_nr)
  {
 -	return folio_wait_bit_common(folio, bit_nr, TASK_KILLABLE, SHARED);
 +	wait_queue_head_t *q = page_waitqueue(page);
 +	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, SHARED);
  }
 -EXPORT_SYMBOL(folio_wait_bit_killable);
 +EXPORT_SYMBOL(wait_on_page_bit_killable);
  
  /**
 - * folio_put_wait_locked - Drop a reference and wait for it to be unlocked
 - * @folio: The folio to wait for.
 - * @state: The sleep state (TASK_KILLABLE, TASK_UNINTERRUPTIBLE, etc).
 + * put_and_wait_on_page_locked - Drop a reference and wait for it to be unlocked
 + * @page: The page to wait for.
   *
 - * The caller should hold a reference on @folio.  They expect the page to
 + * The caller should hold a reference on @page.  They expect the page to
   * become unlocked relatively soon, but do not wish to hold up migration
 - * (for example) by holding the reference while waiting for the folio to
 + * (for example) by holding the reference while waiting for the page to
   * come unlocked.  After this function returns, the caller should not
 - * dereference @folio.
 - *
 - * Return: 0 if the folio was unlocked or -EINTR if interrupted by a signal.
 + * dereference @page.
   */
 -int folio_put_wait_locked(struct folio *folio, int state)
 +void put_and_wait_on_page_locked(struct page *page)
  {
 -	return folio_wait_bit_common(folio, PG_locked, state, DROP);
 +	wait_queue_head_t *q;
 +
 +	page = compound_head(page);
 +	q = page_waitqueue(page);
 +	wait_on_page_bit_common(q, page, PG_locked, TASK_UNINTERRUPTIBLE, DROP);
  }
  
  /**
* Unmerged path Documentation/accounting/delay-accounting.txt
* Unmerged path mm/filemap.c
