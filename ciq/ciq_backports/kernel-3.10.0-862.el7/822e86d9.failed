net_sched: remove tcf_block_put_deferred()

jira LE-1907
Rebuild_History Non-Buildable kernel-3.10.0-862.el7
commit-author Cong Wang <xiyou.wangcong@gmail.com>
commit 822e86d997e4d8f942818ea6ac1711f59a66ebef
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-862.el7/822e86d9.failed

In commit 7aa0045dadb6 ("net_sched: introduce a workqueue for RCU callbacks of tc filter")
I defer tcf_chain_flush() to a workqueue, this causes a use-after-free
because qdisc is already destroyed after we queue this work.

The tcf_block_put_deferred() is no longer necessary after we get RTNL
for each tc filter destroy work, no others could jump in at this point.
Same for tcf_chain_hold(), we are fully serialized now.

This also reduces one indirection therefore makes the code more
readable. Note this brings back a rcu_barrier(), however comparing
to the code prior to commit 7aa0045dadb6 we still reduced one
rcu_barrier(). For net-next, we can consider to refcnt tcf block to
avoid it.

Fixes: 7aa0045dadb6 ("net_sched: introduce a workqueue for RCU callbacks of tc filter")
	Cc: Daniel Borkmann <daniel@iogearbox.net>
	Cc: Jiri Pirko <jiri@resnulli.us>
	Cc: John Fastabend <john.fastabend@gmail.com>
	Cc: Jamal Hadi Salim <jhs@mojatatu.com>
	Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
	Cc: Eric Dumazet <edumazet@google.com>
	Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
	Signed-off-by: David S. Miller <davem@davemloft.net>
(cherry picked from commit 822e86d997e4d8f942818ea6ac1711f59a66ebef)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	net/sched/cls_api.c
diff --cc net/sched/cls_api.c
index 1dc6d123ed94,b2d310745487..000000000000
--- a/net/sched/cls_api.c
+++ b/net/sched/cls_api.c
@@@ -124,7 -118,415 +124,419 @@@ static inline u32 tcf_auto_prio(struct 
  	if (tp)
  		first = tp->prio - 1;
  
++<<<<<<< HEAD
 +	return first;
++=======
+ 	return TC_H_MAJ(first);
+ }
+ 
+ static struct tcf_proto *tcf_proto_create(const char *kind, u32 protocol,
+ 					  u32 prio, u32 parent, struct Qdisc *q,
+ 					  struct tcf_chain *chain)
+ {
+ 	struct tcf_proto *tp;
+ 	int err;
+ 
+ 	tp = kzalloc(sizeof(*tp), GFP_KERNEL);
+ 	if (!tp)
+ 		return ERR_PTR(-ENOBUFS);
+ 
+ 	err = -ENOENT;
+ 	tp->ops = tcf_proto_lookup_ops(kind);
+ 	if (!tp->ops) {
+ #ifdef CONFIG_MODULES
+ 		rtnl_unlock();
+ 		request_module("cls_%s", kind);
+ 		rtnl_lock();
+ 		tp->ops = tcf_proto_lookup_ops(kind);
+ 		/* We dropped the RTNL semaphore in order to perform
+ 		 * the module load. So, even if we succeeded in loading
+ 		 * the module we have to replay the request. We indicate
+ 		 * this using -EAGAIN.
+ 		 */
+ 		if (tp->ops) {
+ 			module_put(tp->ops->owner);
+ 			err = -EAGAIN;
+ 		} else {
+ 			err = -ENOENT;
+ 		}
+ 		goto errout;
+ #endif
+ 	}
+ 	tp->classify = tp->ops->classify;
+ 	tp->protocol = protocol;
+ 	tp->prio = prio;
+ 	tp->classid = parent;
+ 	tp->q = q;
+ 	tp->chain = chain;
+ 
+ 	err = tp->ops->init(tp);
+ 	if (err) {
+ 		module_put(tp->ops->owner);
+ 		goto errout;
+ 	}
+ 	return tp;
+ 
+ errout:
+ 	kfree(tp);
+ 	return ERR_PTR(err);
+ }
+ 
+ static void tcf_proto_destroy(struct tcf_proto *tp)
+ {
+ 	tp->ops->destroy(tp);
+ 	module_put(tp->ops->owner);
+ 	kfree_rcu(tp, rcu);
+ }
+ 
+ static struct tcf_chain *tcf_chain_create(struct tcf_block *block,
+ 					  u32 chain_index)
+ {
+ 	struct tcf_chain *chain;
+ 
+ 	chain = kzalloc(sizeof(*chain), GFP_KERNEL);
+ 	if (!chain)
+ 		return NULL;
+ 	list_add_tail(&chain->list, &block->chain_list);
+ 	chain->block = block;
+ 	chain->index = chain_index;
+ 	chain->refcnt = 1;
+ 	return chain;
+ }
+ 
+ static void tcf_chain_flush(struct tcf_chain *chain)
+ {
+ 	struct tcf_proto *tp;
+ 
+ 	if (chain->p_filter_chain)
+ 		RCU_INIT_POINTER(*chain->p_filter_chain, NULL);
+ 	while ((tp = rtnl_dereference(chain->filter_chain)) != NULL) {
+ 		RCU_INIT_POINTER(chain->filter_chain, tp->next);
+ 		tcf_chain_put(chain);
+ 		tcf_proto_destroy(tp);
+ 	}
+ }
+ 
+ static void tcf_chain_destroy(struct tcf_chain *chain)
+ {
+ 	list_del(&chain->list);
+ 	kfree(chain);
+ }
+ 
+ static void tcf_chain_hold(struct tcf_chain *chain)
+ {
+ 	++chain->refcnt;
+ }
+ 
+ struct tcf_chain *tcf_chain_get(struct tcf_block *block, u32 chain_index,
+ 				bool create)
+ {
+ 	struct tcf_chain *chain;
+ 
+ 	list_for_each_entry(chain, &block->chain_list, list) {
+ 		if (chain->index == chain_index) {
+ 			tcf_chain_hold(chain);
+ 			return chain;
+ 		}
+ 	}
+ 
+ 	return create ? tcf_chain_create(block, chain_index) : NULL;
+ }
+ EXPORT_SYMBOL(tcf_chain_get);
+ 
+ void tcf_chain_put(struct tcf_chain *chain)
+ {
+ 	if (--chain->refcnt == 0)
+ 		tcf_chain_destroy(chain);
+ }
+ EXPORT_SYMBOL(tcf_chain_put);
+ 
+ static void
+ tcf_chain_filter_chain_ptr_set(struct tcf_chain *chain,
+ 			       struct tcf_proto __rcu **p_filter_chain)
+ {
+ 	chain->p_filter_chain = p_filter_chain;
+ }
+ 
+ int tcf_block_get(struct tcf_block **p_block,
+ 		  struct tcf_proto __rcu **p_filter_chain)
+ {
+ 	struct tcf_block *block = kzalloc(sizeof(*block), GFP_KERNEL);
+ 	struct tcf_chain *chain;
+ 	int err;
+ 
+ 	if (!block)
+ 		return -ENOMEM;
+ 	INIT_LIST_HEAD(&block->chain_list);
+ 	/* Create chain 0 by default, it has to be always present. */
+ 	chain = tcf_chain_create(block, 0);
+ 	if (!chain) {
+ 		err = -ENOMEM;
+ 		goto err_chain_create;
+ 	}
+ 	tcf_chain_filter_chain_ptr_set(chain, p_filter_chain);
+ 	*p_block = block;
+ 	return 0;
+ 
+ err_chain_create:
+ 	kfree(block);
+ 	return err;
+ }
+ EXPORT_SYMBOL(tcf_block_get);
+ 
+ static void tcf_block_put_final(struct work_struct *work)
+ {
+ 	struct tcf_block *block = container_of(work, struct tcf_block, work);
+ 	struct tcf_chain *chain, *tmp;
+ 
+ 	rtnl_lock();
+ 	/* Only chain 0 should be still here. */
+ 	list_for_each_entry_safe(chain, tmp, &block->chain_list, list)
+ 		tcf_chain_put(chain);
+ 	rtnl_unlock();
+ 	kfree(block);
+ }
+ 
+ /* XXX: Standalone actions are not allowed to jump to any chain, and bound
+  * actions should be all removed after flushing. However, filters are now
+  * destroyed in tc filter workqueue with RTNL lock, they can not race here.
+  */
+ void tcf_block_put(struct tcf_block *block)
+ {
+ 	struct tcf_chain *chain, *tmp;
+ 
+ 	if (!block)
+ 		return;
+ 
+ 	list_for_each_entry_safe(chain, tmp, &block->chain_list, list)
+ 		tcf_chain_flush(chain);
+ 
+ 	INIT_WORK(&block->work, tcf_block_put_final);
+ 	/* Wait for RCU callbacks to release the reference count and make
+ 	 * sure their works have been queued before this.
+ 	 */
+ 	rcu_barrier();
+ 	tcf_queue_work(&block->work);
+ }
+ EXPORT_SYMBOL(tcf_block_put);
+ 
+ /* Main classifier routine: scans classifier chain attached
+  * to this qdisc, (optionally) tests for protocol and asks
+  * specific classifiers.
+  */
+ int tcf_classify(struct sk_buff *skb, const struct tcf_proto *tp,
+ 		 struct tcf_result *res, bool compat_mode)
+ {
+ 	__be16 protocol = tc_skb_protocol(skb);
+ #ifdef CONFIG_NET_CLS_ACT
+ 	const int max_reclassify_loop = 4;
+ 	const struct tcf_proto *orig_tp = tp;
+ 	const struct tcf_proto *first_tp;
+ 	int limit = 0;
+ 
+ reclassify:
+ #endif
+ 	for (; tp; tp = rcu_dereference_bh(tp->next)) {
+ 		int err;
+ 
+ 		if (tp->protocol != protocol &&
+ 		    tp->protocol != htons(ETH_P_ALL))
+ 			continue;
+ 
+ 		err = tp->classify(skb, tp, res);
+ #ifdef CONFIG_NET_CLS_ACT
+ 		if (unlikely(err == TC_ACT_RECLASSIFY && !compat_mode)) {
+ 			first_tp = orig_tp;
+ 			goto reset;
+ 		} else if (unlikely(TC_ACT_EXT_CMP(err, TC_ACT_GOTO_CHAIN))) {
+ 			first_tp = res->goto_tp;
+ 			goto reset;
+ 		}
+ #endif
+ 		if (err >= 0)
+ 			return err;
+ 	}
+ 
+ 	return TC_ACT_UNSPEC; /* signal: continue lookup */
+ #ifdef CONFIG_NET_CLS_ACT
+ reset:
+ 	if (unlikely(limit++ >= max_reclassify_loop)) {
+ 		net_notice_ratelimited("%s: reclassify loop, rule prio %u, protocol %02x\n",
+ 				       tp->q->ops->id, tp->prio & 0xffff,
+ 				       ntohs(tp->protocol));
+ 		return TC_ACT_SHOT;
+ 	}
+ 
+ 	tp = first_tp;
+ 	protocol = tc_skb_protocol(skb);
+ 	goto reclassify;
+ #endif
+ }
+ EXPORT_SYMBOL(tcf_classify);
+ 
+ struct tcf_chain_info {
+ 	struct tcf_proto __rcu **pprev;
+ 	struct tcf_proto __rcu *next;
+ };
+ 
+ static struct tcf_proto *tcf_chain_tp_prev(struct tcf_chain_info *chain_info)
+ {
+ 	return rtnl_dereference(*chain_info->pprev);
+ }
+ 
+ static void tcf_chain_tp_insert(struct tcf_chain *chain,
+ 				struct tcf_chain_info *chain_info,
+ 				struct tcf_proto *tp)
+ {
+ 	if (chain->p_filter_chain &&
+ 	    *chain_info->pprev == chain->filter_chain)
+ 		rcu_assign_pointer(*chain->p_filter_chain, tp);
+ 	RCU_INIT_POINTER(tp->next, tcf_chain_tp_prev(chain_info));
+ 	rcu_assign_pointer(*chain_info->pprev, tp);
+ 	tcf_chain_hold(chain);
+ }
+ 
+ static void tcf_chain_tp_remove(struct tcf_chain *chain,
+ 				struct tcf_chain_info *chain_info,
+ 				struct tcf_proto *tp)
+ {
+ 	struct tcf_proto *next = rtnl_dereference(chain_info->next);
+ 
+ 	if (chain->p_filter_chain && tp == chain->filter_chain)
+ 		RCU_INIT_POINTER(*chain->p_filter_chain, next);
+ 	RCU_INIT_POINTER(*chain_info->pprev, next);
+ 	tcf_chain_put(chain);
+ }
+ 
+ static struct tcf_proto *tcf_chain_tp_find(struct tcf_chain *chain,
+ 					   struct tcf_chain_info *chain_info,
+ 					   u32 protocol, u32 prio,
+ 					   bool prio_allocate)
+ {
+ 	struct tcf_proto **pprev;
+ 	struct tcf_proto *tp;
+ 
+ 	/* Check the chain for existence of proto-tcf with this priority */
+ 	for (pprev = &chain->filter_chain;
+ 	     (tp = rtnl_dereference(*pprev)); pprev = &tp->next) {
+ 		if (tp->prio >= prio) {
+ 			if (tp->prio == prio) {
+ 				if (prio_allocate ||
+ 				    (tp->protocol != protocol && protocol))
+ 					return ERR_PTR(-EINVAL);
+ 			} else {
+ 				tp = NULL;
+ 			}
+ 			break;
+ 		}
+ 	}
+ 	chain_info->pprev = pprev;
+ 	chain_info->next = tp ? tp->next : NULL;
+ 	return tp;
+ }
+ 
+ static int tcf_fill_node(struct net *net, struct sk_buff *skb,
+ 			 struct tcf_proto *tp, void *fh, u32 portid,
+ 			 u32 seq, u16 flags, int event)
+ {
+ 	struct tcmsg *tcm;
+ 	struct nlmsghdr  *nlh;
+ 	unsigned char *b = skb_tail_pointer(skb);
+ 
+ 	nlh = nlmsg_put(skb, portid, seq, event, sizeof(*tcm), flags);
+ 	if (!nlh)
+ 		goto out_nlmsg_trim;
+ 	tcm = nlmsg_data(nlh);
+ 	tcm->tcm_family = AF_UNSPEC;
+ 	tcm->tcm__pad1 = 0;
+ 	tcm->tcm__pad2 = 0;
+ 	tcm->tcm_ifindex = qdisc_dev(tp->q)->ifindex;
+ 	tcm->tcm_parent = tp->classid;
+ 	tcm->tcm_info = TC_H_MAKE(tp->prio, tp->protocol);
+ 	if (nla_put_string(skb, TCA_KIND, tp->ops->kind))
+ 		goto nla_put_failure;
+ 	if (nla_put_u32(skb, TCA_CHAIN, tp->chain->index))
+ 		goto nla_put_failure;
+ 	if (!fh) {
+ 		tcm->tcm_handle = 0;
+ 	} else {
+ 		if (tp->ops->dump && tp->ops->dump(net, tp, fh, skb, tcm) < 0)
+ 			goto nla_put_failure;
+ 	}
+ 	nlh->nlmsg_len = skb_tail_pointer(skb) - b;
+ 	return skb->len;
+ 
+ out_nlmsg_trim:
+ nla_put_failure:
+ 	nlmsg_trim(skb, b);
+ 	return -1;
+ }
+ 
+ static int tfilter_notify(struct net *net, struct sk_buff *oskb,
+ 			  struct nlmsghdr *n, struct tcf_proto *tp,
+ 			  void *fh, int event, bool unicast)
+ {
+ 	struct sk_buff *skb;
+ 	u32 portid = oskb ? NETLINK_CB(oskb).portid : 0;
+ 
+ 	skb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);
+ 	if (!skb)
+ 		return -ENOBUFS;
+ 
+ 	if (tcf_fill_node(net, skb, tp, fh, portid, n->nlmsg_seq,
+ 			  n->nlmsg_flags, event) <= 0) {
+ 		kfree_skb(skb);
+ 		return -EINVAL;
+ 	}
+ 
+ 	if (unicast)
+ 		return netlink_unicast(net->rtnl, skb, portid, MSG_DONTWAIT);
+ 
+ 	return rtnetlink_send(skb, net, portid, RTNLGRP_TC,
+ 			      n->nlmsg_flags & NLM_F_ECHO);
+ }
+ 
+ static int tfilter_del_notify(struct net *net, struct sk_buff *oskb,
+ 			      struct nlmsghdr *n, struct tcf_proto *tp,
+ 			      void *fh, bool unicast, bool *last)
+ {
+ 	struct sk_buff *skb;
+ 	u32 portid = oskb ? NETLINK_CB(oskb).portid : 0;
+ 	int err;
+ 
+ 	skb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);
+ 	if (!skb)
+ 		return -ENOBUFS;
+ 
+ 	if (tcf_fill_node(net, skb, tp, fh, portid, n->nlmsg_seq,
+ 			  n->nlmsg_flags, RTM_DELTFILTER) <= 0) {
+ 		kfree_skb(skb);
+ 		return -EINVAL;
+ 	}
+ 
+ 	err = tp->ops->delete(tp, fh, last);
+ 	if (err) {
+ 		kfree_skb(skb);
+ 		return err;
+ 	}
+ 
+ 	if (unicast)
+ 		return netlink_unicast(net->rtnl, skb, portid, MSG_DONTWAIT);
+ 
+ 	return rtnetlink_send(skb, net, portid, RTNLGRP_TC,
+ 			      n->nlmsg_flags & NLM_F_ECHO);
+ }
+ 
+ static void tfilter_notify_chain(struct net *net, struct sk_buff *oskb,
+ 				 struct nlmsghdr *n,
+ 				 struct tcf_chain *chain, int event)
+ {
+ 	struct tcf_proto *tp;
+ 
+ 	for (tp = rtnl_dereference(chain->filter_chain);
+ 	     tp; tp = rtnl_dereference(tp->next))
+ 		tfilter_notify(net, oskb, n, tp, 0, event, false);
++>>>>>>> 822e86d997e4 (net_sched: remove tcf_block_put_deferred())
  }
  
  /* Add/change/delete/get a filter node */
* Unmerged path net/sched/cls_api.c
