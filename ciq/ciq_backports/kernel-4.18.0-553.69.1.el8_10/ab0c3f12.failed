mm/khugepaged: fix collapse_pte_mapped_thp() to allow anon_vma

jira LE-3845
cve CVE-2025-38085
Rebuild_History Non-Buildable kernel-4.18.0-553.69.1.el8_10
commit-author Hugh Dickins <hughd@google.com>
commit ab0c3f1251b4670978fde0bd54161795a139b060
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.69.1.el8_10/ab0c3f12.failed

uprobe_write_opcode() uses collapse_pte_mapped_thp() to restore huge pmd,
when removing a breakpoint from hugepage text: vma->anon_vma is always set
in that case, so undo the prohibition.  And MADV_COLLAPSE ought to be able
to collapse some page tables in a vma which happens to have anon_vma set
from CoWing elsewhere.

Is anon_vma lock required?  Almost not: if any page other than expected
subpage of the non-anon huge page is found in the page table, collapse is
aborted without making any change.  However, it is possible that an anon
page was CoWed from this extent in another mm or vma, in which case a
concurrent lookup might look here: so keep it away while clearing pmd (but
perhaps we shall go back to using pmd_lock() there in future).

Note that collapse_pte_mapped_thp() is exceptional in freeing a page table
without having cleared its ptes: I'm uneasy about that, and had thought
pte_clear()ing appropriate; but exclusive i_mmap lock does fix the
problem, and we would have to move the mmu_notification if clearing those
ptes.

What this fixes is not a dangerous instability.  But I suggest Cc stable
because uprobes "healing" has regressed in that way, so this should follow
8d3c106e19e8 into those stable releases where it was backported (and may
want adjustment there - I'll supply backports as needed).

Link: https://lkml.kernel.org/r/b740c9fb-edba-92ba-59fb-7a5592e5dfc@google.com
Fixes: 8d3c106e19e8 ("mm/khugepaged: take the right locks for page table retraction")
	Signed-off-by: Hugh Dickins <hughd@google.com>
	Acked-by: David Hildenbrand <david@redhat.com>
	Cc: Jann Horn <jannh@google.com>
	Cc: Yang Shi <shy828301@gmail.com>
	Cc: Zach O'Keefe <zokeefe@google.com>
	Cc: Song Liu <songliubraving@fb.com>
	Cc: <stable@vger.kernel.org>    [5.4+]
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit ab0c3f1251b4670978fde0bd54161795a139b060)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
diff --cc mm/khugepaged.c
index 59999ec63bb6,9a0135b39b19..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1411,21 -1436,33 +1411,30 @@@ void collapse_pte_mapped_thp(struct mm_
  	pte_t *start_pte, *pte;
  	pmd_t *pmd;
  	spinlock_t *ptl;
 -	int count = 0, result = SCAN_FAIL;
 +	int count = 0;
  	int i;
  
 -	mmap_assert_write_locked(mm);
 -
 -	/* Fast check before locking page if already PMD-mapped */
 -	result = find_pmd_or_thp_or_none(mm, haddr, &pmd);
 -	if (result == SCAN_PMD_MAPPED)
 -		return result;
 -
  	if (!vma || !vma->vm_file ||
 -	    !range_in_vma(vma, haddr, haddr + HPAGE_PMD_SIZE))
 -		return SCAN_VMA_CHECK;
 +	    vma->vm_start > haddr || vma->vm_end < haddr + HPAGE_PMD_SIZE)
 +		return;
  
  	/*
 -	 * If we are here, we've succeeded in replacing all the native pages
 -	 * in the page cache with a single hugepage. If a mm were to fault-in
 -	 * this memory (mapped by a suitably aligned VMA), we'd get the hugepage
 -	 * and map it by a PMD, regardless of sysfs THP settings. As such, let's
 -	 * analogously elide sysfs THP settings here.
 +	 * This vm_flags may not have VM_HUGEPAGE if the page was not
 +	 * collapsed by this mm. But we can still collapse if the page is
 +	 * the valid THP. Add extra VM_HUGEPAGE so hugepage_vma_check()
 +	 * will not fail the vma for missing VM_HUGEPAGE
  	 */
++<<<<<<< HEAD
 +	if (!hugepage_vma_check(vma, vma->vm_flags | VM_HUGEPAGE))
 +		return;
++=======
+ 	if (!hugepage_vma_check(vma, vma->vm_flags, false, false, false))
+ 		return SCAN_VMA_CHECK;
+ 
+ 	/* Keep pmd pgtable for uffd-wp; see comment in retract_page_tables() */
+ 	if (userfaultfd_wp(vma))
+ 		return SCAN_PTE_UFFD_WP;
++>>>>>>> ab0c3f1251b4 (mm/khugepaged: fix collapse_pte_mapped_thp() to allow anon_vma)
  
  	hpage = find_lock_page(vma->vm_file->f_mapping,
  			       linear_page_index(vma, haddr));
@@@ -1484,8 -1558,23 +1493,28 @@@
  		add_mm_counter(vma->vm_mm, mm_counter_file(hpage), -count);
  	}
  
++<<<<<<< HEAD
 +	/* step 4: collapse pmd */
 +	collapse_and_free_pmd(mm, vma, haddr, pmd);
++=======
+ 	/* step 4: remove pte entries */
+ 	/* we make no change to anon, but protect concurrent anon page lookup */
+ 	if (vma->anon_vma)
+ 		anon_vma_lock_write(vma->anon_vma);
+ 
+ 	collapse_and_free_pmd(mm, vma, haddr, pmd);
+ 
+ 	if (vma->anon_vma)
+ 		anon_vma_unlock_write(vma->anon_vma);
+ 	i_mmap_unlock_write(vma->vm_file->f_mapping);
+ 
+ maybe_install_pmd:
+ 	/* step 5: install pmd entry */
+ 	result = install_pmd
+ 			? set_huge_pmd(vma, haddr, pmd, hpage)
+ 			: SCAN_SUCCEED;
+ 
++>>>>>>> ab0c3f1251b4 (mm/khugepaged: fix collapse_pte_mapped_thp() to allow anon_vma)
  drop_hpage:
  	unlock_page(hpage);
  	put_page(hpage);
* Unmerged path mm/khugepaged.c
