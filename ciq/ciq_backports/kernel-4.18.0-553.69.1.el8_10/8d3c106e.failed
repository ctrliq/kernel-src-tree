mm/khugepaged: take the right locks for page table retraction

jira LE-3845
cve CVE-2025-38085
Rebuild_History Non-Buildable kernel-4.18.0-553.69.1.el8_10
commit-author Jann Horn <jannh@google.com>
commit 8d3c106e19e8d251da31ff4cc7462e4565d65084
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.69.1.el8_10/8d3c106e.failed

pagetable walks on address ranges mapped by VMAs can be done under the
mmap lock, the lock of an anon_vma attached to the VMA, or the lock of the
VMA's address_space.  Only one of these needs to be held, and it does not
need to be held in exclusive mode.

Under those circumstances, the rules for concurrent access to page table
entries are:

 - Terminal page table entries (entries that don't point to another page
   table) can be arbitrarily changed under the page table lock, with the
   exception that they always need to be consistent for
   hardware page table walks and lockless_pages_from_mm().
   This includes that they can be changed into non-terminal entries.
 - Non-terminal page table entries (which point to another page table)
   can not be modified; readers are allowed to READ_ONCE() an entry, verify
   that it is non-terminal, and then assume that its value will stay as-is.

Retracting a page table involves modifying a non-terminal entry, so
page-table-level locks are insufficient to protect against concurrent page
table traversal; it requires taking all the higher-level locks under which
it is possible to start a page walk in the relevant range in exclusive
mode.

The collapse_huge_page() path for anonymous THP already follows this rule,
but the shmem/file THP path was getting it wrong, making it possible for
concurrent rmap-based operations to cause corruption.

Link: https://lkml.kernel.org/r/20221129154730.2274278-1-jannh@google.com
Link: https://lkml.kernel.org/r/20221128180252.1684965-1-jannh@google.com
Link: https://lkml.kernel.org/r/20221125213714.4115729-1-jannh@google.com
Fixes: 27e1f8273113 ("khugepaged: enable collapse pmd for pte-mapped THP")
	Signed-off-by: Jann Horn <jannh@google.com>
	Reviewed-by: Yang Shi <shy828301@gmail.com>
	Acked-by: David Hildenbrand <david@redhat.com>
	Cc: John Hubbard <jhubbard@nvidia.com>
	Cc: Peter Xu <peterx@redhat.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 8d3c106e19e8d251da31ff4cc7462e4565d65084)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/khugepaged.c
diff --cc mm/khugepaged.c
index 59999ec63bb6,0a11e132ad6b..000000000000
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@@ -1376,23 -1348,70 +1376,49 @@@ static int khugepaged_add_pte_mapped_th
  	VM_BUG_ON(addr & ~HPAGE_PMD_MASK);
  
  	spin_lock(&khugepaged_mm_lock);
 -	slot = mm_slot_lookup(mm_slots_hash, mm);
 -	mm_slot = mm_slot_entry(slot, struct khugepaged_mm_slot, slot);
 -	if (likely(mm_slot && mm_slot->nr_pte_mapped_thp < MAX_PTE_MAPPED_THP)) {
 +	mm_slot = get_mm_slot(mm);
 +	if (likely(mm_slot && mm_slot->nr_pte_mapped_thp < MAX_PTE_MAPPED_THP))
  		mm_slot->pte_mapped_thp[mm_slot->nr_pte_mapped_thp++] = addr;
 -		ret = true;
 -	}
  	spin_unlock(&khugepaged_mm_lock);
 -	return ret;
 -}
 -
 -/* hpage must be locked, and mmap_lock must be held in write */
 -static int set_huge_pmd(struct vm_area_struct *vma, unsigned long addr,
 -			pmd_t *pmdp, struct page *hpage)
 -{
 -	struct vm_fault vmf = {
 -		.vma = vma,
 -		.address = addr,
 -		.flags = 0,
 -		.pmd = pmdp,
 -	};
 -
 -	VM_BUG_ON(!PageTransHuge(hpage));
 -	mmap_assert_write_locked(vma->vm_mm);
 -
 -	if (do_set_pmd(&vmf, hpage))
 -		return SCAN_FAIL;
 -
 -	get_page(hpage);
 -	return SCAN_SUCCEED;
 +	return 0;
  }
  
+ /*
+  * A note about locking:
+  * Trying to take the page table spinlocks would be useless here because those
+  * are only used to synchronize:
+  *
+  *  - modifying terminal entries (ones that point to a data page, not to another
+  *    page table)
+  *  - installing *new* non-terminal entries
+  *
+  * Instead, we need roughly the same kind of protection as free_pgtables() or
+  * mm_take_all_locks() (but only for a single VMA):
+  * The mmap lock together with this VMA's rmap locks covers all paths towards
+  * the page table entries we're messing with here, except for hardware page
+  * table walks and lockless_pages_from_mm().
+  */
  static void collapse_and_free_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
  				  unsigned long addr, pmd_t *pmdp)
  {
- 	spinlock_t *ptl;
  	pmd_t pmd;
  
++<<<<<<< HEAD
 +	ptl = pmd_lock(vma->vm_mm, pmdp);
++=======
+ 	mmap_assert_write_locked(mm);
+ 	if (vma->vm_file)
+ 		lockdep_assert_held_write(&vma->vm_file->f_mapping->i_mmap_rwsem);
+ 	/*
+ 	 * All anon_vmas attached to the VMA have the same root and are
+ 	 * therefore locked by the same lock.
+ 	 */
+ 	if (vma->anon_vma)
+ 		lockdep_assert_held_write(&vma->anon_vma->root->rwsem);
+ 
++>>>>>>> 8d3c106e19e8 (mm/khugepaged: take the right locks for page table retraction)
  	pmd = pmdp_collapse_flush(vma, addr, pmdp);
- 	spin_unlock(ptl);
  	mm_dec_nr_ptes(mm);
 -	page_table_check_pte_clear_range(mm, addr, pmd);
  	pte_free(mm, pmd_pgtable(pmd));
  }
  
@@@ -1411,35 -1436,87 +1437,66 @@@ void collapse_pte_mapped_thp(struct mm_
  	pte_t *start_pte, *pte;
  	pmd_t *pmd;
  	spinlock_t *ptl;
 -	int count = 0, result = SCAN_FAIL;
 +	int count = 0;
  	int i;
  
 -	mmap_assert_write_locked(mm);
 -
 -	/* Fast check before locking page if already PMD-mapped */
 -	result = find_pmd_or_thp_or_none(mm, haddr, &pmd);
 -	if (result == SCAN_PMD_MAPPED)
 -		return result;
 -
  	if (!vma || !vma->vm_file ||
 -	    !range_in_vma(vma, haddr, haddr + HPAGE_PMD_SIZE))
 -		return SCAN_VMA_CHECK;
 +	    vma->vm_start > haddr || vma->vm_end < haddr + HPAGE_PMD_SIZE)
 +		return;
  
  	/*
 -	 * If we are here, we've succeeded in replacing all the native pages
 -	 * in the page cache with a single hugepage. If a mm were to fault-in
 -	 * this memory (mapped by a suitably aligned VMA), we'd get the hugepage
 -	 * and map it by a PMD, regardless of sysfs THP settings. As such, let's
 -	 * analogously elide sysfs THP settings here.
 +	 * This vm_flags may not have VM_HUGEPAGE if the page was not
 +	 * collapsed by this mm. But we can still collapse if the page is
 +	 * the valid THP. Add extra VM_HUGEPAGE so hugepage_vma_check()
 +	 * will not fail the vma for missing VM_HUGEPAGE
  	 */
++<<<<<<< HEAD
 +	if (!hugepage_vma_check(vma, vma->vm_flags | VM_HUGEPAGE))
 +		return;
++=======
+ 	if (!hugepage_vma_check(vma, vma->vm_flags, false, false, false))
+ 		return SCAN_VMA_CHECK;
+ 
+ 	/*
+ 	 * Symmetry with retract_page_tables(): Exclude MAP_PRIVATE mappings
+ 	 * that got written to. Without this, we'd have to also lock the
+ 	 * anon_vma if one exists.
+ 	 */
+ 	if (vma->anon_vma)
+ 		return SCAN_VMA_CHECK;
+ 
+ 	/* Keep pmd pgtable for uffd-wp; see comment in retract_page_tables() */
+ 	if (userfaultfd_wp(vma))
+ 		return SCAN_PTE_UFFD_WP;
++>>>>>>> 8d3c106e19e8 (mm/khugepaged: take the right locks for page table retraction)
  
  	hpage = find_lock_page(vma->vm_file->f_mapping,
  			       linear_page_index(vma, haddr));
  	if (!hpage)
 -		return SCAN_PAGE_NULL;
 +		return;
  
 -	if (!PageHead(hpage)) {
 -		result = SCAN_FAIL;
 +	if (!PageHead(hpage))
  		goto drop_hpage;
 -	}
  
 -	if (compound_order(hpage) != HPAGE_PMD_ORDER) {
 -		result = SCAN_PAGE_COMPOUND;
 +	pmd = mm_find_pmd(mm, haddr);
 +	if (!pmd)
  		goto drop_hpage;
 -	}
 -
 -	switch (result) {
 -	case SCAN_SUCCEED:
 -		break;
 -	case SCAN_PMD_NONE:
 -		/*
 -		 * In MADV_COLLAPSE path, possible race with khugepaged where
 -		 * all pte entries have been removed and pmd cleared.  If so,
 -		 * skip all the pte checks and just update the pmd mapping.
 -		 */
 -		goto maybe_install_pmd;
 -	default:
 -		goto drop_hpage;
 -	}
  
+ 	/*
+ 	 * We need to lock the mapping so that from here on, only GUP-fast and
+ 	 * hardware page walks can access the parts of the page tables that
+ 	 * we're operating on.
+ 	 * See collapse_and_free_pmd().
+ 	 */
+ 	i_mmap_lock_write(vma->vm_file->f_mapping);
+ 
+ 	/*
+ 	 * This spinlock should be unnecessary: Nobody else should be accessing
+ 	 * the page tables under spinlock protection here, only
+ 	 * lockless_pages_from_mm() and the hardware page walker can access page
+ 	 * tables while all the high-level locks are held in write mode.
+ 	 */
  	start_pte = pte_offset_map_lock(mm, pmd, haddr, &ptl);
 -	result = SCAN_FAIL;
  
  	/* step 1: check all mapped PTEs are to the right huge page */
  	for (i = 0, addr = haddr, pte = start_pte;
@@@ -1484,8 -1566,17 +1541,20 @@@
  		add_mm_counter(vma->vm_mm, mm_counter_file(hpage), -count);
  	}
  
 -	/* step 4: remove pte entries */
 +	/* step 4: collapse pmd */
  	collapse_and_free_pmd(mm, vma, haddr, pmd);
++<<<<<<< HEAD
++=======
+ 
+ 	i_mmap_unlock_write(vma->vm_file->f_mapping);
+ 
+ maybe_install_pmd:
+ 	/* step 5: install pmd entry */
+ 	result = install_pmd
+ 			? set_huge_pmd(vma, haddr, pmd, hpage)
+ 			: SCAN_SUCCEED;
+ 
++>>>>>>> 8d3c106e19e8 (mm/khugepaged: take the right locks for page table retraction)
  drop_hpage:
  	unlock_page(hpage);
  	put_page(hpage);
@@@ -1542,19 -1641,24 +1612,20 @@@ static void retract_page_tables(struct 
  		 * An alternative would be drop the check, but check that page
  		 * table is clear before calling pmdp_collapse_flush() under
  		 * ptl. It has higher chance to recover THP for the VMA, but
- 		 * has higher cost too.
+ 		 * has higher cost too. It would also probably require locking
+ 		 * the anon_vma.
  		 */
 -		if (vma->anon_vma) {
 -			result = SCAN_PAGE_ANON;
 -			goto next;
 -		}
 +		if (vma->anon_vma)
 +			continue;
  		addr = vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);
 -		if (addr & ~HPAGE_PMD_MASK ||
 -		    vma->vm_end < addr + HPAGE_PMD_SIZE) {
 -			result = SCAN_VMA_CHECK;
 -			goto next;
 -		}
 +		if (addr & ~HPAGE_PMD_MASK)
 +			continue;
 +		if (vma->vm_end < addr + HPAGE_PMD_SIZE)
 +			continue;
  		mm = vma->vm_mm;
 -		is_target = mm == target_mm && addr == target_addr;
 -		result = find_pmd_or_thp_or_none(mm, addr, &pmd);
 -		if (result != SCAN_SUCCEED)
 -			goto next;
 +		pmd = mm_find_pmd(mm, addr);
 +		if (!pmd)
 +			continue;
  		/*
  		 * We need exclusive mmap_lock to retract page table.
  		 *
* Unmerged path mm/khugepaged.c
