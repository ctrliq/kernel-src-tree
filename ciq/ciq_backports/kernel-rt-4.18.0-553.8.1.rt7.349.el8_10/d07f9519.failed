crypto: s390/aes - Fix buffer overread in CTR mode

jira LE-1907
cve CVE-2023-52669
Rebuild_History Non-Buildable kernel-rt-4.18.0-553.8.1.rt7.349.el8_10
commit-author Herbert Xu <herbert@gondor.apana.org.au>
commit d07f951903fa9922c375b8ab1ce81b18a0034e3b
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-553.8.1.rt7.349.el8_10/d07f9519.failed

When processing the last block, the s390 ctr code will always read
a whole block, even if there isn't a whole block of data left.  Fix
this by using the actual length left and copy it into a buffer first
for processing.

Fixes: 0200f3ecc196 ("crypto: s390 - add System z hardware support for CTR mode")
	Cc: <stable@vger.kernel.org>
	Reported-by: Guangwu Zhang <guazhang@redhat.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
Reviewd-by: Harald Freudenberger <freude@de.ibm.com>
	Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
(cherry picked from commit d07f951903fa9922c375b8ab1ce81b18a0034e3b)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/s390/crypto/aes_s390.c
#	arch/s390/crypto/paes_s390.c
diff --cc arch/s390/crypto/aes_s390.c
index 2e339a11a776,c6fe5405de4a..000000000000
--- a/arch/s390/crypto/aes_s390.c
+++ b/arch/s390/crypto/aes_s390.c
@@@ -721,12 -597,13 +721,22 @@@ static int ctr_aes_crypt(struct blkciph
  	 * final block may be < AES_BLOCK_SIZE, copy only nbytes
  	 */
  	if (nbytes) {
++<<<<<<< HEAD
 +		cpacf_kmctr(sctx->fc | modifier, sctx->key,
 +			    buf, walk->src.virt.addr,
 +			    AES_BLOCK_SIZE, walk->iv);
 +		memcpy(walk->dst.virt.addr, buf, nbytes);
 +		crypto_inc(walk->iv, AES_BLOCK_SIZE);
 +		ret = blkcipher_walk_done(desc, walk, 0);
++=======
+ 		memset(buf, 0, AES_BLOCK_SIZE);
+ 		memcpy(buf, walk.src.virt.addr, nbytes);
+ 		cpacf_kmctr(sctx->fc, sctx->key, buf, buf,
+ 			    AES_BLOCK_SIZE, walk.iv);
+ 		memcpy(walk.dst.virt.addr, buf, nbytes);
+ 		crypto_inc(walk.iv, AES_BLOCK_SIZE);
+ 		ret = skcipher_walk_done(&walk, 0);
++>>>>>>> d07f951903fa (crypto: s390/aes - Fix buffer overread in CTR mode)
  	}
  
  	return ret;
diff --cc arch/s390/crypto/paes_s390.c
index 0ea22917afcf,55ee5567a5ea..000000000000
--- a/arch/s390/crypto/paes_s390.c
+++ b/arch/s390/crypto/paes_s390.c
@@@ -751,13 -693,15 +751,21 @@@ static int ctr_paes_crypt(struct blkcip
  	 * final block may be < AES_BLOCK_SIZE, copy only nbytes
  	 */
  	if (nbytes) {
+ 		memset(buf, 0, AES_BLOCK_SIZE);
+ 		memcpy(buf, walk.src.virt.addr, nbytes);
  		while (1) {
++<<<<<<< HEAD
 +			if (cpacf_kmctr(ctx->fc | modifier, &param, buf,
 +					walk->src.virt.addr, AES_BLOCK_SIZE,
 +					walk->iv) == AES_BLOCK_SIZE)
++=======
+ 			if (cpacf_kmctr(ctx->fc, &param, buf,
+ 					buf, AES_BLOCK_SIZE,
+ 					walk.iv) == AES_BLOCK_SIZE)
++>>>>>>> d07f951903fa (crypto: s390/aes - Fix buffer overread in CTR mode)
  				break;
  			if (__paes_convert_key(ctx))
 -				return skcipher_walk_done(&walk, -EIO);
 +				return blkcipher_walk_done(desc, walk, -EIO);
  			spin_lock_bh(&ctx->pk_lock);
  			memcpy(param.key, ctx->pk.protkey, MAXPROTKEYSIZE);
  			spin_unlock_bh(&ctx->pk_lock);
* Unmerged path arch/s390/crypto/aes_s390.c
* Unmerged path arch/s390/crypto/paes_s390.c
