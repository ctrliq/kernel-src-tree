mm/userfaultfd: fix uffd-wp handling for THP migration entries

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-514.el8
commit-author David Hildenbrand <david@redhat.com>
commit 24bf08c4376be417f16ceb609188b16f461b0443
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-514.el8/24bf08c4.failed

Looks like what we fixed for hugetlb in commit 44f86392bdd1 ("mm/hugetlb:
fix uffd-wp handling for migration entries in
hugetlb_change_protection()") similarly applies to THP.

Setting/clearing uffd-wp on THP migration entries is not implemented
properly.  Further, while removing migration PMDs considers the uffd-wp
bit, inserting migration PMDs does not consider the uffd-wp bit.

We have to set/clear independently of the migration entry type in
change_huge_pmd() and properly copy the uffd-wp bit in
set_pmd_migration_entry().

Verified using a simple reproducer that triggers migration of a THP, that
the set_pmd_migration_entry() no longer loses the uffd-wp bit.

Link: https://lkml.kernel.org/r/20230405160236.587705-2-david@redhat.com
Fixes: f45ec5ff16a7 ("userfaultfd: wp: support swap and page migration")
	Signed-off-by: David Hildenbrand <david@redhat.com>
	Reviewed-by: Peter Xu <peterx@redhat.com>
	Cc: <stable@vger.kernel.org>
	Cc: Muhammad Usama Anjum <usama.anjum@collabora.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit 24bf08c4376be417f16ceb609188b16f461b0443)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/huge_memory.c
diff --cc mm/huge_memory.c
index 90eefa9de607,e3706a2b34b2..000000000000
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -1835,10 -1837,11 +1835,18 @@@ int change_huge_pmd(struct vm_area_stru
  #ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
  	if (is_swap_pmd(*pmd)) {
  		swp_entry_t entry = pmd_to_swp_entry(*pmd);
++<<<<<<< HEAD
 +
 +		VM_BUG_ON(!is_pmd_migration_entry(*pmd));
 +		if (is_write_migration_entry(entry)) {
 +			pmd_t newpmd;
++=======
+ 		struct page *page = pfn_swap_entry_to_page(entry);
+ 		pmd_t newpmd;
+ 
+ 		VM_BUG_ON(!is_pmd_migration_entry(*pmd));
+ 		if (is_writable_migration_entry(entry)) {
++>>>>>>> 24bf08c4376b (mm/userfaultfd: fix uffd-wp handling for THP migration entries)
  			/*
  			 * A protection check is difficult so
  			 * just be safe and disable write
@@@ -1847,8 -1853,18 +1855,22 @@@
  			newpmd = swp_entry_to_pmd(entry);
  			if (pmd_swp_soft_dirty(*pmd))
  				newpmd = pmd_swp_mksoft_dirty(newpmd);
++<<<<<<< HEAD
 +			set_pmd_at(mm, addr, pmd, newpmd);
++=======
+ 			if (pmd_swp_uffd_wp(*pmd))
+ 				newpmd = pmd_swp_mkuffd_wp(newpmd);
+ 		} else {
+ 			newpmd = *pmd;
++>>>>>>> 24bf08c4376b (mm/userfaultfd: fix uffd-wp handling for THP migration entries)
  		}
+ 
+ 		if (uffd_wp)
+ 			newpmd = pmd_swp_mkuffd_wp(newpmd);
+ 		else if (uffd_wp_resolve)
+ 			newpmd = pmd_swp_clear_uffd_wp(newpmd);
+ 		if (!pmd_same(*pmd, newpmd))
+ 			set_pmd_at(mm, addr, pmd, newpmd);
  		goto unlock;
  	}
  #endif
@@@ -3026,9 -3259,14 +3048,11 @@@ void set_pmd_migration_entry(struct pag
  	pmdswp = swp_entry_to_pmd(entry);
  	if (pmd_soft_dirty(pmdval))
  		pmdswp = pmd_swp_mksoft_dirty(pmdswp);
+ 	if (pmd_uffd_wp(pmdval))
+ 		pmdswp = pmd_swp_mkuffd_wp(pmdswp);
  	set_pmd_at(mm, address, pvmw->pmd, pmdswp);
 -	page_remove_rmap(page, vma, true);
 +	page_remove_rmap(page, true);
  	put_page(page);
 -	trace_set_migration_pmd(address, pmd_val(pmdswp));
 -
 -	return 0;
  }
  
  void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)
* Unmerged path mm/huge_memory.c
