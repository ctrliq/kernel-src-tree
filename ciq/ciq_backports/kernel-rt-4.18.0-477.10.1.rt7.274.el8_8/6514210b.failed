net/mlx5e: kTLS, Use _safe() iterator in mlx5e_tls_priv_tx_list_cleanup()

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Dan Carpenter <dan.carpenter@oracle.com>
commit 6514210b6d0dc36352fda86b71f80f9a9ed4f677
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/6514210b.failed

Use the list_for_each_entry_safe() macro to prevent dereferencing "obj"
after it has been freed.

Fixes: c4dfe704f53f ("net/mlx5e: kTLS, Recycle objects of device-offloaded TLS TX connections")
	Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
	Reviewed-by: Tariq Toukan <tariqt@nvidia.com>
	Signed-off-by: Saeed Mahameed <saeedm@nvidia.com>
(cherry picked from commit 6514210b6d0dc36352fda86b71f80f9a9ed4f677)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
diff --cc drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
index 5fd65d395a52,3a1f76eac542..000000000000
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
@@@ -86,6 -122,347 +86,350 @@@ mlx5e_get_ktls_tx_priv_ctx(struct tls_c
  	return *ctx;
  }
  
++<<<<<<< HEAD
++=======
+ /* struct for callback API management */
+ struct mlx5e_async_ctx {
+ 	struct mlx5_async_work context;
+ 	struct mlx5_async_ctx async_ctx;
+ 	struct work_struct work;
+ 	struct mlx5e_ktls_offload_context_tx *priv_tx;
+ 	struct completion complete;
+ 	int err;
+ 	union {
+ 		u32 out_create[MLX5_ST_SZ_DW(create_tis_out)];
+ 		u32 out_destroy[MLX5_ST_SZ_DW(destroy_tis_out)];
+ 	};
+ };
+ 
+ static struct mlx5e_async_ctx *mlx5e_bulk_async_init(struct mlx5_core_dev *mdev, int n)
+ {
+ 	struct mlx5e_async_ctx *bulk_async;
+ 	int i;
+ 
+ 	bulk_async = kvcalloc(n, sizeof(struct mlx5e_async_ctx), GFP_KERNEL);
+ 	if (!bulk_async)
+ 		return NULL;
+ 
+ 	for (i = 0; i < n; i++) {
+ 		struct mlx5e_async_ctx *async = &bulk_async[i];
+ 
+ 		mlx5_cmd_init_async_ctx(mdev, &async->async_ctx);
+ 		init_completion(&async->complete);
+ 	}
+ 
+ 	return bulk_async;
+ }
+ 
+ static void mlx5e_bulk_async_cleanup(struct mlx5e_async_ctx *bulk_async, int n)
+ {
+ 	int i;
+ 
+ 	for (i = 0; i < n; i++) {
+ 		struct mlx5e_async_ctx *async = &bulk_async[i];
+ 
+ 		mlx5_cmd_cleanup_async_ctx(&async->async_ctx);
+ 	}
+ 	kvfree(bulk_async);
+ }
+ 
+ static void create_tis_callback(int status, struct mlx5_async_work *context)
+ {
+ 	struct mlx5e_async_ctx *async =
+ 		container_of(context, struct mlx5e_async_ctx, context);
+ 	struct mlx5e_ktls_offload_context_tx *priv_tx = async->priv_tx;
+ 
+ 	if (status) {
+ 		async->err = status;
+ 		priv_tx->create_err = 1;
+ 		goto out;
+ 	}
+ 
+ 	priv_tx->tisn = MLX5_GET(create_tis_out, async->out_create, tisn);
+ out:
+ 	complete(&async->complete);
+ }
+ 
+ static void destroy_tis_callback(int status, struct mlx5_async_work *context)
+ {
+ 	struct mlx5e_async_ctx *async =
+ 		container_of(context, struct mlx5e_async_ctx, context);
+ 	struct mlx5e_ktls_offload_context_tx *priv_tx = async->priv_tx;
+ 
+ 	complete(&async->complete);
+ 	kfree(priv_tx);
+ }
+ 
+ static struct mlx5e_ktls_offload_context_tx *
+ mlx5e_tls_priv_tx_init(struct mlx5_core_dev *mdev, struct mlx5e_tls_sw_stats *sw_stats,
+ 		       struct mlx5e_async_ctx *async)
+ {
+ 	struct mlx5e_ktls_offload_context_tx *priv_tx;
+ 	int err;
+ 
+ 	priv_tx = kzalloc(sizeof(*priv_tx), GFP_KERNEL);
+ 	if (!priv_tx)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	priv_tx->mdev = mdev;
+ 	priv_tx->sw_stats = sw_stats;
+ 
+ 	if (!async) {
+ 		err = mlx5e_ktls_create_tis(mdev, &priv_tx->tisn);
+ 		if (err)
+ 			goto err_out;
+ 	} else {
+ 		async->priv_tx = priv_tx;
+ 		err = mlx5e_ktls_create_tis_cb(mdev, &async->async_ctx,
+ 					       async->out_create, sizeof(async->out_create),
+ 					       create_tis_callback, &async->context);
+ 		if (err)
+ 			goto err_out;
+ 	}
+ 
+ 	return priv_tx;
+ 
+ err_out:
+ 	kfree(priv_tx);
+ 	return ERR_PTR(err);
+ }
+ 
+ static void mlx5e_tls_priv_tx_cleanup(struct mlx5e_ktls_offload_context_tx *priv_tx,
+ 				      struct mlx5e_async_ctx *async)
+ {
+ 	if (priv_tx->create_err) {
+ 		complete(&async->complete);
+ 		kfree(priv_tx);
+ 		return;
+ 	}
+ 	async->priv_tx = priv_tx;
+ 	mlx5e_ktls_destroy_tis_cb(priv_tx->mdev, priv_tx->tisn,
+ 				  &async->async_ctx,
+ 				  async->out_destroy, sizeof(async->out_destroy),
+ 				  destroy_tis_callback, &async->context);
+ }
+ 
+ static void mlx5e_tls_priv_tx_list_cleanup(struct mlx5_core_dev *mdev,
+ 					   struct list_head *list, int size)
+ {
+ 	struct mlx5e_ktls_offload_context_tx *obj, *n;
+ 	struct mlx5e_async_ctx *bulk_async;
+ 	int i;
+ 
+ 	bulk_async = mlx5e_bulk_async_init(mdev, size);
+ 	if (!bulk_async)
+ 		return;
+ 
+ 	i = 0;
+ 	list_for_each_entry_safe(obj, n, list, list_node) {
+ 		mlx5e_tls_priv_tx_cleanup(obj, &bulk_async[i]);
+ 		i++;
+ 	}
+ 
+ 	for (i = 0; i < size; i++) {
+ 		struct mlx5e_async_ctx *async = &bulk_async[i];
+ 
+ 		wait_for_completion(&async->complete);
+ 	}
+ 	mlx5e_bulk_async_cleanup(bulk_async, size);
+ }
+ 
+ /* Recycling pool API */
+ 
+ #define MLX5E_TLS_TX_POOL_BULK (16)
+ #define MLX5E_TLS_TX_POOL_HIGH (4 * 1024)
+ #define MLX5E_TLS_TX_POOL_LOW (MLX5E_TLS_TX_POOL_HIGH / 4)
+ 
+ struct mlx5e_tls_tx_pool {
+ 	struct mlx5_core_dev *mdev;
+ 	struct mlx5e_tls_sw_stats *sw_stats;
+ 	struct mutex lock; /* Protects access to the pool */
+ 	struct list_head list;
+ 	size_t size;
+ 
+ 	struct workqueue_struct *wq;
+ 	struct work_struct create_work;
+ 	struct work_struct destroy_work;
+ };
+ 
+ static void create_work(struct work_struct *work)
+ {
+ 	struct mlx5e_tls_tx_pool *pool =
+ 		container_of(work, struct mlx5e_tls_tx_pool, create_work);
+ 	struct mlx5e_ktls_offload_context_tx *obj;
+ 	struct mlx5e_async_ctx *bulk_async;
+ 	LIST_HEAD(local_list);
+ 	int i, j, err = 0;
+ 
+ 	bulk_async = mlx5e_bulk_async_init(pool->mdev, MLX5E_TLS_TX_POOL_BULK);
+ 	if (!bulk_async)
+ 		return;
+ 
+ 	for (i = 0; i < MLX5E_TLS_TX_POOL_BULK; i++) {
+ 		obj = mlx5e_tls_priv_tx_init(pool->mdev, pool->sw_stats, &bulk_async[i]);
+ 		if (IS_ERR(obj)) {
+ 			err = PTR_ERR(obj);
+ 			break;
+ 		}
+ 		list_add(&obj->list_node, &local_list);
+ 	}
+ 
+ 	for (j = 0; j < i; j++) {
+ 		struct mlx5e_async_ctx *async = &bulk_async[j];
+ 
+ 		wait_for_completion(&async->complete);
+ 		if (!err && async->err)
+ 			err = async->err;
+ 	}
+ 	atomic64_add(i, &pool->sw_stats->tx_tls_pool_alloc);
+ 	mlx5e_bulk_async_cleanup(bulk_async, MLX5E_TLS_TX_POOL_BULK);
+ 	if (err)
+ 		goto err_out;
+ 
+ 	mutex_lock(&pool->lock);
+ 	if (pool->size + MLX5E_TLS_TX_POOL_BULK >= MLX5E_TLS_TX_POOL_HIGH) {
+ 		mutex_unlock(&pool->lock);
+ 		goto err_out;
+ 	}
+ 	list_splice(&local_list, &pool->list);
+ 	pool->size += MLX5E_TLS_TX_POOL_BULK;
+ 	if (pool->size <= MLX5E_TLS_TX_POOL_LOW)
+ 		queue_work(pool->wq, work);
+ 	mutex_unlock(&pool->lock);
+ 	return;
+ 
+ err_out:
+ 	mlx5e_tls_priv_tx_list_cleanup(pool->mdev, &local_list, i);
+ 	atomic64_add(i, &pool->sw_stats->tx_tls_pool_free);
+ }
+ 
+ static void destroy_work(struct work_struct *work)
+ {
+ 	struct mlx5e_tls_tx_pool *pool =
+ 		container_of(work, struct mlx5e_tls_tx_pool, destroy_work);
+ 	struct mlx5e_ktls_offload_context_tx *obj;
+ 	LIST_HEAD(local_list);
+ 	int i = 0;
+ 
+ 	mutex_lock(&pool->lock);
+ 	if (pool->size < MLX5E_TLS_TX_POOL_HIGH) {
+ 		mutex_unlock(&pool->lock);
+ 		return;
+ 	}
+ 
+ 	list_for_each_entry(obj, &pool->list, list_node)
+ 		if (++i == MLX5E_TLS_TX_POOL_BULK)
+ 			break;
+ 
+ 	list_cut_position(&local_list, &pool->list, &obj->list_node);
+ 	pool->size -= MLX5E_TLS_TX_POOL_BULK;
+ 	if (pool->size >= MLX5E_TLS_TX_POOL_HIGH)
+ 		queue_work(pool->wq, work);
+ 	mutex_unlock(&pool->lock);
+ 
+ 	mlx5e_tls_priv_tx_list_cleanup(pool->mdev, &local_list, MLX5E_TLS_TX_POOL_BULK);
+ 	atomic64_add(MLX5E_TLS_TX_POOL_BULK, &pool->sw_stats->tx_tls_pool_free);
+ }
+ 
+ static struct mlx5e_tls_tx_pool *mlx5e_tls_tx_pool_init(struct mlx5_core_dev *mdev,
+ 							struct mlx5e_tls_sw_stats *sw_stats)
+ {
+ 	struct mlx5e_tls_tx_pool *pool;
+ 
+ 	BUILD_BUG_ON(MLX5E_TLS_TX_POOL_LOW + MLX5E_TLS_TX_POOL_BULK >= MLX5E_TLS_TX_POOL_HIGH);
+ 
+ 	pool = kvzalloc(sizeof(*pool), GFP_KERNEL);
+ 	if (!pool)
+ 		return NULL;
+ 
+ 	pool->wq = create_singlethread_workqueue("mlx5e_tls_tx_pool");
+ 	if (!pool->wq)
+ 		goto err_free;
+ 
+ 	INIT_LIST_HEAD(&pool->list);
+ 	mutex_init(&pool->lock);
+ 
+ 	INIT_WORK(&pool->create_work, create_work);
+ 	INIT_WORK(&pool->destroy_work, destroy_work);
+ 
+ 	pool->mdev = mdev;
+ 	pool->sw_stats = sw_stats;
+ 
+ 	return pool;
+ 
+ err_free:
+ 	kvfree(pool);
+ 	return NULL;
+ }
+ 
+ static void mlx5e_tls_tx_pool_list_cleanup(struct mlx5e_tls_tx_pool *pool)
+ {
+ 	while (pool->size > MLX5E_TLS_TX_POOL_BULK) {
+ 		struct mlx5e_ktls_offload_context_tx *obj;
+ 		LIST_HEAD(local_list);
+ 		int i = 0;
+ 
+ 		list_for_each_entry(obj, &pool->list, list_node)
+ 			if (++i == MLX5E_TLS_TX_POOL_BULK)
+ 				break;
+ 
+ 		list_cut_position(&local_list, &pool->list, &obj->list_node);
+ 		mlx5e_tls_priv_tx_list_cleanup(pool->mdev, &local_list, MLX5E_TLS_TX_POOL_BULK);
+ 		atomic64_add(MLX5E_TLS_TX_POOL_BULK, &pool->sw_stats->tx_tls_pool_free);
+ 		pool->size -= MLX5E_TLS_TX_POOL_BULK;
+ 	}
+ 	if (pool->size) {
+ 		mlx5e_tls_priv_tx_list_cleanup(pool->mdev, &pool->list, pool->size);
+ 		atomic64_add(pool->size, &pool->sw_stats->tx_tls_pool_free);
+ 	}
+ }
+ 
+ static void mlx5e_tls_tx_pool_cleanup(struct mlx5e_tls_tx_pool *pool)
+ {
+ 	mlx5e_tls_tx_pool_list_cleanup(pool);
+ 	destroy_workqueue(pool->wq);
+ 	kvfree(pool);
+ }
+ 
+ static void pool_push(struct mlx5e_tls_tx_pool *pool, struct mlx5e_ktls_offload_context_tx *obj)
+ {
+ 	mutex_lock(&pool->lock);
+ 	list_add(&obj->list_node, &pool->list);
+ 	if (++pool->size == MLX5E_TLS_TX_POOL_HIGH)
+ 		queue_work(pool->wq, &pool->destroy_work);
+ 	mutex_unlock(&pool->lock);
+ }
+ 
+ static struct mlx5e_ktls_offload_context_tx *pool_pop(struct mlx5e_tls_tx_pool *pool)
+ {
+ 	struct mlx5e_ktls_offload_context_tx *obj;
+ 
+ 	mutex_lock(&pool->lock);
+ 	if (unlikely(pool->size == 0)) {
+ 		/* pool is empty:
+ 		 * - trigger the populating work, and
+ 		 * - serve the current context via the regular blocking api.
+ 		 */
+ 		queue_work(pool->wq, &pool->create_work);
+ 		mutex_unlock(&pool->lock);
+ 		obj = mlx5e_tls_priv_tx_init(pool->mdev, pool->sw_stats, NULL);
+ 		if (!IS_ERR(obj))
+ 			atomic64_inc(&pool->sw_stats->tx_tls_pool_alloc);
+ 		return obj;
+ 	}
+ 
+ 	obj = list_first_entry(&pool->list, struct mlx5e_ktls_offload_context_tx,
+ 			       list_node);
+ 	list_del(&obj->list_node);
+ 	if (--pool->size == MLX5E_TLS_TX_POOL_LOW)
+ 		queue_work(pool->wq, &pool->create_work);
+ 	mutex_unlock(&pool->lock);
+ 	return obj;
+ }
+ 
+ /* End of pool API */
+ 
++>>>>>>> 6514210b6d0d (net/mlx5e: kTLS, Use _safe() iterator in mlx5e_tls_priv_tx_list_cleanup())
  int mlx5e_ktls_add_tx(struct net_device *netdev, struct sock *sk,
  		      struct tls_crypto_info *crypto_info, u32 start_offload_tcp_sn)
  {
* Unmerged path drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c
