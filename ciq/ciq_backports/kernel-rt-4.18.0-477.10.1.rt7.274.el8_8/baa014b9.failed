perf/x86/amd: Fix crash due to race between amd_pmu_enable_all, perf NMI and throttling

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Ravi Bangoria <ravi.bangoria@amd.com>
commit baa014b9543c8e5e94f5d15b66abfe60750b8284
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/baa014b9.failed

amd_pmu_enable_all() does:

      if (!test_bit(idx, cpuc->active_mask))
              continue;

      amd_pmu_enable_event(cpuc->events[idx]);

A perf NMI of another event can come between these two steps. Perf NMI
handler internally disables and enables _all_ events, including the one
which nmi-intercepted amd_pmu_enable_all() was in process of enabling.
If that unintentionally enabled event has very low sampling period and
causes immediate successive NMI, causing the event to be throttled,
cpuc->events[idx] and cpuc->active_mask gets cleared by x86_pmu_stop().
This will result in amd_pmu_enable_event() getting called with event=NULL
when amd_pmu_enable_all() resumes after handling the NMIs. This causes a
kernel crash:

  BUG: kernel NULL pointer dereference, address: 0000000000000198
  #PF: supervisor read access in kernel mode
  #PF: error_code(0x0000) - not-present page
  [...]
  Call Trace:
   <TASK>
   amd_pmu_enable_all+0x68/0xb0
   ctx_resched+0xd9/0x150
   event_function+0xb8/0x130
   ? hrtimer_start_range_ns+0x141/0x4a0
   ? perf_duration_warn+0x30/0x30
   remote_function+0x4d/0x60
   __flush_smp_call_function_queue+0xc4/0x500
   flush_smp_call_function_queue+0x11d/0x1b0
   do_idle+0x18f/0x2d0
   cpu_startup_entry+0x19/0x20
   start_secondary+0x121/0x160
   secondary_startup_64_no_verify+0xe5/0xeb
   </TASK>

amd_pmu_disable_all()/amd_pmu_enable_all() calls inside perf NMI handler
were recently added as part of BRS enablement but I'm not sure whether
we really need them. We can just disable BRS in the beginning and enable
it back while returning from NMI. This will solve the issue by not
enabling those events whose active_masks are set but are not yet enabled
in hw pmu.

Fixes: ada543459cab ("perf/x86/amd: Add AMD Fam19h Branch Sampling support")
	Reported-by: Linux Kernel Functional Testing <lkft@linaro.org>
	Signed-off-by: Ravi Bangoria <ravi.bangoria@amd.com>
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lkml.kernel.org/r/20221114044029.373-1-ravi.bangoria@amd.com
(cherry picked from commit baa014b9543c8e5e94f5d15b66abfe60750b8284)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/amd/core.c
diff --cc arch/x86/events/amd/core.c
index dbe0c295e628,d6f3703e4119..000000000000
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@@ -691,6 -848,123 +691,126 @@@ static int amd_pmu_handle_irq(struct pt
  	return NMI_HANDLED;
  }
  
++<<<<<<< HEAD
++=======
+ static int amd_pmu_handle_irq(struct pt_regs *regs)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 	int handled;
+ 	int pmu_enabled;
+ 
+ 	/*
+ 	 * Save the PMU state.
+ 	 * It needs to be restored when leaving the handler.
+ 	 */
+ 	pmu_enabled = cpuc->enabled;
+ 	cpuc->enabled = 0;
+ 
+ 	amd_brs_disable_all();
+ 
+ 	/* Drain BRS is in use (could be inactive) */
+ 	if (cpuc->lbr_users)
+ 		amd_brs_drain();
+ 
+ 	/* Process any counter overflows */
+ 	handled = x86_pmu_handle_irq(regs);
+ 
+ 	cpuc->enabled = pmu_enabled;
+ 	if (pmu_enabled)
+ 		amd_brs_enable_all();
+ 
+ 	return amd_pmu_adjust_nmi_window(handled);
+ }
+ 
+ static int amd_pmu_v2_handle_irq(struct pt_regs *regs)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 	struct perf_sample_data data;
+ 	struct hw_perf_event *hwc;
+ 	struct perf_event *event;
+ 	int handled = 0, idx;
+ 	u64 status, mask;
+ 	bool pmu_enabled;
+ 
+ 	/*
+ 	 * Save the PMU state as it needs to be restored when leaving the
+ 	 * handler
+ 	 */
+ 	pmu_enabled = cpuc->enabled;
+ 	cpuc->enabled = 0;
+ 
+ 	/* Stop counting but do not disable LBR */
+ 	amd_pmu_core_disable_all();
+ 
+ 	status = amd_pmu_get_global_status();
+ 
+ 	/* Check if any overflows are pending */
+ 	if (!status)
+ 		goto done;
+ 
+ 	/* Read branch records before unfreezing */
+ 	if (status & GLOBAL_STATUS_LBRS_FROZEN) {
+ 		amd_pmu_lbr_read();
+ 		status &= ~GLOBAL_STATUS_LBRS_FROZEN;
+ 	}
+ 
+ 	for (idx = 0; idx < x86_pmu.num_counters; idx++) {
+ 		if (!test_bit(idx, cpuc->active_mask))
+ 			continue;
+ 
+ 		event = cpuc->events[idx];
+ 		hwc = &event->hw;
+ 		x86_perf_event_update(event);
+ 		mask = BIT_ULL(idx);
+ 
+ 		if (!(status & mask))
+ 			continue;
+ 
+ 		/* Event overflow */
+ 		handled++;
+ 		perf_sample_data_init(&data, 0, hwc->last_period);
+ 
+ 		if (!x86_perf_event_set_period(event))
+ 			continue;
+ 
+ 		if (has_branch_stack(event)) {
+ 			data.br_stack = &cpuc->lbr_stack;
+ 			data.sample_flags |= PERF_SAMPLE_BRANCH_STACK;
+ 		}
+ 
+ 		if (perf_event_overflow(event, &data, regs))
+ 			x86_pmu_stop(event, 0);
+ 
+ 		status &= ~mask;
+ 	}
+ 
+ 	/*
+ 	 * It should never be the case that some overflows are not handled as
+ 	 * the corresponding PMCs are expected to be inactive according to the
+ 	 * active_mask
+ 	 */
+ 	WARN_ON(status > 0);
+ 
+ 	/* Clear overflow and freeze bits */
+ 	amd_pmu_ack_global_status(~status);
+ 
+ 	/*
+ 	 * Unmasking the LVTPC is not required as the Mask (M) bit of the LVT
+ 	 * PMI entry is not set by the local APIC when a PMC overflow occurs
+ 	 */
+ 	inc_irq_stat(apic_perf_irqs);
+ 
+ done:
+ 	cpuc->enabled = pmu_enabled;
+ 
+ 	/* Resume counting only if PMU is active */
+ 	if (pmu_enabled)
+ 		amd_pmu_core_enable_all();
+ 
+ 	return amd_pmu_adjust_nmi_window(handled);
+ }
+ 
++>>>>>>> baa014b9543c (perf/x86/amd: Fix crash due to race between amd_pmu_enable_all, perf NMI and throttling)
  static struct event_constraint *
  amd_get_event_constraints(struct cpu_hw_events *cpuc, int idx,
  			  struct perf_event *event)
* Unmerged path arch/x86/events/amd/core.c
