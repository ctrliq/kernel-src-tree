mm: allow a controlled amount of unfairness in the page lock

jira LE-1907
cve CVE-2022-3623
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Linus Torvalds <torvalds@linux-foundation.org>
commit 5ef64cc8987a9211d3f3667331ba3411a94ddc79
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/5ef64cc8.failed

Commit 2a9127fcf229 ("mm: rewrite wait_on_page_bit_common() logic") made
the page locking entirely fair, in that if a waiter came in while the
lock was held, the lock would be transferred to the lockers strictly in
order.

That was intended to finally get rid of the long-reported watchdog
failures that involved the page lock under extreme load, where a process
could end up waiting essentially forever, as other page lockers stole
the lock from under it.

It also improved some benchmarks, but it ended up causing huge
performance regressions on others, simply because fair lock behavior
doesn't end up giving out the lock as aggressively, causing better
worst-case latency, but potentially much worse average latencies and
throughput.

Instead of reverting that change entirely, this introduces a controlled
amount of unfairness, with a sysctl knob to tune it if somebody needs
to.  But the default value should hopefully be good for any normal load,
allowing a few rounds of lock stealing, but enforcing the strict
ordering before the lock has been stolen too many times.

There is also a hint from Matthieu Baerts that the fair page coloring
may end up exposing an ABBA deadlock that is hidden by the usual
optimistic lock stealing, and while the unfairness doesn't fix the
fundamental issue (and I'm still looking at that), it avoids it in
practice.

The amount of unfairness can be modified by writing a new value to the
'sysctl_page_lock_unfairness' variable (default value of 5, exposed
through /proc/sys/vm/page_lock_unfairness), but that is hopefully
something we'd use mainly for debugging rather than being necessary for
any deep system tuning.

This whole issue has exposed just how critical the page lock can be, and
how contended it gets under certain locks.  And the main contention
doesn't really seem to be anything related to IO (which was the origin
of this lock), but for things like just verifying that the page file
mapping is stable while faulting in the page into a page table.

Link: https://lore.kernel.org/linux-fsdevel/ed8442fd-6f54-dd84-cd4a-941e8b7ee603@MichaelLarabel.com/
Link: https://www.phoronix.com/scan.php?page=article&item=linux-50-59&num=1
Link: https://lore.kernel.org/linux-fsdevel/c560a38d-8313-51fb-b1ec-e904bd8836bc@tessares.net/
Reported-and-tested-by: Michael Larabel <Michael@michaellarabel.com>
	Tested-by: Matthieu Baerts <matthieu.baerts@tessares.net>
	Cc: Dave Chinner <david@fromorbit.com>
	Cc: Matthew Wilcox <willy@infradead.org>
	Cc: Chris Mason <clm@fb.com>
	Cc: Jan Kara <jack@suse.cz>
	Cc: Amir Goldstein <amir73il@gmail.com>
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit 5ef64cc8987a9211d3f3667331ba3411a94ddc79)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/wait.h
#	mm/filemap.c
diff --cc include/linux/wait.h
index 01d559739175,27fb99cfeb02..000000000000
--- a/include/linux/wait.h
+++ b/include/linux/wait.h
@@@ -21,7 -21,7 +21,11 @@@ int default_wake_function(struct wait_q
  #define WQ_FLAG_WOKEN		0x02
  #define WQ_FLAG_BOOKMARK	0x04
  #define WQ_FLAG_CUSTOM		0x08
++<<<<<<< HEAD
 +#define WQ_FLAG_PRIORITY	0x20
++=======
+ #define WQ_FLAG_DONE		0x10
++>>>>>>> 5ef64cc8987a (mm: allow a controlled amount of unfairness in the page lock)
  
  /*
   * A single wait-queue entry structure:
diff --cc mm/filemap.c
index 931209c1c503,6aa08e7714ce..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -1026,22 -988,43 +1026,59 @@@ void __init pagecache_init(void
  	page_writeback_init();
  }
  
++<<<<<<< HEAD
 +/* This has the same layout as wait_bit_key - see fs/cachefiles/rdwr.c */
 +struct wait_page_key {
 +	struct page *page;
 +	int bit_nr;
 +	int page_match;
 +};
 +
 +struct wait_page_queue {
 +	struct page *page;
 +	int bit_nr;
 +	wait_queue_entry_t wait;
 +};
 +
++=======
+ /*
+  * The page wait code treats the "wait->flags" somewhat unusually, because
+  * we have multiple different kinds of waits, not just he usual "exclusive"
+  * one.
+  *
+  * We have:
+  *
+  *  (a) no special bits set:
+  *
+  *	We're just waiting for the bit to be released, and when a waker
+  *	calls the wakeup function, we set WQ_FLAG_WOKEN and wake it up,
+  *	and remove it from the wait queue.
+  *
+  *	Simple and straightforward.
+  *
+  *  (b) WQ_FLAG_EXCLUSIVE:
+  *
+  *	The waiter is waiting to get the lock, and only one waiter should
+  *	be woken up to avoid any thundering herd behavior. We'll set the
+  *	WQ_FLAG_WOKEN bit, wake it up, and remove it from the wait queue.
+  *
+  *	This is the traditional exclusive wait.
+  *
+  *  (b) WQ_FLAG_EXCLUSIVE | WQ_FLAG_CUSTOM:
+  *
+  *	The waiter is waiting to get the bit, and additionally wants the
+  *	lock to be transferred to it for fair lock behavior. If the lock
+  *	cannot be taken, we stop walking the wait queue without waking
+  *	the waiter.
+  *
+  *	This is the "fair lock handoff" case, and in addition to setting
+  *	WQ_FLAG_WOKEN, we set WQ_FLAG_DONE to let the waiter easily see
+  *	that it now has the lock.
+  */
++>>>>>>> 5ef64cc8987a (mm: allow a controlled amount of unfairness in the page lock)
  static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)
  {
- 	int ret;
+ 	unsigned int flags;
  	struct wait_page_key *key = arg;
  	struct wait_page_queue *wait_page
  		= container_of(wait, struct wait_page_queue, wait);
@@@ -1076,18 -1063,14 +1121,24 @@@
  	 * Ok, we have successfully done what we're waiting for,
  	 * and we can unconditionally remove the wait entry.
  	 *
- 	 * Note that this has to be the absolute last thing we do,
- 	 * since after list_del_init(&wait->entry) the wait entry
+ 	 * Note that this pairs with the "finish_wait()" in the
+ 	 * waiter, and has to be the absolute last thing we do.
+ 	 * After this list_del_init(&wait->entry) the wait entry
  	 * might be de-allocated and the process might even have
  	 * exited.
 +	 *
 +	 * We _really_ should have a "list_del_init_careful()" to
 +	 * properly pair with the unlocked "list_empty_careful()"
 +	 * in finish_wait().
  	 */
++<<<<<<< HEAD
 +	smp_mb();
 +	list_del_init(&wait->entry);
 +	return ret;
++=======
+ 	list_del_init_careful(&wait->entry);
+ 	return (flags & WQ_FLAG_EXCLUSIVE) != 0;
++>>>>>>> 5ef64cc8987a (mm: allow a controlled amount of unfairness in the page lock)
  }
  
  static void wake_up_page_bit(struct page *page, int bit_nr)
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 7fe72b43d151..abaf48b06f5b 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -42,6 +42,8 @@ struct writeback_control;
 struct bdi_writeback;
 struct pt_regs;
 
+extern int sysctl_page_lock_unfairness;
+
 void init_mm_internals(void);
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES	/* Don't use mapnrs, do it properly */
* Unmerged path include/linux/wait.h
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 73c099c9bd4a..6aaaacbc3f7a 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1552,6 +1552,14 @@ static struct ctl_table vm_table[] = {
 		.proc_handler	= percpu_pagelist_fraction_sysctl_handler,
 		.extra1		= SYSCTL_ZERO,
 	},
+	{
+		.procname	= "page_lock_unfairness",
+		.data		= &sysctl_page_lock_unfairness,
+		.maxlen		= sizeof(sysctl_page_lock_unfairness),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+	},
 #ifdef CONFIG_MMU
 	{
 		.procname	= "max_map_count",
* Unmerged path mm/filemap.c
