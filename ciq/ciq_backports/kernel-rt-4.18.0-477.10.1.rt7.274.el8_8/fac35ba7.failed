mm/hugetlb: fix races when looking up a CONT-PTE/PMD size hugetlb page

jira LE-1907
cve CVE-2022-3623
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Baolin Wang <baolin.wang@linux.alibaba.com>
commit fac35ba763ed07ba93154c95ffc0c4a55023707f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/fac35ba7.failed

On some architectures (like ARM64), it can support CONT-PTE/PMD size
hugetlb, which means it can support not only PMD/PUD size hugetlb (2M and
1G), but also CONT-PTE/PMD size(64K and 32M) if a 4K page size specified.

So when looking up a CONT-PTE size hugetlb page by follow_page(), it will
use pte_offset_map_lock() to get the pte entry lock for the CONT-PTE size
hugetlb in follow_page_pte().  However this pte entry lock is incorrect
for the CONT-PTE size hugetlb, since we should use huge_pte_lock() to get
the correct lock, which is mm->page_table_lock.

That means the pte entry of the CONT-PTE size hugetlb under current pte
lock is unstable in follow_page_pte(), we can continue to migrate or
poison the pte entry of the CONT-PTE size hugetlb, which can cause some
potential race issues, even though they are under the 'pte lock'.

For example, suppose thread A is trying to look up a CONT-PTE size hugetlb
page by move_pages() syscall under the lock, however antoher thread B can
migrate the CONT-PTE hugetlb page at the same time, which will cause
thread A to get an incorrect page, if thread A also wants to do page
migration, then data inconsistency error occurs.

Moreover we have the same issue for CONT-PMD size hugetlb in
follow_huge_pmd().

To fix above issues, rename the follow_huge_pmd() as follow_huge_pmd_pte()
to handle PMD and PTE level size hugetlb, which uses huge_pte_lock() to
get the correct pte entry lock to make the pte entry stable.

Mike said:

Support for CONT_PMD/_PTE was added with bb9dd3df8ee9 ("arm64: hugetlb:
refactor find_num_contig()").  Patch series "Support for contiguous pte
hugepages", v4.  However, I do not believe these code paths were
executed until migration support was added with 5480280d3f2d ("arm64/mm:
enable HugeTLB migration for contiguous bit HugeTLB pages") I would go
with 5480280d3f2d for the Fixes: targe.

Link: https://lkml.kernel.org/r/635f43bdd85ac2615a58405da82b4d33c6e5eb05.1662017562.git.baolin.wang@linux.alibaba.com
Fixes: 5480280d3f2d ("arm64/mm: enable HugeTLB migration for contiguous bit HugeTLB pages")
	Signed-off-by: Baolin Wang <baolin.wang@linux.alibaba.com>
	Suggested-by: Mike Kravetz <mike.kravetz@oracle.com>
	Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: Muchun Song <songmuchun@bytedance.com>
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit fac35ba763ed07ba93154c95ffc0c4a55023707f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	mm/hugetlb.c
diff --cc mm/hugetlb.c
index 8ea00c3bd45c,9564bf817e6a..000000000000
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@@ -5665,46 -6946,31 +5665,49 @@@ follow_huge_pd(struct vm_area_struct *v
  }
  
  struct page * __weak
- follow_huge_pmd(struct mm_struct *mm, unsigned long address,
- 		pmd_t *pmd, int flags)
+ follow_huge_pmd_pte(struct vm_area_struct *vma, unsigned long address, int flags)
  {
+ 	struct hstate *h = hstate_vma(vma);
+ 	struct mm_struct *mm = vma->vm_mm;
  	struct page *page = NULL;
  	spinlock_t *ptl;
- 	pte_t pte;
+ 	pte_t *ptep, pte;
  
 -	/*
 -	 * FOLL_PIN is not supported for follow_page(). Ordinary GUP goes via
 -	 * follow_hugetlb_page().
 -	 */
 -	if (WARN_ON_ONCE(flags & FOLL_PIN))
 +	/* FOLL_GET and FOLL_PIN are mutually exclusive. */
 +	if (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==
 +			 (FOLL_PIN | FOLL_GET)))
  		return NULL;
  
  retry:
- 	ptl = pmd_lockptr(mm, pmd);
- 	spin_lock(ptl);
- 	/*
- 	 * make sure that the address range covered by this pmd is not
- 	 * unmapped from other threads.
- 	 */
- 	if (!pmd_huge(*pmd))
- 		goto out;
- 	pte = huge_ptep_get((pte_t *)pmd);
+ 	ptep = huge_pte_offset(mm, address, huge_page_size(h));
+ 	if (!ptep)
+ 		return NULL;
+ 
+ 	ptl = huge_pte_lock(h, mm, ptep);
+ 	pte = huge_ptep_get(ptep);
  	if (pte_present(pte)) {
++<<<<<<< HEAD
 +		struct page *head_page = pmd_page(*pmd);
 +		/*
 +		 * gup_must_unshare() isn't strictly needed in
 +		 * follow_page() as long as all follow_page() users
 +		 * never can expose the page payload to userland or
 +		 * devices. follow_page_mask() isn't invoked by the
 +		 * hugetlb paths (hugetlbfs goes through
 +		 * follow_hugetlb_page() instead of
 +		 * follow_page_mask()). So the gup_must_unshare()
 +		 * check here is just in case.
 +		 */
 +		if (!huge_pte_write(pte) &&
 +		    gup_must_unshare(flags, head_page, true)) {
 +			page = NULL;
 +			goto out;
 +		}
 +		page = head_page + ((address & ~PMD_MASK) >> PAGE_SHIFT);
++=======
+ 		page = pte_page(pte) +
+ 			((address & ~huge_page_mask(h)) >> PAGE_SHIFT);
++>>>>>>> fac35ba763ed (mm/hugetlb: fix races when looking up a CONT-PTE/PMD size hugetlb page)
  		/*
  		 * try_grab_page() should always succeed here, because: a) we
  		 * hold the pmd (ptl) lock, and b) we've just checked that the
@@@ -5720,7 -6986,7 +5723,11 @@@
  	} else {
  		if (is_hugetlb_entry_migration(pte)) {
  			spin_unlock(ptl);
++<<<<<<< HEAD
 +			__migration_entry_wait(mm, (pte_t *)pmd, ptl);
++=======
+ 			__migration_entry_wait_huge(ptep, ptl);
++>>>>>>> fac35ba763ed (mm/hugetlb: fix races when looking up a CONT-PTE/PMD size hugetlb page)
  			goto retry;
  		}
  		/*
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index efcae4e1f98e..bd627f70e04c 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -175,8 +175,8 @@ struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 struct page *follow_huge_pd(struct vm_area_struct *vma,
 			    unsigned long address, hugepd_t hpd,
 			    int flags, int pdshift);
-struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
-				pmd_t *pmd, int flags);
+struct page *follow_huge_pmd_pte(struct vm_area_struct *vma, unsigned long address,
+				 int flags);
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
 				pud_t *pud, int flags);
 struct page *follow_huge_pgd(struct mm_struct *mm, unsigned long address,
@@ -262,8 +262,8 @@ static inline struct page *follow_huge_pd(struct vm_area_struct *vma,
 	return NULL;
 }
 
-static inline struct page *follow_huge_pmd(struct mm_struct *mm,
-				unsigned long address, pmd_t *pmd, int flags)
+static inline struct page *follow_huge_pmd_pte(struct vm_area_struct *vma,
+				unsigned long address, int flags)
 {
 	return NULL;
 }
diff --git a/mm/gup.c b/mm/gup.c
index 059921e45c37..5ecaf24a4019 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -598,6 +598,18 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
 	if (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==
 			 (FOLL_PIN | FOLL_GET)))
 		return ERR_PTR(-EINVAL);
+
+	/*
+	 * Considering PTE level hugetlb, like continuous-PTE hugetlb on
+	 * ARM64 architecture.
+	 */
+	if (is_vm_hugetlb_page(vma)) {
+		page = follow_huge_pmd_pte(vma, address, flags);
+		if (page)
+			return page;
+		return no_page_table(vma, flags);
+	}
+
 retry:
 	if (unlikely(pmd_bad(*pmd)))
 		return no_page_table(vma, flags);
@@ -768,7 +780,7 @@ static struct page *follow_pmd_mask(struct vm_area_struct *vma,
 	if (pmd_none(pmdval))
 		return no_page_table(vma, flags);
 	if (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {
-		page = follow_huge_pmd(mm, address, pmd, flags);
+		page = follow_huge_pmd_pte(vma, address, flags);
 		if (page)
 			return page;
 		return no_page_table(vma, flags);
* Unmerged path mm/hugetlb.c
