RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Aharon Landau <aharonl@nvidia.com>
commit b3d47ebd490823514a2d637caee0870b6f192b07
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/b3d47ebd.failed

Move mlx5_ib_update_mr_pas logic to umr.c, and use
mlx5_umr_post_send_wait() instead of mlx5_ib_post_send_wait().

Link: https://lore.kernel.org/r/ed8f2ee6c64804072155d727149abf7105f92536.1649747695.git.leonro@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Michael Guralnik <michaelgur@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit b3d47ebd490823514a2d637caee0870b6f192b07)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
#	drivers/infiniband/hw/mlx5/odp.c
#	drivers/infiniband/hw/mlx5/umr.c
#	drivers/infiniband/hw/mlx5/umr.h
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 1faf0a4d0186,df79fd5be5f2..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -1185,75 -1078,11 +1185,78 @@@ int mlx5_ib_update_xlt(struct mlx5_ib_m
  		err = mlx5_ib_post_send_wait(dev, &wr);
  	}
  	sg.length = orig_sg_length;
 -	mlx5r_umr_unmap_free_xlt(dev, xlt, &sg);
 +	mlx5_ib_unmap_free_xlt(dev, xlt, &sg);
 +	return err;
 +}
 +
 +/*
++<<<<<<< HEAD
 + * Send the DMA list to the HW for a normal MR using UMR.
 + */
 +static int mlx5_ib_update_mr_pas(struct mlx5_ib_mr *mr, unsigned int flags)
 +{
 +	struct mlx5_ib_dev *dev = mr_to_mdev(mr);
 +	struct device *ddev = &dev->mdev->pdev->dev;
 +	struct ib_block_iter biter;
 +	struct mlx5_mtt *cur_mtt;
 +	struct mlx5_umr_wr wr;
 +	size_t orig_sg_length;
 +	struct mlx5_mtt *mtt;
 +	size_t final_size;
 +	struct ib_sge sg;
 +	int err = 0;
 +
 +	if (WARN_ON(mr->umem->is_odp))
 +		return -EINVAL;
 +
 +	mtt = mlx5_ib_create_xlt_wr(mr, &wr, &sg,
 +				    ib_umem_num_dma_blocks(mr->umem,
 +							   1 << mr->page_shift),
 +				    sizeof(*mtt), flags);
 +	if (!mtt)
 +		return -ENOMEM;
 +	orig_sg_length = sg.length;
 +
 +	cur_mtt = mtt;
 +	rdma_for_each_block (mr->umem->sgt_append.sgt.sgl, &biter,
 +			     mr->umem->sgt_append.sgt.nents,
 +			     BIT(mr->page_shift)) {
 +		if (cur_mtt == (void *)mtt + sg.length) {
 +			dma_sync_single_for_device(ddev, sg.addr, sg.length,
 +						   DMA_TO_DEVICE);
 +			err = mlx5_ib_post_send_wait(dev, &wr);
 +			if (err)
 +				goto err;
 +			dma_sync_single_for_cpu(ddev, sg.addr, sg.length,
 +						DMA_TO_DEVICE);
 +			wr.offset += sg.length;
 +			cur_mtt = mtt;
 +		}
 +
 +		cur_mtt->ptag =
 +			cpu_to_be64(rdma_block_iter_dma_address(&biter) |
 +				    MLX5_IB_MTT_PRESENT);
 +		cur_mtt++;
 +	}
 +
 +	final_size = (void *)cur_mtt - (void *)mtt;
 +	sg.length = ALIGN(final_size, MLX5_UMR_MTT_ALIGNMENT);
 +	memset(cur_mtt, 0, sg.length - final_size);
 +	wr.wr.send_flags |= xlt_wr_final_send_flags(flags);
 +	wr.xlt_size = sg.length;
 +
 +	dma_sync_single_for_device(ddev, sg.addr, sg.length, DMA_TO_DEVICE);
 +	err = mlx5_ib_post_send_wait(dev, &wr);
 +
 +err:
 +	sg.length = orig_sg_length;
 +	mlx5_ib_unmap_free_xlt(dev, mtt, &sg);
  	return err;
  }
  
  /*
++=======
++>>>>>>> b3d47ebd4908 (RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas)
   * If ibmr is NULL it will be allocated by reg_create.
   * Else, the given ibmr will be used.
   */
@@@ -1528,18 -1298,7 +1531,22 @@@ struct ib_mr *mlx5_ib_reg_user_mr(struc
  		 * configured properly but left disabled. It is safe to go ahead
  		 * and configure it again via UMR while enabling it.
  		 */
++<<<<<<< HEAD
 +		int update_xlt_flags = MLX5_IB_UPD_XLT_ENABLE;
 +
 +		if (access_flags & IB_ACCESS_ON_DEMAND)
 +			update_xlt_flags |= MLX5_IB_UPD_XLT_ZAP;
 +
 +		if (access_flags & IB_ACCESS_ON_DEMAND)
 +			err = mlx5_ib_update_xlt(
 +						 mr, 0,
 +						 ib_umem_num_dma_blocks(umem, 1UL << mr->page_shift),
 +						 mr->page_shift, update_xlt_flags);
 +		else
 +			err = mlx5_ib_update_mr_pas(mr, update_xlt_flags);
++=======
+ 		err = mlx5r_umr_update_mr_pas(mr, MLX5_IB_UPD_XLT_ENABLE);
++>>>>>>> b3d47ebd4908 (RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas)
  		if (err) {
  			mlx5_ib_dereg_mr(&mr->ibmr, NULL);
  			return ERR_PTR(err);
@@@ -1561,52 -1365,187 +1568,172 @@@ error
  	return ERR_PTR(err);
  }
  
++<<<<<<< HEAD
 +/**
 + * revoke_mr - Fence all DMA on the MR
 + * @mr: The MR to fence
 + *
 + * Upon return the NIC will not be doing any DMA to the pages under the MR,
 + * and any DMA in progress will be completed. Failure of this function
 + * indicates the HW has failed catastrophically.
++=======
+ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
+ 				  u64 iova, int access_flags,
+ 				  struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct ib_umem *umem;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	mlx5_ib_dbg(dev, "start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\n",
+ 		    start, iova, length, access_flags);
+ 
+ 	if (access_flags & IB_ACCESS_ON_DEMAND)
+ 		return create_user_odp_mr(pd, start, length, iova, access_flags,
+ 					  udata);
+ 	umem = ib_umem_get(&dev->ib_dev, start, length, access_flags);
+ 	if (IS_ERR(umem))
+ 		return ERR_CAST(umem);
+ 	return create_real_mr(pd, umem, iova, access_flags);
+ }
+ 
+ static void mlx5_ib_dmabuf_invalidate_cb(struct dma_buf_attachment *attach)
+ {
+ 	struct ib_umem_dmabuf *umem_dmabuf = attach->importer_priv;
+ 	struct mlx5_ib_mr *mr = umem_dmabuf->private;
+ 
+ 	dma_resv_assert_held(umem_dmabuf->attach->dmabuf->resv);
+ 
+ 	if (!umem_dmabuf->sgt)
+ 		return;
+ 
+ 	mlx5r_umr_update_mr_pas(mr, MLX5_IB_UPD_XLT_ZAP);
+ 	ib_umem_dmabuf_unmap_pages(umem_dmabuf);
+ }
+ 
+ static struct dma_buf_attach_ops mlx5_ib_dmabuf_attach_ops = {
+ 	.allow_peer2peer = 1,
+ 	.move_notify = mlx5_ib_dmabuf_invalidate_cb,
+ };
+ 
+ struct ib_mr *mlx5_ib_reg_user_mr_dmabuf(struct ib_pd *pd, u64 offset,
+ 					 u64 length, u64 virt_addr,
+ 					 int fd, int access_flags,
+ 					 struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct mlx5_ib_mr *mr = NULL;
+ 	struct ib_umem_dmabuf *umem_dmabuf;
+ 	int err;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM) ||
+ 	    !IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	mlx5_ib_dbg(dev,
+ 		    "offset 0x%llx, virt_addr 0x%llx, length 0x%llx, fd %d, access_flags 0x%x\n",
+ 		    offset, virt_addr, length, fd, access_flags);
+ 
+ 	/* dmabuf requires xlt update via umr to work. */
+ 	if (!mlx5r_umr_can_load_pas(dev, length))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	umem_dmabuf = ib_umem_dmabuf_get(&dev->ib_dev, offset, length, fd,
+ 					 access_flags,
+ 					 &mlx5_ib_dmabuf_attach_ops);
+ 	if (IS_ERR(umem_dmabuf)) {
+ 		mlx5_ib_dbg(dev, "umem_dmabuf get failed (%ld)\n",
+ 			    PTR_ERR(umem_dmabuf));
+ 		return ERR_CAST(umem_dmabuf);
+ 	}
+ 
+ 	mr = alloc_cacheable_mr(pd, &umem_dmabuf->umem, virt_addr,
+ 				access_flags);
+ 	if (IS_ERR(mr)) {
+ 		ib_umem_release(&umem_dmabuf->umem);
+ 		return ERR_CAST(mr);
+ 	}
+ 
+ 	mlx5_ib_dbg(dev, "mkey 0x%x\n", mr->mmkey.key);
+ 
+ 	atomic_add(ib_umem_num_pages(mr->umem), &dev->mdev->priv.reg_pages);
+ 	umem_dmabuf->private = mr;
+ 	err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 
+ 	err = mlx5_ib_init_dmabuf_mr(mr);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 	return &mr->ibmr;
+ 
+ err_dereg_mr:
+ 	mlx5_ib_dereg_mr(&mr->ibmr, NULL);
+ 	return ERR_PTR(err);
+ }
+ 
+ /*
+  * True if the change in access flags can be done via UMR, only some access
+  * flags can be updated.
++>>>>>>> b3d47ebd4908 (RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas)
   */
 -static bool can_use_umr_rereg_access(struct mlx5_ib_dev *dev,
 -				     unsigned int current_access_flags,
 -				     unsigned int target_access_flags)
 +static int revoke_mr(struct mlx5_ib_mr *mr)
  {
 -	unsigned int diffs = current_access_flags ^ target_access_flags;
 +	struct mlx5_umr_wr umrwr = {};
  
 -	if (diffs & ~(IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE |
 -		      IB_ACCESS_REMOTE_READ | IB_ACCESS_RELAXED_ORDERING))
 -		return false;
 -	return mlx5r_umr_can_reconfig(dev, current_access_flags,
 -				      target_access_flags);
 -}
 +	if (mr_to_mdev(mr)->mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR)
 +		return 0;
  
 -static bool can_use_umr_rereg_pas(struct mlx5_ib_mr *mr,
 -				  struct ib_umem *new_umem,
 -				  int new_access_flags, u64 iova,
 -				  unsigned long *page_size)
 -{
 -	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 +	umrwr.wr.send_flags = MLX5_IB_SEND_UMR_DISABLE_MR |
 +			      MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
 +	umrwr.wr.opcode = MLX5_IB_WR_UMR;
 +	umrwr.pd = mr_to_mdev(mr)->umrc.pd;
 +	umrwr.mkey = mr->mmkey.key;
 +	umrwr.ignore_free_state = 1;
  
 -	/* We only track the allocated sizes of MRs from the cache */
 -	if (!mr->cache_ent)
 -		return false;
 -	if (!mlx5r_umr_can_load_pas(dev, new_umem->length))
 -		return false;
 -
 -	*page_size =
 -		mlx5_umem_find_best_pgsz(new_umem, mkc, log_page_size, 0, iova);
 -	if (WARN_ON(!*page_size))
 -		return false;
 -	return (1ULL << mr->cache_ent->order) >=
 -	       ib_umem_num_dma_blocks(new_umem, *page_size);
 +	return mlx5_ib_post_send_wait(mr_to_mdev(mr), &umrwr);
  }
  
 -static int umr_rereg_pas(struct mlx5_ib_mr *mr, struct ib_pd *pd,
 -			 int access_flags, int flags, struct ib_umem *new_umem,
 -			 u64 iova, unsigned long page_size)
 +static int rereg_umr(struct ib_pd *pd, struct mlx5_ib_mr *mr,
 +		     int access_flags, int flags)
  {
 -	struct mlx5_ib_dev *dev = to_mdev(mr->ibmr.device);
 -	int upd_flags = MLX5_IB_UPD_XLT_ADDR | MLX5_IB_UPD_XLT_ENABLE;
 -	struct ib_umem *old_umem = mr->umem;
 +	struct mlx5_ib_dev *dev = to_mdev(pd->device);
 +	struct mlx5_umr_wr umrwr = {};
  	int err;
  
 -	/*
 -	 * To keep everything simple the MR is revoked before we start to mess
 -	 * with it. This ensure the change is atomic relative to any use of the
 -	 * MR.
 -	 */
 -	err = mlx5r_umr_revoke_mr(mr);
 -	if (err)
 -		return err;
 +	umrwr.wr.send_flags = MLX5_IB_SEND_UMR_FAIL_IF_FREE;
  
 -	if (flags & IB_MR_REREG_PD) {
 -		mr->ibmr.pd = pd;
 -		upd_flags |= MLX5_IB_UPD_XLT_PD;
 -	}
 -	if (flags & IB_MR_REREG_ACCESS) {
 -		mr->access_flags = access_flags;
 -		upd_flags |= MLX5_IB_UPD_XLT_ACCESS;
 +	umrwr.wr.opcode = MLX5_IB_WR_UMR;
 +	umrwr.mkey = mr->mmkey.key;
 +
 +	if (flags & IB_MR_REREG_PD || flags & IB_MR_REREG_ACCESS) {
 +		umrwr.pd = pd;
 +		umrwr.access_flags = access_flags;
 +		umrwr.wr.send_flags |= MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS;
  	}
  
++<<<<<<< HEAD
 +	err = mlx5_ib_post_send_wait(dev, &umrwr);
++=======
+ 	mr->ibmr.length = new_umem->length;
+ 	mr->ibmr.iova = iova;
+ 	mr->ibmr.length = new_umem->length;
+ 	mr->page_shift = order_base_2(page_size);
+ 	mr->umem = new_umem;
+ 	err = mlx5r_umr_update_mr_pas(mr, upd_flags);
+ 	if (err) {
+ 		/*
+ 		 * The MR is revoked at this point so there is no issue to free
+ 		 * new_umem.
+ 		 */
+ 		mr->umem = old_umem;
+ 		return err;
+ 	}
++>>>>>>> b3d47ebd4908 (RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas)
  
 -	atomic_sub(ib_umem_num_pages(old_umem), &dev->mdev->priv.reg_pages);
 -	ib_umem_release(old_umem);
 -	atomic_add(ib_umem_num_pages(new_umem), &dev->mdev->priv.reg_pages);
 -	return 0;
 +	return err;
  }
  
  struct ib_mr *mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
diff --cc drivers/infiniband/hw/mlx5/odp.c
index 446d722140da,c00e70e74d94..000000000000
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@@ -695,6 -690,44 +695,47 @@@ out
  	return ret;
  }
  
++<<<<<<< HEAD
++=======
+ static int pagefault_dmabuf_mr(struct mlx5_ib_mr *mr, size_t bcnt,
+ 			       u32 *bytes_mapped, u32 flags)
+ {
+ 	struct ib_umem_dmabuf *umem_dmabuf = to_ib_umem_dmabuf(mr->umem);
+ 	u32 xlt_flags = 0;
+ 	int err;
+ 	unsigned int page_size;
+ 
+ 	if (flags & MLX5_PF_FLAGS_ENABLE)
+ 		xlt_flags |= MLX5_IB_UPD_XLT_ENABLE;
+ 
+ 	dma_resv_lock(umem_dmabuf->attach->dmabuf->resv, NULL);
+ 	err = ib_umem_dmabuf_map_pages(umem_dmabuf);
+ 	if (err) {
+ 		dma_resv_unlock(umem_dmabuf->attach->dmabuf->resv);
+ 		return err;
+ 	}
+ 
+ 	page_size = mlx5_umem_find_best_pgsz(&umem_dmabuf->umem, mkc,
+ 					     log_page_size, 0,
+ 					     umem_dmabuf->umem.iova);
+ 	if (unlikely(page_size < PAGE_SIZE)) {
+ 		ib_umem_dmabuf_unmap_pages(umem_dmabuf);
+ 		err = -EINVAL;
+ 	} else {
+ 		err = mlx5r_umr_update_mr_pas(mr, xlt_flags);
+ 	}
+ 	dma_resv_unlock(umem_dmabuf->attach->dmabuf->resv);
+ 
+ 	if (err)
+ 		return err;
+ 
+ 	if (bytes_mapped)
+ 		*bytes_mapped += bcnt;
+ 
+ 	return ib_umem_num_pages(mr->umem);
+ }
+ 
++>>>>>>> b3d47ebd4908 (RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas)
  /*
   * Returns:
   *  -EFAULT: The io_virt->bcnt is not within the MR, it covers pages that are
diff --cc drivers/infiniband/hw/mlx5/umr.c
index 46eaf919eb49,35904f9aa178..000000000000
--- a/drivers/infiniband/hw/mlx5/umr.c
+++ b/drivers/infiniband/hw/mlx5/umr.c
@@@ -104,3 -236,425 +104,428 @@@ void mlx5r_umr_resource_cleanup(struct 
  	ib_free_cq(dev->umrc.cq);
  	ib_dealloc_pd(dev->umrc.pd);
  }
++<<<<<<< HEAD
++=======
+ 
+ static int mlx5r_umr_post_send(struct ib_qp *ibqp, u32 mkey, struct ib_cqe *cqe,
+ 			       struct mlx5r_umr_wqe *wqe, bool with_data)
+ {
+ 	unsigned int wqe_size =
+ 		with_data ? sizeof(struct mlx5r_umr_wqe) :
+ 			    sizeof(struct mlx5r_umr_wqe) -
+ 				    sizeof(struct mlx5_wqe_data_seg);
+ 	struct mlx5_ib_dev *dev = to_mdev(ibqp->device);
+ 	struct mlx5_core_dev *mdev = dev->mdev;
+ 	struct mlx5_ib_qp *qp = to_mqp(ibqp);
+ 	struct mlx5_wqe_ctrl_seg *ctrl;
+ 	union {
+ 		struct ib_cqe *ib_cqe;
+ 		u64 wr_id;
+ 	} id;
+ 	void *cur_edge, *seg;
+ 	unsigned long flags;
+ 	unsigned int idx;
+ 	int size, err;
+ 
+ 	if (unlikely(mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR))
+ 		return -EIO;
+ 
+ 	spin_lock_irqsave(&qp->sq.lock, flags);
+ 
+ 	err = mlx5r_begin_wqe(qp, &seg, &ctrl, &idx, &size, &cur_edge, 0,
+ 			      cpu_to_be32(mkey), false, false);
+ 	if (WARN_ON(err))
+ 		goto out;
+ 
+ 	qp->sq.wr_data[idx] = MLX5_IB_WR_UMR;
+ 
+ 	mlx5r_memcpy_send_wqe(&qp->sq, &cur_edge, &seg, &size, wqe, wqe_size);
+ 
+ 	id.ib_cqe = cqe;
+ 	mlx5r_finish_wqe(qp, ctrl, seg, size, cur_edge, idx, id.wr_id, 0,
+ 			 MLX5_FENCE_MODE_NONE, MLX5_OPCODE_UMR);
+ 
+ 	mlx5r_ring_db(qp, 1, ctrl);
+ 
+ out:
+ 	spin_unlock_irqrestore(&qp->sq.lock, flags);
+ 
+ 	return err;
+ }
+ 
+ static void mlx5r_umr_done(struct ib_cq *cq, struct ib_wc *wc)
+ {
+ 	struct mlx5_ib_umr_context *context =
+ 		container_of(wc->wr_cqe, struct mlx5_ib_umr_context, cqe);
+ 
+ 	context->status = wc->status;
+ 	complete(&context->done);
+ }
+ 
+ static inline void mlx5r_umr_init_context(struct mlx5r_umr_context *context)
+ {
+ 	context->cqe.done = mlx5r_umr_done;
+ 	init_completion(&context->done);
+ }
+ 
+ static int mlx5r_umr_post_send_wait(struct mlx5_ib_dev *dev, u32 mkey,
+ 				   struct mlx5r_umr_wqe *wqe, bool with_data)
+ {
+ 	struct umr_common *umrc = &dev->umrc;
+ 	struct mlx5r_umr_context umr_context;
+ 	int err;
+ 
+ 	err = umr_check_mkey_mask(dev, be64_to_cpu(wqe->ctrl_seg.mkey_mask));
+ 	if (WARN_ON(err))
+ 		return err;
+ 
+ 	mlx5r_umr_init_context(&umr_context);
+ 
+ 	down(&umrc->sem);
+ 	err = mlx5r_umr_post_send(umrc->qp, mkey, &umr_context.cqe, wqe,
+ 				  with_data);
+ 	if (err)
+ 		mlx5_ib_warn(dev, "UMR post send failed, err %d\n", err);
+ 	else {
+ 		wait_for_completion(&umr_context.done);
+ 		if (umr_context.status != IB_WC_SUCCESS) {
+ 			mlx5_ib_warn(dev, "reg umr failed (%u)\n",
+ 				     umr_context.status);
+ 			err = -EFAULT;
+ 		}
+ 	}
+ 	up(&umrc->sem);
+ 	return err;
+ }
+ 
+ /**
+  * mlx5r_umr_revoke_mr - Fence all DMA on the MR
+  * @mr: The MR to fence
+  *
+  * Upon return the NIC will not be doing any DMA to the pages under the MR,
+  * and any DMA in progress will be completed. Failure of this function
+  * indicates the HW has failed catastrophically.
+  */
+ int mlx5r_umr_revoke_mr(struct mlx5_ib_mr *mr)
+ {
+ 	struct mlx5_ib_dev *dev = mr_to_mdev(mr);
+ 	struct mlx5r_umr_wqe wqe = {};
+ 
+ 	if (dev->mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR)
+ 		return 0;
+ 
+ 	wqe.ctrl_seg.mkey_mask |= get_umr_update_pd_mask();
+ 	wqe.ctrl_seg.mkey_mask |= get_umr_disable_mr_mask();
+ 	wqe.ctrl_seg.flags |= MLX5_UMR_INLINE;
+ 
+ 	MLX5_SET(mkc, &wqe.mkey_seg, free, 1);
+ 	MLX5_SET(mkc, &wqe.mkey_seg, pd, to_mpd(dev->umrc.pd)->pdn);
+ 	MLX5_SET(mkc, &wqe.mkey_seg, qpn, 0xffffff);
+ 	MLX5_SET(mkc, &wqe.mkey_seg, mkey_7_0,
+ 		 mlx5_mkey_variant(mr->mmkey.key));
+ 
+ 	return mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe, false);
+ }
+ 
+ static void mlx5r_umr_set_access_flags(struct mlx5_ib_dev *dev,
+ 				       struct mlx5_mkey_seg *seg,
+ 				       unsigned int access_flags)
+ {
+ 	MLX5_SET(mkc, seg, a, !!(access_flags & IB_ACCESS_REMOTE_ATOMIC));
+ 	MLX5_SET(mkc, seg, rw, !!(access_flags & IB_ACCESS_REMOTE_WRITE));
+ 	MLX5_SET(mkc, seg, rr, !!(access_flags & IB_ACCESS_REMOTE_READ));
+ 	MLX5_SET(mkc, seg, lw, !!(access_flags & IB_ACCESS_LOCAL_WRITE));
+ 	MLX5_SET(mkc, seg, lr, 1);
+ 	MLX5_SET(mkc, seg, relaxed_ordering_write,
+ 		 !!(access_flags & IB_ACCESS_RELAXED_ORDERING));
+ 	MLX5_SET(mkc, seg, relaxed_ordering_read,
+ 		 !!(access_flags & IB_ACCESS_RELAXED_ORDERING));
+ }
+ 
+ int mlx5r_umr_rereg_pd_access(struct mlx5_ib_mr *mr, struct ib_pd *pd,
+ 			      int access_flags)
+ {
+ 	struct mlx5_ib_dev *dev = mr_to_mdev(mr);
+ 	struct mlx5r_umr_wqe wqe = {};
+ 	int err;
+ 
+ 	wqe.ctrl_seg.mkey_mask = get_umr_update_access_mask(dev);
+ 	wqe.ctrl_seg.mkey_mask |= get_umr_update_pd_mask();
+ 	wqe.ctrl_seg.flags = MLX5_UMR_CHECK_FREE;
+ 	wqe.ctrl_seg.flags |= MLX5_UMR_INLINE;
+ 
+ 	mlx5r_umr_set_access_flags(dev, &wqe.mkey_seg, access_flags);
+ 	MLX5_SET(mkc, &wqe.mkey_seg, pd, to_mpd(pd)->pdn);
+ 	MLX5_SET(mkc, &wqe.mkey_seg, qpn, 0xffffff);
+ 	MLX5_SET(mkc, &wqe.mkey_seg, mkey_7_0,
+ 		 mlx5_mkey_variant(mr->mmkey.key));
+ 
+ 	err = mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe, false);
+ 	if (err)
+ 		return err;
+ 
+ 	mr->access_flags = access_flags;
+ 	return 0;
+ }
+ 
+ #define MLX5_MAX_UMR_CHUNK                                                     \
+ 	((1 << (MLX5_MAX_UMR_SHIFT + 4)) - MLX5_UMR_MTT_ALIGNMENT)
+ #define MLX5_SPARE_UMR_CHUNK 0x10000
+ 
+ /*
+  * Allocate a temporary buffer to hold the per-page information to transfer to
+  * HW. For efficiency this should be as large as it can be, but buffer
+  * allocation failure is not allowed, so try smaller sizes.
+  */
+ static void *mlx5r_umr_alloc_xlt(size_t *nents, size_t ent_size, gfp_t gfp_mask)
+ {
+ 	const size_t xlt_chunk_align = MLX5_UMR_MTT_ALIGNMENT / ent_size;
+ 	size_t size;
+ 	void *res = NULL;
+ 
+ 	static_assert(PAGE_SIZE % MLX5_UMR_MTT_ALIGNMENT == 0);
+ 
+ 	/*
+ 	 * MLX5_IB_UPD_XLT_ATOMIC doesn't signal an atomic context just that the
+ 	 * allocation can't trigger any kind of reclaim.
+ 	 */
+ 	might_sleep();
+ 
+ 	gfp_mask |= __GFP_ZERO | __GFP_NORETRY;
+ 
+ 	/*
+ 	 * If the system already has a suitable high order page then just use
+ 	 * that, but don't try hard to create one. This max is about 1M, so a
+ 	 * free x86 huge page will satisfy it.
+ 	 */
+ 	size = min_t(size_t, ent_size * ALIGN(*nents, xlt_chunk_align),
+ 		     MLX5_MAX_UMR_CHUNK);
+ 	*nents = size / ent_size;
+ 	res = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,
+ 				       get_order(size));
+ 	if (res)
+ 		return res;
+ 
+ 	if (size > MLX5_SPARE_UMR_CHUNK) {
+ 		size = MLX5_SPARE_UMR_CHUNK;
+ 		*nents = size / ent_size;
+ 		res = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,
+ 					       get_order(size));
+ 		if (res)
+ 			return res;
+ 	}
+ 
+ 	*nents = PAGE_SIZE / ent_size;
+ 	res = (void *)__get_free_page(gfp_mask);
+ 	if (res)
+ 		return res;
+ 
+ 	mutex_lock(&xlt_emergency_page_mutex);
+ 	memset(xlt_emergency_page, 0, PAGE_SIZE);
+ 	return xlt_emergency_page;
+ }
+ 
+ static void mlx5r_umr_free_xlt(void *xlt, size_t length)
+ {
+ 	if (xlt == xlt_emergency_page) {
+ 		mutex_unlock(&xlt_emergency_page_mutex);
+ 		return;
+ 	}
+ 
+ 	free_pages((unsigned long)xlt, get_order(length));
+ }
+ 
+ void mlx5r_umr_unmap_free_xlt(struct mlx5_ib_dev *dev, void *xlt,
+ 			     struct ib_sge *sg)
+ {
+ 	struct device *ddev = &dev->mdev->pdev->dev;
+ 
+ 	dma_unmap_single(ddev, sg->addr, sg->length, DMA_TO_DEVICE);
+ 	mlx5r_umr_free_xlt(xlt, sg->length);
+ }
+ 
+ /*
+  * Create an XLT buffer ready for submission.
+  */
+ void *mlx5r_umr_create_xlt(struct mlx5_ib_dev *dev, struct ib_sge *sg,
+ 			  size_t nents, size_t ent_size, unsigned int flags)
+ {
+ 	struct device *ddev = &dev->mdev->pdev->dev;
+ 	dma_addr_t dma;
+ 	void *xlt;
+ 
+ 	xlt = mlx5r_umr_alloc_xlt(&nents, ent_size,
+ 				 flags & MLX5_IB_UPD_XLT_ATOMIC ? GFP_ATOMIC :
+ 								  GFP_KERNEL);
+ 	sg->length = nents * ent_size;
+ 	dma = dma_map_single(ddev, xlt, sg->length, DMA_TO_DEVICE);
+ 	if (dma_mapping_error(ddev, dma)) {
+ 		mlx5_ib_err(dev, "unable to map DMA during XLT update.\n");
+ 		mlx5r_umr_free_xlt(xlt, sg->length);
+ 		return NULL;
+ 	}
+ 	sg->addr = dma;
+ 	sg->lkey = dev->umrc.pd->local_dma_lkey;
+ 
+ 	return xlt;
+ }
+ 
+ static void
+ mlx5r_umr_set_update_xlt_ctrl_seg(struct mlx5_wqe_umr_ctrl_seg *ctrl_seg,
+ 				  unsigned int flags, struct ib_sge *sg)
+ {
+ 	if (!(flags & MLX5_IB_UPD_XLT_ENABLE))
+ 		/* fail if free */
+ 		ctrl_seg->flags = MLX5_UMR_CHECK_FREE;
+ 	else
+ 		/* fail if not free */
+ 		ctrl_seg->flags = MLX5_UMR_CHECK_NOT_FREE;
+ 	ctrl_seg->xlt_octowords =
+ 		cpu_to_be16(mlx5r_umr_get_xlt_octo(sg->length));
+ }
+ 
+ static void mlx5r_umr_set_update_xlt_mkey_seg(struct mlx5_ib_dev *dev,
+ 					      struct mlx5_mkey_seg *mkey_seg,
+ 					      struct mlx5_ib_mr *mr,
+ 					      unsigned int page_shift)
+ {
+ 	mlx5r_umr_set_access_flags(dev, mkey_seg, mr->access_flags);
+ 	MLX5_SET(mkc, mkey_seg, pd, to_mpd(mr->ibmr.pd)->pdn);
+ 	MLX5_SET64(mkc, mkey_seg, start_addr, mr->ibmr.iova);
+ 	MLX5_SET64(mkc, mkey_seg, len, mr->ibmr.length);
+ 	MLX5_SET(mkc, mkey_seg, log_page_size, page_shift);
+ 	MLX5_SET(mkc, mkey_seg, qpn, 0xffffff);
+ 	MLX5_SET(mkc, mkey_seg, mkey_7_0, mlx5_mkey_variant(mr->mmkey.key));
+ }
+ 
+ static void
+ mlx5r_umr_set_update_xlt_data_seg(struct mlx5_wqe_data_seg *data_seg,
+ 				  struct ib_sge *sg)
+ {
+ 	data_seg->byte_count = cpu_to_be32(sg->length);
+ 	data_seg->lkey = cpu_to_be32(sg->lkey);
+ 	data_seg->addr = cpu_to_be64(sg->addr);
+ }
+ 
+ static void mlx5r_umr_update_offset(struct mlx5_wqe_umr_ctrl_seg *ctrl_seg,
+ 				    u64 offset)
+ {
+ 	u64 octo_offset = mlx5r_umr_get_xlt_octo(offset);
+ 
+ 	ctrl_seg->xlt_offset = cpu_to_be16(octo_offset & 0xffff);
+ 	ctrl_seg->xlt_offset_47_16 = cpu_to_be32(octo_offset >> 16);
+ 	ctrl_seg->flags |= MLX5_UMR_TRANSLATION_OFFSET_EN;
+ }
+ 
+ static void mlx5r_umr_final_update_xlt(struct mlx5_ib_dev *dev,
+ 				       struct mlx5r_umr_wqe *wqe,
+ 				       struct mlx5_ib_mr *mr, struct ib_sge *sg,
+ 				       unsigned int flags)
+ {
+ 	bool update_pd_access, update_translation;
+ 
+ 	if (flags & MLX5_IB_UPD_XLT_ENABLE)
+ 		wqe->ctrl_seg.mkey_mask |= get_umr_enable_mr_mask();
+ 
+ 	update_pd_access = flags & MLX5_IB_UPD_XLT_ENABLE ||
+ 			   flags & MLX5_IB_UPD_XLT_PD ||
+ 			   flags & MLX5_IB_UPD_XLT_ACCESS;
+ 
+ 	if (update_pd_access) {
+ 		wqe->ctrl_seg.mkey_mask |= get_umr_update_access_mask(dev);
+ 		wqe->ctrl_seg.mkey_mask |= get_umr_update_pd_mask();
+ 	}
+ 
+ 	update_translation =
+ 		flags & MLX5_IB_UPD_XLT_ENABLE || flags & MLX5_IB_UPD_XLT_ADDR;
+ 
+ 	if (update_translation) {
+ 		wqe->ctrl_seg.mkey_mask |= get_umr_update_translation_mask();
+ 		if (!mr->ibmr.length)
+ 			MLX5_SET(mkc, &wqe->mkey_seg, length64, 1);
+ 	}
+ 
+ 	wqe->ctrl_seg.xlt_octowords =
+ 		cpu_to_be16(mlx5r_umr_get_xlt_octo(sg->length));
+ 	wqe->data_seg.byte_count = cpu_to_be32(sg->length);
+ }
+ 
+ /*
+  * Send the DMA list to the HW for a normal MR using UMR.
+  * Dmabuf MR is handled in a similar way, except that the MLX5_IB_UPD_XLT_ZAP
+  * flag may be used.
+  */
+ int mlx5r_umr_update_mr_pas(struct mlx5_ib_mr *mr, unsigned int flags)
+ {
+ 	struct mlx5_ib_dev *dev = mr_to_mdev(mr);
+ 	struct device *ddev = &dev->mdev->pdev->dev;
+ 	struct mlx5r_umr_wqe wqe = {};
+ 	struct ib_block_iter biter;
+ 	struct mlx5_mtt *cur_mtt;
+ 	size_t orig_sg_length;
+ 	struct mlx5_mtt *mtt;
+ 	size_t final_size;
+ 	struct ib_sge sg;
+ 	u64 offset = 0;
+ 	int err = 0;
+ 
+ 	if (WARN_ON(mr->umem->is_odp))
+ 		return -EINVAL;
+ 
+ 	mtt = mlx5r_umr_create_xlt(
+ 		dev, &sg, ib_umem_num_dma_blocks(mr->umem, 1 << mr->page_shift),
+ 		sizeof(*mtt), flags);
+ 	if (!mtt)
+ 		return -ENOMEM;
+ 
+ 	orig_sg_length = sg.length;
+ 
+ 	mlx5r_umr_set_update_xlt_ctrl_seg(&wqe.ctrl_seg, flags, &sg);
+ 	mlx5r_umr_set_update_xlt_mkey_seg(dev, &wqe.mkey_seg, mr,
+ 					  mr->page_shift);
+ 	mlx5r_umr_set_update_xlt_data_seg(&wqe.data_seg, &sg);
+ 
+ 	cur_mtt = mtt;
+ 	rdma_for_each_block(mr->umem->sgt_append.sgt.sgl, &biter,
+ 			    mr->umem->sgt_append.sgt.nents,
+ 			    BIT(mr->page_shift)) {
+ 		if (cur_mtt == (void *)mtt + sg.length) {
+ 			dma_sync_single_for_device(ddev, sg.addr, sg.length,
+ 						   DMA_TO_DEVICE);
+ 
+ 			err = mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe,
+ 						       true);
+ 			if (err)
+ 				goto err;
+ 			dma_sync_single_for_cpu(ddev, sg.addr, sg.length,
+ 						DMA_TO_DEVICE);
+ 			offset += sg.length;
+ 			mlx5r_umr_update_offset(&wqe.ctrl_seg, offset);
+ 
+ 			cur_mtt = mtt;
+ 		}
+ 
+ 		cur_mtt->ptag =
+ 			cpu_to_be64(rdma_block_iter_dma_address(&biter) |
+ 				    MLX5_IB_MTT_PRESENT);
+ 
+ 		if (mr->umem->is_dmabuf && (flags & MLX5_IB_UPD_XLT_ZAP))
+ 			cur_mtt->ptag = 0;
+ 
+ 		cur_mtt++;
+ 	}
+ 
+ 	final_size = (void *)cur_mtt - (void *)mtt;
+ 	sg.length = ALIGN(final_size, MLX5_UMR_MTT_ALIGNMENT);
+ 	memset(cur_mtt, 0, sg.length - final_size);
+ 	mlx5r_umr_final_update_xlt(dev, &wqe, mr, &sg, flags);
+ 
+ 	dma_sync_single_for_device(ddev, sg.addr, sg.length, DMA_TO_DEVICE);
+ 	err = mlx5r_umr_post_send_wait(dev, mr->mmkey.key, &wqe, true);
+ 
+ err:
+ 	sg.length = orig_sg_length;
+ 	mlx5r_umr_unmap_free_xlt(dev, mtt, &sg);
+ 	return err;
+ }
++>>>>>>> b3d47ebd4908 (RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas)
diff --cc drivers/infiniband/hw/mlx5/umr.h
index cb1a2c95aac2,48c7dfdd1ab6..000000000000
--- a/drivers/infiniband/hw/mlx5/umr.h
+++ b/drivers/infiniband/hw/mlx5/umr.h
@@@ -9,4 -9,95 +9,91 @@@
  int mlx5r_umr_resource_init(struct mlx5_ib_dev *dev);
  void mlx5r_umr_resource_cleanup(struct mlx5_ib_dev *dev);
  
++<<<<<<< HEAD
++=======
+ static inline bool mlx5r_umr_can_load_pas(struct mlx5_ib_dev *dev,
+ 					  size_t length)
+ {
+ 	/*
+ 	 * umr_check_mkey_mask() rejects MLX5_MKEY_MASK_PAGE_SIZE which is
+ 	 * always set if MLX5_IB_SEND_UMR_UPDATE_TRANSLATION (aka
+ 	 * MLX5_IB_UPD_XLT_ADDR and MLX5_IB_UPD_XLT_ENABLE) is set. Thus, a mkey
+ 	 * can never be enabled without this capability. Simplify this weird
+ 	 * quirky hardware by just saying it can't use PAS lists with UMR at
+ 	 * all.
+ 	 */
+ 	if (MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))
+ 		return false;
+ 
+ 	/*
+ 	 * length is the size of the MR in bytes when mlx5_ib_update_xlt() is
+ 	 * used.
+ 	 */
+ 	if (!MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset) &&
+ 	    length >= MLX5_MAX_UMR_PAGES * PAGE_SIZE)
+ 		return false;
+ 	return true;
+ }
+ 
+ /*
+  * true if an existing MR can be reconfigured to new access_flags using UMR.
+  * Older HW cannot use UMR to update certain elements of the MKC. See
+  * get_umr_update_access_mask() and umr_check_mkey_mask()
+  */
+ static inline bool mlx5r_umr_can_reconfig(struct mlx5_ib_dev *dev,
+ 					  unsigned int current_access_flags,
+ 					  unsigned int target_access_flags)
+ {
+ 	unsigned int diffs = current_access_flags ^ target_access_flags;
+ 
+ 	if ((diffs & IB_ACCESS_REMOTE_ATOMIC) &&
+ 	    MLX5_CAP_GEN(dev->mdev, atomic) &&
+ 	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
+ 		return false;
+ 
+ 	if ((diffs & IB_ACCESS_RELAXED_ORDERING) &&
+ 	    MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write) &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write_umr))
+ 		return false;
+ 
+ 	if ((diffs & IB_ACCESS_RELAXED_ORDERING) &&
+ 	    MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read) &&
+ 	    !MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read_umr))
+ 		return false;
+ 
+ 	return true;
+ }
+ 
+ static inline u64 mlx5r_umr_get_xlt_octo(u64 bytes)
+ {
+ 	return ALIGN(bytes, MLX5_IB_UMR_XLT_ALIGNMENT) /
+ 	       MLX5_IB_UMR_OCTOWORD;
+ }
+ 
+ int mlx5r_umr_set_umr_ctrl_seg(struct mlx5_ib_dev *dev,
+ 			       struct mlx5_wqe_umr_ctrl_seg *umr,
+ 			       const struct ib_send_wr *wr);
+ 
+ struct mlx5r_umr_context {
+ 	struct ib_cqe cqe;
+ 	enum ib_wc_status status;
+ 	struct completion done;
+ };
+ 
+ struct mlx5r_umr_wqe {
+ 	struct mlx5_wqe_umr_ctrl_seg ctrl_seg;
+ 	struct mlx5_mkey_seg mkey_seg;
+ 	struct mlx5_wqe_data_seg data_seg;
+ };
+ 
+ int mlx5r_umr_revoke_mr(struct mlx5_ib_mr *mr);
+ int mlx5r_umr_rereg_pd_access(struct mlx5_ib_mr *mr, struct ib_pd *pd,
+ 			      int access_flags);
+ void *mlx5r_umr_create_xlt(struct mlx5_ib_dev *dev, struct ib_sge *sg,
+ 			   size_t nents, size_t ent_size, unsigned int flags);
+ void mlx5r_umr_unmap_free_xlt(struct mlx5_ib_dev *dev, void *xlt,
+ 			      struct ib_sge *sg);
+ int mlx5r_umr_update_mr_pas(struct mlx5_ib_mr *mr, unsigned int flags);
+ 
++>>>>>>> b3d47ebd4908 (RDMA/mlx5: Use mlx5_umr_post_send_wait() to update MR pas)
  #endif /* _MLX5_IB_UMR_H */
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
* Unmerged path drivers/infiniband/hw/mlx5/odp.c
* Unmerged path drivers/infiniband/hw/mlx5/umr.c
* Unmerged path drivers/infiniband/hw/mlx5/umr.h
