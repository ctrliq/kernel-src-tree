perf/x86/amd: Add idle hooks for branch sampling

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Stephane Eranian <eranian@google.com>
commit d5616bac7adadbf42a3b63b8717e75eb82a2cc2c
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/d5616bac.failed

On AMD Fam19h Zen3, the branch sampling (BRS) feature must be disabled before
entering low power and re-enabled (if was active) when returning from low
power. Otherwise, the NMI interrupt may be held up for too long and cause
problems. Stopping BRS will cause the NMI to be delivered if it was held up.

Define a perf_amd_brs_lopwr_cb() callback to stop/restart BRS.  The callback
is protected by a jump label which is enabled only when AMD BRS is detected.
In all other cases, the callback is never called.

	Signed-off-by: Stephane Eranian <eranian@google.com>
[peterz: static_call() and build fixes]
	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Link: https://lore.kernel.org/r/20220322221517.2510440-10-eranian@google.com
(cherry picked from commit d5616bac7adadbf42a3b63b8717e75eb82a2cc2c)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/events/amd/brs.c
#	arch/x86/events/amd/core.c
#	arch/x86/events/perf_event.h
diff --cc arch/x86/events/amd/core.c
index fc84e3f05348,8e1e818f8195..000000000000
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@@ -1,4 -1,6 +1,5 @@@
 -// SPDX-License-Identifier: GPL-2.0-only
  #include <linux/perf_event.h>
+ #include <linux/jump_label.h>
  #include <linux/export.h>
  #include <linux/types.h>
  #include <linux/init.h>
@@@ -988,6 -1217,22 +989,25 @@@ static int __init amd_core_pmu_init(voi
  		x86_pmu.flags |= PMU_FL_PAIR;
  	}
  
++<<<<<<< HEAD
++=======
+ 	/*
+ 	 * BRS requires special event constraints and flushing on ctxsw.
+ 	 */
+ 	if (boot_cpu_data.x86 >= 0x19 && !amd_brs_init()) {
+ 		x86_pmu.get_event_constraints = amd_get_event_constraints_f19h;
+ 		x86_pmu.sched_task = amd_pmu_sched_task;
+ 		/*
+ 		 * put_event_constraints callback same as Fam17h, set above
+ 		 */
+ 
+ 		/* branch sampling must be stopped when entering low power */
+ 		amd_brs_lopwr_init();
+ 	}
+ 
+ 	x86_pmu.attr_update = amd_attr_update;
+ 
++>>>>>>> d5616bac7ada (perf/x86/amd: Add idle hooks for branch sampling)
  	pr_cont("core perfctr, ");
  	return 0;
  }
diff --cc arch/x86/events/perf_event.h
index 139832e590d3,3b0324584da3..000000000000
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@@ -1223,6 -1219,88 +1223,91 @@@ static inline bool fixed_counter_disabl
  
  int amd_pmu_init(void);
  
++<<<<<<< HEAD
++=======
+ #ifdef CONFIG_PERF_EVENTS_AMD_BRS
+ int amd_brs_init(void);
+ void amd_brs_disable(void);
+ void amd_brs_enable(void);
+ void amd_brs_enable_all(void);
+ void amd_brs_disable_all(void);
+ void amd_brs_drain(void);
+ void amd_brs_lopwr_init(void);
+ void amd_brs_disable_all(void);
+ int amd_brs_setup_filter(struct perf_event *event);
+ void amd_brs_reset(void);
+ 
+ static inline void amd_pmu_brs_add(struct perf_event *event)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 
+ 	perf_sched_cb_inc(event->ctx->pmu);
+ 	cpuc->lbr_users++;
+ 	/*
+ 	 * No need to reset BRS because it is reset
+ 	 * on brs_enable() and it is saturating
+ 	 */
+ }
+ 
+ static inline void amd_pmu_brs_del(struct perf_event *event)
+ {
+ 	struct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);
+ 
+ 	cpuc->lbr_users--;
+ 	WARN_ON_ONCE(cpuc->lbr_users < 0);
+ 
+ 	perf_sched_cb_dec(event->ctx->pmu);
+ }
+ 
+ void amd_pmu_brs_sched_task(struct perf_event_context *ctx, bool sched_in);
+ 
+ static inline s64 amd_brs_adjust_period(s64 period)
+ {
+ 	if (period > x86_pmu.lbr_nr)
+ 		return period - x86_pmu.lbr_nr;
+ 
+ 	return period;
+ }
+ #else
+ static inline int amd_brs_init(void)
+ {
+ 	return 0;
+ }
+ static inline void amd_brs_disable(void) {}
+ static inline void amd_brs_enable(void) {}
+ static inline void amd_brs_drain(void) {}
+ static inline void amd_brs_lopwr_init(void) {}
+ static inline void amd_brs_disable_all(void) {}
+ static inline int amd_brs_setup_filter(struct perf_event *event)
+ {
+ 	return 0;
+ }
+ static inline void amd_brs_reset(void) {}
+ 
+ static inline void amd_pmu_brs_add(struct perf_event *event)
+ {
+ }
+ 
+ static inline void amd_pmu_brs_del(struct perf_event *event)
+ {
+ }
+ 
+ static inline void amd_pmu_brs_sched_task(struct perf_event_context *ctx, bool sched_in)
+ {
+ }
+ 
+ static inline s64 amd_brs_adjust_period(s64 period)
+ {
+ 	return period;
+ }
+ 
+ static inline void amd_brs_enable_all(void)
+ {
+ }
+ 
+ #endif
+ 
++>>>>>>> d5616bac7ada (perf/x86/amd: Add idle hooks for branch sampling)
  #else /* CONFIG_CPU_SUP_AMD */
  
  static inline int amd_pmu_init(void)
* Unmerged path arch/x86/events/amd/brs.c
* Unmerged path arch/x86/events/amd/brs.c
* Unmerged path arch/x86/events/amd/core.c
* Unmerged path arch/x86/events/perf_event.h
diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 4a0e00661c00..b9dcdf18ef66 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -2,6 +2,8 @@
 #ifndef _ASM_X86_PERF_EVENT_H
 #define _ASM_X86_PERF_EVENT_H
 
+#include <linux/static_call.h>
+
 /*
  * Performance event hw details:
  */
@@ -517,6 +519,27 @@ static inline void intel_pt_handle_vmx(int on)
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD)
  extern void amd_pmu_enable_virt(void);
  extern void amd_pmu_disable_virt(void);
+
+#if defined(CONFIG_PERF_EVENTS_AMD_BRS)
+
+#define PERF_NEEDS_LOPWR_CB 1
+
+/*
+ * architectural low power callback impacts
+ * drivers/acpi/processor_idle.c
+ * drivers/acpi/acpi_pad.c
+ */
+extern void perf_amd_brs_lopwr_cb(bool lopwr_in);
+
+DECLARE_STATIC_CALL(perf_lopwr_cb, perf_amd_brs_lopwr_cb);
+
+static inline void perf_lopwr_cb(bool lopwr_in)
+{
+	static_call_mod(perf_lopwr_cb)(lopwr_in);
+}
+
+#endif /* PERF_NEEDS_LOPWR_CB */
+
 #else
  static inline void amd_pmu_enable_virt(void) { }
  static inline void amd_pmu_disable_virt(void) { }
