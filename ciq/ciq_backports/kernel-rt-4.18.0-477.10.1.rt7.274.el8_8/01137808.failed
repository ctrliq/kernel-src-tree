RDMA/mlx5: Rename the mkey cache variables and functions

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Aharon Landau <aharonl@nvidia.com>
commit 0113780870b1597ae49f30abfa4957c239f913d3
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/01137808.failed

After replacing the MR cache with an Mkey cache, rename the variables and
functions to fit the new meaning.

Link: https://lore.kernel.org/r/20220726071911.122765-6-michaelgur@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 0113780870b1597ae49f30abfa4957c239f913d3)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mlx5_ib.h
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mlx5_ib.h
index bc16ce369c59,2e2ad3918385..000000000000
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@@ -790,9 -754,19 +790,23 @@@ struct mlx5_cache_ent 
  	struct delayed_work	dwork;
  };
  
++<<<<<<< HEAD
 +struct mlx5_mr_cache {
++=======
+ struct mlx5r_async_create_mkey {
+ 	union {
+ 		u32 in[MLX5_ST_SZ_BYTES(create_mkey_in)];
+ 		u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
+ 	};
+ 	struct mlx5_async_work cb_work;
+ 	struct mlx5_cache_ent *ent;
+ 	u32 mkey;
+ };
+ 
+ struct mlx5_mkey_cache {
++>>>>>>> 0113780870b1 (RDMA/mlx5: Rename the mkey cache variables and functions)
  	struct workqueue_struct *wq;
- 	struct mlx5_cache_ent	ent[MAX_MR_CACHE_ENTRIES];
+ 	struct mlx5_cache_ent	ent[MAX_MKEY_CACHE_ENTRIES];
  	struct dentry		*root;
  	unsigned long		last_add;
  };
@@@ -1365,9 -1339,7 +1379,13 @@@ int mlx5r_odp_create_eq(struct mlx5_ib_
  void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *ibdev);
  int __init mlx5_ib_odp_init(void);
  void mlx5_ib_odp_cleanup(void);
++<<<<<<< HEAD
 +void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
 +			      unsigned long end);
 +void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
++=======
+ void mlx5_odp_init_mkey_cache_entry(struct mlx5_cache_ent *ent);
++>>>>>>> 0113780870b1 (RDMA/mlx5: Rename the mkey cache variables and functions)
  void mlx5_odp_populate_xlt(void *xlt, size_t idx, size_t nentries,
  			   struct mlx5_ib_mr *mr, int flags);
  
diff --cc drivers/infiniband/hw/mlx5/mr.c
index 2aee9118633b,129d531bd01b..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -107,27 -105,23 +107,27 @@@ static int mlx5_ib_create_mkey(struct m
  	return ret;
  }
  
 -static int mlx5_ib_create_mkey_cb(struct mlx5r_async_create_mkey *async_create)
 +static int
 +mlx5_ib_create_mkey_cb(struct mlx5_ib_dev *dev,
 +		       struct mlx5_ib_mkey *mkey,
 +		       struct mlx5_async_ctx *async_ctx,
 +		       u32 *in, int inlen, u32 *out, int outlen,
 +		       struct mlx5_async_work *context)
  {
 -	struct mlx5_ib_dev *dev = async_create->ent->dev;
 -	size_t inlen = MLX5_ST_SZ_BYTES(create_mkey_in);
 -	size_t outlen = MLX5_ST_SZ_BYTES(create_mkey_out);
 -
 -	MLX5_SET(create_mkey_in, async_create->in, opcode,
 -		 MLX5_CMD_OP_CREATE_MKEY);
 -	assign_mkey_variant(dev, &async_create->mkey, async_create->in);
 -	return mlx5_cmd_exec_cb(&dev->async_ctx, async_create->in, inlen,
 -				async_create->out, outlen, create_mkey_callback,
 -				&async_create->cb_work);
 +	MLX5_SET(create_mkey_in, in, opcode, MLX5_CMD_OP_CREATE_MKEY);
 +	assign_mkey_variant(dev, mkey, in);
 +	return mlx5_cmd_exec_cb(async_ctx, in, inlen, out, outlen,
 +				create_mkey_callback, context);
  }
  
- static int mr_cache_max_order(struct mlx5_ib_dev *dev);
+ static int mkey_cache_max_order(struct mlx5_ib_dev *dev);
  static void queue_adjust_cache_locked(struct mlx5_cache_ent *ent);
  
 +static bool umr_can_use_indirect_mkey(struct mlx5_ib_dev *dev)
 +{
 +	return !MLX5_CAP_GEN(dev->mdev, umr_indirect_mkey_disabled);
 +}
 +
  static int destroy_mkey(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
  {
  	WARN_ON(xa_load(&dev->odp_mkeys, mlx5_base_mkey(mr->mmkey.key)));
@@@ -526,10 -569,10 +526,10 @@@ static void queue_adjust_cache_locked(s
  static void __cache_work_func(struct mlx5_cache_ent *ent)
  {
  	struct mlx5_ib_dev *dev = ent->dev;
- 	struct mlx5_mr_cache *cache = &dev->cache;
+ 	struct mlx5_mkey_cache *cache = &dev->cache;
  	int err;
  
 -	xa_lock_irq(&ent->mkeys);
 +	spin_lock_irq(&ent->lock);
  	if (ent->disabled)
  		goto out;
  
@@@ -627,58 -676,27 +627,58 @@@ struct mlx5_ib_mr *mlx5_mr_cache_alloc(
  	return mr;
  }
  
 +static void detach_mr_from_cache(struct mlx5_ib_mr *mr)
 +{
 +	struct mlx5_cache_ent *ent = mr->cache_ent;
 +
 +	mr->cache_ent = NULL;
 +	spin_lock_irq(&ent->lock);
 +	ent->total_mrs--;
 +	spin_unlock_irq(&ent->lock);
 +}
 +
 +static void mlx5_mr_cache_free(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr)
 +{
 +	struct mlx5_cache_ent *ent = mr->cache_ent;
 +
 +	WRITE_ONCE(dev->cache.last_add, jiffies);
 +	spin_lock_irq(&ent->lock);
 +	list_add_tail(&mr->list, &ent->head);
 +	ent->available_mrs++;
 +	queue_adjust_cache_locked(ent);
 +	spin_unlock_irq(&ent->lock);
 +}
 +
  static void clean_keys(struct mlx5_ib_dev *dev, int c)
  {
- 	struct mlx5_mr_cache *cache = &dev->cache;
+ 	struct mlx5_mkey_cache *cache = &dev->cache;
  	struct mlx5_cache_ent *ent = &cache->ent[c];
 -	u32 mkey;
 +	struct mlx5_ib_mr *tmp_mr;
 +	struct mlx5_ib_mr *mr;
 +	LIST_HEAD(del_list);
  
  	cancel_delayed_work(&ent->dwork);
 -	xa_lock_irq(&ent->mkeys);
 -	while (ent->stored) {
 -		mkey = pop_stored_mkey(ent);
 -		xa_unlock_irq(&ent->mkeys);
 -		mlx5_core_destroy_mkey(dev->mdev, mkey);
 -		xa_lock_irq(&ent->mkeys);
 +	while (1) {
 +		spin_lock_irq(&ent->lock);
 +		if (list_empty(&ent->head)) {
 +			spin_unlock_irq(&ent->lock);
 +			break;
 +		}
 +		mr = list_first_entry(&ent->head, struct mlx5_ib_mr, list);
 +		list_move(&mr->list, &del_list);
 +		ent->available_mrs--;
 +		ent->total_mrs--;
 +		spin_unlock_irq(&ent->lock);
 +		mlx5_core_destroy_mkey(dev->mdev, mr->mmkey.key);
 +	}
 +
 +	list_for_each_entry_safe(mr, tmp_mr, &del_list, list) {
 +		list_del(&mr->list);
 +		kfree(mr);
  	}
 -	xa_unlock_irq(&ent->mkeys);
  }
  
- static void mlx5_mr_cache_debugfs_cleanup(struct mlx5_ib_dev *dev)
+ static void mlx5_mkey_cache_debugfs_cleanup(struct mlx5_ib_dev *dev)
  {
  	if (!mlx5_debugfs_root || dev->is_rep)
  		return;
@@@ -732,10 -750,9 +732,10 @@@ int mlx5_mkey_cache_init(struct mlx5_ib
  
  	mlx5_cmd_init_async_ctx(dev->mdev, &dev->async_ctx);
  	timer_setup(&dev->delay_timer, delay_time_func, 0);
- 	for (i = 0; i < MAX_MR_CACHE_ENTRIES; i++) {
+ 	for (i = 0; i < MAX_MKEY_CACHE_ENTRIES; i++) {
  		ent = &cache->ent[i];
 -		xa_init_flags(&ent->mkeys, XA_FLAGS_LOCK_IRQ);
 +		INIT_LIST_HEAD(&ent->head);
 +		spin_lock_init(&ent->lock);
  		ent->order = i + 2;
  		ent->dev = dev;
  		ent->limit = 0;
@@@ -759,12 -776,12 +759,12 @@@
  			ent->limit = dev->mdev->profile.mr_cache[i].limit;
  		else
  			ent->limit = 0;
 -		xa_lock_irq(&ent->mkeys);
 +		spin_lock_irq(&ent->lock);
  		queue_adjust_cache_locked(ent);
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  	}
  
- 	mlx5_mr_cache_debugfs_init(dev);
+ 	mlx5_mkey_cache_debugfs_init(dev);
  
  	return 0;
  }
@@@ -776,12 -793,12 +776,12 @@@ int mlx5_mkey_cache_cleanup(struct mlx5
  	if (!dev->cache.wq)
  		return 0;
  
- 	for (i = 0; i < MAX_MR_CACHE_ENTRIES; i++) {
+ 	for (i = 0; i < MAX_MKEY_CACHE_ENTRIES; i++) {
  		struct mlx5_cache_ent *ent = &dev->cache.ent[i];
  
 -		xa_lock_irq(&ent->mkeys);
 +		spin_lock_irq(&ent->lock);
  		ent->disabled = true;
 -		xa_unlock_irq(&ent->mkeys);
 +		spin_unlock_irq(&ent->lock);
  		cancel_delayed_work_sync(&ent->dwork);
  	}
  
@@@ -862,78 -879,10 +862,83 @@@ static int mkey_cache_max_order(struct 
  	return MLX5_MAX_UMR_SHIFT;
  }
  
++<<<<<<< HEAD
 +static struct ib_umem *mr_umem_get(struct mlx5_ib_dev *dev, struct ib_udata *udata,
 +				   u64 start, u64 length, int access_flags)
 +{
 +	struct ib_umem *u;
 +
 +	if (access_flags & IB_ACCESS_ON_DEMAND) {
 +		struct ib_umem_odp *odp;
 +
 +		odp = ib_umem_odp_get(udata, start, length, access_flags);
 +		if (IS_ERR(odp)) {
 +			mlx5_ib_dbg(dev, "umem get failed (%ld)\n",
 +				    PTR_ERR(odp));
 +			return ERR_CAST(odp);
 +		}
 +		return &odp->umem;
 +	}
 +
 +	 u = ib_umem_get(udata, start, length, access_flags);
 +	if (IS_ERR(u)) {
 +		mlx5_ib_dbg(dev, "umem get failed (%ld)\n", PTR_ERR(u));
 +		return u;
 +	}
 +	return u;
 +}
 +
 +static void mlx5_ib_umr_done(struct ib_cq *cq, struct ib_wc *wc)
 +{
 +	struct mlx5_ib_umr_context *context =
 +		container_of(wc->wr_cqe, struct mlx5_ib_umr_context, cqe);
 +
 +	context->status = wc->status;
 +	complete(&context->done);
 +}
 +
 +static inline void mlx5_ib_init_umr_context(struct mlx5_ib_umr_context *context)
 +{
 +	context->cqe.done = mlx5_ib_umr_done;
 +	context->status = -1;
 +	init_completion(&context->done);
 +}
 +
 +static int mlx5_ib_post_send_wait(struct mlx5_ib_dev *dev,
 +				  struct mlx5_umr_wr *umrwr)
 +{
 +	struct umr_common *umrc = &dev->umrc;
 +	const struct ib_send_wr *bad;
 +	int err;
 +	struct mlx5_ib_umr_context umr_context;
 +
 +	mlx5_ib_init_umr_context(&umr_context);
 +	umrwr->wr.wr_cqe = &umr_context.cqe;
 +
 +	down(&umrc->sem);
 +	err = ib_post_send(umrc->qp, &umrwr->wr, &bad);
 +	if (err) {
 +		mlx5_ib_warn(dev, "UMR post send failed, err %d\n", err);
 +	} else {
 +		wait_for_completion(&umr_context.done);
 +		if (umr_context.status != IB_WC_SUCCESS) {
 +			mlx5_ib_warn(dev, "reg umr failed (%u)\n",
 +				     umr_context.status);
 +			err = -EFAULT;
 +		}
 +	}
 +	up(&umrc->sem);
 +	return err;
 +}
 +
 +static struct mlx5_cache_ent *mr_cache_ent_from_order(struct mlx5_ib_dev *dev,
 +						      unsigned int order)
++=======
+ static struct mlx5_cache_ent *mkey_cache_ent_from_order(struct mlx5_ib_dev *dev,
+ 							unsigned int order)
++>>>>>>> 0113780870b1 (RDMA/mlx5: Rename the mkey cache variables and functions)
  {
- 	struct mlx5_mr_cache *cache = &dev->cache;
+ 	struct mlx5_mkey_cache *cache = &dev->cache;
  
  	if (order < cache->ent[0].order)
  		return &cache->ent[0];
@@@ -952,17 -923,26 +957,17 @@@ static struct mlx5_ib_mr *alloc_mr_from
  	struct mlx5_ib_mr *mr;
  	unsigned int page_size;
  
 -	if (umem->is_dmabuf)
 -		page_size = mlx5_umem_dmabuf_default_pgsz(umem, iova);
 -	else
 -		page_size = mlx5_umem_find_best_pgsz(umem, mkc, log_page_size,
 -						     0, iova);
 +	page_size = mlx5_umem_find_best_pgsz(umem, mkc, log_page_size, 0, iova);
  	if (WARN_ON(!page_size))
  		return ERR_PTR(-EINVAL);
- 	ent = mr_cache_ent_from_order(
+ 	ent = mkey_cache_ent_from_order(
  		dev, order_base_2(ib_umem_num_dma_blocks(umem, page_size)));
 -	/*
 -	 * Matches access in alloc_cache_mr(). If the MR can't come from the
 -	 * cache then synchronously create an uncached one.
 -	 */
 -	if (!ent || ent->limit == 0 ||
 -	    !mlx5r_umr_can_reconfig(dev, 0, access_flags)) {
 -		mutex_lock(&dev->slow_path_mutex);
 -		mr = reg_create(pd, umem, iova, access_flags, page_size, false);
 -		mutex_unlock(&dev->slow_path_mutex);
 -		return mr;
 -	}
 +	if (!ent)
 +		return ERR_PTR(-E2BIG);
 +
 +	/* Matches access in alloc_cache_mr() */
 +	if (!mlx5_ib_can_reconfig_with_umr(dev, 0, access_flags))
 +		return ERR_PTR(-EOPNOTSUPP);
  
  	mr = mlx5_mr_cache_alloc(dev, ent, access_flags);
  	if (IS_ERR(mr))
diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index 95805368ddab..344d25bd435c 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -4005,7 +4005,7 @@ static void mlx5_ib_stage_pre_ib_reg_umr_cleanup(struct mlx5_ib_dev *dev)
 {
 	int err;
 
-	err = mlx5_mr_cache_cleanup(dev);
+	err = mlx5_mkey_cache_cleanup(dev);
 	if (err)
 		mlx5_ib_warn(dev, "mr cache cleanup failed\n");
 
@@ -4025,7 +4025,7 @@ static int mlx5_ib_stage_post_ib_reg_umr_init(struct mlx5_ib_dev *dev)
 	if (ret)
 		return ret;
 
-	ret = mlx5_mr_cache_init(dev);
+	ret = mlx5_mkey_cache_init(dev);
 	if (ret) {
 		mlx5_ib_warn(dev, "mr cache init failed %d\n", ret);
 		mlx5r_umr_resource_cleanup(dev);
* Unmerged path drivers/infiniband/hw/mlx5/mlx5_ib.h
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
diff --git a/drivers/infiniband/hw/mlx5/odp.c b/drivers/infiniband/hw/mlx5/odp.c
index 446d722140da..9a8d5af4544c 100644
--- a/drivers/infiniband/hw/mlx5/odp.c
+++ b/drivers/infiniband/hw/mlx5/odp.c
@@ -1533,7 +1533,7 @@ mlx5_ib_odp_destroy_eq(struct mlx5_ib_dev *dev, struct mlx5_ib_pf_eq *eq)
 	return err;
 }
 
-void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent)
+void mlx5_odp_init_mkey_cache_entry(struct mlx5_cache_ent *ent)
 {
 	if (!(ent->dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
 		return;
diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h
index ba8cc6c5c91f..5d52d3ac3994 100644
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -728,10 +728,10 @@ enum {
 };
 
 enum {
-	MR_CACHE_LAST_STD_ENTRY = 20,
+	MKEY_CACHE_LAST_STD_ENTRY = 20,
 	MLX5_IMR_MTT_CACHE_ENTRY,
 	MLX5_IMR_KSM_CACHE_ENTRY,
-	MAX_MR_CACHE_ENTRIES
+	MAX_MKEY_CACHE_ENTRIES
 };
 
 struct mlx5_profile {
@@ -740,7 +740,7 @@ struct mlx5_profile {
 	struct {
 		int	size;
 		int	limit;
-	} mr_cache[MAX_MR_CACHE_ENTRIES];
+	} mr_cache[MAX_MKEY_CACHE_ENTRIES];
 };
 
 struct mlx5_hca_cap {
