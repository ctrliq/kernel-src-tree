delayacct: support re-entrance detection of thrashing accounting

jira LE-1907
Rebuild_History Non-Buildable kernel-rt-4.18.0-477.10.1.rt7.274.el8_8
commit-author Yang Yang <yang.yang29@zte.com.cn>
commit aa1cf99b87e934e761b46ce2b925335a398980da
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-477.10.1.rt7.274.el8_8/aa1cf99b.failed

Once upon a time, we only support accounting thrashing of page cache. 
Then Joonsoo introduced workingset detection for anonymous pages and we
gained the ability to account thrashing of them[1].

For page cache thrashing accounting, there is no suitable place to do it
in fs level likes swap_readpage().  So we have to do it in
folio_wait_bit_common().

Then for anonymous pages thrashing accounting, we have to do it in both
swap_readpage() and folio_wait_bit_common().  This likes PSI, so we should
let thrashing accounting supports re-entrance detection.

This patch is to prepare complete thrashing accounting, and is based on
patch "filemap: make the accounting of thrashing more consistent".

[1] commit aae466b0052e ("mm/swap: implement workingset detection for anonymous LRU")

Link: https://lkml.kernel.org/r/20220815071134.74551-1-yang.yang29@zte.com.cn
	Signed-off-by: Yang Yang <yang.yang29@zte.com.cn>
	Signed-off-by: CGEL ZTE <cgel.zte@gmail.com>
	Reviewed-by: Ran Xiaokai <ran.xiaokai@zte.com.cn>
	Reviewed-by: wangyong <wang.yong12@zte.com.cn>
	Acked-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
(cherry picked from commit aa1cf99b87e934e761b46ce2b925335a398980da)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	include/linux/delayacct.h
#	kernel/delayacct.c
#	mm/filemap.c
diff --cc include/linux/delayacct.h
index 6e892b01659e,0da97dba9ef8..000000000000
--- a/include/linux/delayacct.h
+++ b/include/linux/delayacct.h
@@@ -87,28 -73,14 +87,39 @@@ extern int __delayacct_add_tsk(struct t
  extern __u64 __delayacct_blkio_ticks(struct task_struct *);
  extern void __delayacct_freepages_start(void);
  extern void __delayacct_freepages_end(void);
++<<<<<<< HEAD
 +extern void __delayacct_thrashing_start(void);
 +extern void __delayacct_thrashing_end(void);
 +
 +static inline int delayacct_is_task_waiting_on_io(struct task_struct *p)
 +{
 +	if (p->delays)
 +		return (p->delays->flags & DELAYACCT_PF_BLKIO);
 +	else
 +		return 0;
 +}
 +
 +static inline void delayacct_set_flag(int flag)
 +{
 +	if (current->delays)
 +		current->delays->flags |= flag;
 +}
 +
 +static inline void delayacct_clear_flag(int flag)
 +{
 +	if (current->delays)
 +		current->delays->flags &= ~flag;
 +}
++=======
+ extern void __delayacct_thrashing_start(bool *in_thrashing);
+ extern void __delayacct_thrashing_end(bool *in_thrashing);
+ extern void __delayacct_swapin_start(void);
+ extern void __delayacct_swapin_end(void);
+ extern void __delayacct_compact_start(void);
+ extern void __delayacct_compact_end(void);
+ extern void __delayacct_wpcopy_start(void);
+ extern void __delayacct_wpcopy_end(void);
++>>>>>>> aa1cf99b87e9 (delayacct: support re-entrance detection of thrashing accounting)
  
  static inline void delayacct_tsk_init(struct task_struct *tsk)
  {
@@@ -169,23 -143,79 +180,23 @@@ static inline void delayacct_freepages_
  		__delayacct_freepages_end();
  }
  
- static inline void delayacct_thrashing_start(void)
+ static inline void delayacct_thrashing_start(bool *in_thrashing)
  {
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
  	if (current->delays)
- 		__delayacct_thrashing_start();
+ 		__delayacct_thrashing_start(in_thrashing);
  }
  
- static inline void delayacct_thrashing_end(void)
+ static inline void delayacct_thrashing_end(bool *in_thrashing)
  {
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
  	if (current->delays)
- 		__delayacct_thrashing_end();
+ 		__delayacct_thrashing_end(in_thrashing);
  }
  
 -static inline void delayacct_swapin_start(void)
 -{
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
 -	if (current->delays)
 -		__delayacct_swapin_start();
 -}
 -
 -static inline void delayacct_swapin_end(void)
 -{
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
 -	if (current->delays)
 -		__delayacct_swapin_end();
 -}
 -
 -static inline void delayacct_compact_start(void)
 -{
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
 -	if (current->delays)
 -		__delayacct_compact_start();
 -}
 -
 -static inline void delayacct_compact_end(void)
 -{
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
 -	if (current->delays)
 -		__delayacct_compact_end();
 -}
 -
 -static inline void delayacct_wpcopy_start(void)
 -{
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
 -	if (current->delays)
 -		__delayacct_wpcopy_start();
 -}
 -
 -static inline void delayacct_wpcopy_end(void)
 -{
 -	if (!static_branch_unlikely(&delayacct_key))
 -		return;
 -
 -	if (current->delays)
 -		__delayacct_wpcopy_end();
 -}
 -
  #else
 +static inline void delayacct_set_flag(int flag)
 +{}
 +static inline void delayacct_clear_flag(int flag)
 +{}
  static inline void delayacct_init(void)
  {}
  static inline void delayacct_tsk_init(struct task_struct *tsk)
@@@ -207,10 -237,22 +218,10 @@@ static inline void delayacct_freepages_
  {}
  static inline void delayacct_freepages_end(void)
  {}
- static inline void delayacct_thrashing_start(void)
+ static inline void delayacct_thrashing_start(bool *in_thrashing)
  {}
- static inline void delayacct_thrashing_end(void)
+ static inline void delayacct_thrashing_end(bool *in_thrashing)
  {}
 -static inline void delayacct_swapin_start(void)
 -{}
 -static inline void delayacct_swapin_end(void)
 -{}
 -static inline void delayacct_compact_start(void)
 -{}
 -static inline void delayacct_compact_end(void)
 -{}
 -static inline void delayacct_wpcopy_start(void)
 -{}
 -static inline void delayacct_wpcopy_end(void)
 -{}
  
  #endif /* CONFIG_TASK_DELAY_ACCT */
  
diff --cc kernel/delayacct.c
index 2a12b988c717,e39cb696cfbd..000000000000
--- a/kernel/delayacct.c
+++ b/kernel/delayacct.c
@@@ -165,20 -208,28 +165,33 @@@ void __delayacct_freepages_start(void
  
  void __delayacct_freepages_end(void)
  {
 -	delayacct_end(&current->delays->lock,
 -		      &current->delays->freepages_start,
 -		      &current->delays->freepages_delay,
 -		      &current->delays->freepages_count);
 +	delayacct_end(
 +		&current->delays->lock,
 +		&current->delays->freepages_start,
 +		&current->delays->freepages_delay,
 +		&current->delays->freepages_count);
  }
  
- void __delayacct_thrashing_start(void)
+ void __delayacct_thrashing_start(bool *in_thrashing)
  {
++<<<<<<< HEAD
 +	current->delays->thrashing_start = ktime_get_ns();
++=======
+ 	*in_thrashing = !!current->in_thrashing;
+ 	if (*in_thrashing)
+ 		return;
+ 
+ 	current->in_thrashing = 1;
+ 	current->delays->thrashing_start = local_clock();
++>>>>>>> aa1cf99b87e9 (delayacct: support re-entrance detection of thrashing accounting)
  }
  
- void __delayacct_thrashing_end(void)
+ void __delayacct_thrashing_end(bool *in_thrashing)
  {
+ 	if (*in_thrashing)
+ 		return;
+ 
+ 	current->in_thrashing = 0;
  	delayacct_end(&current->delays->lock,
  		      &current->delays->thrashing_start,
  		      &current->delays->thrashing_delay,
diff --cc mm/filemap.c
index 87e41a01a79e,68bd70fe71d5..000000000000
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -1142,124 -1193,275 +1142,261 @@@ enum behavior 
  			 */
  };
  
 -/*
 - * Attempt to check (or get) the folio flag, and mark us done
 - * if successful.
 - */
 -static inline bool folio_trylock_flag(struct folio *folio, int bit_nr,
 -					struct wait_queue_entry *wait)
 -{
 -	if (wait->flags & WQ_FLAG_EXCLUSIVE) {
 -		if (test_and_set_bit(bit_nr, &folio->flags))
 -			return false;
 -	} else if (test_bit(bit_nr, &folio->flags))
 -		return false;
 -
 -	wait->flags |= WQ_FLAG_WOKEN | WQ_FLAG_DONE;
 -	return true;
 -}
 -
 -/* How many times do we accept lock stealing from under a waiter? */
 -int sysctl_page_lock_unfairness = 5;
 -
 -static inline int folio_wait_bit_common(struct folio *folio, int bit_nr,
 -		int state, enum behavior behavior)
 +static inline int wait_on_page_bit_common(wait_queue_head_t *q,
 +	struct page *page, int bit_nr, int state, enum behavior behavior)
  {
 -	wait_queue_head_t *q = folio_waitqueue(folio);
 -	int unfairness = sysctl_page_lock_unfairness;
  	struct wait_page_queue wait_page;
  	wait_queue_entry_t *wait = &wait_page.wait;
 +	bool bit_is_set;
  	bool thrashing = false;
  	unsigned long pflags;
++<<<<<<< HEAD
 +	int ret = 0;
 +
 +	if (bit_nr == PG_locked &&
 +	    !PageUptodate(page) && PageWorkingset(page)) {
 +		if (!PageSwapBacked(page))
 +			delayacct_thrashing_start();
++=======
+ 	bool in_thrashing;
+ 
+ 	if (bit_nr == PG_locked &&
+ 	    !folio_test_uptodate(folio) && folio_test_workingset(folio)) {
+ 		delayacct_thrashing_start(&in_thrashing);
++>>>>>>> aa1cf99b87e9 (delayacct: support re-entrance detection of thrashing accounting)
  		psi_memstall_enter(&pflags);
  		thrashing = true;
  	}
  
  	init_wait(wait);
 +	wait->flags = behavior == EXCLUSIVE ? WQ_FLAG_EXCLUSIVE : 0;
  	wait->func = wake_page_function;
 -	wait_page.folio = folio;
 +	wait_page.page = page;
  	wait_page.bit_nr = bit_nr;
  
 -repeat:
 -	wait->flags = 0;
 -	if (behavior == EXCLUSIVE) {
 -		wait->flags = WQ_FLAG_EXCLUSIVE;
 -		if (--unfairness < 0)
 -			wait->flags |= WQ_FLAG_CUSTOM;
 -	}
 +	for (;;) {
 +		spin_lock_irq(&q->lock);
  
 -	/*
 -	 * Do one last check whether we can get the
 -	 * page bit synchronously.
 -	 *
 -	 * Do the folio_set_waiters() marking before that
 -	 * to let any waker we _just_ missed know they
 -	 * need to wake us up (otherwise they'll never
 -	 * even go to the slow case that looks at the
 -	 * page queue), and add ourselves to the wait
 -	 * queue if we need to sleep.
 -	 *
 -	 * This part needs to be done under the queue
 -	 * lock to avoid races.
 -	 */
 -	spin_lock_irq(&q->lock);
 -	folio_set_waiters(folio);
 -	if (!folio_trylock_flag(folio, bit_nr, wait))
 -		__add_wait_queue_entry_tail(q, wait);
 -	spin_unlock_irq(&q->lock);
 +		if (likely(list_empty(&wait->entry))) {
 +			__add_wait_queue_entry_tail(q, wait);
 +			SetPageWaiters(page);
 +		}
  
 -	/*
 -	 * From now on, all the logic will be based on
 -	 * the WQ_FLAG_WOKEN and WQ_FLAG_DONE flag, to
 -	 * see whether the page bit testing has already
 -	 * been done by the wake function.
 -	 *
 -	 * We can drop our reference to the folio.
 -	 */
 -	if (behavior == DROP)
 -		folio_put(folio);
 +		set_current_state(state);
  
 -	/*
 -	 * Note that until the "finish_wait()", or until
 -	 * we see the WQ_FLAG_WOKEN flag, we need to
 -	 * be very careful with the 'wait->flags', because
 -	 * we may race with a waker that sets them.
 -	 */
 -	for (;;) {
 -		unsigned int flags;
 +		spin_unlock_irq(&q->lock);
  
 -		set_current_state(state);
 +		bit_is_set = test_bit(bit_nr, &page->flags);
 +		if (behavior == DROP)
 +			put_page(page);
  
 -		/* Loop until we've been woken or interrupted */
 -		flags = smp_load_acquire(&wait->flags);
 -		if (!(flags & WQ_FLAG_WOKEN)) {
 -			if (signal_pending_state(state, current))
 +		if (likely(bit_is_set))
 +			io_schedule();
 +
 +		if (behavior == EXCLUSIVE) {
 +			if (!test_and_set_bit_lock(bit_nr, &page->flags))
 +				break;
 +		} else if (behavior == SHARED) {
 +			if (!test_bit(bit_nr, &page->flags))
  				break;
 +		}
  
 -			io_schedule();
 -			continue;
 +		if (signal_pending_state(state, current)) {
 +			ret = -EINTR;
 +			break;
  		}
  
 -		/* If we were non-exclusive, we're done */
 -		if (behavior != EXCLUSIVE)
 +		if (behavior == DROP) {
 +			/*
 +			 * We can no longer safely access page->flags:
 +			 * even if CONFIG_MEMORY_HOTREMOVE is not enabled,
 +			 * there is a risk of waiting forever on a page reused
 +			 * for something that keeps it locked indefinitely.
 +			 * But best check for -EINTR above before breaking.
 +			 */
  			break;
++<<<<<<< HEAD
++=======
+ 
+ 		/* If the waker got the lock for us, we're done */
+ 		if (flags & WQ_FLAG_DONE)
+ 			break;
+ 
+ 		/*
+ 		 * Otherwise, if we're getting the lock, we need to
+ 		 * try to get it ourselves.
+ 		 *
+ 		 * And if that fails, we'll have to retry this all.
+ 		 */
+ 		if (unlikely(test_and_set_bit(bit_nr, folio_flags(folio, 0))))
+ 			goto repeat;
+ 
+ 		wait->flags |= WQ_FLAG_DONE;
+ 		break;
+ 	}
+ 
+ 	/*
+ 	 * If a signal happened, this 'finish_wait()' may remove the last
+ 	 * waiter from the wait-queues, but the folio waiters bit will remain
+ 	 * set. That's ok. The next wakeup will take care of it, and trying
+ 	 * to do it here would be difficult and prone to races.
+ 	 */
+ 	finish_wait(q, wait);
+ 
+ 	if (thrashing) {
+ 		delayacct_thrashing_end(&in_thrashing);
+ 		psi_memstall_leave(&pflags);
+ 	}
+ 
+ 	/*
+ 	 * NOTE! The wait->flags weren't stable until we've done the
+ 	 * 'finish_wait()', and we could have exited the loop above due
+ 	 * to a signal, and had a wakeup event happen after the signal
+ 	 * test but before the 'finish_wait()'.
+ 	 *
+ 	 * So only after the finish_wait() can we reliably determine
+ 	 * if we got woken up or not, so we can now figure out the final
+ 	 * return value based on that state without races.
+ 	 *
+ 	 * Also note that WQ_FLAG_WOKEN is sufficient for a non-exclusive
+ 	 * waiter, but an exclusive one requires WQ_FLAG_DONE.
+ 	 */
+ 	if (behavior == EXCLUSIVE)
+ 		return wait->flags & WQ_FLAG_DONE ? 0 : -EINTR;
+ 
+ 	return wait->flags & WQ_FLAG_WOKEN ? 0 : -EINTR;
+ }
+ 
+ #ifdef CONFIG_MIGRATION
+ /**
+  * migration_entry_wait_on_locked - Wait for a migration entry to be removed
+  * @entry: migration swap entry.
+  * @ptep: mapped pte pointer. Will return with the ptep unmapped. Only required
+  *        for pte entries, pass NULL for pmd entries.
+  * @ptl: already locked ptl. This function will drop the lock.
+  *
+  * Wait for a migration entry referencing the given page to be removed. This is
+  * equivalent to put_and_wait_on_page_locked(page, TASK_UNINTERRUPTIBLE) except
+  * this can be called without taking a reference on the page. Instead this
+  * should be called while holding the ptl for the migration entry referencing
+  * the page.
+  *
+  * Returns after unmapping and unlocking the pte/ptl with pte_unmap_unlock().
+  *
+  * This follows the same logic as folio_wait_bit_common() so see the comments
+  * there.
+  */
+ void migration_entry_wait_on_locked(swp_entry_t entry, pte_t *ptep,
+ 				spinlock_t *ptl)
+ {
+ 	struct wait_page_queue wait_page;
+ 	wait_queue_entry_t *wait = &wait_page.wait;
+ 	bool thrashing = false;
+ 	unsigned long pflags;
+ 	bool in_thrashing;
+ 	wait_queue_head_t *q;
+ 	struct folio *folio = page_folio(pfn_swap_entry_to_page(entry));
+ 
+ 	q = folio_waitqueue(folio);
+ 	if (!folio_test_uptodate(folio) && folio_test_workingset(folio)) {
+ 		delayacct_thrashing_start(&in_thrashing);
+ 		psi_memstall_enter(&pflags);
+ 		thrashing = true;
+ 	}
+ 
+ 	init_wait(wait);
+ 	wait->func = wake_page_function;
+ 	wait_page.folio = folio;
+ 	wait_page.bit_nr = PG_locked;
+ 	wait->flags = 0;
+ 
+ 	spin_lock_irq(&q->lock);
+ 	folio_set_waiters(folio);
+ 	if (!folio_trylock_flag(folio, PG_locked, wait))
+ 		__add_wait_queue_entry_tail(q, wait);
+ 	spin_unlock_irq(&q->lock);
+ 
+ 	/*
+ 	 * If a migration entry exists for the page the migration path must hold
+ 	 * a valid reference to the page, and it must take the ptl to remove the
+ 	 * migration entry. So the page is valid until the ptl is dropped.
+ 	 */
+ 	if (ptep)
+ 		pte_unmap_unlock(ptep, ptl);
+ 	else
+ 		spin_unlock(ptl);
+ 
+ 	for (;;) {
+ 		unsigned int flags;
+ 
+ 		set_current_state(TASK_UNINTERRUPTIBLE);
+ 
+ 		/* Loop until we've been woken or interrupted */
+ 		flags = smp_load_acquire(&wait->flags);
+ 		if (!(flags & WQ_FLAG_WOKEN)) {
+ 			if (signal_pending_state(TASK_UNINTERRUPTIBLE, current))
+ 				break;
+ 
+ 			io_schedule();
+ 			continue;
++>>>>>>> aa1cf99b87e9 (delayacct: support re-entrance detection of thrashing accounting)
  		}
 -		break;
  	}
  
  	finish_wait(q, wait);
  
  	if (thrashing) {
++<<<<<<< HEAD
 +		if (!PageSwapBacked(page))
 +			delayacct_thrashing_end();
++=======
+ 		delayacct_thrashing_end(&in_thrashing);
++>>>>>>> aa1cf99b87e9 (delayacct: support re-entrance detection of thrashing accounting)
  		psi_memstall_leave(&pflags);
  	}
 +
 +	/*
 +	 * A signal could leave PageWaiters set. Clearing it here if
 +	 * !waitqueue_active would be possible (by open-coding finish_wait),
 +	 * but still fail to catch it in the case of wait hash collision. We
 +	 * already can fail to clear wait hash collision cases, so don't
 +	 * bother with signals either.
 +	 */
 +
 +	return ret;
  }
 -#endif
  
 -void folio_wait_bit(struct folio *folio, int bit_nr)
 +void wait_on_page_bit(struct page *page, int bit_nr)
  {
 -	folio_wait_bit_common(folio, bit_nr, TASK_UNINTERRUPTIBLE, SHARED);
 +	wait_queue_head_t *q = page_waitqueue(page);
 +	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, SHARED);
  }
 -EXPORT_SYMBOL(folio_wait_bit);
 +EXPORT_SYMBOL(wait_on_page_bit);
  
 -int folio_wait_bit_killable(struct folio *folio, int bit_nr)
 +int wait_on_page_bit_killable(struct page *page, int bit_nr)
  {
 -	return folio_wait_bit_common(folio, bit_nr, TASK_KILLABLE, SHARED);
 +	wait_queue_head_t *q = page_waitqueue(page);
 +	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, SHARED);
  }
 -EXPORT_SYMBOL(folio_wait_bit_killable);
 +EXPORT_SYMBOL(wait_on_page_bit_killable);
  
  /**
 - * folio_put_wait_locked - Drop a reference and wait for it to be unlocked
 - * @folio: The folio to wait for.
 - * @state: The sleep state (TASK_KILLABLE, TASK_UNINTERRUPTIBLE, etc).
 + * put_and_wait_on_page_locked - Drop a reference and wait for it to be unlocked
 + * @page: The page to wait for.
   *
 - * The caller should hold a reference on @folio.  They expect the page to
 + * The caller should hold a reference on @page.  They expect the page to
   * become unlocked relatively soon, but do not wish to hold up migration
 - * (for example) by holding the reference while waiting for the folio to
 + * (for example) by holding the reference while waiting for the page to
   * come unlocked.  After this function returns, the caller should not
 - * dereference @folio.
 - *
 - * Return: 0 if the folio was unlocked or -EINTR if interrupted by a signal.
 + * dereference @page.
   */
 -int folio_put_wait_locked(struct folio *folio, int state)
 +void put_and_wait_on_page_locked(struct page *page)
  {
 -	return folio_wait_bit_common(folio, PG_locked, state, DROP);
 +	wait_queue_head_t *q;
 +
 +	page = compound_head(page);
 +	q = page_waitqueue(page);
 +	wait_on_page_bit_common(q, page, PG_locked, TASK_UNINTERRUPTIBLE, DROP);
  }
  
  /**
* Unmerged path include/linux/delayacct.h
diff --git a/include/linux/sched.h b/include/linux/sched.h
index e33c3b66d5ae..ef21ff67d4fd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -921,6 +921,10 @@ struct task_struct {
 #ifdef CONFIG_IOMMU_SVA
 	RH_KABI_FILL_HOLE(unsigned	pasid_activated:1)
 #endif
+#ifdef CONFIG_TASK_DELAY_ACCT
+	/* delay due to memory thrashing */
+	unsigned                        in_thrashing:1;
+#endif
 
 	unsigned long			atomic_flags; /* Flags requiring atomic access. */
 
* Unmerged path kernel/delayacct.c
* Unmerged path mm/filemap.c
