x86: Prepare asm files for straight-line-speculation

jira LE-1907
cve CVE-2022-29901
cve CVE-2022-29900
cve CVE-2022-23825
cve CVE-2022-23816
Rebuild_History Non-Buildable kernel-3.10.0-1160.80.1.el7
commit-author Peter Zijlstra <peterz@infradead.org>
commit f94909ceb1ed4bfdb2ada72f93236305e6d6951f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-3.10.0-1160.80.1.el7/f94909ce.failed

Replace all ret/retq instructions with RET in preparation of making
RET a macro. Since AS is case insensitive it's a big no-op without
RET defined.

  find arch/x86/ -name \*.S | while read file
  do
	sed -i 's/\<ret[q]*\>/RET/' $file
  done

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Signed-off-by: Borislav Petkov <bp@suse.de>
Link: https://lore.kernel.org/r/20211204134907.905503893@infradead.org
(cherry picked from commit f94909ceb1ed4bfdb2ada72f93236305e6d6951f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/boot/compressed/efi_thunk_64.S
#	arch/x86/boot/compressed/head_64.S
#	arch/x86/boot/compressed/mem_encrypt.S
#	arch/x86/crypto/aegis128-aesni-asm.S
#	arch/x86/crypto/aesni-intel_asm.S
#	arch/x86/crypto/aesni-intel_avx-x86_64.S
#	arch/x86/crypto/blake2s-core.S
#	arch/x86/crypto/blowfish-x86_64-asm_64.S
#	arch/x86/crypto/camellia-aesni-avx-asm_64.S
#	arch/x86/crypto/camellia-aesni-avx2-asm_64.S
#	arch/x86/crypto/camellia-x86_64-asm_64.S
#	arch/x86/crypto/cast5-avx-x86_64-asm_64.S
#	arch/x86/crypto/cast6-avx-x86_64-asm_64.S
#	arch/x86/crypto/chacha-avx2-x86_64.S
#	arch/x86/crypto/chacha-avx512vl-x86_64.S
#	arch/x86/crypto/chacha-ssse3-x86_64.S
#	arch/x86/crypto/crc32-pclmul_asm.S
#	arch/x86/crypto/crc32c-pcl-intel-asm_64.S
#	arch/x86/crypto/crct10dif-pcl-asm_64.S
#	arch/x86/crypto/des3_ede-asm_64.S
#	arch/x86/crypto/ghash-clmulni-intel_asm.S
#	arch/x86/crypto/nh-avx2-x86_64.S
#	arch/x86/crypto/nh-sse2-x86_64.S
#	arch/x86/crypto/serpent-avx-x86_64-asm_64.S
#	arch/x86/crypto/serpent-avx2-asm_64.S
#	arch/x86/crypto/serpent-sse2-i586-asm_32.S
#	arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
#	arch/x86/crypto/sha1_ni_asm.S
#	arch/x86/crypto/sha256-avx-asm.S
#	arch/x86/crypto/sha256-avx2-asm.S
#	arch/x86/crypto/sha256-ssse3-asm.S
#	arch/x86/crypto/sha256_ni_asm.S
#	arch/x86/crypto/sha512-avx-asm.S
#	arch/x86/crypto/sha512-avx2-asm.S
#	arch/x86/crypto/sha512-ssse3-asm.S
#	arch/x86/crypto/sm4-aesni-avx-asm_64.S
#	arch/x86/crypto/sm4-aesni-avx2-asm_64.S
#	arch/x86/crypto/twofish-avx-x86_64-asm_64.S
#	arch/x86/crypto/twofish-i586-asm_32.S
#	arch/x86/crypto/twofish-x86_64-asm_64-3way.S
#	arch/x86/crypto/twofish-x86_64-asm_64.S
#	arch/x86/entry/entry_32.S
#	arch/x86/entry/entry_64.S
#	arch/x86/entry/thunk_32.S
#	arch/x86/entry/thunk_64.S
#	arch/x86/entry/vdso/vdso32/system_call.S
#	arch/x86/entry/vdso/vsgx.S
#	arch/x86/kernel/acpi/wakeup_32.S
#	arch/x86/kernel/ftrace_32.S
#	arch/x86/kernel/ftrace_64.S
#	arch/x86/kernel/irqflags.S
#	arch/x86/kernel/relocate_kernel_32.S
#	arch/x86/kernel/relocate_kernel_64.S
#	arch/x86/kernel/sev_verify_cbit.S
#	arch/x86/kernel/verify_cpu.S
#	arch/x86/kvm/svm/vmenter.S
#	arch/x86/kvm/vmx/vmenter.S
#	arch/x86/lib/atomic64_386_32.S
#	arch/x86/lib/atomic64_cx8_32.S
#	arch/x86/lib/checksum_32.S
#	arch/x86/lib/clear_page_64.S
#	arch/x86/lib/copy_mc_64.S
#	arch/x86/lib/copy_page_64.S
#	arch/x86/lib/copy_user_64.S
#	arch/x86/lib/csum-copy_64.S
#	arch/x86/lib/getuser.S
#	arch/x86/lib/hweight.S
#	arch/x86/lib/iomap_copy_64.S
#	arch/x86/lib/memcpy_64.S
#	arch/x86/lib/memmove_64.S
#	arch/x86/lib/memset_64.S
#	arch/x86/lib/msr-reg.S
#	arch/x86/lib/putuser.S
#	arch/x86/lib/retpoline.S
#	arch/x86/math-emu/div_small.S
#	arch/x86/math-emu/mul_Xsig.S
#	arch/x86/math-emu/polynom_Xsig.S
#	arch/x86/math-emu/reg_norm.S
#	arch/x86/math-emu/reg_u_sub.S
#	arch/x86/math-emu/round_Xsig.S
#	arch/x86/math-emu/shr_Xsig.S
#	arch/x86/math-emu/wm_shrx.S
#	arch/x86/mm/mem_encrypt_boot.S
#	arch/x86/platform/efi/efi_stub_32.S
#	arch/x86/platform/efi/efi_stub_64.S
#	arch/x86/platform/efi/efi_thunk_64.S
#	arch/x86/platform/olpc/xo1-wakeup.S
#	arch/x86/power/hibernate_asm_32.S
#	arch/x86/power/hibernate_asm_64.S
#	arch/x86/xen/xen-asm.S
#	arch/x86/xen/xen-head.S
diff --cc arch/x86/boot/compressed/efi_thunk_64.S
index 630384a4c14a,70052779b235..000000000000
--- a/arch/x86/boot/compressed/efi_thunk_64.S
+++ b/arch/x86/boot/compressed/efi_thunk_64.S
@@@ -84,26 -88,13 +84,31 @@@ ENTRY(efi64_thunk
  	/*
  	 * Convert 32-bit status code into 64-bit.
  	 */
 -	roll	$1, %eax
 -	rorq	$1, %rax
 -
 +	test	%rax, %rax
 +	jz	1f
 +	movl	%eax, %ecx
 +	andl	$0x0fffffff, %ecx
 +	andl	$0xf0000000, %eax
 +	shl	$32, %rax
 +	or	%rcx, %rax
 +1:
 +	addq	$8, %rsp
  	pop	%rbx
  	pop	%rbp
++<<<<<<< HEAD
 +	ret
 +ENDPROC(efi64_thunk)
 +
 +ENTRY(efi_exit32)
 +	movq	func_rt_ptr(%rip), %rax
 +	push	%rax
 +	mov	%rdi, %rax
 +	ret
 +ENDPROC(efi_exit32)
++=======
+ 	RET
+ SYM_FUNC_END(__efi64_thunk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	.code32
  /*
diff --cc arch/x86/boot/compressed/head_64.S
index 0b23c8f08670,fd9441f40457..000000000000
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@@ -466,28 -707,255 +466,249 @@@ gdt
  	.quad	0x00cf92000000ffff	/* __KERNEL_DS */
  	.quad	0x0080890000000000	/* TS descriptor */
  	.quad   0x0000000000000000	/* TS continued */
 -SYM_DATA_END_LABEL(gdt, SYM_L_LOCAL, gdt_end)
 +gdt_end:
  
 -SYM_DATA_START(boot_idt_desc)
 -	.word	boot_idt_end - boot_idt - 1
 -	.quad	0
 -SYM_DATA_END(boot_idt_desc)
 -	.balign 8
 -SYM_DATA_START(boot_idt)
 -	.rept	BOOT_IDT_ENTRIES
 -	.quad	0
 +#ifdef CONFIG_EFI_STUB
 +efi_config:
  	.quad	0
 -	.endr
 -SYM_DATA_END_LABEL(boot_idt, SYM_L_GLOBAL, boot_idt_end)
  
 -#ifdef CONFIG_AMD_MEM_ENCRYPT
 -SYM_DATA_START(boot32_idt_desc)
 -	.word   boot32_idt_end - boot32_idt - 1
 -	.long   0
 -SYM_DATA_END(boot32_idt_desc)
 -	.balign 8
 -SYM_DATA_START(boot32_idt)
 -	.rept 32
 -	.quad 0
 -	.endr
 -SYM_DATA_END_LABEL(boot32_idt, SYM_L_GLOBAL, boot32_idt_end)
 +#ifdef CONFIG_EFI_MIXED
++<<<<<<< HEAD
 +	.global efi32_config
 +efi32_config:
 +	.fill	11,8,0
 +	.quad	efi64_thunk
 +	.fill	1,8,0
 +	.byte	0
  #endif
  
 -#ifdef CONFIG_EFI_STUB
 -SYM_DATA(image_offset, .long 0)
 -#endif
 -#ifdef CONFIG_EFI_MIXED
 +	.global efi64_config
 +efi64_config:
 +	.fill	11,8,0
 +	.quad	efi_call
 +	.fill	1,8,0
 +	.byte	1
 +#endif /* CONFIG_EFI_STUB */
++=======
+ SYM_DATA_LOCAL(efi32_boot_args, .long 0, 0, 0)
+ SYM_DATA(efi_is64, .byte 1)
+ 
+ #define ST32_boottime		60 // offsetof(efi_system_table_32_t, boottime)
+ #define BS32_handle_protocol	88 // offsetof(efi_boot_services_32_t, handle_protocol)
+ #define LI32_image_base		32 // offsetof(efi_loaded_image_32_t, image_base)
+ 
+ 	__HEAD
+ 	.code32
+ SYM_FUNC_START(efi32_pe_entry)
+ /*
+  * efi_status_t efi32_pe_entry(efi_handle_t image_handle,
+  *			       efi_system_table_32_t *sys_table)
+  */
+ 
+ 	pushl	%ebp
+ 	movl	%esp, %ebp
+ 	pushl	%eax				// dummy push to allocate loaded_image
+ 
+ 	pushl	%ebx				// save callee-save registers
+ 	pushl	%edi
+ 
+ 	call	verify_cpu			// check for long mode support
+ 	testl	%eax, %eax
+ 	movl	$0x80000003, %eax		// EFI_UNSUPPORTED
+ 	jnz	2f
+ 
+ 	call	1f
+ 1:	pop	%ebx
+ 	subl	$ rva(1b), %ebx
+ 
+ 	/* Get the loaded image protocol pointer from the image handle */
+ 	leal	-4(%ebp), %eax
+ 	pushl	%eax				// &loaded_image
+ 	leal	rva(loaded_image_proto)(%ebx), %eax
+ 	pushl	%eax				// pass the GUID address
+ 	pushl	8(%ebp)				// pass the image handle
+ 
+ 	/*
+ 	 * Note the alignment of the stack frame.
+ 	 *   sys_table
+ 	 *   handle             <-- 16-byte aligned on entry by ABI
+ 	 *   return address
+ 	 *   frame pointer
+ 	 *   loaded_image       <-- local variable
+ 	 *   saved %ebx		<-- 16-byte aligned here
+ 	 *   saved %edi
+ 	 *   &loaded_image
+ 	 *   &loaded_image_proto
+ 	 *   handle             <-- 16-byte aligned for call to handle_protocol
+ 	 */
+ 
+ 	movl	12(%ebp), %eax			// sys_table
+ 	movl	ST32_boottime(%eax), %eax	// sys_table->boottime
+ 	call	*BS32_handle_protocol(%eax)	// sys_table->boottime->handle_protocol
+ 	addl	$12, %esp			// restore argument space
+ 	testl	%eax, %eax
+ 	jnz	2f
+ 
+ 	movl	8(%ebp), %ecx			// image_handle
+ 	movl	12(%ebp), %edx			// sys_table
+ 	movl	-4(%ebp), %esi			// loaded_image
+ 	movl	LI32_image_base(%esi), %esi	// loaded_image->image_base
+ 	movl	%ebx, %ebp			// startup_32 for efi32_pe_stub_entry
+ 	/*
+ 	 * We need to set the image_offset variable here since startup_32() will
+ 	 * use it before we get to the 64-bit efi_pe_entry() in C code.
+ 	 */
+ 	subl	%esi, %ebx
+ 	movl	%ebx, rva(image_offset)(%ebp)	// save image_offset
+ 	jmp	efi32_pe_stub_entry
+ 
+ 2:	popl	%edi				// restore callee-save registers
+ 	popl	%ebx
+ 	leave
+ 	RET
+ SYM_FUNC_END(efi32_pe_entry)
+ 
+ 	.section ".rodata"
+ 	/* EFI loaded image protocol GUID */
+ 	.balign 4
+ SYM_DATA_START_LOCAL(loaded_image_proto)
+ 	.long	0x5b1b31a1
+ 	.word	0x9562, 0x11d2
+ 	.byte	0x8e, 0x3f, 0x00, 0xa0, 0xc9, 0x69, 0x72, 0x3b
+ SYM_DATA_END(loaded_image_proto)
+ #endif
+ 
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ 	__HEAD
+ 	.code32
+ /*
+  * Write an IDT entry into boot32_idt
+  *
+  * Parameters:
+  *
+  * %eax:	Handler address
+  * %edx:	Vector number
+  *
+  * Physical offset is expected in %ebp
+  */
+ SYM_FUNC_START(startup32_set_idt_entry)
+ 	push    %ebx
+ 	push    %ecx
+ 
+ 	/* IDT entry address to %ebx */
+ 	leal    rva(boot32_idt)(%ebp), %ebx
+ 	shl	$3, %edx
+ 	addl    %edx, %ebx
+ 
+ 	/* Build IDT entry, lower 4 bytes */
+ 	movl    %eax, %edx
+ 	andl    $0x0000ffff, %edx	# Target code segment offset [15:0]
+ 	movl    $__KERNEL32_CS, %ecx	# Target code segment selector
+ 	shl     $16, %ecx
+ 	orl     %ecx, %edx
+ 
+ 	/* Store lower 4 bytes to IDT */
+ 	movl    %edx, (%ebx)
+ 
+ 	/* Build IDT entry, upper 4 bytes */
+ 	movl    %eax, %edx
+ 	andl    $0xffff0000, %edx	# Target code segment offset [31:16]
+ 	orl     $0x00008e00, %edx	# Present, Type 32-bit Interrupt Gate
+ 
+ 	/* Store upper 4 bytes to IDT */
+ 	movl    %edx, 4(%ebx)
+ 
+ 	pop     %ecx
+ 	pop     %ebx
+ 	RET
+ SYM_FUNC_END(startup32_set_idt_entry)
+ #endif
+ 
+ SYM_FUNC_START(startup32_load_idt)
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ 	/* #VC handler */
+ 	leal    rva(startup32_vc_handler)(%ebp), %eax
+ 	movl    $X86_TRAP_VC, %edx
+ 	call    startup32_set_idt_entry
+ 
+ 	/* Load IDT */
+ 	leal	rva(boot32_idt)(%ebp), %eax
+ 	movl	%eax, rva(boot32_idt_desc+2)(%ebp)
+ 	lidt    rva(boot32_idt_desc)(%ebp)
+ #endif
+ 	RET
+ SYM_FUNC_END(startup32_load_idt)
+ 
+ /*
+  * Check for the correct C-bit position when the startup_32 boot-path is used.
+  *
+  * The check makes use of the fact that all memory is encrypted when paging is
+  * disabled. The function creates 64 bits of random data using the RDRAND
+  * instruction. RDRAND is mandatory for SEV guests, so always available. If the
+  * hypervisor violates that the kernel will crash right here.
+  *
+  * The 64 bits of random data are stored to a memory location and at the same
+  * time kept in the %eax and %ebx registers. Since encryption is always active
+  * when paging is off the random data will be stored encrypted in main memory.
+  *
+  * Then paging is enabled. When the C-bit position is correct all memory is
+  * still mapped encrypted and comparing the register values with memory will
+  * succeed. An incorrect C-bit position will map all memory unencrypted, so that
+  * the compare will use the encrypted random data and fail.
+  */
+ SYM_FUNC_START(startup32_check_sev_cbit)
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ 	pushl	%eax
+ 	pushl	%ebx
+ 	pushl	%ecx
+ 	pushl	%edx
+ 
+ 	/* Check for non-zero sev_status */
+ 	movl	rva(sev_status)(%ebp), %eax
+ 	testl	%eax, %eax
+ 	jz	4f
+ 
+ 	/*
+ 	 * Get two 32-bit random values - Don't bail out if RDRAND fails
+ 	 * because it is better to prevent forward progress if no random value
+ 	 * can be gathered.
+ 	 */
+ 1:	rdrand	%eax
+ 	jnc	1b
+ 2:	rdrand	%ebx
+ 	jnc	2b
+ 
+ 	/* Store to memory and keep it in the registers */
+ 	movl	%eax, rva(sev_check_data)(%ebp)
+ 	movl	%ebx, rva(sev_check_data+4)(%ebp)
+ 
+ 	/* Enable paging to see if encryption is active */
+ 	movl	%cr0, %edx			 /* Backup %cr0 in %edx */
+ 	movl	$(X86_CR0_PG | X86_CR0_PE), %ecx /* Enable Paging and Protected mode */
+ 	movl	%ecx, %cr0
+ 
+ 	cmpl	%eax, rva(sev_check_data)(%ebp)
+ 	jne	3f
+ 	cmpl	%ebx, rva(sev_check_data+4)(%ebp)
+ 	jne	3f
+ 
+ 	movl	%edx, %cr0	/* Restore previous %cr0 */
+ 
+ 	jmp	4f
+ 
+ 3:	/* Check failed - hlt the machine */
+ 	hlt
+ 	jmp	3b
+ 
+ 4:
+ 	popl	%edx
+ 	popl	%ecx
+ 	popl	%ebx
+ 	popl	%eax
+ #endif
+ 	RET
+ SYM_FUNC_END(startup32_check_sev_cbit)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * Stack and heap for uncompression
diff --cc arch/x86/boot/compressed/mem_encrypt.S
index 69c86bd81b5e,a63424d13627..000000000000
--- a/arch/x86/boot/compressed/mem_encrypt.S
+++ b/arch/x86/boot/compressed/mem_encrypt.S
@@@ -67,13 -58,136 +67,141 @@@ ENTRY(get_sev_encryption_bit
  
  #endif	/* CONFIG_AMD_MEM_ENCRYPT */
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(get_sev_encryption_bit)
++=======
+ 	RET
+ SYM_FUNC_END(get_sev_encryption_bit)
+ 
+ /**
+  * sev_es_req_cpuid - Request a CPUID value from the Hypervisor using
+  *		      the GHCB MSR protocol
+  *
+  * @%eax:	Register to request (0=EAX, 1=EBX, 2=ECX, 3=EDX)
+  * @%edx:	CPUID Function
+  *
+  * Returns 0 in %eax on success, non-zero on failure
+  * %edx returns CPUID value on success
+  */
+ SYM_CODE_START_LOCAL(sev_es_req_cpuid)
+ 	shll	$30, %eax
+ 	orl     $0x00000004, %eax
+ 	movl    $MSR_AMD64_SEV_ES_GHCB, %ecx
+ 	wrmsr
+ 	rep; vmmcall		# VMGEXIT
+ 	rdmsr
+ 
+ 	/* Check response */
+ 	movl	%eax, %ecx
+ 	andl	$0x3ffff000, %ecx	# Bits [12-29] MBZ
+ 	jnz	2f
+ 
+ 	/* Check return code */
+ 	andl    $0xfff, %eax
+ 	cmpl    $5, %eax
+ 	jne	2f
+ 
+ 	/* All good - return success */
+ 	xorl	%eax, %eax
+ 1:
+ 	RET
+ 2:
+ 	movl	$-1, %eax
+ 	jmp	1b
+ SYM_CODE_END(sev_es_req_cpuid)
+ 
+ SYM_CODE_START(startup32_vc_handler)
+ 	pushl	%eax
+ 	pushl	%ebx
+ 	pushl	%ecx
+ 	pushl	%edx
+ 
+ 	/* Keep CPUID function in %ebx */
+ 	movl	%eax, %ebx
+ 
+ 	/* Check if error-code == SVM_EXIT_CPUID */
+ 	cmpl	$0x72, 16(%esp)
+ 	jne	.Lfail
+ 
+ 	movl	$0, %eax		# Request CPUID[fn].EAX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 12(%esp)		# Store result
+ 
+ 	movl	$1, %eax		# Request CPUID[fn].EBX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 8(%esp)		# Store result
+ 
+ 	movl	$2, %eax		# Request CPUID[fn].ECX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 4(%esp)		# Store result
+ 
+ 	movl	$3, %eax		# Request CPUID[fn].EDX
+ 	movl	%ebx, %edx		# CPUID fn
+ 	call	sev_es_req_cpuid	# Call helper
+ 	testl	%eax, %eax		# Check return code
+ 	jnz	.Lfail
+ 	movl	%edx, 0(%esp)		# Store result
+ 
+ 	/*
+ 	 * Sanity check CPUID results from the Hypervisor. See comment in
+ 	 * do_vc_no_ghcb() for more details on why this is necessary.
+ 	 */
+ 
+ 	/* Fail if SEV leaf not available in CPUID[0x80000000].EAX */
+ 	cmpl    $0x80000000, %ebx
+ 	jne     .Lcheck_sev
+ 	cmpl    $0x8000001f, 12(%esp)
+ 	jb      .Lfail
+ 	jmp     .Ldone
+ 
+ .Lcheck_sev:
+ 	/* Fail if SEV bit not set in CPUID[0x8000001f].EAX[1] */
+ 	cmpl    $0x8000001f, %ebx
+ 	jne     .Ldone
+ 	btl     $1, 12(%esp)
+ 	jnc     .Lfail
+ 
+ .Ldone:
+ 	popl	%edx
+ 	popl	%ecx
+ 	popl	%ebx
+ 	popl	%eax
+ 
+ 	/* Remove error code */
+ 	addl	$4, %esp
+ 
+ 	/* Jump over CPUID instruction */
+ 	addl	$2, (%esp)
+ 
+ 	iret
+ .Lfail:
+ 	/* Send terminate request to Hypervisor */
+ 	movl    $0x100, %eax
+ 	xorl    %edx, %edx
+ 	movl    $MSR_AMD64_SEV_ES_GHCB, %ecx
+ 	wrmsr
+ 	rep; vmmcall
+ 
+ 	/* If request fails, go to hlt loop */
+ 	hlt
+ 	jmp .Lfail
+ SYM_CODE_END(startup32_vc_handler)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	.code64
 +ENTRY(get_sev_encryption_mask)
 +	xor	%rax, %rax
  
 -#include "../../kernel/sev_verify_cbit.S"
 -SYM_FUNC_START(set_sev_encryption_mask)
  #ifdef CONFIG_AMD_MEM_ENCRYPT
  	push	%rbp
  	push	%rdx
@@@ -95,5 -220,15 +223,20 @@@
  	pop	%rbp
  #endif
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(get_sev_encryption_mask)
++=======
+ 	xor	%rax, %rax
+ 	RET
+ SYM_FUNC_END(set_sev_encryption_mask)
+ 
+ 	.data
+ 
+ #ifdef CONFIG_AMD_MEM_ENCRYPT
+ 	.balign	8
+ SYM_DATA(sme_me_mask,		.quad 0)
+ SYM_DATA(sev_status,		.quad 0)
+ SYM_DATA(sev_check_data,	.quad 0)
+ #endif
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/aesni-intel_asm.S
index 429be3c61c3f,363699dd7220..000000000000
--- a/arch/x86/crypto/aesni-intel_asm.S
+++ b/arch/x86/crypto/aesni-intel_asm.S
@@@ -1376,198 -1587,15 +1376,207 @@@ _esb_loop_\@
  * poly = x^128 + x^127 + x^126 + x^121 + 1
  *
  *****************************************************************************/
 -SYM_FUNC_START(aesni_gcm_dec)
 -	FUNC_SAVE
 +ENTRY(aesni_gcm_dec)
 +	push	%r12
 +	push	%r13
 +	push	%r14
 +	mov	%rsp, %r14
 +/*
 +* states of %xmm registers %xmm6:%xmm15 not saved
 +* all %xmm registers are clobbered
 +*/
 +	sub	$VARIABLE_OFFSET, %rsp
 +	and	$~63, %rsp                        # align rsp to 64 bytes
 +	mov	%arg6, %r12
 +	movdqu	(%r12), %xmm13			  # %xmm13 = HashKey
 +        movdqa  SHUF_MASK(%rip), %xmm2
 +	PSHUFB_XMM %xmm2, %xmm13
 +
++<<<<<<< HEAD
 +
 +# Precompute HashKey<<1 (mod poly) from the hash key (required for GHASH)
 +
 +	movdqa	%xmm13, %xmm2
 +	psllq	$1, %xmm13
 +	psrlq	$63, %xmm2
 +	movdqa	%xmm2, %xmm1
 +	pslldq	$8, %xmm2
 +	psrldq	$8, %xmm1
 +	por	%xmm2, %xmm13
 +
 +        # Reduction
 +
 +	pshufd	$0x24, %xmm1, %xmm2
 +	pcmpeqd TWOONE(%rip), %xmm2
 +	pand	POLY(%rip), %xmm2
 +	pxor	%xmm2, %xmm13     # %xmm13 holds the HashKey<<1 (mod poly)
  
 +
 +        # Decrypt first few blocks
 +
 +	movdqa %xmm13, HashKey(%rsp)           # store HashKey<<1 (mod poly)
 +	mov %arg4, %r13    # save the number of bytes of plaintext/ciphertext
 +	and $-16, %r13                      # %r13 = %r13 - (%r13 mod 16)
 +	mov %r13, %r12
 +	and $(3<<4), %r12
 +	jz _initial_num_blocks_is_0_decrypt
 +	cmp $(2<<4), %r12
 +	jb _initial_num_blocks_is_1_decrypt
 +	je _initial_num_blocks_is_2_decrypt
 +_initial_num_blocks_is_3_decrypt:
 +	INITIAL_BLOCKS_DEC 3, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 5, 678, dec
 +	sub	$48, %r13
 +	jmp	_initial_blocks_decrypted
 +_initial_num_blocks_is_2_decrypt:
 +	INITIAL_BLOCKS_DEC	2, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 6, 78, dec
 +	sub	$32, %r13
 +	jmp	_initial_blocks_decrypted
 +_initial_num_blocks_is_1_decrypt:
 +	INITIAL_BLOCKS_DEC	1, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 7, 8, dec
 +	sub	$16, %r13
 +	jmp	_initial_blocks_decrypted
 +_initial_num_blocks_is_0_decrypt:
 +	INITIAL_BLOCKS_DEC	0, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 8, 0, dec
 +_initial_blocks_decrypted:
 +	cmp	$0, %r13
 +	je	_zero_cipher_left_decrypt
 +	sub	$64, %r13
 +	je	_four_cipher_left_decrypt
 +_decrypt_by_4:
 +	GHASH_4_ENCRYPT_4_PARALLEL_DEC	%xmm9, %xmm10, %xmm11, %xmm12, %xmm13, \
 +%xmm14, %xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, dec
 +	add	$64, %r11
 +	sub	$64, %r13
 +	jne	_decrypt_by_4
 +_four_cipher_left_decrypt:
 +	GHASH_LAST_4	%xmm9, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, \
 +%xmm15, %xmm1, %xmm2, %xmm3, %xmm4, %xmm8
 +_zero_cipher_left_decrypt:
 +	mov	%arg4, %r13
 +	and	$15, %r13				# %r13 = arg4 (mod 16)
 +	je	_multiple_of_16_bytes_decrypt
 +
 +        # Handle the last <16 byte block separately
 +
 +	paddd ONE(%rip), %xmm0         # increment CNT to get Yn
 +        movdqa SHUF_MASK(%rip), %xmm10
 +	PSHUFB_XMM %xmm10, %xmm0
 +
 +	ENCRYPT_SINGLE_BLOCK  %xmm0, %xmm1    # E(K, Yn)
 +	sub $16, %r11
 +	add %r13, %r11
 +	movdqu (%arg3,%r11,1), %xmm1   # receive the last <16 byte block
 +	lea SHIFT_MASK+16(%rip), %r12
 +	sub %r13, %r12
 +# adjust the shuffle mask pointer to be able to shift 16-%r13 bytes
 +# (%r13 is the number of bytes in plaintext mod 16)
 +	movdqu (%r12), %xmm2           # get the appropriate shuffle mask
 +	PSHUFB_XMM %xmm2, %xmm1            # right shift 16-%r13 butes
 +
 +	movdqa  %xmm1, %xmm2
 +	pxor %xmm1, %xmm0            # Ciphertext XOR E(K, Yn)
 +	movdqu ALL_F-SHIFT_MASK(%r12), %xmm1
 +	# get the appropriate mask to mask out top 16-%r13 bytes of %xmm0
 +	pand %xmm1, %xmm0            # mask out top 16-%r13 bytes of %xmm0
 +	pand    %xmm1, %xmm2
 +        movdqa SHUF_MASK(%rip), %xmm10
 +	PSHUFB_XMM %xmm10 ,%xmm2
 +
 +	pxor %xmm2, %xmm8
 +	GHASH_MUL %xmm8, %xmm13, %xmm9, %xmm10, %xmm11, %xmm5, %xmm6
 +	          # GHASH computation for the last <16 byte block
 +	sub %r13, %r11
 +	add $16, %r11
 +
 +        # output %r13 bytes
 +	MOVQ_R64_XMM	%xmm0, %rax
 +	cmp	$8, %r13
 +	jle	_less_than_8_bytes_left_decrypt
 +	mov	%rax, (%arg2 , %r11, 1)
 +	add	$8, %r11
 +	psrldq	$8, %xmm0
 +	MOVQ_R64_XMM	%xmm0, %rax
 +	sub	$8, %r13
 +_less_than_8_bytes_left_decrypt:
 +	mov	%al,  (%arg2, %r11, 1)
 +	add	$1, %r11
 +	shr	$8, %rax
 +	sub	$1, %r13
 +	jne	_less_than_8_bytes_left_decrypt
 +_multiple_of_16_bytes_decrypt:
 +	mov	arg8, %r12		  # %r13 = aadLen (number of bytes)
 +	shl	$3, %r12		  # convert into number of bits
 +	movd	%r12d, %xmm15		  # len(A) in %xmm15
 +	shl	$3, %arg4		  # len(C) in bits (*128)
 +	MOVQ_R64_XMM	%arg4, %xmm1
 +	pslldq	$8, %xmm15		  # %xmm15 = len(A)||0x0000000000000000
 +	pxor	%xmm1, %xmm15		  # %xmm15 = len(A)||len(C)
 +	pxor	%xmm15, %xmm8
 +	GHASH_MUL	%xmm8, %xmm13, %xmm9, %xmm10, %xmm11, %xmm5, %xmm6
 +	         # final GHASH computation
 +        movdqa SHUF_MASK(%rip), %xmm10
 +	PSHUFB_XMM %xmm10, %xmm8
 +
 +	mov	%arg5, %rax		  # %rax = *Y0
 +	movdqu	(%rax), %xmm0		  # %xmm0 = Y0
 +	ENCRYPT_SINGLE_BLOCK	%xmm0,  %xmm1	  # E(K, Y0)
 +	pxor	%xmm8, %xmm0
 +_return_T_decrypt:
 +	mov	arg9, %r10                # %r10 = authTag
 +	mov	arg10, %r11               # %r11 = auth_tag_len
 +	cmp	$16, %r11
 +	je	_T_16_decrypt
 +	cmp	$8, %r11
 +	jl	_T_4_decrypt
 +_T_8_decrypt:
 +	MOVQ_R64_XMM	%xmm0, %rax
 +	mov	%rax, (%r10)
 +	add	$8, %r10
 +	sub	$8, %r11
 +	psrldq	$8, %xmm0
 +	cmp	$0, %r11
 +	je	_return_T_done_decrypt
 +_T_4_decrypt:
 +	movd	%xmm0, %eax
 +	mov	%eax, (%r10)
 +	add	$4, %r10
 +	sub	$4, %r11
 +	psrldq	$4, %xmm0
 +	cmp	$0, %r11
 +	je	_return_T_done_decrypt
 +_T_123_decrypt:
 +	movd	%xmm0, %eax
 +	cmp	$2, %r11
 +	jl	_T_1_decrypt
 +	mov	%ax, (%r10)
 +	cmp	$2, %r11
 +	je	_return_T_done_decrypt
 +	add	$2, %r10
 +	sar	$16, %eax
 +_T_1_decrypt:
 +	mov	%al, (%r10)
 +	jmp	_return_T_done_decrypt
 +_T_16_decrypt:
 +	movdqu	%xmm0, (%r10)
 +_return_T_done_decrypt:
 +	mov	%r14, %rsp
 +	pop	%r14
 +	pop	%r13
 +	pop	%r12
 +	ret
 +ENDPROC(aesni_gcm_dec)
++=======
+ 	GCM_INIT %arg6, arg7, arg8, arg9
+ 	GCM_ENC_DEC dec
+ 	GCM_COMPLETE arg10, arg11
+ 	FUNC_RESTORE
+ 	RET
+ SYM_FUNC_END(aesni_gcm_dec)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
  /*****************************************************************************
@@@ -1643,212 -1673,81 +1652,282 @@@
  *
  *                         AAD Format with 64-bit Extended Sequence Number
  *
 +* aadLen:
 +*       from the definition of the spec, aadLen can only be 8 or 12 bytes.
 +*       The code supports 16 too but for other sizes, the code will fail.
 +*
 +* TLen:
 +*       from the definition of the spec, TLen can only be 8, 12 or 16 bytes.
 +*       For other sizes, the code will fail.
 +*
  * poly = x^128 + x^127 + x^126 + x^121 + 1
  ***************************************************************************/
 -SYM_FUNC_START(aesni_gcm_enc)
 -	FUNC_SAVE
 +ENTRY(aesni_gcm_enc)
 +	push	%r12
 +	push	%r13
 +	push	%r14
 +	mov	%rsp, %r14
 +#
 +# states of %xmm registers %xmm6:%xmm15 not saved
 +# all %xmm registers are clobbered
 +#
 +	sub	$VARIABLE_OFFSET, %rsp
 +	and	$~63, %rsp
 +	mov	%arg6, %r12
 +	movdqu	(%r12), %xmm13
 +        movdqa  SHUF_MASK(%rip), %xmm2
 +	PSHUFB_XMM %xmm2, %xmm13
 +
 +
++<<<<<<< HEAD
 +# precompute HashKey<<1 mod poly from the HashKey (required for GHASH)
 +
 +	movdqa	%xmm13, %xmm2
 +	psllq	$1, %xmm13
 +	psrlq	$63, %xmm2
 +	movdqa	%xmm2, %xmm1
 +	pslldq	$8, %xmm2
 +	psrldq	$8, %xmm1
 +	por	%xmm2, %xmm13
 +
 +        # reduce HashKey<<1
 +
 +	pshufd	$0x24, %xmm1, %xmm2
 +	pcmpeqd TWOONE(%rip), %xmm2
 +	pand	POLY(%rip), %xmm2
 +	pxor	%xmm2, %xmm13
 +	movdqa	%xmm13, HashKey(%rsp)
 +	mov	%arg4, %r13            # %xmm13 holds HashKey<<1 (mod poly)
 +	and	$-16, %r13
 +	mov	%r13, %r12
  
 -	GCM_INIT %arg6, arg7, arg8, arg9
 -	GCM_ENC_DEC enc
 +        # Encrypt first few blocks
 +
 +	and	$(3<<4), %r12
 +	jz	_initial_num_blocks_is_0_encrypt
 +	cmp	$(2<<4), %r12
 +	jb	_initial_num_blocks_is_1_encrypt
 +	je	_initial_num_blocks_is_2_encrypt
 +_initial_num_blocks_is_3_encrypt:
 +	INITIAL_BLOCKS_ENC	3, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 5, 678, enc
 +	sub	$48, %r13
 +	jmp	_initial_blocks_encrypted
 +_initial_num_blocks_is_2_encrypt:
 +	INITIAL_BLOCKS_ENC	2, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 6, 78, enc
 +	sub	$32, %r13
 +	jmp	_initial_blocks_encrypted
 +_initial_num_blocks_is_1_encrypt:
 +	INITIAL_BLOCKS_ENC	1, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 7, 8, enc
 +	sub	$16, %r13
 +	jmp	_initial_blocks_encrypted
 +_initial_num_blocks_is_0_encrypt:
 +	INITIAL_BLOCKS_ENC	0, %xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \
 +%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 8, 0, enc
 +_initial_blocks_encrypted:
 +
 +        # Main loop - Encrypt remaining blocks
 +
 +	cmp	$0, %r13
 +	je	_zero_cipher_left_encrypt
 +	sub	$64, %r13
 +	je	_four_cipher_left_encrypt
 +_encrypt_by_4_encrypt:
 +	GHASH_4_ENCRYPT_4_PARALLEL_ENC	%xmm9, %xmm10, %xmm11, %xmm12, %xmm13, \
 +%xmm14, %xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, enc
 +	add	$64, %r11
 +	sub	$64, %r13
 +	jne	_encrypt_by_4_encrypt
 +_four_cipher_left_encrypt:
 +	GHASH_LAST_4	%xmm9, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, \
 +%xmm15, %xmm1, %xmm2, %xmm3, %xmm4, %xmm8
 +_zero_cipher_left_encrypt:
 +	mov	%arg4, %r13
 +	and	$15, %r13			# %r13 = arg4 (mod 16)
 +	je	_multiple_of_16_bytes_encrypt
 +
 +         # Handle the last <16 Byte block separately
 +	paddd ONE(%rip), %xmm0                # INCR CNT to get Yn
 +        movdqa SHUF_MASK(%rip), %xmm10
 +	PSHUFB_XMM %xmm10, %xmm0
 +
 +
 +	ENCRYPT_SINGLE_BLOCK	%xmm0, %xmm1        # Encrypt(K, Yn)
 +	sub $16, %r11
 +	add %r13, %r11
 +	movdqu (%arg3,%r11,1), %xmm1     # receive the last <16 byte blocks
 +	lea SHIFT_MASK+16(%rip), %r12
 +	sub %r13, %r12
 +	# adjust the shuffle mask pointer to be able to shift 16-r13 bytes
 +	# (%r13 is the number of bytes in plaintext mod 16)
 +	movdqu	(%r12), %xmm2           # get the appropriate shuffle mask
 +	PSHUFB_XMM	%xmm2, %xmm1            # shift right 16-r13 byte
 +	pxor	%xmm1, %xmm0            # Plaintext XOR Encrypt(K, Yn)
 +	movdqu	ALL_F-SHIFT_MASK(%r12), %xmm1
 +	# get the appropriate mask to mask out top 16-r13 bytes of xmm0
 +	pand	%xmm1, %xmm0            # mask out top 16-r13 bytes of xmm0
 +        movdqa SHUF_MASK(%rip), %xmm10
 +	PSHUFB_XMM %xmm10,%xmm0
 +
 +	pxor	%xmm0, %xmm8
 +	GHASH_MUL %xmm8, %xmm13, %xmm9, %xmm10, %xmm11, %xmm5, %xmm6
 +	# GHASH computation for the last <16 byte block
 +	sub	%r13, %r11
 +	add	$16, %r11
 +
 +	movdqa SHUF_MASK(%rip), %xmm10
 +	PSHUFB_XMM %xmm10, %xmm0
 +
 +	# shuffle xmm0 back to output as ciphertext
 +
 +        # Output %r13 bytes
 +	MOVQ_R64_XMM %xmm0, %rax
 +	cmp $8, %r13
 +	jle _less_than_8_bytes_left_encrypt
 +	mov %rax, (%arg2 , %r11, 1)
 +	add $8, %r11
 +	psrldq $8, %xmm0
 +	MOVQ_R64_XMM %xmm0, %rax
 +	sub $8, %r13
 +_less_than_8_bytes_left_encrypt:
 +	mov %al,  (%arg2, %r11, 1)
 +	add $1, %r11
 +	shr $8, %rax
 +	sub $1, %r13
 +	jne _less_than_8_bytes_left_encrypt
 +_multiple_of_16_bytes_encrypt:
 +	mov	arg8, %r12    # %r12 = addLen (number of bytes)
 +	shl	$3, %r12
 +	movd	%r12d, %xmm15       # len(A) in %xmm15
 +	shl	$3, %arg4               # len(C) in bits (*128)
 +	MOVQ_R64_XMM	%arg4, %xmm1
 +	pslldq	$8, %xmm15          # %xmm15 = len(A)||0x0000000000000000
 +	pxor	%xmm1, %xmm15       # %xmm15 = len(A)||len(C)
 +	pxor	%xmm15, %xmm8
 +	GHASH_MUL	%xmm8, %xmm13, %xmm9, %xmm10, %xmm11, %xmm5, %xmm6
 +	# final GHASH computation
 +        movdqa SHUF_MASK(%rip), %xmm10
 +	PSHUFB_XMM %xmm10, %xmm8         # perform a 16 byte swap
  
 +	mov	%arg5, %rax		       # %rax  = *Y0
 +	movdqu	(%rax), %xmm0		       # %xmm0 = Y0
 +	ENCRYPT_SINGLE_BLOCK	%xmm0, %xmm15         # Encrypt(K, Y0)
 +	pxor	%xmm8, %xmm0
 +_return_T_encrypt:
 +	mov	arg9, %r10                     # %r10 = authTag
 +	mov	arg10, %r11                    # %r11 = auth_tag_len
 +	cmp	$16, %r11
 +	je	_T_16_encrypt
 +	cmp	$8, %r11
 +	jl	_T_4_encrypt
 +_T_8_encrypt:
 +	MOVQ_R64_XMM	%xmm0, %rax
 +	mov	%rax, (%r10)
 +	add	$8, %r10
 +	sub	$8, %r11
 +	psrldq	$8, %xmm0
 +	cmp	$0, %r11
 +	je	_return_T_done_encrypt
 +_T_4_encrypt:
 +	movd	%xmm0, %eax
 +	mov	%eax, (%r10)
 +	add	$4, %r10
 +	sub	$4, %r11
 +	psrldq	$4, %xmm0
 +	cmp	$0, %r11
 +	je	_return_T_done_encrypt
 +_T_123_encrypt:
 +	movd	%xmm0, %eax
 +	cmp	$2, %r11
 +	jl	_T_1_encrypt
 +	mov	%ax, (%r10)
 +	cmp	$2, %r11
 +	je	_return_T_done_encrypt
 +	add	$2, %r10
 +	sar	$16, %eax
 +_T_1_encrypt:
 +	mov	%al, (%r10)
 +	jmp	_return_T_done_encrypt
 +_T_16_encrypt:
 +	movdqu	%xmm0, (%r10)
 +_return_T_done_encrypt:
 +	mov	%r14, %rsp
 +	pop	%r14
 +	pop	%r13
 +	pop	%r12
 +	ret
 +ENDPROC(aesni_gcm_enc)
++=======
+ 	GCM_COMPLETE arg10, arg11
+ 	FUNC_RESTORE
+ 	RET
+ SYM_FUNC_END(aesni_gcm_enc)
+ 
+ /*****************************************************************************
+ * void aesni_gcm_init(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
+ *                     struct gcm_context_data *data,
+ *                                         // context data
+ *                     u8 *iv,             // Pre-counter block j0: 4 byte salt (from Security Association)
+ *                                         // concatenated with 8 byte Initialisation Vector (from IPSec ESP Payload)
+ *                                         // concatenated with 0x00000001. 16-byte aligned pointer.
+ *                     u8 *hash_subkey,    // H, the Hash sub key input. Data starts on a 16-byte boundary.
+ *                     const u8 *aad,      // Additional Authentication Data (AAD)
+ *                     u64 aad_len)        // Length of AAD in bytes.
+ */
+ SYM_FUNC_START(aesni_gcm_init)
+ 	FUNC_SAVE
+ 	GCM_INIT %arg3, %arg4,%arg5, %arg6
+ 	FUNC_RESTORE
+ 	RET
+ SYM_FUNC_END(aesni_gcm_init)
+ 
+ /*****************************************************************************
+ * void aesni_gcm_enc_update(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
+ *                    struct gcm_context_data *data,
+ *                                        // context data
+ *                    u8 *out,            // Ciphertext output. Encrypt in-place is allowed.
+ *                    const u8 *in,       // Plaintext input
+ *                    u64 plaintext_len,  // Length of data in bytes for encryption.
+ */
+ SYM_FUNC_START(aesni_gcm_enc_update)
+ 	FUNC_SAVE
+ 	GCM_ENC_DEC enc
+ 	FUNC_RESTORE
+ 	RET
+ SYM_FUNC_END(aesni_gcm_enc_update)
+ 
+ /*****************************************************************************
+ * void aesni_gcm_dec_update(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
+ *                    struct gcm_context_data *data,
+ *                                        // context data
+ *                    u8 *out,            // Ciphertext output. Encrypt in-place is allowed.
+ *                    const u8 *in,       // Plaintext input
+ *                    u64 plaintext_len,  // Length of data in bytes for encryption.
+ */
+ SYM_FUNC_START(aesni_gcm_dec_update)
+ 	FUNC_SAVE
+ 	GCM_ENC_DEC dec
+ 	FUNC_RESTORE
+ 	RET
+ SYM_FUNC_END(aesni_gcm_dec_update)
+ 
+ /*****************************************************************************
+ * void aesni_gcm_finalize(void *aes_ctx,      // AES Key schedule. Starts on a 16 byte boundary.
+ *                    struct gcm_context_data *data,
+ *                                        // context data
+ *                    u8 *auth_tag,       // Authenticated Tag output.
+ *                    u64 auth_tag_len);  // Authenticated Tag Length in bytes. Valid values are 16 (most likely),
+ *                                        // 12 or 8.
+ */
+ SYM_FUNC_START(aesni_gcm_finalize)
+ 	FUNC_SAVE
+ 	GCM_COMPLETE %arg3 %arg4
+ 	FUNC_RESTORE
+ 	RET
+ SYM_FUNC_END(aesni_gcm_finalize)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  #endif
  
@@@ -1864,12 -1762,11 +1943,18 @@@ _key_expansion_256a
  	pxor %xmm1, %xmm0
  	movaps %xmm0, (TKEYP)
  	add $0x10, TKEYP
++<<<<<<< HEAD
 +	ret
 +ENDPROC(_key_expansion_128)
 +ENDPROC(_key_expansion_256a)
++=======
+ 	RET
+ SYM_FUNC_END(_key_expansion_256a)
+ SYM_FUNC_END_ALIAS(_key_expansion_128)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START_LOCAL(_key_expansion_192a)
 +.align 4
 +_key_expansion_192a:
  	pshufd $0b01010101, %xmm1, %xmm1
  	shufps $0b00010000, %xmm0, %xmm4
  	pxor %xmm4, %xmm0
@@@ -1890,11 -1787,10 +1975,16 @@@
  	shufps $0b01001110, %xmm2, %xmm1
  	movaps %xmm1, 0x10(TKEYP)
  	add $0x20, TKEYP
++<<<<<<< HEAD
 +	ret
 +ENDPROC(_key_expansion_192a)
++=======
+ 	RET
+ SYM_FUNC_END(_key_expansion_192a)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START_LOCAL(_key_expansion_192b)
 +.align 4
 +_key_expansion_192b:
  	pshufd $0b01010101, %xmm1, %xmm1
  	shufps $0b00010000, %xmm0, %xmm4
  	pxor %xmm4, %xmm0
@@@ -1910,11 -1806,10 +2000,16 @@@
  
  	movaps %xmm0, (TKEYP)
  	add $0x10, TKEYP
++<<<<<<< HEAD
 +	ret
 +ENDPROC(_key_expansion_192b)
++=======
+ 	RET
+ SYM_FUNC_END(_key_expansion_192b)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START_LOCAL(_key_expansion_256b)
 +.align 4
 +_key_expansion_256b:
  	pshufd $0b10101010, %xmm1, %xmm1
  	shufps $0b00010000, %xmm2, %xmm4
  	pxor %xmm4, %xmm2
@@@ -1923,8 -1818,8 +2018,13 @@@
  	pxor %xmm1, %xmm2
  	movaps %xmm2, (TKEYP)
  	add $0x10, TKEYP
++<<<<<<< HEAD
 +	ret
 +ENDPROC(_key_expansion_256b)
++=======
+ 	RET
+ SYM_FUNC_END(_key_expansion_256b)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * int aesni_set_key(struct crypto_aes_ctx *ctx, const u8 *in_key,
@@@ -2038,13 -1933,13 +2138,18 @@@ ENTRY(aesni_set_key
  	popl KEYP
  #endif
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_set_key)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_set_key)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
 - * void aesni_enc(const void *ctx, u8 *dst, const u8 *src)
 + * void aesni_enc(struct crypto_aes_ctx *ctx, u8 *dst, const u8 *src)
   */
 -SYM_FUNC_START(aesni_enc)
 +ENTRY(aesni_enc)
  	FRAME_BEGIN
  #ifndef __x86_64__
  	pushl KEYP
@@@ -2062,8 -1957,8 +2167,13 @@@
  	popl KEYP
  #endif
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_enc)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_enc)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * _aesni_enc1:		internal ABI
@@@ -2101,27 -1995,27 +2211,33 @@@ _aesni_enc1
  .align 4
  .Lenc128:
  	movaps -0x20(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps -0x10(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps (TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x10(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x20(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x30(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x40(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x50(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x60(TKEYP), KEY
 -	aesenc KEY, STATE
 +	AESENC KEY STATE
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESENCLAST KEY STATE
 +	ret
 +ENDPROC(_aesni_enc1)
++=======
+ 	aesenclast KEY, STATE
+ 	RET
+ SYM_FUNC_END(_aesni_enc1)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * _aesni_enc4:	internal ABI
@@@ -2180,62 -2073,62 +2296,71 @@@ _aesni_enc4
  #.align 4
  .L4enc128:
  	movaps -0x20(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps -0x10(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps (TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x10(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x20(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x30(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x40(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x50(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x60(TKEYP), KEY
 -	aesenc KEY, STATE1
 -	aesenc KEY, STATE2
 -	aesenc KEY, STATE3
 -	aesenc KEY, STATE4
 +	AESENC KEY STATE1
 +	AESENC KEY STATE2
 +	AESENC KEY STATE3
 +	AESENC KEY STATE4
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESENCLAST KEY STATE1		# last round
 +	AESENCLAST KEY STATE2
 +	AESENCLAST KEY STATE3
 +	AESENCLAST KEY STATE4
 +	ret
 +ENDPROC(_aesni_enc4)
++=======
+ 	aesenclast KEY, STATE1		# last round
+ 	aesenclast KEY, STATE2
+ 	aesenclast KEY, STATE3
+ 	aesenclast KEY, STATE4
+ 	RET
+ SYM_FUNC_END(_aesni_enc4)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
 - * void aesni_dec (const void *ctx, u8 *dst, const u8 *src)
 + * void aesni_dec (struct crypto_aes_ctx *ctx, u8 *dst, const u8 *src)
   */
 -SYM_FUNC_START(aesni_dec)
 +ENTRY(aesni_dec)
  	FRAME_BEGIN
  #ifndef __x86_64__
  	pushl KEYP
@@@ -2254,8 -2147,8 +2379,13 @@@
  	popl KEYP
  #endif
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_dec)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_dec)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * _aesni_dec1:		internal ABI
@@@ -2293,27 -2185,27 +2423,33 @@@ _aesni_dec1
  .align 4
  .Ldec128:
  	movaps -0x20(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps -0x10(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps (TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x10(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x20(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x30(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x40(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x50(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x60(TKEYP), KEY
 -	aesdec KEY, STATE
 +	AESDEC KEY STATE
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESDECLAST KEY STATE
 +	ret
 +ENDPROC(_aesni_dec1)
++=======
+ 	aesdeclast KEY, STATE
+ 	RET
+ SYM_FUNC_END(_aesni_dec1)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * _aesni_dec4:	internal ABI
@@@ -2372,57 -2263,57 +2508,66 @@@ _aesni_dec4
  .align 4
  .L4dec128:
  	movaps -0x20(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps -0x10(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps (TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x10(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x20(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x30(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x40(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x50(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x60(TKEYP), KEY
 -	aesdec KEY, STATE1
 -	aesdec KEY, STATE2
 -	aesdec KEY, STATE3
 -	aesdec KEY, STATE4
 +	AESDEC KEY STATE1
 +	AESDEC KEY STATE2
 +	AESDEC KEY STATE3
 +	AESDEC KEY STATE4
  	movaps 0x70(TKEYP), KEY
++<<<<<<< HEAD
 +	AESDECLAST KEY STATE1		# last round
 +	AESDECLAST KEY STATE2
 +	AESDECLAST KEY STATE3
 +	AESDECLAST KEY STATE4
 +	ret
 +ENDPROC(_aesni_dec4)
++=======
+ 	aesdeclast KEY, STATE1		# last round
+ 	aesdeclast KEY, STATE2
+ 	aesdeclast KEY, STATE3
+ 	aesdeclast KEY, STATE4
+ 	RET
+ SYM_FUNC_END(_aesni_dec4)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * void aesni_ecb_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
@@@ -2481,8 -2372,8 +2626,13 @@@ ENTRY(aesni_ecb_enc
  	popl LEN
  #endif
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_ecb_enc)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_ecb_enc)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * void aesni_ecb_dec(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
@@@ -2542,8 -2433,8 +2692,13 @@@ ENTRY(aesni_ecb_dec
  	popl LEN
  #endif
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_ecb_dec)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_ecb_dec)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * void aesni_cbc_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
@@@ -2586,8 -2477,8 +2741,13 @@@ ENTRY(aesni_cbc_enc
  	popl IVP
  #endif
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_cbc_enc)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_cbc_enc)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * void aesni_cbc_dec(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
@@@ -2679,16 -2570,143 +2839,139 @@@ ENTRY(aesni_cbc_dec
  	popl IVP
  #endif
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_cbc_dec)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_cbc_dec)
+ 
+ /*
+  * void aesni_cts_cbc_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
+  *			  size_t len, u8 *iv)
+  */
+ SYM_FUNC_START(aesni_cts_cbc_enc)
+ 	FRAME_BEGIN
+ #ifndef __x86_64__
+ 	pushl IVP
+ 	pushl LEN
+ 	pushl KEYP
+ 	pushl KLEN
+ 	movl (FRAME_OFFSET+20)(%esp), KEYP	# ctx
+ 	movl (FRAME_OFFSET+24)(%esp), OUTP	# dst
+ 	movl (FRAME_OFFSET+28)(%esp), INP	# src
+ 	movl (FRAME_OFFSET+32)(%esp), LEN	# len
+ 	movl (FRAME_OFFSET+36)(%esp), IVP	# iv
+ 	lea .Lcts_permute_table, T1
+ #else
+ 	lea .Lcts_permute_table(%rip), T1
+ #endif
+ 	mov 480(KEYP), KLEN
+ 	movups (IVP), STATE
+ 	sub $16, LEN
+ 	mov T1, IVP
+ 	add $32, IVP
+ 	add LEN, T1
+ 	sub LEN, IVP
+ 	movups (T1), %xmm4
+ 	movups (IVP), %xmm5
+ 
+ 	movups (INP), IN1
+ 	add LEN, INP
+ 	movups (INP), IN2
+ 
+ 	pxor IN1, STATE
+ 	call _aesni_enc1
+ 
+ 	pshufb %xmm5, IN2
+ 	pxor STATE, IN2
+ 	pshufb %xmm4, STATE
+ 	add OUTP, LEN
+ 	movups STATE, (LEN)
+ 
+ 	movaps IN2, STATE
+ 	call _aesni_enc1
+ 	movups STATE, (OUTP)
+ 
+ #ifndef __x86_64__
+ 	popl KLEN
+ 	popl KEYP
+ 	popl LEN
+ 	popl IVP
+ #endif
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(aesni_cts_cbc_enc)
+ 
+ /*
+  * void aesni_cts_cbc_dec(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
+  *			  size_t len, u8 *iv)
+  */
+ SYM_FUNC_START(aesni_cts_cbc_dec)
+ 	FRAME_BEGIN
+ #ifndef __x86_64__
+ 	pushl IVP
+ 	pushl LEN
+ 	pushl KEYP
+ 	pushl KLEN
+ 	movl (FRAME_OFFSET+20)(%esp), KEYP	# ctx
+ 	movl (FRAME_OFFSET+24)(%esp), OUTP	# dst
+ 	movl (FRAME_OFFSET+28)(%esp), INP	# src
+ 	movl (FRAME_OFFSET+32)(%esp), LEN	# len
+ 	movl (FRAME_OFFSET+36)(%esp), IVP	# iv
+ 	lea .Lcts_permute_table, T1
+ #else
+ 	lea .Lcts_permute_table(%rip), T1
+ #endif
+ 	mov 480(KEYP), KLEN
+ 	add $240, KEYP
+ 	movups (IVP), IV
+ 	sub $16, LEN
+ 	mov T1, IVP
+ 	add $32, IVP
+ 	add LEN, T1
+ 	sub LEN, IVP
+ 	movups (T1), %xmm4
+ 
+ 	movups (INP), STATE
+ 	add LEN, INP
+ 	movups (INP), IN1
+ 
+ 	call _aesni_dec1
+ 	movaps STATE, IN2
+ 	pshufb %xmm4, STATE
+ 	pxor IN1, STATE
+ 
+ 	add OUTP, LEN
+ 	movups STATE, (LEN)
+ 
+ 	movups (IVP), %xmm0
+ 	pshufb %xmm0, IN1
+ 	pblendvb IN2, IN1
+ 	movaps IN1, STATE
+ 	call _aesni_dec1
+ 
+ 	pxor IV, STATE
+ 	movups STATE, (OUTP)
+ 
+ #ifndef __x86_64__
+ 	popl KLEN
+ 	popl KEYP
+ 	popl LEN
+ 	popl IVP
+ #endif
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(aesni_cts_cbc_dec)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 +#ifdef __x86_64__
  .pushsection .rodata
  .align 16
 -.Lcts_permute_table:
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -	.byte		0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07
 -	.byte		0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -	.byte		0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80
 -#ifdef __x86_64__
  .Lbswap_mask:
  	.byte 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0
 -#endif
  .popsection
  
 -#ifdef __x86_64__
  /*
   * _aesni_inc_init:	internal ABI
   *	setup registers used by _aesni_inc
@@@ -2700,16 -2718,15 +2983,23 @@@
   *	INC:	== 1, in little endian
   *	BSWAP_MASK == endian swapping mask
   */
 -SYM_FUNC_START_LOCAL(_aesni_inc_init)
 +.align 4
 +_aesni_inc_init:
  	movaps .Lbswap_mask, BSWAP_MASK
  	movaps IV, CTR
 -	pshufb BSWAP_MASK, CTR
 +	PSHUFB_XMM BSWAP_MASK CTR
  	mov $1, TCTR_LOW
++<<<<<<< HEAD
 +	MOVQ_R64_XMM TCTR_LOW INC
 +	MOVQ_R64_XMM CTR TCTR_LOW
 +	ret
 +ENDPROC(_aesni_inc_init)
++=======
+ 	movq TCTR_LOW, INC
+ 	movq CTR, TCTR_LOW
+ 	RET
+ SYM_FUNC_END(_aesni_inc_init)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * _aesni_inc:		internal ABI
@@@ -2736,9 -2752,9 +3026,15 @@@ _aesni_inc
  	psrldq $8, INC
  .Linc_low:
  	movaps CTR, IV
++<<<<<<< HEAD
 +	PSHUFB_XMM BSWAP_MASK IV
 +	ret
 +ENDPROC(_aesni_inc)
++=======
+ 	pshufb BSWAP_MASK, IV
+ 	RET
+ SYM_FUNC_END(_aesni_inc)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * void aesni_ctr_enc(struct crypto_aes_ctx *ctx, const u8 *dst, u8 *src,
@@@ -2800,8 -2816,16 +3096,21 @@@ ENTRY(aesni_ctr_enc
  	movups IV, (IVP)
  .Lctr_enc_just_ret:
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_ctr_enc)
++=======
+ 	RET
+ SYM_FUNC_END(aesni_ctr_enc)
+ 
+ #endif
+ 
+ .section	.rodata.cst16.gf128mul_x_ble_mask, "aM", @progbits, 16
+ .align 16
+ .Lgf128mul_x_ble_mask:
+ 	.octa 0x00000000000000010000000000000087
+ .previous
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * _aesni_gf128mul_x_ble:		internal ABI
@@@ -2860,25 -2892,151 +3169,146 @@@ ENTRY(aesni_xts_crypt8
  
  	_aesni_gf128mul_x_ble()
  	movdqa IV, STATE4
 -	movdqu 0x30(INP), IN
 -	pxor IN, STATE4
 +	movdqu 0x30(INP), INC
 +	pxor INC, STATE4
  	movdqu IV, 0x30(OUTP)
  
 -	call _aesni_enc4
 +	CALL_NOSPEC %r11
  
 -	movdqu 0x00(OUTP), IN
 -	pxor IN, STATE1
 +	movdqu 0x00(OUTP), INC
 +	pxor INC, STATE1
  	movdqu STATE1, 0x00(OUTP)
  
 -	movdqu 0x10(OUTP), IN
 -	pxor IN, STATE2
 -	movdqu STATE2, 0x10(OUTP)
 -
 -	movdqu 0x20(OUTP), IN
 -	pxor IN, STATE3
 -	movdqu STATE3, 0x20(OUTP)
 -
 -	movdqu 0x30(OUTP), IN
 -	pxor IN, STATE4
 -	movdqu STATE4, 0x30(OUTP)
 -
  	_aesni_gf128mul_x_ble()
++<<<<<<< HEAD
++=======
+ 
+ 	add $64, INP
+ 	add $64, OUTP
+ 	test LEN, LEN
+ 	jnz .Lxts_enc_loop4
+ 
+ .Lxts_enc_ret_iv:
+ 	movups IV, (IVP)
+ 
+ .Lxts_enc_ret:
+ #ifndef __x86_64__
+ 	popl KLEN
+ 	popl KEYP
+ 	popl LEN
+ 	popl IVP
+ #endif
+ 	FRAME_END
+ 	RET
+ 
+ .Lxts_enc_1x:
+ 	add $64, LEN
+ 	jz .Lxts_enc_ret_iv
+ 	sub $16, LEN
+ 	jl .Lxts_enc_cts4
+ 
+ .Lxts_enc_loop1:
+ 	movdqu (INP), STATE
+ 	pxor IV, STATE
+ 	call _aesni_enc1
+ 	pxor IV, STATE
+ 	_aesni_gf128mul_x_ble()
+ 
+ 	test LEN, LEN
+ 	jz .Lxts_enc_out
+ 
+ 	add $16, INP
+ 	sub $16, LEN
+ 	jl .Lxts_enc_cts1
+ 
+ 	movdqu STATE, (OUTP)
+ 	add $16, OUTP
+ 	jmp .Lxts_enc_loop1
+ 
+ .Lxts_enc_out:
+ 	movdqu STATE, (OUTP)
+ 	jmp .Lxts_enc_ret_iv
+ 
+ .Lxts_enc_cts4:
+ 	movdqa STATE4, STATE
+ 	sub $16, OUTP
+ 
+ .Lxts_enc_cts1:
+ #ifndef __x86_64__
+ 	lea .Lcts_permute_table, T1
+ #else
+ 	lea .Lcts_permute_table(%rip), T1
+ #endif
+ 	add LEN, INP		/* rewind input pointer */
+ 	add $16, LEN		/* # bytes in final block */
+ 	movups (INP), IN1
+ 
+ 	mov T1, IVP
+ 	add $32, IVP
+ 	add LEN, T1
+ 	sub LEN, IVP
+ 	add OUTP, LEN
+ 
+ 	movups (T1), %xmm4
+ 	movaps STATE, IN2
+ 	pshufb %xmm4, STATE
+ 	movups STATE, (LEN)
+ 
+ 	movups (IVP), %xmm0
+ 	pshufb %xmm0, IN1
+ 	pblendvb IN2, IN1
+ 	movaps IN1, STATE
+ 
+ 	pxor IV, STATE
+ 	call _aesni_enc1
+ 	pxor IV, STATE
+ 
+ 	movups STATE, (OUTP)
+ 	jmp .Lxts_enc_ret
+ SYM_FUNC_END(aesni_xts_encrypt)
+ 
+ /*
+  * void aesni_xts_decrypt(const struct crypto_aes_ctx *ctx, u8 *dst,
+  *			  const u8 *src, unsigned int len, le128 *iv)
+  */
+ SYM_FUNC_START(aesni_xts_decrypt)
+ 	FRAME_BEGIN
+ #ifndef __x86_64__
+ 	pushl IVP
+ 	pushl LEN
+ 	pushl KEYP
+ 	pushl KLEN
+ 	movl (FRAME_OFFSET+20)(%esp), KEYP	# ctx
+ 	movl (FRAME_OFFSET+24)(%esp), OUTP	# dst
+ 	movl (FRAME_OFFSET+28)(%esp), INP	# src
+ 	movl (FRAME_OFFSET+32)(%esp), LEN	# len
+ 	movl (FRAME_OFFSET+36)(%esp), IVP	# iv
+ 	movdqa .Lgf128mul_x_ble_mask, GF128MUL_MASK
+ #else
+ 	movdqa .Lgf128mul_x_ble_mask(%rip), GF128MUL_MASK
+ #endif
+ 	movups (IVP), IV
+ 
+ 	mov 480(KEYP), KLEN
+ 	add $240, KEYP
+ 
+ 	test $15, LEN
+ 	jz .Lxts_dec_loop4
+ 	sub $16, LEN
+ 
+ .Lxts_dec_loop4:
+ 	sub $64, LEN
+ 	jl .Lxts_dec_1x
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  	movdqa IV, STATE1
 -	movdqu 0x00(INP), IN
 -	pxor IN, STATE1
 -	movdqu IV, 0x00(OUTP)
 +	movdqu 0x40(INP), INC
 +	pxor INC, STATE1
 +	movdqu IV, 0x40(OUTP)
 +
 +	movdqu 0x10(OUTP), INC
 +	pxor INC, STATE2
 +	movdqu STATE2, 0x10(OUTP)
  
  	_aesni_gf128mul_x_ble()
  	movdqa IV, STATE2
@@@ -2901,34 -3075,89 +3331,38 @@@
  	movdqu STATE4, 0x30(OUTP)
  
  	_aesni_gf128mul_x_ble()
 +	movdqa IV, STATE4
 +	movdqu 0x70(INP), INC
 +	pxor INC, STATE4
 +	movdqu IV, 0x70(OUTP)
  
 -	add $64, INP
 -	add $64, OUTP
 -	test LEN, LEN
 -	jnz .Lxts_dec_loop4
 -
 -.Lxts_dec_ret_iv:
 +	_aesni_gf128mul_x_ble()
  	movups IV, (IVP)
  
 -.Lxts_dec_ret:
 -#ifndef __x86_64__
 -	popl KLEN
 -	popl KEYP
 -	popl LEN
 -	popl IVP
 -#endif
 -	FRAME_END
 -	RET
 -
 -.Lxts_dec_1x:
 -	add $64, LEN
 -	jz .Lxts_dec_ret_iv
 -
 -.Lxts_dec_loop1:
 -	movdqu (INP), STATE
 -
 -	add $16, INP
 -	sub $16, LEN
 -	jl .Lxts_dec_cts1
 -
 -	pxor IV, STATE
 -	call _aesni_dec1
 -	pxor IV, STATE
 -	_aesni_gf128mul_x_ble()
 +	CALL_NOSPEC %r11
  
 -	test LEN, LEN
 -	jz .Lxts_dec_out
 +	movdqu 0x40(OUTP), INC
 +	pxor INC, STATE1
 +	movdqu STATE1, 0x40(OUTP)
  
 -	movdqu STATE, (OUTP)
 -	add $16, OUTP
 -	jmp .Lxts_dec_loop1
 +	movdqu 0x50(OUTP), INC
 +	pxor INC, STATE2
 +	movdqu STATE2, 0x50(OUTP)
  
 -.Lxts_dec_out:
 -	movdqu STATE, (OUTP)
 -	jmp .Lxts_dec_ret_iv
 +	movdqu 0x60(OUTP), INC
 +	pxor INC, STATE3
 +	movdqu STATE3, 0x60(OUTP)
  
 -.Lxts_dec_cts1:
 -	movdqa IV, STATE4
 -	_aesni_gf128mul_x_ble()
 +	movdqu 0x70(OUTP), INC
 +	pxor INC, STATE4
 +	movdqu STATE4, 0x70(OUTP)
  
 -	pxor IV, STATE
 -	call _aesni_dec1
 -	pxor IV, STATE
 +	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(aesni_xts_crypt8)
++=======
++	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -#ifndef __x86_64__
 -	lea .Lcts_permute_table, T1
 -#else
 -	lea .Lcts_permute_table(%rip), T1
  #endif
 -	add LEN, INP		/* rewind input pointer */
 -	add $16, LEN		/* # bytes in final block */
 -	movups (INP), IN1
 -
 -	mov T1, IVP
 -	add $32, IVP
 -	add LEN, T1
 -	sub LEN, IVP
 -	add OUTP, LEN
 -
 -	movups (T1), %xmm4
 -	movaps STATE, IN2
 -	pshufb %xmm4, STATE
 -	movups STATE, (LEN)
 -
 -	movups (IVP), %xmm0
 -	pshufb %xmm0, IN1
 -	pblendvb IN2, IN1
 -	movaps IN1, STATE
 -
 -	pxor STATE4, STATE
 -	call _aesni_dec1
 -	pxor STATE4, STATE
 -
 -	movups STATE, (OUTP)
 -	jmp .Lxts_dec_ret
 -SYM_FUNC_END(aesni_xts_decrypt)
diff --cc arch/x86/crypto/aesni-intel_avx-x86_64.S
index b9da0daff62b,0852ab573fd3..000000000000
--- a/arch/x86/crypto/aesni-intel_avx-x86_64.S
+++ b/arch/x86/crypto/aesni-intel_avx-x86_64.S
@@@ -1571,39 -1761,102 +1571,136 @@@ ENDPROC(aesni_gcm_precomp_avx_gen2
  #			Initialisation Vector (from IPSec ESP Payload)
  #			concatenated with 0x00000001. 16-byte aligned pointer. */
  #        const   u8 *aad, /* Additional Authentication Data (AAD)*/
++<<<<<<< HEAD
 +#        u64     aad_len, /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
++=======
+ #        u64     aad_len) /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
+ #############################################################
+ SYM_FUNC_START(aesni_gcm_init_avx_gen2)
+         FUNC_SAVE
+         INIT GHASH_MUL_AVX, PRECOMPUTE_AVX
+         FUNC_RESTORE
+         RET
+ SYM_FUNC_END(aesni_gcm_init_avx_gen2)
+ 
+ ###############################################################################
+ #void   aesni_gcm_enc_update_avx_gen2(
+ #        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
+ #        gcm_context_data *data,
+ #        u8      *out, /* Ciphertext output. Encrypt in-place is allowed.  */
+ #        const   u8 *in, /* Plaintext input */
+ #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
+ ###############################################################################
+ SYM_FUNC_START(aesni_gcm_enc_update_avx_gen2)
+         FUNC_SAVE
+         mov     keysize, %eax
+         cmp     $32, %eax
+         je      key_256_enc_update
+         cmp     $16, %eax
+         je      key_128_enc_update
+         # must be 192
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 11
+         FUNC_RESTORE
+         RET
+ key_128_enc_update:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 9
+         FUNC_RESTORE
+         RET
+ key_256_enc_update:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 13
+         FUNC_RESTORE
+         RET
+ SYM_FUNC_END(aesni_gcm_enc_update_avx_gen2)
+ 
+ ###############################################################################
+ #void   aesni_gcm_dec_update_avx_gen2(
+ #        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
+ #        gcm_context_data *data,
+ #        u8      *out, /* Plaintext output. Decrypt in-place is allowed.  */
+ #        const   u8 *in, /* Ciphertext input */
+ #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
+ ###############################################################################
+ SYM_FUNC_START(aesni_gcm_dec_update_avx_gen2)
+         FUNC_SAVE
+         mov     keysize,%eax
+         cmp     $32, %eax
+         je      key_256_dec_update
+         cmp     $16, %eax
+         je      key_128_dec_update
+         # must be 192
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 11
+         FUNC_RESTORE
+         RET
+ key_128_dec_update:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 9
+         FUNC_RESTORE
+         RET
+ key_256_dec_update:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 13
+         FUNC_RESTORE
+         RET
+ SYM_FUNC_END(aesni_gcm_dec_update_avx_gen2)
+ 
+ ###############################################################################
+ #void   aesni_gcm_finalize_avx_gen2(
+ #        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
+ #        gcm_context_data *data,
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #        u8      *auth_tag, /* Authenticated Tag output. */
  #        u64     auth_tag_len)# /* Authenticated Tag Length in bytes.
  #				Valid values are 16 (most likely), 12 or 8. */
  ###############################################################################
++<<<<<<< HEAD
 +ENTRY(aesni_gcm_enc_avx_gen2)
 +        GCM_ENC_DEC_AVX     ENC
 +	ret
 +ENDPROC(aesni_gcm_enc_avx_gen2)
++=======
+ SYM_FUNC_START(aesni_gcm_finalize_avx_gen2)
+         FUNC_SAVE
+         mov	keysize,%eax
+         cmp     $32, %eax
+         je      key_256_finalize
+         cmp     $16, %eax
+         je      key_128_finalize
+         # must be 192
+         GCM_COMPLETE GHASH_MUL_AVX, 11, arg3, arg4
+         FUNC_RESTORE
+         RET
+ key_128_finalize:
+         GCM_COMPLETE GHASH_MUL_AVX, 9, arg3, arg4
+         FUNC_RESTORE
+         RET
+ key_256_finalize:
+         GCM_COMPLETE GHASH_MUL_AVX, 13, arg3, arg4
+         FUNC_RESTORE
+         RET
+ SYM_FUNC_END(aesni_gcm_finalize_avx_gen2)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +###############################################################################
 +#void   aesni_gcm_dec_avx_gen2(
 +#        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
 +#        u8      *out, /* Plaintext output. Decrypt in-place is allowed.  */
 +#        const   u8 *in, /* Ciphertext input */
 +#        u64     plaintext_len, /* Length of data in Bytes for encryption. */
 +#        u8      *iv, /* Pre-counter block j0: 4 byte salt
 +#			(from Security Association) concatenated with 8 byte
 +#			Initialisation Vector (from IPSec ESP Payload)
 +#			concatenated with 0x00000001. 16-byte aligned pointer. */
 +#        const   u8 *aad, /* Additional Authentication Data (AAD)*/
 +#        u64     aad_len, /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
 +#        u8      *auth_tag, /* Authenticated Tag output. */
 +#        u64     auth_tag_len)# /* Authenticated Tag Length in bytes.
 +#				Valid values are 16 (most likely), 12 or 8. */
 +###############################################################################
 +ENTRY(aesni_gcm_dec_avx_gen2)
 +        GCM_ENC_DEC_AVX     DEC
 +	ret
 +ENDPROC(aesni_gcm_dec_avx_gen2)
 +#endif /* CONFIG_AS_AVX */
  
 +#ifdef CONFIG_AS_AVX2
  ###############################################################################
  # GHASH_MUL MACRO to implement: Data*HashKey mod (128,127,126,121,0)
  # Input: A and B (128-bits each, bit-reflected)
@@@ -2355,576 -2554,273 +2452,667 @@@ _initial_blocks_done\@
          vpclmulqdq      $0x11, \T5, \XMM1, \T6
          vpclmulqdq      $0x00, \T5, \XMM1, \T7
  
 -        vpclmulqdq      $0x00, \T3, \T2, \XMM1
 +        vpclmulqdq      $0x00, \T3, \T2, \XMM1
 +
 +        ######################
 +
 +        vmovdqa         HashKey_7(arg1), \T5
 +        vpshufd         $0b01001110, \XMM2, \T2
 +        vpshufd         $0b01001110, \T5, \T3
 +        vpxor           \XMM2, \T2, \T2
 +        vpxor           \T5, \T3, \T3
 +
 +        vpclmulqdq      $0x11, \T5, \XMM2, \T4
 +        vpxor           \T4, \T6, \T6
 +
 +        vpclmulqdq      $0x00, \T5, \XMM2, \T4
 +        vpxor           \T4, \T7, \T7
 +
 +        vpclmulqdq      $0x00, \T3, \T2, \T2
 +
 +        vpxor           \T2, \XMM1, \XMM1
 +
 +        ######################
 +
 +        vmovdqa         HashKey_6(arg1), \T5
 +        vpshufd         $0b01001110, \XMM3, \T2
 +        vpshufd         $0b01001110, \T5, \T3
 +        vpxor           \XMM3, \T2, \T2
 +        vpxor           \T5, \T3, \T3
 +
 +        vpclmulqdq      $0x11, \T5, \XMM3, \T4
 +        vpxor           \T4, \T6, \T6
 +
 +        vpclmulqdq      $0x00, \T5, \XMM3, \T4
 +        vpxor           \T4, \T7, \T7
 +
 +        vpclmulqdq      $0x00, \T3, \T2, \T2
 +
 +        vpxor           \T2, \XMM1, \XMM1
 +
 +        ######################
 +
 +        vmovdqa         HashKey_5(arg1), \T5
 +        vpshufd         $0b01001110, \XMM4, \T2
 +        vpshufd         $0b01001110, \T5, \T3
 +        vpxor           \XMM4, \T2, \T2
 +        vpxor           \T5, \T3, \T3
 +
 +        vpclmulqdq      $0x11, \T5, \XMM4, \T4
 +        vpxor           \T4, \T6, \T6
 +
 +        vpclmulqdq      $0x00, \T5, \XMM4, \T4
 +        vpxor           \T4, \T7, \T7
 +
 +        vpclmulqdq      $0x00, \T3, \T2, \T2
 +
 +        vpxor           \T2, \XMM1, \XMM1
 +
 +        ######################
 +
 +        vmovdqa         HashKey_4(arg1), \T5
 +        vpshufd         $0b01001110, \XMM5, \T2
 +        vpshufd         $0b01001110, \T5, \T3
 +        vpxor           \XMM5, \T2, \T2
 +        vpxor           \T5, \T3, \T3
 +
 +        vpclmulqdq      $0x11, \T5, \XMM5, \T4
 +        vpxor           \T4, \T6, \T6
 +
 +        vpclmulqdq      $0x00, \T5, \XMM5, \T4
 +        vpxor           \T4, \T7, \T7
 +
 +        vpclmulqdq      $0x00, \T3, \T2, \T2
 +
 +        vpxor           \T2, \XMM1, \XMM1
 +
 +        ######################
 +
 +        vmovdqa         HashKey_3(arg1), \T5
 +        vpshufd         $0b01001110, \XMM6, \T2
 +        vpshufd         $0b01001110, \T5, \T3
 +        vpxor           \XMM6, \T2, \T2
 +        vpxor           \T5, \T3, \T3
 +
 +        vpclmulqdq      $0x11, \T5, \XMM6, \T4
 +        vpxor           \T4, \T6, \T6
 +
 +        vpclmulqdq      $0x00, \T5, \XMM6, \T4
 +        vpxor           \T4, \T7, \T7
 +
 +        vpclmulqdq      $0x00, \T3, \T2, \T2
 +
 +        vpxor           \T2, \XMM1, \XMM1
 +
 +        ######################
 +
 +        vmovdqa         HashKey_2(arg1), \T5
 +        vpshufd         $0b01001110, \XMM7, \T2
 +        vpshufd         $0b01001110, \T5, \T3
 +        vpxor           \XMM7, \T2, \T2
 +        vpxor           \T5, \T3, \T3
 +
 +        vpclmulqdq      $0x11, \T5, \XMM7, \T4
 +        vpxor           \T4, \T6, \T6
 +
 +        vpclmulqdq      $0x00, \T5, \XMM7, \T4
 +        vpxor           \T4, \T7, \T7
 +
 +        vpclmulqdq      $0x00, \T3, \T2, \T2
 +
 +        vpxor           \T2, \XMM1, \XMM1
 +
 +        ######################
 +
 +        vmovdqa         HashKey(arg1), \T5
 +        vpshufd         $0b01001110, \XMM8, \T2
 +        vpshufd         $0b01001110, \T5, \T3
 +        vpxor           \XMM8, \T2, \T2
 +        vpxor           \T5, \T3, \T3
 +
 +        vpclmulqdq      $0x11, \T5, \XMM8, \T4
 +        vpxor           \T4, \T6, \T6
 +
 +        vpclmulqdq      $0x00, \T5, \XMM8, \T4
 +        vpxor           \T4, \T7, \T7
 +
 +        vpclmulqdq      $0x00, \T3, \T2, \T2
 +
 +        vpxor           \T2, \XMM1, \XMM1
 +        vpxor           \T6, \XMM1, \XMM1
 +        vpxor           \T7, \XMM1, \T2
 +
 +
 +
 +
 +        vpslldq $8, \T2, \T4
 +        vpsrldq $8, \T2, \T2
 +
 +        vpxor   \T4, \T7, \T7
 +        vpxor   \T2, \T6, \T6                      # <T6:T7> holds the result of the
 +						   # accumulated carry-less multiplications
 +
 +        #######################################################################
 +        #first phase of the reduction
 +        vmovdqa         POLY2(%rip), \T3
 +
 +        vpclmulqdq      $0x01, \T7, \T3, \T2
 +        vpslldq         $8, \T2, \T2               # shift-L xmm2 2 DWs
 +
 +        vpxor           \T2, \T7, \T7              # first phase of the reduction complete
 +        #######################################################################
 +
 +
 +        #second phase of the reduction
 +        vpclmulqdq      $0x00, \T7, \T3, \T2
 +        vpsrldq         $4, \T2, \T2               # shift-R T2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)
 +
 +        vpclmulqdq      $0x10, \T7, \T3, \T4
 +        vpslldq         $4, \T4, \T4               # shift-L T4 1 DW (Shift-L 1-DW to obtain result with no shifts)
 +
 +        vpxor           \T2, \T4, \T4              # second phase of the reduction complete
 +        #######################################################################
 +        vpxor           \T4, \T6, \T6              # the result is in T6
 +.endm
 +
 +
 +
 +# combined for GCM encrypt and decrypt functions
 +# clobbering all xmm registers
 +# clobbering r10, r11, r12, r13, r14, r15
 +.macro  GCM_ENC_DEC_AVX2     ENC_DEC
 +
 +        #the number of pushes must equal STACK_OFFSET
 +        push    %r12
 +        push    %r13
 +        push    %r14
 +        push    %r15
 +
 +        mov     %rsp, %r14
 +
 +
 +
 +
 +        sub     $VARIABLE_OFFSET, %rsp
 +        and     $~63, %rsp                         # align rsp to 64 bytes
 +
 +
 +        vmovdqu  HashKey(arg1), %xmm13             # xmm13 = HashKey
 +
 +        mov     arg4, %r13                         # save the number of bytes of plaintext/ciphertext
 +        and     $-16, %r13                         # r13 = r13 - (r13 mod 16)
 +
 +        mov     %r13, %r12
 +        shr     $4, %r12
 +        and     $7, %r12
 +        jz      _initial_num_blocks_is_0\@
 +
 +        cmp     $7, %r12
 +        je      _initial_num_blocks_is_7\@
 +        cmp     $6, %r12
 +        je      _initial_num_blocks_is_6\@
 +        cmp     $5, %r12
 +        je      _initial_num_blocks_is_5\@
 +        cmp     $4, %r12
 +        je      _initial_num_blocks_is_4\@
 +        cmp     $3, %r12
 +        je      _initial_num_blocks_is_3\@
 +        cmp     $2, %r12
 +        je      _initial_num_blocks_is_2\@
 +
 +        jmp     _initial_num_blocks_is_1\@
 +
 +_initial_num_blocks_is_7\@:
 +        INITIAL_BLOCKS_AVX2  7, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +        sub     $16*7, %r13
 +        jmp     _initial_blocks_encrypted\@
 +
 +_initial_num_blocks_is_6\@:
 +        INITIAL_BLOCKS_AVX2  6, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +        sub     $16*6, %r13
 +        jmp     _initial_blocks_encrypted\@
 +
 +_initial_num_blocks_is_5\@:
 +        INITIAL_BLOCKS_AVX2  5, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +        sub     $16*5, %r13
 +        jmp     _initial_blocks_encrypted\@
 +
 +_initial_num_blocks_is_4\@:
 +        INITIAL_BLOCKS_AVX2  4, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +        sub     $16*4, %r13
 +        jmp     _initial_blocks_encrypted\@
 +
 +_initial_num_blocks_is_3\@:
 +        INITIAL_BLOCKS_AVX2  3, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +        sub     $16*3, %r13
 +        jmp     _initial_blocks_encrypted\@
 +
 +_initial_num_blocks_is_2\@:
 +        INITIAL_BLOCKS_AVX2  2, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +        sub     $16*2, %r13
 +        jmp     _initial_blocks_encrypted\@
 +
 +_initial_num_blocks_is_1\@:
 +        INITIAL_BLOCKS_AVX2  1, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +        sub     $16*1, %r13
 +        jmp     _initial_blocks_encrypted\@
 +
 +_initial_num_blocks_is_0\@:
 +        INITIAL_BLOCKS_AVX2  0, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \ENC_DEC
 +
 +
 +_initial_blocks_encrypted\@:
 +        cmp     $0, %r13
 +        je      _zero_cipher_left\@
 +
 +        sub     $128, %r13
 +        je      _eight_cipher_left\@
 +
 +
 +
 +
 +        vmovd   %xmm9, %r15d
 +        and     $255, %r15d
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
 +
 +
 +_encrypt_by_8_new\@:
 +        cmp     $(255-8), %r15d
 +        jg      _encrypt_by_8\@
 +
 +
 +
 +        add     $8, %r15b
 +        GHASH_8_ENCRYPT_8_PARALLEL_AVX2      %xmm0, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm15, out_order, \ENC_DEC
 +        add     $128, %r11
 +        sub     $128, %r13
 +        jne     _encrypt_by_8_new\@
 +
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
 +        jmp     _eight_cipher_left\@
  
 -        ######################
 +_encrypt_by_8\@:
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
 +        add     $8, %r15b
 +        GHASH_8_ENCRYPT_8_PARALLEL_AVX2      %xmm0, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm15, in_order, \ENC_DEC
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
 +        add     $128, %r11
 +        sub     $128, %r13
 +        jne     _encrypt_by_8_new\@
  
 -        vmovdqu         HashKey_7(arg2), \T5
 -        vpshufd         $0b01001110, \XMM2, \T2
 -        vpshufd         $0b01001110, \T5, \T3
 -        vpxor           \XMM2, \T2, \T2
 -        vpxor           \T5, \T3, \T3
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
  
 -        vpclmulqdq      $0x11, \T5, \XMM2, \T4
 -        vpxor           \T4, \T6, \T6
  
 -        vpclmulqdq      $0x00, \T5, \XMM2, \T4
 -        vpxor           \T4, \T7, \T7
  
 -        vpclmulqdq      $0x00, \T3, \T2, \T2
  
 -        vpxor           \T2, \XMM1, \XMM1
 +_eight_cipher_left\@:
 +        GHASH_LAST_8_AVX2    %xmm0, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm15, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8
  
 -        ######################
  
 -        vmovdqu         HashKey_6(arg2), \T5
 -        vpshufd         $0b01001110, \XMM3, \T2
 -        vpshufd         $0b01001110, \T5, \T3
 -        vpxor           \XMM3, \T2, \T2
 -        vpxor           \T5, \T3, \T3
 +_zero_cipher_left\@:
 +        cmp     $16, arg4
 +        jl      _only_less_than_16\@
  
 -        vpclmulqdq      $0x11, \T5, \XMM3, \T4
 -        vpxor           \T4, \T6, \T6
 +        mov     arg4, %r13
 +        and     $15, %r13                            # r13 = (arg4 mod 16)
  
 -        vpclmulqdq      $0x00, \T5, \XMM3, \T4
 -        vpxor           \T4, \T7, \T7
 +        je      _multiple_of_16_bytes\@
  
 -        vpclmulqdq      $0x00, \T3, \T2, \T2
 +        # handle the last <16 Byte block seperately
  
 -        vpxor           \T2, \XMM1, \XMM1
  
 -        ######################
 +        vpaddd   ONE(%rip), %xmm9, %xmm9             # INCR CNT to get Yn
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
 +        ENCRYPT_SINGLE_BLOCK    %xmm9                # E(K, Yn)
  
 -        vmovdqu         HashKey_5(arg2), \T5
 -        vpshufd         $0b01001110, \XMM4, \T2
 -        vpshufd         $0b01001110, \T5, \T3
 -        vpxor           \XMM4, \T2, \T2
 -        vpxor           \T5, \T3, \T3
 +        sub     $16, %r11
 +        add     %r13, %r11
 +        vmovdqu (arg3, %r11), %xmm1                  # receive the last <16 Byte block
  
 -        vpclmulqdq      $0x11, \T5, \XMM4, \T4
 -        vpxor           \T4, \T6, \T6
 +        lea     SHIFT_MASK+16(%rip), %r12
 +        sub     %r13, %r12                           # adjust the shuffle mask pointer
 +						     # to be able to shift 16-r13 bytes
 +						     # (r13 is the number of bytes in plaintext mod 16)
 +        vmovdqu (%r12), %xmm2                        # get the appropriate shuffle mask
 +        vpshufb %xmm2, %xmm1, %xmm1                  # shift right 16-r13 bytes
 +        jmp     _final_ghash_mul\@
 +
 +_only_less_than_16\@:
 +        # check for 0 length
 +        mov     arg4, %r13
 +        and     $15, %r13                            # r13 = (arg4 mod 16)
  
 -        vpclmulqdq      $0x00, \T5, \XMM4, \T4
 -        vpxor           \T4, \T7, \T7
 +        je      _multiple_of_16_bytes\@
  
 -        vpclmulqdq      $0x00, \T3, \T2, \T2
 +        # handle the last <16 Byte block seperately
  
 -        vpxor           \T2, \XMM1, \XMM1
  
 -        ######################
 +        vpaddd  ONE(%rip), %xmm9, %xmm9              # INCR CNT to get Yn
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
 +        ENCRYPT_SINGLE_BLOCK    %xmm9                # E(K, Yn)
  
 -        vmovdqu         HashKey_4(arg2), \T5
 -        vpshufd         $0b01001110, \XMM5, \T2
 -        vpshufd         $0b01001110, \T5, \T3
 -        vpxor           \XMM5, \T2, \T2
 -        vpxor           \T5, \T3, \T3
  
 -        vpclmulqdq      $0x11, \T5, \XMM5, \T4
 -        vpxor           \T4, \T6, \T6
 +        lea     SHIFT_MASK+16(%rip), %r12
 +        sub     %r13, %r12                           # adjust the shuffle mask pointer to be
 +						     # able to shift 16-r13 bytes (r13 is the
 +						     # number of bytes in plaintext mod 16)
  
 -        vpclmulqdq      $0x00, \T5, \XMM5, \T4
 -        vpxor           \T4, \T7, \T7
 +_get_last_16_byte_loop\@:
 +        movb    (arg3, %r11),  %al
 +        movb    %al,  TMP1 (%rsp , %r11)
 +        add     $1, %r11
 +        cmp     %r13,  %r11
 +        jne     _get_last_16_byte_loop\@
  
 -        vpclmulqdq      $0x00, \T3, \T2, \T2
 +        vmovdqu  TMP1(%rsp), %xmm1
  
 -        vpxor           \T2, \XMM1, \XMM1
 +        sub     $16, %r11
  
 -        ######################
 +_final_ghash_mul\@:
 +        .if  \ENC_DEC ==  DEC
 +        vmovdqa %xmm1, %xmm2
 +        vpxor   %xmm1, %xmm9, %xmm9                  # Plaintext XOR E(K, Yn)
 +        vmovdqu ALL_F-SHIFT_MASK(%r12), %xmm1        # get the appropriate mask to mask out top 16-r13 bytes of xmm9
 +        vpand   %xmm1, %xmm9, %xmm9                  # mask out top 16-r13 bytes of xmm9
 +        vpand   %xmm1, %xmm2, %xmm2
 +        vpshufb SHUF_MASK(%rip), %xmm2, %xmm2
 +        vpxor   %xmm2, %xmm14, %xmm14
 +	#GHASH computation for the last <16 Byte block
 +        GHASH_MUL_AVX2       %xmm14, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6
 +        sub     %r13, %r11
 +        add     $16, %r11
 +        .else
 +        vpxor   %xmm1, %xmm9, %xmm9                  # Plaintext XOR E(K, Yn)
 +        vmovdqu ALL_F-SHIFT_MASK(%r12), %xmm1        # get the appropriate mask to mask out top 16-r13 bytes of xmm9
 +        vpand   %xmm1, %xmm9, %xmm9                  # mask out top 16-r13 bytes of xmm9
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9
 +        vpxor   %xmm9, %xmm14, %xmm14
 +	#GHASH computation for the last <16 Byte block
 +        GHASH_MUL_AVX2       %xmm14, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6
 +        sub     %r13, %r11
 +        add     $16, %r11
 +        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9        # shuffle xmm9 back to output as ciphertext
 +        .endif
  
 -        vmovdqu         HashKey_3(arg2), \T5
 -        vpshufd         $0b01001110, \XMM6, \T2
 -        vpshufd         $0b01001110, \T5, \T3
 -        vpxor           \XMM6, \T2, \T2
 -        vpxor           \T5, \T3, \T3
  
 -        vpclmulqdq      $0x11, \T5, \XMM6, \T4
 -        vpxor           \T4, \T6, \T6
 +        #############################
 +        # output r13 Bytes
 +        vmovq   %xmm9, %rax
 +        cmp     $8, %r13
 +        jle     _less_than_8_bytes_left\@
  
 -        vpclmulqdq      $0x00, \T5, \XMM6, \T4
 -        vpxor           \T4, \T7, \T7
 +        mov     %rax, (arg2 , %r11)
 +        add     $8, %r11
 +        vpsrldq $8, %xmm9, %xmm9
 +        vmovq   %xmm9, %rax
 +        sub     $8, %r13
  
 -        vpclmulqdq      $0x00, \T3, \T2, \T2
 +_less_than_8_bytes_left\@:
 +        movb    %al, (arg2 , %r11)
 +        add     $1, %r11
 +        shr     $8, %rax
 +        sub     $1, %r13
 +        jne     _less_than_8_bytes_left\@
 +        #############################
  
 -        vpxor           \T2, \XMM1, \XMM1
 +_multiple_of_16_bytes\@:
 +        mov     arg7, %r12                           # r12 = aadLen (number of bytes)
 +        shl     $3, %r12                             # convert into number of bits
 +        vmovd   %r12d, %xmm15                        # len(A) in xmm15
  
 -        ######################
 +        shl     $3, arg4                             # len(C) in bits  (*128)
 +        vmovq   arg4, %xmm1
 +        vpslldq $8, %xmm15, %xmm15                   # xmm15 = len(A)|| 0x0000000000000000
 +        vpxor   %xmm1, %xmm15, %xmm15                # xmm15 = len(A)||len(C)
  
 -        vmovdqu         HashKey_2(arg2), \T5
 -        vpshufd         $0b01001110, \XMM7, \T2
 -        vpshufd         $0b01001110, \T5, \T3
 -        vpxor           \XMM7, \T2, \T2
 -        vpxor           \T5, \T3, \T3
 +        vpxor   %xmm15, %xmm14, %xmm14
 +        GHASH_MUL_AVX2       %xmm14, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6    # final GHASH computation
 +        vpshufb SHUF_MASK(%rip), %xmm14, %xmm14              # perform a 16Byte swap
  
 -        vpclmulqdq      $0x11, \T5, \XMM7, \T4
 -        vpxor           \T4, \T6, \T6
 +        mov     arg5, %rax                           # rax = *Y0
 +        vmovdqu (%rax), %xmm9                        # xmm9 = Y0
  
 -        vpclmulqdq      $0x00, \T5, \XMM7, \T4
 -        vpxor           \T4, \T7, \T7
 +        ENCRYPT_SINGLE_BLOCK    %xmm9                # E(K, Y0)
  
 -        vpclmulqdq      $0x00, \T3, \T2, \T2
 +        vpxor   %xmm14, %xmm9, %xmm9
  
 -        vpxor           \T2, \XMM1, \XMM1
  
 -        ######################
  
 -        vmovdqu         HashKey(arg2), \T5
 -        vpshufd         $0b01001110, \XMM8, \T2
 -        vpshufd         $0b01001110, \T5, \T3
 -        vpxor           \XMM8, \T2, \T2
 -        vpxor           \T5, \T3, \T3
 +_return_T\@:
 +        mov     arg8, %r10              # r10 = authTag
 +        mov     arg9, %r11              # r11 = auth_tag_len
  
 -        vpclmulqdq      $0x11, \T5, \XMM8, \T4
 -        vpxor           \T4, \T6, \T6
 +        cmp     $16, %r11
 +        je      _T_16\@
  
 -        vpclmulqdq      $0x00, \T5, \XMM8, \T4
 -        vpxor           \T4, \T7, \T7
 +        cmp     $8, %r11
 +        jl      _T_4\@
  
 -        vpclmulqdq      $0x00, \T3, \T2, \T2
 +_T_8\@:
 +        vmovq   %xmm9, %rax
 +        mov     %rax, (%r10)
 +        add     $8, %r10
 +        sub     $8, %r11
 +        vpsrldq $8, %xmm9, %xmm9
 +        cmp     $0, %r11
 +        je     _return_T_done\@
 +_T_4\@:
 +        vmovd   %xmm9, %eax
 +        mov     %eax, (%r10)
 +        add     $4, %r10
 +        sub     $4, %r11
 +        vpsrldq     $4, %xmm9, %xmm9
 +        cmp     $0, %r11
 +        je     _return_T_done\@
 +_T_123\@:
 +        vmovd     %xmm9, %eax
 +        cmp     $2, %r11
 +        jl     _T_1\@
 +        mov     %ax, (%r10)
 +        cmp     $2, %r11
 +        je     _return_T_done\@
 +        add     $2, %r10
 +        sar     $16, %eax
 +_T_1\@:
 +        mov     %al, (%r10)
 +        jmp     _return_T_done\@
  
 -        vpxor           \T2, \XMM1, \XMM1
 -        vpxor           \T6, \XMM1, \XMM1
 -        vpxor           \T7, \XMM1, \T2
 +_T_16\@:
 +        vmovdqu %xmm9, (%r10)
  
 +_return_T_done\@:
 +        mov     %r14, %rsp
  
 +        pop     %r15
 +        pop     %r14
 +        pop     %r13
 +        pop     %r12
 +.endm
  
  
 -        vpslldq $8, \T2, \T4
 -        vpsrldq $8, \T2, \T2
 +#############################################################
 +#void   aesni_gcm_precomp_avx_gen4
 +#        (gcm_data     *my_ctx_data,
 +#        u8     *hash_subkey)# /* H, the Hash sub key input.
 +#				Data starts on a 16-byte boundary. */
 +#############################################################
++<<<<<<< HEAD
 +ENTRY(aesni_gcm_precomp_avx_gen4)
 +        #the number of pushes must equal STACK_OFFSET
 +        push    %r12
 +        push    %r13
 +        push    %r14
 +        push    %r15
  
 -        vpxor   \T4, \T7, \T7
 -        vpxor   \T2, \T6, \T6                      # <T6:T7> holds the result of the
 -						   # accumulated carry-less multiplications
 +        mov     %rsp, %r14
  
 -        #######################################################################
 -        #first phase of the reduction
 -        vmovdqa         POLY2(%rip), \T3
  
 -        vpclmulqdq      $0x01, \T7, \T3, \T2
 -        vpslldq         $8, \T2, \T2               # shift-L xmm2 2 DWs
  
 -        vpxor           \T2, \T7, \T7              # first phase of the reduction complete
 -        #######################################################################
 +        sub     $VARIABLE_OFFSET, %rsp
 +        and     $~63, %rsp                    # align rsp to 64 bytes
  
 +        vmovdqu  (arg2), %xmm6                # xmm6 = HashKey
  
 -        #second phase of the reduction
 -        vpclmulqdq      $0x00, \T7, \T3, \T2
 -        vpsrldq         $4, \T2, \T2               # shift-R T2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)
 +        vpshufb  SHUF_MASK(%rip), %xmm6, %xmm6
 +        ###############  PRECOMPUTATION of HashKey<<1 mod poly from the HashKey
 +        vmovdqa  %xmm6, %xmm2
 +        vpsllq   $1, %xmm6, %xmm6
 +        vpsrlq   $63, %xmm2, %xmm2
 +        vmovdqa  %xmm2, %xmm1
 +        vpslldq  $8, %xmm2, %xmm2
 +        vpsrldq  $8, %xmm1, %xmm1
 +        vpor     %xmm2, %xmm6, %xmm6
 +        #reduction
 +        vpshufd  $0b00100100, %xmm1, %xmm2
 +        vpcmpeqd TWOONE(%rip), %xmm2, %xmm2
 +        vpand    POLY(%rip), %xmm2, %xmm2
 +        vpxor    %xmm2, %xmm6, %xmm6          # xmm6 holds the HashKey<<1 mod poly
 +        #######################################################################
 +        vmovdqa  %xmm6, HashKey(arg1)         # store HashKey<<1 mod poly
  
 -        vpclmulqdq      $0x10, \T7, \T3, \T4
 -        vpslldq         $4, \T4, \T4               # shift-L T4 1 DW (Shift-L 1-DW to obtain result with no shifts)
  
 -        vpxor           \T2, \T4, \T4              # second phase of the reduction complete
 -        #######################################################################
 -        vpxor           \T4, \T6, \T6              # the result is in T6
 -.endm
 +        PRECOMPUTE_AVX2  %xmm6, %xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5
  
 +        mov     %r14, %rsp
  
 +        pop     %r15
 +        pop     %r14
 +        pop     %r13
 +        pop     %r12
 +        ret
 +ENDPROC(aesni_gcm_precomp_avx_gen4)
  
 -#############################################################
 -#void   aesni_gcm_init_avx_gen4
 -#        (gcm_data     *my_ctx_data,
 -#         gcm_context_data *data,
 -#        u8      *iv, /* Pre-counter block j0: 4 byte salt
 -#			(from Security Association) concatenated with 8 byte
 -#			Initialisation Vector (from IPSec ESP Payload)
 -#			concatenated with 0x00000001. 16-byte aligned pointer. */
 -#        u8     *hash_subkey# /* H, the Hash sub key input. Data starts on a 16-byte boundary. */
 -#        const   u8 *aad, /* Additional Authentication Data (AAD)*/
 -#        u64     aad_len) /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
 -#############################################################
++=======
+ SYM_FUNC_START(aesni_gcm_init_avx_gen4)
+         FUNC_SAVE
+         INIT GHASH_MUL_AVX2, PRECOMPUTE_AVX2
+         FUNC_RESTORE
+         RET
+ SYM_FUNC_END(aesni_gcm_init_avx_gen4)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  ###############################################################################
  #void   aesni_gcm_enc_avx_gen4(
  #        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
 -#        gcm_context_data *data,
  #        u8      *out, /* Ciphertext output. Encrypt in-place is allowed.  */
  #        const   u8 *in, /* Plaintext input */
++<<<<<<< HEAD
 +#        u64     plaintext_len, /* Length of data in Bytes for encryption. */
 +#        u8      *iv, /* Pre-counter block j0: 4 byte salt
 +#			(from Security Association) concatenated with 8 byte
 +#			 Initialisation Vector (from IPSec ESP Payload)
 +#			 concatenated with 0x00000001. 16-byte aligned pointer. */
 +#        const   u8 *aad, /* Additional Authentication Data (AAD)*/
 +#        u64     aad_len, /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
++=======
+ #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
+ ###############################################################################
+ SYM_FUNC_START(aesni_gcm_enc_update_avx_gen4)
+         FUNC_SAVE
+         mov     keysize,%eax
+         cmp     $32, %eax
+         je      key_256_enc_update4
+         cmp     $16, %eax
+         je      key_128_enc_update4
+         # must be 192
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 11
+         FUNC_RESTORE
+ 	RET
+ key_128_enc_update4:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 9
+         FUNC_RESTORE
+ 	RET
+ key_256_enc_update4:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 13
+         FUNC_RESTORE
+ 	RET
+ SYM_FUNC_END(aesni_gcm_enc_update_avx_gen4)
+ 
+ ###############################################################################
+ #void   aesni_gcm_dec_update_avx_gen4(
+ #        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
+ #        gcm_context_data *data,
+ #        u8      *out, /* Plaintext output. Decrypt in-place is allowed.  */
+ #        const   u8 *in, /* Ciphertext input */
+ #        u64     plaintext_len) /* Length of data in Bytes for encryption. */
+ ###############################################################################
+ SYM_FUNC_START(aesni_gcm_dec_update_avx_gen4)
+         FUNC_SAVE
+         mov     keysize,%eax
+         cmp     $32, %eax
+         je      key_256_dec_update4
+         cmp     $16, %eax
+         je      key_128_dec_update4
+         # must be 192
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 11
+         FUNC_RESTORE
+         RET
+ key_128_dec_update4:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 9
+         FUNC_RESTORE
+         RET
+ key_256_dec_update4:
+         GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 13
+         FUNC_RESTORE
+         RET
+ SYM_FUNC_END(aesni_gcm_dec_update_avx_gen4)
+ 
+ ###############################################################################
+ #void   aesni_gcm_finalize_avx_gen4(
+ #        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
+ #        gcm_context_data *data,
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #        u8      *auth_tag, /* Authenticated Tag output. */
  #        u64     auth_tag_len)# /* Authenticated Tag Length in bytes.
 -#                              Valid values are 16 (most likely), 12 or 8. */
 +#				Valid values are 16 (most likely), 12 or 8. */
 +###############################################################################
++<<<<<<< HEAD
 +ENTRY(aesni_gcm_enc_avx_gen4)
 +        GCM_ENC_DEC_AVX2     ENC
 +	ret
 +ENDPROC(aesni_gcm_enc_avx_gen4)
 +
 +###############################################################################
 +#void   aesni_gcm_dec_avx_gen4(
 +#        gcm_data        *my_ctx_data,     /* aligned to 16 Bytes */
 +#        u8      *out, /* Plaintext output. Decrypt in-place is allowed.  */
 +#        const   u8 *in, /* Ciphertext input */
 +#        u64     plaintext_len, /* Length of data in Bytes for encryption. */
 +#        u8      *iv, /* Pre-counter block j0: 4 byte salt
 +#			(from Security Association) concatenated with 8 byte
 +#			Initialisation Vector (from IPSec ESP Payload)
 +#			concatenated with 0x00000001. 16-byte aligned pointer. */
 +#        const   u8 *aad, /* Additional Authentication Data (AAD)*/
 +#        u64     aad_len, /* Length of AAD in bytes. With RFC4106 this is going to be 8 or 12 Bytes */
 +#        u8      *auth_tag, /* Authenticated Tag output. */
 +#        u64     auth_tag_len)# /* Authenticated Tag Length in bytes.
 +#				Valid values are 16 (most likely), 12 or 8. */
  ###############################################################################
 +ENTRY(aesni_gcm_dec_avx_gen4)
 +        GCM_ENC_DEC_AVX2     DEC
 +	ret
 +ENDPROC(aesni_gcm_dec_avx_gen4)
 +
 +#endif /* CONFIG_AS_AVX2 */
++=======
+ SYM_FUNC_START(aesni_gcm_finalize_avx_gen4)
+         FUNC_SAVE
+         mov	keysize,%eax
+         cmp     $32, %eax
+         je      key_256_finalize4
+         cmp     $16, %eax
+         je      key_128_finalize4
+         # must be 192
+         GCM_COMPLETE GHASH_MUL_AVX2, 11, arg3, arg4
+         FUNC_RESTORE
+         RET
+ key_128_finalize4:
+         GCM_COMPLETE GHASH_MUL_AVX2, 9, arg3, arg4
+         FUNC_RESTORE
+         RET
+ key_256_finalize4:
+         GCM_COMPLETE GHASH_MUL_AVX2, 13, arg3, arg4
+         FUNC_RESTORE
+         RET
+ SYM_FUNC_END(aesni_gcm_finalize_avx_gen4)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/blowfish-x86_64-asm_64.S
index 246c67006ed0,802d71582689..000000000000
--- a/arch/x86/crypto/blowfish-x86_64-asm_64.S
+++ b/arch/x86/crypto/blowfish-x86_64-asm_64.S
@@@ -149,15 -135,15 +149,20 @@@ ENTRY(__blowfish_enc_blk
  	jnz .L__enc_xor;
  
  	write_block();
- 	ret;
+ 	RET;
  .L__enc_xor:
  	xor_block();
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__blowfish_enc_blk)
++=======
+ 	RET;
+ SYM_FUNC_END(__blowfish_enc_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(blowfish_dec_blk)
 +ENTRY(blowfish_dec_blk)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 */
@@@ -181,10 -168,10 +186,15 @@@
  	movq %r10, RIO;
  	write_block();
  
 -	movq %r11, %r12;
 +	movq %r11, %rbp;
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(blowfish_dec_blk)
++=======
+ 	RET;
+ SYM_FUNC_END(blowfish_dec_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /**********************************************************************
    4-way blowfish, four blocks parallel
@@@ -333,20 -321,20 +343,31 @@@ ENTRY(__blowfish_enc_blk_4way
  	write_block4();
  
  	popq %rbx;
++<<<<<<< HEAD
 +	popq %rbp;
 +	ret;
++=======
+ 	popq %r12;
+ 	RET;
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .L__enc_xor4:
  	xor_block4();
  
  	popq %rbx;
++<<<<<<< HEAD
 +	popq %rbp;
 +	ret;
 +ENDPROC(__blowfish_enc_blk_4way)
++=======
+ 	popq %r12;
+ 	RET;
+ SYM_FUNC_END(__blowfish_enc_blk_4way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(blowfish_dec_blk_4way)
 +ENTRY(blowfish_dec_blk_4way)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 */
@@@ -373,7 -362,7 +394,12 @@@
  	write_block4();
  
  	popq %rbx;
 -	popq %r12;
 +	popq %rbp;
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(blowfish_dec_blk_4way)
++=======
+ 	RET;
+ SYM_FUNC_END(blowfish_dec_blk_4way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/camellia-aesni-avx-asm_64.S
index 77ff4de2224d,2e1658ddbe1a..000000000000
--- a/arch/x86/crypto/camellia-aesni-avx-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx-asm_64.S
@@@ -193,16 -192,16 +193,26 @@@ roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1
  	roundsm16(%xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7,
  		  %xmm8, %xmm9, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm15,
  		  %rcx, (%r9));
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
++=======
+ 	RET;
+ SYM_FUNC_END(roundsm16_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .align 8
 -SYM_FUNC_START_LOCAL(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 +roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab:
  	roundsm16(%xmm4, %xmm5, %xmm6, %xmm7, %xmm0, %xmm1, %xmm2, %xmm3,
  		  %xmm12, %xmm13, %xmm14, %xmm15, %xmm8, %xmm9, %xmm10, %xmm11,
  		  %rax, (%r9));
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
++=======
+ 	RET;
+ SYM_FUNC_END(roundsm16_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * IN/OUT:
@@@ -912,10 -906,10 +922,15 @@@ ENTRY(camellia_ecb_enc_16way
  		     %xmm8, %rsi);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(camellia_ecb_enc_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(camellia_ecb_enc_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(camellia_ecb_dec_16way)
 +ENTRY(camellia_ecb_dec_16way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst (16 blocks)
@@@ -942,10 -936,10 +957,15 @@@
  		     %xmm8, %rsi);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(camellia_ecb_dec_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(camellia_ecb_dec_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(camellia_cbc_dec_16way)
 +ENTRY(camellia_cbc_dec_16way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst (16 blocks)
@@@ -993,294 -987,5 +1013,299 @@@
  		     %xmm8, %rsi);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(camellia_cbc_dec_16way)
 +
 +#define inc_le128(x, minus_one, tmp) \
 +	vpcmpeqq minus_one, x, tmp; \
 +	vpsubq minus_one, x, x; \
 +	vpslldq $8, tmp, tmp; \
 +	vpsubq tmp, x, x;
 +
 +ENTRY(camellia_ctr_16way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (16 blocks)
 +	 *	%rdx: src (16 blocks)
 +	 *	%rcx: iv (little endian, 128bit)
 +	 */
 +	FRAME_BEGIN
 +
 +	subq $(16 * 16), %rsp;
 +	movq %rsp, %rax;
 +
 +	vmovdqa .Lbswap128_mask, %xmm14;
 +
 +	/* load IV and byteswap */
 +	vmovdqu (%rcx), %xmm0;
 +	vpshufb %xmm14, %xmm0, %xmm15;
 +	vmovdqu %xmm15, 15 * 16(%rax);
 +
 +	vpcmpeqd %xmm15, %xmm15, %xmm15;
 +	vpsrldq $8, %xmm15, %xmm15; /* low: -1, high: 0 */
 +
 +	/* construct IVs */
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm13;
 +	vmovdqu %xmm13, 14 * 16(%rax);
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm13;
 +	vmovdqu %xmm13, 13 * 16(%rax);
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm12;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm11;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm10;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm9;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm8;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm7;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm6;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm5;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm4;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm3;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm2;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vpshufb %xmm14, %xmm0, %xmm1;
 +	inc_le128(%xmm0, %xmm15, %xmm13);
 +	vmovdqa %xmm0, %xmm13;
 +	vpshufb %xmm14, %xmm0, %xmm0;
 +	inc_le128(%xmm13, %xmm15, %xmm14);
 +	vmovdqu %xmm13, (%rcx);
 +
 +	/* inpack16_pre: */
 +	vmovq (key_table)(CTX), %xmm15;
 +	vpshufb .Lpack_bswap, %xmm15, %xmm15;
 +	vpxor %xmm0, %xmm15, %xmm0;
 +	vpxor %xmm1, %xmm15, %xmm1;
 +	vpxor %xmm2, %xmm15, %xmm2;
 +	vpxor %xmm3, %xmm15, %xmm3;
 +	vpxor %xmm4, %xmm15, %xmm4;
 +	vpxor %xmm5, %xmm15, %xmm5;
 +	vpxor %xmm6, %xmm15, %xmm6;
 +	vpxor %xmm7, %xmm15, %xmm7;
 +	vpxor %xmm8, %xmm15, %xmm8;
 +	vpxor %xmm9, %xmm15, %xmm9;
 +	vpxor %xmm10, %xmm15, %xmm10;
 +	vpxor %xmm11, %xmm15, %xmm11;
 +	vpxor %xmm12, %xmm15, %xmm12;
 +	vpxor 13 * 16(%rax), %xmm15, %xmm13;
 +	vpxor 14 * 16(%rax), %xmm15, %xmm14;
 +	vpxor 15 * 16(%rax), %xmm15, %xmm15;
 +
 +	call __camellia_enc_blk16;
 +
 +	addq $(16 * 16), %rsp;
 +
 +	vpxor 0 * 16(%rdx), %xmm7, %xmm7;
 +	vpxor 1 * 16(%rdx), %xmm6, %xmm6;
 +	vpxor 2 * 16(%rdx), %xmm5, %xmm5;
 +	vpxor 3 * 16(%rdx), %xmm4, %xmm4;
 +	vpxor 4 * 16(%rdx), %xmm3, %xmm3;
 +	vpxor 5 * 16(%rdx), %xmm2, %xmm2;
 +	vpxor 6 * 16(%rdx), %xmm1, %xmm1;
 +	vpxor 7 * 16(%rdx), %xmm0, %xmm0;
 +	vpxor 8 * 16(%rdx), %xmm15, %xmm15;
 +	vpxor 9 * 16(%rdx), %xmm14, %xmm14;
 +	vpxor 10 * 16(%rdx), %xmm13, %xmm13;
 +	vpxor 11 * 16(%rdx), %xmm12, %xmm12;
 +	vpxor 12 * 16(%rdx), %xmm11, %xmm11;
 +	vpxor 13 * 16(%rdx), %xmm10, %xmm10;
 +	vpxor 14 * 16(%rdx), %xmm9, %xmm9;
 +	vpxor 15 * 16(%rdx), %xmm8, %xmm8;
 +	write_output(%xmm7, %xmm6, %xmm5, %xmm4, %xmm3, %xmm2, %xmm1, %xmm0,
 +		     %xmm15, %xmm14, %xmm13, %xmm12, %xmm11, %xmm10, %xmm9,
 +		     %xmm8, %rsi);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(camellia_ctr_16way)
 +
 +#define gf128mul_x_ble(iv, mask, tmp) \
 +	vpsrad $31, iv, tmp; \
 +	vpaddq iv, iv, iv; \
 +	vpshufd $0x13, tmp, tmp; \
 +	vpand mask, tmp, tmp; \
 +	vpxor tmp, iv, iv;
 +
 +.align 8
 +camellia_xts_crypt_16way:
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (16 blocks)
 +	 *	%rdx: src (16 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 *	%r8: index for input whitening key
 +	 *	%r9: pointer to  __camellia_enc_blk16 or __camellia_dec_blk16
 +	 */
 +	FRAME_BEGIN
 +
 +	subq $(16 * 16), %rsp;
 +	movq %rsp, %rax;
 +
 +	vmovdqa .Lxts_gf128mul_and_shl1_mask, %xmm14;
 +
 +	/* load IV */
 +	vmovdqu (%rcx), %xmm0;
 +	vpxor 0 * 16(%rdx), %xmm0, %xmm15;
 +	vmovdqu %xmm15, 15 * 16(%rax);
 +	vmovdqu %xmm0, 0 * 16(%rsi);
 +
 +	/* construct IVs */
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 1 * 16(%rdx), %xmm0, %xmm15;
 +	vmovdqu %xmm15, 14 * 16(%rax);
 +	vmovdqu %xmm0, 1 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 2 * 16(%rdx), %xmm0, %xmm13;
 +	vmovdqu %xmm0, 2 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 3 * 16(%rdx), %xmm0, %xmm12;
 +	vmovdqu %xmm0, 3 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 4 * 16(%rdx), %xmm0, %xmm11;
 +	vmovdqu %xmm0, 4 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 5 * 16(%rdx), %xmm0, %xmm10;
 +	vmovdqu %xmm0, 5 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 6 * 16(%rdx), %xmm0, %xmm9;
 +	vmovdqu %xmm0, 6 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 7 * 16(%rdx), %xmm0, %xmm8;
 +	vmovdqu %xmm0, 7 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 8 * 16(%rdx), %xmm0, %xmm7;
 +	vmovdqu %xmm0, 8 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 9 * 16(%rdx), %xmm0, %xmm6;
 +	vmovdqu %xmm0, 9 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 10 * 16(%rdx), %xmm0, %xmm5;
 +	vmovdqu %xmm0, 10 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 11 * 16(%rdx), %xmm0, %xmm4;
 +	vmovdqu %xmm0, 11 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 12 * 16(%rdx), %xmm0, %xmm3;
 +	vmovdqu %xmm0, 12 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 13 * 16(%rdx), %xmm0, %xmm2;
 +	vmovdqu %xmm0, 13 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 14 * 16(%rdx), %xmm0, %xmm1;
 +	vmovdqu %xmm0, 14 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vpxor 15 * 16(%rdx), %xmm0, %xmm15;
 +	vmovdqu %xmm15, 0 * 16(%rax);
 +	vmovdqu %xmm0, 15 * 16(%rsi);
 +
 +	gf128mul_x_ble(%xmm0, %xmm14, %xmm15);
 +	vmovdqu %xmm0, (%rcx);
 +
 +	/* inpack16_pre: */
 +	vmovq (key_table)(CTX, %r8, 8), %xmm15;
 +	vpshufb .Lpack_bswap, %xmm15, %xmm15;
 +	vpxor 0 * 16(%rax), %xmm15, %xmm0;
 +	vpxor %xmm1, %xmm15, %xmm1;
 +	vpxor %xmm2, %xmm15, %xmm2;
 +	vpxor %xmm3, %xmm15, %xmm3;
 +	vpxor %xmm4, %xmm15, %xmm4;
 +	vpxor %xmm5, %xmm15, %xmm5;
 +	vpxor %xmm6, %xmm15, %xmm6;
 +	vpxor %xmm7, %xmm15, %xmm7;
 +	vpxor %xmm8, %xmm15, %xmm8;
 +	vpxor %xmm9, %xmm15, %xmm9;
 +	vpxor %xmm10, %xmm15, %xmm10;
 +	vpxor %xmm11, %xmm15, %xmm11;
 +	vpxor %xmm12, %xmm15, %xmm12;
 +	vpxor %xmm13, %xmm15, %xmm13;
 +	vpxor 14 * 16(%rax), %xmm15, %xmm14;
 +	vpxor 15 * 16(%rax), %xmm15, %xmm15;
 +
 +	CALL_NOSPEC %r9;
 +
 +	addq $(16 * 16), %rsp;
 +
 +	vpxor 0 * 16(%rsi), %xmm7, %xmm7;
 +	vpxor 1 * 16(%rsi), %xmm6, %xmm6;
 +	vpxor 2 * 16(%rsi), %xmm5, %xmm5;
 +	vpxor 3 * 16(%rsi), %xmm4, %xmm4;
 +	vpxor 4 * 16(%rsi), %xmm3, %xmm3;
 +	vpxor 5 * 16(%rsi), %xmm2, %xmm2;
 +	vpxor 6 * 16(%rsi), %xmm1, %xmm1;
 +	vpxor 7 * 16(%rsi), %xmm0, %xmm0;
 +	vpxor 8 * 16(%rsi), %xmm15, %xmm15;
 +	vpxor 9 * 16(%rsi), %xmm14, %xmm14;
 +	vpxor 10 * 16(%rsi), %xmm13, %xmm13;
 +	vpxor 11 * 16(%rsi), %xmm12, %xmm12;
 +	vpxor 12 * 16(%rsi), %xmm11, %xmm11;
 +	vpxor 13 * 16(%rsi), %xmm10, %xmm10;
 +	vpxor 14 * 16(%rsi), %xmm9, %xmm9;
 +	vpxor 15 * 16(%rsi), %xmm8, %xmm8;
 +	write_output(%xmm7, %xmm6, %xmm5, %xmm4, %xmm3, %xmm2, %xmm1, %xmm0,
 +		     %xmm15, %xmm14, %xmm13, %xmm12, %xmm11, %xmm10, %xmm9,
 +		     %xmm8, %rsi);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(camellia_xts_crypt_16way)
 +
 +ENTRY(camellia_xts_enc_16way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (16 blocks)
 +	 *	%rdx: src (16 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	xorl %r8d, %r8d; /* input whitening key, 0 for enc */
 +
 +	leaq __camellia_enc_blk16, %r9;
 +
 +	jmp camellia_xts_crypt_16way;
 +ENDPROC(camellia_xts_enc_16way)
 +
 +ENTRY(camellia_xts_dec_16way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (16 blocks)
 +	 *	%rdx: src (16 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +
 +	cmpl $16, key_length(CTX);
 +	movl $32, %r8d;
 +	movl $24, %eax;
 +	cmovel %eax, %r8d;  /* input whitening key, last for dec */
 +
 +	leaq __camellia_dec_blk16, %r9;
 +
 +	jmp camellia_xts_crypt_16way;
 +ENDPROC(camellia_xts_dec_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(camellia_cbc_dec_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/camellia-aesni-avx2-asm_64.S
index 511fc58d8406,0e4e9abbf4de..000000000000
--- a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
+++ b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S
@@@ -214,16 -226,16 +214,26 @@@ roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1
  	roundsm32(%ymm0, %ymm1, %ymm2, %ymm3, %ymm4, %ymm5, %ymm6, %ymm7,
  		  %ymm8, %ymm9, %ymm10, %ymm11, %ymm12, %ymm13, %ymm14, %ymm15,
  		  %rcx, (%r9));
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
++=======
+ 	RET;
+ SYM_FUNC_END(roundsm32_x0_x1_x2_x3_x4_x5_x6_x7_y0_y1_y2_y3_y4_y5_y6_y7_cd)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .align 8
 -SYM_FUNC_START_LOCAL(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
 +roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab:
  	roundsm32(%ymm4, %ymm5, %ymm6, %ymm7, %ymm0, %ymm1, %ymm2, %ymm3,
  		  %ymm12, %ymm13, %ymm14, %ymm15, %ymm8, %ymm9, %ymm10, %ymm11,
  		  %rax, (%r9));
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
++=======
+ 	RET;
+ SYM_FUNC_END(roundsm32_x4_x5_x6_x7_x0_x1_x2_x3_y4_y5_y6_y7_y0_y1_y2_y3_ab)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * IN/OUT:
@@@ -938,10 -946,10 +948,15 @@@ ENTRY(camellia_ecb_enc_32way
  	vzeroupper;
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(camellia_ecb_enc_32way)
++=======
+ 	RET;
+ SYM_FUNC_END(camellia_ecb_enc_32way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(camellia_ecb_dec_32way)
 +ENTRY(camellia_ecb_dec_32way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst (32 blocks)
@@@ -972,10 -980,10 +987,15 @@@
  	vzeroupper;
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(camellia_ecb_dec_32way)
++=======
+ 	RET;
+ SYM_FUNC_END(camellia_ecb_dec_32way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(camellia_cbc_dec_32way)
 +ENTRY(camellia_cbc_dec_32way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst (32 blocks)
@@@ -1039,346 -1045,7 +1059,351 @@@
  
  	vzeroupper;
  
 +	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(camellia_cbc_dec_32way)
 +
 +#define inc_le128(x, minus_one, tmp) \
 +	vpcmpeqq minus_one, x, tmp; \
 +	vpsubq minus_one, x, x; \
 +	vpslldq $8, tmp, tmp; \
 +	vpsubq tmp, x, x;
 +
 +#define add2_le128(x, minus_one, minus_two, tmp1, tmp2) \
 +	vpcmpeqq minus_one, x, tmp1; \
 +	vpcmpeqq minus_two, x, tmp2; \
 +	vpsubq minus_two, x, x; \
 +	vpor tmp2, tmp1, tmp1; \
 +	vpslldq $8, tmp1, tmp1; \
 +	vpsubq tmp1, x, x;
 +
 +ENTRY(camellia_ctr_32way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (32 blocks)
 +	 *	%rdx: src (32 blocks)
 +	 *	%rcx: iv (little endian, 128bit)
 +	 */
 +	FRAME_BEGIN
 +
 +	vzeroupper;
 +
 +	movq %rsp, %r10;
 +	cmpq %rsi, %rdx;
 +	je .Lctr_use_stack;
 +
 +	/* dst can be used as temporary storage, src is not overwritten. */
 +	movq %rsi, %rax;
 +	jmp .Lctr_continue;
 +
 +.Lctr_use_stack:
 +	subq $(16 * 32), %rsp;
 +	movq %rsp, %rax;
 +
 +.Lctr_continue:
 +	vpcmpeqd %ymm15, %ymm15, %ymm15;
 +	vpsrldq $8, %ymm15, %ymm15; /* ab: -1:0 ; cd: -1:0 */
 +	vpaddq %ymm15, %ymm15, %ymm12; /* ab: -2:0 ; cd: -2:0 */
 +
 +	/* load IV and byteswap */
 +	vmovdqu (%rcx), %xmm0;
 +	vmovdqa %xmm0, %xmm1;
 +	inc_le128(%xmm0, %xmm15, %xmm14);
 +	vbroadcasti128 .Lbswap128_mask, %ymm14;
 +	vinserti128 $1, %xmm0, %ymm1, %ymm0;
 +	vpshufb %ymm14, %ymm0, %ymm13;
 +	vmovdqu %ymm13, 15 * 32(%rax);
 +
 +	/* construct IVs */
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13); /* ab:le2 ; cd:le3 */
 +	vpshufb %ymm14, %ymm0, %ymm13;
 +	vmovdqu %ymm13, 14 * 32(%rax);
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm13;
 +	vmovdqu %ymm13, 13 * 32(%rax);
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm13;
 +	vmovdqu %ymm13, 12 * 32(%rax);
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm13;
 +	vmovdqu %ymm13, 11 * 32(%rax);
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm10;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm9;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm8;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm7;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm6;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm5;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm4;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm3;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm2;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vpshufb %ymm14, %ymm0, %ymm1;
 +	add2_le128(%ymm0, %ymm15, %ymm12, %ymm11, %ymm13);
 +	vextracti128 $1, %ymm0, %xmm13;
 +	vpshufb %ymm14, %ymm0, %ymm0;
 +	inc_le128(%xmm13, %xmm15, %xmm14);
 +	vmovdqu %xmm13, (%rcx);
 +
 +	/* inpack32_pre: */
 +	vpbroadcastq (key_table)(CTX), %ymm15;
 +	vpshufb .Lpack_bswap, %ymm15, %ymm15;
 +	vpxor %ymm0, %ymm15, %ymm0;
 +	vpxor %ymm1, %ymm15, %ymm1;
 +	vpxor %ymm2, %ymm15, %ymm2;
 +	vpxor %ymm3, %ymm15, %ymm3;
 +	vpxor %ymm4, %ymm15, %ymm4;
 +	vpxor %ymm5, %ymm15, %ymm5;
 +	vpxor %ymm6, %ymm15, %ymm6;
 +	vpxor %ymm7, %ymm15, %ymm7;
 +	vpxor %ymm8, %ymm15, %ymm8;
 +	vpxor %ymm9, %ymm15, %ymm9;
 +	vpxor %ymm10, %ymm15, %ymm10;
 +	vpxor 11 * 32(%rax), %ymm15, %ymm11;
 +	vpxor 12 * 32(%rax), %ymm15, %ymm12;
 +	vpxor 13 * 32(%rax), %ymm15, %ymm13;
 +	vpxor 14 * 32(%rax), %ymm15, %ymm14;
 +	vpxor 15 * 32(%rax), %ymm15, %ymm15;
 +
 +	call __camellia_enc_blk32;
 +
 +	movq %r10, %rsp;
 +
 +	vpxor 0 * 32(%rdx), %ymm7, %ymm7;
 +	vpxor 1 * 32(%rdx), %ymm6, %ymm6;
 +	vpxor 2 * 32(%rdx), %ymm5, %ymm5;
 +	vpxor 3 * 32(%rdx), %ymm4, %ymm4;
 +	vpxor 4 * 32(%rdx), %ymm3, %ymm3;
 +	vpxor 5 * 32(%rdx), %ymm2, %ymm2;
 +	vpxor 6 * 32(%rdx), %ymm1, %ymm1;
 +	vpxor 7 * 32(%rdx), %ymm0, %ymm0;
 +	vpxor 8 * 32(%rdx), %ymm15, %ymm15;
 +	vpxor 9 * 32(%rdx), %ymm14, %ymm14;
 +	vpxor 10 * 32(%rdx), %ymm13, %ymm13;
 +	vpxor 11 * 32(%rdx), %ymm12, %ymm12;
 +	vpxor 12 * 32(%rdx), %ymm11, %ymm11;
 +	vpxor 13 * 32(%rdx), %ymm10, %ymm10;
 +	vpxor 14 * 32(%rdx), %ymm9, %ymm9;
 +	vpxor 15 * 32(%rdx), %ymm8, %ymm8;
 +	write_output(%ymm7, %ymm6, %ymm5, %ymm4, %ymm3, %ymm2, %ymm1, %ymm0,
 +		     %ymm15, %ymm14, %ymm13, %ymm12, %ymm11, %ymm10, %ymm9,
 +		     %ymm8, %rsi);
 +
 +	vzeroupper;
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(camellia_ctr_32way)
 +
 +#define gf128mul_x_ble(iv, mask, tmp) \
 +	vpsrad $31, iv, tmp; \
 +	vpaddq iv, iv, iv; \
 +	vpshufd $0x13, tmp, tmp; \
 +	vpand mask, tmp, tmp; \
 +	vpxor tmp, iv, iv;
 +
 +#define gf128mul_x2_ble(iv, mask1, mask2, tmp0, tmp1) \
 +	vpsrad $31, iv, tmp0; \
 +	vpaddq iv, iv, tmp1; \
 +	vpsllq $2, iv, iv; \
 +	vpshufd $0x13, tmp0, tmp0; \
 +	vpsrad $31, tmp1, tmp1; \
 +	vpand mask2, tmp0, tmp0; \
 +	vpshufd $0x13, tmp1, tmp1; \
 +	vpxor tmp0, iv, iv; \
 +	vpand mask1, tmp1, tmp1; \
 +	vpxor tmp1, iv, iv;
 +
 +.align 8
 +camellia_xts_crypt_32way:
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (32 blocks)
 +	 *	%rdx: src (32 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 *	%r8: index for input whitening key
 +	 *	%r9: pointer to  __camellia_enc_blk32 or __camellia_dec_blk32
 +	 */
 +	FRAME_BEGIN
 +
 +	vzeroupper;
 +
 +	subq $(16 * 32), %rsp;
 +	movq %rsp, %rax;
 +
 +	vbroadcasti128 .Lxts_gf128mul_and_shl1_mask_0, %ymm12;
 +
 +	/* load IV and construct second IV */
 +	vmovdqu (%rcx), %xmm0;
 +	vmovdqa %xmm0, %xmm15;
 +	gf128mul_x_ble(%xmm0, %xmm12, %xmm13);
 +	vbroadcasti128 .Lxts_gf128mul_and_shl1_mask_1, %ymm13;
 +	vinserti128 $1, %xmm0, %ymm15, %ymm0;
 +	vpxor 0 * 32(%rdx), %ymm0, %ymm15;
 +	vmovdqu %ymm15, 15 * 32(%rax);
 +	vmovdqu %ymm0, 0 * 32(%rsi);
 +
 +	/* construct IVs */
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 1 * 32(%rdx), %ymm0, %ymm15;
 +	vmovdqu %ymm15, 14 * 32(%rax);
 +	vmovdqu %ymm0, 1 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 2 * 32(%rdx), %ymm0, %ymm15;
 +	vmovdqu %ymm15, 13 * 32(%rax);
 +	vmovdqu %ymm0, 2 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 3 * 32(%rdx), %ymm0, %ymm15;
 +	vmovdqu %ymm15, 12 * 32(%rax);
 +	vmovdqu %ymm0, 3 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 4 * 32(%rdx), %ymm0, %ymm11;
 +	vmovdqu %ymm0, 4 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 5 * 32(%rdx), %ymm0, %ymm10;
 +	vmovdqu %ymm0, 5 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 6 * 32(%rdx), %ymm0, %ymm9;
 +	vmovdqu %ymm0, 6 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 7 * 32(%rdx), %ymm0, %ymm8;
 +	vmovdqu %ymm0, 7 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 8 * 32(%rdx), %ymm0, %ymm7;
 +	vmovdqu %ymm0, 8 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 9 * 32(%rdx), %ymm0, %ymm6;
 +	vmovdqu %ymm0, 9 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 10 * 32(%rdx), %ymm0, %ymm5;
 +	vmovdqu %ymm0, 10 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 11 * 32(%rdx), %ymm0, %ymm4;
 +	vmovdqu %ymm0, 11 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 12 * 32(%rdx), %ymm0, %ymm3;
 +	vmovdqu %ymm0, 12 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 13 * 32(%rdx), %ymm0, %ymm2;
 +	vmovdqu %ymm0, 13 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 14 * 32(%rdx), %ymm0, %ymm1;
 +	vmovdqu %ymm0, 14 * 32(%rsi);
 +
 +	gf128mul_x2_ble(%ymm0, %ymm12, %ymm13, %ymm14, %ymm15);
 +	vpxor 15 * 32(%rdx), %ymm0, %ymm15;
 +	vmovdqu %ymm15, 0 * 32(%rax);
 +	vmovdqu %ymm0, 15 * 32(%rsi);
 +
 +	vextracti128 $1, %ymm0, %xmm0;
 +	gf128mul_x_ble(%xmm0, %xmm12, %xmm15);
 +	vmovdqu %xmm0, (%rcx);
 +
 +	/* inpack32_pre: */
 +	vpbroadcastq (key_table)(CTX, %r8, 8), %ymm15;
 +	vpshufb .Lpack_bswap, %ymm15, %ymm15;
 +	vpxor 0 * 32(%rax), %ymm15, %ymm0;
 +	vpxor %ymm1, %ymm15, %ymm1;
 +	vpxor %ymm2, %ymm15, %ymm2;
 +	vpxor %ymm3, %ymm15, %ymm3;
 +	vpxor %ymm4, %ymm15, %ymm4;
 +	vpxor %ymm5, %ymm15, %ymm5;
 +	vpxor %ymm6, %ymm15, %ymm6;
 +	vpxor %ymm7, %ymm15, %ymm7;
 +	vpxor %ymm8, %ymm15, %ymm8;
 +	vpxor %ymm9, %ymm15, %ymm9;
 +	vpxor %ymm10, %ymm15, %ymm10;
 +	vpxor %ymm11, %ymm15, %ymm11;
 +	vpxor 12 * 32(%rax), %ymm15, %ymm12;
 +	vpxor 13 * 32(%rax), %ymm15, %ymm13;
 +	vpxor 14 * 32(%rax), %ymm15, %ymm14;
 +	vpxor 15 * 32(%rax), %ymm15, %ymm15;
 +
 +	CALL_NOSPEC %r9;
 +
  	addq $(16 * 32), %rsp;
 +
 +	vpxor 0 * 32(%rsi), %ymm7, %ymm7;
 +	vpxor 1 * 32(%rsi), %ymm6, %ymm6;
 +	vpxor 2 * 32(%rsi), %ymm5, %ymm5;
 +	vpxor 3 * 32(%rsi), %ymm4, %ymm4;
 +	vpxor 4 * 32(%rsi), %ymm3, %ymm3;
 +	vpxor 5 * 32(%rsi), %ymm2, %ymm2;
 +	vpxor 6 * 32(%rsi), %ymm1, %ymm1;
 +	vpxor 7 * 32(%rsi), %ymm0, %ymm0;
 +	vpxor 8 * 32(%rsi), %ymm15, %ymm15;
 +	vpxor 9 * 32(%rsi), %ymm14, %ymm14;
 +	vpxor 10 * 32(%rsi), %ymm13, %ymm13;
 +	vpxor 11 * 32(%rsi), %ymm12, %ymm12;
 +	vpxor 12 * 32(%rsi), %ymm11, %ymm11;
 +	vpxor 13 * 32(%rsi), %ymm10, %ymm10;
 +	vpxor 14 * 32(%rsi), %ymm9, %ymm9;
 +	vpxor 15 * 32(%rsi), %ymm8, %ymm8;
 +	write_output(%ymm7, %ymm6, %ymm5, %ymm4, %ymm3, %ymm2, %ymm1, %ymm0,
 +		     %ymm15, %ymm14, %ymm13, %ymm12, %ymm11, %ymm10, %ymm9,
 +		     %ymm8, %rsi);
 +
 +	vzeroupper;
 +
  	FRAME_END
 +	ret;
 +ENDPROC(camellia_xts_crypt_32way)
 +
 +ENTRY(camellia_xts_enc_32way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (32 blocks)
 +	 *	%rdx: src (32 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +
 +	xorl %r8d, %r8d; /* input whitening key, 0 for enc */
 +
 +	leaq __camellia_enc_blk32, %r9;
 +
 +	jmp camellia_xts_crypt_32way;
 +ENDPROC(camellia_xts_enc_32way)
 +
 +ENTRY(camellia_xts_dec_32way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (32 blocks)
 +	 *	%rdx: src (32 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +
 +	cmpl $16, key_length(CTX);
 +	movl $32, %r8d;
 +	movl $24, %eax;
 +	cmovel %eax, %r8d;  /* input whitening key, last for dec */
 +
 +	leaq __camellia_dec_blk32, %r9;
 +
 +	jmp camellia_xts_crypt_32way;
 +ENDPROC(camellia_xts_dec_32way)
++=======
+ 	RET;
+ SYM_FUNC_END(camellia_cbc_dec_32way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/camellia-x86_64-asm_64.S
index 310319c601ed,347c059f5940..000000000000
--- a/arch/x86/crypto/camellia-x86_64-asm_64.S
+++ b/arch/x86/crypto/camellia-x86_64-asm_64.S
@@@ -227,17 -212,17 +227,28 @@@ ENTRY(__camellia_enc_blk
  
  	enc_outunpack(mov, RT1);
  
++<<<<<<< HEAD
 +	movq RRBP, %rbp;
 +	ret;
++=======
+ 	movq RR12, %r12;
+ 	RET;
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .L__enc_xor:
  	enc_outunpack(xor, RT1);
  
++<<<<<<< HEAD
 +	movq RRBP, %rbp;
 +	ret;
 +ENDPROC(__camellia_enc_blk)
++=======
+ 	movq RR12, %r12;
+ 	RET;
+ SYM_FUNC_END(__camellia_enc_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(camellia_dec_blk)
 +ENTRY(camellia_dec_blk)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -271,9 -256,9 +282,15 @@@
  
  	dec_outunpack();
  
++<<<<<<< HEAD
 +	movq RRBP, %rbp;
 +	ret;
 +ENDPROC(camellia_dec_blk)
++=======
+ 	movq RR12, %r12;
+ 	RET;
+ SYM_FUNC_END(camellia_dec_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /**********************************************************************
    2-way camellia
@@@ -461,19 -446,19 +478,24 @@@ ENTRY(__camellia_enc_blk_2way
  
  	enc_outunpack2(mov, RT2);
  
 -	movq RR12, %r12;
 +	movq RRBP, %rbp;
  	popq %rbx;
- 	ret;
+ 	RET;
  
  .L__enc2_xor:
  	enc_outunpack2(xor, RT2);
  
 -	movq RR12, %r12;
 +	movq RRBP, %rbp;
  	popq %rbx;
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__camellia_enc_blk_2way)
++=======
+ 	RET;
+ SYM_FUNC_END(__camellia_enc_blk_2way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(camellia_dec_blk_2way)
 +ENTRY(camellia_dec_blk_2way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -508,7 -493,7 +530,12 @@@
  
  	dec_outunpack2();
  
 -	movq RR12, %r12;
 +	movq RRBP, %rbp;
  	movq RXOR, %rbx;
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(camellia_dec_blk_2way)
++=======
+ 	RET;
+ SYM_FUNC_END(camellia_dec_blk_2way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/cast5-avx-x86_64-asm_64.S
index 14fa1966bf01,b258af420c92..000000000000
--- a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
@@@ -282,13 -279,13 +282,18 @@@ __cast5_enc_blk16
  	outunpack_blocks(RR3, RL3, RTMP, RX, RKM);
  	outunpack_blocks(RR4, RL4, RTMP, RX, RKM);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__cast5_enc_blk16)
++=======
+ 	RET;
+ SYM_FUNC_END(__cast5_enc_blk16)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .align 16
 -SYM_FUNC_START_LOCAL(__cast5_dec_blk16)
 +__cast5_dec_blk16:
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	RL1: encrypted blocks 1 and 2
  	 *	RR1: encrypted blocks 3 and 4
  	 *	RL2: encrypted blocks 5 and 6
@@@ -390,13 -391,14 +395,18 @@@ ENTRY(cast5_ecb_enc_16way
  	vmovdqu RR4, (6*4*4)(%r11);
  	vmovdqu RL4, (7*4*4)(%r11);
  
 -	popq %r15;
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(cast5_ecb_enc_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(cast5_ecb_enc_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(cast5_ecb_dec_16way)
 +ENTRY(cast5_ecb_dec_16way)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 */
@@@ -424,13 -429,14 +434,18 @@@
  	vmovdqu RR4, (6*4*4)(%r11);
  	vmovdqu RL4, (7*4*4)(%r11);
  
 -	popq %r15;
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(cast5_ecb_dec_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(cast5_ecb_dec_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(cast5_cbc_dec_16way)
 +ENTRY(cast5_cbc_dec_16way)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 */
@@@ -473,15 -480,15 +488,20 @@@
  	vmovdqu RR4, (6*16)(%r11);
  	vmovdqu RL4, (7*16)(%r11);
  
 -	popq %r15;
  	popq %r12;
 +
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(cast5_cbc_dec_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(cast5_cbc_dec_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(cast5_ctr_16way)
 +ENTRY(cast5_ctr_16way)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 *	%rcx: iv (big endian, 64bit)
@@@ -548,8 -556,8 +568,13 @@@
  	vmovdqu RR4, (6*16)(%r11);
  	vmovdqu RL4, (7*16)(%r11);
  
 -	popq %r15;
  	popq %r12;
 +
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(cast5_ctr_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(cast5_ctr_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/cast6-avx-x86_64-asm_64.S
index c419389889cd,82b716fd5dba..000000000000
--- a/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast6-avx-x86_64-asm_64.S
@@@ -296,13 -289,13 +296,18 @@@ __cast6_enc_blk8
  	outunpack_blocks(RA1, RB1, RC1, RD1, RTMP, RX, RKRF, RKM);
  	outunpack_blocks(RA2, RB2, RC2, RD2, RTMP, RX, RKRF, RKM);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__cast6_enc_blk8)
++=======
+ 	RET;
+ SYM_FUNC_END(__cast6_enc_blk8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .align 8
 -SYM_FUNC_START_LOCAL(__cast6_dec_blk8)
 +__cast6_dec_blk8:
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2: encrypted blocks
  	 * output:
  	 *	RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2: decrypted blocks
@@@ -341,12 -336,12 +346,17 @@@
  	outunpack_blocks(RA1, RB1, RC1, RD1, RTMP, RX, RKRF, RKM);
  	outunpack_blocks(RA2, RB2, RC2, RD2, RTMP, RX, RKRF, RKM);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__cast6_dec_blk8)
++=======
+ 	RET;
+ SYM_FUNC_END(__cast6_dec_blk8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(cast6_ecb_enc_8way)
 +ENTRY(cast6_ecb_enc_8way)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 */
@@@ -360,13 -357,14 +370,18 @@@
  
  	store_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
  
 -	popq %r15;
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(cast6_ecb_enc_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(cast6_ecb_enc_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(cast6_ecb_dec_8way)
 +ENTRY(cast6_ecb_dec_8way)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 */
@@@ -380,13 -380,14 +395,18 @@@
  
  	store_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
  
 -	popq %r15;
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(cast6_ecb_dec_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(cast6_ecb_dec_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(cast6_cbc_dec_8way)
 +ENTRY(cast6_cbc_dec_8way)
  	/* input:
 -	 *	%rdi: ctx
 +	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
  	 *	%rdx: src
  	 */
@@@ -403,83 -405,8 +423,88 @@@
  
  	store_cbc_8way(%r12, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
  
 -	popq %r15;
  	popq %r12;
 +
 +	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(cast6_cbc_dec_8way)
 +
 +ENTRY(cast6_ctr_8way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (little endian, 128bit)
 +	 */
 +	FRAME_BEGIN
 +
 +	pushq %r12;
 +
 +	movq %rsi, %r11;
 +	movq %rdx, %r12;
 +
 +	load_ctr_8way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 +		      RD2, RX, RKR, RKM);
 +
 +	call __cast6_enc_blk8;
 +
 +	store_ctr_8way(%r12, %r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
 +	popq %r12;
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(cast6_ctr_8way)
 +
 +ENTRY(cast6_xts_enc_8way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	movq %rsi, %r11;
 +
 +	/* regs <= src, dst <= IVs, regs <= regs xor IVs */
 +	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 +		      RX, RKR, RKM, .Lxts_gf128mul_and_shl1_mask);
 +
 +	call __cast6_enc_blk8;
 +
 +	/* dst <= regs xor IVs(in dst) */
 +	store_xts_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(cast6_xts_enc_8way)
 +
 +ENTRY(cast6_xts_dec_8way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	movq %rsi, %r11;
 +
 +	/* regs <= src, dst <= IVs, regs <= regs xor IVs */
 +	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 +		      RX, RKR, RKM, .Lxts_gf128mul_and_shl1_mask);
 +
 +	call __cast6_dec_blk8;
 +
 +	/* dst <= regs xor IVs(in dst) */
 +	store_xts_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
  	FRAME_END
 +	ret;
 +ENDPROC(cast6_xts_dec_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(cast6_cbc_dec_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/crc32-pclmul_asm.S
index f247304299a2,c392a6edbfff..000000000000
--- a/arch/x86/crypto/crc32-pclmul_asm.S
+++ b/arch/x86/crypto/crc32-pclmul_asm.S
@@@ -236,11 -230,11 +236,16 @@@ fold_64
  #endif
  	movdqa  %xmm1, %xmm2
  	pand    %xmm3, %xmm1
 -	pclmulqdq $0x10, CONSTANT, %xmm1
 +	PCLMULQDQ 0x10, CONSTANT, %xmm1
  	pand    %xmm3, %xmm1
 -	pclmulqdq $0x00, CONSTANT, %xmm1
 +	PCLMULQDQ 0x00, CONSTANT, %xmm1
  	pxor    %xmm2, %xmm1
 -	pextrd  $0x01, %xmm1, %eax
 +	PEXTRD  0x01, %xmm1, %eax
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(crc32_pclmul_le_16)
++=======
+ 	RET
+ SYM_FUNC_END(crc32_pclmul_le_16)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/crc32c-pcl-intel-asm_64.S
index 358bd59de4d8,80c0d22fc42c..000000000000
--- a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
+++ b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S
@@@ -313,10 -306,10 +313,15 @@@ do_return
  	popq    %rsi
  	popq    %rdi
  	popq    %rbx
++<<<<<<< HEAD
 +        ret
 +ENDPROC(crc_pcl)
++=======
+         RET
+ SYM_FUNC_END(crc_pcl)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -.section	.rodata, "a", @progbits
 +.section	.rodata, "a", %progbits
          ################################################################
          ## jump table        Table is 129 entries x 2 bytes each
          ################################################################
diff --cc arch/x86/crypto/crct10dif-pcl-asm_64.S
index 35e97569d05f,721474abfb71..000000000000
--- a/arch/x86/crypto/crct10dif-pcl-asm_64.S
+++ b/arch/x86/crypto/crct10dif-pcl-asm_64.S
@@@ -62,582 -54,280 +62,609 @@@
  
  .text
  
 -#define		init_crc	%edi
 -#define		buf		%rsi
 -#define		len		%rdx
 -
 -#define		FOLD_CONSTS	%xmm10
 -#define		BSWAP_MASK	%xmm11
 -
 -# Fold reg1, reg2 into the next 32 data bytes, storing the result back into
 -# reg1, reg2.
 -.macro	fold_32_bytes	offset, reg1, reg2
 -	movdqu	\offset(buf), %xmm9
 -	movdqu	\offset+16(buf), %xmm12
 -	pshufb	BSWAP_MASK, %xmm9
 -	pshufb	BSWAP_MASK, %xmm12
 -	movdqa	\reg1, %xmm8
 -	movdqa	\reg2, %xmm13
 -	pclmulqdq	$0x00, FOLD_CONSTS, \reg1
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm8
 -	pclmulqdq	$0x00, FOLD_CONSTS, \reg2
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm13
 -	pxor	%xmm9 , \reg1
 -	xorps	%xmm8 , \reg1
 -	pxor	%xmm12, \reg2
 -	xorps	%xmm13, \reg2
 -.endm
 -
 -# Fold src_reg into dst_reg.
 -.macro	fold_16_bytes	src_reg, dst_reg
 -	movdqa	\src_reg, %xmm8
 -	pclmulqdq	$0x11, FOLD_CONSTS, \src_reg
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
 -	pxor	%xmm8, \dst_reg
 -	xorps	\src_reg, \dst_reg
 -.endm
 +#define        arg1 %rdi
 +#define        arg2 %rsi
 +#define        arg3 %rdx
  
 -#
 -# u16 crc_t10dif_pcl(u16 init_crc, const *u8 buf, size_t len);
 -#
 -# Assumes len >= 16.
 -#
 +#define        arg1_low32 %edi
 +
 +ENTRY(crc_t10dif_pcl)
  .align 16
 -SYM_FUNC_START(crc_t10dif_pcl)
 -
 -	movdqa	.Lbswap_mask(%rip), BSWAP_MASK
 -
 -	# For sizes less than 256 bytes, we can't fold 128 bytes at a time.
 -	cmp	$256, len
 -	jl	.Lless_than_256_bytes
 -
 -	# Load the first 128 data bytes.  Byte swapping is necessary to make the
 -	# bit order match the polynomial coefficient order.
 -	movdqu	16*0(buf), %xmm0
 -	movdqu	16*1(buf), %xmm1
 -	movdqu	16*2(buf), %xmm2
 -	movdqu	16*3(buf), %xmm3
 -	movdqu	16*4(buf), %xmm4
 -	movdqu	16*5(buf), %xmm5
 -	movdqu	16*6(buf), %xmm6
 -	movdqu	16*7(buf), %xmm7
 -	add	$128, buf
 -	pshufb	BSWAP_MASK, %xmm0
 -	pshufb	BSWAP_MASK, %xmm1
 -	pshufb	BSWAP_MASK, %xmm2
 -	pshufb	BSWAP_MASK, %xmm3
 -	pshufb	BSWAP_MASK, %xmm4
 -	pshufb	BSWAP_MASK, %xmm5
 -	pshufb	BSWAP_MASK, %xmm6
 -	pshufb	BSWAP_MASK, %xmm7
  
 -	# XOR the first 16 data *bits* with the initial CRC value.
 -	pxor	%xmm8, %xmm8
 -	pinsrw	$7, init_crc, %xmm8
 -	pxor	%xmm8, %xmm0
 -
 -	movdqa	.Lfold_across_128_bytes_consts(%rip), FOLD_CONSTS
 -
 -	# Subtract 128 for the 128 data bytes just consumed.  Subtract another
 -	# 128 to simplify the termination condition of the following loop.
 -	sub	$256, len
 -
 -	# While >= 128 data bytes remain (not counting xmm0-7), fold the 128
 -	# bytes xmm0-7 into them, storing the result back into xmm0-7.
 -.Lfold_128_bytes_loop:
 -	fold_32_bytes	0, %xmm0, %xmm1
 -	fold_32_bytes	32, %xmm2, %xmm3
 -	fold_32_bytes	64, %xmm4, %xmm5
 -	fold_32_bytes	96, %xmm6, %xmm7
 -	add	$128, buf
 -	sub	$128, len
 -	jge	.Lfold_128_bytes_loop
 -
 -	# Now fold the 112 bytes in xmm0-xmm6 into the 16 bytes in xmm7.
 -
 -	# Fold across 64 bytes.
 -	movdqa	.Lfold_across_64_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm0, %xmm4
 -	fold_16_bytes	%xmm1, %xmm5
 -	fold_16_bytes	%xmm2, %xmm6
 -	fold_16_bytes	%xmm3, %xmm7
 -	# Fold across 32 bytes.
 -	movdqa	.Lfold_across_32_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm4, %xmm6
 -	fold_16_bytes	%xmm5, %xmm7
 -	# Fold across 16 bytes.
 -	movdqa	.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS
 -	fold_16_bytes	%xmm6, %xmm7
 -
 -	# Add 128 to get the correct number of data bytes remaining in 0...127
 -	# (not counting xmm7), following the previous extra subtraction by 128.
 -	# Then subtract 16 to simplify the termination condition of the
 -	# following loop.
 -	add	$128-16, len
 -
 -	# While >= 16 data bytes remain (not counting xmm7), fold the 16 bytes
 -	# xmm7 into them, storing the result back into xmm7.
 -	jl	.Lfold_16_bytes_loop_done
 -.Lfold_16_bytes_loop:
 +	# adjust the 16-bit initial_crc value, scale it to 32 bits
 +	shl	$16, arg1_low32
 +
 +	# Allocate Stack Space
 +	mov     %rsp, %rcx
 +	sub	$16*2, %rsp
 +	# align stack to 16 byte boundary
 +	and     $~(0x10 - 1), %rsp
 +
 +	# check if smaller than 256
 +	cmp	$256, arg3
 +
 +	# for sizes less than 128, we can't fold 64B at a time...
 +	jl	_less_than_128
 +
 +
 +	# load the initial crc value
 +	movd	arg1_low32, %xmm10	# initial crc
 +
 +	# crc value does not need to be byte-reflected, but it needs
 +	# to be moved to the high part of the register.
 +	# because data will be byte-reflected and will align with
 +	# initial crc at correct place.
 +	pslldq	$12, %xmm10
 +
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +	# receive the initial 64B data, xor the initial crc value
 +	movdqu	16*0(arg2), %xmm0
 +	movdqu	16*1(arg2), %xmm1
 +	movdqu	16*2(arg2), %xmm2
 +	movdqu	16*3(arg2), %xmm3
 +	movdqu	16*4(arg2), %xmm4
 +	movdqu	16*5(arg2), %xmm5
 +	movdqu	16*6(arg2), %xmm6
 +	movdqu	16*7(arg2), %xmm7
 +
 +	pshufb	%xmm11, %xmm0
 +	# XOR the initial_crc value
 +	pxor	%xmm10, %xmm0
 +	pshufb	%xmm11, %xmm1
 +	pshufb	%xmm11, %xmm2
 +	pshufb	%xmm11, %xmm3
 +	pshufb	%xmm11, %xmm4
 +	pshufb	%xmm11, %xmm5
 +	pshufb	%xmm11, %xmm6
 +	pshufb	%xmm11, %xmm7
 +
 +	movdqa	rk3(%rip), %xmm10	#xmm10 has rk3 and rk4
 +					#imm value of pclmulqdq instruction
 +					#will determine which constant to use
 +
 +	#################################################################
 +	# we subtract 256 instead of 128 to save one instruction from the loop
 +	sub	$256, arg3
 +
 +	# at this section of the code, there is 64*x+y (0<=y<64) bytes of
 +	# buffer. The _fold_64_B_loop will fold 64B at a time
 +	# until we have 64+y Bytes of buffer
 +
 +
 +	# fold 64B at a time. This section of the code folds 4 xmm
 +	# registers in parallel
 +_fold_64_B_loop:
 +
 +	# update the buffer pointer
 +	add	$128, arg2		#    buf += 64#
 +
 +	movdqu	16*0(arg2), %xmm9
 +	movdqu	16*1(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm0, %xmm8
 +	movdqa	%xmm1, %xmm13
 +	pclmulqdq	$0x0 , %xmm10, %xmm0
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0 , %xmm10, %xmm1
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm0
 +	xorps	%xmm8 , %xmm0
 +	pxor	%xmm12, %xmm1
 +	xorps	%xmm13, %xmm1
 +
 +	movdqu	16*2(arg2), %xmm9
 +	movdqu	16*3(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm2, %xmm8
 +	movdqa	%xmm3, %xmm13
 +	pclmulqdq	$0x0, %xmm10, %xmm2
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0, %xmm10, %xmm3
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm2
 +	xorps	%xmm8 , %xmm2
 +	pxor	%xmm12, %xmm3
 +	xorps	%xmm13, %xmm3
 +
 +	movdqu	16*4(arg2), %xmm9
 +	movdqu	16*5(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm4, %xmm8
 +	movdqa	%xmm5, %xmm13
 +	pclmulqdq	$0x0,  %xmm10, %xmm4
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0,  %xmm10, %xmm5
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 ,  %xmm4
 +	xorps	%xmm8 ,  %xmm4
 +	pxor	%xmm12,  %xmm5
 +	xorps	%xmm13,  %xmm5
 +
 +	movdqu	16*6(arg2), %xmm9
 +	movdqu	16*7(arg2), %xmm12
 +	pshufb	%xmm11, %xmm9
 +	pshufb	%xmm11, %xmm12
 +	movdqa	%xmm6 , %xmm8
 +	movdqa	%xmm7 , %xmm13
 +	pclmulqdq	$0x0 , %xmm10, %xmm6
 +	pclmulqdq	$0x11, %xmm10, %xmm8
 +	pclmulqdq	$0x0 , %xmm10, %xmm7
 +	pclmulqdq	$0x11, %xmm10, %xmm13
 +	pxor	%xmm9 , %xmm6
 +	xorps	%xmm8 , %xmm6
 +	pxor	%xmm12, %xmm7
 +	xorps	%xmm13, %xmm7
 +
 +	sub	$128, arg3
 +
 +	# check if there is another 64B in the buffer to be able to fold
 +	jge	_fold_64_B_loop
 +	##################################################################
 +
 +
 +	add	$128, arg2
 +	# at this point, the buffer pointer is pointing at the last y Bytes
 +	# of the buffer the 64B of folded data is in 4 of the xmm
 +	# registers: xmm0, xmm1, xmm2, xmm3
 +
 +
 +	# fold the 8 xmm registers to 1 xmm register with different constants
 +
 +	movdqa	rk9(%rip), %xmm10
 +	movdqa	%xmm0, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm0
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm0, %xmm7
 +
 +	movdqa	rk11(%rip), %xmm10
 +	movdqa	%xmm1, %xmm8
 +	pclmulqdq	 $0x11, %xmm10, %xmm1
 +	pclmulqdq	 $0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm1, %xmm7
 +
 +	movdqa	rk13(%rip), %xmm10
 +	movdqa	%xmm2, %xmm8
 +	pclmulqdq	 $0x11, %xmm10, %xmm2
 +	pclmulqdq	 $0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm2, %xmm7
 +
 +	movdqa	rk15(%rip), %xmm10
 +	movdqa	%xmm3, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm3
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm3, %xmm7
 +
 +	movdqa	rk17(%rip), %xmm10
 +	movdqa	%xmm4, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm4
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm4, %xmm7
 +
 +	movdqa	rk19(%rip), %xmm10
 +	movdqa	%xmm5, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm5
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	xorps	%xmm5, %xmm7
 +
 +	movdqa	rk1(%rip), %xmm10	#xmm10 has rk1 and rk2
 +					#imm value of pclmulqdq instruction
 +					#will determine which constant to use
 +	movdqa	%xmm6, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm6
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
 +	pxor	%xmm8, %xmm7
 +	pxor	%xmm6, %xmm7
 +
 +
 +	# instead of 64, we add 48 to the loop counter to save 1 instruction
 +	# from the loop instead of a cmp instruction, we use the negative
 +	# flag with the jl instruction
 +	add	$128-16, arg3
 +	jl	_final_reduction_for_128
 +
 +	# now we have 16+y bytes left to reduce. 16 Bytes is in register xmm7
 +	# and the rest is in memory. We can fold 16 bytes at a time if y>=16
 +	# continue folding 16B at a time
 +
 +_16B_reduction_loop:
  	movdqa	%xmm7, %xmm8
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
  	pxor	%xmm8, %xmm7
 -	movdqu	(buf), %xmm0
 -	pshufb	BSWAP_MASK, %xmm0
 +	movdqu	(arg2), %xmm0
 +	pshufb	%xmm11, %xmm0
  	pxor	%xmm0 , %xmm7
 -	add	$16, buf
 -	sub	$16, len
 -	jge	.Lfold_16_bytes_loop
 -
 -.Lfold_16_bytes_loop_done:
 -	# Add 16 to get the correct number of data bytes remaining in 0...15
 -	# (not counting xmm7), following the previous extra subtraction by 16.
 -	add	$16, len
 -	je	.Lreduce_final_16_bytes
 -
 -.Lhandle_partial_segment:
 -	# Reduce the last '16 + len' bytes where 1 <= len <= 15 and the first 16
 -	# bytes are in xmm7 and the rest are the remaining data in 'buf'.  To do
 -	# this without needing a fold constant for each possible 'len', redivide
 -	# the bytes into a first chunk of 'len' bytes and a second chunk of 16
 -	# bytes, then fold the first chunk into the second.
 -
 +	add	$16, arg2
 +	sub	$16, arg3
 +	# instead of a cmp instruction, we utilize the flags with the
 +	# jge instruction equivalent of: cmp arg3, 16-16
 +	# check if there is any more 16B in the buffer to be able to fold
 +	jge	_16B_reduction_loop
 +
 +	#now we have 16+z bytes left to reduce, where 0<= z < 16.
 +	#first, we reduce the data in the xmm7 register
 +
 +
 +_final_reduction_for_128:
 +	# check if any more data to fold. If not, compute the CRC of
 +	# the final 128 bits
 +	add	$16, arg3
 +	je	_128_done
 +
 +	# here we are getting data that is less than 16 bytes.
 +	# since we know that there was data before the pointer, we can
 +	# offset the input pointer before the actual point, to receive
 +	# exactly 16 bytes. after that the registers need to be adjusted.
 +_get_last_two_xmms:
  	movdqa	%xmm7, %xmm2
  
 -	# xmm1 = last 16 original data bytes
 -	movdqu	-16(buf, len), %xmm1
 -	pshufb	BSWAP_MASK, %xmm1
 +	movdqu	-16(arg2, arg3), %xmm1
 +	pshufb	%xmm11, %xmm1
  
 -	# xmm2 = high order part of second chunk: xmm7 left-shifted by 'len' bytes.
 -	lea	.Lbyteshift_table+16(%rip), %rax
 -	sub	len, %rax
 +	# get rid of the extra data that was loaded before
 +	# load the shift constant
 +	lea	pshufb_shf_table+16(%rip), %rax
 +	sub	arg3, %rax
  	movdqu	(%rax), %xmm0
 +
 +	# shift xmm2 to the left by arg3 bytes
  	pshufb	%xmm0, %xmm2
  
 -	# xmm7 = first chunk: xmm7 right-shifted by '16-len' bytes.
 -	pxor	.Lmask1(%rip), %xmm0
 +	# shift xmm7 to the right by 16-arg3 bytes
 +	pxor	mask1(%rip), %xmm0
  	pshufb	%xmm0, %xmm7
 -
 -	# xmm1 = second chunk: 'len' bytes from xmm1 (low-order bytes),
 -	# then '16-len' bytes from xmm2 (high-order bytes).
  	pblendvb	%xmm2, %xmm1	#xmm0 is implicit
  
 -	# Fold the first chunk into the second chunk, storing the result in xmm7.
 +	# fold 16 Bytes
 +	movdqa	%xmm1, %xmm2
  	movdqa	%xmm7, %xmm8
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm8
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +	pclmulqdq	$0x0 , %xmm10, %xmm8
  	pxor	%xmm8, %xmm7
 -	pxor	%xmm1, %xmm7
 +	pxor	%xmm2, %xmm7
  
 -.Lreduce_final_16_bytes:
 -	# Reduce the 128-bit value M(x), stored in xmm7, to the final 16-bit CRC
 -
 -	# Load 'x^48 * (x^48 mod G(x))' and 'x^48 * (x^80 mod G(x))'.
 -	movdqa	.Lfinal_fold_consts(%rip), FOLD_CONSTS
 -
 -	# Fold the high 64 bits into the low 64 bits, while also multiplying by
 -	# x^64.  This produces a 128-bit value congruent to x^64 * M(x) and
 -	# whose low 48 bits are 0.
 +_128_done:
 +	# compute crc of a 128-bit value
 +	movdqa	rk5(%rip), %xmm10	# rk5 and rk6 in xmm10
  	movdqa	%xmm7, %xmm0
 -	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7 # high bits * x^48 * (x^80 mod G(x))
 -	pslldq	$8, %xmm0
 -	pxor	%xmm0, %xmm7			  # + low bits * x^64
  
 -	# Fold the high 32 bits into the low 96 bits.  This produces a 96-bit
 -	# value congruent to x^64 * M(x) and whose low 48 bits are 0.
 +	#64b fold
 +	pclmulqdq	$0x1, %xmm10, %xmm7
 +	pslldq	$8   ,  %xmm0
 +	pxor	%xmm0,  %xmm7
 +
 +	#32b fold
  	movdqa	%xmm7, %xmm0
 -	pand	.Lmask2(%rip), %xmm0		  # zero high 32 bits
 -	psrldq	$12, %xmm7			  # extract high 32 bits
 -	pclmulqdq	$0x00, FOLD_CONSTS, %xmm7 # high 32 bits * x^48 * (x^48 mod G(x))
 -	pxor	%xmm0, %xmm7			  # + low bits
  
 -	# Load G(x) and floor(x^48 / G(x)).
 -	movdqa	.Lbarrett_reduction_consts(%rip), FOLD_CONSTS
 +	pand	mask2(%rip), %xmm0
  
++<<<<<<< HEAD
 +	psrldq	$12, %xmm7
 +	pclmulqdq	$0x10, %xmm10, %xmm7
++=======
+ 	# Use Barrett reduction to compute the final CRC value.
+ 	movdqa	%xmm7, %xmm0
+ 	pclmulqdq	$0x11, FOLD_CONSTS, %xmm7 # high 32 bits * floor(x^48 / G(x))
+ 	psrlq	$32, %xmm7			  # /= x^32
+ 	pclmulqdq	$0x00, FOLD_CONSTS, %xmm7 # *= G(x)
+ 	psrlq	$48, %xmm0
+ 	pxor	%xmm7, %xmm0		     # + low 16 nonzero bits
+ 	# Final CRC value (x^16 * M(x)) mod G(x) is in low 16 bits of xmm0.
+ 
+ 	pextrw	$0, %xmm0, %eax
+ 	RET
+ 
+ .align 16
+ .Lless_than_256_bytes:
+ 	# Checksumming a buffer of length 16...255 bytes
+ 
+ 	# Load the first 16 data bytes.
+ 	movdqu	(buf), %xmm7
+ 	pshufb	BSWAP_MASK, %xmm7
+ 	add	$16, buf
+ 
+ 	# XOR the first 16 data *bits* with the initial CRC value.
+ 	pxor	%xmm0, %xmm0
+ 	pinsrw	$7, init_crc, %xmm0
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  	pxor	%xmm0, %xmm7
  
 -	movdqa	.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS
 -	cmp	$16, len
 -	je	.Lreduce_final_16_bytes		# len == 16
 -	sub	$32, len
 -	jge	.Lfold_16_bytes_loop		# 32 <= len <= 255
 -	add	$16, len
 -	jmp	.Lhandle_partial_segment	# 17 <= len <= 31
 -SYM_FUNC_END(crc_t10dif_pcl)
 +	#barrett reduction
 +_barrett:
 +	movdqa	rk7(%rip), %xmm10	# rk7 and rk8 in xmm10
 +	movdqa	%xmm7, %xmm0
 +	pclmulqdq	$0x01, %xmm10, %xmm7
 +	pslldq	$4, %xmm7
 +	pclmulqdq	$0x11, %xmm10, %xmm7
 +
 +	pslldq	$4, %xmm7
 +	pxor	%xmm0, %xmm7
 +	pextrd	$1, %xmm7, %eax
 +
 +_cleanup:
 +	# scale the result back to 16 bits
 +	shr	$16, %eax
 +	mov     %rcx, %rsp
 +	ret
 +
 +########################################################################
  
 -.section	.rodata, "a", @progbits
  .align 16
 +_less_than_128:
 +
 +	# check if there is enough buffer to be able to fold 16B at a time
 +	cmp	$32, arg3
 +	jl	_less_than_32
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +
 +	# now if there is, load the constants
 +	movdqa	rk1(%rip), %xmm10	# rk1 and rk2 in xmm10
 +
 +	movd	arg1_low32, %xmm0	# get the initial crc value
 +	pslldq	$12, %xmm0	# align it to its correct place
 +	movdqu	(arg2), %xmm7	# load the plaintext
 +	pshufb	%xmm11, %xmm7	# byte-reflect the plaintext
 +	pxor	%xmm0, %xmm7
 +
 +
 +	# update the buffer pointer
 +	add	$16, arg2
 +
 +	# update the counter. subtract 32 instead of 16 to save one
 +	# instruction from the loop
 +	sub	$32, arg3
 +
 +	jmp	_16B_reduction_loop
 +
  
 -# Fold constants precomputed from the polynomial 0x18bb7
 -# G(x) = x^16 + x^15 + x^11 + x^9 + x^8 + x^7 + x^5 + x^4 + x^2 + x^1 + x^0
 -.Lfold_across_128_bytes_consts:
 -	.quad		0x0000000000006123	# x^(8*128)	mod G(x)
 -	.quad		0x0000000000002295	# x^(8*128+64)	mod G(x)
 -.Lfold_across_64_bytes_consts:
 -	.quad		0x0000000000001069	# x^(4*128)	mod G(x)
 -	.quad		0x000000000000dd31	# x^(4*128+64)	mod G(x)
 -.Lfold_across_32_bytes_consts:
 -	.quad		0x000000000000857d	# x^(2*128)	mod G(x)
 -	.quad		0x0000000000007acc	# x^(2*128+64)	mod G(x)
 -.Lfold_across_16_bytes_consts:
 -	.quad		0x000000000000a010	# x^(1*128)	mod G(x)
 -	.quad		0x0000000000001faa	# x^(1*128+64)	mod G(x)
 -.Lfinal_fold_consts:
 -	.quad		0x1368000000000000	# x^48 * (x^48 mod G(x))
 -	.quad		0x2d56000000000000	# x^48 * (x^80 mod G(x))
 -.Lbarrett_reduction_consts:
 -	.quad		0x0000000000018bb7	# G(x)
 -	.quad		0x00000001f65a57f8	# floor(x^48 / G(x))
 -
 -.section	.rodata.cst16.mask1, "aM", @progbits, 16
  .align 16
 -.Lmask1:
 -	.octa	0x80808080808080808080808080808080
 +_less_than_32:
 +	# mov initial crc to the return value. this is necessary for
 +	# zero-length buffers.
 +	mov	arg1_low32, %eax
 +	test	arg3, arg3
 +	je	_cleanup
 +
 +	movdqa  SHUF_MASK(%rip), %xmm11
 +
 +	movd	arg1_low32, %xmm0	# get the initial crc value
 +	pslldq	$12, %xmm0	# align it to its correct place
 +
 +	cmp	$16, arg3
 +	je	_exact_16_left
 +	jl	_less_than_16_left
 +
 +	movdqu	(arg2), %xmm7	# load the plaintext
 +	pshufb	%xmm11, %xmm7	# byte-reflect the plaintext
 +	pxor	%xmm0 , %xmm7	# xor the initial crc value
 +	add	$16, arg2
 +	sub	$16, arg3
 +	movdqa	rk1(%rip), %xmm10	# rk1 and rk2 in xmm10
 +	jmp	_get_last_two_xmms
 +
  
 -.section	.rodata.cst16.mask2, "aM", @progbits, 16
  .align 16
 -.Lmask2:
 -	.octa	0x00000000FFFFFFFFFFFFFFFFFFFFFFFF
 +_less_than_16_left:
 +	# use stack space to load data less than 16 bytes, zero-out
 +	# the 16B in memory first.
 +
 +	pxor	%xmm1, %xmm1
 +	mov	%rsp, %r11
 +	movdqa	%xmm1, (%r11)
 +
 +	cmp	$4, arg3
 +	jl	_only_less_than_4
 +
 +	# backup the counter value
 +	mov	arg3, %r9
 +	cmp	$8, arg3
 +	jl	_less_than_8_left
 +
 +	# load 8 Bytes
 +	mov	(arg2), %rax
 +	mov	%rax, (%r11)
 +	add	$8, %r11
 +	sub	$8, arg3
 +	add	$8, arg2
 +_less_than_8_left:
 +
 +	cmp	$4, arg3
 +	jl	_less_than_4_left
 +
 +	# load 4 Bytes
 +	mov	(arg2), %eax
 +	mov	%eax, (%r11)
 +	add	$4, %r11
 +	sub	$4, arg3
 +	add	$4, arg2
 +_less_than_4_left:
 +
 +	cmp	$2, arg3
 +	jl	_less_than_2_left
 +
 +	# load 2 Bytes
 +	mov	(arg2), %ax
 +	mov	%ax, (%r11)
 +	add	$2, %r11
 +	sub	$2, arg3
 +	add	$2, arg2
 +_less_than_2_left:
 +	cmp     $1, arg3
 +        jl      _zero_left
 +
 +	# load 1 Byte
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +_zero_left:
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7	# xor the initial crc value
 +
 +	# shl r9, 4
 +	lea	pshufb_shf_table+16(%rip), %rax
 +	sub	%r9, %rax
 +	movdqu	(%rax), %xmm0
 +	pxor	mask1(%rip), %xmm0
 +
 +	pshufb	%xmm0, %xmm7
 +	jmp	_128_done
  
 -.section	.rodata.cst16.bswap_mask, "aM", @progbits, 16
  .align 16
 -.Lbswap_mask:
 -	.octa	0x000102030405060708090A0B0C0D0E0F
 +_exact_16_left:
 +	movdqu	(arg2), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	jmp	_128_done
 +
 +_only_less_than_4:
 +	cmp	$3, arg3
 +	jl	_only_less_than_3
 +
 +	# load 3 Bytes
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	mov	1(arg2), %al
 +	mov	%al, 1(%r11)
 +
 +	mov	2(arg2), %al
 +	mov	%al, 2(%r11)
 +
 +	movdqa	 (%rsp), %xmm7
 +	pshufb	 %xmm11, %xmm7
 +	pxor	 %xmm0 , %xmm7  # xor the initial crc value
 +
 +	psrldq	$5, %xmm7
 +
 +	jmp	_barrett
 +_only_less_than_3:
 +	cmp	$2, arg3
 +	jl	_only_less_than_2
 +
 +	# load 2 Bytes
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	mov	1(arg2), %al
 +	mov	%al, 1(%r11)
 +
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	psrldq	$6, %xmm7
 +
 +	jmp	_barrett
 +_only_less_than_2:
 +
 +	# load 1 Byte
 +	mov	(arg2), %al
 +	mov	%al, (%r11)
 +
 +	movdqa	(%rsp), %xmm7
 +	pshufb	%xmm11, %xmm7
 +	pxor	%xmm0 , %xmm7   # xor the initial crc value
 +
 +	psrldq	$7, %xmm7
 +
 +	jmp	_barrett
 +
 +ENDPROC(crc_t10dif_pcl)
 +
 +.data
  
 -.section	.rodata.cst32.byteshift_table, "aM", @progbits, 32
 +# precomputed constants
 +# these constants are precomputed from the poly:
 +# 0x8bb70000 (0x8bb7 scaled to 32 bits)
  .align 16
 -# For 1 <= len <= 15, the 16-byte vector beginning at &byteshift_table[16 - len]
 -# is the index vector to shift left by 'len' bytes, and is also {0x80, ...,
 -# 0x80} XOR the index vector to shift right by '16 - len' bytes.
 -.Lbyteshift_table:
 -	.byte		 0x0, 0x81, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87
 -	.byte		0x88, 0x89, 0x8a, 0x8b, 0x8c, 0x8d, 0x8e, 0x8f
 -	.byte		 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7
 -	.byte		 0x8,  0x9,  0xa,  0xb,  0xc,  0xd,  0xe , 0x0
 +# Q = 0x18BB70000
 +# rk1 = 2^(32*3) mod Q << 32
 +# rk2 = 2^(32*5) mod Q << 32
 +# rk3 = 2^(32*15) mod Q << 32
 +# rk4 = 2^(32*17) mod Q << 32
 +# rk5 = 2^(32*3) mod Q << 32
 +# rk6 = 2^(32*2) mod Q << 32
 +# rk7 = floor(2^64/Q)
 +# rk8 = Q
 +rk1:
 +.quad 0x2d56000000000000
 +rk2:
 +.quad 0x06df000000000000
 +rk3:
 +.quad 0x9d9d000000000000
 +rk4:
 +.quad 0x7cf5000000000000
 +rk5:
 +.quad 0x2d56000000000000
 +rk6:
 +.quad 0x1368000000000000
 +rk7:
 +.quad 0x00000001f65a57f8
 +rk8:
 +.quad 0x000000018bb70000
 +
 +rk9:
 +.quad 0xceae000000000000
 +rk10:
 +.quad 0xbfd6000000000000
 +rk11:
 +.quad 0x1e16000000000000
 +rk12:
 +.quad 0x713c000000000000
 +rk13:
 +.quad 0xf7f9000000000000
 +rk14:
 +.quad 0x80a6000000000000
 +rk15:
 +.quad 0x044c000000000000
 +rk16:
 +.quad 0xe658000000000000
 +rk17:
 +.quad 0xad18000000000000
 +rk18:
 +.quad 0xa497000000000000
 +rk19:
 +.quad 0x6ee3000000000000
 +rk20:
 +.quad 0xe7b5000000000000
 +
 +
 +
 +mask1:
 +.octa 0x80808080808080808080808080808080
 +mask2:
 +.octa 0x00000000FFFFFFFFFFFFFFFFFFFFFFFF
 +
 +SHUF_MASK:
 +.octa 0x000102030405060708090A0B0C0D0E0F
 +
 +pshufb_shf_table:
 +# use these values for shift constants for the pshufb instruction
 +# different alignments result in values as shown:
 +#	DDQ 0x008f8e8d8c8b8a898887868584838281 # shl 15 (16-1) / shr1
 +#	DDQ 0x01008f8e8d8c8b8a8988878685848382 # shl 14 (16-3) / shr2
 +#	DDQ 0x0201008f8e8d8c8b8a89888786858483 # shl 13 (16-4) / shr3
 +#	DDQ 0x030201008f8e8d8c8b8a898887868584 # shl 12 (16-4) / shr4
 +#	DDQ 0x04030201008f8e8d8c8b8a8988878685 # shl 11 (16-5) / shr5
 +#	DDQ 0x0504030201008f8e8d8c8b8a89888786 # shl 10 (16-6) / shr6
 +#	DDQ 0x060504030201008f8e8d8c8b8a898887 # shl 9  (16-7) / shr7
 +#	DDQ 0x07060504030201008f8e8d8c8b8a8988 # shl 8  (16-8) / shr8
 +#	DDQ 0x0807060504030201008f8e8d8c8b8a89 # shl 7  (16-9) / shr9
 +#	DDQ 0x090807060504030201008f8e8d8c8b8a # shl 6  (16-10) / shr10
 +#	DDQ 0x0a090807060504030201008f8e8d8c8b # shl 5  (16-11) / shr11
 +#	DDQ 0x0b0a090807060504030201008f8e8d8c # shl 4  (16-12) / shr12
 +#	DDQ 0x0c0b0a090807060504030201008f8e8d # shl 3  (16-13) / shr13
 +#	DDQ 0x0d0c0b0a090807060504030201008f8e # shl 2  (16-14) / shr14
 +#	DDQ 0x0e0d0c0b0a090807060504030201008f # shl 1  (16-15) / shr15
 +.octa 0x8f8e8d8c8b8a89888786858483828100
 +.octa 0x000e0d0c0b0a09080706050403020100
diff --cc arch/x86/crypto/ghash-clmulni-intel_asm.S
index 88ef500be1f0,2bf871899920..000000000000
--- a/arch/x86/crypto/ghash-clmulni-intel_asm.S
+++ b/arch/x86/crypto/ghash-clmulni-intel_asm.S
@@@ -94,22 -85,22 +94,32 @@@ __clmul_gf128mul_ble
  	psrlq $1, T2
  	pxor T2, T1
  	pxor T1, DATA
++<<<<<<< HEAD
 +	ret
 +ENDPROC(__clmul_gf128mul_ble)
++=======
+ 	RET
+ SYM_FUNC_END(__clmul_gf128mul_ble)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -/* void clmul_ghash_mul(char *dst, const u128 *shash) */
 -SYM_FUNC_START(clmul_ghash_mul)
 +/* void clmul_ghash_mul(char *dst, const be128 *shash) */
 +ENTRY(clmul_ghash_mul)
  	FRAME_BEGIN
  	movups (%rdi), DATA
  	movups (%rsi), SHASH
  	movaps .Lbswap_mask, BSWAP
 -	pshufb BSWAP, DATA
 +	PSHUFB_XMM BSWAP DATA
  	call __clmul_gf128mul_ble
 -	pshufb BSWAP, DATA
 +	PSHUFB_XMM BSWAP DATA
  	movups DATA, (%rdi)
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(clmul_ghash_mul)
++=======
+ 	RET
+ SYM_FUNC_END(clmul_ghash_mul)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * void clmul_ghash_update(char *dst, const char *src, unsigned int srclen,
@@@ -137,30 -128,5 +147,35 @@@ ENTRY(clmul_ghash_update
  	movups DATA, (%rdi)
  .Lupdate_just_ret:
  	FRAME_END
++<<<<<<< HEAD
 +	ret
 +ENDPROC(clmul_ghash_update)
 +
 +/*
 + * void clmul_ghash_setkey(be128 *shash, const u8 *key);
 + *
 + * Calculate hash_key << 1 mod poly
 + */
 +ENTRY(clmul_ghash_setkey)
 +	movaps .Lbswap_mask, BSWAP
 +	movups (%rsi), %xmm0
 +	PSHUFB_XMM BSWAP %xmm0
 +	movaps %xmm0, %xmm1
 +	psllq $1, %xmm0
 +	psrlq $63, %xmm1
 +	movaps %xmm1, %xmm2
 +	pslldq $8, %xmm1
 +	psrldq $8, %xmm2
 +	por %xmm1, %xmm0
 +	# reduction
 +	pshufd $0b00100100, %xmm2, %xmm1
 +	pcmpeqd .Ltwo_one, %xmm1
 +	pand .Lpoly, %xmm1
 +	pxor %xmm1, %xmm0
 +	movups %xmm0, (%rdi)
 +	ret
 +ENDPROC(clmul_ghash_setkey)
++=======
+ 	RET
+ SYM_FUNC_END(clmul_ghash_update)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/serpent-avx-x86_64-asm_64.S
index 8be571808342,82f2313f512b..000000000000
--- a/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-avx-x86_64-asm_64.S
@@@ -619,11 -601,11 +619,16 @@@ __serpent_enc_blk8_avx
  	write_blocks(RA1, RB1, RC1, RD1, RK0, RK1, RK2);
  	write_blocks(RA2, RB2, RC2, RD2, RK0, RK1, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__serpent_enc_blk8_avx)
++=======
+ 	RET;
+ SYM_FUNC_END(__serpent_enc_blk8_avx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .align 8
 -SYM_FUNC_START_LOCAL(__serpent_dec_blk8_avx)
 +__serpent_dec_blk8_avx:
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2: encrypted blocks
@@@ -673,10 -655,10 +678,15 @@@
  	write_blocks(RC1, RD1, RB1, RE1, RK0, RK1, RK2);
  	write_blocks(RC2, RD2, RB2, RE2, RK0, RK1, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__serpent_dec_blk8_avx)
++=======
+ 	RET;
+ SYM_FUNC_END(__serpent_dec_blk8_avx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_ecb_enc_8way_avx)
 +ENTRY(serpent_ecb_enc_8way_avx)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -691,10 -673,10 +701,15 @@@
  	store_8way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_ecb_enc_8way_avx)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_ecb_enc_8way_avx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_ecb_dec_8way_avx)
 +ENTRY(serpent_ecb_dec_8way_avx)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -709,10 -691,10 +724,15 @@@
  	store_8way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_ecb_dec_8way_avx)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_ecb_dec_8way_avx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_cbc_dec_8way_avx)
 +ENTRY(serpent_cbc_dec_8way_avx)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -727,69 -709,5 +747,74 @@@
  	store_cbc_8way(%rdx, %rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_cbc_dec_8way_avx)
 +
 +ENTRY(serpent_ctr_8way_avx)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (little endian, 128bit)
 +	 */
 +	FRAME_BEGIN
 +
 +	load_ctr_8way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 +		      RD2, RK0, RK1, RK2);
 +
 +	call __serpent_enc_blk8_avx;
 +
 +	store_ctr_8way(%rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(serpent_ctr_8way_avx)
 +
 +ENTRY(serpent_xts_enc_8way_avx)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	/* regs <= src, dst <= IVs, regs <= regs xor IVs */
 +	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 +		      RK0, RK1, RK2, .Lxts_gf128mul_and_shl1_mask);
 +
 +	call __serpent_enc_blk8_avx;
 +
 +	/* dst <= regs xor IVs(in dst) */
 +	store_xts_8way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(serpent_xts_enc_8way_avx)
 +
 +ENTRY(serpent_xts_dec_8way_avx)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	/* regs <= src, dst <= IVs, regs <= regs xor IVs */
 +	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 +		      RK0, RK1, RK2, .Lxts_gf128mul_and_shl1_mask);
 +
 +	call __serpent_dec_blk8_avx;
 +
 +	/* dst <= regs xor IVs(in dst) */
 +	store_xts_8way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(serpent_xts_dec_8way_avx)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_cbc_dec_8way_avx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/serpent-avx2-asm_64.S
index 97c48add33ed,8ea34c9b9316..000000000000
--- a/arch/x86/crypto/serpent-avx2-asm_64.S
+++ b/arch/x86/crypto/serpent-avx2-asm_64.S
@@@ -611,11 -601,11 +611,16 @@@ __serpent_enc_blk16
  	write_blocks(RA1, RB1, RC1, RD1, RK0, RK1, RK2);
  	write_blocks(RA2, RB2, RC2, RD2, RK0, RK1, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__serpent_enc_blk16)
++=======
+ 	RET;
+ SYM_FUNC_END(__serpent_enc_blk16)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .align 8
 -SYM_FUNC_START_LOCAL(__serpent_dec_blk16)
 +__serpent_dec_blk16:
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2: ciphertext
@@@ -665,10 -655,10 +670,15 @@@
  	write_blocks(RC1, RD1, RB1, RE1, RK0, RK1, RK2);
  	write_blocks(RC2, RD2, RB2, RE2, RK0, RK1, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__serpent_dec_blk16)
++=======
+ 	RET;
+ SYM_FUNC_END(__serpent_dec_blk16)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_ecb_enc_16way)
 +ENTRY(serpent_ecb_enc_16way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -687,10 -677,10 +697,15 @@@
  	vzeroupper;
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_ecb_enc_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_ecb_enc_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_ecb_dec_16way)
 +ENTRY(serpent_ecb_dec_16way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -709,10 -699,10 +724,15 @@@
  	vzeroupper;
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_ecb_dec_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_ecb_dec_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_cbc_dec_16way)
 +ENTRY(serpent_cbc_dec_16way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -732,82 -722,5 +752,87 @@@
  	vzeroupper;
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_cbc_dec_16way)
 +
 +ENTRY(serpent_ctr_16way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (16 blocks)
 +	 *	%rdx: src (16 blocks)
 +	 *	%rcx: iv (little endian, 128bit)
 +	 */
 +	FRAME_BEGIN
 +
 +	vzeroupper;
 +
 +	load_ctr_16way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 +		       RD2, RK0, RK0x, RK1, RK1x, RK2, RK2x, RK3, RK3x, RNOT,
 +		       tp);
 +
 +	call __serpent_enc_blk16;
 +
 +	store_ctr_16way(%rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
 +	vzeroupper;
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(serpent_ctr_16way)
 +
 +ENTRY(serpent_xts_enc_16way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (16 blocks)
 +	 *	%rdx: src (16 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	vzeroupper;
 +
 +	load_xts_16way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 +		       RD2, RK0, RK0x, RK1, RK1x, RK2, RK2x, RK3, RK3x, RNOT,
 +		       .Lxts_gf128mul_and_shl1_mask_0,
 +		       .Lxts_gf128mul_and_shl1_mask_1);
 +
 +	call __serpent_enc_blk16;
 +
 +	store_xts_16way(%rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
 +	vzeroupper;
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(serpent_xts_enc_16way)
 +
 +ENTRY(serpent_xts_dec_16way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst (16 blocks)
 +	 *	%rdx: src (16 blocks)
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	vzeroupper;
 +
 +	load_xts_16way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 +		       RD2, RK0, RK0x, RK1, RK1x, RK2, RK2x, RK3, RK3x, RNOT,
 +		       .Lxts_gf128mul_and_shl1_mask_0,
 +		       .Lxts_gf128mul_and_shl1_mask_1);
 +
 +	call __serpent_dec_blk16;
 +
 +	store_xts_16way(%rsi, RC1, RD1, RB1, RE1, RC2, RD2, RB2, RE2);
 +
 +	vzeroupper;
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(serpent_xts_dec_16way)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_cbc_dec_16way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/serpent-sse2-i586-asm_32.S
index d348f1553a79,8ccb03ad7cef..000000000000
--- a/arch/x86/crypto/serpent-sse2-i586-asm_32.S
+++ b/arch/x86/crypto/serpent-sse2-i586-asm_32.S
@@@ -573,10 -558,10 +573,15 @@@ ENTRY(__serpent_enc_blk_4way
  .L__enc_xor4:
  	xor_blocks(%eax, RA, RB, RC, RD, RT0, RT1, RE);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__serpent_enc_blk_4way)
++=======
+ 	RET;
+ SYM_FUNC_END(__serpent_enc_blk_4way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_dec_blk_4way)
 +ENTRY(serpent_dec_blk_4way)
  	/* input:
  	 *	arg_ctx(%esp): ctx, CTX
  	 *	arg_dst(%esp): dst
@@@ -627,5 -612,5 +632,10 @@@
  	movl arg_dst(%esp), %eax;
  	write_blocks(%eax, RC, RD, RB, RE, RT0, RT1, RA);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_dec_blk_4way)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_dec_blk_4way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
index acc066c7c6b2,e0998a011d1d..000000000000
--- a/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
+++ b/arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
@@@ -696,10 -681,10 +696,15 @@@ ENTRY(__serpent_enc_blk_8way
  	xor_blocks(%rsi, RA1, RB1, RC1, RD1, RK0, RK1, RK2);
  	xor_blocks(%rax, RA2, RB2, RC2, RD2, RK0, RK1, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__serpent_enc_blk_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(__serpent_enc_blk_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(serpent_dec_blk_8way)
 +ENTRY(serpent_dec_blk_8way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -750,5 -735,5 +755,10 @@@
  	write_blocks(%rsi, RC1, RD1, RB1, RE1, RK0, RK1, RK2);
  	write_blocks(%rax, RC2, RD2, RB2, RE2, RK0, RK1, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(serpent_dec_blk_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(serpent_dec_blk_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/sha1_ni_asm.S
index 874a651b9e7d,2f94ec0e763b..000000000000
--- a/arch/x86/crypto/sha1_ni_asm.S
+++ b/arch/x86/crypto/sha1_ni_asm.S
@@@ -288,15 -287,18 +288,20 @@@ ENTRY(sha1_ni_transform
  	pextrd		$3, E0, 1*16(DIGEST_PTR)
  
  .Ldone_hash:
 -	mov		%rbp, %rsp
 -	pop		%rbp
 +	mov		RSPSAVE, %rsp
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sha1_ni_transform)
++=======
+ 	RET
+ SYM_FUNC_END(sha1_ni_transform)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +.data
  
 -.section	.rodata.cst16.PSHUFFLE_BYTE_FLIP_MASK, "aM", @progbits, 16
 -.align 16
 +.align 64
  PSHUFFLE_BYTE_FLIP_MASK:
  	.octa 0x000102030405060708090a0b0c0d0e0f
 -
 -.section	.rodata.cst16.UPPER_WORD_MASK, "aM", @progbits, 16
 -.align 16
  UPPER_WORD_MASK:
  	.octa 0xFFFFFFFF000000000000000000000000
diff --cc arch/x86/crypto/sha256-avx-asm.S
index 92b3b5d75ba9,3baa1ec39097..000000000000
--- a/arch/x86/crypto/sha256-avx-asm.S
+++ b/arch/x86/crypto/sha256-avx-asm.S
@@@ -458,12 -456,12 +458,17 @@@ done_hash
  	popq    %r15
  	popq    %r14
  	popq    %r13
 -	popq	%r12
 +	popq    %rbp
  	popq    %rbx
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sha256_transform_avx)
++=======
+ 	RET
+ SYM_FUNC_END(sha256_transform_avx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -.section	.rodata.cst256.K256, "aM", @progbits, 256
 +.data
  .align 64
  K256:
  	.long 0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
diff --cc arch/x86/crypto/sha256-avx2-asm.S
index 570ec5ec62d7,9bcdbc47b8b4..000000000000
--- a/arch/x86/crypto/sha256-avx2-asm.S
+++ b/arch/x86/crypto/sha256-avx2-asm.S
@@@ -718,12 -709,11 +718,17 @@@ done_hash
  	popq	%r14
  	popq	%r13
  	popq	%r12
 +	popq	%rbp
  	popq	%rbx
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sha256_transform_rorx)
++=======
+ 	RET
+ SYM_FUNC_END(sha256_transform_rorx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -.section	.rodata.cst512.K256, "aM", @progbits, 512
 +.data
  .align 64
  K256:
  	.long	0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
diff --cc arch/x86/crypto/sha256-ssse3-asm.S
index 2cedc44e8121,c4a5db612c32..000000000000
--- a/arch/x86/crypto/sha256-ssse3-asm.S
+++ b/arch/x86/crypto/sha256-ssse3-asm.S
@@@ -468,13 -469,13 +468,18 @@@ done_hash
  	popq    %r15
  	popq    %r14
  	popq    %r13
 -	popq    %r12
 +	popq    %rbp
  	popq    %rbx
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sha256_transform_ssse3)
++=======
+ 	RET
+ SYM_FUNC_END(sha256_transform_ssse3)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -.section	.rodata.cst256.K256, "aM", @progbits, 256
 +.data
  .align 64
  K256:
          .long 0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
diff --cc arch/x86/crypto/sha256_ni_asm.S
index 748cdf21a938,94d50dd27cb5..000000000000
--- a/arch/x86/crypto/sha256_ni_asm.S
+++ b/arch/x86/crypto/sha256_ni_asm.S
@@@ -326,10 -326,10 +326,15 @@@ ENTRY(sha256_ni_transform
  
  .Ldone_hash:
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sha256_ni_transform)
++=======
+ 	RET
+ SYM_FUNC_END(sha256_ni_transform)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -.section	.rodata.cst256.K256, "aM", @progbits, 256
 +.data
  .align 64
  K256:
  	.long	0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
diff --cc arch/x86/crypto/sha512-avx-asm.S
index 565274d6a641,1fefe6dd3a9e..000000000000
--- a/arch/x86/crypto/sha512-avx-asm.S
+++ b/arch/x86/crypto/sha512-avx-asm.S
@@@ -353,19 -349,20 +353,24 @@@ updateblock
  	dec     msglen
  	jnz     updateblock
  
 -	# Restore Stack Pointer
 -	mov	%rbp, %rsp
 -	pop	%rbp
 -
  	# Restore GPRs
 -	pop	%r15
 -	pop	%r14
 -	pop	%r13
 -	pop	%r12
 -	pop	%rbx
 +	mov     frame_GPRSAVE(%rsp),      %rbx
 +	mov     frame_GPRSAVE +8*1(%rsp), %r12
 +	mov     frame_GPRSAVE +8*2(%rsp), %r13
 +	mov     frame_GPRSAVE +8*3(%rsp), %r14
 +	mov     frame_GPRSAVE +8*4(%rsp), %r15
 +
 +	# Restore Stack Pointer
 +	mov	frame_RSPSAVE(%rsp), %rsp
  
  nowork:
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sha512_transform_avx)
++=======
+ 	RET
+ SYM_FUNC_END(sha512_transform_avx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  ########################################################################
  ### Binary Data
diff --cc arch/x86/crypto/sha512-avx2-asm.S
index 1f20b35d8573,5cdaab7d6901..000000000000
--- a/arch/x86/crypto/sha512-avx2-asm.S
+++ b/arch/x86/crypto/sha512-avx2-asm.S
@@@ -668,18 -668,19 +668,33 @@@ loop2
  
  done_hash:
  
 +# Restore GPRs
 +	mov	frame_GPRSAVE(%rsp)     ,%rbp
 +	mov	8*1+frame_GPRSAVE(%rsp) ,%rbx
 +	mov	8*2+frame_GPRSAVE(%rsp) ,%r12
 +	mov	8*3+frame_GPRSAVE(%rsp) ,%r13
 +	mov	8*4+frame_GPRSAVE(%rsp) ,%r14
 +	mov	8*5+frame_GPRSAVE(%rsp) ,%r15
 +
  	# Restore Stack Pointer
++<<<<<<< HEAD
 +	mov	frame_RSPSAVE(%rsp), %rsp
 +	ret
 +ENDPROC(sha512_transform_rorx)
++=======
+ 	mov	%rbp, %rsp
+ 	pop	%rbp
+ 
+ 	# Restore GPRs
+ 	pop	%r15
+ 	pop	%r14
+ 	pop	%r13
+ 	pop	%r12
+ 	pop	%rbx
+ 
+ 	RET
+ SYM_FUNC_END(sha512_transform_rorx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  ########################################################################
  ### Binary Data
diff --cc arch/x86/crypto/sha512-ssse3-asm.S
index e610e29cbc81,b84c22e06c5f..000000000000
--- a/arch/x86/crypto/sha512-ssse3-asm.S
+++ b/arch/x86/crypto/sha512-ssse3-asm.S
@@@ -352,19 -351,20 +352,24 @@@ updateblock
  	dec	msglen
  	jnz	updateblock
  
 -	# Restore Stack Pointer
 -	mov	%rbp, %rsp
 -	pop	%rbp
 -
  	# Restore GPRs
 -	pop	%r15
 -	pop	%r14
 -	pop	%r13
 -	pop	%r12
 -	pop	%rbx
 +	mov	frame_GPRSAVE(%rsp),      %rbx
 +	mov	frame_GPRSAVE +8*1(%rsp), %r12
 +	mov	frame_GPRSAVE +8*2(%rsp), %r13
 +	mov	frame_GPRSAVE +8*3(%rsp), %r14
 +	mov	frame_GPRSAVE +8*4(%rsp), %r15
 +
 +	# Restore Stack Pointer
 +	mov	frame_RSPSAVE(%rsp), %rsp
  
  nowork:
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sha512_transform_ssse3)
++=======
+ 	RET
+ SYM_FUNC_END(sha512_transform_ssse3)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  ########################################################################
  ### Binary Data
diff --cc arch/x86/crypto/twofish-avx-x86_64-asm_64.S
index dc66273e610d,31f9b2ec3857..000000000000
--- a/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-avx-x86_64-asm_64.S
@@@ -285,11 -267,11 +285,16 @@@ __twofish_enc_blk8
  	outunpack_blocks(RC1, RD1, RA1, RB1, RK1, RX0, RY0, RK2);
  	outunpack_blocks(RC2, RD2, RA2, RB2, RK1, RX0, RY0, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__twofish_enc_blk8)
++=======
+ 	RET;
+ SYM_FUNC_END(__twofish_enc_blk8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .align 8
 -SYM_FUNC_START_LOCAL(__twofish_dec_blk8)
 +__twofish_dec_blk8:
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2: encrypted blocks
@@@ -325,10 -307,10 +330,15 @@@
  	outunpack_blocks(RA1, RB1, RC1, RD1, RK1, RX0, RY0, RK2);
  	outunpack_blocks(RA2, RB2, RC2, RD2, RK1, RX0, RY0, RK2);
  
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(__twofish_dec_blk8)
++=======
+ 	RET;
+ SYM_FUNC_END(__twofish_dec_blk8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(twofish_ecb_enc_8way)
 +ENTRY(twofish_ecb_enc_8way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -345,10 -327,10 +355,15 @@@
  	store_8way(%r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(twofish_ecb_enc_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(twofish_ecb_enc_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(twofish_ecb_dec_8way)
 +ENTRY(twofish_ecb_dec_8way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -365,10 -347,10 +380,15 @@@
  	store_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(twofish_ecb_dec_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(twofish_ecb_dec_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(twofish_cbc_dec_8way)
 +ENTRY(twofish_cbc_dec_8way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -390,80 -372,5 +410,85 @@@
  	popq %r12;
  
  	FRAME_END
++<<<<<<< HEAD
 +	ret;
 +ENDPROC(twofish_cbc_dec_8way)
 +
 +ENTRY(twofish_ctr_8way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (little endian, 128bit)
 +	 */
 +	FRAME_BEGIN
 +
 +	pushq %r12;
 +
 +	movq %rsi, %r11;
 +	movq %rdx, %r12;
 +
 +	load_ctr_8way(%rcx, .Lbswap128_mask, RA1, RB1, RC1, RD1, RA2, RB2, RC2,
 +		      RD2, RX0, RX1, RY0);
 +
 +	call __twofish_enc_blk8;
 +
 +	store_ctr_8way(%r12, %r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 +
 +	popq %r12;
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(twofish_ctr_8way)
 +
 +ENTRY(twofish_xts_enc_8way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	movq %rsi, %r11;
 +
 +	/* regs <= src, dst <= IVs, regs <= regs xor IVs */
 +	load_xts_8way(%rcx, %rdx, %rsi, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2,
 +		      RX0, RX1, RY0, .Lxts_gf128mul_and_shl1_mask);
 +
 +	call __twofish_enc_blk8;
 +
 +	/* dst <= regs xor IVs(in dst) */
 +	store_xts_8way(%r11, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(twofish_xts_enc_8way)
 +
 +ENTRY(twofish_xts_dec_8way)
 +	/* input:
 +	 *	%rdi: ctx, CTX
 +	 *	%rsi: dst
 +	 *	%rdx: src
 +	 *	%rcx: iv (t    GF(2))
 +	 */
 +	FRAME_BEGIN
 +
 +	movq %rsi, %r11;
 +
 +	/* regs <= src, dst <= IVs, regs <= regs xor IVs */
 +	load_xts_8way(%rcx, %rdx, %rsi, RC1, RD1, RA1, RB1, RC2, RD2, RA2, RB2,
 +		      RX0, RX1, RY0, .Lxts_gf128mul_and_shl1_mask);
 +
 +	call __twofish_dec_blk8;
 +
 +	/* dst <= regs xor IVs(in dst) */
 +	store_xts_8way(%r11, RA1, RB1, RC1, RD1, RA2, RB2, RC2, RD2);
 +
 +	FRAME_END
 +	ret;
 +ENDPROC(twofish_xts_dec_8way)
++=======
+ 	RET;
+ SYM_FUNC_END(twofish_cbc_dec_8way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/twofish-i586-asm_32.S
index 694ea4587ba7,3abcad661884..000000000000
--- a/arch/x86/crypto/twofish-i586-asm_32.S
+++ b/arch/x86/crypto/twofish-i586-asm_32.S
@@@ -273,10 -260,10 +273,15 @@@ ENTRY(twofish_enc_blk
  	pop	%ebx
  	pop	%ebp
  	mov	$1,	%eax
++<<<<<<< HEAD
 +	ret
 +ENDPROC(twofish_enc_blk)
++=======
+ 	RET
+ SYM_FUNC_END(twofish_enc_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(twofish_dec_blk)
 +ENTRY(twofish_dec_blk)
  	push	%ebp			/* save registers according to calling convention*/
  	push    %ebx
  	push    %esi
@@@ -330,5 -317,5 +335,10 @@@
  	pop	%ebx
  	pop	%ebp
  	mov	$1,	%eax
++<<<<<<< HEAD
 +	ret
 +ENDPROC(twofish_dec_blk)
++=======
+ 	RET
+ SYM_FUNC_END(twofish_dec_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/twofish-x86_64-asm_64-3way.S
index 1c3b7ceb36d2,d2288bf38a8a..000000000000
--- a/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64-3way.S
@@@ -253,26 -256,20 +253,35 @@@ ENTRY(__twofish_enc_blk_3way
  	outunpack_enc3(mov);
  
  	popq %rbx;
 +	popq %rbp;
  	popq %r12;
  	popq %r13;
++<<<<<<< HEAD
 +	popq %r14;
 +	popq %r15;
 +	ret;
++=======
+ 	RET;
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .L__enc_xor3:
  	outunpack_enc3(xor);
  
  	popq %rbx;
 +	popq %rbp;
  	popq %r12;
  	popq %r13;
++<<<<<<< HEAD
 +	popq %r14;
 +	popq %r15;
 +	ret;
 +ENDPROC(__twofish_enc_blk_3way)
++=======
+ 	RET;
+ SYM_FUNC_END(__twofish_enc_blk_3way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(twofish_dec_blk_3way)
 +ENTRY(twofish_dec_blk_3way)
  	/* input:
  	 *	%rdi: ctx, CTX
  	 *	%rsi: dst
@@@ -303,10 -299,7 +312,15 @@@
  	outunpack_dec3();
  
  	popq %rbx;
 +	popq %rbp;
  	popq %r12;
  	popq %r13;
++<<<<<<< HEAD
 +	popq %r14;
 +	popq %r15;
 +	ret;
 +ENDPROC(twofish_dec_blk_3way)
++=======
+ 	RET;
+ SYM_FUNC_END(twofish_dec_blk_3way)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/crypto/twofish-x86_64-asm_64.S
index a039d21986a2,775af290cd19..000000000000
--- a/arch/x86/crypto/twofish-x86_64-asm_64.S
+++ b/arch/x86/crypto/twofish-x86_64-asm_64.S
@@@ -264,11 -251,11 +264,17 @@@ ENTRY(twofish_enc_blk
  	movq	R1,	8(%rsi)
  
  	popq	R1
++<<<<<<< HEAD
 +	movq	$1,%rax
 +	ret
 +ENDPROC(twofish_enc_blk)
++=======
+ 	movl	$1,%eax
+ 	RET
+ SYM_FUNC_END(twofish_enc_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(twofish_dec_blk)
 +ENTRY(twofish_dec_blk)
  	pushq    R1
  
  	/* %rdi contains the ctx address */
@@@ -316,6 -303,6 +322,12 @@@
  	movq	R1,	8(%rsi)
  
  	popq	R1
++<<<<<<< HEAD
 +	movq	$1,%rax
 +	ret
 +ENDPROC(twofish_dec_blk)
++=======
+ 	movl	$1,%eax
+ 	RET
+ SYM_FUNC_END(twofish_dec_blk)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/kernel/acpi/wakeup_32.S
index 665c6b7d2ea9,cf69081073b5..000000000000
--- a/arch/x86/kernel/acpi/wakeup_32.S
+++ b/arch/x86/kernel/acpi/wakeup_32.S
@@@ -67,9 -70,9 +67,9 @@@ restore_registers
  	movl	saved_context_edi, %edi
  	pushl	saved_context_eflags
  	popfl
- 	ret
+ 	RET
  
 -SYM_CODE_START(do_suspend_lowlevel)
 +ENTRY(do_suspend_lowlevel)
  	call	save_processor_state
  	call	save_registers
  	pushl	$3
@@@ -83,7 -86,8 +83,12 @@@
  ret_point:
  	call	restore_registers
  	call	restore_processor_state
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(do_suspend_lowlevel)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .data
  ALIGN
diff --cc arch/x86/kernel/relocate_kernel_32.S
index 36818f8ec2be,fcc8a7699103..000000000000
--- a/arch/x86/kernel/relocate_kernel_32.S
+++ b/arch/x86/kernel/relocate_kernel_32.S
@@@ -94,9 -91,10 +94,14 @@@ relocate_kernel
  	movl    %edi, %eax
  	addl    $(identity_mapped - relocate_kernel), %eax
  	pushl   %eax
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(relocate_kernel)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
 +identity_mapped:
  	/* set return address to 0 if not preserving context */
  	pushl	$0
  	/* store the start address on the stack */
@@@ -192,9 -190,10 +197,14 @@@
  	movl	%edi, %eax
  	addl	$(virtual_mapped - relocate_kernel), %eax
  	pushl	%eax
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(identity_mapped)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_CODE_START_LOCAL_NOALIGN(virtual_mapped)
 +virtual_mapped:
  	movl	CR4(%edi), %eax
  	movl	%eax, %cr4
  	movl	CR3(%edi), %eax
@@@ -209,10 -208,11 +219,15 @@@
  	popl	%edi
  	popl	%esi
  	popl	%ebx
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(virtual_mapped)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	/* Do the copies */
 -SYM_CODE_START_LOCAL_NOALIGN(swap_pages)
 +swap_pages:
  	movl	8(%esp), %edx
  	movl	4(%esp), %ecx
  	pushl	%ebp
@@@ -271,7 -271,8 +286,12 @@@
  	popl	%edi
  	popl	%ebx
  	popl	%ebp
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(swap_pages)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	.globl kexec_control_code_size
  .set kexec_control_code_size, . - relocate_kernel
diff --cc arch/x86/kernel/relocate_kernel_64.S
index a59dd47921a2,399f075ccdc4..000000000000
--- a/arch/x86/kernel/relocate_kernel_64.S
+++ b/arch/x86/kernel/relocate_kernel_64.S
@@@ -101,9 -104,11 +101,14 @@@ relocate_kernel
  	/* jump to identity mapped page */
  	addq	$(identity_mapped - relocate_kernel), %r8
  	pushq	%r8
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(relocate_kernel)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_CODE_START_LOCAL_NOALIGN(identity_mapped)
 -	UNWIND_HINT_EMPTY
 +identity_mapped:
  	/* set return address to 0 if not preserving context */
  	pushq	$0
  	/* store the start address on the stack */
@@@ -165,23 -175,23 +170,23 @@@
  
  	testq	%r11, %r11
  	jnz 1f
 -	xorl	%eax, %eax
 -	xorl	%ebx, %ebx
 -	xorl    %ecx, %ecx
 -	xorl    %edx, %edx
 -	xorl    %esi, %esi
 -	xorl    %edi, %edi
 -	xorl    %ebp, %ebp
 -	xorl	%r8d, %r8d
 -	xorl	%r9d, %r9d
 -	xorl	%r10d, %r10d
 -	xorl	%r11d, %r11d
 -	xorl	%r12d, %r12d
 -	xorl	%r13d, %r13d
 -	xorl	%r14d, %r14d
 -	xorl	%r15d, %r15d
 +	xorq	%rax, %rax
 +	xorq	%rbx, %rbx
 +	xorq    %rcx, %rcx
 +	xorq    %rdx, %rdx
 +	xorq    %rsi, %rsi
 +	xorq    %rdi, %rdi
 +	xorq    %rbp, %rbp
 +	xorq	%r8,  %r8
 +	xorq	%r9,  %r9
 +	xorq	%r10, %r10
 +	xorq	%r11, %r11
 +	xorq	%r12, %r12
 +	xorq	%r13, %r13
 +	xorq	%r14, %r14
 +	xorq	%r15, %r15
  
- 	ret
+ 	RET
  
  1:
  	popq	%rdx
@@@ -202,9 -210,11 +207,14 @@@
  	call	swap_pages
  	movq	$virtual_mapped, %rax
  	pushq	%rax
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(identity_mapped)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_CODE_START_LOCAL_NOALIGN(virtual_mapped)
 -	UNWIND_HINT_EMPTY
 +virtual_mapped:
  	movq	RSP(%r8), %rsp
  	movq	CR4(%r8), %rax
  	movq	%rax, %cr4
@@@ -221,13 -231,15 +231,18 @@@
  	popq	%r12
  	popq	%rbp
  	popq	%rbx
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(virtual_mapped)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	/* Do the copies */
 -SYM_CODE_START_LOCAL_NOALIGN(swap_pages)
 -	UNWIND_HINT_EMPTY
 +swap_pages:
  	movq	%rdi, %rcx 	/* Put the page_list in %rcx */
 -	xorl	%edi, %edi
 -	xorl	%esi, %esi
 +	xorq	%rdi, %rdi
 +	xorq	%rsi, %rsi
  	jmp	1f
  
  0:	/* top, read another word for the indirection page */
@@@ -276,7 -288,8 +291,12 @@@
  	lea	PAGE_SIZE(%rax), %rsi
  	jmp	0b
  3:
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(swap_pages)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	.globl kexec_control_code_size
  .set kexec_control_code_size, . - relocate_kernel
diff --cc arch/x86/kernel/verify_cpu.S
index b7c9db5deebe,1258a5872d12..000000000000
--- a/arch/x86/kernel/verify_cpu.S
+++ b/arch/x86/kernel/verify_cpu.S
@@@ -129,13 -127,14 +129,22 @@@ verify_cpu_sse_test
  	btr	$15,%eax		# enable SSE
  	wrmsr
  	xor	%di,%di			# don't loop
 -	jmp	.Lverify_cpu_sse_test	# try again
 +	jmp	verify_cpu_sse_test	# try again
  
 -.Lverify_cpu_no_longmode:
 +verify_cpu_no_longmode:
  	popf				# Restore caller passed flags
  	movl $1,%eax
++<<<<<<< HEAD
 +	ret
 +verify_cpu_sse_ok:
 +	popf				# Restore caller passed flags
 +	xorl %eax, %eax
 +	ret
++=======
+ 	RET
+ .Lverify_cpu_sse_ok:
+ 	popf				# Restore caller passed flags
+ 	xorl %eax, %eax
+ 	RET
+ SYM_FUNC_END(verify_cpu)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/atomic64_386_32.S
index 00933d5e992f,e768815e58ae..000000000000
--- a/arch/x86/lib/atomic64_386_32.S
+++ b/arch/x86/lib/atomic64_386_32.S
@@@ -35,19 -28,16 +35,25 @@@ ENTRY(atomic64_##op##_386); 
  
  #define ENDP endp
  
++<<<<<<< HEAD
 +#define RET \
 +	UNLOCK v; \
 +	ret
++=======
+ #define RET_IRQ_RESTORE \
+ 	IRQ_RESTORE v; \
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +#define RET_ENDP \
 +	RET; \
 +	ENDP
  
  #define v %ecx
 -BEGIN_IRQ_SAVE(read)
 +BEGIN(read)
  	movl  (v), %eax
  	movl 4(v), %edx
 -	RET_IRQ_RESTORE
 -ENDP
 +RET_ENDP
  #undef v
  
  #define v %esi
diff --cc arch/x86/lib/atomic64_cx8_32.S
index f5cc9eb1d51b,90afb488b396..000000000000
--- a/arch/x86/lib/atomic64_cx8_32.S
+++ b/arch/x86/lib/atomic64_cx8_32.S
@@@ -31,46 -16,36 +31,61 @@@
  	cmpxchg8b (\reg)
  .endm
  
 -SYM_FUNC_START(atomic64_read_cx8)
 +ENTRY(atomic64_read_cx8)
 +	CFI_STARTPROC
 +
  	read64 %ecx
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(atomic64_read_cx8)
 +
 +ENTRY(atomic64_set_cx8)
 +	CFI_STARTPROC
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_read_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(atomic64_set_cx8)
  1:
  /* we don't need LOCK_PREFIX since aligned 64-bit writes
   * are atomic on 586 and newer */
  	cmpxchg8b (%esi)
  	jne 1b
  
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(atomic64_set_cx8)
 +
 +ENTRY(atomic64_xchg_cx8)
 +	CFI_STARTPROC
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_set_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(atomic64_xchg_cx8)
  1:
  	LOCK_PREFIX
  	cmpxchg8b (%esi)
  	jne 1b
  
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(atomic64_xchg_cx8)
++=======
+ 	RET
+ SYM_FUNC_END(atomic64_xchg_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .macro addsub_return func ins insc
 -SYM_FUNC_START(atomic64_\func\()_return_cx8)
 -	pushl %ebp
 -	pushl %ebx
 -	pushl %esi
 -	pushl %edi
 +ENTRY(atomic64_\func\()_return_cx8)
 +	CFI_STARTPROC
 +	SAVE ebp
 +	SAVE ebx
 +	SAVE esi
 +	SAVE edi
  
  	movl %eax, %esi
  	movl %edx, %edi
@@@ -89,13 -64,12 +104,22 @@@
  10:
  	movl %ebx, %eax
  	movl %ecx, %edx
++<<<<<<< HEAD
 +	RESTORE edi
 +	RESTORE esi
 +	RESTORE ebx
 +	RESTORE ebp
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(atomic64_\func\()_return_cx8)
++=======
+ 	popl %edi
+ 	popl %esi
+ 	popl %ebx
+ 	popl %ebp
+ 	RET
+ SYM_FUNC_END(atomic64_\func\()_return_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  .endm
  
  addsub_return add add adc
@@@ -119,10 -92,9 +143,16 @@@ ENTRY(atomic64_\func\()_return_cx8
  10:
  	movl %ebx, %eax
  	movl %ecx, %edx
++<<<<<<< HEAD
 +	RESTORE ebx
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(atomic64_\func\()_return_cx8)
++=======
+ 	popl %ebx
+ 	RET
+ SYM_FUNC_END(atomic64_\func\()_return_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  .endm
  
  incdec_return inc add adc
@@@ -146,18 -117,16 +176,24 @@@ ENTRY(atomic64_dec_if_positive_cx8
  2:
  	movl %ebx, %eax
  	movl %ecx, %edx
++<<<<<<< HEAD
 +	RESTORE ebx
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(atomic64_dec_if_positive_cx8)
++=======
+ 	popl %ebx
+ 	RET
+ SYM_FUNC_END(atomic64_dec_if_positive_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(atomic64_add_unless_cx8)
 -	pushl %ebp
 -	pushl %ebx
 +ENTRY(atomic64_add_unless_cx8)
 +	CFI_STARTPROC
 +	SAVE ebp
 +	SAVE ebx
  /* these just push these two parameters on the stack */
 -	pushl %edi
 -	pushl %ecx
 +	SAVE edi
 +	SAVE ecx
  
  	movl %eax, %ebp
  	movl %edx, %edi
@@@ -178,10 -147,9 +214,16 @@@
  	movl $1, %eax
  3:
  	addl $8, %esp
++<<<<<<< HEAD
 +	CFI_ADJUST_CFA_OFFSET -8
 +	RESTORE ebx
 +	RESTORE ebp
 +	ret
++=======
+ 	popl %ebx
+ 	popl %ebp
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  4:
  	cmpl %edx, 4(%esp)
  	jne 2b
@@@ -209,7 -175,6 +251,13 @@@ ENTRY(atomic64_inc_not_zero_cx8
  
  	movl $1, %eax
  3:
++<<<<<<< HEAD
 +	RESTORE ebx
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(atomic64_inc_not_zero_cx8)
++=======
+ 	popl %ebx
+ 	RET
+ SYM_FUNC_END(atomic64_inc_not_zero_cx8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/checksum_32.S
index e78b8eee6615,929ad1747dea..000000000000
--- a/arch/x86/lib/checksum_32.S
+++ b/arch/x86/lib/checksum_32.S
@@@ -131,13 -125,10 +131,20 @@@ ENTRY(csum_partial
  	jz 8f
  	roll $8, %eax
  8:
++<<<<<<< HEAD
 +	popl_cfi %ebx
 +	CFI_RESTORE ebx
 +	popl_cfi %esi
 +	CFI_RESTORE esi
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(csum_partial)
++=======
+ 	popl %ebx
+ 	popl %esi
+ 	RET
+ SYM_FUNC_END(csum_partial)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  #else
  
@@@ -255,15 -243,13 +262,22 @@@ ENTRY(csum_partial
  	jz 90f
  	roll $8, %eax
  90: 
++<<<<<<< HEAD
 +	popl_cfi %ebx
 +	CFI_RESTORE ebx
 +	popl_cfi %esi
 +	CFI_RESTORE esi
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(csum_partial)
++=======
+ 	popl %ebx
+ 	popl %esi
+ 	RET
+ SYM_FUNC_END(csum_partial)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  				
  #endif
 -EXPORT_SYMBOL(csum_partial)
  
  /*
  unsigned int csum_partial_copy_generic (const char *src, char *dst,
@@@ -412,16 -367,12 +426,25 @@@ DST(	movb %cl, (%edi)	
  
  .previous
  
++<<<<<<< HEAD
 +	popl_cfi %ebx
 +	CFI_RESTORE ebx
 +	popl_cfi %esi
 +	CFI_RESTORE esi
 +	popl_cfi %edi
 +	CFI_RESTORE edi
 +	popl_cfi %ecx			# equivalent to addl $4,%esp
 +	ret	
 +	CFI_ENDPROC
 +ENDPROC(csum_partial_copy_generic)
++=======
+ 	popl %ebx
+ 	popl %esi
+ 	popl %edi
+ 	popl %ecx			# equivalent to addl $4,%esp
+ 	RET
+ SYM_FUNC_END(csum_partial_copy_generic)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  #else
  
@@@ -506,15 -444,11 +529,23 @@@ DST(	movb %dl, (%edi)         
  	jmp  7b			
  .previous				
  
++<<<<<<< HEAD
 +	popl_cfi %esi
 +	CFI_RESTORE esi
 +	popl_cfi %edi
 +	CFI_RESTORE edi
 +	popl_cfi %ebx
 +	CFI_RESTORE ebx
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(csum_partial_copy_generic)
++=======
+ 	popl %esi
+ 	popl %edi
+ 	popl %ebx
+ 	RET
+ SYM_FUNC_END(csum_partial_copy_generic)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  				
  #undef ROUND
  #undef ROUND1		
diff --cc arch/x86/lib/clear_page_64.S
index 82767a28144a,fe59b8ac4fcc..000000000000
--- a/arch/x86/lib/clear_page_64.S
+++ b/arch/x86/lib/clear_page_64.S
@@@ -11,21 -17,11 +11,27 @@@ ENTRY(clear_page_c
  	movl $4096/8,%ecx
  	xorl %eax,%eax
  	rep stosq
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(clear_page_c)
++=======
+ 	RET
+ SYM_FUNC_END(clear_page_rep)
+ EXPORT_SYMBOL_GPL(clear_page_rep)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(clear_page_orig)
 +ENTRY(clear_page_c_e)
 +	CFI_STARTPROC
 +	movl $4096,%ecx
 +	xorl %eax,%eax
 +	rep stosb
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(clear_page_c_e)
 +
 +ENTRY(clear_page)
 +	CFI_STARTPROC
  	xorl   %eax,%eax
  	movl   $4096/64,%ecx
  	.p2align 4
@@@ -43,31 -39,14 +49,45 @@@
  	leaq	64(%rdi),%rdi
  	jnz	.Lloop
  	nop
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +.Lclear_page_end:
 +ENDPROC(clear_page)
 +
 +	/*
 +	 * Some CPUs support enhanced REP MOVSB/STOSB instructions.
 +	 * It is recommended to use this when possible.
 +	 * If enhanced REP MOVSB/STOSB is not available, try to use fast string.
 +	 * Otherwise, use original function.
 +	 *
 +	 */
 +
 +#include <asm/cpufeatures.h>
 +
 +	.section .altinstr_replacement,"ax"
 +1:	.byte 0xeb					/* jmp <disp8> */
 +	.byte (clear_page_c - clear_page) - (2f - 1b)	/* offset */
 +2:	.byte 0xeb					/* jmp <disp8> */
 +	.byte (clear_page_c_e - clear_page) - (3f - 2b)	/* offset */
 +3:
 +	.previous
 +	.section .altinstructions,"a"
 +	altinstruction_entry clear_page,1b,X86_FEATURE_REP_GOOD,\
 +			     .Lclear_page_end-clear_page, 2b-1b
 +	altinstruction_entry clear_page,2b,X86_FEATURE_ERMS,   \
 +			     .Lclear_page_end-clear_page,3b-2b
 +	.previous
++=======
+ 	RET
+ SYM_FUNC_END(clear_page_orig)
+ EXPORT_SYMBOL_GPL(clear_page_orig)
+ 
+ SYM_FUNC_START(clear_page_erms)
+ 	movl $4096,%ecx
+ 	xorl %eax,%eax
+ 	rep stosb
+ 	RET
+ SYM_FUNC_END(clear_page_erms)
+ EXPORT_SYMBOL_GPL(clear_page_erms)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/copy_page_64.S
index c6acbc3db5af,30ea644bf446..000000000000
--- a/arch/x86/lib/copy_page_64.S
+++ b/arch/x86/lib/copy_page_64.S
@@@ -1,31 -1,30 +1,37 @@@
  /* Written 2003 by Andi Kleen, based on a kernel by Evandro Menezes */
  
  #include <linux/linkage.h>
 -#include <asm/cpufeatures.h>
 -#include <asm/alternative.h>
 -#include <asm/export.h>
 +#include <asm/dwarf2.h>
 +#include <asm/alternative-asm.h>
  
 -/*
 - * Some CPUs run faster using the string copy instructions (sane microcode).
 - * It is also a lot simpler. Use this when possible. But, don't use streaming
 - * copy unless the CPU indicates X86_FEATURE_REP_GOOD. Could vary the
 - * prefetch distance based on SMP/UP.
 - */
  	ALIGN
 -SYM_FUNC_START(copy_page)
 -	ALTERNATIVE "jmp copy_page_regs", "", X86_FEATURE_REP_GOOD
 +copy_page_rep:
 +	CFI_STARTPROC
  	movl	$4096/8, %ecx
  	rep	movsq
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(copy_page_rep)
++=======
+ 	RET
+ SYM_FUNC_END(copy_page)
+ EXPORT_SYMBOL(copy_page)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +/*
 + *  Don't use streaming copy unless the CPU indicates X86_FEATURE_REP_GOOD.
 + *  Could vary the prefetch distance based on SMP/UP.
 +*/
  
 -SYM_FUNC_START_LOCAL(copy_page_regs)
 +ENTRY(copy_page)
 +	CFI_STARTPROC
  	subq	$2*8,	%rsp
 +	CFI_ADJUST_CFA_OFFSET 2*8
  	movq	%rbx,	(%rsp)
 +	CFI_REL_OFFSET rbx, 0
  	movq	%r12,	1*8(%rsp)
 +	CFI_REL_OFFSET r12, 1*8
  
  	movl	$(4096/64)-5,	%ecx
  	.p2align 4
@@@ -84,27 -83,7 +90,32 @@@
  	jnz	.Loop2
  
  	movq	(%rsp), %rbx
 +	CFI_RESTORE rbx
  	movq	1*8(%rsp), %r12
 +	CFI_RESTORE r12
  	addq	$2*8, %rsp
++<<<<<<< HEAD
 +	CFI_ADJUST_CFA_OFFSET -2*8
 +	ret
 +.Lcopy_page_end:
 +	CFI_ENDPROC
 +ENDPROC(copy_page)
 +
 +	/* Some CPUs run faster using the string copy instructions.
 +	   It is also a lot simpler. Use this when possible */
 +
 +#include <asm/cpufeatures.h>
 +
 +	.section .altinstr_replacement,"ax"
 +1:	.byte 0xeb					/* jmp <disp8> */
 +	.byte (copy_page_rep - copy_page) - (2f - 1b)	/* offset */
 +2:
 +	.previous
 +	.section .altinstructions,"a"
 +	altinstruction_entry copy_page, 1b, X86_FEATURE_REP_GOOD,	\
 +		.Lcopy_page_end-copy_page, 2b-1b
 +	.previous
++=======
+ 	RET
+ SYM_FUNC_END(copy_page_regs)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/copy_user_64.S
index 8725b418fdfa,8fb562f1dfaf..000000000000
--- a/arch/x86/lib/copy_user_64.S
+++ b/arch/x86/lib/copy_user_64.S
@@@ -249,20 -171,20 +249,20 @@@ ENTRY(copy_user_generic_string
  2:	movl %edx,%ecx
  3:	rep
  	movsb
 -	xorl %eax,%eax
 +4:	xorl %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  
  	.section .fixup,"ax"
 -11:	leal (%rdx,%rcx,8),%ecx
 +11:	lea (%rdx,%rcx,8),%rcx
  12:	movl %ecx,%edx		/* ecx is zerorest also */
 -	jmp .Lcopy_user_handle_tail
 +	jmp copy_user_handle_tail
  	.previous
  
 -	_ASM_EXTABLE_CPY(1b, 11b)
 -	_ASM_EXTABLE_CPY(3b, 12b)
 -SYM_FUNC_END(copy_user_generic_string)
 -EXPORT_SYMBOL(copy_user_generic_string)
 +	_ASM_EXTABLE(1b,11b)
 +	_ASM_EXTABLE(3b,12b)
 +	CFI_ENDPROC
 +ENDPROC(copy_user_generic_string)
  
  /*
   * Some CPUs are adding enhanced REP MOVSB/STOSB instructions.
@@@ -284,15 -205,190 +284,196 @@@ ENTRY(copy_user_enhanced_fast_string
  	movl %edx,%ecx
  1:	rep
  	movsb
 -	xorl %eax,%eax
 +2:	xorl %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  
  	.section .fixup,"ax"
  12:	movl %ecx,%edx		/* ecx is zerorest also */
 -	jmp .Lcopy_user_handle_tail
 +	jmp copy_user_handle_tail
  	.previous
  
++<<<<<<< HEAD
 +	_ASM_EXTABLE(1b,12b)
 +	CFI_ENDPROC
 +ENDPROC(copy_user_enhanced_fast_string)
++=======
+ 	_ASM_EXTABLE_CPY(1b, 12b)
+ SYM_FUNC_END(copy_user_enhanced_fast_string)
+ EXPORT_SYMBOL(copy_user_enhanced_fast_string)
+ 
+ /*
+  * Try to copy last bytes and clear the rest if needed.
+  * Since protection fault in copy_from/to_user is not a normal situation,
+  * it is not necessary to optimize tail handling.
+  * Don't try to copy the tail if machine check happened
+  *
+  * Input:
+  * rdi destination
+  * rsi source
+  * rdx count
+  *
+  * Output:
+  * eax uncopied bytes or 0 if successful.
+  */
+ SYM_CODE_START_LOCAL(.Lcopy_user_handle_tail)
+ 	movl %edx,%ecx
+ 1:	rep movsb
+ 2:	mov %ecx,%eax
+ 	ASM_CLAC
+ 	RET
+ 
+ 	_ASM_EXTABLE_CPY(1b, 2b)
+ SYM_CODE_END(.Lcopy_user_handle_tail)
+ 
+ /*
+  * copy_user_nocache - Uncached memory copy with exception handling
+  * This will force destination out of cache for more performance.
+  *
+  * Note: Cached memory copy is used when destination or size is not
+  * naturally aligned. That is:
+  *  - Require 8-byte alignment when size is 8 bytes or larger.
+  *  - Require 4-byte alignment when size is 4 bytes.
+  */
+ SYM_FUNC_START(__copy_user_nocache)
+ 	ASM_STAC
+ 
+ 	/* If size is less than 8 bytes, go to 4-byte copy */
+ 	cmpl $8,%edx
+ 	jb .L_4b_nocache_copy_entry
+ 
+ 	/* If destination is not 8-byte aligned, "cache" copy to align it */
+ 	ALIGN_DESTINATION
+ 
+ 	/* Set 4x8-byte copy count and remainder */
+ 	movl %edx,%ecx
+ 	andl $63,%edx
+ 	shrl $6,%ecx
+ 	jz .L_8b_nocache_copy_entry	/* jump if count is 0 */
+ 
+ 	/* Perform 4x8-byte nocache loop-copy */
+ .L_4x8b_nocache_copy_loop:
+ 1:	movq (%rsi),%r8
+ 2:	movq 1*8(%rsi),%r9
+ 3:	movq 2*8(%rsi),%r10
+ 4:	movq 3*8(%rsi),%r11
+ 5:	movnti %r8,(%rdi)
+ 6:	movnti %r9,1*8(%rdi)
+ 7:	movnti %r10,2*8(%rdi)
+ 8:	movnti %r11,3*8(%rdi)
+ 9:	movq 4*8(%rsi),%r8
+ 10:	movq 5*8(%rsi),%r9
+ 11:	movq 6*8(%rsi),%r10
+ 12:	movq 7*8(%rsi),%r11
+ 13:	movnti %r8,4*8(%rdi)
+ 14:	movnti %r9,5*8(%rdi)
+ 15:	movnti %r10,6*8(%rdi)
+ 16:	movnti %r11,7*8(%rdi)
+ 	leaq 64(%rsi),%rsi
+ 	leaq 64(%rdi),%rdi
+ 	decl %ecx
+ 	jnz .L_4x8b_nocache_copy_loop
+ 
+ 	/* Set 8-byte copy count and remainder */
+ .L_8b_nocache_copy_entry:
+ 	movl %edx,%ecx
+ 	andl $7,%edx
+ 	shrl $3,%ecx
+ 	jz .L_4b_nocache_copy_entry	/* jump if count is 0 */
+ 
+ 	/* Perform 8-byte nocache loop-copy */
+ .L_8b_nocache_copy_loop:
+ 20:	movq (%rsi),%r8
+ 21:	movnti %r8,(%rdi)
+ 	leaq 8(%rsi),%rsi
+ 	leaq 8(%rdi),%rdi
+ 	decl %ecx
+ 	jnz .L_8b_nocache_copy_loop
+ 
+ 	/* If no byte left, we're done */
+ .L_4b_nocache_copy_entry:
+ 	andl %edx,%edx
+ 	jz .L_finish_copy
+ 
+ 	/* If destination is not 4-byte aligned, go to byte copy: */
+ 	movl %edi,%ecx
+ 	andl $3,%ecx
+ 	jnz .L_1b_cache_copy_entry
+ 
+ 	/* Set 4-byte copy count (1 or 0) and remainder */
+ 	movl %edx,%ecx
+ 	andl $3,%edx
+ 	shrl $2,%ecx
+ 	jz .L_1b_cache_copy_entry	/* jump if count is 0 */
+ 
+ 	/* Perform 4-byte nocache copy: */
+ 30:	movl (%rsi),%r8d
+ 31:	movnti %r8d,(%rdi)
+ 	leaq 4(%rsi),%rsi
+ 	leaq 4(%rdi),%rdi
+ 
+ 	/* If no bytes left, we're done: */
+ 	andl %edx,%edx
+ 	jz .L_finish_copy
+ 
+ 	/* Perform byte "cache" loop-copy for the remainder */
+ .L_1b_cache_copy_entry:
+ 	movl %edx,%ecx
+ .L_1b_cache_copy_loop:
+ 40:	movb (%rsi),%al
+ 41:	movb %al,(%rdi)
+ 	incq %rsi
+ 	incq %rdi
+ 	decl %ecx
+ 	jnz .L_1b_cache_copy_loop
+ 
+ 	/* Finished copying; fence the prior stores */
+ .L_finish_copy:
+ 	xorl %eax,%eax
+ 	ASM_CLAC
+ 	sfence
+ 	RET
+ 
+ 	.section .fixup,"ax"
+ .L_fixup_4x8b_copy:
+ 	shll $6,%ecx
+ 	addl %ecx,%edx
+ 	jmp .L_fixup_handle_tail
+ .L_fixup_8b_copy:
+ 	lea (%rdx,%rcx,8),%rdx
+ 	jmp .L_fixup_handle_tail
+ .L_fixup_4b_copy:
+ 	lea (%rdx,%rcx,4),%rdx
+ 	jmp .L_fixup_handle_tail
+ .L_fixup_1b_copy:
+ 	movl %ecx,%edx
+ .L_fixup_handle_tail:
+ 	sfence
+ 	jmp .Lcopy_user_handle_tail
+ 	.previous
+ 
+ 	_ASM_EXTABLE_CPY(1b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(2b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(3b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(4b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(5b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(6b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(7b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(8b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(9b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(10b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(11b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(12b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(13b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(14b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(15b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(16b, .L_fixup_4x8b_copy)
+ 	_ASM_EXTABLE_CPY(20b, .L_fixup_8b_copy)
+ 	_ASM_EXTABLE_CPY(21b, .L_fixup_8b_copy)
+ 	_ASM_EXTABLE_CPY(30b, .L_fixup_4b_copy)
+ 	_ASM_EXTABLE_CPY(31b, .L_fixup_4b_copy)
+ 	_ASM_EXTABLE_CPY(40b, .L_fixup_1b_copy)
+ 	_ASM_EXTABLE_CPY(41b, .L_fixup_1b_copy)
+ SYM_FUNC_END(__copy_user_nocache)
+ EXPORT_SYMBOL(__copy_user_nocache)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/csum-copy_64.S
index 2419d5fefae3,d9e16a2cf285..000000000000
--- a/arch/x86/lib/csum-copy_64.S
+++ b/arch/x86/lib/csum-copy_64.S
@@@ -206,36 -191,66 +206,77 @@@ ENTRY(csum_partial_copy_generic
  	addl %ebx, %eax
  	adcl %r9d, %eax		/* carry */
  
 +	CFI_REMEMBER_STATE
  .Lende:
++<<<<<<< HEAD
 +	movq 2*8(%rsp), %rbx
 +	CFI_RESTORE rbx
 +	movq 3*8(%rsp), %r12
 +	CFI_RESTORE r12
 +	movq 4*8(%rsp), %r14
 +	CFI_RESTORE r14
 +	movq 5*8(%rsp), %r13
 +	CFI_RESTORE r13
 +	movq 6*8(%rsp), %rbp
 +	CFI_RESTORE rbp
 +	addq $7*8, %rsp
 +	CFI_ADJUST_CFA_OFFSET -7*8
 +	ret
 +	CFI_RESTORE_STATE
++=======
+ 	testq %r10, %r10
+ 	js  .Lwas_odd
+ .Lout:
+ 	movq 0*8(%rsp), %rbx
+ 	movq 1*8(%rsp), %r12
+ 	movq 2*8(%rsp), %r14
+ 	movq 3*8(%rsp), %r13
+ 	movq 4*8(%rsp), %r15
+ 	addq $5*8, %rsp
+ 	RET
+ .Lshort:
+ 	movl %ecx, %r10d
+ 	jmp  .L1
+ .Lunaligned:
+ 	xorl %ebx, %ebx
+ 	testb $1, %sil
+ 	jne  .Lodd
+ 1:	testb $2, %sil
+ 	je   2f
+ 	source
+ 	movw (%rdi), %bx
+ 	dest
+ 	movw %bx, (%rsi)
+ 	leaq 2(%rdi), %rdi
+ 	subq $2, %rcx
+ 	leaq 2(%rsi), %rsi
+ 	addq %rbx, %rax
+ 2:	testb $4, %sil
+ 	je .Laligned
+ 	source
+ 	movl (%rdi), %ebx
+ 	dest
+ 	movl %ebx, (%rsi)
+ 	leaq 4(%rdi), %rdi
+ 	subq $4, %rcx
+ 	leaq 4(%rsi), %rsi
+ 	addq %rbx, %rax
+ 	jmp .Laligned
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -.Lodd:
 -	source
 -	movb (%rdi), %bl
 -	dest
 -	movb %bl, (%rsi)
 -	leaq 1(%rdi), %rdi
 -	leaq 1(%rsi), %rsi
 -	/* decrement, set MSB */
 -	leaq -1(%rcx, %rcx), %rcx
 -	rorq $1, %rcx
 -	shll $8, %ebx
 -	addq %rbx, %rax
 -	jmp 1b
 -
 -.Lwas_odd:
 -	roll $8, %eax
 -	jmp .Lout
 +	/* Exception handlers. Very simple, zeroing is done in the wrappers */
 +.Lbad_source:
 +	movq (%rsp), %rax
 +	testq %rax, %rax
 +	jz   .Lende
 +	movl $-EFAULT, (%rax)
 +	jmp  .Lende
  
 -	/* Exception: just return 0 */
 -.Lfault:
 -	xorl %eax, %eax
 -	jmp  .Lout
 -SYM_FUNC_END(csum_partial_copy_generic)
 +.Lbad_dest:
 +	movq 8(%rsp), %rax
 +	testq %rax, %rax
 +	jz   .Lende
 +	movl $-EFAULT, (%rax)
 +	jmp .Lende
 +	CFI_ENDPROC
 +ENDPROC(csum_partial_copy_generic)
diff --cc arch/x86/lib/getuser.S
index 3917307fca99,b70d98d79a9d..000000000000
--- a/arch/x86/lib/getuser.S
+++ b/arch/x86/lib/getuser.S
@@@ -46,105 -57,150 +46,184 @@@ ENTRY(__get_user_1
  1:	movzbl (%_ASM_AX),%edx
  	xor %eax,%eax
  	ASM_CLAC
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(__get_user_1)
++=======
+ 	RET
+ SYM_FUNC_END(__get_user_1)
+ EXPORT_SYMBOL(__get_user_1)
 -
 -SYM_FUNC_START(__get_user_2)
 -	LOAD_TASK_SIZE_MINUS_N(1)
 -	cmp %_ASM_DX,%_ASM_AX
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +ENTRY(__get_user_2)
 +	CFI_STARTPROC
 +	add $1,%_ASM_AX
 +	jc bad_get_user
 +	GET_THREAD_INFO(%_ASM_DX)
 +	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
  	jae bad_get_user
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -2:	movzwl (%_ASM_AX),%edx
 +2:	movzwl -1(%_ASM_AX),%edx
  	xor %eax,%eax
  	ASM_CLAC
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(__get_user_2)
++=======
+ 	RET
+ SYM_FUNC_END(__get_user_2)
+ EXPORT_SYMBOL(__get_user_2)
 -
 -SYM_FUNC_START(__get_user_4)
 -	LOAD_TASK_SIZE_MINUS_N(3)
 -	cmp %_ASM_DX,%_ASM_AX
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +ENTRY(__get_user_4)
 +	CFI_STARTPROC
 +	add $3,%_ASM_AX
 +	jc bad_get_user
 +	GET_THREAD_INFO(%_ASM_DX)
 +	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
  	jae bad_get_user
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -3:	movl (%_ASM_AX),%edx
 +3:	movl -3(%_ASM_AX),%edx
  	xor %eax,%eax
  	ASM_CLAC
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(__get_user_4)
++=======
+ 	RET
+ SYM_FUNC_END(__get_user_4)
+ EXPORT_SYMBOL(__get_user_4)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(__get_user_8)
 +ENTRY(__get_user_8)
 +	CFI_STARTPROC
  #ifdef CONFIG_X86_64
 -	LOAD_TASK_SIZE_MINUS_N(7)
 -	cmp %_ASM_DX,%_ASM_AX
 +	add $7,%_ASM_AX
 +	jc bad_get_user
 +	GET_THREAD_INFO(%_ASM_DX)
 +	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
  	jae bad_get_user
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -4:	movq (%_ASM_AX),%rdx
 +4:	movq -7(%_ASM_AX),%rdx
  	xor %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  #else
 -	LOAD_TASK_SIZE_MINUS_N(7)
 -	cmp %_ASM_DX,%_ASM_AX
 +	add $7,%_ASM_AX
 +	jc bad_get_user_8
 +	GET_THREAD_INFO(%_ASM_DX)
 +	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
  	jae bad_get_user_8
  	sbb %_ASM_DX, %_ASM_DX		/* array_index_mask_nospec() */
  	and %_ASM_DX, %_ASM_AX
  	ASM_STAC
 -4:	movl (%_ASM_AX),%edx
 -5:	movl 4(%_ASM_AX),%ecx
 +4:	movl -7(%_ASM_AX),%edx
 +5:	movl -3(%_ASM_AX),%ecx
  	xor %eax,%eax
  	ASM_CLAC
- 	ret
+ 	RET
  #endif
++<<<<<<< HEAD
 +	CFI_ENDPROC
 +ENDPROC(__get_user_8)
++=======
+ SYM_FUNC_END(__get_user_8)
+ EXPORT_SYMBOL(__get_user_8)
+ 
+ /* .. and the same for __get_user, just without the range checks */
+ SYM_FUNC_START(__get_user_nocheck_1)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ 6:	movzbl (%_ASM_AX),%edx
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_1)
+ EXPORT_SYMBOL(__get_user_nocheck_1)
+ 
+ SYM_FUNC_START(__get_user_nocheck_2)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ 7:	movzwl (%_ASM_AX),%edx
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_2)
+ EXPORT_SYMBOL(__get_user_nocheck_2)
+ 
+ SYM_FUNC_START(__get_user_nocheck_4)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ 8:	movl (%_ASM_AX),%edx
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_4)
+ EXPORT_SYMBOL(__get_user_nocheck_4)
+ 
+ SYM_FUNC_START(__get_user_nocheck_8)
+ 	ASM_STAC
+ 	ASM_BARRIER_NOSPEC
+ #ifdef CONFIG_X86_64
+ 9:	movq (%_ASM_AX),%rdx
+ #else
+ 9:	movl (%_ASM_AX),%edx
+ 10:	movl 4(%_ASM_AX),%ecx
+ #endif
+ 	xor %eax,%eax
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__get_user_nocheck_8)
+ EXPORT_SYMBOL(__get_user_nocheck_8)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
 -SYM_CODE_START_LOCAL(.Lbad_get_user_clac)
 -	ASM_CLAC
  bad_get_user:
 +	CFI_STARTPROC
  	xor %edx,%edx
  	mov $(-EFAULT),%_ASM_AX
++<<<<<<< HEAD
 +	ASM_CLAC
 +	ret
 +	CFI_ENDPROC
 +END(bad_get_user)
++=======
+ 	RET
+ SYM_CODE_END(.Lbad_get_user_clac)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  #ifdef CONFIG_X86_32
 -SYM_CODE_START_LOCAL(.Lbad_get_user_8_clac)
 -	ASM_CLAC
  bad_get_user_8:
 +	CFI_STARTPROC
  	xor %edx,%edx
  	xor %ecx,%ecx
  	mov $(-EFAULT),%_ASM_AX
++<<<<<<< HEAD
 +	ASM_CLAC
 +	ret
 +	CFI_ENDPROC
 +END(bad_get_user_8)
++=======
+ 	RET
+ SYM_CODE_END(.Lbad_get_user_8_clac)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #endif
  
 -/* get_user */
 -	_ASM_EXTABLE_UA(1b, .Lbad_get_user_clac)
 -	_ASM_EXTABLE_UA(2b, .Lbad_get_user_clac)
 -	_ASM_EXTABLE_UA(3b, .Lbad_get_user_clac)
 -#ifdef CONFIG_X86_64
 -	_ASM_EXTABLE_UA(4b, .Lbad_get_user_clac)
 -#else
 -	_ASM_EXTABLE_UA(4b, .Lbad_get_user_8_clac)
 -	_ASM_EXTABLE_UA(5b, .Lbad_get_user_8_clac)
 -#endif
 -
 -/* __get_user */
 -	_ASM_EXTABLE_UA(6b, .Lbad_get_user_clac)
 -	_ASM_EXTABLE_UA(7b, .Lbad_get_user_clac)
 -	_ASM_EXTABLE_UA(8b, .Lbad_get_user_clac)
 +	_ASM_EXTABLE(1b,bad_get_user)
 +	_ASM_EXTABLE(2b,bad_get_user)
 +	_ASM_EXTABLE(3b,bad_get_user)
  #ifdef CONFIG_X86_64
 -	_ASM_EXTABLE_UA(9b, .Lbad_get_user_clac)
 +	_ASM_EXTABLE(4b,bad_get_user)
  #else
 -	_ASM_EXTABLE_UA(9b, .Lbad_get_user_8_clac)
 -	_ASM_EXTABLE_UA(10b, .Lbad_get_user_8_clac)
 +	_ASM_EXTABLE(4b,bad_get_user_8)
 +	_ASM_EXTABLE(5b,bad_get_user_8)
  #endif
diff --cc arch/x86/lib/iomap_copy_64.S
index 05a95e713da8,a1f9416bf67a..000000000000
--- a/arch/x86/lib/iomap_copy_64.S
+++ b/arch/x86/lib/iomap_copy_64.S
@@@ -21,10 -8,8 +21,15 @@@
  /*
   * override generic version in lib/iomap_copy.c
   */
 -SYM_FUNC_START(__iowrite32_copy)
 +ENTRY(__iowrite32_copy)
 +	CFI_STARTPROC
  	movl %edx,%ecx
  	rep movsd
++<<<<<<< HEAD
 +	ret
 +	CFI_ENDPROC
 +ENDPROC(__iowrite32_copy)
++=======
+ 	RET
+ SYM_FUNC_END(__iowrite32_copy)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/memcpy_64.S
index dcf2dd48a122,59cf2343f3d9..000000000000
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@@ -34,29 -39,24 +34,42 @@@
  	rep movsq
  	movl %edx, %ecx
  	rep movsb
++<<<<<<< HEAD
 +	ret
 +.Lmemcpy_e:
 +	.previous
++=======
+ 	RET
+ SYM_FUNC_END(memcpy)
+ SYM_FUNC_END_ALIAS(__memcpy)
+ EXPORT_SYMBOL(memcpy)
+ EXPORT_SYMBOL(__memcpy)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
 - * memcpy_erms() - enhanced fast string memcpy. This is faster and
 - * simpler than memcpy. Use memcpy_erms when possible.
 + * memcpy_c_e() - enhanced fast string memcpy. This is faster and simpler than
 + * memcpy_c. Use memcpy_c_e when possible.
 + *
 + * This gets patched over the unrolled variant (below) via the
 + * alternative instructions framework:
   */
 -SYM_FUNC_START_LOCAL(memcpy_erms)
 +	.section .altinstr_replacement, "ax", @progbits
 +.Lmemcpy_c_e:
  	movq %rdi, %rax
  	movq %rdx, %rcx
  	rep movsb
++<<<<<<< HEAD
 +	ret
 +.Lmemcpy_e_e:
 +	.previous
++=======
+ 	RET
+ SYM_FUNC_END(memcpy_erms)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START_LOCAL(memcpy_orig)
 +ENTRY(__memcpy)
 +ENTRY(memcpy)
 +	CFI_STARTPROC
  	movq %rdi, %rax
  
  	cmpq $0x20, %rdx
@@@ -180,140 -180,7 +193,145 @@@
  	movb %cl, (%rdi)
  
  .Lend:
++<<<<<<< HEAD
 +	retq
 +	CFI_ENDPROC
 +ENDPROC(memcpy)
 +ENDPROC(__memcpy)
++=======
+ 	RET
+ SYM_FUNC_END(memcpy_orig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
 +
 +	/*
 +	 * Some CPUs are adding enhanced REP MOVSB/STOSB feature
 +	 * If the feature is supported, memcpy_c_e() is the first choice.
 +	 * If enhanced rep movsb copy is not available, use fast string copy
 +	 * memcpy_c() when possible. This is faster and code is simpler than
 +	 * original memcpy().
 +	 * Otherwise, original memcpy() is used.
 +	 * In .altinstructions section, ERMS feature is placed after REG_GOOD
 +         * feature to implement the right patch order.
 +	 *
 +	 * Replace only beginning, memcpy is used to apply alternatives,
 +	 * so it is silly to overwrite itself with nops - reboot is the
 +	 * only outcome...
 +	 */
 +	.section .altinstructions, "a"
 +	altinstruction_entry memcpy,.Lmemcpy_c,X86_FEATURE_REP_GOOD,\
 +			     .Lmemcpy_e-.Lmemcpy_c,.Lmemcpy_e-.Lmemcpy_c
 +	altinstruction_entry memcpy,.Lmemcpy_c_e,X86_FEATURE_ERMS, \
 +			     .Lmemcpy_e_e-.Lmemcpy_c_e,.Lmemcpy_e_e-.Lmemcpy_c_e
 +	.previous
 +
 +#ifndef CONFIG_UML
 +
 +MCSAFE_TEST_CTL
 +
 +/*
 + * __memcpy_mcsafe - memory copy with machine check exception handling
 + * Note that we only catch machine checks when reading the source addresses.
 + * Writes to target are posted and don't generate machine checks.
 + */
 +ENTRY(__memcpy_mcsafe)
 +	cmpl $8, %edx
 +	/* Less than 8 bytes? Go to byte copy loop */
 +	jb .L_no_whole_words
 +
 +	/* Check for bad alignment of source */
 +	testl $7, %esi
 +	/* Already aligned */
 +	jz .L_8byte_aligned
 +
 +	/* Copy one byte at a time until source is 8-byte aligned */
 +	movl %esi, %ecx
 +	andl $7, %ecx
 +	subl $8, %ecx
 +	negl %ecx
 +	subl %ecx, %edx
 +.L_read_leading_bytes:
 +	movb (%rsi), %al
 +	MCSAFE_TEST_SRC %rsi 1 .E_leading_bytes
 +	MCSAFE_TEST_DST %rdi 1 .E_leading_bytes
 +.L_write_leading_bytes:
 +	movb %al, (%rdi)
 +	incq %rsi
 +	incq %rdi
 +	decl %ecx
 +	jnz .L_read_leading_bytes
 +
 +.L_8byte_aligned:
 +	movl %edx, %ecx
 +	andl $7, %edx
 +	shrl $3, %ecx
 +	jz .L_no_whole_words
 +
 +.L_read_words:
 +	movq (%rsi), %r8
 +	MCSAFE_TEST_SRC %rsi 8 .E_read_words
 +	MCSAFE_TEST_DST %rdi 8 .E_write_words
 +.L_write_words:
 +	movq %r8, (%rdi)
 +	addq $8, %rsi
 +	addq $8, %rdi
 +	decl %ecx
 +	jnz .L_read_words
 +
 +	/* Any trailing bytes? */
 +.L_no_whole_words:
 +	andl %edx, %edx
 +	jz .L_done_memcpy_trap
 +
 +	/* Copy trailing bytes */
 +	movl %edx, %ecx
 +.L_read_trailing_bytes:
 +	movb (%rsi), %al
 +	MCSAFE_TEST_SRC %rsi 1 .E_trailing_bytes
 +	MCSAFE_TEST_DST %rdi 1 .E_trailing_bytes
 +.L_write_trailing_bytes:
 +	movb %al, (%rdi)
 +	incq %rsi
 +	incq %rdi
 +	decl %ecx
 +	jnz .L_read_trailing_bytes
 +
 +	/* Copy successful. Return zero */
 +.L_done_memcpy_trap:
 +	xorq %rax, %rax
 +	ret
 +ENDPROC(__memcpy_mcsafe)
 +
 +	.section .fixup, "ax"
 +	/*
 +	 * Return number of bytes not copied for any failure. Note that
 +	 * there is no "tail" handling since the source buffer is 8-byte
 +	 * aligned and poison is cacheline aligned.
 +	 */
 +.E_read_words:
 +	shll	$3, %ecx
 +.E_leading_bytes:
 +	addl	%edx, %ecx
 +.E_trailing_bytes:
 +	mov	%ecx, %eax
 +	ret
 +
 +	/*
 +	 * For write fault handling, given the destination is unaligned,
 +	 * we handle faults on multi-byte writes with a byte-by-byte
 +	 * copy up to the write-protected page.
 +	 */
 +.E_write_words:
 +	shll	$3, %ecx
 +	addl	%edx, %ecx
 +	movl	%ecx, %edx
 +	jmp mcsafe_handle_tail
 +
 +	.previous
  
 -.popsection
 +	_ASM_EXTABLE_FAULT(.L_read_leading_bytes, .E_leading_bytes)
 +	_ASM_EXTABLE_FAULT(.L_read_words, .E_read_words)
 +	_ASM_EXTABLE_FAULT(.L_read_trailing_bytes, .E_trailing_bytes)
 +	_ASM_EXTABLE(.L_write_leading_bytes, .E_leading_bytes)
 +	_ASM_EXTABLE(.L_write_words, .E_write_words)
 +	_ASM_EXTABLE(.L_write_trailing_bytes, .E_trailing_bytes)
 +#endif
diff --cc arch/x86/lib/memmove_64.S
index 284f9d71e15b,e84d649620c4..000000000000
--- a/arch/x86/lib/memmove_64.S
+++ b/arch/x86/lib/memmove_64.S
@@@ -40,7 -37,11 +40,13 @@@ ENTRY(memmove
  	cmp %rdi, %r8
  	jg 2f
  
 -	/* FSRM implies ERMS => no length checks, do the copy directly */
  .Lmemmove_begin_forward:
++<<<<<<< HEAD
++=======
+ 	ALTERNATIVE "cmp $0x20, %rdx; jb 1f", "", X86_FEATURE_FSRM
+ 	ALTERNATIVE "", "movq %rdx, %rcx; rep movsb; RET", X86_FEATURE_ERMS
+ 
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  	/*
  	 * movsq instruction have many startup latency
  	 * so we handle small size by general register.
@@@ -202,22 -205,8 +208,30 @@@
  	movb (%rsi), %r11b
  	movb %r11b, (%rdi)
  13:
++<<<<<<< HEAD
 +	retq
 +	CFI_ENDPROC
 +
 +	.section .altinstr_replacement,"ax"
 +.Lmemmove_begin_forward_efs:
 +	/* Forward moving data. */
 +	movq %rdx, %rcx
 +	rep movsb
 +	retq
 +.Lmemmove_end_forward_efs:
 +	.previous
 +
 +	.section .altinstructions,"a"
 +	altinstruction_entry .Lmemmove_begin_forward,		\
 +		.Lmemmove_begin_forward_efs,X86_FEATURE_ERMS,	\
 +		.Lmemmove_end_forward-.Lmemmove_begin_forward,	\
 +		.Lmemmove_end_forward_efs-.Lmemmove_begin_forward_efs
 +	.previous
 +ENDPROC(memmove)
++=======
+ 	RET
+ SYM_FUNC_END(__memmove)
+ SYM_FUNC_END_ALIAS(memmove)
+ EXPORT_SYMBOL(__memmove)
+ EXPORT_SYMBOL(memmove)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/lib/memset_64.S
index 14df4d77b69a,d624f2bc42f1..000000000000
--- a/arch/x86/lib/memset_64.S
+++ b/arch/x86/lib/memset_64.S
@@@ -30,9 -40,11 +30,17 @@@
  	movl %edx,%ecx
  	rep stosb
  	movq %r9,%rax
++<<<<<<< HEAD
 +	ret
 +.Lmemset_e:
 +	.previous
++=======
+ 	RET
+ SYM_FUNC_END(__memset)
+ SYM_FUNC_END_ALIAS(memset)
+ EXPORT_SYMBOL(memset)
+ EXPORT_SYMBOL(__memset)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * ISO C memset - set a memory block to a byte value. This function uses
@@@ -52,13 -63,10 +60,18 @@@
  	movq %rdx,%rcx
  	rep stosb
  	movq %r9,%rax
++<<<<<<< HEAD
 +	ret
 +.Lmemset_e_e:
 +	.previous
++=======
+ 	RET
+ SYM_FUNC_END(memset_erms)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START_LOCAL(memset_orig)
 +ENTRY(memset)
 +ENTRY(__memset)
 +	CFI_STARTPROC
  	movq %rdi,%r10
  
  	/* expand byte value  */
@@@ -118,9 -125,8 +131,9 @@@
  
  .Lende:
  	movq	%r10,%rax
- 	ret
+ 	RET
  
 +	CFI_RESTORE_STATE
  .Lbad_alignment:
  	cmpq $7,%rdx
  	jbe	.Lhandle_7
diff --cc arch/x86/lib/msr-reg.S
index f6d13eefad10,ebd259f31496..000000000000
--- a/arch/x86/lib/msr-reg.S
+++ b/arch/x86/lib/msr-reg.S
@@@ -32,14 -30,13 +32,20 @@@ ENTRY(\op\()_safe_regs
  	movl    %ecx, 4(%r10)
  	movl    %edx, 8(%r10)
  	movl    %ebx, 12(%r10)
 -	movl    %r12d, 20(%r10)
 +	movl    %ebp, 20(%r10)
  	movl    %esi, 24(%r10)
  	movl    %edi, 28(%r10)
++<<<<<<< HEAD
 +	popq_cfi %rbp
 +	popq_cfi %rbx
 +	ret
++=======
+ 	popq %r12
+ 	popq %rbx
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  3:
 +	CFI_RESTORE_STATE
  	movl    $-EIO, %r11d
  	jmp     2b
  
@@@ -79,14 -72,13 +85,23 @@@ ENTRY(\op\()_safe_regs
  	movl    %ebp, 20(%eax)
  	movl    %esi, 24(%eax)
  	movl    %edi, 28(%eax)
++<<<<<<< HEAD
 +	popl_cfi %eax
 +	popl_cfi %edi
 +	popl_cfi %esi
 +	popl_cfi %ebp
 +	popl_cfi %ebx
 +	ret
++=======
+ 	popl %eax
+ 	popl %edi
+ 	popl %esi
+ 	popl %ebp
+ 	popl %ebx
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  3:
 +	CFI_RESTORE_STATE
  	movl    $-EIO, 4(%esp)
  	jmp     2b
  
diff --cc arch/x86/lib/putuser.S
index fc6ba17a7eec,ecb2049c1273..000000000000
--- a/arch/x86/lib/putuser.S
+++ b/arch/x86/lib/putuser.S
@@@ -30,53 -33,63 +30,80 @@@
   * as they get called from within inline assembly.
   */
  
 -#ifdef CONFIG_X86_5LEVEL
 -#define LOAD_TASK_SIZE_MINUS_N(n) \
 -	ALTERNATIVE __stringify(mov $((1 << 47) - 4096 - (n)),%rbx), \
 -		    __stringify(mov $((1 << 56) - 4096 - (n)),%rbx), X86_FEATURE_LA57
 -#else
 -#define LOAD_TASK_SIZE_MINUS_N(n) \
 -	mov $(TASK_SIZE_MAX - (n)),%_ASM_BX
 -#endif
 +#define ENTER	CFI_STARTPROC ; \
 +		GET_THREAD_INFO(%_ASM_BX)
 +#define EXIT	ASM_CLAC ;	\
 +		ret ;		\
 +		CFI_ENDPROC
  
  .text
 -SYM_FUNC_START(__put_user_1)
 -	LOAD_TASK_SIZE_MINUS_N(0)
 -	cmp %_ASM_BX,%_ASM_CX
 -	jae .Lbad_put_user
 -SYM_INNER_LABEL(__put_user_nocheck_1, SYM_L_GLOBAL)
 +ENTRY(__put_user_1)
 +	ENTER
 +	cmp TI_addr_limit(%_ASM_BX),%_ASM_CX
 +	jae bad_put_user
  	ASM_STAC
  1:	movb %al,(%_ASM_CX)
++<<<<<<< HEAD
 +	xor %eax,%eax
 +	EXIT
 +ENDPROC(__put_user_1)
++=======
+ 	xor %ecx,%ecx
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__put_user_1)
+ EXPORT_SYMBOL(__put_user_1)
+ EXPORT_SYMBOL(__put_user_nocheck_1)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(__put_user_2)
 -	LOAD_TASK_SIZE_MINUS_N(1)
 +ENTRY(__put_user_2)
 +	ENTER
 +	mov TI_addr_limit(%_ASM_BX),%_ASM_BX
 +	sub $1,%_ASM_BX
  	cmp %_ASM_BX,%_ASM_CX
 -	jae .Lbad_put_user
 -SYM_INNER_LABEL(__put_user_nocheck_2, SYM_L_GLOBAL)
 +	jae bad_put_user
  	ASM_STAC
  2:	movw %ax,(%_ASM_CX)
++<<<<<<< HEAD
 +	xor %eax,%eax
 +	EXIT
 +ENDPROC(__put_user_2)
++=======
+ 	xor %ecx,%ecx
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__put_user_2)
+ EXPORT_SYMBOL(__put_user_2)
+ EXPORT_SYMBOL(__put_user_nocheck_2)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(__put_user_4)
 -	LOAD_TASK_SIZE_MINUS_N(3)
 +ENTRY(__put_user_4)
 +	ENTER
 +	mov TI_addr_limit(%_ASM_BX),%_ASM_BX
 +	sub $3,%_ASM_BX
  	cmp %_ASM_BX,%_ASM_CX
 -	jae .Lbad_put_user
 -SYM_INNER_LABEL(__put_user_nocheck_4, SYM_L_GLOBAL)
 +	jae bad_put_user
  	ASM_STAC
  3:	movl %eax,(%_ASM_CX)
++<<<<<<< HEAD
 +	xor %eax,%eax
 +	EXIT
 +ENDPROC(__put_user_4)
++=======
+ 	xor %ecx,%ecx
+ 	ASM_CLAC
+ 	RET
+ SYM_FUNC_END(__put_user_4)
+ EXPORT_SYMBOL(__put_user_4)
+ EXPORT_SYMBOL(__put_user_nocheck_4)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(__put_user_8)
 -	LOAD_TASK_SIZE_MINUS_N(7)
 +ENTRY(__put_user_8)
 +	ENTER
 +	mov TI_addr_limit(%_ASM_BX),%_ASM_BX
 +	sub $7,%_ASM_BX
  	cmp %_ASM_BX,%_ASM_CX
 -	jae .Lbad_put_user
 -SYM_INNER_LABEL(__put_user_nocheck_8, SYM_L_GLOBAL)
 +	jae bad_put_user
  	ASM_STAC
  4:	mov %_ASM_AX,(%_ASM_CX)
  #ifdef CONFIG_X86_32
diff --cc arch/x86/lib/retpoline.S
index b3bec3619100,a842866062c8..000000000000
--- a/arch/x86/lib/retpoline.S
+++ b/arch/x86/lib/retpoline.S
@@@ -4,15 -4,38 +4,36 @@@
  #include <linux/linkage.h>
  #include <asm/dwarf2.h>
  #include <asm/cpufeatures.h>
 -#include <asm/alternative.h>
 -#include <asm/export.h>
 +#include <asm/alternative-asm.h>
  #include <asm/nospec-branch.h>
++<<<<<<< HEAD
++=======
+ #include <asm/unwind_hints.h>
+ #include <asm/frame.h>
+ 
+ 	.section .text.__x86.indirect_thunk
+ 
+ .macro RETPOLINE reg
+ 	ANNOTATE_INTRA_FUNCTION_CALL
+ 	call    .Ldo_rop_\@
+ .Lspec_trap_\@:
+ 	UNWIND_HINT_EMPTY
+ 	pause
+ 	lfence
+ 	jmp .Lspec_trap_\@
+ .Ldo_rop_\@:
+ 	mov     %\reg, (%_ASM_SP)
+ 	UNWIND_HINT_FUNC
+ 	RET
+ .endm
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .macro THUNK reg
 -
 -	.align RETPOLINE_THUNK_SIZE
 -SYM_INNER_LABEL(__x86_indirect_thunk_\reg, SYM_L_GLOBAL)
 -	UNWIND_HINT_EMPTY
 -
 -	ALTERNATIVE_2 __stringify(ANNOTATE_RETPOLINE_SAFE; jmp *%\reg), \
 -		      __stringify(RETPOLINE \reg), X86_FEATURE_RETPOLINE, \
 -		      __stringify(lfence; ANNOTATE_RETPOLINE_SAFE; jmp *%\reg), X86_FEATURE_RETPOLINE_AMD
 -
 +ENTRY(__x86_indirect_thunk_\reg)
 +	CFI_STARTPROC
 +	JMP_NOSPEC %\reg
 +	CFI_ENDPROC
 +ENDPROC(__x86_indirect_thunk_\reg)
  .endm
  
  /*
diff --cc arch/x86/math-emu/div_small.S
index 47099628fa4c,637439bfefa4..000000000000
--- a/arch/x86/math-emu/div_small.S
+++ b/arch/x86/math-emu/div_small.S
@@@ -43,5 -44,5 +43,10 @@@ ENTRY(FPU_div_small
  	popl	%esi
  
  	leave
++<<<<<<< HEAD
 +	ret
 +
++=======
+ 	RET
+ SYM_FUNC_END(FPU_div_small)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/mul_Xsig.S
index 717785a53eb4,54a031b66142..000000000000
--- a/arch/x86/math-emu/mul_Xsig.S
+++ b/arch/x86/math-emu/mul_Xsig.S
@@@ -61,10 -62,11 +61,15 @@@ ENTRY(mul32_Xsig
  
  	popl %esi
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(mul32_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
 -SYM_FUNC_START(mul64_Xsig)
 +ENTRY(mul64_Xsig)
  	pushl %ebp
  	movl %esp,%ebp
  	subl $16,%esp
@@@ -113,7 -115,8 +118,12 @@@
  
  	popl %esi
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(mul64_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
  
@@@ -172,5 -175,5 +182,10 @@@ ENTRY(mul_Xsig_Xsig
  
  	popl %esi
  	leave
++<<<<<<< HEAD
 +	ret
 +
++=======
+ 	RET
+ SYM_FUNC_END(mul_Xsig_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/polynom_Xsig.S
index 17315c89ff3d,35fd723fc0df..000000000000
--- a/arch/x86/math-emu/polynom_Xsig.S
+++ b/arch/x86/math-emu/polynom_Xsig.S
@@@ -132,4 -133,5 +132,9 @@@ L_accum_done
  	popl	%edi
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(polynomial_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/reg_norm.S
index 8b6352efceef,594936eeed67..000000000000
--- a/arch/x86/math-emu/reg_norm.S
+++ b/arch/x86/math-emu/reg_norm.S
@@@ -144,4 -146,5 +144,9 @@@ L_exit_nuo_zero
  
  	popl	%ebx
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(FPU_normalize_nuo)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/reg_u_sub.S
index 1b6c24801d22,4c900c29e4ff..000000000000
--- a/arch/x86/math-emu/reg_u_sub.S
+++ b/arch/x86/math-emu/reg_u_sub.S
@@@ -269,4 -270,5 +269,9 @@@ L_exit
  	popl	%edi
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(FPU_u_sub)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/round_Xsig.S
index bbe0e87718e4,126c40473bad..000000000000
--- a/arch/x86/math-emu/round_Xsig.S
+++ b/arch/x86/math-emu/round_Xsig.S
@@@ -77,7 -78,8 +77,12 @@@ L_exit
  	popl	%esi
  	popl	%ebx
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(round_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
  
@@@ -137,5 -138,5 +142,10 @@@ L_n_exit
  	popl	%esi
  	popl	%ebx
  	leave
++<<<<<<< HEAD
 +	ret
 +
++=======
+ 	RET
+ SYM_FUNC_END(norm_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/shr_Xsig.S
index 31cdd118e918,f726bf6f6396..000000000000
--- a/arch/x86/math-emu/shr_Xsig.S
+++ b/arch/x86/math-emu/shr_Xsig.S
@@@ -84,4 -85,5 +84,9 @@@ L_more_than_95
  	movl	%eax,8(%esi)
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(shr_Xsig)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/math-emu/wm_shrx.S
index 518428317985,f608a28a4c43..000000000000
--- a/arch/x86/math-emu/wm_shrx.S
+++ b/arch/x86/math-emu/wm_shrx.S
@@@ -91,7 -92,8 +91,12 @@@ L_more_than_95
  	movl	%eax,4(%esi)
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(FPU_shrx)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  
  /*---------------------------------------------------------------------------+
@@@ -201,4 -203,5 +206,9 @@@ Ls_more_than_95
  	popl	%ebx
  	popl	%esi
  	leave
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_FUNC_END(FPU_shrxs)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/mm/mem_encrypt_boot.S
index 01f682cf77a8,3d1dba05fce4..000000000000
--- a/arch/x86/mm/mem_encrypt_boot.S
+++ b/arch/x86/mm/mem_encrypt_boot.S
@@@ -66,10 -65,10 +66,15 @@@ ENTRY(sme_encrypt_execute
  	movq	%rbp, %rsp		/* Restore original stack pointer */
  	pop	%rbp
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(sme_encrypt_execute)
++=======
+ 	RET
+ SYM_FUNC_END(sme_encrypt_execute)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 -SYM_FUNC_START(__enc_copy)
 +ENTRY(__enc_copy)
  /*
   * Routine used to encrypt memory in place.
   *   This routine must be run outside of the kernel proper since
@@@ -152,6 -151,6 +157,6 @@@
  	pop	%r12
  	pop	%r15
  
- 	ret
+ 	RET
  .L__enc_copy_end:
 -SYM_FUNC_END(__enc_copy)
 +ENDPROC(__enc_copy)
diff --cc arch/x86/platform/efi/efi_stub_32.S
index 040192b50d02,f3cfdb1c9a35..000000000000
--- a/arch/x86/platform/efi/efi_stub_32.S
+++ b/arch/x86/platform/efi/efi_stub_32.S
@@@ -88,36 -53,8 +88,43 @@@ ENTRY(efi_call_phys
  	movl	%cr0, %edx
  	orl	$0x80000000, %edx
  	movl	%edx, %cr0
 +	jmp	1f
 +1:
 +	/*
 +	 * 8. Now restore the virtual mode from flat mode by
 +	 * adding EIP with PAGE_OFFSET.
 +	 */
 +	movl	$1f, %edx
 +	jmp	*%edx
 +1:
 +
++<<<<<<< HEAD
 +	/*
 +	 * 9. Balance the stack. And because EAX contain the return value,
 +	 * we'd better not clobber it.
 +	 */
 +	leal	efi_rt_function_ptr, %edx
 +	movl	(%edx), %ecx
 +	pushl	%ecx
 +
 +	/*
 +	 * 10. Push the saved return address onto the stack and return.
 +	 */
 +	leal	saved_return_addr, %edx
 +	movl	(%edx), %ecx
 +	pushl	%ecx
 +	ret
 +ENDPROC(efi_call_phys)
 +.previous
  
 +.data
 +saved_return_addr:
 +	.long 0
 +efi_rt_function_ptr:
 +	.long 0
++=======
+ 	movl	16(%esp), %ebx
+ 	leave
+ 	RET
+ SYM_FUNC_END(efi_call_svam)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/platform/efi/efi_stub_64.S
index cd95075944ab,2206b8bc47b8..000000000000
--- a/arch/x86/platform/efi/efi_stub_64.S
+++ b/arch/x86/platform/efi/efi_stub_64.S
@@@ -49,9 -21,7 +49,16 @@@ ENTRY(efi_call
  	mov %r8, %r9
  	mov %rcx, %r8
  	mov %rsi, %rcx
++<<<<<<< HEAD
 +	call *%rdi
 +	addq $48, %rsp
 +	RESTORE_XMM
 +	popq %rbp
 +	ret
 +ENDPROC(efi_call)
++=======
+ 	CALL_NOSPEC rdi
+ 	leave
+ 	RET
+ SYM_FUNC_END(__efi_call)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/platform/efi/efi_thunk_64.S
index 5cdc72ebbc82,f2a8eec69f8f..000000000000
--- a/arch/x86/platform/efi/efi_thunk_64.S
+++ b/arch/x86/platform/efi/efi_thunk_64.S
@@@ -40,113 -42,35 +40,117 @@@ ENTRY(efi64_thunk
  	movq	$__START_KERNEL_map, %rax
  	subq	phys_base(%rip), %rax
  
 -	leaq	1f(%rip), %rbp
 -	leaq	2f(%rip), %rbx
 -	subq	%rax, %rbp
 +	/*
 +	 * Push some physical addresses onto the stack. This is easier
 +	 * to do now in a code64 section while the assembler can address
 +	 * 64-bit values. Note that all the addresses on the stack are
 +	 * 32-bit.
 +	 */
 +	subq	$16, %rsp
 +	leaq	efi_exit32(%rip), %rbx
 +	subq	%rax, %rbx
 +	movl	%ebx, 8(%rsp)
 +
 +	leaq	__efi64_thunk(%rip), %rbx
  	subq	%rax, %rbx
 +	call	*%rbx
 +
 +	movq	efi_saved_sp(%rip), %rsp
 +	pop	%rbx
 +	pop	%rbp
++<<<<<<< HEAD
 +	retq
 +ENDPROC(efi64_thunk)
 +
 +/*
 + * We run this function from the 1:1 mapping.
 + *
 + * This function must be invoked with a 1:1 mapped stack.
 + */
 +ENTRY(__efi64_thunk)
 +	movl	%ds, %eax
 +	push	%rax
 +	movl	%es, %eax
 +	push	%rax
 +	movl	%ss, %eax
 +	push	%rax
 +
 +	subq	$32, %rsp
 +	movl	%esi, 0x0(%rsp)
 +	movl	%edx, 0x4(%rsp)
 +	movl	%ecx, 0x8(%rsp)
 +	movq	%r8, %rsi
 +	movl	%esi, 0xc(%rsp)
 +	movq	%r9, %rsi
 +	movl	%esi,  0x10(%rsp)
  
 -	subq	$28, %rsp
 -	movl	%ebx, 0x0(%rsp)		/* return address */
 -	movl	%esi, 0x4(%rsp)
 -	movl	%edx, 0x8(%rsp)
 -	movl	%ecx, 0xc(%rsp)
 -	movl	%r8d, 0x10(%rsp)
 -	movl	%r9d, 0x14(%rsp)
 +	leaq	1f(%rip), %rbx
 +	movq	%rbx, func_rt_ptr(%rip)
  
  	/* Switch to 32-bit descriptor */
  	pushq	$__KERNEL32_CS
 -	pushq	%rdi			/* EFI runtime service address */
 +	leaq	efi_enter32(%rip), %rax
 +	pushq	%rax
  	lretq
  
 -1:	movq	24(%rsp), %rsp
 +1:	addq	$32, %rsp
 +
  	pop	%rbx
 -	pop	%rbp
 +	movl	%ebx, %ss
 +	pop	%rbx
 +	movl	%ebx, %es
 +	pop	%rbx
 +	movl	%ebx, %ds
 +
 +	/*
 +	 * Convert 32-bit status code into 64-bit.
 +	 */
 +	test	%rax, %rax
 +	jz	1f
 +	movl	%eax, %ecx
 +	andl	$0x0fffffff, %ecx
 +	andl	$0xf0000000, %eax
 +	shl	$32, %rax
 +	or	%rcx, %rax
 +1:
 +	ret
 +ENDPROC(__efi64_thunk)
 +
 +ENTRY(efi_exit32)
 +	movq	func_rt_ptr(%rip), %rax
 +	push	%rax
 +	mov	%rdi, %rax
 +	ret
 +ENDPROC(efi_exit32)
++=======
+ 	RET
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  	.code32
 -2:	pushl	$__KERNEL_CS
 -	pushl	%ebp
 +/*
 + * EFI service pointer must be in %edi.
 + *
 + * The stack should represent the 32-bit calling convention.
 + */
 +ENTRY(efi_enter32)
 +	movl	$__KERNEL_DS, %eax
 +	movl	%eax, %ds
 +	movl	%eax, %es
 +	movl	%eax, %ss
 +
 +	call	*%edi
 +
 +	/* We must preserve return value */
 +	movl	%eax, %edi
 +
 +	movl	72(%esp), %eax
 +	pushl	$__KERNEL_CS
 +	pushl	%eax
 +
  	lret
 -SYM_CODE_END(__efi64_thunk)
 +ENDPROC(efi_enter32)
  
 -	.bss
 -	.balign 8
 -SYM_DATA(efi_mixed_mode_stack_pa, .quad 0)
 +	.data
 +	.balign	8
 +func_rt_ptr:		.quad 0
 +efi_saved_sp:		.quad 0
diff --cc arch/x86/platform/olpc/xo1-wakeup.S
index 948deb289753,3a5abffe5660..000000000000
--- a/arch/x86/platform/olpc/xo1-wakeup.S
+++ b/arch/x86/platform/olpc/xo1-wakeup.S
@@@ -87,9 -88,9 +87,9 @@@ restore_registers
  	pushl saved_context_eflags
  	popfl
  
- 	ret
+ 	RET
  
 -SYM_CODE_START(do_olpc_suspend_lowlevel)
 +ENTRY(do_olpc_suspend_lowlevel)
  	call	save_processor_state
  	call	save_registers
  
@@@ -108,7 -109,8 +108,12 @@@ ret_point
  
  	call	restore_registers
  	call	restore_processor_state
++<<<<<<< HEAD
 +	ret
++=======
+ 	RET
+ SYM_CODE_END(do_olpc_suspend_lowlevel)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  .data
  saved_gdt:             .long   0,0
diff --cc arch/x86/power/hibernate_asm_32.S
index 1d0fa0e24070,5606a15cf9a1..000000000000
--- a/arch/x86/power/hibernate_asm_32.S
+++ b/arch/x86/power/hibernate_asm_32.S
@@@ -23,13 -25,31 +23,24 @@@ ENTRY(swsusp_arch_suspend
  	pushfl
  	popl saved_context_eflags
  
 -	/* save cr3 */
 -	movl	%cr3, %eax
 -	movl	%eax, restore_cr3
 -
 -	FRAME_BEGIN
  	call swsusp_save
++<<<<<<< HEAD
 +	ret
++=======
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(swsusp_arch_suspend)
+ 
+ SYM_CODE_START(restore_image)
+ 	/* prepare to jump to the image kernel */
+ 	movl	restore_jump_address, %ebx
+ 	movl	restore_cr3, %ebp
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
 +ENTRY(restore_image)
  	movl	mmu_cr4_features, %ecx
 -
 -	/* jump to relocated restore code */
 -	movl	relocated_restore_code, %eax
 -	jmpl	*%eax
 -SYM_CODE_END(restore_image)
 -
 -/* code below has been relocated to a safe page */
 -SYM_CODE_START(core_restore_code)
 -	movl	temp_pgt, %eax
 +	movl	resume_pg_dir, %eax
 +	subl	$__PAGE_OFFSET, %eax
  	movl	%eax, %cr3
  
  	jecxz	1f	# cr4 Pentium and higher, skip if zero
@@@ -81,4 -105,8 +92,12 @@@ done
  
  	xorl	%eax, %eax
  
++<<<<<<< HEAD
 +	ret
++=======
+ 	/* tell the hibernation core that we've just restored the memory */
+ 	movl	%eax, in_suspend
+ 
+ 	RET
+ SYM_FUNC_END(restore_registers)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/power/hibernate_asm_64.S
index ce8da3a0412c,0a0539e1cc81..000000000000
--- a/arch/x86/power/hibernate_asm_64.S
+++ b/arch/x86/power/hibernate_asm_64.S
@@@ -142,5 -66,85 +142,90 @@@ ENTRY(restore_registers
  	/* tell the hibernation core that we've just restored the memory */
  	movq	%rax, in_suspend(%rip)
  
++<<<<<<< HEAD
 +	ret
 +ENDPROC(restore_registers)
++=======
+ 	RET
+ SYM_FUNC_END(restore_registers)
+ 
+ SYM_FUNC_START(swsusp_arch_suspend)
+ 	movq	$saved_context, %rax
+ 	movq	%rsp, pt_regs_sp(%rax)
+ 	movq	%rbp, pt_regs_bp(%rax)
+ 	movq	%rsi, pt_regs_si(%rax)
+ 	movq	%rdi, pt_regs_di(%rax)
+ 	movq	%rbx, pt_regs_bx(%rax)
+ 	movq	%rcx, pt_regs_cx(%rax)
+ 	movq	%rdx, pt_regs_dx(%rax)
+ 	movq	%r8, pt_regs_r8(%rax)
+ 	movq	%r9, pt_regs_r9(%rax)
+ 	movq	%r10, pt_regs_r10(%rax)
+ 	movq	%r11, pt_regs_r11(%rax)
+ 	movq	%r12, pt_regs_r12(%rax)
+ 	movq	%r13, pt_regs_r13(%rax)
+ 	movq	%r14, pt_regs_r14(%rax)
+ 	movq	%r15, pt_regs_r15(%rax)
+ 	pushfq
+ 	popq	pt_regs_flags(%rax)
+ 
+ 	/* save cr3 */
+ 	movq	%cr3, %rax
+ 	movq	%rax, restore_cr3(%rip)
+ 
+ 	FRAME_BEGIN
+ 	call swsusp_save
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(swsusp_arch_suspend)
+ 
+ SYM_FUNC_START(restore_image)
+ 	/* prepare to jump to the image kernel */
+ 	movq	restore_jump_address(%rip), %r8
+ 	movq	restore_cr3(%rip), %r9
+ 
+ 	/* prepare to switch to temporary page tables */
+ 	movq	temp_pgt(%rip), %rax
+ 	movq	mmu_cr4_features(%rip), %rbx
+ 
+ 	/* prepare to copy image data to their original locations */
+ 	movq	restore_pblist(%rip), %rdx
+ 
+ 	/* jump to relocated restore code */
+ 	movq	relocated_restore_code(%rip), %rcx
+ 	ANNOTATE_RETPOLINE_SAFE
+ 	jmpq	*%rcx
+ SYM_FUNC_END(restore_image)
+ 
+ 	/* code below has been relocated to a safe page */
+ SYM_FUNC_START(core_restore_code)
+ 	/* switch to temporary page tables */
+ 	movq	%rax, %cr3
+ 	/* flush TLB */
+ 	movq	%rbx, %rcx
+ 	andq	$~(X86_CR4_PGE), %rcx
+ 	movq	%rcx, %cr4;  # turn off PGE
+ 	movq	%cr3, %rcx;  # flush TLB
+ 	movq	%rcx, %cr3;
+ 	movq	%rbx, %cr4;  # turn PGE back on
+ .Lloop:
+ 	testq	%rdx, %rdx
+ 	jz	.Ldone
+ 
+ 	/* get addresses from the pbe and copy the page */
+ 	movq	pbe_address(%rdx), %rsi
+ 	movq	pbe_orig_address(%rdx), %rdi
+ 	movq	$(PAGE_SIZE >> 3), %rcx
+ 	rep
+ 	movsq
+ 
+ 	/* progress to the next pbe */
+ 	movq	pbe_next(%rdx), %rdx
+ 	jmp	.Lloop
+ 
+ .Ldone:
+ 	/* jump to the restore_registers address from the image header */
+ 	ANNOTATE_RETPOLINE_SAFE
+ 	jmpq	*%r8
+ SYM_FUNC_END(core_restore_code)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
diff --cc arch/x86/xen/xen-asm.S
index eff224df813f,e730e6200e64..000000000000
--- a/arch/x86/xen/xen-asm.S
+++ b/arch/x86/xen/xen-asm.S
@@@ -51,64 -27,10 +51,69 @@@ ENDPATCH(xen_irq_enable_direct
   * Disabling events is simply a matter of making the event mask
   * non-zero.
   */
 -SYM_FUNC_START(xen_irq_disable_direct)
 +ENTRY(xen_irq_disable_direct)
  	movb $1, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
++<<<<<<< HEAD
 +ENDPATCH(xen_irq_disable_direct)
 +	ret
 +	ENDPROC(xen_irq_disable_direct)
 +	RELOC(xen_irq_disable_direct, 0)
 +
 +/*
 + * (xen_)save_fl is used to get the current interrupt enable status.
 + * Callers expect the status to be in X86_EFLAGS_IF, and other bits
 + * may be set in the return value.  We take advantage of this by
 + * making sure that X86_EFLAGS_IF has the right value (and other bits
 + * in that byte are 0), but other bits in the return value are
 + * undefined.  We need to toggle the state of the bit, because Xen and
 + * x86 use opposite senses (mask vs enable).
 + */
 +ENTRY(xen_save_fl_direct)
 +	testb $0xff, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
 +	setz %ah
 +	addb %ah, %ah
 +ENDPATCH(xen_save_fl_direct)
 +	ret
 +	ENDPROC(xen_save_fl_direct)
 +	RELOC(xen_save_fl_direct, 0)
 +
 +
 +/*
 + * In principle the caller should be passing us a value return from
 + * xen_save_fl_direct, but for robustness sake we test only the
 + * X86_EFLAGS_IF flag rather than the whole byte. After setting the
 + * interrupt mask state, it checks for unmasked pending events and
 + * enters the hypervisor to get them delivered if so.
 + */
 +ENTRY(xen_restore_fl_direct)
 +	FRAME_BEGIN
 +#ifdef CONFIG_X86_64
 +	testw $X86_EFLAGS_IF, %di
 +#else
 +	testb $X86_EFLAGS_IF>>8, %ah
 +#endif
 +	setz PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
 +	/*
 +	 * Preempt here doesn't matter because that will deal with any
 +	 * pending interrupts.  The pending check may end up being run
 +	 * on the wrong CPU, but that doesn't hurt.
 +	 */
 +
 +	/* check for unmasked and pending */
 +	cmpw $0x0001, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_pending
 +	jnz 1f
 +2:	call check_events
 +1:
 +ENDPATCH(xen_restore_fl_direct)
 +	FRAME_END
 +	ret
 +	ENDPROC(xen_restore_fl_direct)
 +	RELOC(xen_restore_fl_direct, 2b+1)
 +
++=======
+ 	RET
+ SYM_FUNC_END(xen_irq_disable_direct)
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  
  /*
   * Force an event check by making a hypercall, but preserve regs
@@@ -144,7 -57,243 +149,106 @@@ ENTRY(check_events
  	pop %rdx
  	pop %rcx
  	pop %rax
++<<<<<<< HEAD
++=======
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(check_events)
+ 
+ /*
+  * Enable events.  This clears the event mask and tests the pending
+  * event status with one and operation.  If there are pending events,
+  * then enter the hypervisor to get them handled.
+  */
+ SYM_FUNC_START(xen_irq_enable_direct)
+ 	FRAME_BEGIN
+ 	/* Unmask events */
+ 	movb $0, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
+ 
+ 	/*
+ 	 * Preempt here doesn't matter because that will deal with any
+ 	 * pending interrupts.  The pending check may end up being run
+ 	 * on the wrong CPU, but that doesn't hurt.
+ 	 */
+ 
+ 	/* Test for pending */
+ 	testb $0xff, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_pending
+ 	jz 1f
+ 
+ 	call check_events
+ 1:
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(xen_irq_enable_direct)
+ 
+ /*
+  * (xen_)save_fl is used to get the current interrupt enable status.
+  * Callers expect the status to be in X86_EFLAGS_IF, and other bits
+  * may be set in the return value.  We take advantage of this by
+  * making sure that X86_EFLAGS_IF has the right value (and other bits
+  * in that byte are 0), but other bits in the return value are
+  * undefined.  We need to toggle the state of the bit, because Xen and
+  * x86 use opposite senses (mask vs enable).
+  */
+ SYM_FUNC_START(xen_save_fl_direct)
+ 	testb $0xff, PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_mask
+ 	setz %ah
+ 	addb %ah, %ah
+ 	RET
+ SYM_FUNC_END(xen_save_fl_direct)
+ 
+ SYM_FUNC_START(xen_read_cr2)
+ 	FRAME_BEGIN
+ 	_ASM_MOV PER_CPU_VAR(xen_vcpu), %_ASM_AX
+ 	_ASM_MOV XEN_vcpu_info_arch_cr2(%_ASM_AX), %_ASM_AX
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(xen_read_cr2);
+ 
+ SYM_FUNC_START(xen_read_cr2_direct)
+ 	FRAME_BEGIN
+ 	_ASM_MOV PER_CPU_VAR(xen_vcpu_info) + XEN_vcpu_info_arch_cr2, %_ASM_AX
+ 	FRAME_END
+ 	RET
+ SYM_FUNC_END(xen_read_cr2_direct);
+ .popsection
+ 
+ .macro xen_pv_trap name
+ SYM_CODE_START(xen_\name)
+ 	UNWIND_HINT_EMPTY
+ 	pop %rcx
+ 	pop %r11
+ 	jmp  \name
+ SYM_CODE_END(xen_\name)
+ _ASM_NOKPROBE(xen_\name)
+ .endm
+ 
+ xen_pv_trap asm_exc_divide_error
+ xen_pv_trap asm_xenpv_exc_debug
+ xen_pv_trap asm_exc_int3
+ xen_pv_trap asm_xenpv_exc_nmi
+ xen_pv_trap asm_exc_overflow
+ xen_pv_trap asm_exc_bounds
+ xen_pv_trap asm_exc_invalid_op
+ xen_pv_trap asm_exc_device_not_available
+ xen_pv_trap asm_xenpv_exc_double_fault
+ xen_pv_trap asm_exc_coproc_segment_overrun
+ xen_pv_trap asm_exc_invalid_tss
+ xen_pv_trap asm_exc_segment_not_present
+ xen_pv_trap asm_exc_stack_segment
+ xen_pv_trap asm_exc_general_protection
+ xen_pv_trap asm_exc_page_fault
+ xen_pv_trap asm_exc_spurious_interrupt_bug
+ xen_pv_trap asm_exc_coprocessor_error
+ xen_pv_trap asm_exc_alignment_check
+ #ifdef CONFIG_X86_MCE
+ xen_pv_trap asm_xenpv_exc_machine_check
+ #endif /* CONFIG_X86_MCE */
+ xen_pv_trap asm_exc_simd_coprocessor_error
+ #ifdef CONFIG_IA32_EMULATION
+ xen_pv_trap entry_INT80_compat
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  #endif
 -xen_pv_trap asm_exc_xen_unknown_trap
 -xen_pv_trap asm_exc_xen_hypervisor_callback
 -
 -	__INIT
 -SYM_CODE_START(xen_early_idt_handler_array)
 -	i = 0
 -	.rept NUM_EXCEPTION_VECTORS
 -	UNWIND_HINT_EMPTY
 -	pop %rcx
 -	pop %r11
 -	jmp early_idt_handler_array + i*EARLY_IDT_HANDLER_SIZE
 -	i = i + 1
 -	.fill xen_early_idt_handler_array + i*XEN_EARLY_IDT_HANDLER_SIZE - ., 1, 0xcc
 -	.endr
 -SYM_CODE_END(xen_early_idt_handler_array)
 -	__FINIT
 -
 -hypercall_iret = hypercall_page + __HYPERVISOR_iret * 32
 -/*
 - * Xen64 iret frame:
 - *
 - *	ss
 - *	rsp
 - *	rflags
 - *	cs
 - *	rip		<-- standard iret frame
 - *
 - *	flags
 - *
 - *	rcx		}
 - *	r11		}<-- pushed by hypercall page
 - * rsp->rax		}
 - */
 -SYM_CODE_START(xen_iret)
 -	UNWIND_HINT_EMPTY
 -	pushq $0
 -	jmp hypercall_iret
 -SYM_CODE_END(xen_iret)
 -
 -/*
 - * XEN pv doesn't use trampoline stack, PER_CPU_VAR(cpu_tss_rw + TSS_sp0) is
 - * also the kernel stack.  Reusing swapgs_restore_regs_and_return_to_usermode()
 - * in XEN pv would cause %rsp to move up to the top of the kernel stack and
 - * leave the IRET frame below %rsp, which is dangerous to be corrupted if #NMI
 - * interrupts. And swapgs_restore_regs_and_return_to_usermode() pushing the IRET
 - * frame at the same address is useless.
 - */
 -SYM_CODE_START(xenpv_restore_regs_and_return_to_usermode)
 -	UNWIND_HINT_REGS
 -	POP_REGS
 -
 -	/* stackleak_erase() can work safely on the kernel stack. */
 -	STACKLEAK_ERASE_NOCLOBBER
 -
 -	addq	$8, %rsp	/* skip regs->orig_ax */
 -	jmp xen_iret
 -SYM_CODE_END(xenpv_restore_regs_and_return_to_usermode)
 -
 -/*
 - * Xen handles syscall callbacks much like ordinary exceptions, which
 - * means we have:
 - * - kernel gs
 - * - kernel rsp
 - * - an iret-like stack frame on the stack (including rcx and r11):
 - *	ss
 - *	rsp
 - *	rflags
 - *	cs
 - *	rip
 - *	r11
 - * rsp->rcx
 - */
 -
 -/* Normal 64-bit system call target */
 -SYM_CODE_START(xen_syscall_target)
 -	UNWIND_HINT_EMPTY
 -	popq %rcx
 -	popq %r11
 -
 -	/*
 -	 * Neither Xen nor the kernel really knows what the old SS and
 -	 * CS were.  The kernel expects __USER_DS and __USER_CS, so
 -	 * report those values even though Xen will guess its own values.
 -	 */
 -	movq $__USER_DS, 4*8(%rsp)
 -	movq $__USER_CS, 1*8(%rsp)
 -
 -	jmp entry_SYSCALL_64_after_hwframe
 -SYM_CODE_END(xen_syscall_target)
 -
 -#ifdef CONFIG_IA32_EMULATION
 -
 -/* 32-bit compat syscall target */
 -SYM_CODE_START(xen_syscall32_target)
 -	UNWIND_HINT_EMPTY
 -	popq %rcx
 -	popq %r11
 -
 -	/*
 -	 * Neither Xen nor the kernel really knows what the old SS and
 -	 * CS were.  The kernel expects __USER32_DS and __USER32_CS, so
 -	 * report those values even though Xen will guess its own values.
 -	 */
 -	movq $__USER32_DS, 4*8(%rsp)
 -	movq $__USER32_CS, 1*8(%rsp)
 -
 -	jmp entry_SYSCALL_compat_after_hwframe
 -SYM_CODE_END(xen_syscall32_target)
 -
 -/* 32-bit compat sysenter target */
 -SYM_CODE_START(xen_sysenter_target)
 -	UNWIND_HINT_EMPTY
 -	/*
 -	 * NB: Xen is polite and clears TF from EFLAGS for us.  This means
 -	 * that we don't need to guard against single step exceptions here.
 -	 */
 -	popq %rcx
 -	popq %r11
 -
 -	/*
 -	 * Neither Xen nor the kernel really knows what the old SS and
 -	 * CS were.  The kernel expects __USER32_DS and __USER32_CS, so
 -	 * report those values even though Xen will guess its own values.
 -	 */
 -	movq $__USER32_DS, 4*8(%rsp)
 -	movq $__USER32_CS, 1*8(%rsp)
 -
 -	jmp entry_SYSENTER_compat_after_hwframe
 -SYM_CODE_END(xen_sysenter_target)
 -
 -#else /* !CONFIG_IA32_EMULATION */
 -
 -SYM_CODE_START(xen_syscall32_target)
 -SYM_CODE_START(xen_sysenter_target)
 -	UNWIND_HINT_EMPTY
 -	lea 16(%rsp), %rsp	/* strip %rcx, %r11 */
 -	mov $-ENOSYS, %rax
 -	pushq $0
 -	jmp hypercall_iret
 -SYM_CODE_END(xen_sysenter_target)
 -SYM_CODE_END(xen_syscall32_target)
 -
 -#endif	/* CONFIG_IA32_EMULATION */
 +	FRAME_END
 +	ret
 +ENDPROC(check_events)
diff --cc arch/x86/xen/xen-head.S
index 7faed5869e5b,11d286529fe5..000000000000
--- a/arch/x86/xen/xen-head.S
+++ b/arch/x86/xen/xen-head.S
@@@ -8,82 -9,77 +8,103 @@@
  
  #include <asm/boot.h>
  #include <asm/asm.h>
 -#include <asm/msr.h>
  #include <asm/page_types.h>
 -#include <asm/percpu.h>
 -#include <asm/unwind_hints.h>
  
  #include <xen/interface/elfnote.h>
 -#include <xen/interface/features.h>
 -#include <xen/interface/xen.h>
 -#include <xen/interface/xen-mca.h>
  #include <asm/xen/interface.h>
  
++<<<<<<< HEAD
++=======
+ .pushsection .noinstr.text, "ax"
+ 	.balign PAGE_SIZE
+ SYM_CODE_START(hypercall_page)
+ 	.rept (PAGE_SIZE / 32)
+ 		UNWIND_HINT_FUNC
+ 		.skip 31, 0x90
+ 		RET
+ 	.endr
+ 
+ #define HYPERCALL(n) \
+ 	.equ xen_hypercall_##n, hypercall_page + __HYPERVISOR_##n * 32; \
+ 	.type xen_hypercall_##n, @function; .size xen_hypercall_##n, 32
+ #include <asm/xen-hypercalls.h>
+ #undef HYPERCALL
+ SYM_CODE_END(hypercall_page)
+ .popsection
+ 
+ #ifdef CONFIG_XEN_PV
++>>>>>>> f94909ceb1ed (x86: Prepare asm files for straight-line-speculation)
  	__INIT
 -SYM_CODE_START(startup_xen)
 -	UNWIND_HINT_EMPTY
 +ENTRY(startup_xen)
  	cld
 +#ifdef CONFIG_X86_32
 +	mov %esi,xen_start_info
 +	mov $init_thread_union+THREAD_SIZE,%esp
 +#else
 +	mov %rsi,xen_start_info
 +	mov $init_thread_union+THREAD_SIZE,%rsp
 +#endif
 +	jmp xen_start_kernel
  
 -	/* Clear .bss */
 -	xor %eax,%eax
 -	mov $__bss_start, %rdi
 -	mov $__bss_stop, %rcx
 -	sub %rdi, %rcx
 -	shr $3, %rcx
 -	rep stosq
 -
 -	mov %rsi, xen_start_info
 -	mov initial_stack(%rip), %rsp
 -
 -	/* Set up %gs.
 -	 *
 -	 * The base of %gs always points to fixed_percpu_data.  If the
 -	 * stack protector canary is enabled, it is located at %gs:40.
 -	 * Note that, on SMP, the boot cpu uses init data section until
 -	 * the per cpu areas are set up.
 -	 */
 -	movl	$MSR_GS_BASE,%ecx
 -	movq	$INIT_PER_CPU_VAR(fixed_percpu_data),%rax
 -	cdq
 -	wrmsr
 -
 -	call xen_start_kernel
 -SYM_CODE_END(startup_xen)
  	__FINIT
  
 -#ifdef CONFIG_XEN_PV_SMP
  .pushsection .text
 -SYM_CODE_START(asm_cpu_bringup_and_idle)
 -	UNWIND_HINT_EMPTY
 +	.balign PAGE_SIZE
 +ENTRY(hypercall_page)
 +#define NEXT_HYPERCALL(x) \
 +	ENTRY(xen_hypercall_##x) \
 +	.skip 32
  
 -	call cpu_bringup_and_idle
 -SYM_CODE_END(asm_cpu_bringup_and_idle)
 +NEXT_HYPERCALL(set_trap_table)
 +NEXT_HYPERCALL(mmu_update)
 +NEXT_HYPERCALL(set_gdt)
 +NEXT_HYPERCALL(stack_switch)
 +NEXT_HYPERCALL(set_callbacks)
 +NEXT_HYPERCALL(fpu_taskswitch)
 +NEXT_HYPERCALL(sched_op_compat)
 +NEXT_HYPERCALL(platform_op)
 +NEXT_HYPERCALL(set_debugreg)
 +NEXT_HYPERCALL(get_debugreg)
 +NEXT_HYPERCALL(update_descriptor)
 +NEXT_HYPERCALL(ni)
 +NEXT_HYPERCALL(memory_op)
 +NEXT_HYPERCALL(multicall)
 +NEXT_HYPERCALL(update_va_mapping)
 +NEXT_HYPERCALL(set_timer_op)
 +NEXT_HYPERCALL(event_channel_op_compat)
 +NEXT_HYPERCALL(xen_version)
 +NEXT_HYPERCALL(console_io)
 +NEXT_HYPERCALL(physdev_op_compat)
 +NEXT_HYPERCALL(grant_table_op)
 +NEXT_HYPERCALL(vm_assist)
 +NEXT_HYPERCALL(update_va_mapping_otherdomain)
 +NEXT_HYPERCALL(iret)
 +NEXT_HYPERCALL(vcpu_op)
 +NEXT_HYPERCALL(set_segment_base)
 +NEXT_HYPERCALL(mmuext_op)
 +NEXT_HYPERCALL(xsm_op)
 +NEXT_HYPERCALL(nmi_op)
 +NEXT_HYPERCALL(sched_op)
 +NEXT_HYPERCALL(callback_op)
 +NEXT_HYPERCALL(xenoprof_op)
 +NEXT_HYPERCALL(event_channel_op)
 +NEXT_HYPERCALL(physdev_op)
 +NEXT_HYPERCALL(hvm_op)
 +NEXT_HYPERCALL(sysctl)
 +NEXT_HYPERCALL(domctl)
 +NEXT_HYPERCALL(kexec_op)
 +NEXT_HYPERCALL(tmem_op) /* 38 */
 +ENTRY(xen_hypercall_rsvr)
 +	.skip 320
 +NEXT_HYPERCALL(mca) /* 48 */
 +NEXT_HYPERCALL(arch_1)
 +NEXT_HYPERCALL(arch_2)
 +NEXT_HYPERCALL(arch_3)
 +NEXT_HYPERCALL(arch_4)
 +NEXT_HYPERCALL(arch_5)
 +NEXT_HYPERCALL(arch_6)
 +	.balign PAGE_SIZE
  .popsection
 -#endif
 -#endif
  
  	ELFNOTE(Xen, XEN_ELFNOTE_GUEST_OS,       .asciz "linux")
  	ELFNOTE(Xen, XEN_ELFNOTE_GUEST_VERSION,  .asciz "2.6")
* Unmerged path arch/x86/crypto/aegis128-aesni-asm.S
* Unmerged path arch/x86/crypto/blake2s-core.S
* Unmerged path arch/x86/crypto/chacha-avx2-x86_64.S
* Unmerged path arch/x86/crypto/chacha-avx512vl-x86_64.S
* Unmerged path arch/x86/crypto/chacha-ssse3-x86_64.S
* Unmerged path arch/x86/crypto/des3_ede-asm_64.S
* Unmerged path arch/x86/crypto/nh-avx2-x86_64.S
* Unmerged path arch/x86/crypto/nh-sse2-x86_64.S
* Unmerged path arch/x86/crypto/sm4-aesni-avx-asm_64.S
* Unmerged path arch/x86/crypto/sm4-aesni-avx2-asm_64.S
* Unmerged path arch/x86/entry/entry_32.S
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/entry/thunk_32.S
* Unmerged path arch/x86/entry/thunk_64.S
* Unmerged path arch/x86/entry/vdso/vdso32/system_call.S
* Unmerged path arch/x86/entry/vdso/vsgx.S
* Unmerged path arch/x86/kernel/ftrace_32.S
* Unmerged path arch/x86/kernel/ftrace_64.S
* Unmerged path arch/x86/kernel/irqflags.S
* Unmerged path arch/x86/kernel/sev_verify_cbit.S
* Unmerged path arch/x86/kvm/svm/vmenter.S
* Unmerged path arch/x86/kvm/vmx/vmenter.S
* Unmerged path arch/x86/lib/copy_mc_64.S
* Unmerged path arch/x86/lib/hweight.S
* Unmerged path arch/x86/boot/compressed/efi_thunk_64.S
* Unmerged path arch/x86/boot/compressed/head_64.S
* Unmerged path arch/x86/boot/compressed/mem_encrypt.S
* Unmerged path arch/x86/crypto/aegis128-aesni-asm.S
diff --git a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
index a916c4a61165..d870d3dcc7fd 100644
--- a/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
+++ b/arch/x86/crypto/aes_ctrby8_avx-x86_64.S
@@ -537,7 +537,7 @@ ddq_add_8:
 	/* return updated IV */
 	vpshufb	xbyteswap, xcounter, xcounter
 	vmovdqu	xcounter, (p_iv)
-	ret
+	RET
 .endm
 
 /*
* Unmerged path arch/x86/crypto/aesni-intel_asm.S
* Unmerged path arch/x86/crypto/aesni-intel_avx-x86_64.S
* Unmerged path arch/x86/crypto/blake2s-core.S
* Unmerged path arch/x86/crypto/blowfish-x86_64-asm_64.S
* Unmerged path arch/x86/crypto/camellia-aesni-avx-asm_64.S
* Unmerged path arch/x86/crypto/camellia-aesni-avx2-asm_64.S
* Unmerged path arch/x86/crypto/camellia-x86_64-asm_64.S
* Unmerged path arch/x86/crypto/cast5-avx-x86_64-asm_64.S
* Unmerged path arch/x86/crypto/cast6-avx-x86_64-asm_64.S
* Unmerged path arch/x86/crypto/chacha-avx2-x86_64.S
* Unmerged path arch/x86/crypto/chacha-avx512vl-x86_64.S
* Unmerged path arch/x86/crypto/chacha-ssse3-x86_64.S
* Unmerged path arch/x86/crypto/crc32-pclmul_asm.S
* Unmerged path arch/x86/crypto/crc32c-pcl-intel-asm_64.S
* Unmerged path arch/x86/crypto/crct10dif-pcl-asm_64.S
* Unmerged path arch/x86/crypto/des3_ede-asm_64.S
* Unmerged path arch/x86/crypto/ghash-clmulni-intel_asm.S
* Unmerged path arch/x86/crypto/nh-avx2-x86_64.S
* Unmerged path arch/x86/crypto/nh-sse2-x86_64.S
* Unmerged path arch/x86/crypto/serpent-avx-x86_64-asm_64.S
* Unmerged path arch/x86/crypto/serpent-avx2-asm_64.S
* Unmerged path arch/x86/crypto/serpent-sse2-i586-asm_32.S
* Unmerged path arch/x86/crypto/serpent-sse2-x86_64-asm_64.S
diff --git a/arch/x86/crypto/sha1_avx2_x86_64_asm.S b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
index 1eab79c9ac48..a07468fdbdf0 100644
--- a/arch/x86/crypto/sha1_avx2_x86_64_asm.S
+++ b/arch/x86/crypto/sha1_avx2_x86_64_asm.S
@@ -676,7 +676,7 @@ _loop3:
 	pop	%rbp
 	pop	%rbx
 
-	ret
+	RET
 
 	ENDPROC(\name)
 .endm
* Unmerged path arch/x86/crypto/sha1_ni_asm.S
diff --git a/arch/x86/crypto/sha1_ssse3_asm.S b/arch/x86/crypto/sha1_ssse3_asm.S
index a4109506a5e8..eee433409a12 100644
--- a/arch/x86/crypto/sha1_ssse3_asm.S
+++ b/arch/x86/crypto/sha1_ssse3_asm.S
@@ -104,7 +104,7 @@
 	pop	%r12
 	pop	%rbp
 	pop	%rbx
-	ret
+	RET
 
 	ENDPROC(\name)
 .endm
* Unmerged path arch/x86/crypto/sha256-avx-asm.S
* Unmerged path arch/x86/crypto/sha256-avx2-asm.S
* Unmerged path arch/x86/crypto/sha256-ssse3-asm.S
* Unmerged path arch/x86/crypto/sha256_ni_asm.S
* Unmerged path arch/x86/crypto/sha512-avx-asm.S
* Unmerged path arch/x86/crypto/sha512-avx2-asm.S
* Unmerged path arch/x86/crypto/sha512-ssse3-asm.S
* Unmerged path arch/x86/crypto/sm4-aesni-avx-asm_64.S
* Unmerged path arch/x86/crypto/sm4-aesni-avx2-asm_64.S
* Unmerged path arch/x86/crypto/twofish-avx-x86_64-asm_64.S
* Unmerged path arch/x86/crypto/twofish-i586-asm_32.S
* Unmerged path arch/x86/crypto/twofish-x86_64-asm_64-3way.S
* Unmerged path arch/x86/crypto/twofish-x86_64-asm_64.S
* Unmerged path arch/x86/entry/entry_32.S
* Unmerged path arch/x86/entry/entry_64.S
* Unmerged path arch/x86/entry/thunk_32.S
* Unmerged path arch/x86/entry/thunk_64.S
* Unmerged path arch/x86/entry/vdso/vdso32/system_call.S
* Unmerged path arch/x86/entry/vdso/vsgx.S
* Unmerged path arch/x86/kernel/acpi/wakeup_32.S
* Unmerged path arch/x86/kernel/ftrace_32.S
* Unmerged path arch/x86/kernel/ftrace_64.S
diff --git a/arch/x86/kernel/head_32.S b/arch/x86/kernel/head_32.S
index 24f28c480484..abd2eddb2dca 100644
--- a/arch/x86/kernel/head_32.S
+++ b/arch/x86/kernel/head_32.S
@@ -540,7 +540,7 @@ setup_once:
 #endif
 
 	andl $0,setup_once_ref	/* Once is enough, thanks */
-	ret
+	RET
 
 ENTRY(early_idt_handler_array)
 	# 36(%esp) %eflags
* Unmerged path arch/x86/kernel/irqflags.S
* Unmerged path arch/x86/kernel/relocate_kernel_32.S
* Unmerged path arch/x86/kernel/relocate_kernel_64.S
* Unmerged path arch/x86/kernel/sev_verify_cbit.S
* Unmerged path arch/x86/kernel/verify_cpu.S
diff --git a/arch/x86/kernel/vsyscall_emu_64.S b/arch/x86/kernel/vsyscall_emu_64.S
index c9596a9af159..dfbf8791cf4d 100644
--- a/arch/x86/kernel/vsyscall_emu_64.S
+++ b/arch/x86/kernel/vsyscall_emu_64.S
@@ -20,17 +20,17 @@ __vsyscall_page:
 
 	mov $__NR_gettimeofday, %rax
 	syscall
-	ret
+	RET
 
 	.balign 1024, 0xcc
 	mov $__NR_time, %rax
 	syscall
-	ret
+	RET
 
 	.balign 1024, 0xcc
 	mov $__NR_getcpu, %rax
 	syscall
-	ret
+	RET
 
 	.balign 4096, 0xcc
 
* Unmerged path arch/x86/kvm/svm/vmenter.S
* Unmerged path arch/x86/kvm/vmx/vmenter.S
* Unmerged path arch/x86/lib/atomic64_386_32.S
* Unmerged path arch/x86/lib/atomic64_cx8_32.S
* Unmerged path arch/x86/lib/checksum_32.S
* Unmerged path arch/x86/lib/clear_page_64.S
diff --git a/arch/x86/lib/cmpxchg16b_emu.S b/arch/x86/lib/cmpxchg16b_emu.S
index 1e572c507d06..fbed80fa80b3 100644
--- a/arch/x86/lib/cmpxchg16b_emu.S
+++ b/arch/x86/lib/cmpxchg16b_emu.S
@@ -53,12 +53,12 @@ this_cpu_cmpxchg16b_emu:
 
 	popf
 	mov $1, %al
-	ret
+	RET
 
  not_same:
 	popf
 	xor %al,%al
-	ret
+	RET
 
 CFI_ENDPROC
 
diff --git a/arch/x86/lib/cmpxchg8b_emu.S b/arch/x86/lib/cmpxchg8b_emu.S
index 828cb710dec2..7de8a11785d2 100644
--- a/arch/x86/lib/cmpxchg8b_emu.S
+++ b/arch/x86/lib/cmpxchg8b_emu.S
@@ -43,7 +43,7 @@ cmpxchg8b_emu:
 	movl %ecx, 4(%esi)
 
 	popfl
-	ret
+	RET
 
  not_same:
 	movl  (%esi), %eax
@@ -51,7 +51,7 @@ cmpxchg8b_emu:
 	movl 4(%esi), %edx
 
 	popfl
-	ret
+	RET
 
 CFI_ENDPROC
 ENDPROC(cmpxchg8b_emu)
* Unmerged path arch/x86/lib/copy_mc_64.S
* Unmerged path arch/x86/lib/copy_page_64.S
* Unmerged path arch/x86/lib/copy_user_64.S
* Unmerged path arch/x86/lib/csum-copy_64.S
* Unmerged path arch/x86/lib/getuser.S
* Unmerged path arch/x86/lib/hweight.S
* Unmerged path arch/x86/lib/iomap_copy_64.S
* Unmerged path arch/x86/lib/memcpy_64.S
* Unmerged path arch/x86/lib/memmove_64.S
* Unmerged path arch/x86/lib/memset_64.S
* Unmerged path arch/x86/lib/msr-reg.S
* Unmerged path arch/x86/lib/putuser.S
* Unmerged path arch/x86/lib/retpoline.S
diff --git a/arch/x86/math-emu/div_Xsig.S b/arch/x86/math-emu/div_Xsig.S
index f77ba3058b31..ec60fc7faa2c 100644
--- a/arch/x86/math-emu/div_Xsig.S
+++ b/arch/x86/math-emu/div_Xsig.S
@@ -340,7 +340,7 @@ L_exit:
 	popl	%esi
 
 	leave
-	ret
+	RET
 
 
 #ifdef PARANOID
* Unmerged path arch/x86/math-emu/div_small.S
* Unmerged path arch/x86/math-emu/mul_Xsig.S
* Unmerged path arch/x86/math-emu/polynom_Xsig.S
* Unmerged path arch/x86/math-emu/reg_norm.S
diff --git a/arch/x86/math-emu/reg_round.S b/arch/x86/math-emu/reg_round.S
index d1d4e48b4f67..9aee9e2059df 100644
--- a/arch/x86/math-emu/reg_round.S
+++ b/arch/x86/math-emu/reg_round.S
@@ -436,7 +436,7 @@ fpu_Arith_exit:
 	popl	%edi
 	popl	%esi
 	leave
-	ret
+	RET
 
 
 /*
diff --git a/arch/x86/math-emu/reg_u_add.S b/arch/x86/math-emu/reg_u_add.S
index 47c4c2434d85..dfc70b0a2917 100644
--- a/arch/x86/math-emu/reg_u_add.S
+++ b/arch/x86/math-emu/reg_u_add.S
@@ -163,5 +163,5 @@ L_exit:
 	popl	%edi
 	popl	%esi
 	leave
-	ret
+	RET
 #endif /* PARANOID */
diff --git a/arch/x86/math-emu/reg_u_div.S b/arch/x86/math-emu/reg_u_div.S
index cc00654b6f9a..35d808a54883 100644
--- a/arch/x86/math-emu/reg_u_div.S
+++ b/arch/x86/math-emu/reg_u_div.S
@@ -467,5 +467,5 @@ L_exit:
 	popl	%esi
 
 	leave
-	ret
+	RET
 #endif /* PARANOID */ 
diff --git a/arch/x86/math-emu/reg_u_mul.S b/arch/x86/math-emu/reg_u_mul.S
index 973f12af97df..d2f410808bd1 100644
--- a/arch/x86/math-emu/reg_u_mul.S
+++ b/arch/x86/math-emu/reg_u_mul.S
@@ -143,6 +143,6 @@ L_exit:
 	popl	%edi
 	popl	%esi
 	leave
-	ret
+	RET
 #endif /* PARANOID */ 
 
* Unmerged path arch/x86/math-emu/reg_u_sub.S
* Unmerged path arch/x86/math-emu/round_Xsig.S
* Unmerged path arch/x86/math-emu/shr_Xsig.S
* Unmerged path arch/x86/math-emu/wm_shrx.S
* Unmerged path arch/x86/mm/mem_encrypt_boot.S
* Unmerged path arch/x86/platform/efi/efi_stub_32.S
* Unmerged path arch/x86/platform/efi/efi_stub_64.S
* Unmerged path arch/x86/platform/efi/efi_thunk_64.S
* Unmerged path arch/x86/platform/olpc/xo1-wakeup.S
* Unmerged path arch/x86/power/hibernate_asm_32.S
* Unmerged path arch/x86/power/hibernate_asm_64.S
diff --git a/arch/x86/um/checksum_32.S b/arch/x86/um/checksum_32.S
index 8d0c420465cc..ea414c730668 100644
--- a/arch/x86/um/checksum_32.S
+++ b/arch/x86/um/checksum_32.S
@@ -113,7 +113,7 @@ csum_partial:
 7:	
 	popl %ebx
 	popl %esi
-	ret
+	RET
 
 #else
 
@@ -211,7 +211,7 @@ csum_partial:
 80: 
 	popl %ebx
 	popl %esi
-	ret
+	RET
 				
 #endif
 
diff --git a/arch/x86/um/setjmp_32.S b/arch/x86/um/setjmp_32.S
index b766792c9933..a27b1da72522 100644
--- a/arch/x86/um/setjmp_32.S
+++ b/arch/x86/um/setjmp_32.S
@@ -33,7 +33,7 @@ setjmp:
 	movl %esi,12(%edx)
 	movl %edi,16(%edx)
 	movl %ecx,20(%edx)		# Return address
-	ret
+	RET
 
 	.size setjmp,.-setjmp
 
diff --git a/arch/x86/um/setjmp_64.S b/arch/x86/um/setjmp_64.S
index 45f547b4043e..e97fae42a0d1 100644
--- a/arch/x86/um/setjmp_64.S
+++ b/arch/x86/um/setjmp_64.S
@@ -32,7 +32,7 @@ setjmp:
 	movq %r14,40(%rdi)
 	movq %r15,48(%rdi)
 	movq %rsi,56(%rdi)		# Return address
-	ret
+	RET
 
 	.size setjmp,.-setjmp
 
* Unmerged path arch/x86/xen/xen-asm.S
* Unmerged path arch/x86/xen/xen-head.S
