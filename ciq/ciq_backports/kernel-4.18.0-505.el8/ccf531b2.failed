ice: update VSI instead of init in some case

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-505.el8
commit-author Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
commit ccf531b2d670cf805787e007196a02321b03b68e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-505.el8/ccf531b2.failed

ice_vsi_cfg() is called from different contexts:
1) VSI exsist in HW, but it is reconfigured, because of changing queues
   for example -> update instead of init should be used
2) VSI doesn't exsist, because rest has happened -> init command should
   be sent

To support both cases pass boolean value which will store information
what type of command has to be sent to HW.

	Signed-off-by: Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
	Tested-by: Gurucharan G <gurucharanx.g@intel.com> (A Contingent worker at Intel)
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit ccf531b2d670cf805787e007196a02321b03b68e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_lib.c
#	drivers/net/ethernet/intel/ice/ice_lib.h
#	drivers/net/ethernet/intel/ice/ice_main.c
diff --cc drivers/net/ethernet/intel/ice/ice_lib.c
index 5a7867ad1ffd,1f770a15d1a4..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_lib.c
+++ b/drivers/net/ethernet/intel/ice/ice_lib.c
@@@ -2497,23 -2636,37 +2497,31 @@@ static void ice_set_agg_vsi(struct ice_
  }
  
  /**
 - * ice_free_vf_ctrl_res - Free the VF control VSI resource
 - * @pf: pointer to PF structure
 - * @vsi: the VSI to free resources for
 + * ice_vsi_setup - Set up a VSI by a given type
 + * @pf: board private structure
 + * @pi: pointer to the port_info instance
 + * @vsi_type: VSI type
 + * @vf: pointer to VF to which this VSI connects. This field is used primarily
 + *      for the ICE_VSI_VF type. Other VSI types should pass NULL.
 + * @ch: ptr to channel
++<<<<<<< HEAD
   *
 - * Check if the VF control VSI resource is still in use. If no VF is using it
 - * any more, release the VSI resource. Otherwise, leave it to be cleaned up
 - * once no other VF uses it.
 + * This allocates the sw VSI structure and its queue resources.
 + *
 + * Returns pointer to the successfully allocated and configured VSI sw struct on
 + * success, NULL on failure.
   */
 -static void ice_free_vf_ctrl_res(struct ice_pf *pf,  struct ice_vsi *vsi)
 -{
 -	struct ice_vf *vf;
 -	unsigned int bkt;
 -
 -	rcu_read_lock();
 -	ice_for_each_vf_rcu(pf, bkt, vf) {
 -		if (vf != vsi->vf && vf->ctrl_vsi_idx != ICE_NO_VSI) {
 -			rcu_read_unlock();
 -			return;
 -		}
 -	}
 -	rcu_read_unlock();
 -
 -	/* No other VFs left that have control VSI. It is now safe to reclaim
 -	 * SW interrupts back to the common pool.
 -	 */
 -	ice_free_res(pf->irq_tracker, vsi->base_vector,
 -		     ICE_RES_VF_CTRL_VEC_ID);
 -	pf->num_avail_sw_msix += vsi->num_q_vectors;
 -}
 -
 -static int ice_vsi_cfg_tc_lan(struct ice_pf *pf, struct ice_vsi *vsi)
 +struct ice_vsi *
 +ice_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi,
 +	      enum ice_vsi_type vsi_type, struct ice_vf *vf,
 +	      struct ice_channel *ch)
++=======
++ * @init_vsi: is this an initialization or a reconfigure of the VSI
++ */
++static int
++ice_vsi_cfg_def(struct ice_vsi *vsi, struct ice_vf *vf, struct ice_channel *ch,
++		int init_vsi)
++>>>>>>> ccf531b2d670 (ice: update VSI instead of init in some case)
  {
  	u16 max_txqs[ICE_MAX_TRAFFIC_CLASS] = { 0 };
  	struct device *dev = ice_pf_to_dev(pf);
@@@ -2637,29 -2843,138 +2645,156 @@@
  		goto unroll_vsi_init;
  	}
  
 -	return 0;
 +	/* configure VSI nodes based on number of queues and TC's */
 +	ice_for_each_traffic_class(i) {
 +		if (!(vsi->tc_cfg.ena_tc & BIT(i)))
 +			continue;
  
++<<<<<<< HEAD
 +		if (vsi->type == ICE_VSI_CHNL) {
 +			if (!vsi->alloc_txq && vsi->num_txq)
 +				max_txqs[i] = vsi->num_txq;
 +			else
 +				max_txqs[i] = pf->num_lan_tx;
 +		} else {
 +			max_txqs[i] = vsi->alloc_txq;
 +		}
++=======
+ unroll_vector_base:
+ 	/* reclaim SW interrupts back to the common pool */
+ 	ice_free_res(pf->irq_tracker, vsi->base_vector, vsi->idx);
+ 	pf->num_avail_sw_msix += vsi->num_q_vectors;
+ unroll_alloc_q_vector:
+ 	ice_vsi_free_q_vectors(vsi);
+ unroll_vsi_init:
+ 	ice_vsi_delete_from_hw(vsi);
+ unroll_get_qs:
+ 	ice_vsi_put_qs(vsi);
+ unroll_vsi_alloc_stat:
+ 	ice_vsi_free_stats(vsi);
+ unroll_vsi_alloc:
+ 	ice_vsi_free_arrays(vsi);
+ 	return ret;
+ }
+ 
+ /**
+  * ice_vsi_cfg - configure VSI and tc on it
+  * @vsi: pointer to VSI
+  * @vf: pointer to VF to which this VSI connects. This field is used primarily
+  *      for the ICE_VSI_VF type. Other VSI types should pass NULL.
+  * @ch: ptr to channel
+  * @init_vsi: is this an initialization or a reconfigure of the VSI
+  */
+ int ice_vsi_cfg(struct ice_vsi *vsi, struct ice_vf *vf, struct ice_channel *ch,
+ 		int init_vsi)
+ {
+ 	int ret;
+ 
+ 	ret = ice_vsi_cfg_def(vsi, vf, ch, init_vsi);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = ice_vsi_cfg_tc_lan(vsi->back, vsi);
+ 	if (ret)
+ 		ice_vsi_decfg(vsi);
+ 
+ 	return ret;
+ }
+ 
+ /**
+  * ice_vsi_decfg - remove all VSI configuration
+  * @vsi: pointer to VSI
+  */
+ void ice_vsi_decfg(struct ice_vsi *vsi)
+ {
+ 	struct ice_pf *pf = vsi->back;
+ 	int err;
+ 
+ 	/* The Rx rule will only exist to remove if the LLDP FW
+ 	 * engine is currently stopped
+ 	 */
+ 	if (!ice_is_safe_mode(pf) && vsi->type == ICE_VSI_PF &&
+ 	    !test_bit(ICE_FLAG_FW_LLDP_AGENT, pf->flags))
+ 		ice_cfg_sw_lldp(vsi, false, false);
+ 
+ 	ice_fltr_remove_all(vsi);
+ 	ice_rm_vsi_lan_cfg(vsi->port_info, vsi->idx);
+ 	err = ice_rm_vsi_rdma_cfg(vsi->port_info, vsi->idx);
+ 	if (err)
+ 		dev_err(ice_pf_to_dev(pf), "Failed to remove RDMA scheduler config for VSI %u, err %d\n",
+ 			vsi->vsi_num, err);
+ 
+ 	if (ice_is_xdp_ena_vsi(vsi))
+ 		/* return value check can be skipped here, it always returns
+ 		 * 0 if reset is in progress
+ 		 */
+ 		ice_destroy_xdp_rings(vsi);
+ 
+ 	ice_vsi_clear_rings(vsi);
+ 	ice_vsi_free_q_vectors(vsi);
+ 	ice_vsi_put_qs(vsi);
+ 	ice_vsi_free_arrays(vsi);
+ 
+ 	/* SR-IOV determines needed MSIX resources all at once instead of per
+ 	 * VSI since when VFs are spawned we know how many VFs there are and how
+ 	 * many interrupts each VF needs. SR-IOV MSIX resources are also
+ 	 * cleared in the same manner.
+ 	 */
+ 	if (vsi->type == ICE_VSI_CTRL && vsi->vf) {
+ 		ice_free_vf_ctrl_res(pf, vsi);
+ 	} else if (vsi->type != ICE_VSI_VF) {
+ 		/* reclaim SW interrupts back to the common pool */
+ 		ice_free_res(pf->irq_tracker, vsi->base_vector, vsi->idx);
+ 		pf->num_avail_sw_msix += vsi->num_q_vectors;
+ 		vsi->base_vector = 0;
++>>>>>>> ccf531b2d670 (ice: update VSI instead of init in some case)
  	}
  
 -	if (vsi->type == ICE_VSI_VF &&
 -	    vsi->agg_node && vsi->agg_node->valid)
 -		vsi->agg_node->num_vsis--;
 -	if (vsi->agg_node) {
 -		vsi->agg_node->valid = false;
 -		vsi->agg_node->agg_id = 0;
 +	dev_dbg(dev, "vsi->tc_cfg.ena_tc = %d\n", vsi->tc_cfg.ena_tc);
 +	ret = ice_cfg_vsi_lan(vsi->port_info, vsi->idx, vsi->tc_cfg.ena_tc,
 +			      max_txqs);
 +	if (ret) {
 +		dev_err(dev, "VSI %d failed lan queue config, error %d\n",
 +			vsi->vsi_num, ret);
 +		goto unroll_clear_rings;
  	}
++<<<<<<< HEAD
++=======
+ }
+ 
+ /**
+  * ice_vsi_setup - Set up a VSI by a given type
+  * @pf: board private structure
+  * @pi: pointer to the port_info instance
+  * @vsi_type: VSI type
+  * @vf: pointer to VF to which this VSI connects. This field is used primarily
+  *      for the ICE_VSI_VF type. Other VSI types should pass NULL.
+  * @ch: ptr to channel
+  *
+  * This allocates the sw VSI structure and its queue resources.
+  *
+  * Returns pointer to the successfully allocated and configured VSI sw struct on
+  * success, NULL on failure.
+  */
+ struct ice_vsi *
+ ice_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi,
+ 	      enum ice_vsi_type vsi_type, struct ice_vf *vf,
+ 	      struct ice_channel *ch)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_vsi *vsi;
+ 	int ret;
+ 
+ 	vsi = ice_vsi_alloc(pf, pi, vsi_type, ch, vf);
+ 	if (!vsi) {
+ 		dev_err(dev, "could not allocate VSI\n");
+ 		return NULL;
+ 	}
+ 
+ 	ret = ice_vsi_cfg(vsi, vf, ch, ICE_VSI_FLAG_INIT);
+ 	if (ret)
+ 		goto err_vsi_cfg;
++>>>>>>> ccf531b2d670 (ice: update VSI instead of init in some case)
  
  	/* Add switch rule to drop all Tx Flow Control Frames, of look up
  	 * type ETHERTYPE from VSIs, and restrict malicious VF from sending
@@@ -3239,156 -3502,21 +3374,164 @@@ int ice_vsi_rebuild(struct ice_vsi *vsi
  
  	prev_num_q_vectors = ice_vsi_rebuild_get_coalesce(vsi, coalesce);
  
++<<<<<<< HEAD
 +	ice_rm_vsi_lan_cfg(vsi->port_info, vsi->idx);
 +	ret = ice_rm_vsi_rdma_cfg(vsi->port_info, vsi->idx);
++=======
+ 	prev_txq = vsi->num_txq;
+ 	prev_rxq = vsi->num_rxq;
+ 
+ 	ice_vsi_decfg(vsi);
+ 	ret = ice_vsi_cfg_def(vsi, vsi->vf, vsi->ch, init_vsi);
++>>>>>>> ccf531b2d670 (ice: update VSI instead of init in some case)
  	if (ret)
 -		goto err_vsi_cfg;
 +		dev_err(ice_pf_to_dev(vsi->back), "Failed to remove RDMA scheduler config for VSI %u, err %d\n",
 +			vsi->vsi_num, ret);
 +	ice_vsi_free_q_vectors(vsi);
 +
 +	/* SR-IOV determines needed MSIX resources all at once instead of per
 +	 * VSI since when VFs are spawned we know how many VFs there are and how
 +	 * many interrupts each VF needs. SR-IOV MSIX resources are also
 +	 * cleared in the same manner.
 +	 */
 +	if (vtype != ICE_VSI_VF) {
 +		/* reclaim SW interrupts back to the common pool */
 +		ice_free_res(pf->irq_tracker, vsi->base_vector, vsi->idx);
 +		pf->num_avail_sw_msix += vsi->num_q_vectors;
 +		vsi->base_vector = 0;
 +	}
 +
 +	if (ice_is_xdp_ena_vsi(vsi))
 +		/* return value check can be skipped here, it always returns
 +		 * 0 if reset is in progress
 +		 */
 +		ice_destroy_xdp_rings(vsi);
 +	ice_vsi_put_qs(vsi);
 +	ice_vsi_clear_rings(vsi);
 +	ice_vsi_free_arrays(vsi);
 +	if (vtype == ICE_VSI_VF)
 +		ice_vsi_set_num_qs(vsi, vsi->vf);
 +	else
 +		ice_vsi_set_num_qs(vsi, NULL);
 +
 +	ret = ice_vsi_alloc_arrays(vsi);
 +	if (ret < 0)
 +		goto err_vsi;
 +
 +	ice_vsi_get_qs(vsi);
 +
 +	ice_alloc_fd_res(vsi);
 +	ice_vsi_set_tc_cfg(vsi);
 +
 +	/* Initialize VSI struct elements and create VSI in FW */
 +	ret = ice_vsi_init(vsi, init_vsi);
 +	if (ret < 0)
 +		goto err_vsi;
 +
 +	switch (vtype) {
 +	case ICE_VSI_CTRL:
 +	case ICE_VSI_SWITCHDEV_CTRL:
 +	case ICE_VSI_PF:
 +		ret = ice_vsi_alloc_q_vectors(vsi);
 +		if (ret)
 +			goto err_rings;
 +
 +		ret = ice_vsi_setup_vector_base(vsi);
 +		if (ret)
 +			goto err_vectors;
 +
 +		ret = ice_vsi_set_q_vectors_reg_idx(vsi);
 +		if (ret)
 +			goto err_vectors;
 +
 +		ret = ice_vsi_alloc_rings(vsi);
 +		if (ret)
 +			goto err_vectors;
 +
 +		ice_vsi_map_rings_to_vectors(vsi);
 +
 +		vsi->stat_offsets_loaded = false;
 +		if (ice_is_xdp_ena_vsi(vsi)) {
 +			ret = ice_vsi_determine_xdp_res(vsi);
 +			if (ret)
 +				goto err_vectors;
 +			ret = ice_prepare_xdp_rings(vsi, vsi->xdp_prog);
 +			if (ret)
 +				goto err_vectors;
 +		}
 +		/* ICE_VSI_CTRL does not need RSS so skip RSS processing */
 +		if (vtype != ICE_VSI_CTRL)
 +			/* Do not exit if configuring RSS had an issue, at
 +			 * least receive traffic on first queue. Hence no
 +			 * need to capture return value
 +			 */
 +			if (test_bit(ICE_FLAG_RSS_ENA, pf->flags))
 +				ice_vsi_cfg_rss_lut_key(vsi);
 +
 +		/* disable or enable CRC stripping */
 +		if (vsi->netdev)
 +			ice_vsi_cfg_crc_strip(vsi, !!(vsi->netdev->features &
 +					      NETIF_F_RXFCS));
 +
 +		break;
 +	case ICE_VSI_VF:
 +		ret = ice_vsi_alloc_q_vectors(vsi);
 +		if (ret)
 +			goto err_rings;
 +
 +		ret = ice_vsi_set_q_vectors_reg_idx(vsi);
 +		if (ret)
 +			goto err_vectors;
 +
 +		ret = ice_vsi_alloc_rings(vsi);
 +		if (ret)
 +			goto err_vectors;
 +
 +		vsi->stat_offsets_loaded = false;
 +		break;
 +	case ICE_VSI_CHNL:
 +		if (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) {
 +			ice_vsi_cfg_rss_lut_key(vsi);
 +			ice_vsi_set_rss_flow_fld(vsi);
 +		}
 +		break;
 +	default:
 +		break;
 +	}
 +
 +	/* configure VSI nodes based on number of queues and TC's */
 +	for (i = 0; i < vsi->tc_cfg.numtc; i++) {
 +		/* configure VSI nodes based on number of queues and TC's.
 +		 * ADQ creates VSIs for each TC/Channel but doesn't
 +		 * allocate queues instead it reconfigures the PF queues
 +		 * as per the TC command. So max_txqs should point to the
 +		 * PF Tx queues.
 +		 */
 +		if (vtype == ICE_VSI_CHNL)
 +			max_txqs[i] = pf->num_lan_tx;
 +		else
 +			max_txqs[i] = vsi->alloc_txq;
 +
 +		if (ice_is_xdp_ena_vsi(vsi))
 +			max_txqs[i] += vsi->num_xdp_txq;
 +	}
 +
 +	if (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))
 +		/* If MQPRIO is set, means channel code path, hence for main
 +		 * VSI's, use TC as 1
 +		 */
 +		ret = ice_cfg_vsi_lan(vsi->port_info, vsi->idx, 1, max_txqs);
 +	else
 +		ret = ice_cfg_vsi_lan(vsi->port_info, vsi->idx,
 +				      vsi->tc_cfg.ena_tc, max_txqs);
  
 -	ret = ice_vsi_cfg_tc_lan(pf, vsi);
  	if (ret) {
 -		if (init_vsi & ICE_VSI_FLAG_INIT) {
 +		dev_err(ice_pf_to_dev(pf), "VSI %d failed lan queue config, error %d\n",
 +			vsi->vsi_num, ret);
 +		if (init_vsi) {
  			ret = -EIO;
 -			goto err_vsi_cfg_tc_lan;
 +			goto err_vectors;
  		} else {
 -			kfree(coalesce);
  			return ice_schedule_reset(pf, ICE_RESET_PFR);
  		}
  	}
diff --cc drivers/net/ethernet/intel/ice/ice_lib.h
index dcdf69a693e9,b76f05e1f8a3..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_lib.h
+++ b/drivers/net/ethernet/intel/ice/ice_lib.h
@@@ -70,7 -70,11 +70,15 @@@ int ice_free_res(struct ice_res_tracke
  int
  ice_get_res(struct ice_pf *pf, struct ice_res_tracker *res, u16 needed, u16 id);
  
++<<<<<<< HEAD
 +int ice_vsi_rebuild(struct ice_vsi *vsi, bool init_vsi);
++=======
+ #define ICE_VSI_FLAG_INIT	BIT(0)
+ #define ICE_VSI_FLAG_NO_INIT	0
+ int ice_vsi_rebuild(struct ice_vsi *vsi, int init_vsi);
+ int ice_vsi_cfg(struct ice_vsi *vsi, struct ice_vf *vf,
+ 		struct ice_channel *ch, int init_vsi);
++>>>>>>> ccf531b2d670 (ice: update VSI instead of init in some case)
  
  bool ice_is_reset_in_progress(unsigned long *state);
  int ice_wait_for_reset(struct ice_pf *pf, unsigned long timeout);
diff --cc drivers/net/ethernet/intel/ice/ice_main.c
index 4662285d8d28,433298d0014a..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_main.c
+++ b/drivers/net/ethernet/intel/ice/ice_main.c
@@@ -4654,6 -4616,445 +4654,448 @@@ err_devlink_create
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static void ice_deinit_eth(struct ice_pf *pf)
+ {
+ 	struct ice_vsi *vsi = ice_get_main_vsi(pf);
+ 
+ 	if (!vsi)
+ 		return;
+ 
+ 	ice_vsi_close(vsi);
+ 	ice_unregister_netdev(vsi);
+ 	ice_devlink_destroy_pf_port(pf);
+ 	ice_tc_indir_block_unregister(vsi);
+ 	ice_decfg_netdev(vsi);
+ }
+ 
+ static int ice_init_dev(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_hw *hw = &pf->hw;
+ 	int err;
+ 
+ 	err = ice_init_hw(hw);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_hw failed: %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	ice_init_feature_support(pf);
+ 
+ 	ice_request_fw(pf);
+ 
+ 	/* if ice_request_fw fails, ICE_FLAG_ADV_FEATURES bit won't be
+ 	 * set in pf->state, which will cause ice_is_safe_mode to return
+ 	 * true
+ 	 */
+ 	if (ice_is_safe_mode(pf)) {
+ 		/* we already got function/device capabilities but these don't
+ 		 * reflect what the driver needs to do in safe mode. Instead of
+ 		 * adding conditional logic everywhere to ignore these
+ 		 * device/function capabilities, override them.
+ 		 */
+ 		ice_set_safe_mode_caps(hw);
+ 	}
+ 
+ 	err = ice_init_pf(pf);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_pf failed: %d\n", err);
+ 		goto err_init_pf;
+ 	}
+ 
+ 	pf->hw.udp_tunnel_nic.set_port = ice_udp_tunnel_set_port;
+ 	pf->hw.udp_tunnel_nic.unset_port = ice_udp_tunnel_unset_port;
+ 	pf->hw.udp_tunnel_nic.flags = UDP_TUNNEL_NIC_INFO_MAY_SLEEP;
+ 	pf->hw.udp_tunnel_nic.shared = &pf->hw.udp_tunnel_shared;
+ 	if (pf->hw.tnl.valid_count[TNL_VXLAN]) {
+ 		pf->hw.udp_tunnel_nic.tables[0].n_entries =
+ 			pf->hw.tnl.valid_count[TNL_VXLAN];
+ 		pf->hw.udp_tunnel_nic.tables[0].tunnel_types =
+ 			UDP_TUNNEL_TYPE_VXLAN;
+ 	}
+ 	if (pf->hw.tnl.valid_count[TNL_GENEVE]) {
+ 		pf->hw.udp_tunnel_nic.tables[1].n_entries =
+ 			pf->hw.tnl.valid_count[TNL_GENEVE];
+ 		pf->hw.udp_tunnel_nic.tables[1].tunnel_types =
+ 			UDP_TUNNEL_TYPE_GENEVE;
+ 	}
+ 
+ 	err = ice_init_interrupt_scheme(pf);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_interrupt_scheme failed: %d\n", err);
+ 		err = -EIO;
+ 		goto err_init_interrupt_scheme;
+ 	}
+ 
+ 	/* In case of MSIX we are going to setup the misc vector right here
+ 	 * to handle admin queue events etc. In case of legacy and MSI
+ 	 * the misc functionality and queue processing is combined in
+ 	 * the same vector and that gets setup at open.
+ 	 */
+ 	err = ice_req_irq_msix_misc(pf);
+ 	if (err) {
+ 		dev_err(dev, "setup of misc vector failed: %d\n", err);
+ 		goto err_req_irq_msix_misc;
+ 	}
+ 
+ 	return 0;
+ 
+ err_req_irq_msix_misc:
+ 	ice_clear_interrupt_scheme(pf);
+ err_init_interrupt_scheme:
+ 	ice_deinit_pf(pf);
+ err_init_pf:
+ 	ice_deinit_hw(hw);
+ 	return err;
+ }
+ 
+ static void ice_deinit_dev(struct ice_pf *pf)
+ {
+ 	ice_free_irq_msix_misc(pf);
+ 	ice_clear_interrupt_scheme(pf);
+ 	ice_deinit_pf(pf);
+ 	ice_deinit_hw(&pf->hw);
+ }
+ 
+ static void ice_init_features(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	if (ice_is_safe_mode(pf))
+ 		return;
+ 
+ 	/* initialize DDP driven features */
+ 	if (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))
+ 		ice_ptp_init(pf);
+ 
+ 	if (ice_is_feature_supported(pf, ICE_F_GNSS))
+ 		ice_gnss_init(pf);
+ 
+ 	/* Note: Flow director init failure is non-fatal to load */
+ 	if (ice_init_fdir(pf))
+ 		dev_err(dev, "could not initialize flow director\n");
+ 
+ 	/* Note: DCB init failure is non-fatal to load */
+ 	if (ice_init_pf_dcb(pf, false)) {
+ 		clear_bit(ICE_FLAG_DCB_CAPABLE, pf->flags);
+ 		clear_bit(ICE_FLAG_DCB_ENA, pf->flags);
+ 	} else {
+ 		ice_cfg_lldp_mib_change(&pf->hw, true);
+ 	}
+ 
+ 	if (ice_init_lag(pf))
+ 		dev_warn(dev, "Failed to init link aggregation support\n");
+ }
+ 
+ static void ice_deinit_features(struct ice_pf *pf)
+ {
+ 	ice_deinit_lag(pf);
+ 	if (test_bit(ICE_FLAG_DCB_CAPABLE, pf->flags))
+ 		ice_cfg_lldp_mib_change(&pf->hw, false);
+ 	ice_deinit_fdir(pf);
+ 	if (ice_is_feature_supported(pf, ICE_F_GNSS))
+ 		ice_gnss_exit(pf);
+ 	if (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))
+ 		ice_ptp_release(pf);
+ }
+ 
+ static void ice_init_wakeup(struct ice_pf *pf)
+ {
+ 	/* Save wakeup reason register for later use */
+ 	pf->wakeup_reason = rd32(&pf->hw, PFPM_WUS);
+ 
+ 	/* check for a power management event */
+ 	ice_print_wake_reason(pf);
+ 
+ 	/* clear wake status, all bits */
+ 	wr32(&pf->hw, PFPM_WUS, U32_MAX);
+ 
+ 	/* Disable WoL at init, wait for user to enable */
+ 	device_set_wakeup_enable(ice_pf_to_dev(pf), false);
+ }
+ 
+ static int ice_init_link(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	err = ice_init_link_events(pf->hw.port_info);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_link_events failed: %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	/* not a fatal error if this fails */
+ 	err = ice_init_nvm_phy_type(pf->hw.port_info);
+ 	if (err)
+ 		dev_err(dev, "ice_init_nvm_phy_type failed: %d\n", err);
+ 
+ 	/* not a fatal error if this fails */
+ 	err = ice_update_link_info(pf->hw.port_info);
+ 	if (err)
+ 		dev_err(dev, "ice_update_link_info failed: %d\n", err);
+ 
+ 	ice_init_link_dflt_override(pf->hw.port_info);
+ 
+ 	ice_check_link_cfg_err(pf,
+ 			       pf->hw.port_info->phy.link_info.link_cfg_err);
+ 
+ 	/* if media available, initialize PHY settings */
+ 	if (pf->hw.port_info->phy.link_info.link_info &
+ 	    ICE_AQ_MEDIA_AVAILABLE) {
+ 		/* not a fatal error if this fails */
+ 		err = ice_init_phy_user_cfg(pf->hw.port_info);
+ 		if (err)
+ 			dev_err(dev, "ice_init_phy_user_cfg failed: %d\n", err);
+ 
+ 		if (!test_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, pf->flags)) {
+ 			struct ice_vsi *vsi = ice_get_main_vsi(pf);
+ 
+ 			if (vsi)
+ 				ice_configure_phy(vsi);
+ 		}
+ 	} else {
+ 		set_bit(ICE_FLAG_NO_MEDIA, pf->flags);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int ice_init_pf_sw(struct ice_pf *pf)
+ {
+ 	bool dvm = ice_is_dvm_ena(&pf->hw);
+ 	struct ice_vsi *vsi;
+ 	int err;
+ 
+ 	/* create switch struct for the switch element created by FW on boot */
+ 	pf->first_sw = kzalloc(sizeof(*pf->first_sw), GFP_KERNEL);
+ 	if (!pf->first_sw)
+ 		return -ENOMEM;
+ 
+ 	if (pf->hw.evb_veb)
+ 		pf->first_sw->bridge_mode = BRIDGE_MODE_VEB;
+ 	else
+ 		pf->first_sw->bridge_mode = BRIDGE_MODE_VEPA;
+ 
+ 	pf->first_sw->pf = pf;
+ 
+ 	/* record the sw_id available for later use */
+ 	pf->first_sw->sw_id = pf->hw.port_info->sw_id;
+ 
+ 	err = ice_aq_set_port_params(pf->hw.port_info, dvm, NULL);
+ 	if (err)
+ 		goto err_aq_set_port_params;
+ 
+ 	vsi = ice_pf_vsi_setup(pf, pf->hw.port_info);
+ 	if (!vsi) {
+ 		err = -ENOMEM;
+ 		goto err_pf_vsi_setup;
+ 	}
+ 
+ 	return 0;
+ 
+ err_pf_vsi_setup:
+ err_aq_set_port_params:
+ 	kfree(pf->first_sw);
+ 	return err;
+ }
+ 
+ static void ice_deinit_pf_sw(struct ice_pf *pf)
+ {
+ 	struct ice_vsi *vsi = ice_get_main_vsi(pf);
+ 
+ 	if (!vsi)
+ 		return;
+ 
+ 	ice_vsi_release(vsi);
+ 	kfree(pf->first_sw);
+ }
+ 
+ static int ice_alloc_vsis(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	pf->num_alloc_vsi = pf->hw.func_caps.guar_num_vsi;
+ 	if (!pf->num_alloc_vsi)
+ 		return -EIO;
+ 
+ 	if (pf->num_alloc_vsi > UDP_TUNNEL_NIC_MAX_SHARING_DEVICES) {
+ 		dev_warn(dev,
+ 			 "limiting the VSI count due to UDP tunnel limitation %d > %d\n",
+ 			 pf->num_alloc_vsi, UDP_TUNNEL_NIC_MAX_SHARING_DEVICES);
+ 		pf->num_alloc_vsi = UDP_TUNNEL_NIC_MAX_SHARING_DEVICES;
+ 	}
+ 
+ 	pf->vsi = devm_kcalloc(dev, pf->num_alloc_vsi, sizeof(*pf->vsi),
+ 			       GFP_KERNEL);
+ 	if (!pf->vsi)
+ 		return -ENOMEM;
+ 
+ 	pf->vsi_stats = devm_kcalloc(dev, pf->num_alloc_vsi,
+ 				     sizeof(*pf->vsi_stats), GFP_KERNEL);
+ 	if (!pf->vsi_stats) {
+ 		devm_kfree(dev, pf->vsi);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void ice_dealloc_vsis(struct ice_pf *pf)
+ {
+ 	devm_kfree(ice_pf_to_dev(pf), pf->vsi_stats);
+ 	pf->vsi_stats = NULL;
+ 
+ 	pf->num_alloc_vsi = 0;
+ 	devm_kfree(ice_pf_to_dev(pf), pf->vsi);
+ 	pf->vsi = NULL;
+ }
+ 
+ static int ice_init_devlink(struct ice_pf *pf)
+ {
+ 	int err;
+ 
+ 	err = ice_devlink_register_params(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	ice_devlink_init_regions(pf);
+ 	ice_devlink_register(pf);
+ 
+ 	return 0;
+ }
+ 
+ static void ice_deinit_devlink(struct ice_pf *pf)
+ {
+ 	ice_devlink_unregister(pf);
+ 	ice_devlink_destroy_regions(pf);
+ 	ice_devlink_unregister_params(pf);
+ }
+ 
+ static int ice_init(struct ice_pf *pf)
+ {
+ 	int err;
+ 
+ 	err = ice_init_dev(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ice_alloc_vsis(pf);
+ 	if (err)
+ 		goto err_alloc_vsis;
+ 
+ 	err = ice_init_pf_sw(pf);
+ 	if (err)
+ 		goto err_init_pf_sw;
+ 
+ 	ice_init_wakeup(pf);
+ 
+ 	err = ice_init_link(pf);
+ 	if (err)
+ 		goto err_init_link;
+ 
+ 	err = ice_send_version(pf);
+ 	if (err)
+ 		goto err_init_link;
+ 
+ 	ice_verify_cacheline_size(pf);
+ 
+ 	if (ice_is_safe_mode(pf))
+ 		ice_set_safe_mode_vlan_cfg(pf);
+ 	else
+ 		/* print PCI link speed and width */
+ 		pcie_print_link_status(pf->pdev);
+ 
+ 	/* ready to go, so clear down state bit */
+ 	clear_bit(ICE_DOWN, pf->state);
+ 	clear_bit(ICE_SERVICE_DIS, pf->state);
+ 
+ 	/* since everything is good, start the service timer */
+ 	mod_timer(&pf->serv_tmr, round_jiffies(jiffies + pf->serv_tmr_period));
+ 
+ 	return 0;
+ 
+ err_init_link:
+ 	ice_deinit_pf_sw(pf);
+ err_init_pf_sw:
+ 	ice_dealloc_vsis(pf);
+ err_alloc_vsis:
+ 	ice_deinit_dev(pf);
+ 	return err;
+ }
+ 
+ static void ice_deinit(struct ice_pf *pf)
+ {
+ 	set_bit(ICE_SERVICE_DIS, pf->state);
+ 	set_bit(ICE_DOWN, pf->state);
+ 
+ 	ice_deinit_pf_sw(pf);
+ 	ice_dealloc_vsis(pf);
+ 	ice_deinit_dev(pf);
+ }
+ 
+ /**
+  * ice_load - load pf by init hw and starting VSI
+  * @pf: pointer to the pf instance
+  */
+ int ice_load(struct ice_pf *pf)
+ {
+ 	struct ice_vsi *vsi;
+ 	int err;
+ 
+ 	err = ice_reset(&pf->hw, ICE_RESET_PFR);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ice_init_dev(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	vsi = ice_get_main_vsi(pf);
+ 	err = ice_vsi_cfg(vsi, NULL, NULL, ICE_VSI_FLAG_INIT);
+ 	if (err)
+ 		goto err_vsi_cfg;
+ 
+ 	err = ice_start_eth(ice_get_main_vsi(pf));
+ 	if (err)
+ 		goto err_start_eth;
+ 
+ 	err = ice_init_rdma(pf);
+ 	if (err)
+ 		goto err_init_rdma;
+ 
+ 	ice_init_features(pf);
+ 	ice_service_task_restart(pf);
+ 
+ 	clear_bit(ICE_DOWN, pf->state);
+ 
+ 	return 0;
+ 
+ err_init_rdma:
+ 	ice_vsi_close(ice_get_main_vsi(pf));
+ err_start_eth:
+ 	ice_vsi_decfg(ice_get_main_vsi(pf));
+ err_vsi_cfg:
+ 	ice_deinit_dev(pf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_unload - unload pf by stopping VSI and deinit hw
+  * @pf: pointer to the pf instance
+  */
+ void ice_unload(struct ice_pf *pf)
+ {
+ 	ice_deinit_features(pf);
+ 	ice_deinit_rdma(pf);
+ 	ice_vsi_close(ice_get_main_vsi(pf));
+ 	ice_vsi_decfg(ice_get_main_vsi(pf));
+ 	ice_deinit_dev(pf);
+ }
+ 
++>>>>>>> ccf531b2d670 (ice: update VSI instead of init in some case)
  /**
   * ice_probe - Device initialization routine
   * @pdev: PCI device information struct
* Unmerged path drivers/net/ethernet/intel/ice/ice_lib.c
* Unmerged path drivers/net/ethernet/intel/ice/ice_lib.h
* Unmerged path drivers/net/ethernet/intel/ice/ice_main.c
