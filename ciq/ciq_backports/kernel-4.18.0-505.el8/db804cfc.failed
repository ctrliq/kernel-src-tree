ice: Use the xsk batched rx allocation interface

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-505.el8
commit-author Magnus Karlsson <magnus.karlsson@intel.com>
commit db804cfc21e969a5a4ada4b8142f711def5ed339
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-505.el8/db804cfc.failed

Use the new xsk batched rx allocation interface for the zero-copy data
path. As the array of struct xdp_buff pointers kept by the driver is
really a ring that wraps, the allocation routine is modified to detect
a wrap and in that case call the allocation function twice. The
allocation function cannot deal with wrapped rings, only arrays. As we
now know exactly how many buffers we get and that there is no
wrapping, the allocation function can be simplified even more as all
if-statements in the allocation loop can be removed, improving
performance.

	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
Link: https://lore.kernel.org/bpf/20210922075613.12186-5-magnus.karlsson@gmail.com
(cherry picked from commit db804cfc21e969a5a4ada4b8142f711def5ed339)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_xsk.c
diff --cc drivers/net/ethernet/intel/ice/ice_xsk.c
index 71d9a26e480a,7682eaa9a9ec..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_xsk.c
+++ b/drivers/net/ethernet/intel/ice/ice_xsk.c
@@@ -438,25 -365,21 +438,30 @@@ bool ice_alloc_rx_bufs_zc(struct ice_rx
  	union ice_32b_rx_flex_desc *rx_desc;
  	u16 ntu = rx_ring->next_to_use;
  	struct xdp_buff **xdp;
- 	bool ok = true;
+ 	u32 nb_buffs, i;
  	dma_addr_t dma;
  
- 	if (!count)
- 		return true;
- 
  	rx_desc = ICE_RX_DESC(rx_ring, ntu);
 -	xdp = &rx_ring->xdp_buf[ntu];
 +	xdp = ice_xdp_buf(rx_ring, ntu);
  
++<<<<<<< HEAD
 +	do {
 +		*xdp = xsk_buff_alloc(rx_ring->xsk_pool);
 +		if (!*xdp) {
 +			ok = false;
 +			break;
 +		}
- 
++=======
+ 	nb_buffs = min_t(u16, count, rx_ring->count - ntu);
+ 	nb_buffs = xsk_buff_alloc_batch(rx_ring->xsk_pool, xdp, nb_buffs);
+ 	if (!nb_buffs)
+ 		return false;
++>>>>>>> db804cfc21e9 (ice: Use the xsk batched rx allocation interface)
+ 
+ 	i = nb_buffs;
+ 	while (i--) {
  		dma = xsk_buff_xdp_get_dma(*xdp);
  		rx_desc->read.pkt_addr = cpu_to_le64(dma);
- 		rx_desc->wb.status_error0 = 0;
  
  		rx_desc++;
  		xdp++;
@@@ -616,25 -533,16 +619,31 @@@ int ice_clean_rx_irq_zc(struct ice_rx_r
  		 */
  		dma_rmb();
  
 -		size = le16_to_cpu(rx_desc->wb.pkt_len) &
 -				   ICE_RX_FLX_DESC_PKT_LEN_M;
 -		if (!size)
 +		if (unlikely(rx_ring->next_to_clean == rx_ring->next_to_use))
  			break;
  
++<<<<<<< HEAD
 +		xdp = *ice_xdp_buf(rx_ring, rx_ring->next_to_clean);
++=======
+ 		xdp = &rx_ring->xdp_buf[rx_ring->next_to_clean];
+ 		xsk_buff_set_size(*xdp, size);
+ 		xsk_buff_dma_sync_for_cpu(*xdp, rx_ring->xsk_pool);
++>>>>>>> db804cfc21e9 (ice: Use the xsk batched rx allocation interface)
 +
 +		size = le16_to_cpu(rx_desc->wb.pkt_len) &
 +				   ICE_RX_FLX_DESC_PKT_LEN_M;
 +		if (!size) {
 +			xdp->data = NULL;
 +			xdp->data_end = NULL;
 +			xdp->data_hard_start = NULL;
 +			xdp->data_meta = NULL;
 +			goto construct_skb;
 +		}
 +
 +		xdp->data_end = xdp->data + size;
 +		xsk_buff_dma_sync_for_cpu(xdp, rx_ring->xsk_pool);
  
 -		xdp_res = ice_run_xdp_zc(rx_ring, *xdp);
 +		xdp_res = ice_run_xdp_zc(rx_ring, xdp, xdp_prog, xdp_ring);
  		if (xdp_res) {
  			if (xdp_res & (ICE_XDP_TX | ICE_XDP_REDIR))
  				xdp_xmit |= xdp_res;
* Unmerged path drivers/net/ethernet/intel/ice/ice_xsk.c
