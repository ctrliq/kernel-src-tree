ice: xsk: Borrow xdp_tx_active logic from i40e

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-505.el8
commit-author Maciej Fijalkowski <maciej.fijalkowski@intel.com>
commit 59e92bfe4df71a833678a94b9947843a4c9f55bb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-505.el8/59e92bfe.failed

One of the things that commit 5574ff7b7b3d ("i40e: optimize AF_XDP Tx
completion path") introduced was the @xdp_tx_active field. Its usage
from i40e can be adjusted to ice driver and give us positive performance
results.

If the descriptor that @next_dd points to has been sent by HW (its DD
bit is set), then we are sure that at least quarter of the ring is ready
to be cleaned. If @xdp_tx_active is 0 which means that related xdp_ring
is not used for XDP_{TX, REDIRECT} workloads, then we know how many XSK
entries should placed to completion queue, IOW walking through the ring
can be skipped.

	Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
	Reviewed-by: Alexander Lobakin <alexandr.lobakin@intel.com>
	Acked-by: Magnus Karlsson <magnus.karlsson@intel.com>
Link: https://lore.kernel.org/bpf/20220125160446.78976-9-maciej.fijalkowski@intel.com
(cherry picked from commit 59e92bfe4df71a833678a94b9947843a4c9f55bb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_txrx.h
#	drivers/net/ethernet/intel/ice/ice_xsk.c
diff --cc drivers/net/ethernet/intel/ice/ice_txrx.h
index 63b85c59bf08,466253ac2ee1..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@@ -332,9 -332,9 +332,14 @@@ struct ice_tx_ring 
  	struct ice_ptp_tx *tx_tstamps;
  	spinlock_t tx_lock;
  	u32 txq_teid;			/* Added Tx queue TEID */
++<<<<<<< HEAD
++=======
+ 	/* CL4 - 4th cacheline starts here */
+ 	u16 xdp_tx_active;
++>>>>>>> 59e92bfe4df7 (ice: xsk: Borrow xdp_tx_active logic from i40e)
  #define ICE_TX_FLAGS_RING_XDP		BIT(0)
 +#define ICE_TX_FLAGS_RING_VLAN_L2TAG1	BIT(1)
 +#define ICE_TX_FLAGS_RING_VLAN_L2TAG2	BIT(2)
  	u8 flags;
  	u8 dcb_tc;			/* Traffic class of ring */
  	u8 ptp_tx;
diff --cc drivers/net/ethernet/intel/ice/ice_xsk.c
index 66f1901798d7,2976991c0ab2..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_xsk.c
+++ b/drivers/net/ethernet/intel/ice/ice_xsk.c
@@@ -766,60 -694,201 +767,94 @@@ ice_clean_xdp_tx_buf(struct ice_tx_rin
  }
  
  /**
 - * ice_clean_xdp_irq_zc - Reclaim resources after transmit completes on XDP ring
 - * @xdp_ring: XDP ring to clean
 - * @napi_budget: amount of descriptors that NAPI allows us to clean
 + * ice_clean_tx_irq_zc - Completes AF_XDP entries, and cleans XDP entries
 + * @xdp_ring: XDP Tx ring
 + * @budget: NAPI budget
   *
 - * Returns count of cleaned descriptors
 + * Returns true if cleanup/tranmission is done.
   */
 -static u16 ice_clean_xdp_irq_zc(struct ice_tx_ring *xdp_ring, int napi_budget)
 +bool ice_clean_tx_irq_zc(struct ice_tx_ring *xdp_ring, int budget)
  {
++<<<<<<< HEAD
 +	int total_packets = 0, total_bytes = 0;
 +	s16 ntc = xdp_ring->next_to_clean;
 +	struct ice_tx_desc *tx_desc;
 +	struct ice_tx_buf *tx_buf;
 +	u32 xsk_frames = 0;
 +	bool xmit_done;
 +
 +	tx_desc = ICE_TX_DESC(xdp_ring, ntc);
 +	tx_buf = &xdp_ring->tx_buf[ntc];
 +	ntc -= xdp_ring->count;
++=======
+ 	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
+ 	int budget = napi_budget / tx_thresh;
+ 	u16 next_dd = xdp_ring->next_dd;
+ 	u16 ntc, cleared_dds = 0;
++>>>>>>> 59e92bfe4df7 (ice: xsk: Borrow xdp_tx_active logic from i40e)
  
  	do {
 -		struct ice_tx_desc *next_dd_desc;
 -		u16 desc_cnt = xdp_ring->count;
 -		struct ice_tx_buf *tx_buf;
 -		u32 xsk_frames;
 -		u16 i;
 -
 -		next_dd_desc = ICE_TX_DESC(xdp_ring, next_dd);
 -		if (!(next_dd_desc->cmd_type_offset_bsz &
 -		    cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)))
 +		if (!(tx_desc->cmd_type_offset_bsz &
 +		      cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)))
  			break;
  
++<<<<<<< HEAD
 +		total_bytes += tx_buf->bytecount;
 +		total_packets++;
++=======
+ 		cleared_dds++;
+ 		xsk_frames = 0;
+ 		if (likely(!xdp_ring->xdp_tx_active)) {
+ 			xsk_frames = tx_thresh;
+ 			goto skip;
+ 		}
+ 
+ 		ntc = xdp_ring->next_to_clean;
++>>>>>>> 59e92bfe4df7 (ice: xsk: Borrow xdp_tx_active logic from i40e)
  
 -		for (i = 0; i < tx_thresh; i++) {
 -			tx_buf = &xdp_ring->tx_buf[ntc];
 +		if (tx_buf->raw_buf) {
 +			ice_clean_xdp_tx_buf(xdp_ring, tx_buf);
 +			tx_buf->raw_buf = NULL;
 +		} else {
 +			xsk_frames++;
 +		}
++<<<<<<< HEAD
  
 -			if (tx_buf->raw_buf) {
 -				ice_clean_xdp_tx_buf(xdp_ring, tx_buf);
 -				tx_buf->raw_buf = NULL;
 -			} else {
 -				xsk_frames++;
 -			}
 +		tx_desc->cmd_type_offset_bsz = 0;
 +		tx_buf++;
 +		tx_desc++;
 +		ntc++;
  
 -			ntc++;
 -			if (ntc >= xdp_ring->count)
 -				ntc = 0;
 +		if (unlikely(!ntc)) {
 +			ntc -= xdp_ring->count;
 +			tx_buf = xdp_ring->tx_buf;
 +			tx_desc = ICE_TX_DESC(xdp_ring, 0);
  		}
 +
 +		prefetch(tx_desc);
 +
 +	} while (likely(--budget));
 +
 +	ntc += xdp_ring->count;
 +	xdp_ring->next_to_clean = ntc;
++=======
+ skip:
+ 		xdp_ring->next_to_clean += tx_thresh;
+ 		if (xdp_ring->next_to_clean >= desc_cnt)
+ 			xdp_ring->next_to_clean -= desc_cnt;
+ 		if (xsk_frames)
+ 			xsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);
+ 		next_dd_desc->cmd_type_offset_bsz = 0;
+ 		next_dd = next_dd + tx_thresh;
+ 		if (next_dd >= desc_cnt)
+ 			next_dd = tx_thresh - 1;
+ 	} while (budget--);
+ 
+ 	xdp_ring->next_dd = next_dd;
++>>>>>>> 59e92bfe4df7 (ice: xsk: Borrow xdp_tx_active logic from i40e)
  
 -	return cleared_dds * tx_thresh;
 -}
 -
 -/**
 - * ice_xmit_pkt - produce a single HW Tx descriptor out of AF_XDP descriptor
 - * @xdp_ring: XDP ring to produce the HW Tx descriptor on
 - * @desc: AF_XDP descriptor to pull the DMA address and length from
 - * @total_bytes: bytes accumulator that will be used for stats update
 - */
 -static void ice_xmit_pkt(struct ice_tx_ring *xdp_ring, struct xdp_desc *desc,
 -			 unsigned int *total_bytes)
 -{
 -	struct ice_tx_desc *tx_desc;
 -	dma_addr_t dma;
 -
 -	dma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, desc->addr);
 -	xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, desc->len);
 -
 -	tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_to_use++);
 -	tx_desc->buf_addr = cpu_to_le64(dma);
 -	tx_desc->cmd_type_offset_bsz = ice_build_ctob(ICE_TX_DESC_CMD_EOP,
 -						      0, desc->len, 0);
 -
 -	*total_bytes += desc->len;
 -}
 -
 -/**
 - * ice_xmit_pkt_batch - produce a batch of HW Tx descriptors out of AF_XDP descriptors
 - * @xdp_ring: XDP ring to produce the HW Tx descriptors on
 - * @descs: AF_XDP descriptors to pull the DMA addresses and lengths from
 - * @total_bytes: bytes accumulator that will be used for stats update
 - */
 -static void ice_xmit_pkt_batch(struct ice_tx_ring *xdp_ring, struct xdp_desc *descs,
 -			       unsigned int *total_bytes)
 -{
 -	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
 -	u16 ntu = xdp_ring->next_to_use;
 -	struct ice_tx_desc *tx_desc;
 -	u32 i;
 -
 -	loop_unrolled_for(i = 0; i < PKTS_PER_BATCH; i++) {
 -		dma_addr_t dma;
 -
 -		dma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, descs[i].addr);
 -		xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, descs[i].len);
 -
 -		tx_desc = ICE_TX_DESC(xdp_ring, ntu++);
 -		tx_desc->buf_addr = cpu_to_le64(dma);
 -		tx_desc->cmd_type_offset_bsz = ice_build_ctob(ICE_TX_DESC_CMD_EOP,
 -							      0, descs[i].len, 0);
 -
 -		*total_bytes += descs[i].len;
 -	}
 -
 -	xdp_ring->next_to_use = ntu;
 -
 -	if (xdp_ring->next_to_use > xdp_ring->next_rs) {
 -		tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_rs);
 -		tx_desc->cmd_type_offset_bsz |=
 -			cpu_to_le64(ICE_TX_DESC_CMD_RS << ICE_TXD_QW1_CMD_S);
 -		xdp_ring->next_rs += tx_thresh;
 -	}
 -}
 -
 -/**
 - * ice_fill_tx_hw_ring - produce the number of Tx descriptors onto ring
 - * @xdp_ring: XDP ring to produce the HW Tx descriptors on
 - * @descs: AF_XDP descriptors to pull the DMA addresses and lengths from
 - * @nb_pkts: count of packets to be send
 - * @total_bytes: bytes accumulator that will be used for stats update
 - */
 -static void ice_fill_tx_hw_ring(struct ice_tx_ring *xdp_ring, struct xdp_desc *descs,
 -				u32 nb_pkts, unsigned int *total_bytes)
 -{
 -	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
 -	u32 batched, leftover, i;
 -
 -	batched = ALIGN_DOWN(nb_pkts, PKTS_PER_BATCH);
 -	leftover = nb_pkts & (PKTS_PER_BATCH - 1);
 -	for (i = 0; i < batched; i += PKTS_PER_BATCH)
 -		ice_xmit_pkt_batch(xdp_ring, &descs[i], total_bytes);
 -	for (; i < batched + leftover; i++)
 -		ice_xmit_pkt(xdp_ring, &descs[i], total_bytes);
 -
 -	if (xdp_ring->next_to_use > xdp_ring->next_rs) {
 -		struct ice_tx_desc *tx_desc;
 -
 -		tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_rs);
 -		tx_desc->cmd_type_offset_bsz |=
 -			cpu_to_le64(ICE_TX_DESC_CMD_RS << ICE_TXD_QW1_CMD_S);
 -		xdp_ring->next_rs += tx_thresh;
 -	}
 -}
 -
 -/**
 - * ice_xmit_zc - take entries from XSK Tx ring and place them onto HW Tx ring
 - * @xdp_ring: XDP ring to produce the HW Tx descriptors on
 - * @budget: number of free descriptors on HW Tx ring that can be used
 - * @napi_budget: amount of descriptors that NAPI allows us to clean
 - *
 - * Returns true if there is no more work that needs to be done, false otherwise
 - */
 -bool ice_xmit_zc(struct ice_tx_ring *xdp_ring, u32 budget, int napi_budget)
 -{
 -	struct xdp_desc *descs = xdp_ring->xsk_pool->tx_descs;
 -	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
 -	u32 nb_pkts, nb_processed = 0;
 -	unsigned int total_bytes = 0;
 -
 -	if (budget < tx_thresh)
 -		budget += ice_clean_xdp_irq_zc(xdp_ring, napi_budget);
 -
 -	nb_pkts = xsk_tx_peek_release_desc_batch(xdp_ring->xsk_pool, budget);
 -	if (!nb_pkts)
 -		return true;
 -
 -	if (xdp_ring->next_to_use + nb_pkts >= xdp_ring->count) {
 -		struct ice_tx_desc *tx_desc;
 -
 -		nb_processed = xdp_ring->count - xdp_ring->next_to_use;
 -		ice_fill_tx_hw_ring(xdp_ring, descs, nb_processed, &total_bytes);
 -		tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_rs);
 -		tx_desc->cmd_type_offset_bsz |=
 -			cpu_to_le64(ICE_TX_DESC_CMD_RS << ICE_TXD_QW1_CMD_S);
 -		xdp_ring->next_rs = tx_thresh - 1;
 -		xdp_ring->next_to_use = 0;
 -	}
 -
 -	ice_fill_tx_hw_ring(xdp_ring, &descs[nb_processed], nb_pkts - nb_processed,
 -			    &total_bytes);
 -
 -	ice_xdp_ring_update_tail(xdp_ring);
 -	ice_update_tx_ring_stats(xdp_ring, nb_pkts, total_bytes);
 +	if (xsk_frames)
 +		xsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);
  
  	if (xsk_uses_need_wakeup(xdp_ring->xsk_pool))
  		xsk_set_tx_need_wakeup(xdp_ring->xsk_pool);
* Unmerged path drivers/net/ethernet/intel/ice/ice_txrx.h
diff --git a/drivers/net/ethernet/intel/ice/ice_txrx_lib.c b/drivers/net/ethernet/intel/ice/ice_txrx_lib.c
index 95e27a4eae58..bec2e8d2d63b 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx_lib.c
+++ b/drivers/net/ethernet/intel/ice/ice_txrx_lib.c
@@ -305,6 +305,7 @@ int ice_xmit_xdp_ring(void *data, u16 size, struct ice_tx_ring *xdp_ring)
 	tx_desc->cmd_type_offset_bsz = ice_build_ctob(ICE_TX_DESC_CMD_EOP, 0,
 						      size, 0);
 
+	xdp_ring->xdp_tx_active++;
 	i++;
 	if (i == xdp_ring->count) {
 		i = 0;
* Unmerged path drivers/net/ethernet/intel/ice/ice_xsk.c
