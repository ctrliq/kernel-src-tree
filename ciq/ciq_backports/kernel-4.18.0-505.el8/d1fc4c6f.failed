ice: xsk: check if Rx ring was filled up to the end

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-505.el8
commit-author Maciej Fijalkowski <maciej.fijalkowski@intel.com>
commit d1fc4c6feac18f893e55aeefa267a281e132c7b7
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-505.el8/d1fc4c6f.failed

__ice_alloc_rx_bufs_zc() checks if a number of the descriptors to be
allocated would cause the ring wrap. In that case, driver will issue two
calls to xsk_buff_alloc_batch() - one that will fill the ring up to the
end and the second one that will start with filling descriptors from the
beginning of the ring.

ice_fill_rx_descs() is a wrapper for taking care of what
xsk_buff_alloc_batch() gave back to the driver. It works in a best
effort approach, so for example when driver asks for 64 buffers,
ice_fill_rx_descs() could assign only 32. Such case needs to be checked
when ring is being filled up to the end, because in that situation ntu
might not reached the end of the ring.

Fix the ring wrap by checking if nb_buffs_extra has the expected value.
If not, bump ntu and go directly to tail update.

Fixes: 3876ff525de7 ("ice: xsk: Handle SW XDP ring wrap and bump tail more often")
	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
	Tested-by: Shwetha Nagaraju <Shwetha.nagaraju@intel.com>
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit d1fc4c6feac18f893e55aeefa267a281e132c7b7)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_xsk.c
diff --cc drivers/net/ethernet/intel/ice/ice_xsk.c
index 66f1901798d7,9dd38f667059..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_xsk.c
+++ b/drivers/net/ethernet/intel/ice/ice_xsk.c
@@@ -468,22 -397,84 +468,70 @@@ bool ice_alloc_rx_bufs_zc(struct ice_rx
  
  		rx_desc++;
  		xdp++;
 -	}
 +		ntu++;
  
 -	return buffs;
 -}
 +		if (unlikely(ntu == rx_ring->count)) {
 +			rx_desc = ICE_RX_DESC(rx_ring, 0);
 +			xdp = rx_ring->xdp_buf;
 +			ntu = 0;
 +		}
 +	} while (--count);
  
++<<<<<<< HEAD
 +	if (rx_ring->next_to_use != ntu) {
 +		/* clear the status bits for the next_to_use descriptor */
 +		rx_desc->wb.status_error0 = 0;
++=======
+ /**
+  * __ice_alloc_rx_bufs_zc - allocate a number of Rx buffers
+  * @rx_ring: Rx ring
+  * @count: The number of buffers to allocate
+  *
+  * Place the @count of descriptors onto Rx ring. Handle the ring wrap
+  * for case where space from next_to_use up to the end of ring is less
+  * than @count. Finally do a tail bump.
+  *
+  * Returns true if all allocations were successful, false if any fail.
+  */
+ static bool __ice_alloc_rx_bufs_zc(struct ice_rx_ring *rx_ring, u16 count)
+ {
+ 	u32 nb_buffs_extra = 0, nb_buffs = 0;
+ 	union ice_32b_rx_flex_desc *rx_desc;
+ 	u16 ntu = rx_ring->next_to_use;
+ 	u16 total_count = count;
+ 	struct xdp_buff **xdp;
+ 
+ 	rx_desc = ICE_RX_DESC(rx_ring, ntu);
+ 	xdp = ice_xdp_buf(rx_ring, ntu);
+ 
+ 	if (ntu + count >= rx_ring->count) {
+ 		nb_buffs_extra = ice_fill_rx_descs(rx_ring->xsk_pool, xdp,
+ 						   rx_desc,
+ 						   rx_ring->count - ntu);
+ 		if (nb_buffs_extra != rx_ring->count - ntu) {
+ 			ntu += nb_buffs_extra;
+ 			goto exit;
+ 		}
+ 		rx_desc = ICE_RX_DESC(rx_ring, 0);
+ 		xdp = ice_xdp_buf(rx_ring, 0);
+ 		ntu = 0;
+ 		count -= nb_buffs_extra;
+ 		ice_release_rx_desc(rx_ring, 0);
+ 	}
+ 
+ 	nb_buffs = ice_fill_rx_descs(rx_ring->xsk_pool, xdp, rx_desc, count);
+ 
+ 	ntu += nb_buffs;
+ 	if (ntu == rx_ring->count)
+ 		ntu = 0;
+ 
+ exit:
+ 	if (rx_ring->next_to_use != ntu)
++>>>>>>> d1fc4c6feac1 (ice: xsk: check if Rx ring was filled up to the end)
  		ice_release_rx_desc(rx_ring, ntu);
 +	}
  
 -	return total_count == (nb_buffs_extra + nb_buffs);
 -}
 -
 -/**
 - * ice_alloc_rx_bufs_zc - allocate a number of Rx buffers
 - * @rx_ring: Rx ring
 - * @count: The number of buffers to allocate
 - *
 - * Wrapper for internal allocation routine; figure out how many tail
 - * bumps should take place based on the given threshold
 - *
 - * Returns true if all calls to internal alloc routine succeeded
 - */
 -bool ice_alloc_rx_bufs_zc(struct ice_rx_ring *rx_ring, u16 count)
 -{
 -	u16 rx_thresh = ICE_RING_QUARTER(rx_ring);
 -	u16 batched, leftover, i, tail_bumps;
 -
 -	batched = ALIGN_DOWN(count, rx_thresh);
 -	tail_bumps = batched / rx_thresh;
 -	leftover = count & (rx_thresh - 1);
 -
 -	for (i = 0; i < tail_bumps; i++)
 -		if (!__ice_alloc_rx_bufs_zc(rx_ring, rx_thresh))
 -			return false;
 -	return __ice_alloc_rx_bufs_zc(rx_ring, leftover);
 +	return ok;
  }
  
  /**
* Unmerged path drivers/net/ethernet/intel/ice/ice_xsk.c
