ice: xsk: Eliminate unnecessary loop iteration

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-505.el8
commit-author Magnus Karlsson <magnus.karlsson@intel.com>
commit 30d19d57d513821c58de4556e7445982ed22b923
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-505.el8/30d19d57.failed

The NIC Tx ring completion routine cleans entries from the ring in
batches. However, it processes one more batch than it is supposed
to. Note that this does not matter from a functionality point of view
since it will not find a set DD bit for the next batch and just exit
the loop. But from a performance perspective, it is faster to
terminate the loop before and not issue an expensive read over PCIe to
get the DD bit.

Fixes: 126cdfe1007a ("ice: xsk: Improve AF_XDP ZC Tx and use batching API")
	Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
	Signed-off-by: Alexei Starovoitov <ast@kernel.org>
Link: https://lore.kernel.org/bpf/20220328142123.170157-3-maciej.fijalkowski@intel.com
(cherry picked from commit 30d19d57d513821c58de4556e7445982ed22b923)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_xsk.c
diff --cc drivers/net/ethernet/intel/ice/ice_xsk.c
index 66f1901798d7,51427cb4971a..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_xsk.c
+++ b/drivers/net/ethernet/intel/ice/ice_xsk.c
@@@ -766,60 -696,201 +766,74 @@@ ice_clean_xdp_tx_buf(struct ice_tx_rin
  }
  
  /**
 - * ice_clean_xdp_irq_zc - Reclaim resources after transmit completes on XDP ring
 - * @xdp_ring: XDP ring to clean
 - * @napi_budget: amount of descriptors that NAPI allows us to clean
 + * ice_clean_tx_irq_zc - Completes AF_XDP entries, and cleans XDP entries
 + * @xdp_ring: XDP Tx ring
 + * @budget: NAPI budget
   *
 - * Returns count of cleaned descriptors
 + * Returns true if cleanup/tranmission is done.
   */
 -static u16 ice_clean_xdp_irq_zc(struct ice_tx_ring *xdp_ring, int napi_budget)
 +bool ice_clean_tx_irq_zc(struct ice_tx_ring *xdp_ring, int budget)
  {
 -	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
 -	int budget = napi_budget / tx_thresh;
 -	u16 next_dd = xdp_ring->next_dd;
 -	u16 ntc, cleared_dds = 0;
 +	int total_packets = 0, total_bytes = 0;
 +	s16 ntc = xdp_ring->next_to_clean;
 +	struct ice_tx_desc *tx_desc;
 +	struct ice_tx_buf *tx_buf;
 +	u32 xsk_frames = 0;
 +	bool xmit_done;
  
 -	do {
 -		struct ice_tx_desc *next_dd_desc;
 -		u16 desc_cnt = xdp_ring->count;
 -		struct ice_tx_buf *tx_buf;
 -		u32 xsk_frames;
 -		u16 i;
 +	tx_desc = ICE_TX_DESC(xdp_ring, ntc);
 +	tx_buf = &xdp_ring->tx_buf[ntc];
 +	ntc -= xdp_ring->count;
  
 -		next_dd_desc = ICE_TX_DESC(xdp_ring, next_dd);
 -		if (!(next_dd_desc->cmd_type_offset_bsz &
 -		    cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)))
 +	do {
 +		if (!(tx_desc->cmd_type_offset_bsz &
 +		      cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)))
  			break;
  
 -		cleared_dds++;
 -		xsk_frames = 0;
 -		if (likely(!xdp_ring->xdp_tx_active)) {
 -			xsk_frames = tx_thresh;
 -			goto skip;
 -		}
 -
 -		ntc = xdp_ring->next_to_clean;
 +		total_bytes += tx_buf->bytecount;
 +		total_packets++;
  
 -		for (i = 0; i < tx_thresh; i++) {
 -			tx_buf = &xdp_ring->tx_buf[ntc];
 +		if (tx_buf->raw_buf) {
 +			ice_clean_xdp_tx_buf(xdp_ring, tx_buf);
 +			tx_buf->raw_buf = NULL;
 +		} else {
 +			xsk_frames++;
 +		}
  
 -			if (tx_buf->raw_buf) {
 -				ice_clean_xdp_tx_buf(xdp_ring, tx_buf);
 -				tx_buf->raw_buf = NULL;
 -			} else {
 -				xsk_frames++;
 -			}
 +		tx_desc->cmd_type_offset_bsz = 0;
 +		tx_buf++;
 +		tx_desc++;
 +		ntc++;
  
 -			ntc++;
 -			if (ntc >= xdp_ring->count)
 -				ntc = 0;
 +		if (unlikely(!ntc)) {
 +			ntc -= xdp_ring->count;
 +			tx_buf = xdp_ring->tx_buf;
 +			tx_desc = ICE_TX_DESC(xdp_ring, 0);
  		}
++<<<<<<< HEAD
++=======
+ skip:
+ 		xdp_ring->next_to_clean += tx_thresh;
+ 		if (xdp_ring->next_to_clean >= desc_cnt)
+ 			xdp_ring->next_to_clean -= desc_cnt;
+ 		if (xsk_frames)
+ 			xsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);
+ 		next_dd_desc->cmd_type_offset_bsz = 0;
+ 		next_dd = next_dd + tx_thresh;
+ 		if (next_dd >= desc_cnt)
+ 			next_dd = tx_thresh - 1;
+ 	} while (--budget);
++>>>>>>> 30d19d57d513 (ice: xsk: Eliminate unnecessary loop iteration)
  
 -	xdp_ring->next_dd = next_dd;
 +		prefetch(tx_desc);
  
 -	return cleared_dds * tx_thresh;
 -}
 +	} while (likely(--budget));
  
 -/**
 - * ice_xmit_pkt - produce a single HW Tx descriptor out of AF_XDP descriptor
 - * @xdp_ring: XDP ring to produce the HW Tx descriptor on
 - * @desc: AF_XDP descriptor to pull the DMA address and length from
 - * @total_bytes: bytes accumulator that will be used for stats update
 - */
 -static void ice_xmit_pkt(struct ice_tx_ring *xdp_ring, struct xdp_desc *desc,
 -			 unsigned int *total_bytes)
 -{
 -	struct ice_tx_desc *tx_desc;
 -	dma_addr_t dma;
 +	ntc += xdp_ring->count;
 +	xdp_ring->next_to_clean = ntc;
  
 -	dma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, desc->addr);
 -	xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, desc->len);
 -
 -	tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_to_use++);
 -	tx_desc->buf_addr = cpu_to_le64(dma);
 -	tx_desc->cmd_type_offset_bsz = ice_build_ctob(ICE_TX_DESC_CMD_EOP,
 -						      0, desc->len, 0);
 -
 -	*total_bytes += desc->len;
 -}
 -
 -/**
 - * ice_xmit_pkt_batch - produce a batch of HW Tx descriptors out of AF_XDP descriptors
 - * @xdp_ring: XDP ring to produce the HW Tx descriptors on
 - * @descs: AF_XDP descriptors to pull the DMA addresses and lengths from
 - * @total_bytes: bytes accumulator that will be used for stats update
 - */
 -static void ice_xmit_pkt_batch(struct ice_tx_ring *xdp_ring, struct xdp_desc *descs,
 -			       unsigned int *total_bytes)
 -{
 -	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
 -	u16 ntu = xdp_ring->next_to_use;
 -	struct ice_tx_desc *tx_desc;
 -	u32 i;
 -
 -	loop_unrolled_for(i = 0; i < PKTS_PER_BATCH; i++) {
 -		dma_addr_t dma;
 -
 -		dma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, descs[i].addr);
 -		xsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, descs[i].len);
 -
 -		tx_desc = ICE_TX_DESC(xdp_ring, ntu++);
 -		tx_desc->buf_addr = cpu_to_le64(dma);
 -		tx_desc->cmd_type_offset_bsz = ice_build_ctob(ICE_TX_DESC_CMD_EOP,
 -							      0, descs[i].len, 0);
 -
 -		*total_bytes += descs[i].len;
 -	}
 -
 -	xdp_ring->next_to_use = ntu;
 -
 -	if (xdp_ring->next_to_use > xdp_ring->next_rs) {
 -		tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_rs);
 -		tx_desc->cmd_type_offset_bsz |=
 -			cpu_to_le64(ICE_TX_DESC_CMD_RS << ICE_TXD_QW1_CMD_S);
 -		xdp_ring->next_rs += tx_thresh;
 -	}
 -}
 -
 -/**
 - * ice_fill_tx_hw_ring - produce the number of Tx descriptors onto ring
 - * @xdp_ring: XDP ring to produce the HW Tx descriptors on
 - * @descs: AF_XDP descriptors to pull the DMA addresses and lengths from
 - * @nb_pkts: count of packets to be send
 - * @total_bytes: bytes accumulator that will be used for stats update
 - */
 -static void ice_fill_tx_hw_ring(struct ice_tx_ring *xdp_ring, struct xdp_desc *descs,
 -				u32 nb_pkts, unsigned int *total_bytes)
 -{
 -	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
 -	u32 batched, leftover, i;
 -
 -	batched = ALIGN_DOWN(nb_pkts, PKTS_PER_BATCH);
 -	leftover = nb_pkts & (PKTS_PER_BATCH - 1);
 -	for (i = 0; i < batched; i += PKTS_PER_BATCH)
 -		ice_xmit_pkt_batch(xdp_ring, &descs[i], total_bytes);
 -	for (; i < batched + leftover; i++)
 -		ice_xmit_pkt(xdp_ring, &descs[i], total_bytes);
 -
 -	if (xdp_ring->next_to_use > xdp_ring->next_rs) {
 -		struct ice_tx_desc *tx_desc;
 -
 -		tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_rs);
 -		tx_desc->cmd_type_offset_bsz |=
 -			cpu_to_le64(ICE_TX_DESC_CMD_RS << ICE_TXD_QW1_CMD_S);
 -		xdp_ring->next_rs += tx_thresh;
 -	}
 -}
 -
 -/**
 - * ice_xmit_zc - take entries from XSK Tx ring and place them onto HW Tx ring
 - * @xdp_ring: XDP ring to produce the HW Tx descriptors on
 - * @budget: number of free descriptors on HW Tx ring that can be used
 - * @napi_budget: amount of descriptors that NAPI allows us to clean
 - *
 - * Returns true if there is no more work that needs to be done, false otherwise
 - */
 -bool ice_xmit_zc(struct ice_tx_ring *xdp_ring, u32 budget, int napi_budget)
 -{
 -	struct xdp_desc *descs = xdp_ring->xsk_pool->tx_descs;
 -	u16 tx_thresh = ICE_RING_QUARTER(xdp_ring);
 -	u32 nb_pkts, nb_processed = 0;
 -	unsigned int total_bytes = 0;
 -
 -	if (budget < tx_thresh)
 -		budget += ice_clean_xdp_irq_zc(xdp_ring, napi_budget);
 -
 -	nb_pkts = xsk_tx_peek_release_desc_batch(xdp_ring->xsk_pool, budget);
 -	if (!nb_pkts)
 -		return true;
 -
 -	if (xdp_ring->next_to_use + nb_pkts >= xdp_ring->count) {
 -		struct ice_tx_desc *tx_desc;
 -
 -		nb_processed = xdp_ring->count - xdp_ring->next_to_use;
 -		ice_fill_tx_hw_ring(xdp_ring, descs, nb_processed, &total_bytes);
 -		tx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_rs);
 -		tx_desc->cmd_type_offset_bsz |=
 -			cpu_to_le64(ICE_TX_DESC_CMD_RS << ICE_TXD_QW1_CMD_S);
 -		xdp_ring->next_rs = tx_thresh - 1;
 -		xdp_ring->next_to_use = 0;
 -	}
 -
 -	ice_fill_tx_hw_ring(xdp_ring, &descs[nb_processed], nb_pkts - nb_processed,
 -			    &total_bytes);
 -
 -	ice_xdp_ring_update_tail(xdp_ring);
 -	ice_update_tx_ring_stats(xdp_ring, nb_pkts, total_bytes);
 +	if (xsk_frames)
 +		xsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);
  
  	if (xsk_uses_need_wakeup(xdp_ring->xsk_pool))
  		xsk_set_tx_need_wakeup(xdp_ring->xsk_pool);
* Unmerged path drivers/net/ethernet/intel/ice/ice_xsk.c
