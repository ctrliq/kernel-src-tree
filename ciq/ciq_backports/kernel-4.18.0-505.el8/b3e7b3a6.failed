ice: prevent NULL pointer deref during reload

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-505.el8
commit-author Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
commit b3e7b3a6ee92ab927f750a6b19615ce88ece808f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-505.el8/b3e7b3a6.failed

Calling ethtool during reload can lead to call trace, because VSI isn't
configured for some time, but netdev is alive.

To fix it add rtnl lock for VSI deconfig and config. Set ::num_q_vectors
to 0 after freeing and add a check for ::tx/rx_rings in ring related
ethtool ops.

Add proper unroll of filters in ice_start_eth().

Reproduction:
$watch -n 0.1 -d 'ethtool -g enp24s0f0np0'
$devlink dev reload pci/0000:18:00.0 action driver_reinit

Call trace before fix:
[66303.926205] BUG: kernel NULL pointer dereference, address: 0000000000000000
[66303.926259] #PF: supervisor read access in kernel mode
[66303.926286] #PF: error_code(0x0000) - not-present page
[66303.926311] PGD 0 P4D 0
[66303.926332] Oops: 0000 [#1] PREEMPT SMP PTI
[66303.926358] CPU: 4 PID: 933821 Comm: ethtool Kdump: loaded Tainted: G           OE      6.4.0-rc5+ #1
[66303.926400] Hardware name: Intel Corporation S2600WFT/S2600WFT, BIOS SE5C620.86B.00.01.0014.070920180847 07/09/2018
[66303.926446] RIP: 0010:ice_get_ringparam+0x22/0x50 [ice]
[66303.926649] Code: 90 90 90 90 90 90 90 90 f3 0f 1e fa 0f 1f 44 00 00 48 8b 87 c0 09 00 00 c7 46 04 e0 1f 00 00 c7 46 10 e0 1f 00 00 48 8b 50 20 <48> 8b 12 0f b7 52 3a 89 56 14 48 8b 40 28 48 8b 00 0f b7 40 58 48
[66303.926722] RSP: 0018:ffffad40472f39c8 EFLAGS: 00010246
[66303.926749] RAX: ffff98a8ada05828 RBX: ffff98a8c46dd060 RCX: ffffad40472f3b48
[66303.926781] RDX: 0000000000000000 RSI: ffff98a8c46dd068 RDI: ffff98a8b23c4000
[66303.926811] RBP: ffffad40472f3b48 R08: 00000000000337b0 R09: 0000000000000000
[66303.926843] R10: 0000000000000001 R11: 0000000000000100 R12: ffff98a8b23c4000
[66303.926874] R13: ffff98a8c46dd060 R14: 000000000000000f R15: ffffad40472f3a50
[66303.926906] FS:  00007f6397966740(0000) GS:ffff98b390900000(0000) knlGS:0000000000000000
[66303.926941] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[66303.926967] CR2: 0000000000000000 CR3: 000000011ac20002 CR4: 00000000007706e0
[66303.926999] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
[66303.927029] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
[66303.927060] PKRU: 55555554
[66303.927075] Call Trace:
[66303.927094]  <TASK>
[66303.927111]  ? __die+0x23/0x70
[66303.927140]  ? page_fault_oops+0x171/0x4e0
[66303.927176]  ? exc_page_fault+0x7f/0x180
[66303.927209]  ? asm_exc_page_fault+0x26/0x30
[66303.927244]  ? ice_get_ringparam+0x22/0x50 [ice]
[66303.927433]  rings_prepare_data+0x62/0x80
[66303.927469]  ethnl_default_doit+0xe2/0x350
[66303.927501]  genl_family_rcv_msg_doit.isra.0+0xe3/0x140
[66303.927538]  genl_rcv_msg+0x1b1/0x2c0
[66303.927561]  ? __pfx_ethnl_default_doit+0x10/0x10
[66303.927590]  ? __pfx_genl_rcv_msg+0x10/0x10
[66303.927615]  netlink_rcv_skb+0x58/0x110
[66303.927644]  genl_rcv+0x28/0x40
[66303.927665]  netlink_unicast+0x19e/0x290
[66303.927691]  netlink_sendmsg+0x254/0x4d0
[66303.927717]  sock_sendmsg+0x93/0xa0
[66303.927743]  __sys_sendto+0x126/0x170
[66303.927780]  __x64_sys_sendto+0x24/0x30
[66303.928593]  do_syscall_64+0x5d/0x90
[66303.929370]  ? __count_memcg_events+0x60/0xa0
[66303.930146]  ? count_memcg_events.constprop.0+0x1a/0x30
[66303.930920]  ? handle_mm_fault+0x9e/0x350
[66303.931688]  ? do_user_addr_fault+0x258/0x740
[66303.932452]  ? exc_page_fault+0x7f/0x180
[66303.933193]  entry_SYSCALL_64_after_hwframe+0x72/0xdc

Fixes: 5b246e533d01 ("ice: split probe into smaller functions")
	Reviewed-by: Przemek Kitszel <przemyslaw.kitszel@intel.com>
	Signed-off-by: Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
	Reviewed-by: Simon Horman <simon.horman@corigine.com>
	Tested-by: Pucha Himasekhar Reddy <himasekharx.reddy.pucha@intel.com> (A Contingent worker at Intel)
	Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
(cherry picked from commit b3e7b3a6ee92ab927f750a6b19615ce88ece808f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/net/ethernet/intel/ice/ice_main.c
diff --cc drivers/net/ethernet/intel/ice/ice_main.c
index 70911484af91,f02d44455772..000000000000
--- a/drivers/net/ethernet/intel/ice/ice_main.c
+++ b/drivers/net/ethernet/intel/ice/ice_main.c
@@@ -4639,18 -4362,586 +4639,545 @@@ static int ice_register_netdev(struct i
  	netif_carrier_off(vsi->netdev);
  	netif_tx_stop_all_queues(vsi->netdev);
  
 -	return 0;
 -}
 -
 -static void ice_unregister_netdev(struct ice_vsi *vsi)
 -{
 -	if (!vsi || !vsi->netdev)
 -		return;
 -
 -	unregister_netdev(vsi->netdev);
 -	clear_bit(ICE_VSI_NETDEV_REGISTERED, vsi->state);
 -}
 -
 -/**
 - * ice_cfg_netdev - Allocate, configure and register a netdev
 - * @vsi: the VSI associated with the new netdev
 - *
 - * Returns 0 on success, negative value on failure
 - */
 -static int ice_cfg_netdev(struct ice_vsi *vsi)
 -{
 -	struct ice_netdev_priv *np;
 -	struct net_device *netdev;
 -	u8 mac_addr[ETH_ALEN];
 -
 -	netdev = alloc_etherdev_mqs(sizeof(*np), vsi->alloc_txq,
 -				    vsi->alloc_rxq);
 -	if (!netdev)
 -		return -ENOMEM;
 -
 -	set_bit(ICE_VSI_NETDEV_ALLOCD, vsi->state);
 -	vsi->netdev = netdev;
 -	np = netdev_priv(netdev);
 -	np->vsi = vsi;
 -
 -	ice_set_netdev_features(netdev);
 -	ice_set_ops(vsi);
 -
 -	if (vsi->type == ICE_VSI_PF) {
 -		SET_NETDEV_DEV(netdev, ice_pf_to_dev(vsi->back));
 -		ether_addr_copy(mac_addr, vsi->port_info->mac.perm_addr);
 -		eth_hw_addr_set(netdev, mac_addr);
 -	}
 -
 -	netdev->priv_flags |= IFF_UNICAST_FLT;
 -
 -	/* Setup netdev TC information */
 -	ice_vsi_cfg_netdev_tc(vsi, vsi->tc_cfg.ena_tc);
 -
 -	netdev->max_mtu = ICE_MAX_MTU;
 +	devlink_port_type_eth_set(&pf->devlink_port, vsi->netdev);
  
  	return 0;
++<<<<<<< HEAD
++=======
+ }
+ 
+ static void ice_decfg_netdev(struct ice_vsi *vsi)
+ {
+ 	clear_bit(ICE_VSI_NETDEV_ALLOCD, vsi->state);
+ 	free_netdev(vsi->netdev);
+ 	vsi->netdev = NULL;
+ }
+ 
+ static int ice_start_eth(struct ice_vsi *vsi)
+ {
+ 	int err;
+ 
+ 	err = ice_init_mac_fltr(vsi->back);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ice_vsi_open(vsi);
+ 	if (err)
+ 		ice_fltr_remove_all(vsi);
+ 
+ 	return err;
+ }
+ 
+ static void ice_stop_eth(struct ice_vsi *vsi)
+ {
+ 	ice_fltr_remove_all(vsi);
+ 	ice_vsi_close(vsi);
+ }
+ 
+ static int ice_init_eth(struct ice_pf *pf)
+ {
+ 	struct ice_vsi *vsi = ice_get_main_vsi(pf);
+ 	int err;
+ 
+ 	if (!vsi)
+ 		return -EINVAL;
+ 
+ 	/* init channel list */
+ 	INIT_LIST_HEAD(&vsi->ch_list);
+ 
+ 	err = ice_cfg_netdev(vsi);
+ 	if (err)
+ 		return err;
+ 	/* Setup DCB netlink interface */
+ 	ice_dcbnl_setup(vsi);
+ 
+ 	err = ice_init_mac_fltr(pf);
+ 	if (err)
+ 		goto err_init_mac_fltr;
+ 
+ 	err = ice_devlink_create_pf_port(pf);
+ 	if (err)
+ 		goto err_devlink_create_pf_port;
+ 
+ 	SET_NETDEV_DEVLINK_PORT(vsi->netdev, &pf->devlink_port);
+ 
+ 	err = ice_register_netdev(vsi);
+ 	if (err)
+ 		goto err_register_netdev;
+ 
+ 	err = ice_tc_indir_block_register(vsi);
+ 	if (err)
+ 		goto err_tc_indir_block_register;
+ 
+ 	ice_napi_add(vsi);
+ 
+ 	return 0;
+ 
+ err_tc_indir_block_register:
+ 	ice_unregister_netdev(vsi);
++>>>>>>> b3e7b3a6ee92 (ice: prevent NULL pointer deref during reload)
  err_register_netdev:
  	ice_devlink_destroy_pf_port(pf);
 -err_devlink_create_pf_port:
 -err_init_mac_fltr:
 -	ice_decfg_netdev(vsi);
 +err_devlink_create:
 +	free_netdev(vsi->netdev);
 +	vsi->netdev = NULL;
 +	clear_bit(ICE_VSI_NETDEV_ALLOCD, vsi->state);
  	return err;
  }
  
++<<<<<<< HEAD
++=======
+ static void ice_deinit_eth(struct ice_pf *pf)
+ {
+ 	struct ice_vsi *vsi = ice_get_main_vsi(pf);
+ 
+ 	if (!vsi)
+ 		return;
+ 
+ 	ice_vsi_close(vsi);
+ 	ice_unregister_netdev(vsi);
+ 	ice_devlink_destroy_pf_port(pf);
+ 	ice_tc_indir_block_unregister(vsi);
+ 	ice_decfg_netdev(vsi);
+ }
+ 
+ static int ice_init_dev(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	struct ice_hw *hw = &pf->hw;
+ 	int err;
+ 
+ 	err = ice_init_hw(hw);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_hw failed: %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	ice_init_feature_support(pf);
+ 
+ 	ice_request_fw(pf);
+ 
+ 	/* if ice_request_fw fails, ICE_FLAG_ADV_FEATURES bit won't be
+ 	 * set in pf->state, which will cause ice_is_safe_mode to return
+ 	 * true
+ 	 */
+ 	if (ice_is_safe_mode(pf)) {
+ 		/* we already got function/device capabilities but these don't
+ 		 * reflect what the driver needs to do in safe mode. Instead of
+ 		 * adding conditional logic everywhere to ignore these
+ 		 * device/function capabilities, override them.
+ 		 */
+ 		ice_set_safe_mode_caps(hw);
+ 	}
+ 
+ 	err = ice_init_pf(pf);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_pf failed: %d\n", err);
+ 		goto err_init_pf;
+ 	}
+ 
+ 	pf->hw.udp_tunnel_nic.set_port = ice_udp_tunnel_set_port;
+ 	pf->hw.udp_tunnel_nic.unset_port = ice_udp_tunnel_unset_port;
+ 	pf->hw.udp_tunnel_nic.flags = UDP_TUNNEL_NIC_INFO_MAY_SLEEP;
+ 	pf->hw.udp_tunnel_nic.shared = &pf->hw.udp_tunnel_shared;
+ 	if (pf->hw.tnl.valid_count[TNL_VXLAN]) {
+ 		pf->hw.udp_tunnel_nic.tables[0].n_entries =
+ 			pf->hw.tnl.valid_count[TNL_VXLAN];
+ 		pf->hw.udp_tunnel_nic.tables[0].tunnel_types =
+ 			UDP_TUNNEL_TYPE_VXLAN;
+ 	}
+ 	if (pf->hw.tnl.valid_count[TNL_GENEVE]) {
+ 		pf->hw.udp_tunnel_nic.tables[1].n_entries =
+ 			pf->hw.tnl.valid_count[TNL_GENEVE];
+ 		pf->hw.udp_tunnel_nic.tables[1].tunnel_types =
+ 			UDP_TUNNEL_TYPE_GENEVE;
+ 	}
+ 
+ 	err = ice_init_interrupt_scheme(pf);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_interrupt_scheme failed: %d\n", err);
+ 		err = -EIO;
+ 		goto err_init_interrupt_scheme;
+ 	}
+ 
+ 	/* In case of MSIX we are going to setup the misc vector right here
+ 	 * to handle admin queue events etc. In case of legacy and MSI
+ 	 * the misc functionality and queue processing is combined in
+ 	 * the same vector and that gets setup at open.
+ 	 */
+ 	err = ice_req_irq_msix_misc(pf);
+ 	if (err) {
+ 		dev_err(dev, "setup of misc vector failed: %d\n", err);
+ 		goto err_req_irq_msix_misc;
+ 	}
+ 
+ 	return 0;
+ 
+ err_req_irq_msix_misc:
+ 	ice_clear_interrupt_scheme(pf);
+ err_init_interrupt_scheme:
+ 	ice_deinit_pf(pf);
+ err_init_pf:
+ 	ice_deinit_hw(hw);
+ 	return err;
+ }
+ 
+ static void ice_deinit_dev(struct ice_pf *pf)
+ {
+ 	ice_free_irq_msix_misc(pf);
+ 	ice_deinit_pf(pf);
+ 	ice_deinit_hw(&pf->hw);
+ 
+ 	/* Service task is already stopped, so call reset directly. */
+ 	ice_reset(&pf->hw, ICE_RESET_PFR);
+ 	pci_wait_for_pending_transaction(pf->pdev);
+ 	ice_clear_interrupt_scheme(pf);
+ }
+ 
+ static void ice_init_features(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	if (ice_is_safe_mode(pf))
+ 		return;
+ 
+ 	/* initialize DDP driven features */
+ 	if (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))
+ 		ice_ptp_init(pf);
+ 
+ 	if (ice_is_feature_supported(pf, ICE_F_GNSS))
+ 		ice_gnss_init(pf);
+ 
+ 	/* Note: Flow director init failure is non-fatal to load */
+ 	if (ice_init_fdir(pf))
+ 		dev_err(dev, "could not initialize flow director\n");
+ 
+ 	/* Note: DCB init failure is non-fatal to load */
+ 	if (ice_init_pf_dcb(pf, false)) {
+ 		clear_bit(ICE_FLAG_DCB_CAPABLE, pf->flags);
+ 		clear_bit(ICE_FLAG_DCB_ENA, pf->flags);
+ 	} else {
+ 		ice_cfg_lldp_mib_change(&pf->hw, true);
+ 	}
+ 
+ 	if (ice_init_lag(pf))
+ 		dev_warn(dev, "Failed to init link aggregation support\n");
+ }
+ 
+ static void ice_deinit_features(struct ice_pf *pf)
+ {
+ 	ice_deinit_lag(pf);
+ 	if (test_bit(ICE_FLAG_DCB_CAPABLE, pf->flags))
+ 		ice_cfg_lldp_mib_change(&pf->hw, false);
+ 	ice_deinit_fdir(pf);
+ 	if (ice_is_feature_supported(pf, ICE_F_GNSS))
+ 		ice_gnss_exit(pf);
+ 	if (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))
+ 		ice_ptp_release(pf);
+ }
+ 
+ static void ice_init_wakeup(struct ice_pf *pf)
+ {
+ 	/* Save wakeup reason register for later use */
+ 	pf->wakeup_reason = rd32(&pf->hw, PFPM_WUS);
+ 
+ 	/* check for a power management event */
+ 	ice_print_wake_reason(pf);
+ 
+ 	/* clear wake status, all bits */
+ 	wr32(&pf->hw, PFPM_WUS, U32_MAX);
+ 
+ 	/* Disable WoL at init, wait for user to enable */
+ 	device_set_wakeup_enable(ice_pf_to_dev(pf), false);
+ }
+ 
+ static int ice_init_link(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 	int err;
+ 
+ 	err = ice_init_link_events(pf->hw.port_info);
+ 	if (err) {
+ 		dev_err(dev, "ice_init_link_events failed: %d\n", err);
+ 		return err;
+ 	}
+ 
+ 	/* not a fatal error if this fails */
+ 	err = ice_init_nvm_phy_type(pf->hw.port_info);
+ 	if (err)
+ 		dev_err(dev, "ice_init_nvm_phy_type failed: %d\n", err);
+ 
+ 	/* not a fatal error if this fails */
+ 	err = ice_update_link_info(pf->hw.port_info);
+ 	if (err)
+ 		dev_err(dev, "ice_update_link_info failed: %d\n", err);
+ 
+ 	ice_init_link_dflt_override(pf->hw.port_info);
+ 
+ 	ice_check_link_cfg_err(pf,
+ 			       pf->hw.port_info->phy.link_info.link_cfg_err);
+ 
+ 	/* if media available, initialize PHY settings */
+ 	if (pf->hw.port_info->phy.link_info.link_info &
+ 	    ICE_AQ_MEDIA_AVAILABLE) {
+ 		/* not a fatal error if this fails */
+ 		err = ice_init_phy_user_cfg(pf->hw.port_info);
+ 		if (err)
+ 			dev_err(dev, "ice_init_phy_user_cfg failed: %d\n", err);
+ 
+ 		if (!test_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, pf->flags)) {
+ 			struct ice_vsi *vsi = ice_get_main_vsi(pf);
+ 
+ 			if (vsi)
+ 				ice_configure_phy(vsi);
+ 		}
+ 	} else {
+ 		set_bit(ICE_FLAG_NO_MEDIA, pf->flags);
+ 	}
+ 
+ 	return err;
+ }
+ 
+ static int ice_init_pf_sw(struct ice_pf *pf)
+ {
+ 	bool dvm = ice_is_dvm_ena(&pf->hw);
+ 	struct ice_vsi *vsi;
+ 	int err;
+ 
+ 	/* create switch struct for the switch element created by FW on boot */
+ 	pf->first_sw = kzalloc(sizeof(*pf->first_sw), GFP_KERNEL);
+ 	if (!pf->first_sw)
+ 		return -ENOMEM;
+ 
+ 	if (pf->hw.evb_veb)
+ 		pf->first_sw->bridge_mode = BRIDGE_MODE_VEB;
+ 	else
+ 		pf->first_sw->bridge_mode = BRIDGE_MODE_VEPA;
+ 
+ 	pf->first_sw->pf = pf;
+ 
+ 	/* record the sw_id available for later use */
+ 	pf->first_sw->sw_id = pf->hw.port_info->sw_id;
+ 
+ 	err = ice_aq_set_port_params(pf->hw.port_info, dvm, NULL);
+ 	if (err)
+ 		goto err_aq_set_port_params;
+ 
+ 	vsi = ice_pf_vsi_setup(pf, pf->hw.port_info);
+ 	if (!vsi) {
+ 		err = -ENOMEM;
+ 		goto err_pf_vsi_setup;
+ 	}
+ 
+ 	return 0;
+ 
+ err_pf_vsi_setup:
+ err_aq_set_port_params:
+ 	kfree(pf->first_sw);
+ 	return err;
+ }
+ 
+ static void ice_deinit_pf_sw(struct ice_pf *pf)
+ {
+ 	struct ice_vsi *vsi = ice_get_main_vsi(pf);
+ 
+ 	if (!vsi)
+ 		return;
+ 
+ 	ice_vsi_release(vsi);
+ 	kfree(pf->first_sw);
+ }
+ 
+ static int ice_alloc_vsis(struct ice_pf *pf)
+ {
+ 	struct device *dev = ice_pf_to_dev(pf);
+ 
+ 	pf->num_alloc_vsi = pf->hw.func_caps.guar_num_vsi;
+ 	if (!pf->num_alloc_vsi)
+ 		return -EIO;
+ 
+ 	if (pf->num_alloc_vsi > UDP_TUNNEL_NIC_MAX_SHARING_DEVICES) {
+ 		dev_warn(dev,
+ 			 "limiting the VSI count due to UDP tunnel limitation %d > %d\n",
+ 			 pf->num_alloc_vsi, UDP_TUNNEL_NIC_MAX_SHARING_DEVICES);
+ 		pf->num_alloc_vsi = UDP_TUNNEL_NIC_MAX_SHARING_DEVICES;
+ 	}
+ 
+ 	pf->vsi = devm_kcalloc(dev, pf->num_alloc_vsi, sizeof(*pf->vsi),
+ 			       GFP_KERNEL);
+ 	if (!pf->vsi)
+ 		return -ENOMEM;
+ 
+ 	pf->vsi_stats = devm_kcalloc(dev, pf->num_alloc_vsi,
+ 				     sizeof(*pf->vsi_stats), GFP_KERNEL);
+ 	if (!pf->vsi_stats) {
+ 		devm_kfree(dev, pf->vsi);
+ 		return -ENOMEM;
+ 	}
+ 
+ 	return 0;
+ }
+ 
+ static void ice_dealloc_vsis(struct ice_pf *pf)
+ {
+ 	devm_kfree(ice_pf_to_dev(pf), pf->vsi_stats);
+ 	pf->vsi_stats = NULL;
+ 
+ 	pf->num_alloc_vsi = 0;
+ 	devm_kfree(ice_pf_to_dev(pf), pf->vsi);
+ 	pf->vsi = NULL;
+ }
+ 
+ static int ice_init_devlink(struct ice_pf *pf)
+ {
+ 	int err;
+ 
+ 	err = ice_devlink_register_params(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	ice_devlink_init_regions(pf);
+ 	ice_devlink_register(pf);
+ 
+ 	return 0;
+ }
+ 
+ static void ice_deinit_devlink(struct ice_pf *pf)
+ {
+ 	ice_devlink_unregister(pf);
+ 	ice_devlink_destroy_regions(pf);
+ 	ice_devlink_unregister_params(pf);
+ }
+ 
+ static int ice_init(struct ice_pf *pf)
+ {
+ 	int err;
+ 
+ 	err = ice_init_dev(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	err = ice_alloc_vsis(pf);
+ 	if (err)
+ 		goto err_alloc_vsis;
+ 
+ 	err = ice_init_pf_sw(pf);
+ 	if (err)
+ 		goto err_init_pf_sw;
+ 
+ 	ice_init_wakeup(pf);
+ 
+ 	err = ice_init_link(pf);
+ 	if (err)
+ 		goto err_init_link;
+ 
+ 	err = ice_send_version(pf);
+ 	if (err)
+ 		goto err_init_link;
+ 
+ 	ice_verify_cacheline_size(pf);
+ 
+ 	if (ice_is_safe_mode(pf))
+ 		ice_set_safe_mode_vlan_cfg(pf);
+ 	else
+ 		/* print PCI link speed and width */
+ 		pcie_print_link_status(pf->pdev);
+ 
+ 	/* ready to go, so clear down state bit */
+ 	clear_bit(ICE_DOWN, pf->state);
+ 	clear_bit(ICE_SERVICE_DIS, pf->state);
+ 
+ 	/* since everything is good, start the service timer */
+ 	mod_timer(&pf->serv_tmr, round_jiffies(jiffies + pf->serv_tmr_period));
+ 
+ 	return 0;
+ 
+ err_init_link:
+ 	ice_deinit_pf_sw(pf);
+ err_init_pf_sw:
+ 	ice_dealloc_vsis(pf);
+ err_alloc_vsis:
+ 	ice_deinit_dev(pf);
+ 	return err;
+ }
+ 
+ static void ice_deinit(struct ice_pf *pf)
+ {
+ 	set_bit(ICE_SERVICE_DIS, pf->state);
+ 	set_bit(ICE_DOWN, pf->state);
+ 
+ 	ice_deinit_pf_sw(pf);
+ 	ice_dealloc_vsis(pf);
+ 	ice_deinit_dev(pf);
+ }
+ 
+ /**
+  * ice_load - load pf by init hw and starting VSI
+  * @pf: pointer to the pf instance
+  */
+ int ice_load(struct ice_pf *pf)
+ {
+ 	struct ice_vsi_cfg_params params = {};
+ 	struct ice_vsi *vsi;
+ 	int err;
+ 
+ 	err = ice_init_dev(pf);
+ 	if (err)
+ 		return err;
+ 
+ 	vsi = ice_get_main_vsi(pf);
+ 
+ 	params = ice_vsi_to_params(vsi);
+ 	params.flags = ICE_VSI_FLAG_INIT;
+ 
+ 	rtnl_lock();
+ 	err = ice_vsi_cfg(vsi, &params);
+ 	if (err)
+ 		goto err_vsi_cfg;
+ 
+ 	err = ice_start_eth(ice_get_main_vsi(pf));
+ 	if (err)
+ 		goto err_start_eth;
+ 	rtnl_unlock();
+ 
+ 	err = ice_init_rdma(pf);
+ 	if (err)
+ 		goto err_init_rdma;
+ 
+ 	ice_init_features(pf);
+ 	ice_service_task_restart(pf);
+ 
+ 	clear_bit(ICE_DOWN, pf->state);
+ 
+ 	return 0;
+ 
+ err_init_rdma:
+ 	ice_vsi_close(ice_get_main_vsi(pf));
+ 	rtnl_lock();
+ err_start_eth:
+ 	ice_vsi_decfg(ice_get_main_vsi(pf));
+ err_vsi_cfg:
+ 	rtnl_unlock();
+ 	ice_deinit_dev(pf);
+ 	return err;
+ }
+ 
+ /**
+  * ice_unload - unload pf by stopping VSI and deinit hw
+  * @pf: pointer to the pf instance
+  */
+ void ice_unload(struct ice_pf *pf)
+ {
+ 	ice_deinit_features(pf);
+ 	ice_deinit_rdma(pf);
+ 	rtnl_lock();
+ 	ice_stop_eth(ice_get_main_vsi(pf));
+ 	ice_vsi_decfg(ice_get_main_vsi(pf));
+ 	rtnl_unlock();
+ 	ice_deinit_dev(pf);
+ }
+ 
++>>>>>>> b3e7b3a6ee92 (ice: prevent NULL pointer deref during reload)
  /**
   * ice_probe - Device initialization routine
   * @pdev: PCI device information struct
diff --git a/drivers/net/ethernet/intel/ice/ice_base.c b/drivers/net/ethernet/intel/ice/ice_base.c
index 554095b25f44..8a4f3e53936f 100644
--- a/drivers/net/ethernet/intel/ice/ice_base.c
+++ b/drivers/net/ethernet/intel/ice/ice_base.c
@@ -755,6 +755,8 @@ void ice_vsi_free_q_vectors(struct ice_vsi *vsi)
 
 	ice_for_each_q_vector(vsi, v_idx)
 		ice_free_q_vector(vsi, v_idx);
+
+	vsi->num_q_vectors = 0;
 }
 
 /**
diff --git a/drivers/net/ethernet/intel/ice/ice_ethtool.c b/drivers/net/ethernet/intel/ice/ice_ethtool.c
index 6db39a57efab..96a0d372c754 100644
--- a/drivers/net/ethernet/intel/ice/ice_ethtool.c
+++ b/drivers/net/ethernet/intel/ice/ice_ethtool.c
@@ -2920,8 +2920,13 @@ ice_get_ringparam(struct net_device *netdev, struct ethtool_ringparam *ring,
 
 	ring->rx_max_pending = ICE_MAX_NUM_DESC;
 	ring->tx_max_pending = ICE_MAX_NUM_DESC;
-	ring->rx_pending = vsi->rx_rings[0]->count;
-	ring->tx_pending = vsi->tx_rings[0]->count;
+	if (vsi->tx_rings && vsi->rx_rings) {
+		ring->rx_pending = vsi->rx_rings[0]->count;
+		ring->tx_pending = vsi->tx_rings[0]->count;
+	} else {
+		ring->rx_pending = 0;
+		ring->tx_pending = 0;
+	}
 
 	/* Rx mini and jumbo rings are not supported */
 	ring->rx_mini_max_pending = 0;
@@ -2955,6 +2960,10 @@ ice_set_ringparam(struct net_device *netdev, struct ethtool_ringparam *ring,
 		return -EINVAL;
 	}
 
+	/* Return if there is no rings (device is reloading) */
+	if (!vsi->tx_rings || !vsi->rx_rings)
+		return -EBUSY;
+
 	new_tx_cnt = ALIGN(ring->tx_pending, ICE_REQ_DESC_MULTIPLE);
 	if (new_tx_cnt != ring->tx_pending)
 		netdev_info(netdev, "Requested Tx descriptor count rounded up to %d\n",
* Unmerged path drivers/net/ethernet/intel/ice/ice_main.c
