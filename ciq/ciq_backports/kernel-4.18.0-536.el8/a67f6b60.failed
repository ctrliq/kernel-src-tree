x86/hyperv: Move the code in ivm.c around to avoid unnecessary ifdef's

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-536.el8
commit-author Dexuan Cui <decui@microsoft.com>
commit a67f6b60d6ed953d5b23a22f26fc916aab630aaa
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-536.el8/a67f6b60.failed

Group the code this way so that we can avoid too many ifdef's:

  Data only used in an SNP VM with the paravisor;
  Functions only used in an SNP VM with the paravisor;

  Data only used in an SNP VM without the paravisor;
  Functions only used in an SNP VM without the paravisor;

  Functions only used in a TDX VM, with and without the paravisor;

  Functions used in an SNP or TDX VM, when the paravisor is present;

  Functions always used, even in a regular non-CoCo VM.

No functional change.

	Signed-off-by: Dexuan Cui <decui@microsoft.com>
	Reviewed-by: Michael Kelley <mikelley@microsoft.com>
	Reviewed-by: Tianyu Lan <tiala@microsoft.com>
	Signed-off-by: Wei Liu <wei.liu@kernel.org>
Link: https://lore.kernel.org/r/20230824080712.30327-11-decui@microsoft.com
(cherry picked from commit a67f6b60d6ed953d5b23a22f26fc916aab630aaa)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/x86/hyperv/ivm.c
diff --cc arch/x86/hyperv/ivm.c
index 5648efb6c73e,8fb3b28670e9..000000000000
--- a/arch/x86/hyperv/ivm.c
+++ b/arch/x86/hyperv/ivm.c
@@@ -53,8 -63,10 +53,13 @@@ union hv_ghcb 
  	} hypercall;
  } __packed __aligned(HV_HYP_PAGE_SIZE);
  
++<<<<<<< HEAD
++=======
+ /* Only used in an SNP VM with the paravisor */
++>>>>>>> a67f6b60d6ed (x86/hyperv: Move the code in ivm.c around to avoid unnecessary ifdef's)
  static u16 hv_ghcb_version __ro_after_init;
  
+ /* Functions only used in an SNP VM with the paravisor go here. */
  u64 hv_ghcb_hypercall(u64 control, void *input, void *output, u32 input_size)
  {
  	union hv_ghcb *hv_ghcb;
@@@ -232,127 -243,381 +237,463 @@@ void hv_ghcb_msr_read(u64 msr, u64 *val
  			| ((u64)lower_32_bits(hv_ghcb->ghcb.save.rdx) << 32);
  	local_irq_restore(flags);
  }
++<<<<<<< HEAD
 +EXPORT_SYMBOL_GPL(hv_ghcb_msr_read);
 +#endif
 +
 +/*
 + * hv_mark_gpa_visibility - Set pages visible to host via hvcall.
 + *
 + * In Isolation VM, all guest memory is encrypted from host and guest
 + * needs to set memory visible to host via hvcall before sharing memory
 + * with host.
 + */
 +static int hv_mark_gpa_visibility(u16 count, const u64 pfn[],
 +			   enum hv_mem_host_visibility visibility)
 +{
 +	struct hv_gpa_range_for_visibility **input_pcpu, *input;
 +	u16 pages_processed;
 +	u64 hv_status;
 +	unsigned long flags;
 +
 +	/* no-op if partition isolation is not enabled */
 +	if (!hv_is_isolation_supported())
 +		return 0;
 +
 +	if (count > HV_MAX_MODIFY_GPA_REP_COUNT) {
 +		pr_err("Hyper-V: GPA count:%d exceeds supported:%lu\n", count,
 +			HV_MAX_MODIFY_GPA_REP_COUNT);
 +		return -EINVAL;
 +	}
 +
 +	local_irq_save(flags);
 +	input_pcpu = (struct hv_gpa_range_for_visibility **)
 +			this_cpu_ptr(hyperv_pcpu_input_arg);
 +	input = *input_pcpu;
 +	if (unlikely(!input)) {
 +		local_irq_restore(flags);
 +		return -EINVAL;
 +	}
 +
 +	input->partition_id = HV_PARTITION_ID_SELF;
 +	input->host_visibility = visibility;
 +	input->reserved0 = 0;
 +	input->reserved1 = 0;
 +	memcpy((void *)input->gpa_page_list, pfn, count * sizeof(*pfn));
 +	hv_status = hv_do_rep_hypercall(
 +			HVCALL_MODIFY_SPARSE_GPA_PAGE_HOST_VISIBILITY, count,
 +			0, input, &pages_processed);
 +	local_irq_restore(flags);
 +
 +	if (hv_result_success(hv_status))
 +		return 0;
 +	else
 +		return -EFAULT;
 +}
 +
 +/*
 + * hv_set_mem_host_visibility - Set specified memory visible to host.
 + *
 + * In Isolation VM, all guest memory is encrypted from host and guest
 + * needs to set memory visible to host via hvcall before sharing memory
 + * with host. This function works as wrap of hv_mark_gpa_visibility()
 + * with memory base and size.
 + */
 +int hv_set_mem_host_visibility(unsigned long kbuffer, int pagecount, bool visible)
 +{
 +	enum hv_mem_host_visibility visibility = visible ?
 +			VMBUS_PAGE_VISIBLE_READ_WRITE : VMBUS_PAGE_NOT_VISIBLE;
 +	u64 *pfn_array;
 +	int ret = 0;
 +	int i, pfn;
 +
 +	if (!hv_is_isolation_supported() || !hv_hypercall_pg)
 +		return 0;
 +
 +	pfn_array = kmalloc(HV_HYP_PAGE_SIZE, GFP_KERNEL);
 +	if (!pfn_array)
 +		return -ENOMEM;
 +
 +	for (i = 0, pfn = 0; i < pagecount; i++) {
 +		pfn_array[pfn] = virt_to_hvpfn((void *)kbuffer + i * HV_HYP_PAGE_SIZE);
 +		pfn++;
 +
 +		if (pfn == HV_MAX_MODIFY_GPA_REP_COUNT || i == pagecount - 1) {
 +			ret = hv_mark_gpa_visibility(pfn, pfn_array,
 +						     visibility);
 +			if (ret)
 +				goto err_free_pfn_array;
 +			pfn = 0;
 +		}
 +	}
 +
 + err_free_pfn_array:
 +	kfree(pfn_array);
 +	return ret;
 +}
 +
 +/*
 + * hv_map_memory - map memory to extra space in the AMD SEV-SNP Isolation VM.
 + */
 +void *hv_map_memory(void *addr, unsigned long size)
++=======
+ 
+ /* Only used in a fully enlightened SNP VM, i.e. without the paravisor */
+ static u8 ap_start_input_arg[PAGE_SIZE] __bss_decrypted __aligned(PAGE_SIZE);
+ static u8 ap_start_stack[PAGE_SIZE] __aligned(PAGE_SIZE);
+ static DEFINE_PER_CPU(struct sev_es_save_area *, hv_sev_vmsa);
+ 
+ /* Functions only used in an SNP VM without the paravisor go here. */
+ 
+ #define hv_populate_vmcb_seg(seg, gdtr_base)			\
+ do {								\
+ 	if (seg.selector) {					\
+ 		seg.base = 0;					\
+ 		seg.limit = HV_AP_SEGMENT_LIMIT;		\
+ 		seg.attrib = *(u16 *)(gdtr_base + seg.selector + 5);	\
+ 		seg.attrib = (seg.attrib & 0xFF) | ((seg.attrib >> 4) & 0xF00); \
+ 	}							\
+ } while (0)							\
+ 
+ static int snp_set_vmsa(void *va, bool vmsa)
+ {
+ 	u64 attrs;
+ 
+ 	/*
+ 	 * Running at VMPL0 allows the kernel to change the VMSA bit for a page
+ 	 * using the RMPADJUST instruction. However, for the instruction to
+ 	 * succeed it must target the permissions of a lesser privileged
+ 	 * (higher numbered) VMPL level, so use VMPL1 (refer to the RMPADJUST
+ 	 * instruction in the AMD64 APM Volume 3).
+ 	 */
+ 	attrs = 1;
+ 	if (vmsa)
+ 		attrs |= RMPADJUST_VMSA_PAGE_BIT;
+ 
+ 	return rmpadjust((unsigned long)va, RMP_PG_SIZE_4K, attrs);
+ }
+ 
+ static void snp_cleanup_vmsa(struct sev_es_save_area *vmsa)
+ {
+ 	int err;
+ 
+ 	err = snp_set_vmsa(vmsa, false);
+ 	if (err)
+ 		pr_err("clear VMSA page failed (%u), leaking page\n", err);
+ 	else
+ 		free_page((unsigned long)vmsa);
+ }
+ 
+ int hv_snp_boot_ap(int cpu, unsigned long start_ip)
+ {
+ 	struct sev_es_save_area *vmsa = (struct sev_es_save_area *)
+ 		__get_free_page(GFP_KERNEL | __GFP_ZERO);
+ 	struct sev_es_save_area *cur_vmsa;
+ 	struct desc_ptr gdtr;
+ 	u64 ret, retry = 5;
+ 	struct hv_enable_vp_vtl *start_vp_input;
+ 	unsigned long flags;
+ 
+ 	if (!vmsa)
+ 		return -ENOMEM;
+ 
+ 	native_store_gdt(&gdtr);
+ 
+ 	vmsa->gdtr.base = gdtr.address;
+ 	vmsa->gdtr.limit = gdtr.size;
+ 
+ 	asm volatile("movl %%es, %%eax;" : "=a" (vmsa->es.selector));
+ 	hv_populate_vmcb_seg(vmsa->es, vmsa->gdtr.base);
+ 
+ 	asm volatile("movl %%cs, %%eax;" : "=a" (vmsa->cs.selector));
+ 	hv_populate_vmcb_seg(vmsa->cs, vmsa->gdtr.base);
+ 
+ 	asm volatile("movl %%ss, %%eax;" : "=a" (vmsa->ss.selector));
+ 	hv_populate_vmcb_seg(vmsa->ss, vmsa->gdtr.base);
+ 
+ 	asm volatile("movl %%ds, %%eax;" : "=a" (vmsa->ds.selector));
+ 	hv_populate_vmcb_seg(vmsa->ds, vmsa->gdtr.base);
+ 
+ 	vmsa->efer = native_read_msr(MSR_EFER);
+ 
+ 	asm volatile("movq %%cr4, %%rax;" : "=a" (vmsa->cr4));
+ 	asm volatile("movq %%cr3, %%rax;" : "=a" (vmsa->cr3));
+ 	asm volatile("movq %%cr0, %%rax;" : "=a" (vmsa->cr0));
+ 
+ 	vmsa->xcr0 = 1;
+ 	vmsa->g_pat = HV_AP_INIT_GPAT_DEFAULT;
+ 	vmsa->rip = (u64)secondary_startup_64_no_verify;
+ 	vmsa->rsp = (u64)&ap_start_stack[PAGE_SIZE];
+ 
+ 	/*
+ 	 * Set the SNP-specific fields for this VMSA:
+ 	 *   VMPL level
+ 	 *   SEV_FEATURES (matches the SEV STATUS MSR right shifted 2 bits)
+ 	 */
+ 	vmsa->vmpl = 0;
+ 	vmsa->sev_features = sev_status >> 2;
+ 
+ 	ret = snp_set_vmsa(vmsa, true);
+ 	if (!ret) {
+ 		pr_err("RMPADJUST(%llx) failed: %llx\n", (u64)vmsa, ret);
+ 		free_page((u64)vmsa);
+ 		return ret;
+ 	}
+ 
+ 	local_irq_save(flags);
+ 	start_vp_input = (struct hv_enable_vp_vtl *)ap_start_input_arg;
+ 	memset(start_vp_input, 0, sizeof(*start_vp_input));
+ 	start_vp_input->partition_id = -1;
+ 	start_vp_input->vp_index = cpu;
+ 	start_vp_input->target_vtl.target_vtl = ms_hyperv.vtl;
+ 	*(u64 *)&start_vp_input->vp_context = __pa(vmsa) | 1;
+ 
+ 	do {
+ 		ret = hv_do_hypercall(HVCALL_START_VP,
+ 				      start_vp_input, NULL);
+ 	} while (hv_result(ret) == HV_STATUS_TIME_OUT && retry--);
+ 
+ 	local_irq_restore(flags);
+ 
+ 	if (!hv_result_success(ret)) {
+ 		pr_err("HvCallStartVirtualProcessor failed: %llx\n", ret);
+ 		snp_cleanup_vmsa(vmsa);
+ 		vmsa = NULL;
+ 	}
+ 
+ 	cur_vmsa = per_cpu(hv_sev_vmsa, cpu);
+ 	/* Free up any previous VMSA page */
+ 	if (cur_vmsa)
+ 		snp_cleanup_vmsa(cur_vmsa);
+ 
+ 	/* Record the current VMSA page */
+ 	per_cpu(hv_sev_vmsa, cpu) = vmsa;
+ 
+ 	return ret;
+ }
+ 
+ #else
+ static inline void hv_ghcb_msr_write(u64 msr, u64 value) {}
+ static inline void hv_ghcb_msr_read(u64 msr, u64 *value) {}
+ #endif /* CONFIG_AMD_MEM_ENCRYPT */
+ 
+ #ifdef CONFIG_INTEL_TDX_GUEST
+ static void hv_tdx_msr_write(u64 msr, u64 val)
+ {
+ 	struct tdx_hypercall_args args = {
+ 		.r10 = TDX_HYPERCALL_STANDARD,
+ 		.r11 = EXIT_REASON_MSR_WRITE,
+ 		.r12 = msr,
+ 		.r13 = val,
+ 	};
+ 
+ 	u64 ret = __tdx_hypercall(&args);
+ 
+ 	WARN_ONCE(ret, "Failed to emulate MSR write: %lld\n", ret);
+ }
+ 
+ static void hv_tdx_msr_read(u64 msr, u64 *val)
+ {
+ 	struct tdx_hypercall_args args = {
+ 		.r10 = TDX_HYPERCALL_STANDARD,
+ 		.r11 = EXIT_REASON_MSR_READ,
+ 		.r12 = msr,
+ 	};
+ 
+ 	u64 ret = __tdx_hypercall_ret(&args);
+ 
+ 	if (WARN_ONCE(ret, "Failed to emulate MSR read: %lld\n", ret))
+ 		*val = 0;
+ 	else
+ 		*val = args.r11;
+ }
+ 
+ u64 hv_tdx_hypercall(u64 control, u64 param1, u64 param2)
+ {
+ 	struct tdx_hypercall_args args = { };
+ 
+ 	args.r10 = control;
+ 	args.rdx = param1;
+ 	args.r8  = param2;
+ 
+ 	(void)__tdx_hypercall_ret(&args);
+ 
+ 	return args.r11;
+ }
+ 
+ #else
+ static inline void hv_tdx_msr_write(u64 msr, u64 value) {}
+ static inline void hv_tdx_msr_read(u64 msr, u64 *value) {}
+ #endif /* CONFIG_INTEL_TDX_GUEST */
+ 
+ #if defined(CONFIG_AMD_MEM_ENCRYPT) || defined(CONFIG_INTEL_TDX_GUEST)
+ void hv_ivm_msr_write(u64 msr, u64 value)
+ {
+ 	if (!ms_hyperv.paravisor_present)
+ 		return;
+ 
+ 	if (hv_isolation_type_tdx())
+ 		hv_tdx_msr_write(msr, value);
+ 	else if (hv_isolation_type_snp())
+ 		hv_ghcb_msr_write(msr, value);
+ }
+ 
+ void hv_ivm_msr_read(u64 msr, u64 *value)
+ {
+ 	if (!ms_hyperv.paravisor_present)
+ 		return;
+ 
+ 	if (hv_isolation_type_tdx())
+ 		hv_tdx_msr_read(msr, value);
+ 	else if (hv_isolation_type_snp())
+ 		hv_ghcb_msr_read(msr, value);
+ }
+ 
+ /*
+  * hv_mark_gpa_visibility - Set pages visible to host via hvcall.
+  *
+  * In Isolation VM, all guest memory is encrypted from host and guest
+  * needs to set memory visible to host via hvcall before sharing memory
+  * with host.
+  */
+ static int hv_mark_gpa_visibility(u16 count, const u64 pfn[],
+ 			   enum hv_mem_host_visibility visibility)
+ {
+ 	struct hv_gpa_range_for_visibility **input_pcpu, *input;
+ 	u16 pages_processed;
+ 	u64 hv_status;
+ 	unsigned long flags;
+ 
+ 	/* no-op if partition isolation is not enabled */
+ 	if (!hv_is_isolation_supported())
+ 		return 0;
+ 
+ 	if (count > HV_MAX_MODIFY_GPA_REP_COUNT) {
+ 		pr_err("Hyper-V: GPA count:%d exceeds supported:%lu\n", count,
+ 			HV_MAX_MODIFY_GPA_REP_COUNT);
+ 		return -EINVAL;
+ 	}
+ 
+ 	local_irq_save(flags);
+ 	input_pcpu = (struct hv_gpa_range_for_visibility **)
+ 			this_cpu_ptr(hyperv_pcpu_input_arg);
+ 	input = *input_pcpu;
+ 	if (unlikely(!input)) {
+ 		local_irq_restore(flags);
+ 		return -EINVAL;
+ 	}
+ 
+ 	input->partition_id = HV_PARTITION_ID_SELF;
+ 	input->host_visibility = visibility;
+ 	input->reserved0 = 0;
+ 	input->reserved1 = 0;
+ 	memcpy((void *)input->gpa_page_list, pfn, count * sizeof(*pfn));
+ 	hv_status = hv_do_rep_hypercall(
+ 			HVCALL_MODIFY_SPARSE_GPA_PAGE_HOST_VISIBILITY, count,
+ 			0, input, &pages_processed);
+ 	local_irq_restore(flags);
+ 
+ 	if (hv_result_success(hv_status))
+ 		return 0;
+ 	else
+ 		return -EFAULT;
+ }
+ 
+ /*
+  * hv_vtom_set_host_visibility - Set specified memory visible to host.
+  *
+  * In Isolation VM, all guest memory is encrypted from host and guest
+  * needs to set memory visible to host via hvcall before sharing memory
+  * with host. This function works as wrap of hv_mark_gpa_visibility()
+  * with memory base and size.
+  */
+ static bool hv_vtom_set_host_visibility(unsigned long kbuffer, int pagecount, bool enc)
+ {
+ 	enum hv_mem_host_visibility visibility = enc ?
+ 			VMBUS_PAGE_NOT_VISIBLE : VMBUS_PAGE_VISIBLE_READ_WRITE;
+ 	u64 *pfn_array;
+ 	int ret = 0;
+ 	bool result = true;
+ 	int i, pfn;
+ 
+ 	pfn_array = kmalloc(HV_HYP_PAGE_SIZE, GFP_KERNEL);
+ 	if (!pfn_array)
+ 		return false;
+ 
+ 	for (i = 0, pfn = 0; i < pagecount; i++) {
+ 		pfn_array[pfn] = virt_to_hvpfn((void *)kbuffer + i * HV_HYP_PAGE_SIZE);
+ 		pfn++;
+ 
+ 		if (pfn == HV_MAX_MODIFY_GPA_REP_COUNT || i == pagecount - 1) {
+ 			ret = hv_mark_gpa_visibility(pfn, pfn_array,
+ 						     visibility);
+ 			if (ret) {
+ 				result = false;
+ 				goto err_free_pfn_array;
+ 			}
+ 			pfn = 0;
+ 		}
+ 	}
+ 
+  err_free_pfn_array:
+ 	kfree(pfn_array);
+ 	return result;
+ }
+ 
+ static bool hv_vtom_tlb_flush_required(bool private)
+ {
+ 	return true;
+ }
+ 
+ static bool hv_vtom_cache_flush_required(void)
+ {
+ 	return false;
+ }
+ 
+ static bool hv_is_private_mmio(u64 addr)
+ {
+ 	/*
+ 	 * Hyper-V always provides a single IO-APIC in a guest VM.
+ 	 * When a paravisor is used, it is emulated by the paravisor
+ 	 * in the guest context and must be mapped private.
+ 	 */
+ 	if (addr >= HV_IOAPIC_BASE_ADDRESS &&
+ 	    addr < (HV_IOAPIC_BASE_ADDRESS + PAGE_SIZE))
+ 		return true;
+ 
+ 	/* Same with a vTPM */
+ 	if (addr >= VTPM_BASE_ADDRESS &&
+ 	    addr < (VTPM_BASE_ADDRESS + PAGE_SIZE))
+ 		return true;
+ 
+ 	return false;
+ }
+ 
+ void __init hv_vtom_init(void)
++>>>>>>> a67f6b60d6ed (x86/hyperv: Move the code in ivm.c around to avoid unnecessary ifdef's)
  {
 -	enum hv_isolation_type type = hv_get_isolation_type();
 -
 -	switch (type) {
 -	case HV_ISOLATION_TYPE_VBS:
 -		fallthrough;
 -	/*
 -	 * By design, a VM using vTOM doesn't see the SEV setting,
 -	 * so SEV initialization is bypassed and sev_status isn't set.
 -	 * Set it here to indicate a vTOM VM.
 -	 *
 -	 * Note: if CONFIG_AMD_MEM_ENCRYPT is not set, sev_status is
 -	 * defined as 0ULL, to which we can't assigned a value.
 -	 */
 -#ifdef CONFIG_AMD_MEM_ENCRYPT
 -	case HV_ISOLATION_TYPE_SNP:
 -		sev_status = MSR_AMD64_SNP_VTOM;
 -		cc_vendor = CC_VENDOR_AMD;
 -		break;
 -#endif
 -
 -	case HV_ISOLATION_TYPE_TDX:
 -		cc_vendor = CC_VENDOR_INTEL;
 -		break;
 +	unsigned long *pfns = kcalloc(size / PAGE_SIZE,
 +				      sizeof(unsigned long), GFP_KERNEL);
 +	void *vaddr;
 +	int i;
  
 -	default:
 -		panic("hv_vtom_init: unsupported isolation type %d\n", type);
 -	}
 +	if (!pfns)
 +		return NULL;
  
 -	cc_set_mask(ms_hyperv.shared_gpa_boundary);
 -	physical_mask &= ms_hyperv.shared_gpa_boundary - 1;
 +	for (i = 0; i < size / PAGE_SIZE; i++)
 +		pfns[i] = vmalloc_to_pfn(addr + i * PAGE_SIZE) +
 +			(ms_hyperv.shared_gpa_boundary >> PAGE_SHIFT);
  
 -	x86_platform.hyper.is_private_mmio = hv_is_private_mmio;
 -	x86_platform.guest.enc_cache_flush_required = hv_vtom_cache_flush_required;
 -	x86_platform.guest.enc_tlb_flush_required = hv_vtom_tlb_flush_required;
 -	x86_platform.guest.enc_status_change_finish = hv_vtom_set_host_visibility;
 +	vaddr = vmap_pfn(pfns, size / PAGE_SIZE, pgprot_decrypted(PAGE_KERNEL));
 +	kfree(pfns);
  
 -	/* Set WB as the default cache mode. */
 -	mtrr_overwrite_state(NULL, 0, MTRR_TYPE_WRBACK);
 +	return vaddr;
  }
  
 -#endif /* defined(CONFIG_AMD_MEM_ENCRYPT) || defined(CONFIG_INTEL_TDX_GUEST) */
 +void hv_unmap_memory(void *addr)
 +{
 +	vunmap(addr);
 +}
  
  enum hv_isolation_type hv_get_isolation_type(void)
  {
@@@ -387,3 -652,13 +728,16 @@@ bool hv_isolation_type_snp(void
  {
  	return static_branch_unlikely(&isolation_type_snp);
  }
++<<<<<<< HEAD
++=======
+ 
+ DEFINE_STATIC_KEY_FALSE(isolation_type_tdx);
+ /*
+  * hv_isolation_type_tdx - Check if the system runs in an Intel TDX based
+  * isolated VM.
+  */
+ bool hv_isolation_type_tdx(void)
+ {
+ 	return static_branch_unlikely(&isolation_type_tdx);
+ }
++>>>>>>> a67f6b60d6ed (x86/hyperv: Move the code in ivm.c around to avoid unnecessary ifdef's)
* Unmerged path arch/x86/hyperv/ivm.c
