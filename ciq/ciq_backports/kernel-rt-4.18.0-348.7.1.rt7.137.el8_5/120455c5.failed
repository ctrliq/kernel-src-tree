sched: Fix hotplug vs CPU bandwidth control

jira NONE_AUTOMATION
Rebuild_History Non-Buildable kernel-rt-4.18.0-348.7.1.rt7.137.el8_5
commit-author Peter Zijlstra <peterz@infradead.org>
commit 120455c514f7321981c907a01c543b05aff3f254
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-rt-4.18.0-348.7.1.rt7.137.el8_5/120455c5.failed

Since we now migrate tasks away before DYING, we should also move
bandwidth unthrottle, otherwise we can gain tasks from unthrottle
after we expect all tasks to be gone already.

Also; it looks like the RT balancers don't respect cpu_active() and
instead rely on rq->online in part, complete this. This too requires
we do set_rq_offline() earlier to match the cpu_active() semantics.
(The bigger patch is to convert RT to cpu_active() entirely)

Since set_rq_online() is called from sched_cpu_activate(), place
set_rq_offline() in sched_cpu_deactivate().

	Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
	Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
	Reviewed-by: Daniel Bristot de Oliveira <bristot@redhat.com>
Link: https://lkml.kernel.org/r/20201023102346.639538965@infradead.org
(cherry picked from commit 120455c514f7321981c907a01c543b05aff3f254)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	kernel/sched/core.c
diff --cc kernel/sched/core.c
index 6068e0ed9d1c,dcb88a06ef14..000000000000
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@@ -6647,11 -7082,6 +6657,14 @@@ int sched_cpu_dying(unsigned int cpu
  	sched_tick_stop(cpu);
  
  	rq_lock_irqsave(rq, &rf);
++<<<<<<< HEAD
 +	if (rq->rd) {
 +		BUG_ON(!cpumask_test_cpu(cpu, rq->rd->span));
 +		set_rq_offline(rq);
 +	}
 +	migrate_tasks(rq, &rf);
++=======
++>>>>>>> 120455c514f7 (sched: Fix hotplug vs CPU bandwidth control)
  	BUG_ON(rq->nr_running != 1);
  	rq_unlock_irqrestore(rq, &rf);
  
* Unmerged path kernel/sched/core.c
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 7ffc40e0955b..2a4c2ebda9cd 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -581,7 +581,7 @@ static int push_dl_task(struct rq *rq);
 
 static inline bool need_pull_dl_task(struct rq *rq, struct task_struct *prev)
 {
-	return dl_task(prev);
+	return rq->online && dl_task(prev);
 }
 
 static DEFINE_PER_CPU(struct callback_head, dl_push_head);
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 47ba410a21c2..70a78d2ccd24 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -265,7 +265,7 @@ static void pull_rt_task(struct rq *this_rq);
 static inline bool need_pull_rt_task(struct rq *rq, struct task_struct *prev)
 {
 	/* Try to pull RT tasks here if we lower this rq's prio */
-	return rq->rt.highest_prio.curr > prev->prio;
+	return rq->online && rq->rt.highest_prio.curr > prev->prio;
 }
 
 static inline int rt_overloaded(struct rq *rq)
