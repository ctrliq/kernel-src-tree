cifs: deal with the channel loading lag while picking channels

jira LE-4669
Rebuild_History Non-Buildable kernel-4.18.0-553.82.1.el8_10
commit-author Shyam Prasad N <sprasad@microsoft.com>
commit 66d590b828b1fd9fa337047ae58fe1c4c6f43609
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-553.82.1.el8_10/66d590b8.failed

Our current approach to select a channel for sending requests is this:
1. iterate all channels to find the min and max queue depth
2. if min and max are not the same, pick the channel with min depth
3. if min and max are same, round robin, as all channels are equally loaded

The problem with this approach is that there's a lag between selecting
a channel and sending the request (that increases the queue depth on the channel).
While these numbers will eventually catch up, there could be a skew in the
channel usage, depending on the application's I/O parallelism and the server's
speed of handling requests.

With sufficient parallelism, this lag can artificially increase the queue depth,
thereby impacting the performance negatively.

This change will change the step 1 above to start the iteration from the last
selected channel. This is to reduce the skew in channel usage even in the presence
of this lag.

Fixes: ea90708d3cf3 ("cifs: use the least loaded channel for sending requests")
	Cc: <stable@vger.kernel.org>
	Signed-off-by: Shyam Prasad N <sprasad@microsoft.com>
	Signed-off-by: Steve French <stfrench@microsoft.com>
(cherry picked from commit 66d590b828b1fd9fa337047ae58fe1c4c6f43609)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/cifs/transport.c
diff --cc fs/cifs/transport.c
index 1bfe2ba50372,191783f553ce..000000000000
--- a/fs/cifs/transport.c
+++ b/fs/cifs/transport.c
@@@ -1043,21 -1016,43 +1043,59 @@@ cifs_cancelled_callback(struct mid_q_en
  struct TCP_Server_Info *cifs_pick_channel(struct cifs_ses *ses)
  {
  	uint index = 0;
++<<<<<<< HEAD:fs/cifs/transport.c
++=======
+ 	unsigned int min_in_flight = UINT_MAX, max_in_flight = 0;
+ 	struct TCP_Server_Info *server = NULL;
+ 	int i, start, cur;
++>>>>>>> 66d590b828b1 (cifs: deal with the channel loading lag while picking channels):fs/smb/client/transport.c
  
  	if (!ses)
  		return NULL;
  
  	spin_lock(&ses->chan_lock);
++<<<<<<< HEAD:fs/cifs/transport.c
 +	/* round robin */
 +pick_another:
 +	if (ses->chan_count > 1 &&
 +	    !CIFS_ALL_CHANS_NEED_RECONNECT(ses)) {
 +		index = (uint)atomic_inc_return(&ses->chan_seq);
 +		index %= ses->chan_count;
 +
 +		if (CIFS_CHAN_NEEDS_RECONNECT(ses, index))
 +			goto pick_another;
 +	}
++=======
+ 	start = atomic_inc_return(&ses->chan_seq);
+ 	for (i = 0; i < ses->chan_count; i++) {
+ 		cur = (start + i) % ses->chan_count;
+ 		server = ses->chans[cur].server;
+ 		if (!server || server->terminate)
+ 			continue;
+ 
+ 		if (CIFS_CHAN_NEEDS_RECONNECT(ses, i))
+ 			continue;
+ 
+ 		/*
+ 		 * strictly speaking, we should pick up req_lock to read
+ 		 * server->in_flight. But it shouldn't matter much here if we
+ 		 * race while reading this data. The worst that can happen is
+ 		 * that we could use a channel that's not least loaded. Avoiding
+ 		 * taking the lock could help reduce wait time, which is
+ 		 * important for this function
+ 		 */
+ 		if (server->in_flight < min_in_flight) {
+ 			min_in_flight = server->in_flight;
+ 			index = cur;
+ 		}
+ 		if (server->in_flight > max_in_flight)
+ 			max_in_flight = server->in_flight;
+ 	}
+ 
+ 	/* if all channels are equally loaded, fall back to round-robin */
+ 	if (min_in_flight == max_in_flight)
+ 		index = (uint)start % ses->chan_count;
++>>>>>>> 66d590b828b1 (cifs: deal with the channel loading lag while picking channels):fs/smb/client/transport.c
  
  	server = ses->chans[index].server;
  	spin_unlock(&ses->chan_lock);
* Unmerged path fs/cifs/transport.c
