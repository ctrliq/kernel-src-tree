KVM: arm64: Correctly handle page aging notifiers for unaligned memslot

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-512.el8
commit-author Oliver Upton <oliver.upton@linux.dev>
commit df6556adf27b7372cfcd97e1c0afb0d516c8279f
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-512.el8/df6556ad.failed

Userspace is allowed to select any PAGE_SIZE aligned hva to back guest
memory. This is even the case with hugepages, although it is a rather
suboptimal configuration as PTE level mappings are used at stage-2.

The arm64 page aging handlers have an assumption that the specified
range is exactly one page/block of memory, which in the aforementioned
case is not necessarily true. All together this leads to the WARN() in
kvm_age_gfn() firing.

However, the WARN is only part of the issue as the table walkers visit
at most a single leaf PTE. For hugepage-backed memory in a memslot that
isn't hugepage-aligned, page aging entirely misses accesses to the
hugepage beyond the first page in the memslot.

Add a new walker dedicated to handling page aging MMU notifiers capable
of walking a range of PTEs. Convert kvm(_test)_age_gfn() over to the new
walker and drop the WARN that caught the issue in the first place. The
implementation of this walker was inspired by the test_clear_young()
implementation by Yu Zhao [*], but repurposed to address a bug in the
existing aging implementation.

	Cc: stable@vger.kernel.org # v5.15
Fixes: 056aad67f836 ("kvm: arm/arm64: Rework gpa callback handlers")
Link: https://lore.kernel.org/kvmarm/20230526234435.662652-6-yuzhao@google.com/
Co-developed-by: Yu Zhao <yuzhao@google.com>
	Signed-off-by: Yu Zhao <yuzhao@google.com>
	Reported-by: Reiji Watanabe <reijiw@google.com>
	Reviewed-by: Marc Zyngier <maz@kernel.org>
	Reviewed-by: Shaoqin Huang <shahuang@redhat.com>
Link: https://lore.kernel.org/r/20230627235405.4069823-1-oliver.upton@linux.dev
	Signed-off-by: Oliver Upton <oliver.upton@linux.dev>
(cherry picked from commit df6556adf27b7372cfcd97e1c0afb0d516c8279f)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/arm64/include/asm/kvm_pgtable.h
#	arch/arm64/kvm/hyp/pgtable.c
#	arch/arm64/kvm/mmu.c
diff --cc arch/arm64/kvm/mmu.c
index 235c439bcb99,d3b4feed460c..000000000000
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@@ -2090,50 -1756,25 +2090,69 @@@ bool kvm_set_spte_gfn(struct kvm *kvm, 
  bool kvm_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  {
  	u64 size = (range->end - range->start) << PAGE_SHIFT;
 -
 -	if (!kvm->arch.mmu.pgt)
++<<<<<<< HEAD
 +	pud_t *pud;
 +	pmd_t *pmd;
 +	pte_t *pte;
++=======
++>>>>>>> df6556adf27b (KVM: arm64: Correctly handle page aging notifiers for unaligned memslot)
 +
 +	if (!kvm->arch.pgd)
  		return false;
  
++<<<<<<< HEAD
 +	WARN_ON(size != PAGE_SIZE && size != PMD_SIZE && size != PUD_SIZE);
 +	if (!stage2_get_leaf_entry(kvm, range->start << PAGE_SHIFT, &pud, &pmd, &pte))
 +		return 0;
 +
 +	if (pud)
 +		return stage2_pudp_test_and_clear_young(pud);
 +	else if (pmd)
 +		return stage2_pmdp_test_and_clear_young(pmd);
 +	else
 +		return stage2_ptep_test_and_clear_young(pte);
++=======
+ 	return kvm_pgtable_stage2_test_clear_young(kvm->arch.mmu.pgt,
+ 						   range->start << PAGE_SHIFT,
+ 						   size, true);
++>>>>>>> df6556adf27b (KVM: arm64: Correctly handle page aging notifiers for unaligned memslot)
  }
  
  bool kvm_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
  {
  	u64 size = (range->end - range->start) << PAGE_SHIFT;
++<<<<<<< HEAD
 +	pud_t *pud;
 +	pmd_t *pmd;
 +	pte_t *pte;
 +
 +	if (!kvm->arch.pgd)
 +		return false;
 +
 +	WARN_ON(size != PAGE_SIZE && size != PMD_SIZE && size != PUD_SIZE);
 +	if (!stage2_get_leaf_entry(kvm, range->start << PAGE_SHIFT, &pud, &pmd, &pte))
 +		return 0;
 +
 +	if (pud)
 +		return kvm_s2pud_young(*pud);
 +	else if (pmd)
 +		return pmd_young(*pmd);
 +	else
 +		return pte_young(*pte);
 +}
 +
 +void kvm_mmu_free_memory_caches(struct kvm_vcpu *vcpu)
 +{
 +	kvm_mmu_free_memory_cache(&vcpu->arch.mmu_page_cache);
++=======
+ 
+ 	if (!kvm->arch.mmu.pgt)
+ 		return false;
+ 
+ 	return kvm_pgtable_stage2_test_clear_young(kvm->arch.mmu.pgt,
+ 						   range->start << PAGE_SHIFT,
+ 						   size, false);
++>>>>>>> df6556adf27b (KVM: arm64: Correctly handle page aging notifiers for unaligned memslot)
  }
  
  phys_addr_t kvm_mmu_get_httbr(void)
* Unmerged path arch/arm64/include/asm/kvm_pgtable.h
* Unmerged path arch/arm64/kvm/hyp/pgtable.c
* Unmerged path arch/arm64/include/asm/kvm_pgtable.h
* Unmerged path arch/arm64/kvm/hyp/pgtable.c
* Unmerged path arch/arm64/kvm/mmu.c
