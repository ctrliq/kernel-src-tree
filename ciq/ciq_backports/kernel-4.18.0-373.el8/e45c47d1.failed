block: add bio_start_io_acct_time() to control start_time

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-373.el8
commit-author Mike Snitzer <snitzer@redhat.com>
commit e45c47d1f94e0cc7b6b079fdb4bcce2995e2adc4
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-373.el8/e45c47d1.failed

bio_start_io_acct_time() interface is like bio_start_io_acct() that
allows start_time to be passed in. This gives drivers the ability to
defer starting accounting until after IO is issued (but possibily not
entirely due to bio splitting).

	Reviewed-by: Christoph Hellwig <hch@lst.de>
	Signed-off-by: Mike Snitzer <snitzer@redhat.com>
Link: https://lore.kernel.org/r/20220128155841.39644-2-snitzer@redhat.com
	Signed-off-by: Jens Axboe <axboe@kernel.dk>
(cherry picked from commit e45c47d1f94e0cc7b6b079fdb4bcce2995e2adc4)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	block/blk-core.c
#	include/linux/blkdev.h
diff --cc block/blk-core.c
index 86bee830d43e,d93e3bb9a769..000000000000
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@@ -1132,210 -970,141 +1132,244 @@@ blk_qc_t submit_bio(struct bio *bio
  EXPORT_SYMBOL(submit_bio);
  
  /**
 - * bio_poll - poll for BIO completions
 - * @bio: bio to poll for
 - * @iob: batches of IO
 - * @flags: BLK_POLL_* flags that control the behavior
 + * blk_cloned_rq_check_limits - Helper function to check a cloned request
 + *                              for the new queue limits
 + * @q:  the queue
 + * @rq: the request being checked
   *
 - * Poll for completions on queue associated with the bio. Returns number of
 - * completed entries found.
 + * Description:
 + *    @rq may have been made based on weaker limitations of upper-level queues
 + *    in request stacking drivers, and it may violate the limitation of @q.
 + *    Since the block layer and the underlying device driver trust @rq
 + *    after it is inserted to @q, it should be checked against @q before
 + *    the insertion using this generic function.
   *
 - * Note: the caller must either be the context that submitted @bio, or
 - * be in a RCU critical section to prevent freeing of @bio.
 + *    Request stacking drivers like request-based dm may change the queue
 + *    limits when retrying requests on other queues. Those requests need
 + *    to be checked against the new queue limits again during dispatch.
   */
 -int bio_poll(struct bio *bio, struct io_comp_batch *iob, unsigned int flags)
 +static blk_status_t blk_cloned_rq_check_limits(struct request_queue *q,
 +				      struct request *rq)
  {
 -	struct request_queue *q = bdev_get_queue(bio->bi_bdev);
 -	blk_qc_t cookie = READ_ONCE(bio->bi_cookie);
 -	int ret;
 +	unsigned int max_sectors = blk_queue_get_max_sectors(q, req_op(rq));
  
 -	if (cookie == BLK_QC_T_NONE ||
 -	    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
 -		return 0;
 +	if (blk_rq_sectors(rq) > max_sectors) {
 +		/*
 +		 * SCSI device does not have a good way to return if
 +		 * Write Same/Zero is actually supported. If a device rejects
 +		 * a non-read/write command (discard, write same,etc.) the
 +		 * low-level device driver will set the relevant queue limit to
 +		 * 0 to prevent blk-lib from issuing more of the offending
 +		 * operations. Commands queued prior to the queue limit being
 +		 * reset need to be completed with BLK_STS_NOTSUPP to avoid I/O
 +		 * errors being propagated to upper layers.
 +		 */
 +		if (max_sectors == 0)
 +			return BLK_STS_NOTSUPP;
  
 -	if (current->plug)
 -		blk_flush_plug(current->plug, false);
 +		printk(KERN_ERR "%s: over max size limit. (%u > %u)\n",
 +			__func__, blk_rq_sectors(rq), max_sectors);
 +		return BLK_STS_IOERR;
 +	}
  
 -	if (blk_queue_enter(q, BLK_MQ_REQ_NOWAIT))
 -		return 0;
 -	if (WARN_ON_ONCE(!queue_is_mq(q)))
 -		ret = 0;	/* not yet implemented, should not happen */
 -	else
 -		ret = blk_mq_poll(q, cookie, iob, flags);
 -	blk_queue_exit(q);
 -	return ret;
 +	/*
 +	 * queue's settings related to segment counting like q->bounce_pfn
 +	 * may differ from that of other stacking queues.
 +	 * Recalculate it to check the request correctly on this queue's
 +	 * limitation.
 +	 */
 +	blk_recalc_rq_segments(rq);
 +	if (rq->nr_phys_segments > queue_max_segments(q)) {
 +		printk(KERN_ERR "%s: over max segments limit. (%hu > %hu)\n",
 +			__func__, rq->nr_phys_segments, queue_max_segments(q));
 +		return BLK_STS_IOERR;
 +	}
 +
 +	return BLK_STS_OK;
  }
 -EXPORT_SYMBOL_GPL(bio_poll);
  
 -/*
 - * Helper to implement file_operations.iopoll.  Requires the bio to be stored
 - * in iocb->private, and cleared before freeing the bio.
 +/**
 + * blk_insert_cloned_request - Helper for stacking drivers to submit a request
 + * @q:  the queue to submit the request
 + * @rq: the request being queued
   */
 -int iocb_bio_iopoll(struct kiocb *kiocb, struct io_comp_batch *iob,
 -		    unsigned int flags)
 +blk_status_t blk_insert_cloned_request(struct request_queue *q, struct request *rq)
  {
 +	blk_status_t ret;
 +
 +	ret = blk_cloned_rq_check_limits(q, rq);
 +	if (ret != BLK_STS_OK)
 +		return ret;
 +
 +	if (rq->rq_disk &&
 +	    should_fail_request(&rq->rq_disk->part0, blk_rq_bytes(rq)))
 +		return BLK_STS_IOERR;
 +
 +	if (blk_queue_io_stat(q))
 +		blk_account_io_start(rq);
 +
 +	/*
 +	 * Since we have a scheduler attached on the top device,
 +	 * bypass a potential scheduler on the bottom device for
 +	 * insert.
 +	 */
 +	return blk_mq_request_issue_directly(rq, true);
 +}
 +EXPORT_SYMBOL_GPL(blk_insert_cloned_request);
 +
 +/**
 + * blk_rq_err_bytes - determine number of bytes till the next failure boundary
 + * @rq: request to examine
 + *
 + * Description:
 + *     A request could be merge of IOs which require different failure
 + *     handling.  This function determines the number of bytes which
 + *     can be failed from the beginning of the request without
 + *     crossing into area which need to be retried further.
 + *
 + * Return:
 + *     The number of bytes to fail.
 + */
 +unsigned int blk_rq_err_bytes(const struct request *rq)
 +{
 +	unsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;
 +	unsigned int bytes = 0;
  	struct bio *bio;
 -	int ret = 0;
 +
 +	if (!(rq->rq_flags & RQF_MIXED_MERGE))
 +		return blk_rq_bytes(rq);
  
  	/*
 -	 * Note: the bio cache only uses SLAB_TYPESAFE_BY_RCU, so bio can
 -	 * point to a freshly allocated bio at this point.  If that happens
 -	 * we have a few cases to consider:
 -	 *
 -	 *  1) the bio is beeing initialized and bi_bdev is NULL.  We can just
 -	 *     simply nothing in this case
 -	 *  2) the bio points to a not poll enabled device.  bio_poll will catch
 -	 *     this and return 0
 -	 *  3) the bio points to a poll capable device, including but not
 -	 *     limited to the one that the original bio pointed to.  In this
 -	 *     case we will call into the actual poll method and poll for I/O,
 -	 *     even if we don't need to, but it won't cause harm either.
 -	 *
 -	 * For cases 2) and 3) above the RCU grace period ensures that bi_bdev
 -	 * is still allocated. Because partitions hold a reference to the whole
 -	 * device bdev and thus disk, the disk is also still valid.  Grabbing
 -	 * a reference to the queue in bio_poll() ensures the hctxs and requests
 -	 * are still valid as well.
 +	 * Currently the only 'mixing' which can happen is between
 +	 * different fastfail types.  We can safely fail portions
 +	 * which have all the failfast bits that the first one has -
 +	 * the ones which are at least as eager to fail as the first
 +	 * one.
  	 */
 -	rcu_read_lock();
 -	bio = READ_ONCE(kiocb->private);
 -	if (bio && bio->bi_bdev)
 -		ret = bio_poll(bio, iob, flags);
 -	rcu_read_unlock();
 +	for (bio = rq->bio; bio; bio = bio->bi_next) {
 +		if ((bio->bi_opf & ff) != ff)
 +			break;
 +		bytes += bio->bi_iter.bi_size;
 +	}
  
 -	return ret;
 +	/* this could lead to infinite loop */
 +	BUG_ON(blk_rq_bytes(rq) && !bytes);
 +	return bytes;
  }
 -EXPORT_SYMBOL_GPL(iocb_bio_iopoll);
 +EXPORT_SYMBOL_GPL(blk_rq_err_bytes);
  
 -void update_io_ticks(struct block_device *part, unsigned long now, bool end)
 +static void blk_account_io_completion(struct request *req, unsigned int bytes)
  {
 -	unsigned long stamp;
 -again:
 -	stamp = READ_ONCE(part->bd_stamp);
 -	if (unlikely(time_after(now, stamp))) {
 -		if (likely(cmpxchg(&part->bd_stamp, stamp, now) == stamp))
 -			__part_stat_add(part, io_ticks, end ? now - stamp : 1);
 +	if (req->part && blk_do_io_stat(req)) {
 +		const int sgrp = op_stat_group(req_op(req));
 +		struct hd_struct *part;
 +
 +		part_stat_lock();
 +		part = req->part;
 +		part_stat_add(part, sectors[sgrp], bytes >> 9);
 +		part_stat_unlock();
  	}
 -	if (part->bd_partno) {
 -		part = bdev_whole(part);
 -		goto again;
 +}
 +
++<<<<<<< HEAD
 +void blk_account_io_done(struct request *req, u64 now)
 +{
 +	/*
 +	 * Account IO completion.  flush_rq isn't accounted as a
 +	 * normal IO on queueing nor completion.  Accounting the
 +	 * containing request is enough.
 +	 */
 +	if (req->part && blk_do_io_stat(req) &&
 +	    !(req->rq_flags & RQF_FLUSH_SEQ)) {
 +		const int sgrp = op_stat_group(req_op(req));
 +		struct hd_struct *part;
 +
 +		part_stat_lock();
 +		part = req->part;
 +
 +		update_io_ticks(part, jiffies, true);
 +		part_stat_inc(part, ios[sgrp]);
 +		part_stat_add(part, nsecs[sgrp], now - req->start_time_ns);
 +		part_stat_unlock();
 +
 +		hd_struct_put(part);
  	}
  }
  
 +void blk_account_io_start(struct request *rq)
 +{
 +	if (!blk_do_io_stat(rq))
 +		return;
 +
 +	rq->part = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq));
 +
 +	part_stat_lock();
 +	update_io_ticks(rq->part, jiffies, false);
 +	part_stat_unlock();
 +}
 +
 +static unsigned long __part_start_io_acct(struct hd_struct *part,
 +					  unsigned int sectors, unsigned int op)
++=======
+ static unsigned long __part_start_io_acct(struct block_device *part,
+ 					  unsigned int sectors, unsigned int op,
+ 					  unsigned long start_time)
++>>>>>>> e45c47d1f94e (block: add bio_start_io_acct_time() to control start_time)
  {
  	const int sgrp = op_stat_group(op);
- 	unsigned long now = READ_ONCE(jiffies);
  
  	part_stat_lock();
- 	update_io_ticks(part, now, false);
+ 	update_io_ticks(part, start_time, false);
  	part_stat_inc(part, ios[sgrp]);
  	part_stat_add(part, sectors[sgrp], sectors);
  	part_stat_local_inc(part, in_flight[op_is_write(op)]);
  	part_stat_unlock();
  
- 	return now;
+ 	return start_time;
  }
  
++<<<<<<< HEAD
 +unsigned long part_start_io_acct(struct gendisk *disk, struct hd_struct **part,
 +				 struct bio *bio)
 +{
 +	*part = disk_map_sector_rcu(disk, bio->bi_iter.bi_sector);
 +
 +	return __part_start_io_acct(*part, bio_sectors(bio), bio_op(bio));
++=======
+ /**
+  * bio_start_io_acct_time - start I/O accounting for bio based drivers
+  * @bio:	bio to start account for
+  * @start_time:	start time that should be passed back to bio_end_io_acct().
+  */
+ void bio_start_io_acct_time(struct bio *bio, unsigned long start_time)
+ {
+ 	__part_start_io_acct(bio->bi_bdev, bio_sectors(bio),
+ 			     bio_op(bio), start_time);
+ }
+ EXPORT_SYMBOL_GPL(bio_start_io_acct_time);
+ 
+ /**
+  * bio_start_io_acct - start I/O accounting for bio based drivers
+  * @bio:	bio to start account for
+  *
+  * Returns the start time that should be passed back to bio_end_io_acct().
+  */
+ unsigned long bio_start_io_acct(struct bio *bio)
+ {
+ 	return __part_start_io_acct(bio->bi_bdev, bio_sectors(bio),
+ 				    bio_op(bio), jiffies);
++>>>>>>> e45c47d1f94e (block: add bio_start_io_acct_time() to control start_time)
  }
 -EXPORT_SYMBOL_GPL(bio_start_io_acct);
 +EXPORT_SYMBOL_GPL(part_start_io_acct);
  
  unsigned long disk_start_io_acct(struct gendisk *disk, unsigned int sectors,
  				 unsigned int op)
  {
++<<<<<<< HEAD
 +	return __part_start_io_acct(&disk->part0, sectors, op);
++=======
+ 	return __part_start_io_acct(disk->part0, sectors, op, jiffies);
++>>>>>>> e45c47d1f94e (block: add bio_start_io_acct_time() to control start_time)
  }
  EXPORT_SYMBOL(disk_start_io_acct);
  
diff --cc include/linux/blkdev.h
index 234dc812e7f8,f35aea98bc35..000000000000
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@@ -1941,21 -1258,10 +1941,28 @@@ unsigned long disk_start_io_acct(struc
  void disk_end_io_acct(struct gendisk *disk, unsigned int op,
  		unsigned long start_time);
  
++<<<<<<< HEAD
 +unsigned long part_start_io_acct(struct gendisk *disk, struct hd_struct **part,
 +				 struct bio *bio);
 +void part_end_io_acct(struct hd_struct *part, struct bio *bio,
 +		      unsigned long start_time);
 +
 +/**
 + * bio_start_io_acct - start I/O accounting for bio based drivers
 + * @bio:	bio to start account for
 + *
 + * Returns the start time that should be passed back to bio_end_io_acct().
 + */
 +static inline unsigned long bio_start_io_acct(struct bio *bio)
 +{
 +	return disk_start_io_acct(bio->bi_disk, bio_sectors(bio), bio_op(bio));
 +}
++=======
+ void bio_start_io_acct_time(struct bio *bio, unsigned long start_time);
+ unsigned long bio_start_io_acct(struct bio *bio);
+ void bio_end_io_acct_remapped(struct bio *bio, unsigned long start_time,
+ 		struct block_device *orig_bdev);
++>>>>>>> e45c47d1f94e (block: add bio_start_io_acct_time() to control start_time)
  
  /**
   * bio_end_io_acct - end I/O accounting for bio based drivers
* Unmerged path block/blk-core.c
* Unmerged path include/linux/blkdev.h
