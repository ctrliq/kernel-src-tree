iomap: pass a flags argument to iomap_dio_rw

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-373.el8
commit-author Christoph Hellwig <hch@lst.de>
commit 2f63296578cad1ae681152d5b2122a4595195f16
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-373.el8/2f632965.failed

Pass a set of flags to iomap_dio_rw instead of the boolean
wait_for_completion argument.  The IOMAP_DIO_FORCE_WAIT flag
replaces the wait_for_completion, but only needs to be passed
when the iocb isn't synchronous to start with to simplify the
callers.

	Signed-off-by: Christoph Hellwig <hch@lst.de>
	Reviewed-by: Dave Chinner <dchinner@redhat.com>
	Reviewed-by: Brian Foster <bfoster@redhat.com>
[djwong: rework xfs_file.c so that we can push iomap changes separately]
	Reviewed-by: Darrick J. Wong <djwong@kernel.org>
	Signed-off-by: Darrick J. Wong <djwong@kernel.org>
(cherry picked from commit 2f63296578cad1ae681152d5b2122a4595195f16)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	fs/btrfs/file.c
#	fs/ext4/file.c
#	fs/zonefs/super.c
diff --cc fs/btrfs/file.c
index 06e87d1444a8,ddfd2e2adedf..000000000000
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@@ -1804,19 -1899,77 +1804,77 @@@ static ssize_t __btrfs_direct_write(str
  {
  	struct file *file = iocb->ki_filp;
  	struct inode *inode = file_inode(file);
 -	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
 -	loff_t pos;
 -	ssize_t written = 0;
 +	loff_t pos = iocb->ki_pos;
 +	ssize_t written;
  	ssize_t written_buffered;
  	loff_t endbyte;
 -	ssize_t err;
 -	unsigned int ilock_flags = 0;
 -	struct iomap_dio *dio = NULL;
 +	int err;
  
 -	if (iocb->ki_flags & IOCB_NOWAIT)
 -		ilock_flags |= BTRFS_ILOCK_TRY;
 +	written = generic_file_direct_write(iocb, from);
  
 -	/* If the write DIO is within EOF, use a shared lock */
 -	if (iocb->ki_pos + iov_iter_count(from) <= i_size_read(inode))
 -		ilock_flags |= BTRFS_ILOCK_SHARED;
 +	if (written < 0 || !iov_iter_count(from))
 +		return written;
  
++<<<<<<< HEAD
 +	pos += written;
 +	written_buffered = __btrfs_buffered_write(file, from, pos);
++=======
+ relock:
+ 	err = btrfs_inode_lock(inode, ilock_flags);
+ 	if (err < 0)
+ 		return err;
+ 
+ 	err = generic_write_checks(iocb, from);
+ 	if (err <= 0) {
+ 		btrfs_inode_unlock(inode, ilock_flags);
+ 		return err;
+ 	}
+ 
+ 	err = btrfs_write_check(iocb, from, err);
+ 	if (err < 0) {
+ 		btrfs_inode_unlock(inode, ilock_flags);
+ 		goto out;
+ 	}
+ 
+ 	pos = iocb->ki_pos;
+ 	/*
+ 	 * Re-check since file size may have changed just before taking the
+ 	 * lock or pos may have changed because of O_APPEND in generic_write_check()
+ 	 */
+ 	if ((ilock_flags & BTRFS_ILOCK_SHARED) &&
+ 	    pos + iov_iter_count(from) > i_size_read(inode)) {
+ 		btrfs_inode_unlock(inode, ilock_flags);
+ 		ilock_flags &= ~BTRFS_ILOCK_SHARED;
+ 		goto relock;
+ 	}
+ 
+ 	if (check_direct_IO(fs_info, from, pos)) {
+ 		btrfs_inode_unlock(inode, ilock_flags);
+ 		goto buffered;
+ 	}
+ 
+ 	dio = __iomap_dio_rw(iocb, from, &btrfs_dio_iomap_ops, &btrfs_dio_ops,
+ 			     0);
+ 
+ 	btrfs_inode_unlock(inode, ilock_flags);
+ 
+ 	if (IS_ERR_OR_NULL(dio)) {
+ 		err = PTR_ERR_OR_ZERO(dio);
+ 		if (err < 0 && err != -ENOTBLK)
+ 			goto out;
+ 	} else {
+ 		written = iomap_dio_complete(dio);
+ 	}
+ 
+ 	if (written < 0 || !iov_iter_count(from)) {
+ 		err = written;
+ 		goto out;
+ 	}
+ 
+ buffered:
+ 	pos = iocb->ki_pos;
+ 	written_buffered = btrfs_buffered_write(iocb, from);
++>>>>>>> 2f63296578ca (iomap: pass a flags argument to iomap_dio_rw)
  	if (written_buffered < 0) {
  		err = written_buffered;
  		goto out;
@@@ -3341,11 -3593,60 +3399,62 @@@ static int btrfs_file_open(struct inod
  	return generic_file_open(inode, filp);
  }
  
++<<<<<<< HEAD
++=======
+ static int check_direct_read(struct btrfs_fs_info *fs_info,
+ 			     const struct iov_iter *iter, loff_t offset)
+ {
+ 	int ret;
+ 	int i, seg;
+ 
+ 	ret = check_direct_IO(fs_info, iter, offset);
+ 	if (ret < 0)
+ 		return ret;
+ 
+ 	if (!iter_is_iovec(iter))
+ 		return 0;
+ 
+ 	for (seg = 0; seg < iter->nr_segs; seg++)
+ 		for (i = seg + 1; i < iter->nr_segs; i++)
+ 			if (iter->iov[seg].iov_base == iter->iov[i].iov_base)
+ 				return -EINVAL;
+ 	return 0;
+ }
+ 
+ static ssize_t btrfs_direct_read(struct kiocb *iocb, struct iov_iter *to)
+ {
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 	ssize_t ret;
+ 
+ 	if (check_direct_read(btrfs_sb(inode->i_sb), to, iocb->ki_pos))
+ 		return 0;
+ 
+ 	btrfs_inode_lock(inode, BTRFS_ILOCK_SHARED);
+ 	ret = iomap_dio_rw(iocb, to, &btrfs_dio_iomap_ops, &btrfs_dio_ops, 0);
+ 	btrfs_inode_unlock(inode, BTRFS_ILOCK_SHARED);
+ 	return ret;
+ }
+ 
+ static ssize_t btrfs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
+ {
+ 	ssize_t ret = 0;
+ 
+ 	if (iocb->ki_flags & IOCB_DIRECT) {
+ 		ret = btrfs_direct_read(iocb, to);
+ 		if (ret < 0 || !iov_iter_count(to) ||
+ 		    iocb->ki_pos >= i_size_read(file_inode(iocb->ki_filp)))
+ 			return ret;
+ 	}
+ 
+ 	return generic_file_buffered_read(iocb, to, ret);
+ }
+ 
++>>>>>>> 2f63296578ca (iomap: pass a flags argument to iomap_dio_rw)
  const struct file_operations btrfs_file_operations = {
  	.llseek		= btrfs_file_llseek,
 -	.read_iter      = btrfs_file_read_iter,
 +	.read_iter      = generic_file_read_iter,
  	.splice_read	= generic_file_splice_read,
  	.write_iter	= btrfs_file_write_iter,
 -	.splice_write	= iter_file_splice_write,
  	.mmap		= btrfs_file_mmap,
  	.open		= btrfs_file_open,
  	.release	= btrfs_release_file,
diff --cc fs/ext4/file.c
index b7bc9e10d9dd,194f5d00fa32..000000000000
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@@ -33,6 -34,52 +33,55 @@@
  #include "ext4_jbd2.h"
  #include "xattr.h"
  #include "acl.h"
++<<<<<<< HEAD
++=======
+ #include "truncate.h"
+ 
+ static bool ext4_dio_supported(struct inode *inode)
+ {
+ 	if (IS_ENABLED(CONFIG_FS_ENCRYPTION) && IS_ENCRYPTED(inode))
+ 		return false;
+ 	if (fsverity_active(inode))
+ 		return false;
+ 	if (ext4_should_journal_data(inode))
+ 		return false;
+ 	if (ext4_has_inline_data(inode))
+ 		return false;
+ 	return true;
+ }
+ 
+ static ssize_t ext4_dio_read_iter(struct kiocb *iocb, struct iov_iter *to)
+ {
+ 	ssize_t ret;
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 
+ 	if (iocb->ki_flags & IOCB_NOWAIT) {
+ 		if (!inode_trylock_shared(inode))
+ 			return -EAGAIN;
+ 	} else {
+ 		inode_lock_shared(inode);
+ 	}
+ 
+ 	if (!ext4_dio_supported(inode)) {
+ 		inode_unlock_shared(inode);
+ 		/*
+ 		 * Fallback to buffered I/O if the operation being performed on
+ 		 * the inode is not supported by direct I/O. The IOCB_DIRECT
+ 		 * flag needs to be cleared here in order to ensure that the
+ 		 * direct I/O path within generic_file_read_iter() is not
+ 		 * taken.
+ 		 */
+ 		iocb->ki_flags &= ~IOCB_DIRECT;
+ 		return generic_file_read_iter(iocb, to);
+ 	}
+ 
+ 	ret = iomap_dio_rw(iocb, to, &ext4_iomap_ops, NULL, 0);
+ 	inode_unlock_shared(inode);
+ 
+ 	file_accessed(iocb->ki_filp);
+ 	return ret;
+ }
++>>>>>>> 2f63296578ca (iomap: pass a flags argument to iomap_dio_rw)
  
  #ifdef CONFIG_FS_DAX
  static ssize_t ext4_dax_read_iter(struct kiocb *iocb, struct iov_iter *to)
@@@ -184,6 -235,362 +233,364 @@@ static ssize_t ext4_write_checks(struc
  	return iov_iter_count(from);
  }
  
++<<<<<<< HEAD
++=======
+ static ssize_t ext4_write_checks(struct kiocb *iocb, struct iov_iter *from)
+ {
+ 	ssize_t ret, count;
+ 
+ 	count = ext4_generic_write_checks(iocb, from);
+ 	if (count <= 0)
+ 		return count;
+ 
+ 	ret = file_modified(iocb->ki_filp);
+ 	if (ret)
+ 		return ret;
+ 	return count;
+ }
+ 
+ static ssize_t ext4_buffered_write_iter(struct kiocb *iocb,
+ 					struct iov_iter *from)
+ {
+ 	ssize_t ret;
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 
+ 	if (iocb->ki_flags & IOCB_NOWAIT)
+ 		return -EOPNOTSUPP;
+ 
+ 	ext4_fc_start_update(inode);
+ 	inode_lock(inode);
+ 	ret = ext4_write_checks(iocb, from);
+ 	if (ret <= 0)
+ 		goto out;
+ 
+ 	current->backing_dev_info = inode_to_bdi(inode);
+ 	ret = generic_perform_write(iocb->ki_filp, from, iocb->ki_pos);
+ 	current->backing_dev_info = NULL;
+ 
+ out:
+ 	inode_unlock(inode);
+ 	ext4_fc_stop_update(inode);
+ 	if (likely(ret > 0)) {
+ 		iocb->ki_pos += ret;
+ 		ret = generic_write_sync(iocb, ret);
+ 	}
+ 
+ 	return ret;
+ }
+ 
+ static ssize_t ext4_handle_inode_extension(struct inode *inode, loff_t offset,
+ 					   ssize_t written, size_t count)
+ {
+ 	handle_t *handle;
+ 	bool truncate = false;
+ 	u8 blkbits = inode->i_blkbits;
+ 	ext4_lblk_t written_blk, end_blk;
+ 	int ret;
+ 
+ 	/*
+ 	 * Note that EXT4_I(inode)->i_disksize can get extended up to
+ 	 * inode->i_size while the I/O was running due to writeback of delalloc
+ 	 * blocks. But, the code in ext4_iomap_alloc() is careful to use
+ 	 * zeroed/unwritten extents if this is possible; thus we won't leave
+ 	 * uninitialized blocks in a file even if we didn't succeed in writing
+ 	 * as much as we intended.
+ 	 */
+ 	WARN_ON_ONCE(i_size_read(inode) < EXT4_I(inode)->i_disksize);
+ 	if (offset + count <= EXT4_I(inode)->i_disksize) {
+ 		/*
+ 		 * We need to ensure that the inode is removed from the orphan
+ 		 * list if it has been added prematurely, due to writeback of
+ 		 * delalloc blocks.
+ 		 */
+ 		if (!list_empty(&EXT4_I(inode)->i_orphan) && inode->i_nlink) {
+ 			handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
+ 
+ 			if (IS_ERR(handle)) {
+ 				ext4_orphan_del(NULL, inode);
+ 				return PTR_ERR(handle);
+ 			}
+ 
+ 			ext4_orphan_del(handle, inode);
+ 			ext4_journal_stop(handle);
+ 		}
+ 
+ 		return written;
+ 	}
+ 
+ 	if (written < 0)
+ 		goto truncate;
+ 
+ 	handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
+ 	if (IS_ERR(handle)) {
+ 		written = PTR_ERR(handle);
+ 		goto truncate;
+ 	}
+ 
+ 	if (ext4_update_inode_size(inode, offset + written)) {
+ 		ret = ext4_mark_inode_dirty(handle, inode);
+ 		if (unlikely(ret)) {
+ 			written = ret;
+ 			ext4_journal_stop(handle);
+ 			goto truncate;
+ 		}
+ 	}
+ 
+ 	/*
+ 	 * We may need to truncate allocated but not written blocks beyond EOF.
+ 	 */
+ 	written_blk = ALIGN(offset + written, 1 << blkbits);
+ 	end_blk = ALIGN(offset + count, 1 << blkbits);
+ 	if (written_blk < end_blk && ext4_can_truncate(inode))
+ 		truncate = true;
+ 
+ 	/*
+ 	 * Remove the inode from the orphan list if it has been extended and
+ 	 * everything went OK.
+ 	 */
+ 	if (!truncate && inode->i_nlink)
+ 		ext4_orphan_del(handle, inode);
+ 	ext4_journal_stop(handle);
+ 
+ 	if (truncate) {
+ truncate:
+ 		ext4_truncate_failed_write(inode);
+ 		/*
+ 		 * If the truncate operation failed early, then the inode may
+ 		 * still be on the orphan list. In that case, we need to try
+ 		 * remove the inode from the in-memory linked list.
+ 		 */
+ 		if (inode->i_nlink)
+ 			ext4_orphan_del(NULL, inode);
+ 	}
+ 
+ 	return written;
+ }
+ 
+ static int ext4_dio_write_end_io(struct kiocb *iocb, ssize_t size,
+ 				 int error, unsigned int flags)
+ {
+ 	loff_t offset = iocb->ki_pos;
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 
+ 	if (error)
+ 		return error;
+ 
+ 	if (size && flags & IOMAP_DIO_UNWRITTEN)
+ 		return ext4_convert_unwritten_extents(NULL, inode,
+ 						      offset, size);
+ 
+ 	return 0;
+ }
+ 
+ static const struct iomap_dio_ops ext4_dio_write_ops = {
+ 	.end_io = ext4_dio_write_end_io,
+ };
+ 
+ /*
+  * The intention here is to start with shared lock acquired then see if any
+  * condition requires an exclusive inode lock. If yes, then we restart the
+  * whole operation by releasing the shared lock and acquiring exclusive lock.
+  *
+  * - For unaligned_io we never take shared lock as it may cause data corruption
+  *   when two unaligned IO tries to modify the same block e.g. while zeroing.
+  *
+  * - For extending writes case we don't take the shared lock, since it requires
+  *   updating inode i_disksize and/or orphan handling with exclusive lock.
+  *
+  * - shared locking will only be true mostly with overwrites. Otherwise we will
+  *   switch to exclusive i_rwsem lock.
+  */
+ static ssize_t ext4_dio_write_checks(struct kiocb *iocb, struct iov_iter *from,
+ 				     bool *ilock_shared, bool *extend)
+ {
+ 	struct file *file = iocb->ki_filp;
+ 	struct inode *inode = file_inode(file);
+ 	loff_t offset;
+ 	size_t count;
+ 	ssize_t ret;
+ 
+ restart:
+ 	ret = ext4_generic_write_checks(iocb, from);
+ 	if (ret <= 0)
+ 		goto out;
+ 
+ 	offset = iocb->ki_pos;
+ 	count = ret;
+ 	if (ext4_extending_io(inode, offset, count))
+ 		*extend = true;
+ 	/*
+ 	 * Determine whether the IO operation will overwrite allocated
+ 	 * and initialized blocks.
+ 	 * We need exclusive i_rwsem for changing security info
+ 	 * in file_modified().
+ 	 */
+ 	if (*ilock_shared && (!IS_NOSEC(inode) || *extend ||
+ 	     !ext4_overwrite_io(inode, offset, count))) {
+ 		if (iocb->ki_flags & IOCB_NOWAIT) {
+ 			ret = -EAGAIN;
+ 			goto out;
+ 		}
+ 		inode_unlock_shared(inode);
+ 		*ilock_shared = false;
+ 		inode_lock(inode);
+ 		goto restart;
+ 	}
+ 
+ 	ret = file_modified(file);
+ 	if (ret < 0)
+ 		goto out;
+ 
+ 	return count;
+ out:
+ 	if (*ilock_shared)
+ 		inode_unlock_shared(inode);
+ 	else
+ 		inode_unlock(inode);
+ 	return ret;
+ }
+ 
+ static ssize_t ext4_dio_write_iter(struct kiocb *iocb, struct iov_iter *from)
+ {
+ 	ssize_t ret;
+ 	handle_t *handle;
+ 	struct inode *inode = file_inode(iocb->ki_filp);
+ 	loff_t offset = iocb->ki_pos;
+ 	size_t count = iov_iter_count(from);
+ 	const struct iomap_ops *iomap_ops = &ext4_iomap_ops;
+ 	bool extend = false, unaligned_io = false;
+ 	bool ilock_shared = true;
+ 
+ 	/*
+ 	 * We initially start with shared inode lock unless it is
+ 	 * unaligned IO which needs exclusive lock anyways.
+ 	 */
+ 	if (ext4_unaligned_io(inode, from, offset)) {
+ 		unaligned_io = true;
+ 		ilock_shared = false;
+ 	}
+ 	/*
+ 	 * Quick check here without any i_rwsem lock to see if it is extending
+ 	 * IO. A more reliable check is done in ext4_dio_write_checks() with
+ 	 * proper locking in place.
+ 	 */
+ 	if (offset + count > i_size_read(inode))
+ 		ilock_shared = false;
+ 
+ 	if (iocb->ki_flags & IOCB_NOWAIT) {
+ 		if (ilock_shared) {
+ 			if (!inode_trylock_shared(inode))
+ 				return -EAGAIN;
+ 		} else {
+ 			if (!inode_trylock(inode))
+ 				return -EAGAIN;
+ 		}
+ 	} else {
+ 		if (ilock_shared)
+ 			inode_lock_shared(inode);
+ 		else
+ 			inode_lock(inode);
+ 	}
+ 
+ 	/* Fallback to buffered I/O if the inode does not support direct I/O. */
+ 	if (!ext4_dio_supported(inode)) {
+ 		if (ilock_shared)
+ 			inode_unlock_shared(inode);
+ 		else
+ 			inode_unlock(inode);
+ 		return ext4_buffered_write_iter(iocb, from);
+ 	}
+ 
+ 	ret = ext4_dio_write_checks(iocb, from, &ilock_shared, &extend);
+ 	if (ret <= 0)
+ 		return ret;
+ 
+ 	/* if we're going to block and IOCB_NOWAIT is set, return -EAGAIN */
+ 	if ((iocb->ki_flags & IOCB_NOWAIT) && (unaligned_io || extend)) {
+ 		ret = -EAGAIN;
+ 		goto out;
+ 	}
+ 
+ 	offset = iocb->ki_pos;
+ 	count = ret;
+ 
+ 	/*
+ 	 * Unaligned direct IO must be serialized among each other as zeroing
+ 	 * of partial blocks of two competing unaligned IOs can result in data
+ 	 * corruption.
+ 	 *
+ 	 * So we make sure we don't allow any unaligned IO in flight.
+ 	 * For IOs where we need not wait (like unaligned non-AIO DIO),
+ 	 * below inode_dio_wait() may anyway become a no-op, since we start
+ 	 * with exclusive lock.
+ 	 */
+ 	if (unaligned_io)
+ 		inode_dio_wait(inode);
+ 
+ 	if (extend) {
+ 		handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
+ 		if (IS_ERR(handle)) {
+ 			ret = PTR_ERR(handle);
+ 			goto out;
+ 		}
+ 
+ 		ext4_fc_start_update(inode);
+ 		ret = ext4_orphan_add(handle, inode);
+ 		ext4_fc_stop_update(inode);
+ 		if (ret) {
+ 			ext4_journal_stop(handle);
+ 			goto out;
+ 		}
+ 
+ 		ext4_journal_stop(handle);
+ 	}
+ 
+ 	if (ilock_shared)
+ 		iomap_ops = &ext4_iomap_overwrite_ops;
+ 	ret = iomap_dio_rw(iocb, from, iomap_ops, &ext4_dio_write_ops,
+ 			   (unaligned_io || extend) ? IOMAP_DIO_FORCE_WAIT : 0);
+ 	if (ret == -ENOTBLK)
+ 		ret = 0;
+ 
+ 	if (extend)
+ 		ret = ext4_handle_inode_extension(inode, offset, ret, count);
+ 
+ out:
+ 	if (ilock_shared)
+ 		inode_unlock_shared(inode);
+ 	else
+ 		inode_unlock(inode);
+ 
+ 	if (ret >= 0 && iov_iter_count(from)) {
+ 		ssize_t err;
+ 		loff_t endbyte;
+ 
+ 		offset = iocb->ki_pos;
+ 		err = ext4_buffered_write_iter(iocb, from);
+ 		if (err < 0)
+ 			return err;
+ 
+ 		/*
+ 		 * We need to ensure that the pages within the page cache for
+ 		 * the range covered by this I/O are written to disk and
+ 		 * invalidated. This is in attempt to preserve the expected
+ 		 * direct I/O semantics in the case we fallback to buffered I/O
+ 		 * to complete off the I/O request.
+ 		 */
+ 		ret += err;
+ 		endbyte = offset + err - 1;
+ 		err = filemap_write_and_wait_range(iocb->ki_filp->f_mapping,
+ 						   offset, endbyte);
+ 		if (!err)
+ 			invalidate_mapping_pages(iocb->ki_filp->f_mapping,
+ 						 offset >> PAGE_SHIFT,
+ 						 endbyte >> PAGE_SHIFT);
+ 	}
+ 
+ 	return ret;
+ }
+ 
++>>>>>>> 2f63296578ca (iomap: pass a flags argument to iomap_dio_rw)
  #ifdef CONFIG_FS_DAX
  static ssize_t
  ext4_dax_write_iter(struct kiocb *iocb, struct iov_iter *from)
* Unmerged path fs/zonefs/super.c
* Unmerged path fs/btrfs/file.c
* Unmerged path fs/ext4/file.c
diff --git a/fs/gfs2/file.c b/fs/gfs2/file.c
index 778651957bb8..af1582b0b944 100644
--- a/fs/gfs2/file.c
+++ b/fs/gfs2/file.c
@@ -779,9 +779,7 @@ static ssize_t gfs2_file_direct_read(struct kiocb *iocb, struct iov_iter *to,
 	if (ret)
 		goto out_uninit;
 
-	ret = iomap_dio_rw(iocb, to, &gfs2_iomap_ops, NULL,
-			   is_sync_kiocb(iocb));
-
+	ret = iomap_dio_rw(iocb, to, &gfs2_iomap_ops, NULL, 0);
 	gfs2_glock_dq(gh);
 out_uninit:
 	gfs2_holder_uninit(gh);
@@ -815,8 +813,7 @@ static ssize_t gfs2_file_direct_write(struct kiocb *iocb, struct iov_iter *from,
 	if (offset + len > i_size_read(&ip->i_inode))
 		goto out;
 
-	ret = iomap_dio_rw(iocb, from, &gfs2_iomap_ops, NULL,
-			   is_sync_kiocb(iocb));
+	ret = iomap_dio_rw(iocb, from, &gfs2_iomap_ops, NULL, 0);
 	if (ret == -ENOTBLK)
 		ret = 0;
 out:
diff --git a/fs/iomap/direct-io.c b/fs/iomap/direct-io.c
index 01b8b1de0719..c82dc5290814 100644
--- a/fs/iomap/direct-io.c
+++ b/fs/iomap/direct-io.c
@@ -421,13 +421,15 @@ iomap_dio_actor(struct inode *inode, loff_t pos, loff_t length,
 struct iomap_dio *
 __iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		bool wait_for_completion)
+		unsigned int dio_flags)
 {
 	struct address_space *mapping = iocb->ki_filp->f_mapping;
 	struct inode *inode = file_inode(iocb->ki_filp);
 	size_t count = iov_iter_count(iter);
 	loff_t pos = iocb->ki_pos;
 	loff_t end = iocb->ki_pos + count - 1, ret = 0;
+	bool wait_for_completion =
+		is_sync_kiocb(iocb) || (dio_flags & IOMAP_DIO_FORCE_WAIT);
 	unsigned int iomap_flags = IOMAP_DIRECT;
 	struct blk_plug plug;
 	struct iomap_dio *dio;
@@ -435,9 +437,6 @@ __iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 	if (!count)
 		return NULL;
 
-	if (WARN_ON(is_sync_kiocb(iocb) && !wait_for_completion))
-		return ERR_PTR(-EIO);
-
 	dio = kmalloc(sizeof(*dio), GFP_KERNEL);
 	if (!dio)
 		return ERR_PTR(-ENOMEM);
@@ -599,11 +598,11 @@ EXPORT_SYMBOL_GPL(__iomap_dio_rw);
 ssize_t
 iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		bool wait_for_completion)
+		unsigned int dio_flags)
 {
 	struct iomap_dio *dio;
 
-	dio = __iomap_dio_rw(iocb, iter, ops, dops, wait_for_completion);
+	dio = __iomap_dio_rw(iocb, iter, ops, dops, dio_flags);
 	if (IS_ERR_OR_NULL(dio))
 		return PTR_ERR_OR_ZERO(dio);
 	return iomap_dio_complete(dio);
diff --git a/fs/xfs/xfs_file.c b/fs/xfs/xfs_file.c
index 9d260771a573..8cea619649ff 100644
--- a/fs/xfs/xfs_file.c
+++ b/fs/xfs/xfs_file.c
@@ -245,8 +245,7 @@ xfs_file_dio_aio_read(
 	} else {
 		xfs_ilock(ip, XFS_IOLOCK_SHARED);
 	}
-	ret = iomap_dio_rw(iocb, to, &xfs_read_iomap_ops, NULL,
-			is_sync_kiocb(iocb));
+	ret = iomap_dio_rw(iocb, to, &xfs_read_iomap_ops, NULL, 0);
 	xfs_iunlock(ip, XFS_IOLOCK_SHARED);
 
 	return ret;
@@ -604,7 +603,7 @@ xfs_file_dio_aio_write(
 	 */
 	ret = iomap_dio_rw(iocb, from, &xfs_direct_write_iomap_ops,
 			   &xfs_dio_write_ops,
-			   is_sync_kiocb(iocb) || unaligned_io);
+			   unaligned_io ? IOMAP_DIO_FORCE_WAIT : 0);
 out:
 	xfs_iunlock(ip, iolock);
 
* Unmerged path fs/zonefs/super.c
diff --git a/include/linux/iomap.h b/include/linux/iomap.h
index 93c9172844a5..34de5cd33d82 100644
--- a/include/linux/iomap.h
+++ b/include/linux/iomap.h
@@ -261,12 +261,18 @@ struct iomap_dio_ops {
 		      unsigned flags);
 };
 
+/*
+ * Wait for the I/O to complete in iomap_dio_rw even if the kiocb is not
+ * synchronous.
+ */
+#define IOMAP_DIO_FORCE_WAIT	(1 << 0)
+
 ssize_t iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		bool wait_for_completion);
+		unsigned int dio_flags);
 struct iomap_dio *__iomap_dio_rw(struct kiocb *iocb, struct iov_iter *iter,
 		const struct iomap_ops *ops, const struct iomap_dio_ops *dops,
-		bool wait_for_completion);
+		unsigned int dio_flags);
 ssize_t iomap_dio_complete(struct iomap_dio *dio);
 int iomap_dio_iopoll(struct kiocb *kiocb, bool spin);
 
