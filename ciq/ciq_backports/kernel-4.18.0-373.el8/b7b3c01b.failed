mm/memremap_pages: support multiple ranges per invocation

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-373.el8
commit-author Dan Williams <dan.j.williams@intel.com>
commit b7b3c01b191596d27a6980d1a42504f5b607f802
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-373.el8/b7b3c01b.failed

In support of device-dax growing the ability to front physically
dis-contiguous ranges of memory, update devm_memremap_pages() to track
multiple ranges with a single reference counter and devm instance.

Convert all [devm_]memremap_pages() users to specify the number of ranges
they are mapping in their 'struct dev_pagemap' instance.

	Signed-off-by: Dan Williams <dan.j.williams@intel.com>
	Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
	Cc: Paul Mackerras <paulus@ozlabs.org>
	Cc: Michael Ellerman <mpe@ellerman.id.au>
	Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
	Cc: Vishal Verma <vishal.l.verma@intel.com>
	Cc: Vivek Goyal <vgoyal@redhat.com>
	Cc: Dave Jiang <dave.jiang@intel.com>
	Cc: Ben Skeggs <bskeggs@redhat.com>
	Cc: David Airlie <airlied@linux.ie>
	Cc: Daniel Vetter <daniel@ffwll.ch>
	Cc: Ira Weiny <ira.weiny@intel.com>
	Cc: Bjorn Helgaas <bhelgaas@google.com>
	Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
	Cc: Juergen Gross <jgross@suse.com>
	Cc: Stefano Stabellini <sstabellini@kernel.org>
	Cc: "Jérôme Glisse" <jglisse@redhat.co
	Cc: Andy Lutomirski <luto@kernel.org>
	Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
	Cc: Ard Biesheuvel <ardb@kernel.org>
	Cc: Borislav Petkov <bp@alien8.de>
	Cc: Brice Goglin <Brice.Goglin@inria.fr>
	Cc: Catalin Marinas <catalin.marinas@arm.com>
	Cc: Dave Hansen <dave.hansen@linux.intel.com>
	Cc: David Hildenbrand <david@redhat.com>
	Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
	Cc: "H. Peter Anvin" <hpa@zytor.com>
	Cc: Hulk Robot <hulkci@huawei.com>
	Cc: Ingo Molnar <mingo@redhat.com>
	Cc: Jason Gunthorpe <jgg@mellanox.com>
	Cc: Jason Yan <yanaijie@huawei.com>
	Cc: Jeff Moyer <jmoyer@redhat.com>
	Cc: "Jérôme Glisse" <jglisse@redhat.com>
	Cc: Jia He <justin.he@arm.com>
	Cc: Joao Martins <joao.m.martins@oracle.com>
	Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
	Cc: kernel test robot <lkp@intel.com>
	Cc: Mike Rapoport <rppt@linux.ibm.com>
	Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
	Cc: Peter Zijlstra <peterz@infradead.org>
	Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
	Cc: Randy Dunlap <rdunlap@infradead.org>
	Cc: Thomas Gleixner <tglx@linutronix.de>
	Cc: Tom Lendacky <thomas.lendacky@amd.com>
	Cc: Wei Yang <richard.weiyang@linux.alibaba.com>
	Cc: Will Deacon <will@kernel.org>
Link: https://lkml.kernel.org/r/159643103789.4062302.18426128170217903785.stgit@dwillia2-desk3.amr.corp.intel.com
Link: https://lkml.kernel.org/r/160106116293.30709.13350662794915396198.stgit@dwillia2-desk3.amr.corp.intel.com
	Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
(cherry picked from commit b7b3c01b191596d27a6980d1a42504f5b607f802)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	arch/powerpc/kvm/book3s_hv_uvmem.c
#	drivers/dax/device.c
#	drivers/nvdimm/pfn_devs.c
#	drivers/nvdimm/pmem.c
#	drivers/pci/p2pdma.c
#	drivers/xen/unpopulated-alloc.c
#	include/linux/memremap.h
#	lib/test_hmm.c
#	mm/memremap.c
diff --cc arch/powerpc/kvm/book3s_hv_uvmem.c
index ccd2bc5c5973,84e5a2dc8be5..000000000000
--- a/arch/powerpc/kvm/book3s_hv_uvmem.c
+++ b/arch/powerpc/kvm/book3s_hv_uvmem.c
@@@ -1170,7 -1170,9 +1170,13 @@@ int kvmppc_uvmem_init(void
  	}
  
  	kvmppc_uvmem_pgmap.type = MEMORY_DEVICE_PRIVATE;
++<<<<<<< HEAD
 +	kvmppc_uvmem_pgmap.res = *res;
++=======
+ 	kvmppc_uvmem_pgmap.range.start = res->start;
+ 	kvmppc_uvmem_pgmap.range.end = res->end;
+ 	kvmppc_uvmem_pgmap.nr_range = 1;
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	kvmppc_uvmem_pgmap.ops = &kvmppc_uvmem_ops;
  	/* just one global instance: */
  	kvmppc_uvmem_pgmap.owner = &kvmppc_uvmem_pgmap;
diff --cc drivers/dax/device.c
index eaa651ddeccd,5f808617672a..000000000000
--- a/drivers/dax/device.c
+++ b/drivers/dax/device.c
@@@ -408,8 -411,16 +408,21 @@@ int dev_dax_probe(struct device *dev
  		return -EBUSY;
  	}
  
++<<<<<<< HEAD
 +	dev_dax->pgmap.type = MEMORY_DEVICE_DEVDAX;
 +	addr = devm_memremap_pages(dev, &dev_dax->pgmap);
++=======
+ 	pgmap = dev_dax->pgmap;
+ 	if (!pgmap) {
+ 		pgmap = devm_kzalloc(dev, sizeof(*pgmap), GFP_KERNEL);
+ 		if (!pgmap)
+ 			return -ENOMEM;
+ 		pgmap->range = *range;
+ 		pgmap->nr_range = 1;
+ 	}
+ 	pgmap->type = MEMORY_DEVICE_GENERIC;
+ 	addr = devm_memremap_pages(dev, pgmap);
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	if (IS_ERR(addr))
  		return PTR_ERR(addr);
  
diff --cc drivers/nvdimm/pfn_devs.c
index e6f29d1f118d,b499df630d4d..000000000000
--- a/drivers/nvdimm/pfn_devs.c
+++ b/drivers/nvdimm/pfn_devs.c
@@@ -664,10 -689,11 +664,18 @@@ static int __nvdimm_setup_pfn(struct nd
  		.end_pfn = PHYS_PFN(end),
  	};
  
++<<<<<<< HEAD
 +	memcpy(res, &nsio->res, sizeof(*res));
 +	res->start += start_pad;
 +	res->end -= end_trunc;
 +
++=======
+ 	*range = (struct range) {
+ 		.start = nsio->res.start + start_pad,
+ 		.end = nsio->res.end - end_trunc,
+ 	};
+ 	pgmap->nr_range = 1;
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	if (nd_pfn->mode == PFN_MODE_RAM) {
  		if (offset < reserve)
  			return -EINVAL;
diff --cc drivers/nvdimm/pmem.c
index 974bf22c207b,875076b0ea6c..000000000000
--- a/drivers/nvdimm/pmem.c
+++ b/drivers/nvdimm/pmem.c
@@@ -443,12 -434,14 +443,18 @@@ static int pmem_attach_disk(struct devi
  		pfn_sb = nd_pfn->pfn_sb;
  		pmem->data_offset = le64_to_cpu(pfn_sb->dataoff);
  		pmem->pfn_pad = resource_size(res) -
 -			range_len(&pmem->pgmap.range);
 +			resource_size(&pmem->pgmap.res);
  		pmem->pfn_flags |= PFN_MAP;
 -		bb_range = pmem->pgmap.range;
 -		bb_range.start += pmem->data_offset;
 +		memcpy(&bb_res, &pmem->pgmap.res, sizeof(bb_res));
 +		bb_res.start += pmem->data_offset;
  	} else if (pmem_should_map_pages(dev)) {
++<<<<<<< HEAD
 +		memcpy(&pmem->pgmap.res, &nsio->res, sizeof(pmem->pgmap.res));
++=======
+ 		pmem->pgmap.range.start = res->start;
+ 		pmem->pgmap.range.end = res->end;
+ 		pmem->pgmap.nr_range = 1;
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  		pmem->pgmap.type = MEMORY_DEVICE_FS_DAX;
  		pmem->pgmap.ops = &fsdax_pagemap_ops;
  		addr = devm_memremap_pages(dev, &pmem->pgmap);
diff --cc drivers/pci/p2pdma.c
index 9736ba71ccbf,9d53c16b7329..000000000000
--- a/drivers/pci/p2pdma.c
+++ b/drivers/pci/p2pdma.c
@@@ -157,14 -180,18 +157,22 @@@ int pci_p2pdma_add_resource(struct pci_
  			return error;
  	}
  
 -	p2p_pgmap = devm_kzalloc(&pdev->dev, sizeof(*p2p_pgmap), GFP_KERNEL);
 -	if (!p2p_pgmap)
 +	pgmap = devm_kzalloc(&pdev->dev, sizeof(*pgmap), GFP_KERNEL);
 +	if (!pgmap)
  		return -ENOMEM;
++<<<<<<< HEAD
 +	pgmap->res.start = pci_resource_start(pdev, bar) + offset;
 +	pgmap->res.end = pgmap->res.start + size - 1;
 +	pgmap->res.flags = pci_resource_flags(pdev, bar);
++=======
+ 
+ 	pgmap = &p2p_pgmap->pgmap;
+ 	pgmap->range.start = pci_resource_start(pdev, bar) + offset;
+ 	pgmap->range.end = pgmap->range.start + size - 1;
+ 	pgmap->nr_range = 1;
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	pgmap->type = MEMORY_DEVICE_PCI_P2PDMA;
 -
 -	p2p_pgmap->provider = pdev;
 -	p2p_pgmap->bus_offset = pci_bus_address(pdev, bar) -
 +	pgmap->pci_p2pdma_bus_offset = pci_bus_address(pdev, bar) -
  		pci_resource_start(pdev, bar);
  
  	addr = devm_memremap_pages(&pdev->dev, pgmap);
diff --cc include/linux/memremap.h
index ecd38ce53a36,79c49e7f5c30..000000000000
--- a/include/linux/memremap.h
+++ b/include/linux/memremap.h
@@@ -111,7 -94,6 +111,10 @@@ struct dev_pagemap_ops 
  /**
   * struct dev_pagemap - metadata for ZONE_DEVICE mappings
   * @altmap: pre-allocated/reserved memory for vmemmap allocations
++<<<<<<< HEAD
 + * @res: physical address range covered by @ref
++=======
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
   * @ref: reference count that pins the devm_memremap_pages() mapping
   * @internal_ref: internal reference if @ref is not provided by the caller
   * @done: completion for @internal_ref
@@@ -121,23 -103,24 +124,40 @@@
   * @owner: an opaque pointer identifying the entity that manages this
   *	instance.  Used by various helpers to make sure that no
   *	foreign ZONE_DEVICE memory is accessed.
+  * @nr_range: number of ranges to be mapped
+  * @range: range to be mapped when nr_range == 1
+  * @ranges: array of ranges to be mapped when nr_range > 1
   */
  struct dev_pagemap {
 +	RH_KABI_DEPRECATE(dev_page_fault_t, page_fault)
 +	RH_KABI_DEPRECATE(dev_page_free_t, page_free)
  	struct vmem_altmap altmap;
++<<<<<<< HEAD
 +	RH_KABI_DEPRECATE(bool, altmap_valid)
 +	struct resource res;
++=======
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	struct percpu_ref *ref;
 -	struct percpu_ref internal_ref;
 -	struct completion done;
 +	RH_KABI_DEPRECATE(struct device *, dev)
 +	RH_KABI_DEPRECATE(void *, data)
  	enum memory_type type;
++<<<<<<< HEAD
 +	u64 pci_p2pdma_bus_offset;
 +	RH_KABI_EXTEND(const struct dev_pagemap_ops *ops)
 +	RH_KABI_EXTEND(unsigned int flags)
 +	RH_KABI_EXTEND(struct percpu_ref internal_ref)
 +	RH_KABI_EXTEND(struct completion done)
 +	RH_KABI_EXTEND(void *owner)
++=======
+ 	unsigned int flags;
+ 	const struct dev_pagemap_ops *ops;
+ 	void *owner;
+ 	int nr_range;
+ 	union {
+ 		struct range range;
+ 		struct range ranges[0];
+ 	};
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  };
  
  static inline struct vmem_altmap *pgmap_altmap(struct dev_pagemap *pgmap)
diff --cc mm/memremap.c
index 6bc3fa8cccb8,532ec3d36ab4..000000000000
--- a/mm/memremap.c
+++ b/mm/memremap.c
@@@ -77,17 -77,21 +77,30 @@@ static void pgmap_array_delete(struct r
  	synchronize_rcu();
  }
  
- static unsigned long pfn_first(struct dev_pagemap *pgmap)
+ static unsigned long pfn_first(struct dev_pagemap *pgmap, int range_id)
  {
++<<<<<<< HEAD
 +	return PHYS_PFN(pgmap->res.start) +
 +		vmem_altmap_offset(pgmap_altmap(pgmap));
++=======
+ 	struct range *range = &pgmap->ranges[range_id];
+ 	unsigned long pfn = PHYS_PFN(range->start);
+ 
+ 	if (range_id)
+ 		return pfn;
+ 	return pfn + vmem_altmap_offset(pgmap_altmap(pgmap));
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  }
  
- static unsigned long pfn_end(struct dev_pagemap *pgmap)
+ static unsigned long pfn_end(struct dev_pagemap *pgmap, int range_id)
  {
++<<<<<<< HEAD
 +	const struct resource *res = &pgmap->res;
++=======
+ 	const struct range *range = &pgmap->ranges[range_id];
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  
 -	return (range->start + range_len(range)) >> PAGE_SHIFT;
 +	return (res->start + resource_size(res)) >> PAGE_SHIFT;
  }
  
  static unsigned long pfn_next(unsigned long pfn)
@@@ -124,20 -128,14 +137,18 @@@ static void dev_pagemap_cleanup(struct 
  		pgmap->ref = NULL;
  }
  
- void memunmap_pages(struct dev_pagemap *pgmap)
+ static void pageunmap_range(struct dev_pagemap *pgmap, int range_id)
  {
++<<<<<<< HEAD
 +	struct resource *res = &pgmap->res;
++=======
+ 	struct range *range = &pgmap->ranges[range_id];
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	struct page *first_page;
- 	unsigned long pfn;
  	int nid;
  
- 	dev_pagemap_kill(pgmap);
- 	for_each_device_pfn(pfn, pgmap)
- 		put_page(pfn_to_page(pfn));
- 	dev_pagemap_cleanup(pgmap);
- 
  	/* make sure to access a memmap that was actually initialized */
- 	first_page = pfn_to_page(pfn_first(pgmap));
+ 	first_page = pfn_to_page(pfn_first(pgmap, range_id));
  
  	/* pages are dead and unused, undo the arch mapping */
  	nid = page_to_nid(first_page);
@@@ -155,8 -153,24 +166,29 @@@
  	}
  	mem_hotplug_done();
  
++<<<<<<< HEAD
 +	untrack_pfn(NULL, PHYS_PFN(res->start), resource_size(res));
 +	pgmap_array_delete(res);
++=======
+ 	untrack_pfn(NULL, PHYS_PFN(range->start), range_len(range));
+ 	pgmap_array_delete(range);
+ }
+ 
+ void memunmap_pages(struct dev_pagemap *pgmap)
+ {
+ 	unsigned long pfn;
+ 	int i;
+ 
+ 	dev_pagemap_kill(pgmap);
+ 	for (i = 0; i < pgmap->nr_range; i++)
+ 		for_each_device_pfn(pfn, pgmap, i)
+ 			put_page(pfn_to_page(pfn));
+ 	dev_pagemap_cleanup(pgmap);
+ 
+ 	for (i = 0; i < pgmap->nr_range; i++)
+ 		pageunmap_range(pgmap, i);
+ 
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	WARN_ONCE(pgmap->altmap.alloc, "failed to free all reserved pages\n");
  	devmap_managed_enable_put();
  }
@@@ -182,12 -304,7 +322,12 @@@ err_pfn_remap
   */
  void *memremap_pages(struct dev_pagemap *pgmap, int nid)
  {
++<<<<<<< HEAD
 +	struct resource *res = &pgmap->res;
 +	struct dev_pagemap *conflict_pgmap;
++=======
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	struct mhp_params params = {
- 		/*
- 		 * We do not want any optional features only our own memmap
- 		 */
  		.altmap = pgmap_altmap(pgmap),
  		.pgprot = PAGE_KERNEL,
  	};
@@@ -251,106 -372,27 +395,122 @@@
  			return ERR_PTR(error);
  	}
  
++<<<<<<< HEAD
 +	conflict_pgmap = get_dev_pagemap(PHYS_PFN(res->start), NULL);
 +	if (conflict_pgmap) {
 +		WARN(1, "Conflicting mapping in same section\n");
 +		put_dev_pagemap(conflict_pgmap);
 +		error = -ENOMEM;
 +		goto err_array;
 +	}
 +
 +	conflict_pgmap = get_dev_pagemap(PHYS_PFN(res->end), NULL);
 +	if (conflict_pgmap) {
 +		WARN(1, "Conflicting mapping in same section\n");
 +		put_dev_pagemap(conflict_pgmap);
 +		error = -ENOMEM;
 +		goto err_array;
 +	}
 +
 +	is_ram = region_intersects(res->start, resource_size(res),
 +		IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);
 +
 +	if (is_ram != REGION_DISJOINT) {
 +		WARN_ONCE(1, "%s attempted on %s region %pr\n", __func__,
 +				is_ram == REGION_MIXED ? "mixed" : "ram", res);
 +		error = -ENXIO;
 +		goto err_array;
 +	}
 +
 +	error = xa_err(xa_store_range(&pgmap_array, PHYS_PFN(res->start),
 +				PHYS_PFN(res->end), pgmap, GFP_KERNEL));
 +	if (error)
 +		goto err_array;
 +
 +	if (nid < 0)
 +		nid = numa_mem_id();
 +
 +	error = track_pfn_remap(NULL, &params.pgprot, PHYS_PFN(res->start),
 +				0, resource_size(res));
 +	if (error)
 +		goto err_pfn_remap;
 +
 +	mem_hotplug_begin();
 +
++=======
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  	/*
- 	 * For device private memory we call add_pages() as we only need to
- 	 * allocate and initialize struct page for the device memory. More-
- 	 * over the device memory is un-accessible thus we do not want to
- 	 * create a linear mapping for the memory like arch_add_memory()
- 	 * would do.
- 	 *
- 	 * For all other device memory types, which are accessible by
- 	 * the CPU, we do want the linear mapping and thus use
- 	 * arch_add_memory().
+ 	 * Clear the pgmap nr_range as it will be incremented for each
+ 	 * successfully processed range. This communicates how many
+ 	 * regions to unwind in the abort case.
  	 */
++<<<<<<< HEAD
 +	if (pgmap->type == MEMORY_DEVICE_PRIVATE) {
 +		error = add_pages(nid, PHYS_PFN(res->start),
 +				PHYS_PFN(resource_size(res)), &params);
 +	} else {
 +		error = kasan_add_zero_shadow(__va(res->start), resource_size(res));
 +		if (error) {
 +			mem_hotplug_done();
 +			goto err_kasan;
 +		}
 +
 +		error = arch_add_memory(nid, res->start, resource_size(res),
 +					&params);
 +	}
 +
 +	if (!error) {
 +		struct zone *zone;
 +
 +		zone = &NODE_DATA(nid)->node_zones[ZONE_DEVICE];
 +		move_pfn_range_to_zone(zone, PHYS_PFN(res->start),
 +				PHYS_PFN(resource_size(res)), params.altmap,
 +				MIGRATE_MOVABLE);
 +	}
 +
 +	mem_hotplug_done();
 +	if (error)
 +		goto err_add_memory;
 +
 +	/*
 +	 * Initialization of the pages has been deferred until now in order
 +	 * to allow us to do the work while not holding the hotplug lock.
 +	 */
 +	memmap_init_zone_device(&NODE_DATA(nid)->node_zones[ZONE_DEVICE],
 +				PHYS_PFN(res->start),
 +				PHYS_PFN(resource_size(res)), pgmap);
 +	percpu_ref_get_many(pgmap->ref, pfn_end(pgmap) - pfn_first(pgmap));
 +	return __va(res->start);
 +
 + err_add_memory:
 +	kasan_remove_zero_shadow(__va(res->start), resource_size(res));
 + err_kasan:
 +	untrack_pfn(NULL, PHYS_PFN(res->start), resource_size(res));
 + err_pfn_remap:
 +	pgmap_array_delete(res);
 + err_array:
 +	dev_pagemap_kill(pgmap);
 +	dev_pagemap_cleanup(pgmap);
 +	devmap_managed_enable_put();
 +	return ERR_PTR(error);
++=======
+ 	pgmap->nr_range = 0;
+ 	error = 0;
+ 	for (i = 0; i < nr_range; i++) {
+ 		error = pagemap_range(pgmap, &params, i, nid);
+ 		if (error)
+ 			break;
+ 		pgmap->nr_range++;
+ 	}
+ 
+ 	if (i < nr_range) {
+ 		memunmap_pages(pgmap);
+ 		pgmap->nr_range = nr_range;
+ 		return ERR_PTR(error);
+ 	}
+ 
+ 	return __va(pgmap->ranges[0].start);
++>>>>>>> b7b3c01b1915 (mm/memremap_pages: support multiple ranges per invocation)
  }
  EXPORT_SYMBOL_GPL(memremap_pages);
  
* Unmerged path drivers/xen/unpopulated-alloc.c
* Unmerged path lib/test_hmm.c
* Unmerged path arch/powerpc/kvm/book3s_hv_uvmem.c
* Unmerged path drivers/dax/device.c
* Unmerged path drivers/nvdimm/pfn_devs.c
* Unmerged path drivers/nvdimm/pmem.c
* Unmerged path drivers/pci/p2pdma.c
* Unmerged path drivers/xen/unpopulated-alloc.c
* Unmerged path include/linux/memremap.h
* Unmerged path lib/test_hmm.c
* Unmerged path mm/memremap.c
