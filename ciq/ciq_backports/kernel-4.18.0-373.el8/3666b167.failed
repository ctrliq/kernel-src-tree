selftests/bpf: Adding delay in socketmap_listen to reduce flakyness

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-373.el8
commit-author Yucong Sun <fallentree@fb.com>
commit 3666b167ea68997b73dd5b78678a1c3f0d6730bb
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-373.el8/3666b167.failed

This patch adds a 1ms delay to reduce flakyness of the test.

	Signed-off-by: Yucong Sun <fallentree@fb.com>
	Signed-off-by: Andrii Nakryiko <andrii@kernel.org>
Link: https://lore.kernel.org/bpf/20210819163609.2583758-1-fallentree@fb.com
(cherry picked from commit 3666b167ea68997b73dd5b78678a1c3f0d6730bb)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	tools/testing/selftests/bpf/prog_tests/sockmap_listen.c
diff --cc tools/testing/selftests/bpf/prog_tests/sockmap_listen.c
index 01ab11259809,6a5df28f9a3d..000000000000
--- a/tools/testing/selftests/bpf/prog_tests/sockmap_listen.c
+++ b/tools/testing/selftests/bpf/prog_tests/sockmap_listen.c
@@@ -1563,6 -1559,101 +1563,104 @@@ static void test_redir(struct test_sock
  	}
  }
  
++<<<<<<< HEAD
++=======
+ static void unix_redir_to_connected(int sotype, int sock_mapfd,
+ 			       int verd_mapfd, enum redir_mode mode)
+ {
+ 	const char *log_prefix = redir_mode_str(mode);
+ 	int c0, c1, p0, p1;
+ 	unsigned int pass;
+ 	int retries = 100;
+ 	int err, n;
+ 	int sfd[2];
+ 	u32 key;
+ 	char b;
+ 
+ 	zero_verdict_count(verd_mapfd);
+ 
+ 	if (socketpair(AF_UNIX, sotype | SOCK_NONBLOCK, 0, sfd))
+ 		return;
+ 	c0 = sfd[0], p0 = sfd[1];
+ 
+ 	if (socketpair(AF_UNIX, sotype | SOCK_NONBLOCK, 0, sfd))
+ 		goto close0;
+ 	c1 = sfd[0], p1 = sfd[1];
+ 
+ 	err = add_to_sockmap(sock_mapfd, p0, p1);
+ 	if (err)
+ 		goto close;
+ 
+ 	n = write(c1, "a", 1);
+ 	if (n < 0)
+ 		FAIL_ERRNO("%s: write", log_prefix);
+ 	if (n == 0)
+ 		FAIL("%s: incomplete write", log_prefix);
+ 	if (n < 1)
+ 		goto close;
+ 
+ 	key = SK_PASS;
+ 	err = xbpf_map_lookup_elem(verd_mapfd, &key, &pass);
+ 	if (err)
+ 		goto close;
+ 	if (pass != 1)
+ 		FAIL("%s: want pass count 1, have %d", log_prefix, pass);
+ 
+ again:
+ 	n = read(mode == REDIR_INGRESS ? p0 : c0, &b, 1);
+ 	if (n < 0) {
+ 		if (errno == EAGAIN && retries--) {
+ 			usleep(1000);
+ 			goto again;
+ 		}
+ 		FAIL_ERRNO("%s: read", log_prefix);
+ 	}
+ 	if (n == 0)
+ 		FAIL("%s: incomplete read", log_prefix);
+ 
+ close:
+ 	xclose(c1);
+ 	xclose(p1);
+ close0:
+ 	xclose(c0);
+ 	xclose(p0);
+ }
+ 
+ static void unix_skb_redir_to_connected(struct test_sockmap_listen *skel,
+ 					struct bpf_map *inner_map, int sotype)
+ {
+ 	int verdict = bpf_program__fd(skel->progs.prog_skb_verdict);
+ 	int verdict_map = bpf_map__fd(skel->maps.verdict_map);
+ 	int sock_map = bpf_map__fd(inner_map);
+ 	int err;
+ 
+ 	err = xbpf_prog_attach(verdict, sock_map, BPF_SK_SKB_VERDICT, 0);
+ 	if (err)
+ 		return;
+ 
+ 	skel->bss->test_ingress = false;
+ 	unix_redir_to_connected(sotype, sock_map, verdict_map, REDIR_EGRESS);
+ 	skel->bss->test_ingress = true;
+ 	unix_redir_to_connected(sotype, sock_map, verdict_map, REDIR_INGRESS);
+ 
+ 	xbpf_prog_detach2(verdict, sock_map, BPF_SK_SKB_VERDICT);
+ }
+ 
+ static void test_unix_redir(struct test_sockmap_listen *skel, struct bpf_map *map,
+ 			    int sotype)
+ {
+ 	const char *family_name, *map_name;
+ 	char s[MAX_TEST_NAME];
+ 
+ 	family_name = family_str(AF_UNIX);
+ 	map_name = map_type_str(map);
+ 	snprintf(s, sizeof(s), "%s %s %s", map_name, family_name, __func__);
+ 	if (!test__start_subtest(s))
+ 		return;
+ 	unix_skb_redir_to_connected(skel, map, sotype);
+ }
+ 
++>>>>>>> 3666b167ea68 (selftests/bpf: Adding delay in socketmap_listen to reduce flakyness)
  static void test_reuseport(struct test_sockmap_listen *skel,
  			   struct bpf_map *map, int family, int sotype)
  {
@@@ -1743,6 -1829,189 +1843,192 @@@ static void test_udp_redir(struct test_
  	udp_skb_redir_to_connected(skel, map, family);
  }
  
++<<<<<<< HEAD
++=======
+ static void inet_unix_redir_to_connected(int family, int type, int sock_mapfd,
+ 					int verd_mapfd, enum redir_mode mode)
+ {
+ 	const char *log_prefix = redir_mode_str(mode);
+ 	int c0, c1, p0, p1;
+ 	unsigned int pass;
+ 	int retries = 100;
+ 	int err, n;
+ 	int sfd[2];
+ 	u32 key;
+ 	char b;
+ 
+ 	zero_verdict_count(verd_mapfd);
+ 
+ 	if (socketpair(AF_UNIX, SOCK_DGRAM | SOCK_NONBLOCK, 0, sfd))
+ 		return;
+ 	c0 = sfd[0], p0 = sfd[1];
+ 
+ 	err = inet_socketpair(family, SOCK_DGRAM, &p1, &c1);
+ 	if (err)
+ 		goto close;
+ 
+ 	err = add_to_sockmap(sock_mapfd, p0, p1);
+ 	if (err)
+ 		goto close_cli1;
+ 
+ 	n = write(c1, "a", 1);
+ 	if (n < 0)
+ 		FAIL_ERRNO("%s: write", log_prefix);
+ 	if (n == 0)
+ 		FAIL("%s: incomplete write", log_prefix);
+ 	if (n < 1)
+ 		goto close_cli1;
+ 
+ 	key = SK_PASS;
+ 	err = xbpf_map_lookup_elem(verd_mapfd, &key, &pass);
+ 	if (err)
+ 		goto close_cli1;
+ 	if (pass != 1)
+ 		FAIL("%s: want pass count 1, have %d", log_prefix, pass);
+ 
+ again:
+ 	n = read(mode == REDIR_INGRESS ? p0 : c0, &b, 1);
+ 	if (n < 0) {
+ 		if (errno == EAGAIN && retries--) {
+ 			usleep(1000);
+ 			goto again;
+ 		}
+ 		FAIL_ERRNO("%s: read", log_prefix);
+ 	}
+ 	if (n == 0)
+ 		FAIL("%s: incomplete read", log_prefix);
+ 
+ close_cli1:
+ 	xclose(c1);
+ 	xclose(p1);
+ close:
+ 	xclose(c0);
+ 	xclose(p0);
+ }
+ 
+ static void inet_unix_skb_redir_to_connected(struct test_sockmap_listen *skel,
+ 					    struct bpf_map *inner_map, int family)
+ {
+ 	int verdict = bpf_program__fd(skel->progs.prog_skb_verdict);
+ 	int verdict_map = bpf_map__fd(skel->maps.verdict_map);
+ 	int sock_map = bpf_map__fd(inner_map);
+ 	int err;
+ 
+ 	err = xbpf_prog_attach(verdict, sock_map, BPF_SK_SKB_VERDICT, 0);
+ 	if (err)
+ 		return;
+ 
+ 	skel->bss->test_ingress = false;
+ 	inet_unix_redir_to_connected(family, SOCK_DGRAM, sock_map, verdict_map,
+ 				    REDIR_EGRESS);
+ 	inet_unix_redir_to_connected(family, SOCK_STREAM, sock_map, verdict_map,
+ 				    REDIR_EGRESS);
+ 	skel->bss->test_ingress = true;
+ 	inet_unix_redir_to_connected(family, SOCK_DGRAM, sock_map, verdict_map,
+ 				    REDIR_INGRESS);
+ 	inet_unix_redir_to_connected(family, SOCK_STREAM, sock_map, verdict_map,
+ 				    REDIR_INGRESS);
+ 
+ 	xbpf_prog_detach2(verdict, sock_map, BPF_SK_SKB_VERDICT);
+ }
+ 
+ static void unix_inet_redir_to_connected(int family, int type, int sock_mapfd,
+ 					int verd_mapfd, enum redir_mode mode)
+ {
+ 	const char *log_prefix = redir_mode_str(mode);
+ 	int c0, c1, p0, p1;
+ 	unsigned int pass;
+ 	int err, n;
+ 	int sfd[2];
+ 	u32 key;
+ 	char b;
+ 
+ 	zero_verdict_count(verd_mapfd);
+ 
+ 	err = inet_socketpair(family, SOCK_DGRAM, &p0, &c0);
+ 	if (err)
+ 		return;
+ 
+ 	if (socketpair(AF_UNIX, SOCK_DGRAM | SOCK_NONBLOCK, 0, sfd))
+ 		goto close_cli0;
+ 	c1 = sfd[0], p1 = sfd[1];
+ 
+ 	err = add_to_sockmap(sock_mapfd, p0, p1);
+ 	if (err)
+ 		goto close;
+ 
+ 	n = write(c1, "a", 1);
+ 	if (n < 0)
+ 		FAIL_ERRNO("%s: write", log_prefix);
+ 	if (n == 0)
+ 		FAIL("%s: incomplete write", log_prefix);
+ 	if (n < 1)
+ 		goto close;
+ 
+ 	key = SK_PASS;
+ 	err = xbpf_map_lookup_elem(verd_mapfd, &key, &pass);
+ 	if (err)
+ 		goto close;
+ 	if (pass != 1)
+ 		FAIL("%s: want pass count 1, have %d", log_prefix, pass);
+ 
+ 	n = read(mode == REDIR_INGRESS ? p0 : c0, &b, 1);
+ 	if (n < 0)
+ 		FAIL_ERRNO("%s: read", log_prefix);
+ 	if (n == 0)
+ 		FAIL("%s: incomplete read", log_prefix);
+ 
+ close:
+ 	xclose(c1);
+ 	xclose(p1);
+ close_cli0:
+ 	xclose(c0);
+ 	xclose(p0);
+ 
+ }
+ 
+ static void unix_inet_skb_redir_to_connected(struct test_sockmap_listen *skel,
+ 					    struct bpf_map *inner_map, int family)
+ {
+ 	int verdict = bpf_program__fd(skel->progs.prog_skb_verdict);
+ 	int verdict_map = bpf_map__fd(skel->maps.verdict_map);
+ 	int sock_map = bpf_map__fd(inner_map);
+ 	int err;
+ 
+ 	err = xbpf_prog_attach(verdict, sock_map, BPF_SK_SKB_VERDICT, 0);
+ 	if (err)
+ 		return;
+ 
+ 	skel->bss->test_ingress = false;
+ 	unix_inet_redir_to_connected(family, SOCK_DGRAM, sock_map, verdict_map,
+ 				     REDIR_EGRESS);
+ 	unix_inet_redir_to_connected(family, SOCK_STREAM, sock_map, verdict_map,
+ 				     REDIR_EGRESS);
+ 	skel->bss->test_ingress = true;
+ 	unix_inet_redir_to_connected(family, SOCK_DGRAM, sock_map, verdict_map,
+ 				     REDIR_INGRESS);
+ 	unix_inet_redir_to_connected(family, SOCK_STREAM, sock_map, verdict_map,
+ 				     REDIR_INGRESS);
+ 
+ 	xbpf_prog_detach2(verdict, sock_map, BPF_SK_SKB_VERDICT);
+ }
+ 
+ static void test_udp_unix_redir(struct test_sockmap_listen *skel, struct bpf_map *map,
+ 				int family)
+ {
+ 	const char *family_name, *map_name;
+ 	char s[MAX_TEST_NAME];
+ 
+ 	family_name = family_str(family);
+ 	map_name = map_type_str(map);
+ 	snprintf(s, sizeof(s), "%s %s %s", map_name, family_name, __func__);
+ 	if (!test__start_subtest(s))
+ 		return;
+ 	inet_unix_skb_redir_to_connected(skel, map, family);
+ 	unix_inet_skb_redir_to_connected(skel, map, family);
+ }
+ 
++>>>>>>> 3666b167ea68 (selftests/bpf: Adding delay in socketmap_listen to reduce flakyness)
  static void run_tests(struct test_sockmap_listen *skel, struct bpf_map *map,
  		      int family)
  {
* Unmerged path tools/testing/selftests/bpf/prog_tests/sockmap_listen.c
