RDMA/mlx5: Initialize the ODP xarray when creating an ODP MR

jira LE-1907
Rebuild_History Non-Buildable kernel-4.18.0-373.el8
commit-author Aharon Landau <aharonl@nvidia.com>
commit 5508546631a0f555d7088203dec2614e41b5106e
Empty-Commit: Cherry-Pick Conflicts during history rebuild.
Will be included in final tarball splat. Ref for failed cherry-pick at:
ciq/ciq_backports/kernel-4.18.0-373.el8/55085466.failed

Normally the zero fill would hide the missing initialization, but an
errant set to desc_size in reg_create() causes a crash:

  BUG: unable to handle page fault for address: 0000000800000000
  PGD 0 P4D 0
  Oops: 0000 [#1] SMP PTI
  CPU: 5 PID: 890 Comm: ib_write_bw Not tainted 5.15.0-rc4+ #47
  Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS rel-1.13.0-0-gf21b5a4aeb02-prebuilt.qemu.org 04/01/2014
  RIP: 0010:mlx5_ib_dereg_mr+0x14/0x3b0 [mlx5_ib]
  Code: 48 63 cd 4c 89 f7 48 89 0c 24 e8 37 30 03 e1 48 8b 0c 24 eb a0 90 0f 1f 44 00 00 41 56 41 55 41 54 55 53 48 89 fb 48 83 ec 30 <48> 8b 2f 65 48 8b 04 25 28 00 00 00 48 89 44 24 28 31 c0 8b 87 c8
  RSP: 0018:ffff88811afa3a60 EFLAGS: 00010286
  RAX: 000000000000001c RBX: 0000000800000000 RCX: 0000000000000000
  RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000800000000
  RBP: 0000000800000000 R08: 0000000000000000 R09: c0000000fffff7ff
  R10: ffff88811afa38f8 R11: ffff88811afa38f0 R12: ffffffffa02c7ac0
  R13: 0000000000000000 R14: ffff88811afa3cd8 R15: ffff88810772fa00
  FS:  00007f47b9080740(0000) GS:ffff88852cd40000(0000) knlGS:0000000000000000
  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
  CR2: 0000000800000000 CR3: 000000010761e003 CR4: 0000000000370ea0
  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
  Call Trace:
   mlx5_ib_free_odp_mr+0x95/0xc0 [mlx5_ib]
   mlx5_ib_dereg_mr+0x128/0x3b0 [mlx5_ib]
   ib_dereg_mr_user+0x45/0xb0 [ib_core]
   ? xas_load+0x8/0x80
   destroy_hw_idr_uobject+0x1a/0x50 [ib_uverbs]
   uverbs_destroy_uobject+0x2f/0x150 [ib_uverbs]
   uobj_destroy+0x3c/0x70 [ib_uverbs]
   ib_uverbs_cmd_verbs+0x467/0xb00 [ib_uverbs]
   ? uverbs_finalize_object+0x60/0x60 [ib_uverbs]
   ? ttwu_queue_wakelist+0xa9/0xe0
   ? pty_write+0x85/0x90
   ? file_tty_write.isra.33+0x214/0x330
   ? process_echoes+0x60/0x60
   ib_uverbs_ioctl+0xa7/0x110 [ib_uverbs]
   __x64_sys_ioctl+0x10d/0x8e0
   ? vfs_write+0x17f/0x260
   do_syscall_64+0x3c/0x80
   entry_SYSCALL_64_after_hwframe+0x44/0xae

Add the missing xarray initialization and remove the desc_size set.

Fixes: a639e66703ee ("RDMA/mlx5: Zero out ODP related items in the mlx5_ib_mr")
Link: https://lore.kernel.org/r/a4846a11c9de834663e521770da895007f9f0d30.1634642730.git.leonro@nvidia.com
	Signed-off-by: Aharon Landau <aharonl@nvidia.com>
	Reviewed-by: Maor Gottlieb <maorg@nvidia.com>
	Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
	Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
(cherry picked from commit 5508546631a0f555d7088203dec2614e41b5106e)
	Signed-off-by: Jonathan Maple <jmaple@ciq.com>

# Conflicts:
#	drivers/infiniband/hw/mlx5/mr.c
diff --cc drivers/infiniband/hw/mlx5/mr.c
index e8243e54599a,22e2f4d79743..000000000000
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@@ -1352,7 -1339,8 +1352,12 @@@ static struct mlx5_ib_mr *reg_create(st
  		goto err_2;
  	}
  	mr->mmkey.type = MLX5_MKEY_MR;
++<<<<<<< HEAD
 +	mr->desc_size = sizeof(struct mlx5_mtt);
++=======
+ 	mr->umem = umem;
+ 	set_mr_fields(dev, mr, umem->length, access_flags);
++>>>>>>> 5508546631a0 (RDMA/mlx5: Initialize the ODP xarray when creating an ODP MR)
  	kvfree(in);
  
  	mlx5_ib_dbg(dev, "mkey = 0x%x\n", mr->mmkey.key);
@@@ -1558,19 -1488,161 +1563,176 @@@ struct ib_mr *mlx5_ib_reg_user_mr(struc
  			return ERR_PTR(err);
  		}
  	}
 +
 +	if (is_odp_mr(mr)) {
 +		to_ib_umem_odp(mr->umem)->private = mr;
 +		err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
 +		if (err) {
 +			dereg_mr(dev, mr);
 +			return ERR_PTR(err);
 +		}
 +	}
 +
  	return &mr->ibmr;
++<<<<<<< HEAD
 +error:
 +	ib_umem_release(umem);
++=======
+ }
+ 
+ static struct ib_mr *create_user_odp_mr(struct ib_pd *pd, u64 start, u64 length,
+ 					u64 iova, int access_flags,
+ 					struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct ib_umem_odp *odp;
+ 	struct mlx5_ib_mr *mr;
+ 	int err;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	err = mlx5r_odp_create_eq(dev, &dev->odp_pf_eq);
+ 	if (err)
+ 		return ERR_PTR(err);
+ 	if (!start && length == U64_MAX) {
+ 		if (iova != 0)
+ 			return ERR_PTR(-EINVAL);
+ 		if (!(dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))
+ 			return ERR_PTR(-EINVAL);
+ 
+ 		mr = mlx5_ib_alloc_implicit_mr(to_mpd(pd), access_flags);
+ 		if (IS_ERR(mr))
+ 			return ERR_CAST(mr);
+ 		return &mr->ibmr;
+ 	}
+ 
+ 	/* ODP requires xlt update via umr to work. */
+ 	if (!mlx5_ib_can_load_pas_with_umr(dev, length))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	odp = ib_umem_odp_get(&dev->ib_dev, start, length, access_flags,
+ 			      &mlx5_mn_ops);
+ 	if (IS_ERR(odp))
+ 		return ERR_CAST(odp);
+ 
+ 	mr = alloc_cacheable_mr(pd, &odp->umem, iova, access_flags);
+ 	if (IS_ERR(mr)) {
+ 		ib_umem_release(&odp->umem);
+ 		return ERR_CAST(mr);
+ 	}
+ 	xa_init(&mr->implicit_children);
+ 
+ 	odp->private = mr;
+ 	err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 
+ 	err = mlx5_ib_init_odp_mr(mr);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 	return &mr->ibmr;
+ 
+ err_dereg_mr:
+ 	mlx5_ib_dereg_mr(&mr->ibmr, NULL);
+ 	return ERR_PTR(err);
+ }
+ 
+ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
+ 				  u64 iova, int access_flags,
+ 				  struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct ib_umem *umem;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	mlx5_ib_dbg(dev, "start 0x%llx, iova 0x%llx, length 0x%llx, access_flags 0x%x\n",
+ 		    start, iova, length, access_flags);
+ 
+ 	if (access_flags & IB_ACCESS_ON_DEMAND)
+ 		return create_user_odp_mr(pd, start, length, iova, access_flags,
+ 					  udata);
+ 	umem = ib_umem_get(&dev->ib_dev, start, length, access_flags);
+ 	if (IS_ERR(umem))
+ 		return ERR_CAST(umem);
+ 	return create_real_mr(pd, umem, iova, access_flags);
+ }
+ 
+ static void mlx5_ib_dmabuf_invalidate_cb(struct dma_buf_attachment *attach)
+ {
+ 	struct ib_umem_dmabuf *umem_dmabuf = attach->importer_priv;
+ 	struct mlx5_ib_mr *mr = umem_dmabuf->private;
+ 
+ 	dma_resv_assert_held(umem_dmabuf->attach->dmabuf->resv);
+ 
+ 	if (!umem_dmabuf->sgt)
+ 		return;
+ 
+ 	mlx5_ib_update_mr_pas(mr, MLX5_IB_UPD_XLT_ZAP);
+ 	ib_umem_dmabuf_unmap_pages(umem_dmabuf);
+ }
+ 
+ static struct dma_buf_attach_ops mlx5_ib_dmabuf_attach_ops = {
+ 	.allow_peer2peer = 1,
+ 	.move_notify = mlx5_ib_dmabuf_invalidate_cb,
+ };
+ 
+ struct ib_mr *mlx5_ib_reg_user_mr_dmabuf(struct ib_pd *pd, u64 offset,
+ 					 u64 length, u64 virt_addr,
+ 					 int fd, int access_flags,
+ 					 struct ib_udata *udata)
+ {
+ 	struct mlx5_ib_dev *dev = to_mdev(pd->device);
+ 	struct mlx5_ib_mr *mr = NULL;
+ 	struct ib_umem_dmabuf *umem_dmabuf;
+ 	int err;
+ 
+ 	if (!IS_ENABLED(CONFIG_INFINIBAND_USER_MEM) ||
+ 	    !IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING))
+ 		return ERR_PTR(-EOPNOTSUPP);
+ 
+ 	mlx5_ib_dbg(dev,
+ 		    "offset 0x%llx, virt_addr 0x%llx, length 0x%llx, fd %d, access_flags 0x%x\n",
+ 		    offset, virt_addr, length, fd, access_flags);
+ 
+ 	/* dmabuf requires xlt update via umr to work. */
+ 	if (!mlx5_ib_can_load_pas_with_umr(dev, length))
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	umem_dmabuf = ib_umem_dmabuf_get(&dev->ib_dev, offset, length, fd,
+ 					 access_flags,
+ 					 &mlx5_ib_dmabuf_attach_ops);
+ 	if (IS_ERR(umem_dmabuf)) {
+ 		mlx5_ib_dbg(dev, "umem_dmabuf get failed (%ld)\n",
+ 			    PTR_ERR(umem_dmabuf));
+ 		return ERR_CAST(umem_dmabuf);
+ 	}
+ 
+ 	mr = alloc_cacheable_mr(pd, &umem_dmabuf->umem, virt_addr,
+ 				access_flags);
+ 	if (IS_ERR(mr)) {
+ 		ib_umem_release(&umem_dmabuf->umem);
+ 		return ERR_CAST(mr);
+ 	}
+ 
+ 	mlx5_ib_dbg(dev, "mkey 0x%x\n", mr->mmkey.key);
+ 
+ 	atomic_add(ib_umem_num_pages(mr->umem), &dev->mdev->priv.reg_pages);
+ 	umem_dmabuf->private = mr;
+ 	err = mlx5r_store_odp_mkey(dev, &mr->mmkey);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 
+ 	err = mlx5_ib_init_dmabuf_mr(mr);
+ 	if (err)
+ 		goto err_dereg_mr;
+ 	return &mr->ibmr;
+ 
+ err_dereg_mr:
+ 	mlx5_ib_dereg_mr(&mr->ibmr, NULL);
++>>>>>>> 5508546631a0 (RDMA/mlx5: Initialize the ODP xarray when creating an ODP MR)
  	return ERR_PTR(err);
  }
  
* Unmerged path drivers/infiniband/hw/mlx5/mr.c
